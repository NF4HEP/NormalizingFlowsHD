2023-10-09 09:26:44.874624: Importing os...
2023-10-09 09:26:44.874699: Importing sys...
2023-10-09 09:26:44.874716: Importing and initializing argparse...
Visible devices: [3]
2023-10-09 09:26:44.895084: Importing timer from timeit...
2023-10-09 09:26:44.895699: Setting env variables for tf import (only device [3] will be available)...
2023-10-09 09:26:44.895746: Importing numpy...
2023-10-09 09:26:45.072514: Importing pandas...
2023-10-09 09:26:45.262314: Importing shutil...
2023-10-09 09:26:45.262341: Importing subprocess...
2023-10-09 09:26:45.262349: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-09 09:26:47.642596: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-09 09:26:48.022060: Importing textwrap...
2023-10-09 09:26:48.022089: Importing timeit...
2023-10-09 09:26:48.022099: Importing traceback...
2023-10-09 09:26:48.022105: Importing typing...
2023-10-09 09:26:48.022115: Setting tf configs...
2023-10-09 09:26:48.441605: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-09 09:26:49.775081: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

===========
Generating train data for run 245.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_245/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_245/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_245/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_245
self.data_kwargs: {'seed': 0}
self.x_data: [[2.147613   3.0747468  7.824332   ... 7.103659   2.4434035  1.8856683 ]
 [5.330741   5.782131   0.60243964 ... 0.4385597  6.6599607  1.2875487 ]
 [5.9696746  7.1899557  6.848176   ... 3.815116   2.6560376  7.342258  ]
 ...
 [3.6073484  5.5595417  0.9042071  ... 0.7584033  6.576529   1.36466   ]
 [3.793339   5.232353   1.1439581  ... 1.5372672  6.3050704  1.3603225 ]
 [1.2775126  3.7349403  9.479768   ... 7.3904037  2.7995863  1.7972767 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  826720    
 r)                                                              
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f10683ca6e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f106832a230>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f106832a230>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f1070561030>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f103822e0b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f103822e620>, <keras.callbacks.ModelCheckpoint object at 0x7f103822e770>, <keras.callbacks.EarlyStopping object at 0x7f103822e980>, <keras.callbacks.ReduceLROnPlateau object at 0x7f103822e9b0>, <keras.callbacks.TerminateOnNaN object at 0x7f103822e6e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_245/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 245/720 with hyperparameters:
timestamp = 2023-10-09 09:26:59.038969
ndims = 32
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.147613    3.0747468   7.824332    1.8133726   9.163057    0.34882736
  9.789957    4.1820974   9.68468     6.146849    7.005386    0.3680902
  2.6196527   1.2054405   2.7207217   1.3141394   2.9878392   4.696398
 -0.43681622  6.9252343   5.6900735   2.943952    6.358174    1.1145298
  6.7379575   9.321477    3.595148    7.318303    0.9570862   7.103659
  2.4434035   1.8856683 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 53: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-09 09:29:20.920 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1817.5289, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 142s - loss: nan - MinusLogProbMetric: 1817.5289 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 142s/epoch - 722ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 245.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_245/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_245/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_245/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_245
self.data_kwargs: {'seed': 0}
self.x_data: [[2.147613   3.0747468  7.824332   ... 7.103659   2.4434035  1.8856683 ]
 [5.330741   5.782131   0.60243964 ... 0.4385597  6.6599607  1.2875487 ]
 [5.9696746  7.1899557  6.848176   ... 3.815116   2.6560376  7.342258  ]
 ...
 [3.6073484  5.5595417  0.9042071  ... 0.7584033  6.576529   1.36466   ]
 [3.793339   5.232353   1.1439581  ... 1.5372672  6.3050704  1.3603225 ]
 [1.2775126  3.7349403  9.479768   ... 7.3904037  2.7995863  1.7972767 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f140d99d060>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f140df25570>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f140df25570>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d301b2da0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1404ebcb80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1404ebd0f0>, <keras.callbacks.ModelCheckpoint object at 0x7f1404ebd1b0>, <keras.callbacks.EarlyStopping object at 0x7f1404ebd420>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1404ebd450>, <keras.callbacks.TerminateOnNaN object at 0x7f1404ebd090>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_245/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 245/720 with hyperparameters:
timestamp = 2023-10-09 09:29:28.222623
ndims = 32
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.147613    3.0747468   7.824332    1.8133726   9.163057    0.34882736
  9.789957    4.1820974   9.68468     6.146849    7.005386    0.3680902
  2.6196527   1.2054405   2.7207217   1.3141394   2.9878392   4.696398
 -0.43681622  6.9252343   5.6900735   2.943952    6.358174    1.1145298
  6.7379575   9.321477    3.595148    7.318303    0.9570862   7.103659
  2.4434035   1.8856683 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 16: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-09 09:31:26.034 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2786.4688, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 117s - loss: nan - MinusLogProbMetric: 2786.4688 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 117s/epoch - 599ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 245.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_245/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_245/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_245/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_245
self.data_kwargs: {'seed': 0}
self.x_data: [[2.147613   3.0747468  7.824332   ... 7.103659   2.4434035  1.8856683 ]
 [5.330741   5.782131   0.60243964 ... 0.4385597  6.6599607  1.2875487 ]
 [5.9696746  7.1899557  6.848176   ... 3.815116   2.6560376  7.342258  ]
 ...
 [3.6073484  5.5595417  0.9042071  ... 0.7584033  6.576529   1.36466   ]
 [3.793339   5.232353   1.1439581  ... 1.5372672  6.3050704  1.3603225 ]
 [1.2775126  3.7349403  9.479768   ... 7.3904037  2.7995863  1.7972767 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f0f943588e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0f943edd50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0f943edd50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f13f47e3b80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f13f46a8460>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f13f46a89d0>, <keras.callbacks.ModelCheckpoint object at 0x7f13f46a8a90>, <keras.callbacks.EarlyStopping object at 0x7f13f46a8d00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f13f46a8d30>, <keras.callbacks.TerminateOnNaN object at 0x7f13f46a8970>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[1.9518368 , 3.3973382 , 9.531531  , ..., 7.757203  , 2.6203427 ,
        1.8047177 ],
       [2.0608988 , 4.2499633 , 8.220001  , ..., 7.0034    , 2.2546027 ,
        2.1452668 ],
       [2.843429  , 3.9051921 , 7.4207163 , ..., 7.300193  , 3.1696632 ,
        1.5780848 ],
       ...,
       [5.2732286 , 6.077929  , 1.3790838 , ..., 0.65630984, 7.270858  ,
        1.3828237 ],
       [4.184838  , 7.1681747 , 6.2432003 , ..., 4.410938  , 2.659537  ,
        7.4785604 ],
       [5.6083927 , 7.161964  , 6.0947328 , ..., 4.4707136 , 2.6326852 ,
        7.344711  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_245/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 245/720 with hyperparameters:
timestamp = 2023-10-09 09:31:32.383915
ndims = 32
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.147613    3.0747468   7.824332    1.8133726   9.163057    0.34882736
  9.789957    4.1820974   9.68468     6.146849    7.005386    0.3680902
  2.6196527   1.2054405   2.7207217   1.3141394   2.9878392   4.696398
 -0.43681622  6.9252343   5.6900735   2.943952    6.358174    1.1145298
  6.7379575   9.321477    3.595148    7.318303    0.9570862   7.103659
  2.4434035   1.8856683 ]
Epoch 1/1000
2023-10-09 09:34:13.305 
Epoch 1/1000 
	 loss: 1781.9368, MinusLogProbMetric: 1781.9368, val_loss: 837.7212, val_MinusLogProbMetric: 837.7212

Epoch 1: val_loss improved from inf to 837.72119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 162s - loss: 1781.9368 - MinusLogProbMetric: 1781.9368 - val_loss: 837.7212 - val_MinusLogProbMetric: 837.7212 - lr: 1.1111e-04 - 162s/epoch - 824ms/step
Epoch 2/1000
2023-10-09 09:35:04.134 
Epoch 2/1000 
	 loss: 622.9873, MinusLogProbMetric: 622.9873, val_loss: 534.9788, val_MinusLogProbMetric: 534.9788

Epoch 2: val_loss improved from 837.72119 to 534.97876, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 622.9873 - MinusLogProbMetric: 622.9873 - val_loss: 534.9788 - val_MinusLogProbMetric: 534.9788 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 3/1000
2023-10-09 09:35:54.718 
Epoch 3/1000 
	 loss: 577.2777, MinusLogProbMetric: 577.2777, val_loss: 494.9121, val_MinusLogProbMetric: 494.9121

Epoch 3: val_loss improved from 534.97876 to 494.91214, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 577.2777 - MinusLogProbMetric: 577.2777 - val_loss: 494.9121 - val_MinusLogProbMetric: 494.9121 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 4/1000
2023-10-09 09:36:47.193 
Epoch 4/1000 
	 loss: 514.7122, MinusLogProbMetric: 514.7122, val_loss: 375.1066, val_MinusLogProbMetric: 375.1066

Epoch 4: val_loss improved from 494.91214 to 375.10663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 514.7122 - MinusLogProbMetric: 514.7122 - val_loss: 375.1066 - val_MinusLogProbMetric: 375.1066 - lr: 1.1111e-04 - 52s/epoch - 268ms/step
Epoch 5/1000
2023-10-09 09:37:41.902 
Epoch 5/1000 
	 loss: 372.3949, MinusLogProbMetric: 372.3949, val_loss: 339.6668, val_MinusLogProbMetric: 339.6668

Epoch 5: val_loss improved from 375.10663 to 339.66678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 372.3949 - MinusLogProbMetric: 372.3949 - val_loss: 339.6668 - val_MinusLogProbMetric: 339.6668 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 6/1000
2023-10-09 09:38:32.194 
Epoch 6/1000 
	 loss: 397.1552, MinusLogProbMetric: 397.1552, val_loss: 308.6963, val_MinusLogProbMetric: 308.6963

Epoch 6: val_loss improved from 339.66678 to 308.69632, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 397.1552 - MinusLogProbMetric: 397.1552 - val_loss: 308.6963 - val_MinusLogProbMetric: 308.6963 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 7/1000
2023-10-09 09:39:23.433 
Epoch 7/1000 
	 loss: 289.2946, MinusLogProbMetric: 289.2946, val_loss: 298.0213, val_MinusLogProbMetric: 298.0213

Epoch 7: val_loss improved from 308.69632 to 298.02133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 289.2946 - MinusLogProbMetric: 289.2946 - val_loss: 298.0213 - val_MinusLogProbMetric: 298.0213 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 8/1000
2023-10-09 09:40:13.985 
Epoch 8/1000 
	 loss: 292.3803, MinusLogProbMetric: 292.3803, val_loss: 278.6218, val_MinusLogProbMetric: 278.6218

Epoch 8: val_loss improved from 298.02133 to 278.62180, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 292.3803 - MinusLogProbMetric: 292.3803 - val_loss: 278.6218 - val_MinusLogProbMetric: 278.6218 - lr: 1.1111e-04 - 50s/epoch - 258ms/step
Epoch 9/1000
2023-10-09 09:41:04.470 
Epoch 9/1000 
	 loss: 258.2437, MinusLogProbMetric: 258.2437, val_loss: 255.6042, val_MinusLogProbMetric: 255.6042

Epoch 9: val_loss improved from 278.62180 to 255.60419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 258.2437 - MinusLogProbMetric: 258.2437 - val_loss: 255.6042 - val_MinusLogProbMetric: 255.6042 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 10/1000
2023-10-09 09:41:54.111 
Epoch 10/1000 
	 loss: 238.0067, MinusLogProbMetric: 238.0067, val_loss: 229.4152, val_MinusLogProbMetric: 229.4152

Epoch 10: val_loss improved from 255.60419 to 229.41521, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 238.0067 - MinusLogProbMetric: 238.0067 - val_loss: 229.4152 - val_MinusLogProbMetric: 229.4152 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 11/1000
2023-10-09 09:42:44.229 
Epoch 11/1000 
	 loss: 224.0637, MinusLogProbMetric: 224.0637, val_loss: 218.0925, val_MinusLogProbMetric: 218.0925

Epoch 11: val_loss improved from 229.41521 to 218.09248, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 224.0637 - MinusLogProbMetric: 224.0637 - val_loss: 218.0925 - val_MinusLogProbMetric: 218.0925 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 12/1000
2023-10-09 09:43:35.074 
Epoch 12/1000 
	 loss: 298.2773, MinusLogProbMetric: 298.2773, val_loss: 387.0004, val_MinusLogProbMetric: 387.0004

Epoch 12: val_loss did not improve from 218.09248
196/196 - 50s - loss: 298.2773 - MinusLogProbMetric: 298.2773 - val_loss: 387.0004 - val_MinusLogProbMetric: 387.0004 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 13/1000
2023-10-09 09:44:27.840 
Epoch 13/1000 
	 loss: 332.5270, MinusLogProbMetric: 332.5270, val_loss: 291.5561, val_MinusLogProbMetric: 291.5561

Epoch 13: val_loss did not improve from 218.09248
196/196 - 53s - loss: 332.5270 - MinusLogProbMetric: 332.5270 - val_loss: 291.5561 - val_MinusLogProbMetric: 291.5561 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 14/1000
2023-10-09 09:45:18.420 
Epoch 14/1000 
	 loss: 284.2782, MinusLogProbMetric: 284.2782, val_loss: 275.4059, val_MinusLogProbMetric: 275.4059

Epoch 14: val_loss did not improve from 218.09248
196/196 - 51s - loss: 284.2782 - MinusLogProbMetric: 284.2782 - val_loss: 275.4059 - val_MinusLogProbMetric: 275.4059 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 15/1000
2023-10-09 09:46:13.779 
Epoch 15/1000 
	 loss: 263.1150, MinusLogProbMetric: 263.1150, val_loss: 248.8557, val_MinusLogProbMetric: 248.8557

Epoch 15: val_loss did not improve from 218.09248
196/196 - 55s - loss: 263.1150 - MinusLogProbMetric: 263.1150 - val_loss: 248.8557 - val_MinusLogProbMetric: 248.8557 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 16/1000
2023-10-09 09:47:05.202 
Epoch 16/1000 
	 loss: 205.7706, MinusLogProbMetric: 205.7706, val_loss: 161.4548, val_MinusLogProbMetric: 161.4548

Epoch 16: val_loss improved from 218.09248 to 161.45482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 205.7706 - MinusLogProbMetric: 205.7706 - val_loss: 161.4548 - val_MinusLogProbMetric: 161.4548 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 17/1000
2023-10-09 09:48:02.708 
Epoch 17/1000 
	 loss: 151.4609, MinusLogProbMetric: 151.4609, val_loss: 143.4472, val_MinusLogProbMetric: 143.4472

Epoch 17: val_loss improved from 161.45482 to 143.44717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 151.4609 - MinusLogProbMetric: 151.4609 - val_loss: 143.4472 - val_MinusLogProbMetric: 143.4472 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 18/1000
2023-10-09 09:48:59.853 
Epoch 18/1000 
	 loss: 140.0451, MinusLogProbMetric: 140.0451, val_loss: 135.9142, val_MinusLogProbMetric: 135.9142

Epoch 18: val_loss improved from 143.44717 to 135.91425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 140.0451 - MinusLogProbMetric: 140.0451 - val_loss: 135.9142 - val_MinusLogProbMetric: 135.9142 - lr: 1.1111e-04 - 57s/epoch - 293ms/step
Epoch 19/1000
2023-10-09 09:50:04.358 
Epoch 19/1000 
	 loss: 133.5864, MinusLogProbMetric: 133.5864, val_loss: 131.8105, val_MinusLogProbMetric: 131.8105

Epoch 19: val_loss improved from 135.91425 to 131.81050, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 64s - loss: 133.5864 - MinusLogProbMetric: 133.5864 - val_loss: 131.8105 - val_MinusLogProbMetric: 131.8105 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 20/1000
2023-10-09 09:51:07.885 
Epoch 20/1000 
	 loss: 132.9868, MinusLogProbMetric: 132.9868, val_loss: 131.3021, val_MinusLogProbMetric: 131.3021

Epoch 20: val_loss improved from 131.81050 to 131.30214, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 64s - loss: 132.9868 - MinusLogProbMetric: 132.9868 - val_loss: 131.3021 - val_MinusLogProbMetric: 131.3021 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 21/1000
2023-10-09 09:52:09.835 
Epoch 21/1000 
	 loss: 126.0575, MinusLogProbMetric: 126.0575, val_loss: 122.4103, val_MinusLogProbMetric: 122.4103

Epoch 21: val_loss improved from 131.30214 to 122.41028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 62s - loss: 126.0575 - MinusLogProbMetric: 126.0575 - val_loss: 122.4103 - val_MinusLogProbMetric: 122.4103 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 22/1000
2023-10-09 09:53:13.075 
Epoch 22/1000 
	 loss: 121.0466, MinusLogProbMetric: 121.0466, val_loss: 119.4832, val_MinusLogProbMetric: 119.4832

Epoch 22: val_loss improved from 122.41028 to 119.48319, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 63s - loss: 121.0466 - MinusLogProbMetric: 121.0466 - val_loss: 119.4832 - val_MinusLogProbMetric: 119.4832 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 23/1000
2023-10-09 09:54:11.314 
Epoch 23/1000 
	 loss: 117.2590, MinusLogProbMetric: 117.2590, val_loss: 115.3710, val_MinusLogProbMetric: 115.3710

Epoch 23: val_loss improved from 119.48319 to 115.37104, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 58s - loss: 117.2590 - MinusLogProbMetric: 117.2590 - val_loss: 115.3710 - val_MinusLogProbMetric: 115.3710 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 24/1000
2023-10-09 09:55:11.637 
Epoch 24/1000 
	 loss: 114.4890, MinusLogProbMetric: 114.4890, val_loss: 119.4337, val_MinusLogProbMetric: 119.4337

Epoch 24: val_loss did not improve from 115.37104
196/196 - 60s - loss: 114.4890 - MinusLogProbMetric: 114.4890 - val_loss: 119.4337 - val_MinusLogProbMetric: 119.4337 - lr: 1.1111e-04 - 60s/epoch - 304ms/step
Epoch 25/1000
2023-10-09 09:56:10.643 
Epoch 25/1000 
	 loss: 114.9793, MinusLogProbMetric: 114.9793, val_loss: 111.8185, val_MinusLogProbMetric: 111.8185

Epoch 25: val_loss improved from 115.37104 to 111.81854, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 114.9793 - MinusLogProbMetric: 114.9793 - val_loss: 111.8185 - val_MinusLogProbMetric: 111.8185 - lr: 1.1111e-04 - 60s/epoch - 305ms/step
Epoch 26/1000
2023-10-09 09:57:12.842 
Epoch 26/1000 
	 loss: 110.3851, MinusLogProbMetric: 110.3851, val_loss: 110.0458, val_MinusLogProbMetric: 110.0458

Epoch 26: val_loss improved from 111.81854 to 110.04578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 62s - loss: 110.3851 - MinusLogProbMetric: 110.3851 - val_loss: 110.0458 - val_MinusLogProbMetric: 110.0458 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 27/1000
2023-10-09 09:58:14.404 
Epoch 27/1000 
	 loss: 107.7890, MinusLogProbMetric: 107.7890, val_loss: 107.0523, val_MinusLogProbMetric: 107.0523

Epoch 27: val_loss improved from 110.04578 to 107.05235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 62s - loss: 107.7890 - MinusLogProbMetric: 107.7890 - val_loss: 107.0523 - val_MinusLogProbMetric: 107.0523 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 28/1000
2023-10-09 09:59:14.692 
Epoch 28/1000 
	 loss: 105.7100, MinusLogProbMetric: 105.7100, val_loss: 104.7662, val_MinusLogProbMetric: 104.7662

Epoch 28: val_loss improved from 107.05235 to 104.76622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 105.7100 - MinusLogProbMetric: 105.7100 - val_loss: 104.7662 - val_MinusLogProbMetric: 104.7662 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 29/1000
2023-10-09 10:00:15.614 
Epoch 29/1000 
	 loss: 103.8001, MinusLogProbMetric: 103.8001, val_loss: 102.8396, val_MinusLogProbMetric: 102.8396

Epoch 29: val_loss improved from 104.76622 to 102.83960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 61s - loss: 103.8001 - MinusLogProbMetric: 103.8001 - val_loss: 102.8396 - val_MinusLogProbMetric: 102.8396 - lr: 1.1111e-04 - 61s/epoch - 310ms/step
Epoch 30/1000
2023-10-09 10:01:11.213 
Epoch 30/1000 
	 loss: 102.4158, MinusLogProbMetric: 102.4158, val_loss: 101.1523, val_MinusLogProbMetric: 101.1523

Epoch 30: val_loss improved from 102.83960 to 101.15228, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 102.4158 - MinusLogProbMetric: 102.4158 - val_loss: 101.1523 - val_MinusLogProbMetric: 101.1523 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 31/1000
2023-10-09 10:02:11.697 
Epoch 31/1000 
	 loss: 100.6065, MinusLogProbMetric: 100.6065, val_loss: 100.1096, val_MinusLogProbMetric: 100.1096

Epoch 31: val_loss improved from 101.15228 to 100.10965, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 100.6065 - MinusLogProbMetric: 100.6065 - val_loss: 100.1096 - val_MinusLogProbMetric: 100.1096 - lr: 1.1111e-04 - 60s/epoch - 309ms/step
Epoch 32/1000
2023-10-09 10:03:06.915 
Epoch 32/1000 
	 loss: 99.0178, MinusLogProbMetric: 99.0178, val_loss: 98.1436, val_MinusLogProbMetric: 98.1436

Epoch 32: val_loss improved from 100.10965 to 98.14363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 99.0178 - MinusLogProbMetric: 99.0178 - val_loss: 98.1436 - val_MinusLogProbMetric: 98.1436 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 33/1000
2023-10-09 10:04:03.168 
Epoch 33/1000 
	 loss: 97.5027, MinusLogProbMetric: 97.5027, val_loss: 96.6936, val_MinusLogProbMetric: 96.6936

Epoch 33: val_loss improved from 98.14363 to 96.69363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 97.5027 - MinusLogProbMetric: 97.5027 - val_loss: 96.6936 - val_MinusLogProbMetric: 96.6936 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 34/1000
2023-10-09 10:04:56.515 
Epoch 34/1000 
	 loss: 96.5490, MinusLogProbMetric: 96.5490, val_loss: 95.5421, val_MinusLogProbMetric: 95.5421

Epoch 34: val_loss improved from 96.69363 to 95.54207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 96.5490 - MinusLogProbMetric: 96.5490 - val_loss: 95.5421 - val_MinusLogProbMetric: 95.5421 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 35/1000
2023-10-09 10:05:51.127 
Epoch 35/1000 
	 loss: 94.6578, MinusLogProbMetric: 94.6578, val_loss: 93.8313, val_MinusLogProbMetric: 93.8313

Epoch 35: val_loss improved from 95.54207 to 93.83134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 94.6578 - MinusLogProbMetric: 94.6578 - val_loss: 93.8313 - val_MinusLogProbMetric: 93.8313 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 36/1000
2023-10-09 10:06:43.821 
Epoch 36/1000 
	 loss: 93.1661, MinusLogProbMetric: 93.1661, val_loss: 91.9160, val_MinusLogProbMetric: 91.9160

Epoch 36: val_loss improved from 93.83134 to 91.91601, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 93.1661 - MinusLogProbMetric: 93.1661 - val_loss: 91.9160 - val_MinusLogProbMetric: 91.9160 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 37/1000
2023-10-09 10:07:35.667 
Epoch 37/1000 
	 loss: 92.1809, MinusLogProbMetric: 92.1809, val_loss: 91.5851, val_MinusLogProbMetric: 91.5851

Epoch 37: val_loss improved from 91.91601 to 91.58505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 92.1809 - MinusLogProbMetric: 92.1809 - val_loss: 91.5851 - val_MinusLogProbMetric: 91.5851 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 38/1000
2023-10-09 10:08:29.113 
Epoch 38/1000 
	 loss: 90.2622, MinusLogProbMetric: 90.2622, val_loss: 89.6672, val_MinusLogProbMetric: 89.6672

Epoch 38: val_loss improved from 91.58505 to 89.66718, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 90.2622 - MinusLogProbMetric: 90.2622 - val_loss: 89.6672 - val_MinusLogProbMetric: 89.6672 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 39/1000
2023-10-09 10:09:24.248 
Epoch 39/1000 
	 loss: 88.8716, MinusLogProbMetric: 88.8716, val_loss: 88.1727, val_MinusLogProbMetric: 88.1727

Epoch 39: val_loss improved from 89.66718 to 88.17265, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 88.8716 - MinusLogProbMetric: 88.8716 - val_loss: 88.1727 - val_MinusLogProbMetric: 88.1727 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 40/1000
2023-10-09 10:10:16.567 
Epoch 40/1000 
	 loss: 87.7791, MinusLogProbMetric: 87.7791, val_loss: 86.8210, val_MinusLogProbMetric: 86.8210

Epoch 40: val_loss improved from 88.17265 to 86.82104, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 87.7791 - MinusLogProbMetric: 87.7791 - val_loss: 86.8210 - val_MinusLogProbMetric: 86.8210 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 41/1000
2023-10-09 10:11:10.336 
Epoch 41/1000 
	 loss: 87.1150, MinusLogProbMetric: 87.1150, val_loss: 86.3802, val_MinusLogProbMetric: 86.3802

Epoch 41: val_loss improved from 86.82104 to 86.38020, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 87.1150 - MinusLogProbMetric: 87.1150 - val_loss: 86.3802 - val_MinusLogProbMetric: 86.3802 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 42/1000
2023-10-09 10:12:03.109 
Epoch 42/1000 
	 loss: 86.7145, MinusLogProbMetric: 86.7145, val_loss: 86.2746, val_MinusLogProbMetric: 86.2746

Epoch 42: val_loss improved from 86.38020 to 86.27456, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 86.7145 - MinusLogProbMetric: 86.7145 - val_loss: 86.2746 - val_MinusLogProbMetric: 86.2746 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 43/1000
2023-10-09 10:12:55.383 
Epoch 43/1000 
	 loss: 84.5729, MinusLogProbMetric: 84.5729, val_loss: 84.6217, val_MinusLogProbMetric: 84.6217

Epoch 43: val_loss improved from 86.27456 to 84.62174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 84.5729 - MinusLogProbMetric: 84.5729 - val_loss: 84.6217 - val_MinusLogProbMetric: 84.6217 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 44/1000
2023-10-09 10:13:48.399 
Epoch 44/1000 
	 loss: 83.4510, MinusLogProbMetric: 83.4510, val_loss: 84.8497, val_MinusLogProbMetric: 84.8497

Epoch 44: val_loss did not improve from 84.62174
196/196 - 52s - loss: 83.4510 - MinusLogProbMetric: 83.4510 - val_loss: 84.8497 - val_MinusLogProbMetric: 84.8497 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 45/1000
2023-10-09 10:14:42.288 
Epoch 45/1000 
	 loss: 82.8220, MinusLogProbMetric: 82.8220, val_loss: 81.9954, val_MinusLogProbMetric: 81.9954

Epoch 45: val_loss improved from 84.62174 to 81.99543, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 82.8220 - MinusLogProbMetric: 82.8220 - val_loss: 81.9954 - val_MinusLogProbMetric: 81.9954 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 46/1000
2023-10-09 10:15:35.165 
Epoch 46/1000 
	 loss: 81.5883, MinusLogProbMetric: 81.5883, val_loss: 97.7244, val_MinusLogProbMetric: 97.7244

Epoch 46: val_loss did not improve from 81.99543
196/196 - 52s - loss: 81.5883 - MinusLogProbMetric: 81.5883 - val_loss: 97.7244 - val_MinusLogProbMetric: 97.7244 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 47/1000
2023-10-09 10:16:28.057 
Epoch 47/1000 
	 loss: 87.0720, MinusLogProbMetric: 87.0720, val_loss: 84.1075, val_MinusLogProbMetric: 84.1075

Epoch 47: val_loss did not improve from 81.99543
196/196 - 53s - loss: 87.0720 - MinusLogProbMetric: 87.0720 - val_loss: 84.1075 - val_MinusLogProbMetric: 84.1075 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 48/1000
2023-10-09 10:17:21.404 
Epoch 48/1000 
	 loss: 83.2201, MinusLogProbMetric: 83.2201, val_loss: 83.2047, val_MinusLogProbMetric: 83.2047

Epoch 48: val_loss did not improve from 81.99543
196/196 - 53s - loss: 83.2201 - MinusLogProbMetric: 83.2201 - val_loss: 83.2047 - val_MinusLogProbMetric: 83.2047 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 49/1000
2023-10-09 10:18:15.867 
Epoch 49/1000 
	 loss: 81.6369, MinusLogProbMetric: 81.6369, val_loss: 80.4934, val_MinusLogProbMetric: 80.4934

Epoch 49: val_loss improved from 81.99543 to 80.49338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 81.6369 - MinusLogProbMetric: 81.6369 - val_loss: 80.4934 - val_MinusLogProbMetric: 80.4934 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 50/1000
2023-10-09 10:19:09.231 
Epoch 50/1000 
	 loss: 78.8393, MinusLogProbMetric: 78.8393, val_loss: 77.8001, val_MinusLogProbMetric: 77.8001

Epoch 50: val_loss improved from 80.49338 to 77.80009, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 78.8393 - MinusLogProbMetric: 78.8393 - val_loss: 77.8001 - val_MinusLogProbMetric: 77.8001 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 51/1000
2023-10-09 10:20:03.321 
Epoch 51/1000 
	 loss: 85.2260, MinusLogProbMetric: 85.2260, val_loss: 77.3689, val_MinusLogProbMetric: 77.3689

Epoch 51: val_loss improved from 77.80009 to 77.36887, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 85.2260 - MinusLogProbMetric: 85.2260 - val_loss: 77.3689 - val_MinusLogProbMetric: 77.3689 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 52/1000
2023-10-09 10:20:56.492 
Epoch 52/1000 
	 loss: 76.2508, MinusLogProbMetric: 76.2508, val_loss: 75.9175, val_MinusLogProbMetric: 75.9175

Epoch 52: val_loss improved from 77.36887 to 75.91753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 76.2508 - MinusLogProbMetric: 76.2508 - val_loss: 75.9175 - val_MinusLogProbMetric: 75.9175 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 53/1000
2023-10-09 10:21:49.574 
Epoch 53/1000 
	 loss: 74.6047, MinusLogProbMetric: 74.6047, val_loss: 73.9225, val_MinusLogProbMetric: 73.9225

Epoch 53: val_loss improved from 75.91753 to 73.92247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 74.6047 - MinusLogProbMetric: 74.6047 - val_loss: 73.9225 - val_MinusLogProbMetric: 73.9225 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 54/1000
2023-10-09 10:22:42.173 
Epoch 54/1000 
	 loss: 73.3269, MinusLogProbMetric: 73.3269, val_loss: 72.8142, val_MinusLogProbMetric: 72.8142

Epoch 54: val_loss improved from 73.92247 to 72.81422, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 73.3269 - MinusLogProbMetric: 73.3269 - val_loss: 72.8142 - val_MinusLogProbMetric: 72.8142 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 55/1000
2023-10-09 10:23:36.016 
Epoch 55/1000 
	 loss: 72.4260, MinusLogProbMetric: 72.4260, val_loss: 72.0726, val_MinusLogProbMetric: 72.0726

Epoch 55: val_loss improved from 72.81422 to 72.07263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 72.4260 - MinusLogProbMetric: 72.4260 - val_loss: 72.0726 - val_MinusLogProbMetric: 72.0726 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 56/1000
2023-10-09 10:24:29.702 
Epoch 56/1000 
	 loss: 72.0347, MinusLogProbMetric: 72.0347, val_loss: 72.1712, val_MinusLogProbMetric: 72.1712

Epoch 56: val_loss did not improve from 72.07263
196/196 - 53s - loss: 72.0347 - MinusLogProbMetric: 72.0347 - val_loss: 72.1712 - val_MinusLogProbMetric: 72.1712 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 57/1000
2023-10-09 10:25:22.914 
Epoch 57/1000 
	 loss: 78.6901, MinusLogProbMetric: 78.6901, val_loss: 76.8707, val_MinusLogProbMetric: 76.8707

Epoch 57: val_loss did not improve from 72.07263
196/196 - 53s - loss: 78.6901 - MinusLogProbMetric: 78.6901 - val_loss: 76.8707 - val_MinusLogProbMetric: 76.8707 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 58/1000
2023-10-09 10:26:16.398 
Epoch 58/1000 
	 loss: 74.8631, MinusLogProbMetric: 74.8631, val_loss: 75.4969, val_MinusLogProbMetric: 75.4969

Epoch 58: val_loss did not improve from 72.07263
196/196 - 53s - loss: 74.8631 - MinusLogProbMetric: 74.8631 - val_loss: 75.4969 - val_MinusLogProbMetric: 75.4969 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 59/1000
2023-10-09 10:27:10.416 
Epoch 59/1000 
	 loss: 72.5997, MinusLogProbMetric: 72.5997, val_loss: 72.2236, val_MinusLogProbMetric: 72.2236

Epoch 59: val_loss did not improve from 72.07263
196/196 - 54s - loss: 72.5997 - MinusLogProbMetric: 72.5997 - val_loss: 72.2236 - val_MinusLogProbMetric: 72.2236 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 60/1000
2023-10-09 10:28:02.397 
Epoch 60/1000 
	 loss: 71.0303, MinusLogProbMetric: 71.0303, val_loss: 70.1949, val_MinusLogProbMetric: 70.1949

Epoch 60: val_loss improved from 72.07263 to 70.19492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 71.0303 - MinusLogProbMetric: 71.0303 - val_loss: 70.1949 - val_MinusLogProbMetric: 70.1949 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 61/1000
2023-10-09 10:28:56.879 
Epoch 61/1000 
	 loss: 70.2741, MinusLogProbMetric: 70.2741, val_loss: 69.4047, val_MinusLogProbMetric: 69.4047

Epoch 61: val_loss improved from 70.19492 to 69.40475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 70.2741 - MinusLogProbMetric: 70.2741 - val_loss: 69.4047 - val_MinusLogProbMetric: 69.4047 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 62/1000
2023-10-09 10:29:52.866 
Epoch 62/1000 
	 loss: 68.5558, MinusLogProbMetric: 68.5558, val_loss: 68.1320, val_MinusLogProbMetric: 68.1320

Epoch 62: val_loss improved from 69.40475 to 68.13205, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 68.5558 - MinusLogProbMetric: 68.5558 - val_loss: 68.1320 - val_MinusLogProbMetric: 68.1320 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 63/1000
2023-10-09 10:30:44.976 
Epoch 63/1000 
	 loss: 67.7310, MinusLogProbMetric: 67.7310, val_loss: 67.0051, val_MinusLogProbMetric: 67.0051

Epoch 63: val_loss improved from 68.13205 to 67.00513, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 67.7310 - MinusLogProbMetric: 67.7310 - val_loss: 67.0051 - val_MinusLogProbMetric: 67.0051 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 64/1000
2023-10-09 10:31:39.072 
Epoch 64/1000 
	 loss: 66.6417, MinusLogProbMetric: 66.6417, val_loss: 65.9427, val_MinusLogProbMetric: 65.9427

Epoch 64: val_loss improved from 67.00513 to 65.94266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 66.6417 - MinusLogProbMetric: 66.6417 - val_loss: 65.9427 - val_MinusLogProbMetric: 65.9427 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 65/1000
2023-10-09 10:32:33.211 
Epoch 65/1000 
	 loss: 65.4018, MinusLogProbMetric: 65.4018, val_loss: 65.2067, val_MinusLogProbMetric: 65.2067

Epoch 65: val_loss improved from 65.94266 to 65.20670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 65.4018 - MinusLogProbMetric: 65.4018 - val_loss: 65.2067 - val_MinusLogProbMetric: 65.2067 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 66/1000
2023-10-09 10:33:24.182 
Epoch 66/1000 
	 loss: 138.2614, MinusLogProbMetric: 138.2614, val_loss: 121.0704, val_MinusLogProbMetric: 121.0704

Epoch 66: val_loss did not improve from 65.20670
196/196 - 50s - loss: 138.2614 - MinusLogProbMetric: 138.2614 - val_loss: 121.0704 - val_MinusLogProbMetric: 121.0704 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 67/1000
2023-10-09 10:34:15.248 
Epoch 67/1000 
	 loss: 108.8232, MinusLogProbMetric: 108.8232, val_loss: 97.8445, val_MinusLogProbMetric: 97.8445

Epoch 67: val_loss did not improve from 65.20670
196/196 - 51s - loss: 108.8232 - MinusLogProbMetric: 108.8232 - val_loss: 97.8445 - val_MinusLogProbMetric: 97.8445 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 68/1000
2023-10-09 10:35:06.930 
Epoch 68/1000 
	 loss: 91.3171, MinusLogProbMetric: 91.3171, val_loss: 86.0010, val_MinusLogProbMetric: 86.0010

Epoch 68: val_loss did not improve from 65.20670
196/196 - 52s - loss: 91.3171 - MinusLogProbMetric: 91.3171 - val_loss: 86.0010 - val_MinusLogProbMetric: 86.0010 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 69/1000
2023-10-09 10:35:59.794 
Epoch 69/1000 
	 loss: 82.0842, MinusLogProbMetric: 82.0842, val_loss: 79.5394, val_MinusLogProbMetric: 79.5394

Epoch 69: val_loss did not improve from 65.20670
196/196 - 53s - loss: 82.0842 - MinusLogProbMetric: 82.0842 - val_loss: 79.5394 - val_MinusLogProbMetric: 79.5394 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 70/1000
2023-10-09 10:36:52.143 
Epoch 70/1000 
	 loss: 76.5658, MinusLogProbMetric: 76.5658, val_loss: 74.5992, val_MinusLogProbMetric: 74.5992

Epoch 70: val_loss did not improve from 65.20670
196/196 - 52s - loss: 76.5658 - MinusLogProbMetric: 76.5658 - val_loss: 74.5992 - val_MinusLogProbMetric: 74.5992 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 71/1000
2023-10-09 10:37:45.393 
Epoch 71/1000 
	 loss: 73.1948, MinusLogProbMetric: 73.1948, val_loss: 71.5675, val_MinusLogProbMetric: 71.5675

Epoch 71: val_loss did not improve from 65.20670
196/196 - 53s - loss: 73.1948 - MinusLogProbMetric: 73.1948 - val_loss: 71.5675 - val_MinusLogProbMetric: 71.5675 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 72/1000
2023-10-09 10:38:36.463 
Epoch 72/1000 
	 loss: 70.6720, MinusLogProbMetric: 70.6720, val_loss: 69.3786, val_MinusLogProbMetric: 69.3786

Epoch 72: val_loss did not improve from 65.20670
196/196 - 51s - loss: 70.6720 - MinusLogProbMetric: 70.6720 - val_loss: 69.3786 - val_MinusLogProbMetric: 69.3786 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 73/1000
2023-10-09 10:39:30.503 
Epoch 73/1000 
	 loss: 68.5135, MinusLogProbMetric: 68.5135, val_loss: 67.6558, val_MinusLogProbMetric: 67.6558

Epoch 73: val_loss did not improve from 65.20670
196/196 - 54s - loss: 68.5135 - MinusLogProbMetric: 68.5135 - val_loss: 67.6558 - val_MinusLogProbMetric: 67.6558 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 74/1000
2023-10-09 10:40:22.098 
Epoch 74/1000 
	 loss: 67.0346, MinusLogProbMetric: 67.0346, val_loss: 66.2654, val_MinusLogProbMetric: 66.2654

Epoch 74: val_loss did not improve from 65.20670
196/196 - 52s - loss: 67.0346 - MinusLogProbMetric: 67.0346 - val_loss: 66.2654 - val_MinusLogProbMetric: 66.2654 - lr: 1.1111e-04 - 52s/epoch - 263ms/step
Epoch 75/1000
2023-10-09 10:41:15.726 
Epoch 75/1000 
	 loss: 66.4158, MinusLogProbMetric: 66.4158, val_loss: 64.6354, val_MinusLogProbMetric: 64.6354

Epoch 75: val_loss improved from 65.20670 to 64.63544, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 66.4158 - MinusLogProbMetric: 66.4158 - val_loss: 64.6354 - val_MinusLogProbMetric: 64.6354 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 76/1000
2023-10-09 10:42:08.825 
Epoch 76/1000 
	 loss: 63.8572, MinusLogProbMetric: 63.8572, val_loss: 62.9628, val_MinusLogProbMetric: 62.9628

Epoch 76: val_loss improved from 64.63544 to 62.96277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 63.8572 - MinusLogProbMetric: 63.8572 - val_loss: 62.9628 - val_MinusLogProbMetric: 62.9628 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 77/1000
2023-10-09 10:43:03.350 
Epoch 77/1000 
	 loss: 62.7121, MinusLogProbMetric: 62.7121, val_loss: 62.2718, val_MinusLogProbMetric: 62.2718

Epoch 77: val_loss improved from 62.96277 to 62.27182, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 62.7121 - MinusLogProbMetric: 62.7121 - val_loss: 62.2718 - val_MinusLogProbMetric: 62.2718 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 78/1000
2023-10-09 10:43:56.419 
Epoch 78/1000 
	 loss: 61.7595, MinusLogProbMetric: 61.7595, val_loss: 60.9158, val_MinusLogProbMetric: 60.9158

Epoch 78: val_loss improved from 62.27182 to 60.91578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 61.7595 - MinusLogProbMetric: 61.7595 - val_loss: 60.9158 - val_MinusLogProbMetric: 60.9158 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 79/1000
2023-10-09 10:44:50.845 
Epoch 79/1000 
	 loss: 60.7012, MinusLogProbMetric: 60.7012, val_loss: 60.2728, val_MinusLogProbMetric: 60.2728

Epoch 79: val_loss improved from 60.91578 to 60.27279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 60.7012 - MinusLogProbMetric: 60.7012 - val_loss: 60.2728 - val_MinusLogProbMetric: 60.2728 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 80/1000
2023-10-09 10:45:45.743 
Epoch 80/1000 
	 loss: 59.9597, MinusLogProbMetric: 59.9597, val_loss: 59.4743, val_MinusLogProbMetric: 59.4743

Epoch 80: val_loss improved from 60.27279 to 59.47426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 59.9597 - MinusLogProbMetric: 59.9597 - val_loss: 59.4743 - val_MinusLogProbMetric: 59.4743 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 81/1000
2023-10-09 10:46:40.871 
Epoch 81/1000 
	 loss: 59.2234, MinusLogProbMetric: 59.2234, val_loss: 58.8129, val_MinusLogProbMetric: 58.8129

Epoch 81: val_loss improved from 59.47426 to 58.81292, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 59.2234 - MinusLogProbMetric: 59.2234 - val_loss: 58.8129 - val_MinusLogProbMetric: 58.8129 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 82/1000
2023-10-09 10:47:39.017 
Epoch 82/1000 
	 loss: 58.4374, MinusLogProbMetric: 58.4374, val_loss: 57.9756, val_MinusLogProbMetric: 57.9756

Epoch 82: val_loss improved from 58.81292 to 57.97561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 58s - loss: 58.4374 - MinusLogProbMetric: 58.4374 - val_loss: 57.9756 - val_MinusLogProbMetric: 57.9756 - lr: 1.1111e-04 - 58s/epoch - 297ms/step
Epoch 83/1000
2023-10-09 10:48:33.317 
Epoch 83/1000 
	 loss: 57.6762, MinusLogProbMetric: 57.6762, val_loss: 57.3102, val_MinusLogProbMetric: 57.3102

Epoch 83: val_loss improved from 57.97561 to 57.31024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 57.6762 - MinusLogProbMetric: 57.6762 - val_loss: 57.3102 - val_MinusLogProbMetric: 57.3102 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 84/1000
2023-10-09 10:49:25.478 
Epoch 84/1000 
	 loss: 56.9927, MinusLogProbMetric: 56.9927, val_loss: 56.8402, val_MinusLogProbMetric: 56.8402

Epoch 84: val_loss improved from 57.31024 to 56.84022, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 56.9927 - MinusLogProbMetric: 56.9927 - val_loss: 56.8402 - val_MinusLogProbMetric: 56.8402 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 85/1000
2023-10-09 10:50:19.795 
Epoch 85/1000 
	 loss: 56.3573, MinusLogProbMetric: 56.3573, val_loss: 56.3807, val_MinusLogProbMetric: 56.3807

Epoch 85: val_loss improved from 56.84022 to 56.38068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 56.3573 - MinusLogProbMetric: 56.3573 - val_loss: 56.3807 - val_MinusLogProbMetric: 56.3807 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 86/1000
2023-10-09 10:51:11.693 
Epoch 86/1000 
	 loss: 55.8462, MinusLogProbMetric: 55.8462, val_loss: 55.2082, val_MinusLogProbMetric: 55.2082

Epoch 86: val_loss improved from 56.38068 to 55.20815, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 55.8462 - MinusLogProbMetric: 55.8462 - val_loss: 55.2082 - val_MinusLogProbMetric: 55.2082 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 87/1000
2023-10-09 10:52:05.084 
Epoch 87/1000 
	 loss: 55.1346, MinusLogProbMetric: 55.1346, val_loss: 55.0014, val_MinusLogProbMetric: 55.0014

Epoch 87: val_loss improved from 55.20815 to 55.00137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 55.1346 - MinusLogProbMetric: 55.1346 - val_loss: 55.0014 - val_MinusLogProbMetric: 55.0014 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 88/1000
2023-10-09 10:52:59.193 
Epoch 88/1000 
	 loss: 54.6349, MinusLogProbMetric: 54.6349, val_loss: 54.5742, val_MinusLogProbMetric: 54.5742

Epoch 88: val_loss improved from 55.00137 to 54.57420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 54.6349 - MinusLogProbMetric: 54.6349 - val_loss: 54.5742 - val_MinusLogProbMetric: 54.5742 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 89/1000
2023-10-09 10:53:51.278 
Epoch 89/1000 
	 loss: 54.3491, MinusLogProbMetric: 54.3491, val_loss: 53.7471, val_MinusLogProbMetric: 53.7471

Epoch 89: val_loss improved from 54.57420 to 53.74709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 54.3491 - MinusLogProbMetric: 54.3491 - val_loss: 53.7471 - val_MinusLogProbMetric: 53.7471 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 90/1000
2023-10-09 10:54:43.920 
Epoch 90/1000 
	 loss: 53.6427, MinusLogProbMetric: 53.6427, val_loss: 53.3087, val_MinusLogProbMetric: 53.3087

Epoch 90: val_loss improved from 53.74709 to 53.30869, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 53.6427 - MinusLogProbMetric: 53.6427 - val_loss: 53.3087 - val_MinusLogProbMetric: 53.3087 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 91/1000
2023-10-09 10:55:36.194 
Epoch 91/1000 
	 loss: 53.1301, MinusLogProbMetric: 53.1301, val_loss: 53.2347, val_MinusLogProbMetric: 53.2347

Epoch 91: val_loss improved from 53.30869 to 53.23470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 53.1301 - MinusLogProbMetric: 53.1301 - val_loss: 53.2347 - val_MinusLogProbMetric: 53.2347 - lr: 1.1111e-04 - 52s/epoch - 268ms/step
Epoch 92/1000
2023-10-09 10:56:29.411 
Epoch 92/1000 
	 loss: 52.7635, MinusLogProbMetric: 52.7635, val_loss: 52.2442, val_MinusLogProbMetric: 52.2442

Epoch 92: val_loss improved from 53.23470 to 52.24423, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 52.7635 - MinusLogProbMetric: 52.7635 - val_loss: 52.2442 - val_MinusLogProbMetric: 52.2442 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 93/1000
2023-10-09 10:57:21.184 
Epoch 93/1000 
	 loss: 52.2030, MinusLogProbMetric: 52.2030, val_loss: 51.8704, val_MinusLogProbMetric: 51.8704

Epoch 93: val_loss improved from 52.24423 to 51.87045, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 52.2030 - MinusLogProbMetric: 52.2030 - val_loss: 51.8704 - val_MinusLogProbMetric: 51.8704 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 94/1000
2023-10-09 10:58:13.383 
Epoch 94/1000 
	 loss: 51.6550, MinusLogProbMetric: 51.6550, val_loss: 51.4846, val_MinusLogProbMetric: 51.4846

Epoch 94: val_loss improved from 51.87045 to 51.48461, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 51.6550 - MinusLogProbMetric: 51.6550 - val_loss: 51.4846 - val_MinusLogProbMetric: 51.4846 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 95/1000
2023-10-09 10:59:07.249 
Epoch 95/1000 
	 loss: 51.8652, MinusLogProbMetric: 51.8652, val_loss: 52.6585, val_MinusLogProbMetric: 52.6585

Epoch 95: val_loss did not improve from 51.48461
196/196 - 53s - loss: 51.8652 - MinusLogProbMetric: 51.8652 - val_loss: 52.6585 - val_MinusLogProbMetric: 52.6585 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 96/1000
2023-10-09 11:00:01.497 
Epoch 96/1000 
	 loss: 51.0396, MinusLogProbMetric: 51.0396, val_loss: 50.9966, val_MinusLogProbMetric: 50.9966

Epoch 96: val_loss improved from 51.48461 to 50.99664, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 51.0396 - MinusLogProbMetric: 51.0396 - val_loss: 50.9966 - val_MinusLogProbMetric: 50.9966 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 97/1000
2023-10-09 11:00:53.733 
Epoch 97/1000 
	 loss: 50.4494, MinusLogProbMetric: 50.4494, val_loss: 50.2273, val_MinusLogProbMetric: 50.2273

Epoch 97: val_loss improved from 50.99664 to 50.22728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 50.4494 - MinusLogProbMetric: 50.4494 - val_loss: 50.2273 - val_MinusLogProbMetric: 50.2273 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 98/1000
2023-10-09 11:01:48.166 
Epoch 98/1000 
	 loss: 50.0879, MinusLogProbMetric: 50.0879, val_loss: 50.1037, val_MinusLogProbMetric: 50.1037

Epoch 98: val_loss improved from 50.22728 to 50.10365, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 50.0879 - MinusLogProbMetric: 50.0879 - val_loss: 50.1037 - val_MinusLogProbMetric: 50.1037 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 99/1000
2023-10-09 11:02:41.605 
Epoch 99/1000 
	 loss: 49.7025, MinusLogProbMetric: 49.7025, val_loss: 50.0950, val_MinusLogProbMetric: 50.0950

Epoch 99: val_loss improved from 50.10365 to 50.09496, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 49.7025 - MinusLogProbMetric: 49.7025 - val_loss: 50.0950 - val_MinusLogProbMetric: 50.0950 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 100/1000
2023-10-09 11:03:36.750 
Epoch 100/1000 
	 loss: 49.3583, MinusLogProbMetric: 49.3583, val_loss: 49.1753, val_MinusLogProbMetric: 49.1753

Epoch 100: val_loss improved from 50.09496 to 49.17532, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 49.3583 - MinusLogProbMetric: 49.3583 - val_loss: 49.1753 - val_MinusLogProbMetric: 49.1753 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 101/1000
2023-10-09 11:04:28.488 
Epoch 101/1000 
	 loss: 48.9068, MinusLogProbMetric: 48.9068, val_loss: 48.5189, val_MinusLogProbMetric: 48.5189

Epoch 101: val_loss improved from 49.17532 to 48.51891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 48.9068 - MinusLogProbMetric: 48.9068 - val_loss: 48.5189 - val_MinusLogProbMetric: 48.5189 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 102/1000
2023-10-09 11:05:21.884 
Epoch 102/1000 
	 loss: 48.5403, MinusLogProbMetric: 48.5403, val_loss: 48.1941, val_MinusLogProbMetric: 48.1941

Epoch 102: val_loss improved from 48.51891 to 48.19409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 48.5403 - MinusLogProbMetric: 48.5403 - val_loss: 48.1941 - val_MinusLogProbMetric: 48.1941 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 103/1000
2023-10-09 11:06:17.580 
Epoch 103/1000 
	 loss: 49.0176, MinusLogProbMetric: 49.0176, val_loss: 50.0988, val_MinusLogProbMetric: 50.0988

Epoch 103: val_loss did not improve from 48.19409
196/196 - 55s - loss: 49.0176 - MinusLogProbMetric: 49.0176 - val_loss: 50.0988 - val_MinusLogProbMetric: 50.0988 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 104/1000
2023-10-09 11:07:12.226 
Epoch 104/1000 
	 loss: 48.1783, MinusLogProbMetric: 48.1783, val_loss: 47.9870, val_MinusLogProbMetric: 47.9870

Epoch 104: val_loss improved from 48.19409 to 47.98697, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 48.1783 - MinusLogProbMetric: 48.1783 - val_loss: 47.9870 - val_MinusLogProbMetric: 47.9870 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 105/1000
2023-10-09 11:08:05.260 
Epoch 105/1000 
	 loss: 47.5197, MinusLogProbMetric: 47.5197, val_loss: 47.5117, val_MinusLogProbMetric: 47.5117

Epoch 105: val_loss improved from 47.98697 to 47.51174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 47.5197 - MinusLogProbMetric: 47.5197 - val_loss: 47.5117 - val_MinusLogProbMetric: 47.5117 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 106/1000
2023-10-09 11:08:57.799 
Epoch 106/1000 
	 loss: 47.2341, MinusLogProbMetric: 47.2341, val_loss: 47.2781, val_MinusLogProbMetric: 47.2781

Epoch 106: val_loss improved from 47.51174 to 47.27805, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 47.2341 - MinusLogProbMetric: 47.2341 - val_loss: 47.2781 - val_MinusLogProbMetric: 47.2781 - lr: 1.1111e-04 - 52s/epoch - 268ms/step
Epoch 107/1000
2023-10-09 11:09:51.144 
Epoch 107/1000 
	 loss: 46.7250, MinusLogProbMetric: 46.7250, val_loss: 46.5600, val_MinusLogProbMetric: 46.5600

Epoch 107: val_loss improved from 47.27805 to 46.56003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 46.7250 - MinusLogProbMetric: 46.7250 - val_loss: 46.5600 - val_MinusLogProbMetric: 46.5600 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 108/1000
2023-10-09 11:10:46.430 
Epoch 108/1000 
	 loss: 46.3992, MinusLogProbMetric: 46.3992, val_loss: 46.3434, val_MinusLogProbMetric: 46.3434

Epoch 108: val_loss improved from 46.56003 to 46.34340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 46.3992 - MinusLogProbMetric: 46.3992 - val_loss: 46.3434 - val_MinusLogProbMetric: 46.3434 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 109/1000
2023-10-09 11:11:38.726 
Epoch 109/1000 
	 loss: 46.1796, MinusLogProbMetric: 46.1796, val_loss: 46.0548, val_MinusLogProbMetric: 46.0548

Epoch 109: val_loss improved from 46.34340 to 46.05483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 46.1796 - MinusLogProbMetric: 46.1796 - val_loss: 46.0548 - val_MinusLogProbMetric: 46.0548 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 110/1000
2023-10-09 11:12:34.015 
Epoch 110/1000 
	 loss: 45.8497, MinusLogProbMetric: 45.8497, val_loss: 45.7565, val_MinusLogProbMetric: 45.7565

Epoch 110: val_loss improved from 46.05483 to 45.75648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 45.8497 - MinusLogProbMetric: 45.8497 - val_loss: 45.7565 - val_MinusLogProbMetric: 45.7565 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 111/1000
2023-10-09 11:13:28.541 
Epoch 111/1000 
	 loss: 45.5966, MinusLogProbMetric: 45.5966, val_loss: 45.2826, val_MinusLogProbMetric: 45.2826

Epoch 111: val_loss improved from 45.75648 to 45.28257, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 45.5966 - MinusLogProbMetric: 45.5966 - val_loss: 45.2826 - val_MinusLogProbMetric: 45.2826 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 112/1000
2023-10-09 11:14:22.819 
Epoch 112/1000 
	 loss: 45.4963, MinusLogProbMetric: 45.4963, val_loss: 45.0544, val_MinusLogProbMetric: 45.0544

Epoch 112: val_loss improved from 45.28257 to 45.05437, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 45.4963 - MinusLogProbMetric: 45.4963 - val_loss: 45.0544 - val_MinusLogProbMetric: 45.0544 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 113/1000
2023-10-09 11:15:17.361 
Epoch 113/1000 
	 loss: 44.9699, MinusLogProbMetric: 44.9699, val_loss: 44.9206, val_MinusLogProbMetric: 44.9206

Epoch 113: val_loss improved from 45.05437 to 44.92058, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 44.9699 - MinusLogProbMetric: 44.9699 - val_loss: 44.9206 - val_MinusLogProbMetric: 44.9206 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 114/1000
2023-10-09 11:16:12.571 
Epoch 114/1000 
	 loss: 44.7956, MinusLogProbMetric: 44.7956, val_loss: 45.2460, val_MinusLogProbMetric: 45.2460

Epoch 114: val_loss did not improve from 44.92058
196/196 - 54s - loss: 44.7956 - MinusLogProbMetric: 44.7956 - val_loss: 45.2460 - val_MinusLogProbMetric: 45.2460 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 115/1000
2023-10-09 11:17:03.946 
Epoch 115/1000 
	 loss: 44.5431, MinusLogProbMetric: 44.5431, val_loss: 45.3332, val_MinusLogProbMetric: 45.3332

Epoch 115: val_loss did not improve from 44.92058
196/196 - 51s - loss: 44.5431 - MinusLogProbMetric: 44.5431 - val_loss: 45.3332 - val_MinusLogProbMetric: 45.3332 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 116/1000
2023-10-09 11:17:53.926 
Epoch 116/1000 
	 loss: 44.3998, MinusLogProbMetric: 44.3998, val_loss: 44.4627, val_MinusLogProbMetric: 44.4627

Epoch 116: val_loss improved from 44.92058 to 44.46267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 44.3998 - MinusLogProbMetric: 44.3998 - val_loss: 44.4627 - val_MinusLogProbMetric: 44.4627 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 117/1000
2023-10-09 11:18:47.223 
Epoch 117/1000 
	 loss: 44.1454, MinusLogProbMetric: 44.1454, val_loss: 43.8620, val_MinusLogProbMetric: 43.8620

Epoch 117: val_loss improved from 44.46267 to 43.86201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 44.1454 - MinusLogProbMetric: 44.1454 - val_loss: 43.8620 - val_MinusLogProbMetric: 43.8620 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 118/1000
2023-10-09 11:19:39.446 
Epoch 118/1000 
	 loss: 44.0757, MinusLogProbMetric: 44.0757, val_loss: 43.3587, val_MinusLogProbMetric: 43.3587

Epoch 118: val_loss improved from 43.86201 to 43.35874, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 44.0757 - MinusLogProbMetric: 44.0757 - val_loss: 43.3587 - val_MinusLogProbMetric: 43.3587 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 119/1000
2023-10-09 11:20:32.248 
Epoch 119/1000 
	 loss: 43.5240, MinusLogProbMetric: 43.5240, val_loss: 43.5161, val_MinusLogProbMetric: 43.5161

Epoch 119: val_loss did not improve from 43.35874
196/196 - 52s - loss: 43.5240 - MinusLogProbMetric: 43.5240 - val_loss: 43.5161 - val_MinusLogProbMetric: 43.5161 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 120/1000
2023-10-09 11:21:25.729 
Epoch 120/1000 
	 loss: 43.4696, MinusLogProbMetric: 43.4696, val_loss: 43.5530, val_MinusLogProbMetric: 43.5530

Epoch 120: val_loss did not improve from 43.35874
196/196 - 53s - loss: 43.4696 - MinusLogProbMetric: 43.4696 - val_loss: 43.5530 - val_MinusLogProbMetric: 43.5530 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 121/1000
2023-10-09 11:22:17.846 
Epoch 121/1000 
	 loss: 43.1142, MinusLogProbMetric: 43.1142, val_loss: 43.0942, val_MinusLogProbMetric: 43.0942

Epoch 121: val_loss improved from 43.35874 to 43.09417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 43.1142 - MinusLogProbMetric: 43.1142 - val_loss: 43.0942 - val_MinusLogProbMetric: 43.0942 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 122/1000
2023-10-09 11:23:11.378 
Epoch 122/1000 
	 loss: 42.8714, MinusLogProbMetric: 42.8714, val_loss: 42.8755, val_MinusLogProbMetric: 42.8755

Epoch 122: val_loss improved from 43.09417 to 42.87553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 42.8714 - MinusLogProbMetric: 42.8714 - val_loss: 42.8755 - val_MinusLogProbMetric: 42.8755 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 123/1000
2023-10-09 11:24:04.205 
Epoch 123/1000 
	 loss: 42.6688, MinusLogProbMetric: 42.6688, val_loss: 42.3602, val_MinusLogProbMetric: 42.3602

Epoch 123: val_loss improved from 42.87553 to 42.36016, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 42.6688 - MinusLogProbMetric: 42.6688 - val_loss: 42.3602 - val_MinusLogProbMetric: 42.3602 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 124/1000
2023-10-09 11:24:58.927 
Epoch 124/1000 
	 loss: 42.4359, MinusLogProbMetric: 42.4359, val_loss: 42.3222, val_MinusLogProbMetric: 42.3222

Epoch 124: val_loss improved from 42.36016 to 42.32223, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 42.4359 - MinusLogProbMetric: 42.4359 - val_loss: 42.3222 - val_MinusLogProbMetric: 42.3222 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 125/1000
2023-10-09 11:25:53.463 
Epoch 125/1000 
	 loss: 42.2809, MinusLogProbMetric: 42.2809, val_loss: 42.3436, val_MinusLogProbMetric: 42.3436

Epoch 125: val_loss did not improve from 42.32223
196/196 - 54s - loss: 42.2809 - MinusLogProbMetric: 42.2809 - val_loss: 42.3436 - val_MinusLogProbMetric: 42.3436 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 126/1000
2023-10-09 11:26:46.448 
Epoch 126/1000 
	 loss: 41.9233, MinusLogProbMetric: 41.9233, val_loss: 42.4069, val_MinusLogProbMetric: 42.4069

Epoch 126: val_loss did not improve from 42.32223
196/196 - 53s - loss: 41.9233 - MinusLogProbMetric: 41.9233 - val_loss: 42.4069 - val_MinusLogProbMetric: 42.4069 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 127/1000
2023-10-09 11:27:41.743 
Epoch 127/1000 
	 loss: 41.8457, MinusLogProbMetric: 41.8457, val_loss: 41.7482, val_MinusLogProbMetric: 41.7482

Epoch 127: val_loss improved from 42.32223 to 41.74823, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 41.8457 - MinusLogProbMetric: 41.8457 - val_loss: 41.7482 - val_MinusLogProbMetric: 41.7482 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 128/1000
2023-10-09 11:28:34.229 
Epoch 128/1000 
	 loss: 41.5848, MinusLogProbMetric: 41.5848, val_loss: 41.4350, val_MinusLogProbMetric: 41.4350

Epoch 128: val_loss improved from 41.74823 to 41.43500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 41.5848 - MinusLogProbMetric: 41.5848 - val_loss: 41.4350 - val_MinusLogProbMetric: 41.4350 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 129/1000
2023-10-09 11:29:28.990 
Epoch 129/1000 
	 loss: 41.4170, MinusLogProbMetric: 41.4170, val_loss: 41.2315, val_MinusLogProbMetric: 41.2315

Epoch 129: val_loss improved from 41.43500 to 41.23149, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 41.4170 - MinusLogProbMetric: 41.4170 - val_loss: 41.2315 - val_MinusLogProbMetric: 41.2315 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 130/1000
2023-10-09 11:30:20.703 
Epoch 130/1000 
	 loss: 41.3440, MinusLogProbMetric: 41.3440, val_loss: 41.3306, val_MinusLogProbMetric: 41.3306

Epoch 130: val_loss did not improve from 41.23149
196/196 - 51s - loss: 41.3440 - MinusLogProbMetric: 41.3440 - val_loss: 41.3306 - val_MinusLogProbMetric: 41.3306 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 131/1000
2023-10-09 11:31:11.891 
Epoch 131/1000 
	 loss: 41.0775, MinusLogProbMetric: 41.0775, val_loss: 41.0849, val_MinusLogProbMetric: 41.0849

Epoch 131: val_loss improved from 41.23149 to 41.08495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 41.0775 - MinusLogProbMetric: 41.0775 - val_loss: 41.0849 - val_MinusLogProbMetric: 41.0849 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 132/1000
2023-10-09 11:32:04.401 
Epoch 132/1000 
	 loss: 40.8877, MinusLogProbMetric: 40.8877, val_loss: 40.7359, val_MinusLogProbMetric: 40.7359

Epoch 132: val_loss improved from 41.08495 to 40.73594, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 40.8877 - MinusLogProbMetric: 40.8877 - val_loss: 40.7359 - val_MinusLogProbMetric: 40.7359 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 133/1000
2023-10-09 11:32:58.993 
Epoch 133/1000 
	 loss: 40.7312, MinusLogProbMetric: 40.7312, val_loss: 40.5283, val_MinusLogProbMetric: 40.5283

Epoch 133: val_loss improved from 40.73594 to 40.52825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 40.7312 - MinusLogProbMetric: 40.7312 - val_loss: 40.5283 - val_MinusLogProbMetric: 40.5283 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 134/1000
2023-10-09 11:33:53.318 
Epoch 134/1000 
	 loss: 40.4612, MinusLogProbMetric: 40.4612, val_loss: 41.0319, val_MinusLogProbMetric: 41.0319

Epoch 134: val_loss did not improve from 40.52825
196/196 - 53s - loss: 40.4612 - MinusLogProbMetric: 40.4612 - val_loss: 41.0319 - val_MinusLogProbMetric: 41.0319 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 135/1000
2023-10-09 11:34:45.658 
Epoch 135/1000 
	 loss: 40.2967, MinusLogProbMetric: 40.2967, val_loss: 40.0875, val_MinusLogProbMetric: 40.0875

Epoch 135: val_loss improved from 40.52825 to 40.08752, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 40.2967 - MinusLogProbMetric: 40.2967 - val_loss: 40.0875 - val_MinusLogProbMetric: 40.0875 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 136/1000
2023-10-09 11:35:38.025 
Epoch 136/1000 
	 loss: 40.0801, MinusLogProbMetric: 40.0801, val_loss: 39.9704, val_MinusLogProbMetric: 39.9704

Epoch 136: val_loss improved from 40.08752 to 39.97037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 40.0801 - MinusLogProbMetric: 40.0801 - val_loss: 39.9704 - val_MinusLogProbMetric: 39.9704 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 137/1000
2023-10-09 11:36:32.183 
Epoch 137/1000 
	 loss: 39.9987, MinusLogProbMetric: 39.9987, val_loss: 40.2600, val_MinusLogProbMetric: 40.2600

Epoch 137: val_loss did not improve from 39.97037
196/196 - 53s - loss: 39.9987 - MinusLogProbMetric: 39.9987 - val_loss: 40.2600 - val_MinusLogProbMetric: 40.2600 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 138/1000
2023-10-09 11:37:23.687 
Epoch 138/1000 
	 loss: 40.0977, MinusLogProbMetric: 40.0977, val_loss: 40.5434, val_MinusLogProbMetric: 40.5434

Epoch 138: val_loss did not improve from 39.97037
196/196 - 52s - loss: 40.0977 - MinusLogProbMetric: 40.0977 - val_loss: 40.5434 - val_MinusLogProbMetric: 40.5434 - lr: 1.1111e-04 - 52s/epoch - 263ms/step
Epoch 139/1000
2023-10-09 11:38:17.038 
Epoch 139/1000 
	 loss: 39.7164, MinusLogProbMetric: 39.7164, val_loss: 40.2868, val_MinusLogProbMetric: 40.2868

Epoch 139: val_loss did not improve from 39.97037
196/196 - 53s - loss: 39.7164 - MinusLogProbMetric: 39.7164 - val_loss: 40.2868 - val_MinusLogProbMetric: 40.2868 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 140/1000
2023-10-09 11:39:11.289 
Epoch 140/1000 
	 loss: 39.4337, MinusLogProbMetric: 39.4337, val_loss: 39.6365, val_MinusLogProbMetric: 39.6365

Epoch 140: val_loss improved from 39.97037 to 39.63650, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 39.4337 - MinusLogProbMetric: 39.4337 - val_loss: 39.6365 - val_MinusLogProbMetric: 39.6365 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 141/1000
2023-10-09 11:40:04.427 
Epoch 141/1000 
	 loss: 39.2766, MinusLogProbMetric: 39.2766, val_loss: 39.5127, val_MinusLogProbMetric: 39.5127

Epoch 141: val_loss improved from 39.63650 to 39.51273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 39.2766 - MinusLogProbMetric: 39.2766 - val_loss: 39.5127 - val_MinusLogProbMetric: 39.5127 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 142/1000
2023-10-09 11:40:58.107 
Epoch 142/1000 
	 loss: 39.0796, MinusLogProbMetric: 39.0796, val_loss: 39.3860, val_MinusLogProbMetric: 39.3860

Epoch 142: val_loss improved from 39.51273 to 39.38596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 39.0796 - MinusLogProbMetric: 39.0796 - val_loss: 39.3860 - val_MinusLogProbMetric: 39.3860 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 143/1000
2023-10-09 11:41:53.000 
Epoch 143/1000 
	 loss: 38.8910, MinusLogProbMetric: 38.8910, val_loss: 39.7494, val_MinusLogProbMetric: 39.7494

Epoch 143: val_loss did not improve from 39.38596
196/196 - 54s - loss: 38.8910 - MinusLogProbMetric: 38.8910 - val_loss: 39.7494 - val_MinusLogProbMetric: 39.7494 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 144/1000
2023-10-09 11:42:44.924 
Epoch 144/1000 
	 loss: 38.9615, MinusLogProbMetric: 38.9615, val_loss: 39.1420, val_MinusLogProbMetric: 39.1420

Epoch 144: val_loss improved from 39.38596 to 39.14204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 38.9615 - MinusLogProbMetric: 38.9615 - val_loss: 39.1420 - val_MinusLogProbMetric: 39.1420 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 145/1000
2023-10-09 11:43:39.401 
Epoch 145/1000 
	 loss: 38.6212, MinusLogProbMetric: 38.6212, val_loss: 38.4418, val_MinusLogProbMetric: 38.4418

Epoch 145: val_loss improved from 39.14204 to 38.44181, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 38.6212 - MinusLogProbMetric: 38.6212 - val_loss: 38.4418 - val_MinusLogProbMetric: 38.4418 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 146/1000
2023-10-09 11:44:32.483 
Epoch 146/1000 
	 loss: 38.4923, MinusLogProbMetric: 38.4923, val_loss: 38.5141, val_MinusLogProbMetric: 38.5141

Epoch 146: val_loss did not improve from 38.44181
196/196 - 52s - loss: 38.4923 - MinusLogProbMetric: 38.4923 - val_loss: 38.5141 - val_MinusLogProbMetric: 38.5141 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 147/1000
2023-10-09 11:45:25.705 
Epoch 147/1000 
	 loss: 38.3452, MinusLogProbMetric: 38.3452, val_loss: 38.3059, val_MinusLogProbMetric: 38.3059

Epoch 147: val_loss improved from 38.44181 to 38.30594, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 38.3452 - MinusLogProbMetric: 38.3452 - val_loss: 38.3059 - val_MinusLogProbMetric: 38.3059 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 148/1000
2023-10-09 11:46:19.097 
Epoch 148/1000 
	 loss: 38.3261, MinusLogProbMetric: 38.3261, val_loss: 38.0661, val_MinusLogProbMetric: 38.0661

Epoch 148: val_loss improved from 38.30594 to 38.06610, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 38.3261 - MinusLogProbMetric: 38.3261 - val_loss: 38.0661 - val_MinusLogProbMetric: 38.0661 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 149/1000
2023-10-09 11:47:15.359 
Epoch 149/1000 
	 loss: 38.0414, MinusLogProbMetric: 38.0414, val_loss: 37.9769, val_MinusLogProbMetric: 37.9769

Epoch 149: val_loss improved from 38.06610 to 37.97694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 38.0414 - MinusLogProbMetric: 38.0414 - val_loss: 37.9769 - val_MinusLogProbMetric: 37.9769 - lr: 1.1111e-04 - 56s/epoch - 287ms/step
Epoch 150/1000
2023-10-09 11:48:10.670 
Epoch 150/1000 
	 loss: 37.8667, MinusLogProbMetric: 37.8667, val_loss: 38.1005, val_MinusLogProbMetric: 38.1005

Epoch 150: val_loss did not improve from 37.97694
196/196 - 55s - loss: 37.8667 - MinusLogProbMetric: 37.8667 - val_loss: 38.1005 - val_MinusLogProbMetric: 38.1005 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 151/1000
2023-10-09 11:49:02.488 
Epoch 151/1000 
	 loss: 37.7605, MinusLogProbMetric: 37.7605, val_loss: 37.4430, val_MinusLogProbMetric: 37.4430

Epoch 151: val_loss improved from 37.97694 to 37.44295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 37.7605 - MinusLogProbMetric: 37.7605 - val_loss: 37.4430 - val_MinusLogProbMetric: 37.4430 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 152/1000
2023-10-09 11:49:56.909 
Epoch 152/1000 
	 loss: 37.7165, MinusLogProbMetric: 37.7165, val_loss: 37.6115, val_MinusLogProbMetric: 37.6115

Epoch 152: val_loss did not improve from 37.44295
196/196 - 54s - loss: 37.7165 - MinusLogProbMetric: 37.7165 - val_loss: 37.6115 - val_MinusLogProbMetric: 37.6115 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 153/1000
2023-10-09 11:50:51.550 
Epoch 153/1000 
	 loss: 37.4566, MinusLogProbMetric: 37.4566, val_loss: 37.3204, val_MinusLogProbMetric: 37.3204

Epoch 153: val_loss improved from 37.44295 to 37.32041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 37.4566 - MinusLogProbMetric: 37.4566 - val_loss: 37.3204 - val_MinusLogProbMetric: 37.3204 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 154/1000
2023-10-09 11:51:44.111 
Epoch 154/1000 
	 loss: 37.3241, MinusLogProbMetric: 37.3241, val_loss: 37.2447, val_MinusLogProbMetric: 37.2447

Epoch 154: val_loss improved from 37.32041 to 37.24466, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 37.3241 - MinusLogProbMetric: 37.3241 - val_loss: 37.2447 - val_MinusLogProbMetric: 37.2447 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 155/1000
2023-10-09 11:52:39.782 
Epoch 155/1000 
	 loss: 37.1022, MinusLogProbMetric: 37.1022, val_loss: 37.3528, val_MinusLogProbMetric: 37.3528

Epoch 155: val_loss did not improve from 37.24466
196/196 - 55s - loss: 37.1022 - MinusLogProbMetric: 37.1022 - val_loss: 37.3528 - val_MinusLogProbMetric: 37.3528 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 156/1000
2023-10-09 11:53:30.476 
Epoch 156/1000 
	 loss: 36.9601, MinusLogProbMetric: 36.9601, val_loss: 36.7798, val_MinusLogProbMetric: 36.7798

Epoch 156: val_loss improved from 37.24466 to 36.77979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 36.9601 - MinusLogProbMetric: 36.9601 - val_loss: 36.7798 - val_MinusLogProbMetric: 36.7798 - lr: 1.1111e-04 - 52s/epoch - 263ms/step
Epoch 157/1000
2023-10-09 11:54:22.942 
Epoch 157/1000 
	 loss: 36.8941, MinusLogProbMetric: 36.8941, val_loss: 36.8005, val_MinusLogProbMetric: 36.8005

Epoch 157: val_loss did not improve from 36.77979
196/196 - 52s - loss: 36.8941 - MinusLogProbMetric: 36.8941 - val_loss: 36.8005 - val_MinusLogProbMetric: 36.8005 - lr: 1.1111e-04 - 52s/epoch - 263ms/step
Epoch 158/1000
2023-10-09 11:55:16.067 
Epoch 158/1000 
	 loss: 36.8133, MinusLogProbMetric: 36.8133, val_loss: 37.0329, val_MinusLogProbMetric: 37.0329

Epoch 158: val_loss did not improve from 36.77979
196/196 - 53s - loss: 36.8133 - MinusLogProbMetric: 36.8133 - val_loss: 37.0329 - val_MinusLogProbMetric: 37.0329 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 159/1000
2023-10-09 11:56:08.623 
Epoch 159/1000 
	 loss: 36.5708, MinusLogProbMetric: 36.5708, val_loss: 37.1061, val_MinusLogProbMetric: 37.1061

Epoch 159: val_loss did not improve from 36.77979
196/196 - 53s - loss: 36.5708 - MinusLogProbMetric: 36.5708 - val_loss: 37.1061 - val_MinusLogProbMetric: 37.1061 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 160/1000
2023-10-09 11:56:59.734 
Epoch 160/1000 
	 loss: 36.4883, MinusLogProbMetric: 36.4883, val_loss: 36.7470, val_MinusLogProbMetric: 36.7470

Epoch 160: val_loss improved from 36.77979 to 36.74702, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 36.4883 - MinusLogProbMetric: 36.4883 - val_loss: 36.7470 - val_MinusLogProbMetric: 36.7470 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 161/1000
2023-10-09 11:57:51.005 
Epoch 161/1000 
	 loss: 36.3553, MinusLogProbMetric: 36.3553, val_loss: 36.1755, val_MinusLogProbMetric: 36.1755

Epoch 161: val_loss improved from 36.74702 to 36.17547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 36.3553 - MinusLogProbMetric: 36.3553 - val_loss: 36.1755 - val_MinusLogProbMetric: 36.1755 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 162/1000
2023-10-09 11:58:45.360 
Epoch 162/1000 
	 loss: 36.2580, MinusLogProbMetric: 36.2580, val_loss: 36.3323, val_MinusLogProbMetric: 36.3323

Epoch 162: val_loss did not improve from 36.17547
196/196 - 54s - loss: 36.2580 - MinusLogProbMetric: 36.2580 - val_loss: 36.3323 - val_MinusLogProbMetric: 36.3323 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 163/1000
2023-10-09 11:59:35.617 
Epoch 163/1000 
	 loss: 36.0080, MinusLogProbMetric: 36.0080, val_loss: 35.8783, val_MinusLogProbMetric: 35.8783

Epoch 163: val_loss improved from 36.17547 to 35.87833, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 36.0080 - MinusLogProbMetric: 36.0080 - val_loss: 35.8783 - val_MinusLogProbMetric: 35.8783 - lr: 1.1111e-04 - 56s/epoch - 283ms/step
Epoch 164/1000
2023-10-09 12:00:32.343 
Epoch 164/1000 
	 loss: 35.8764, MinusLogProbMetric: 35.8764, val_loss: 35.7497, val_MinusLogProbMetric: 35.7497

Epoch 164: val_loss improved from 35.87833 to 35.74968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 35.8764 - MinusLogProbMetric: 35.8764 - val_loss: 35.7497 - val_MinusLogProbMetric: 35.7497 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 165/1000
2023-10-09 12:01:25.645 
Epoch 165/1000 
	 loss: 35.8136, MinusLogProbMetric: 35.8136, val_loss: 35.6994, val_MinusLogProbMetric: 35.6994

Epoch 165: val_loss improved from 35.74968 to 35.69939, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 35.8136 - MinusLogProbMetric: 35.8136 - val_loss: 35.6994 - val_MinusLogProbMetric: 35.6994 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 166/1000
2023-10-09 12:02:18.355 
Epoch 166/1000 
	 loss: 35.7641, MinusLogProbMetric: 35.7641, val_loss: 35.5776, val_MinusLogProbMetric: 35.5776

Epoch 166: val_loss improved from 35.69939 to 35.57764, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 35.7641 - MinusLogProbMetric: 35.7641 - val_loss: 35.5776 - val_MinusLogProbMetric: 35.5776 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 167/1000
2023-10-09 12:03:12.887 
Epoch 167/1000 
	 loss: 35.4789, MinusLogProbMetric: 35.4789, val_loss: 35.7244, val_MinusLogProbMetric: 35.7244

Epoch 167: val_loss did not improve from 35.57764
196/196 - 54s - loss: 35.4789 - MinusLogProbMetric: 35.4789 - val_loss: 35.7244 - val_MinusLogProbMetric: 35.7244 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 168/1000
2023-10-09 12:04:06.158 
Epoch 168/1000 
	 loss: 35.5030, MinusLogProbMetric: 35.5030, val_loss: 35.4664, val_MinusLogProbMetric: 35.4664

Epoch 168: val_loss improved from 35.57764 to 35.46642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 35.5030 - MinusLogProbMetric: 35.5030 - val_loss: 35.4664 - val_MinusLogProbMetric: 35.4664 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 169/1000
2023-10-09 12:05:00.014 
Epoch 169/1000 
	 loss: 36.6878, MinusLogProbMetric: 36.6878, val_loss: 39.1747, val_MinusLogProbMetric: 39.1747

Epoch 169: val_loss did not improve from 35.46642
196/196 - 52s - loss: 36.6878 - MinusLogProbMetric: 36.6878 - val_loss: 39.1747 - val_MinusLogProbMetric: 39.1747 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 170/1000
2023-10-09 12:05:51.419 
Epoch 170/1000 
	 loss: 35.4775, MinusLogProbMetric: 35.4775, val_loss: 35.1935, val_MinusLogProbMetric: 35.1935

Epoch 170: val_loss improved from 35.46642 to 35.19349, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 60s - loss: 35.4775 - MinusLogProbMetric: 35.4775 - val_loss: 35.1935 - val_MinusLogProbMetric: 35.1935 - lr: 1.1111e-04 - 60s/epoch - 308ms/step
Epoch 171/1000
2023-10-09 12:06:51.531 
Epoch 171/1000 
	 loss: 35.0967, MinusLogProbMetric: 35.0967, val_loss: 35.2100, val_MinusLogProbMetric: 35.2100

Epoch 171: val_loss did not improve from 35.19349
196/196 - 51s - loss: 35.0967 - MinusLogProbMetric: 35.0967 - val_loss: 35.2100 - val_MinusLogProbMetric: 35.2100 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 172/1000
2023-10-09 12:07:42.120 
Epoch 172/1000 
	 loss: 34.9641, MinusLogProbMetric: 34.9641, val_loss: 34.9021, val_MinusLogProbMetric: 34.9021

Epoch 172: val_loss improved from 35.19349 to 34.90207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 34.9641 - MinusLogProbMetric: 34.9641 - val_loss: 34.9021 - val_MinusLogProbMetric: 34.9021 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 173/1000
2023-10-09 12:08:35.520 
Epoch 173/1000 
	 loss: 87.9963, MinusLogProbMetric: 87.9963, val_loss: 79.9906, val_MinusLogProbMetric: 79.9906

Epoch 173: val_loss did not improve from 34.90207
196/196 - 53s - loss: 87.9963 - MinusLogProbMetric: 87.9963 - val_loss: 79.9906 - val_MinusLogProbMetric: 79.9906 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 174/1000
2023-10-09 12:09:28.377 
Epoch 174/1000 
	 loss: 65.5215, MinusLogProbMetric: 65.5215, val_loss: 57.3260, val_MinusLogProbMetric: 57.3260

Epoch 174: val_loss did not improve from 34.90207
196/196 - 53s - loss: 65.5215 - MinusLogProbMetric: 65.5215 - val_loss: 57.3260 - val_MinusLogProbMetric: 57.3260 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 175/1000
2023-10-09 12:10:21.003 
Epoch 175/1000 
	 loss: 53.1758, MinusLogProbMetric: 53.1758, val_loss: 49.9461, val_MinusLogProbMetric: 49.9461

Epoch 175: val_loss did not improve from 34.90207
196/196 - 53s - loss: 53.1758 - MinusLogProbMetric: 53.1758 - val_loss: 49.9461 - val_MinusLogProbMetric: 49.9461 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 176/1000
2023-10-09 12:11:12.527 
Epoch 176/1000 
	 loss: 47.9516, MinusLogProbMetric: 47.9516, val_loss: 46.2776, val_MinusLogProbMetric: 46.2776

Epoch 176: val_loss did not improve from 34.90207
196/196 - 52s - loss: 47.9516 - MinusLogProbMetric: 47.9516 - val_loss: 46.2776 - val_MinusLogProbMetric: 46.2776 - lr: 1.1111e-04 - 52s/epoch - 263ms/step
Epoch 177/1000
2023-10-09 12:12:02.734 
Epoch 177/1000 
	 loss: 44.8751, MinusLogProbMetric: 44.8751, val_loss: 43.8488, val_MinusLogProbMetric: 43.8488

Epoch 177: val_loss did not improve from 34.90207
196/196 - 50s - loss: 44.8751 - MinusLogProbMetric: 44.8751 - val_loss: 43.8488 - val_MinusLogProbMetric: 43.8488 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 178/1000
2023-10-09 12:12:55.613 
Epoch 178/1000 
	 loss: 42.8074, MinusLogProbMetric: 42.8074, val_loss: 41.9921, val_MinusLogProbMetric: 41.9921

Epoch 178: val_loss did not improve from 34.90207
196/196 - 53s - loss: 42.8074 - MinusLogProbMetric: 42.8074 - val_loss: 41.9921 - val_MinusLogProbMetric: 41.9921 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 179/1000
2023-10-09 12:13:48.573 
Epoch 179/1000 
	 loss: 41.1674, MinusLogProbMetric: 41.1674, val_loss: 40.5671, val_MinusLogProbMetric: 40.5671

Epoch 179: val_loss did not improve from 34.90207
196/196 - 53s - loss: 41.1674 - MinusLogProbMetric: 41.1674 - val_loss: 40.5671 - val_MinusLogProbMetric: 40.5671 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 180/1000
2023-10-09 12:14:42.224 
Epoch 180/1000 
	 loss: 40.0012, MinusLogProbMetric: 40.0012, val_loss: 39.8383, val_MinusLogProbMetric: 39.8383

Epoch 180: val_loss did not improve from 34.90207
196/196 - 54s - loss: 40.0012 - MinusLogProbMetric: 40.0012 - val_loss: 39.8383 - val_MinusLogProbMetric: 39.8383 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 181/1000
2023-10-09 12:15:37.406 
Epoch 181/1000 
	 loss: 39.1566, MinusLogProbMetric: 39.1566, val_loss: 38.9392, val_MinusLogProbMetric: 38.9392

Epoch 181: val_loss did not improve from 34.90207
196/196 - 55s - loss: 39.1566 - MinusLogProbMetric: 39.1566 - val_loss: 38.9392 - val_MinusLogProbMetric: 38.9392 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 182/1000
2023-10-09 12:16:31.767 
Epoch 182/1000 
	 loss: 38.3703, MinusLogProbMetric: 38.3703, val_loss: 38.1827, val_MinusLogProbMetric: 38.1827

Epoch 182: val_loss did not improve from 34.90207
196/196 - 54s - loss: 38.3703 - MinusLogProbMetric: 38.3703 - val_loss: 38.1827 - val_MinusLogProbMetric: 38.1827 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 183/1000
2023-10-09 12:17:25.268 
Epoch 183/1000 
	 loss: 37.7648, MinusLogProbMetric: 37.7648, val_loss: 37.5565, val_MinusLogProbMetric: 37.5565

Epoch 183: val_loss did not improve from 34.90207
196/196 - 53s - loss: 37.7648 - MinusLogProbMetric: 37.7648 - val_loss: 37.5565 - val_MinusLogProbMetric: 37.5565 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 184/1000
2023-10-09 12:18:20.353 
Epoch 184/1000 
	 loss: 37.3104, MinusLogProbMetric: 37.3104, val_loss: 37.1725, val_MinusLogProbMetric: 37.1725

Epoch 184: val_loss did not improve from 34.90207
196/196 - 55s - loss: 37.3104 - MinusLogProbMetric: 37.3104 - val_loss: 37.1725 - val_MinusLogProbMetric: 37.1725 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 185/1000
2023-10-09 12:19:11.992 
Epoch 185/1000 
	 loss: 36.8921, MinusLogProbMetric: 36.8921, val_loss: 36.7417, val_MinusLogProbMetric: 36.7417

Epoch 185: val_loss did not improve from 34.90207
196/196 - 52s - loss: 36.8921 - MinusLogProbMetric: 36.8921 - val_loss: 36.7417 - val_MinusLogProbMetric: 36.7417 - lr: 1.1111e-04 - 52s/epoch - 263ms/step
Epoch 186/1000
2023-10-09 12:20:04.665 
Epoch 186/1000 
	 loss: 36.6195, MinusLogProbMetric: 36.6195, val_loss: 36.9740, val_MinusLogProbMetric: 36.9740

Epoch 186: val_loss did not improve from 34.90207
196/196 - 53s - loss: 36.6195 - MinusLogProbMetric: 36.6195 - val_loss: 36.9740 - val_MinusLogProbMetric: 36.9740 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 187/1000
2023-10-09 12:20:57.661 
Epoch 187/1000 
	 loss: 36.7584, MinusLogProbMetric: 36.7584, val_loss: 36.3188, val_MinusLogProbMetric: 36.3188

Epoch 187: val_loss did not improve from 34.90207
196/196 - 53s - loss: 36.7584 - MinusLogProbMetric: 36.7584 - val_loss: 36.3188 - val_MinusLogProbMetric: 36.3188 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 188/1000
2023-10-09 12:21:52.990 
Epoch 188/1000 
	 loss: 36.1290, MinusLogProbMetric: 36.1290, val_loss: 36.0564, val_MinusLogProbMetric: 36.0564

Epoch 188: val_loss did not improve from 34.90207
196/196 - 55s - loss: 36.1290 - MinusLogProbMetric: 36.1290 - val_loss: 36.0564 - val_MinusLogProbMetric: 36.0564 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 189/1000
2023-10-09 12:22:46.737 
Epoch 189/1000 
	 loss: 35.7904, MinusLogProbMetric: 35.7904, val_loss: 35.6735, val_MinusLogProbMetric: 35.6735

Epoch 189: val_loss did not improve from 34.90207
196/196 - 54s - loss: 35.7904 - MinusLogProbMetric: 35.7904 - val_loss: 35.6735 - val_MinusLogProbMetric: 35.6735 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 190/1000
2023-10-09 12:23:40.516 
Epoch 190/1000 
	 loss: 35.5055, MinusLogProbMetric: 35.5055, val_loss: 35.4819, val_MinusLogProbMetric: 35.4819

Epoch 190: val_loss did not improve from 34.90207
196/196 - 54s - loss: 35.5055 - MinusLogProbMetric: 35.5055 - val_loss: 35.4819 - val_MinusLogProbMetric: 35.4819 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 191/1000
2023-10-09 12:24:32.735 
Epoch 191/1000 
	 loss: 35.4001, MinusLogProbMetric: 35.4001, val_loss: 35.2853, val_MinusLogProbMetric: 35.2853

Epoch 191: val_loss did not improve from 34.90207
196/196 - 52s - loss: 35.4001 - MinusLogProbMetric: 35.4001 - val_loss: 35.2853 - val_MinusLogProbMetric: 35.2853 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 192/1000
2023-10-09 12:25:27.645 
Epoch 192/1000 
	 loss: 35.1518, MinusLogProbMetric: 35.1518, val_loss: 35.1098, val_MinusLogProbMetric: 35.1098

Epoch 192: val_loss did not improve from 34.90207
196/196 - 55s - loss: 35.1518 - MinusLogProbMetric: 35.1518 - val_loss: 35.1098 - val_MinusLogProbMetric: 35.1098 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 193/1000
2023-10-09 12:26:22.443 
Epoch 193/1000 
	 loss: 34.9741, MinusLogProbMetric: 34.9741, val_loss: 35.1137, val_MinusLogProbMetric: 35.1137

Epoch 193: val_loss did not improve from 34.90207
196/196 - 55s - loss: 34.9741 - MinusLogProbMetric: 34.9741 - val_loss: 35.1137 - val_MinusLogProbMetric: 35.1137 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 194/1000
2023-10-09 12:27:14.333 
Epoch 194/1000 
	 loss: 34.8161, MinusLogProbMetric: 34.8161, val_loss: 34.7690, val_MinusLogProbMetric: 34.7690

Epoch 194: val_loss improved from 34.90207 to 34.76903, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 34.8161 - MinusLogProbMetric: 34.8161 - val_loss: 34.7690 - val_MinusLogProbMetric: 34.7690 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 195/1000
2023-10-09 12:28:09.100 
Epoch 195/1000 
	 loss: 34.6388, MinusLogProbMetric: 34.6388, val_loss: 34.9713, val_MinusLogProbMetric: 34.9713

Epoch 195: val_loss did not improve from 34.76903
196/196 - 54s - loss: 34.6388 - MinusLogProbMetric: 34.6388 - val_loss: 34.9713 - val_MinusLogProbMetric: 34.9713 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 196/1000
2023-10-09 12:29:01.863 
Epoch 196/1000 
	 loss: 34.5408, MinusLogProbMetric: 34.5408, val_loss: 34.5626, val_MinusLogProbMetric: 34.5626

Epoch 196: val_loss improved from 34.76903 to 34.56260, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 34.5408 - MinusLogProbMetric: 34.5408 - val_loss: 34.5626 - val_MinusLogProbMetric: 34.5626 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 197/1000
2023-10-09 12:29:55.989 
Epoch 197/1000 
	 loss: 34.3789, MinusLogProbMetric: 34.3789, val_loss: 34.4873, val_MinusLogProbMetric: 34.4873

Epoch 197: val_loss improved from 34.56260 to 34.48727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 34.3789 - MinusLogProbMetric: 34.3789 - val_loss: 34.4873 - val_MinusLogProbMetric: 34.4873 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 198/1000
2023-10-09 12:30:49.537 
Epoch 198/1000 
	 loss: 34.2455, MinusLogProbMetric: 34.2455, val_loss: 34.3769, val_MinusLogProbMetric: 34.3769

Epoch 198: val_loss improved from 34.48727 to 34.37692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 34.2455 - MinusLogProbMetric: 34.2455 - val_loss: 34.3769 - val_MinusLogProbMetric: 34.3769 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 199/1000
2023-10-09 12:31:42.870 
Epoch 199/1000 
	 loss: 34.1866, MinusLogProbMetric: 34.1866, val_loss: 34.1847, val_MinusLogProbMetric: 34.1847

Epoch 199: val_loss improved from 34.37692 to 34.18467, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 34.1866 - MinusLogProbMetric: 34.1866 - val_loss: 34.1847 - val_MinusLogProbMetric: 34.1847 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 200/1000
2023-10-09 12:32:35.457 
Epoch 200/1000 
	 loss: 33.9975, MinusLogProbMetric: 33.9975, val_loss: 33.9647, val_MinusLogProbMetric: 33.9647

Epoch 200: val_loss improved from 34.18467 to 33.96470, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 33.9975 - MinusLogProbMetric: 33.9975 - val_loss: 33.9647 - val_MinusLogProbMetric: 33.9647 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 201/1000
2023-10-09 12:33:28.248 
Epoch 201/1000 
	 loss: 45.2781, MinusLogProbMetric: 45.2781, val_loss: 38.9494, val_MinusLogProbMetric: 38.9494

Epoch 201: val_loss did not improve from 33.96470
196/196 - 52s - loss: 45.2781 - MinusLogProbMetric: 45.2781 - val_loss: 38.9494 - val_MinusLogProbMetric: 38.9494 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 202/1000
2023-10-09 12:34:22.321 
Epoch 202/1000 
	 loss: 37.6454, MinusLogProbMetric: 37.6454, val_loss: 36.9222, val_MinusLogProbMetric: 36.9222

Epoch 202: val_loss did not improve from 33.96470
196/196 - 54s - loss: 37.6454 - MinusLogProbMetric: 37.6454 - val_loss: 36.9222 - val_MinusLogProbMetric: 36.9222 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 203/1000
2023-10-09 12:35:12.947 
Epoch 203/1000 
	 loss: 36.3063, MinusLogProbMetric: 36.3063, val_loss: 35.8946, val_MinusLogProbMetric: 35.8946

Epoch 203: val_loss did not improve from 33.96470
196/196 - 51s - loss: 36.3063 - MinusLogProbMetric: 36.3063 - val_loss: 35.8946 - val_MinusLogProbMetric: 35.8946 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 204/1000
2023-10-09 12:36:03.590 
Epoch 204/1000 
	 loss: 35.4432, MinusLogProbMetric: 35.4432, val_loss: 35.5992, val_MinusLogProbMetric: 35.5992

Epoch 204: val_loss did not improve from 33.96470
196/196 - 51s - loss: 35.4432 - MinusLogProbMetric: 35.4432 - val_loss: 35.5992 - val_MinusLogProbMetric: 35.5992 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 205/1000
2023-10-09 12:36:58.576 
Epoch 205/1000 
	 loss: 34.7830, MinusLogProbMetric: 34.7830, val_loss: 34.8488, val_MinusLogProbMetric: 34.8488

Epoch 205: val_loss did not improve from 33.96470
196/196 - 55s - loss: 34.7830 - MinusLogProbMetric: 34.7830 - val_loss: 34.8488 - val_MinusLogProbMetric: 34.8488 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 206/1000
2023-10-09 12:37:51.209 
Epoch 206/1000 
	 loss: 34.3207, MinusLogProbMetric: 34.3207, val_loss: 34.3443, val_MinusLogProbMetric: 34.3443

Epoch 206: val_loss did not improve from 33.96470
196/196 - 53s - loss: 34.3207 - MinusLogProbMetric: 34.3207 - val_loss: 34.3443 - val_MinusLogProbMetric: 34.3443 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 207/1000
2023-10-09 12:38:45.700 
Epoch 207/1000 
	 loss: 33.9901, MinusLogProbMetric: 33.9901, val_loss: 34.0831, val_MinusLogProbMetric: 34.0831

Epoch 207: val_loss did not improve from 33.96470
196/196 - 54s - loss: 33.9901 - MinusLogProbMetric: 33.9901 - val_loss: 34.0831 - val_MinusLogProbMetric: 34.0831 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 208/1000
2023-10-09 12:39:41.733 
Epoch 208/1000 
	 loss: 33.8061, MinusLogProbMetric: 33.8061, val_loss: 34.0473, val_MinusLogProbMetric: 34.0473

Epoch 208: val_loss did not improve from 33.96470
196/196 - 56s - loss: 33.8061 - MinusLogProbMetric: 33.8061 - val_loss: 34.0473 - val_MinusLogProbMetric: 34.0473 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 209/1000
2023-10-09 12:40:34.721 
Epoch 209/1000 
	 loss: 33.6490, MinusLogProbMetric: 33.6490, val_loss: 33.5960, val_MinusLogProbMetric: 33.5960

Epoch 209: val_loss improved from 33.96470 to 33.59597, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 33.6490 - MinusLogProbMetric: 33.6490 - val_loss: 33.5960 - val_MinusLogProbMetric: 33.5960 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 210/1000
2023-10-09 12:41:28.123 
Epoch 210/1000 
	 loss: 33.3945, MinusLogProbMetric: 33.3945, val_loss: 33.3729, val_MinusLogProbMetric: 33.3729

Epoch 210: val_loss improved from 33.59597 to 33.37288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 33.3945 - MinusLogProbMetric: 33.3945 - val_loss: 33.3729 - val_MinusLogProbMetric: 33.3729 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 211/1000
2023-10-09 12:42:22.583 
Epoch 211/1000 
	 loss: 33.2904, MinusLogProbMetric: 33.2904, val_loss: 33.4879, val_MinusLogProbMetric: 33.4879

Epoch 211: val_loss did not improve from 33.37288
196/196 - 54s - loss: 33.2904 - MinusLogProbMetric: 33.2904 - val_loss: 33.4879 - val_MinusLogProbMetric: 33.4879 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 212/1000
2023-10-09 12:43:15.051 
Epoch 212/1000 
	 loss: 33.1582, MinusLogProbMetric: 33.1582, val_loss: 33.2844, val_MinusLogProbMetric: 33.2844

Epoch 212: val_loss improved from 33.37288 to 33.28436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 33.1582 - MinusLogProbMetric: 33.1582 - val_loss: 33.2844 - val_MinusLogProbMetric: 33.2844 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 213/1000
2023-10-09 12:44:07.996 
Epoch 213/1000 
	 loss: 32.9854, MinusLogProbMetric: 32.9854, val_loss: 32.9749, val_MinusLogProbMetric: 32.9749

Epoch 213: val_loss improved from 33.28436 to 32.97486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 32.9854 - MinusLogProbMetric: 32.9854 - val_loss: 32.9749 - val_MinusLogProbMetric: 32.9749 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 214/1000
2023-10-09 12:45:01.906 
Epoch 214/1000 
	 loss: 32.9333, MinusLogProbMetric: 32.9333, val_loss: 32.9089, val_MinusLogProbMetric: 32.9089

Epoch 214: val_loss improved from 32.97486 to 32.90888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 32.9333 - MinusLogProbMetric: 32.9333 - val_loss: 32.9089 - val_MinusLogProbMetric: 32.9089 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 215/1000
2023-10-09 12:45:56.686 
Epoch 215/1000 
	 loss: 32.7237, MinusLogProbMetric: 32.7237, val_loss: 32.7470, val_MinusLogProbMetric: 32.7470

Epoch 215: val_loss improved from 32.90888 to 32.74704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 32.7237 - MinusLogProbMetric: 32.7237 - val_loss: 32.7470 - val_MinusLogProbMetric: 32.7470 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 216/1000
2023-10-09 12:46:51.609 
Epoch 216/1000 
	 loss: 32.6477, MinusLogProbMetric: 32.6477, val_loss: 32.7298, val_MinusLogProbMetric: 32.7298

Epoch 216: val_loss improved from 32.74704 to 32.72983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 32.6477 - MinusLogProbMetric: 32.6477 - val_loss: 32.7298 - val_MinusLogProbMetric: 32.7298 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 217/1000
2023-10-09 12:47:44.561 
Epoch 217/1000 
	 loss: 32.5487, MinusLogProbMetric: 32.5487, val_loss: 32.7054, val_MinusLogProbMetric: 32.7054

Epoch 217: val_loss improved from 32.72983 to 32.70538, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 32.5487 - MinusLogProbMetric: 32.5487 - val_loss: 32.7054 - val_MinusLogProbMetric: 32.7054 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 218/1000
2023-10-09 12:48:38.964 
Epoch 218/1000 
	 loss: 32.4729, MinusLogProbMetric: 32.4729, val_loss: 32.5882, val_MinusLogProbMetric: 32.5882

Epoch 218: val_loss improved from 32.70538 to 32.58820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 32.4729 - MinusLogProbMetric: 32.4729 - val_loss: 32.5882 - val_MinusLogProbMetric: 32.5882 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 219/1000
2023-10-09 12:49:31.768 
Epoch 219/1000 
	 loss: 32.3756, MinusLogProbMetric: 32.3756, val_loss: 33.2450, val_MinusLogProbMetric: 33.2450

Epoch 219: val_loss did not improve from 32.58820
196/196 - 52s - loss: 32.3756 - MinusLogProbMetric: 32.3756 - val_loss: 33.2450 - val_MinusLogProbMetric: 33.2450 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 220/1000
2023-10-09 12:50:25.745 
Epoch 220/1000 
	 loss: 32.3591, MinusLogProbMetric: 32.3591, val_loss: 32.3003, val_MinusLogProbMetric: 32.3003

Epoch 220: val_loss improved from 32.58820 to 32.30027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 32.3591 - MinusLogProbMetric: 32.3591 - val_loss: 32.3003 - val_MinusLogProbMetric: 32.3003 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 221/1000
2023-10-09 12:51:19.266 
Epoch 221/1000 
	 loss: 32.2150, MinusLogProbMetric: 32.2150, val_loss: 32.3083, val_MinusLogProbMetric: 32.3083

Epoch 221: val_loss did not improve from 32.30027
196/196 - 53s - loss: 32.2150 - MinusLogProbMetric: 32.2150 - val_loss: 32.3083 - val_MinusLogProbMetric: 32.3083 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 222/1000
2023-10-09 12:52:13.219 
Epoch 222/1000 
	 loss: 32.1319, MinusLogProbMetric: 32.1319, val_loss: 32.2758, val_MinusLogProbMetric: 32.2758

Epoch 222: val_loss improved from 32.30027 to 32.27585, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 32.1319 - MinusLogProbMetric: 32.1319 - val_loss: 32.2758 - val_MinusLogProbMetric: 32.2758 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 223/1000
2023-10-09 12:53:09.671 
Epoch 223/1000 
	 loss: 32.0448, MinusLogProbMetric: 32.0448, val_loss: 32.2244, val_MinusLogProbMetric: 32.2244

Epoch 223: val_loss improved from 32.27585 to 32.22438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 57s - loss: 32.0448 - MinusLogProbMetric: 32.0448 - val_loss: 32.2244 - val_MinusLogProbMetric: 32.2244 - lr: 1.1111e-04 - 57s/epoch - 292ms/step
Epoch 224/1000
2023-10-09 12:54:06.433 
Epoch 224/1000 
	 loss: 32.0030, MinusLogProbMetric: 32.0030, val_loss: 32.0887, val_MinusLogProbMetric: 32.0887

Epoch 224: val_loss improved from 32.22438 to 32.08867, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 32.0030 - MinusLogProbMetric: 32.0030 - val_loss: 32.0887 - val_MinusLogProbMetric: 32.0887 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 225/1000
2023-10-09 12:54:59.276 
Epoch 225/1000 
	 loss: 31.9221, MinusLogProbMetric: 31.9221, val_loss: 32.0192, val_MinusLogProbMetric: 32.0192

Epoch 225: val_loss improved from 32.08867 to 32.01915, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 31.9221 - MinusLogProbMetric: 31.9221 - val_loss: 32.0192 - val_MinusLogProbMetric: 32.0192 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 226/1000
2023-10-09 12:55:52.406 
Epoch 226/1000 
	 loss: 31.9367, MinusLogProbMetric: 31.9367, val_loss: 32.0348, val_MinusLogProbMetric: 32.0348

Epoch 226: val_loss did not improve from 32.01915
196/196 - 52s - loss: 31.9367 - MinusLogProbMetric: 31.9367 - val_loss: 32.0348 - val_MinusLogProbMetric: 32.0348 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 227/1000
2023-10-09 12:56:45.803 
Epoch 227/1000 
	 loss: 31.7989, MinusLogProbMetric: 31.7989, val_loss: 31.9543, val_MinusLogProbMetric: 31.9543

Epoch 227: val_loss improved from 32.01915 to 31.95425, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 31.7989 - MinusLogProbMetric: 31.7989 - val_loss: 31.9543 - val_MinusLogProbMetric: 31.9543 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 228/1000
2023-10-09 12:57:39.607 
Epoch 228/1000 
	 loss: 31.6968, MinusLogProbMetric: 31.6968, val_loss: 31.7949, val_MinusLogProbMetric: 31.7949

Epoch 228: val_loss improved from 31.95425 to 31.79492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 31.6968 - MinusLogProbMetric: 31.6968 - val_loss: 31.7949 - val_MinusLogProbMetric: 31.7949 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 229/1000
2023-10-09 12:58:33.811 
Epoch 229/1000 
	 loss: 31.6477, MinusLogProbMetric: 31.6477, val_loss: 31.8851, val_MinusLogProbMetric: 31.8851

Epoch 229: val_loss did not improve from 31.79492
196/196 - 53s - loss: 31.6477 - MinusLogProbMetric: 31.6477 - val_loss: 31.8851 - val_MinusLogProbMetric: 31.8851 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 230/1000
2023-10-09 12:59:27.413 
Epoch 230/1000 
	 loss: 31.6262, MinusLogProbMetric: 31.6262, val_loss: 31.6409, val_MinusLogProbMetric: 31.6409

Epoch 230: val_loss improved from 31.79492 to 31.64092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 31.6262 - MinusLogProbMetric: 31.6262 - val_loss: 31.6409 - val_MinusLogProbMetric: 31.6409 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 231/1000
2023-10-09 13:00:21.088 
Epoch 231/1000 
	 loss: 31.4835, MinusLogProbMetric: 31.4835, val_loss: 31.5218, val_MinusLogProbMetric: 31.5218

Epoch 231: val_loss improved from 31.64092 to 31.52176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 31.4835 - MinusLogProbMetric: 31.4835 - val_loss: 31.5218 - val_MinusLogProbMetric: 31.5218 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 232/1000
2023-10-09 13:01:15.496 
Epoch 232/1000 
	 loss: 31.4432, MinusLogProbMetric: 31.4432, val_loss: 31.6986, val_MinusLogProbMetric: 31.6986

Epoch 232: val_loss did not improve from 31.52176
196/196 - 54s - loss: 31.4432 - MinusLogProbMetric: 31.4432 - val_loss: 31.6986 - val_MinusLogProbMetric: 31.6986 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 233/1000
2023-10-09 13:02:08.686 
Epoch 233/1000 
	 loss: 31.3876, MinusLogProbMetric: 31.3876, val_loss: 31.4954, val_MinusLogProbMetric: 31.4954

Epoch 233: val_loss improved from 31.52176 to 31.49543, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 31.3876 - MinusLogProbMetric: 31.3876 - val_loss: 31.4954 - val_MinusLogProbMetric: 31.4954 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 234/1000
2023-10-09 13:03:03.091 
Epoch 234/1000 
	 loss: 32.1073, MinusLogProbMetric: 32.1073, val_loss: 31.5974, val_MinusLogProbMetric: 31.5974

Epoch 234: val_loss did not improve from 31.49543
196/196 - 54s - loss: 32.1073 - MinusLogProbMetric: 32.1073 - val_loss: 31.5974 - val_MinusLogProbMetric: 31.5974 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 235/1000
2023-10-09 13:03:56.950 
Epoch 235/1000 
	 loss: 31.2599, MinusLogProbMetric: 31.2599, val_loss: 31.4549, val_MinusLogProbMetric: 31.4549

Epoch 235: val_loss improved from 31.49543 to 31.45493, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 31.2599 - MinusLogProbMetric: 31.2599 - val_loss: 31.4549 - val_MinusLogProbMetric: 31.4549 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 236/1000
2023-10-09 13:04:51.281 
Epoch 236/1000 
	 loss: 31.2118, MinusLogProbMetric: 31.2118, val_loss: 31.3783, val_MinusLogProbMetric: 31.3783

Epoch 236: val_loss improved from 31.45493 to 31.37827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 31.2118 - MinusLogProbMetric: 31.2118 - val_loss: 31.3783 - val_MinusLogProbMetric: 31.3783 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 237/1000
2023-10-09 13:05:46.166 
Epoch 237/1000 
	 loss: 31.1477, MinusLogProbMetric: 31.1477, val_loss: 31.2360, val_MinusLogProbMetric: 31.2360

Epoch 237: val_loss improved from 31.37827 to 31.23602, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 31.1477 - MinusLogProbMetric: 31.1477 - val_loss: 31.2360 - val_MinusLogProbMetric: 31.2360 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 238/1000
2023-10-09 13:06:40.878 
Epoch 238/1000 
	 loss: 31.0446, MinusLogProbMetric: 31.0446, val_loss: 31.1043, val_MinusLogProbMetric: 31.1043

Epoch 238: val_loss improved from 31.23602 to 31.10427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 31.0446 - MinusLogProbMetric: 31.0446 - val_loss: 31.1043 - val_MinusLogProbMetric: 31.1043 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 239/1000
2023-10-09 13:07:35.693 
Epoch 239/1000 
	 loss: 31.0004, MinusLogProbMetric: 31.0004, val_loss: 31.1085, val_MinusLogProbMetric: 31.1085

Epoch 239: val_loss did not improve from 31.10427
196/196 - 54s - loss: 31.0004 - MinusLogProbMetric: 31.0004 - val_loss: 31.1085 - val_MinusLogProbMetric: 31.1085 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 240/1000
2023-10-09 13:08:29.637 
Epoch 240/1000 
	 loss: 30.9576, MinusLogProbMetric: 30.9576, val_loss: 30.9526, val_MinusLogProbMetric: 30.9526

Epoch 240: val_loss improved from 31.10427 to 30.95256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 30.9576 - MinusLogProbMetric: 30.9576 - val_loss: 30.9526 - val_MinusLogProbMetric: 30.9526 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 241/1000
2023-10-09 13:09:23.915 
Epoch 241/1000 
	 loss: 30.8627, MinusLogProbMetric: 30.8627, val_loss: 31.0881, val_MinusLogProbMetric: 31.0881

Epoch 241: val_loss did not improve from 30.95256
196/196 - 54s - loss: 30.8627 - MinusLogProbMetric: 30.8627 - val_loss: 31.0881 - val_MinusLogProbMetric: 31.0881 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 242/1000
2023-10-09 13:10:14.882 
Epoch 242/1000 
	 loss: 30.8498, MinusLogProbMetric: 30.8498, val_loss: 30.9462, val_MinusLogProbMetric: 30.9462

Epoch 242: val_loss improved from 30.95256 to 30.94616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 30.8498 - MinusLogProbMetric: 30.8498 - val_loss: 30.9462 - val_MinusLogProbMetric: 30.9462 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 243/1000
2023-10-09 13:11:05.246 
Epoch 243/1000 
	 loss: 30.7437, MinusLogProbMetric: 30.7437, val_loss: 30.9197, val_MinusLogProbMetric: 30.9197

Epoch 243: val_loss improved from 30.94616 to 30.91969, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 30.7437 - MinusLogProbMetric: 30.7437 - val_loss: 30.9197 - val_MinusLogProbMetric: 30.9197 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 244/1000
2023-10-09 13:12:00.306 
Epoch 244/1000 
	 loss: 30.7476, MinusLogProbMetric: 30.7476, val_loss: 30.7535, val_MinusLogProbMetric: 30.7535

Epoch 244: val_loss improved from 30.91969 to 30.75350, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 30.7476 - MinusLogProbMetric: 30.7476 - val_loss: 30.7535 - val_MinusLogProbMetric: 30.7535 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 245/1000
2023-10-09 13:12:55.331 
Epoch 245/1000 
	 loss: 30.6595, MinusLogProbMetric: 30.6595, val_loss: 30.8009, val_MinusLogProbMetric: 30.8009

Epoch 245: val_loss did not improve from 30.75350
196/196 - 54s - loss: 30.6595 - MinusLogProbMetric: 30.6595 - val_loss: 30.8009 - val_MinusLogProbMetric: 30.8009 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 246/1000
2023-10-09 13:13:48.357 
Epoch 246/1000 
	 loss: 30.5975, MinusLogProbMetric: 30.5975, val_loss: 30.7752, val_MinusLogProbMetric: 30.7752

Epoch 246: val_loss did not improve from 30.75350
196/196 - 53s - loss: 30.5975 - MinusLogProbMetric: 30.5975 - val_loss: 30.7752 - val_MinusLogProbMetric: 30.7752 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 247/1000
2023-10-09 13:14:40.628 
Epoch 247/1000 
	 loss: 30.5721, MinusLogProbMetric: 30.5721, val_loss: 30.5686, val_MinusLogProbMetric: 30.5686

Epoch 247: val_loss improved from 30.75350 to 30.56862, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 30.5721 - MinusLogProbMetric: 30.5721 - val_loss: 30.5686 - val_MinusLogProbMetric: 30.5686 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 248/1000
2023-10-09 13:15:34.885 
Epoch 248/1000 
	 loss: 30.8125, MinusLogProbMetric: 30.8125, val_loss: 30.5471, val_MinusLogProbMetric: 30.5471

Epoch 248: val_loss improved from 30.56862 to 30.54710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 30.8125 - MinusLogProbMetric: 30.8125 - val_loss: 30.5471 - val_MinusLogProbMetric: 30.5471 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 249/1000
2023-10-09 13:16:29.165 
Epoch 249/1000 
	 loss: 30.4135, MinusLogProbMetric: 30.4135, val_loss: 30.9195, val_MinusLogProbMetric: 30.9195

Epoch 249: val_loss did not improve from 30.54710
196/196 - 54s - loss: 30.4135 - MinusLogProbMetric: 30.4135 - val_loss: 30.9195 - val_MinusLogProbMetric: 30.9195 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 250/1000
2023-10-09 13:17:23.066 
Epoch 250/1000 
	 loss: 30.4109, MinusLogProbMetric: 30.4109, val_loss: 30.5571, val_MinusLogProbMetric: 30.5571

Epoch 250: val_loss did not improve from 30.54710
196/196 - 54s - loss: 30.4109 - MinusLogProbMetric: 30.4109 - val_loss: 30.5571 - val_MinusLogProbMetric: 30.5571 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 251/1000
2023-10-09 13:18:15.522 
Epoch 251/1000 
	 loss: 30.3775, MinusLogProbMetric: 30.3775, val_loss: 30.4567, val_MinusLogProbMetric: 30.4567

Epoch 251: val_loss improved from 30.54710 to 30.45673, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 30.3775 - MinusLogProbMetric: 30.3775 - val_loss: 30.4567 - val_MinusLogProbMetric: 30.4567 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 252/1000
2023-10-09 13:19:08.851 
Epoch 252/1000 
	 loss: 30.2400, MinusLogProbMetric: 30.2400, val_loss: 30.2796, val_MinusLogProbMetric: 30.2796

Epoch 252: val_loss improved from 30.45673 to 30.27963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 30.2400 - MinusLogProbMetric: 30.2400 - val_loss: 30.2796 - val_MinusLogProbMetric: 30.2796 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 253/1000
2023-10-09 13:20:01.760 
Epoch 253/1000 
	 loss: 30.1964, MinusLogProbMetric: 30.1964, val_loss: 30.3539, val_MinusLogProbMetric: 30.3539

Epoch 253: val_loss did not improve from 30.27963
196/196 - 52s - loss: 30.1964 - MinusLogProbMetric: 30.1964 - val_loss: 30.3539 - val_MinusLogProbMetric: 30.3539 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 254/1000
2023-10-09 13:20:55.744 
Epoch 254/1000 
	 loss: 30.1208, MinusLogProbMetric: 30.1208, val_loss: 30.3867, val_MinusLogProbMetric: 30.3867

Epoch 254: val_loss did not improve from 30.27963
196/196 - 54s - loss: 30.1208 - MinusLogProbMetric: 30.1208 - val_loss: 30.3867 - val_MinusLogProbMetric: 30.3867 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 255/1000
2023-10-09 13:21:49.371 
Epoch 255/1000 
	 loss: 30.0496, MinusLogProbMetric: 30.0496, val_loss: 30.4428, val_MinusLogProbMetric: 30.4428

Epoch 255: val_loss did not improve from 30.27963
196/196 - 54s - loss: 30.0496 - MinusLogProbMetric: 30.0496 - val_loss: 30.4428 - val_MinusLogProbMetric: 30.4428 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 256/1000
2023-10-09 13:22:42.591 
Epoch 256/1000 
	 loss: 30.0294, MinusLogProbMetric: 30.0294, val_loss: 30.3220, val_MinusLogProbMetric: 30.3220

Epoch 256: val_loss did not improve from 30.27963
196/196 - 53s - loss: 30.0294 - MinusLogProbMetric: 30.0294 - val_loss: 30.3220 - val_MinusLogProbMetric: 30.3220 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 257/1000
2023-10-09 13:23:35.933 
Epoch 257/1000 
	 loss: 29.9855, MinusLogProbMetric: 29.9855, val_loss: 30.0776, val_MinusLogProbMetric: 30.0776

Epoch 257: val_loss improved from 30.27963 to 30.07759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 29.9855 - MinusLogProbMetric: 29.9855 - val_loss: 30.0776 - val_MinusLogProbMetric: 30.0776 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 258/1000
2023-10-09 13:24:29.406 
Epoch 258/1000 
	 loss: 29.9067, MinusLogProbMetric: 29.9067, val_loss: 29.9562, val_MinusLogProbMetric: 29.9562

Epoch 258: val_loss improved from 30.07759 to 29.95622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 29.9067 - MinusLogProbMetric: 29.9067 - val_loss: 29.9562 - val_MinusLogProbMetric: 29.9562 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 259/1000
2023-10-09 13:25:23.350 
Epoch 259/1000 
	 loss: 29.8830, MinusLogProbMetric: 29.8830, val_loss: 29.8205, val_MinusLogProbMetric: 29.8205

Epoch 259: val_loss improved from 29.95622 to 29.82047, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 29.8830 - MinusLogProbMetric: 29.8830 - val_loss: 29.8205 - val_MinusLogProbMetric: 29.8205 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 260/1000
2023-10-09 13:26:18.414 
Epoch 260/1000 
	 loss: 29.9563, MinusLogProbMetric: 29.9563, val_loss: 30.1099, val_MinusLogProbMetric: 30.1099

Epoch 260: val_loss did not improve from 29.82047
196/196 - 54s - loss: 29.9563 - MinusLogProbMetric: 29.9563 - val_loss: 30.1099 - val_MinusLogProbMetric: 30.1099 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 261/1000
2023-10-09 13:27:11.842 
Epoch 261/1000 
	 loss: 29.7343, MinusLogProbMetric: 29.7343, val_loss: 29.8213, val_MinusLogProbMetric: 29.8213

Epoch 261: val_loss did not improve from 29.82047
196/196 - 53s - loss: 29.7343 - MinusLogProbMetric: 29.7343 - val_loss: 29.8213 - val_MinusLogProbMetric: 29.8213 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 262/1000
2023-10-09 13:28:05.134 
Epoch 262/1000 
	 loss: 29.6811, MinusLogProbMetric: 29.6811, val_loss: 29.8440, val_MinusLogProbMetric: 29.8440

Epoch 262: val_loss did not improve from 29.82047
196/196 - 53s - loss: 29.6811 - MinusLogProbMetric: 29.6811 - val_loss: 29.8440 - val_MinusLogProbMetric: 29.8440 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 263/1000
2023-10-09 13:28:59.727 
Epoch 263/1000 
	 loss: 29.6492, MinusLogProbMetric: 29.6492, val_loss: 29.5828, val_MinusLogProbMetric: 29.5828

Epoch 263: val_loss improved from 29.82047 to 29.58282, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 29.6492 - MinusLogProbMetric: 29.6492 - val_loss: 29.5828 - val_MinusLogProbMetric: 29.5828 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 264/1000
2023-10-09 13:29:53.933 
Epoch 264/1000 
	 loss: 29.6291, MinusLogProbMetric: 29.6291, val_loss: 29.6670, val_MinusLogProbMetric: 29.6670

Epoch 264: val_loss did not improve from 29.58282
196/196 - 53s - loss: 29.6291 - MinusLogProbMetric: 29.6291 - val_loss: 29.6670 - val_MinusLogProbMetric: 29.6670 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 265/1000
2023-10-09 13:30:46.409 
Epoch 265/1000 
	 loss: 29.5290, MinusLogProbMetric: 29.5290, val_loss: 29.9341, val_MinusLogProbMetric: 29.9341

Epoch 265: val_loss did not improve from 29.58282
196/196 - 52s - loss: 29.5290 - MinusLogProbMetric: 29.5290 - val_loss: 29.9341 - val_MinusLogProbMetric: 29.9341 - lr: 1.1111e-04 - 52s/epoch - 268ms/step
Epoch 266/1000
2023-10-09 13:31:40.206 
Epoch 266/1000 
	 loss: 33.4803, MinusLogProbMetric: 33.4803, val_loss: 29.8799, val_MinusLogProbMetric: 29.8799

Epoch 266: val_loss did not improve from 29.58282
196/196 - 54s - loss: 33.4803 - MinusLogProbMetric: 33.4803 - val_loss: 29.8799 - val_MinusLogProbMetric: 29.8799 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 267/1000
2023-10-09 13:32:33.894 
Epoch 267/1000 
	 loss: 29.5283, MinusLogProbMetric: 29.5283, val_loss: 29.4386, val_MinusLogProbMetric: 29.4386

Epoch 267: val_loss improved from 29.58282 to 29.43860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 29.5283 - MinusLogProbMetric: 29.5283 - val_loss: 29.4386 - val_MinusLogProbMetric: 29.4386 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 268/1000
2023-10-09 13:33:27.527 
Epoch 268/1000 
	 loss: 29.3935, MinusLogProbMetric: 29.3935, val_loss: 29.3891, val_MinusLogProbMetric: 29.3891

Epoch 268: val_loss improved from 29.43860 to 29.38909, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 29.3935 - MinusLogProbMetric: 29.3935 - val_loss: 29.3891 - val_MinusLogProbMetric: 29.3891 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 269/1000
2023-10-09 13:34:20.571 
Epoch 269/1000 
	 loss: 29.3490, MinusLogProbMetric: 29.3490, val_loss: 29.8890, val_MinusLogProbMetric: 29.8890

Epoch 269: val_loss did not improve from 29.38909
196/196 - 52s - loss: 29.3490 - MinusLogProbMetric: 29.3490 - val_loss: 29.8890 - val_MinusLogProbMetric: 29.8890 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 270/1000
2023-10-09 13:35:14.704 
Epoch 270/1000 
	 loss: 29.2867, MinusLogProbMetric: 29.2867, val_loss: 29.5875, val_MinusLogProbMetric: 29.5875

Epoch 270: val_loss did not improve from 29.38909
196/196 - 54s - loss: 29.2867 - MinusLogProbMetric: 29.2867 - val_loss: 29.5875 - val_MinusLogProbMetric: 29.5875 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 271/1000
2023-10-09 13:36:08.275 
Epoch 271/1000 
	 loss: 29.2588, MinusLogProbMetric: 29.2588, val_loss: 29.3181, val_MinusLogProbMetric: 29.3181

Epoch 271: val_loss improved from 29.38909 to 29.31810, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 29.2588 - MinusLogProbMetric: 29.2588 - val_loss: 29.3181 - val_MinusLogProbMetric: 29.3181 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 272/1000
2023-10-09 13:37:02.965 
Epoch 272/1000 
	 loss: 29.2830, MinusLogProbMetric: 29.2830, val_loss: 29.3335, val_MinusLogProbMetric: 29.3335

Epoch 272: val_loss did not improve from 29.31810
196/196 - 54s - loss: 29.2830 - MinusLogProbMetric: 29.2830 - val_loss: 29.3335 - val_MinusLogProbMetric: 29.3335 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 273/1000
2023-10-09 13:37:57.461 
Epoch 273/1000 
	 loss: 29.1111, MinusLogProbMetric: 29.1111, val_loss: 29.3045, val_MinusLogProbMetric: 29.3045

Epoch 273: val_loss improved from 29.31810 to 29.30449, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 29.1111 - MinusLogProbMetric: 29.1111 - val_loss: 29.3045 - val_MinusLogProbMetric: 29.3045 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 274/1000
2023-10-09 13:38:51.262 
Epoch 274/1000 
	 loss: 29.1024, MinusLogProbMetric: 29.1024, val_loss: 29.1972, val_MinusLogProbMetric: 29.1972

Epoch 274: val_loss improved from 29.30449 to 29.19716, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 29.1024 - MinusLogProbMetric: 29.1024 - val_loss: 29.1972 - val_MinusLogProbMetric: 29.1972 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 275/1000
2023-10-09 13:39:45.396 
Epoch 275/1000 
	 loss: 29.0156, MinusLogProbMetric: 29.0156, val_loss: 29.5320, val_MinusLogProbMetric: 29.5320

Epoch 275: val_loss did not improve from 29.19716
196/196 - 53s - loss: 29.0156 - MinusLogProbMetric: 29.0156 - val_loss: 29.5320 - val_MinusLogProbMetric: 29.5320 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 276/1000
2023-10-09 13:40:37.713 
Epoch 276/1000 
	 loss: 29.0037, MinusLogProbMetric: 29.0037, val_loss: 29.0723, val_MinusLogProbMetric: 29.0723

Epoch 276: val_loss improved from 29.19716 to 29.07227, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 29.0037 - MinusLogProbMetric: 29.0037 - val_loss: 29.0723 - val_MinusLogProbMetric: 29.0723 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 277/1000
2023-10-09 13:41:33.588 
Epoch 277/1000 
	 loss: 28.9598, MinusLogProbMetric: 28.9598, val_loss: 28.9604, val_MinusLogProbMetric: 28.9604

Epoch 277: val_loss improved from 29.07227 to 28.96041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 28.9598 - MinusLogProbMetric: 28.9598 - val_loss: 28.9604 - val_MinusLogProbMetric: 28.9604 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 278/1000
2023-10-09 13:42:27.960 
Epoch 278/1000 
	 loss: 28.9052, MinusLogProbMetric: 28.9052, val_loss: 29.1456, val_MinusLogProbMetric: 29.1456

Epoch 278: val_loss did not improve from 28.96041
196/196 - 54s - loss: 28.9052 - MinusLogProbMetric: 28.9052 - val_loss: 29.1456 - val_MinusLogProbMetric: 29.1456 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 279/1000
2023-10-09 13:43:21.618 
Epoch 279/1000 
	 loss: 28.8656, MinusLogProbMetric: 28.8656, val_loss: 28.9194, val_MinusLogProbMetric: 28.9194

Epoch 279: val_loss improved from 28.96041 to 28.91936, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 28.8656 - MinusLogProbMetric: 28.8656 - val_loss: 28.9194 - val_MinusLogProbMetric: 28.9194 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 280/1000
2023-10-09 13:44:16.244 
Epoch 280/1000 
	 loss: 28.7640, MinusLogProbMetric: 28.7640, val_loss: 29.2752, val_MinusLogProbMetric: 29.2752

Epoch 280: val_loss did not improve from 28.91936
196/196 - 54s - loss: 28.7640 - MinusLogProbMetric: 28.7640 - val_loss: 29.2752 - val_MinusLogProbMetric: 29.2752 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 281/1000
2023-10-09 13:45:10.705 
Epoch 281/1000 
	 loss: 28.7627, MinusLogProbMetric: 28.7627, val_loss: 29.1520, val_MinusLogProbMetric: 29.1520

Epoch 281: val_loss did not improve from 28.91936
196/196 - 54s - loss: 28.7627 - MinusLogProbMetric: 28.7627 - val_loss: 29.1520 - val_MinusLogProbMetric: 29.1520 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 282/1000
2023-10-09 13:46:03.928 
Epoch 282/1000 
	 loss: 28.7809, MinusLogProbMetric: 28.7809, val_loss: 28.7794, val_MinusLogProbMetric: 28.7794

Epoch 282: val_loss improved from 28.91936 to 28.77943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 28.7809 - MinusLogProbMetric: 28.7809 - val_loss: 28.7794 - val_MinusLogProbMetric: 28.7794 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 283/1000
2023-10-09 13:46:57.301 
Epoch 283/1000 
	 loss: 28.6303, MinusLogProbMetric: 28.6303, val_loss: 28.8592, val_MinusLogProbMetric: 28.8592

Epoch 283: val_loss did not improve from 28.77943
196/196 - 53s - loss: 28.6303 - MinusLogProbMetric: 28.6303 - val_loss: 28.8592 - val_MinusLogProbMetric: 28.8592 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 284/1000
2023-10-09 13:47:49.959 
Epoch 284/1000 
	 loss: 30.7576, MinusLogProbMetric: 30.7576, val_loss: 29.0349, val_MinusLogProbMetric: 29.0349

Epoch 284: val_loss did not improve from 28.77943
196/196 - 53s - loss: 30.7576 - MinusLogProbMetric: 30.7576 - val_loss: 29.0349 - val_MinusLogProbMetric: 29.0349 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 285/1000
2023-10-09 13:48:42.466 
Epoch 285/1000 
	 loss: 28.6379, MinusLogProbMetric: 28.6379, val_loss: 28.6758, val_MinusLogProbMetric: 28.6758

Epoch 285: val_loss improved from 28.77943 to 28.67585, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 28.6379 - MinusLogProbMetric: 28.6379 - val_loss: 28.6758 - val_MinusLogProbMetric: 28.6758 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 286/1000
2023-10-09 13:49:36.937 
Epoch 286/1000 
	 loss: 28.5609, MinusLogProbMetric: 28.5609, val_loss: 28.8606, val_MinusLogProbMetric: 28.8606

Epoch 286: val_loss did not improve from 28.67585
196/196 - 54s - loss: 28.5609 - MinusLogProbMetric: 28.5609 - val_loss: 28.8606 - val_MinusLogProbMetric: 28.8606 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 287/1000
2023-10-09 13:50:30.185 
Epoch 287/1000 
	 loss: 28.4397, MinusLogProbMetric: 28.4397, val_loss: 28.4228, val_MinusLogProbMetric: 28.4228

Epoch 287: val_loss improved from 28.67585 to 28.42284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 28.4397 - MinusLogProbMetric: 28.4397 - val_loss: 28.4228 - val_MinusLogProbMetric: 28.4228 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 288/1000
2023-10-09 13:51:24.689 
Epoch 288/1000 
	 loss: 28.4108, MinusLogProbMetric: 28.4108, val_loss: 28.5033, val_MinusLogProbMetric: 28.5033

Epoch 288: val_loss did not improve from 28.42284
196/196 - 54s - loss: 28.4108 - MinusLogProbMetric: 28.4108 - val_loss: 28.5033 - val_MinusLogProbMetric: 28.5033 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 289/1000
2023-10-09 13:52:18.782 
Epoch 289/1000 
	 loss: 28.3769, MinusLogProbMetric: 28.3769, val_loss: 28.5375, val_MinusLogProbMetric: 28.5375

Epoch 289: val_loss did not improve from 28.42284
196/196 - 54s - loss: 28.3769 - MinusLogProbMetric: 28.3769 - val_loss: 28.5375 - val_MinusLogProbMetric: 28.5375 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 290/1000
2023-10-09 13:53:08.966 
Epoch 290/1000 
	 loss: 28.3408, MinusLogProbMetric: 28.3408, val_loss: 28.4352, val_MinusLogProbMetric: 28.4352

Epoch 290: val_loss did not improve from 28.42284
196/196 - 50s - loss: 28.3408 - MinusLogProbMetric: 28.3408 - val_loss: 28.4352 - val_MinusLogProbMetric: 28.4352 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 291/1000
2023-10-09 13:53:58.449 
Epoch 291/1000 
	 loss: 28.3434, MinusLogProbMetric: 28.3434, val_loss: 28.6409, val_MinusLogProbMetric: 28.6409

Epoch 291: val_loss did not improve from 28.42284
196/196 - 49s - loss: 28.3434 - MinusLogProbMetric: 28.3434 - val_loss: 28.6409 - val_MinusLogProbMetric: 28.6409 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 292/1000
2023-10-09 13:54:50.514 
Epoch 292/1000 
	 loss: 28.2523, MinusLogProbMetric: 28.2523, val_loss: 28.6289, val_MinusLogProbMetric: 28.6289

Epoch 292: val_loss did not improve from 28.42284
196/196 - 52s - loss: 28.2523 - MinusLogProbMetric: 28.2523 - val_loss: 28.6289 - val_MinusLogProbMetric: 28.6289 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 293/1000
2023-10-09 13:55:43.577 
Epoch 293/1000 
	 loss: 29.6814, MinusLogProbMetric: 29.6814, val_loss: 28.9955, val_MinusLogProbMetric: 28.9955

Epoch 293: val_loss did not improve from 28.42284
196/196 - 53s - loss: 29.6814 - MinusLogProbMetric: 29.6814 - val_loss: 28.9955 - val_MinusLogProbMetric: 28.9955 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 294/1000
2023-10-09 13:56:36.697 
Epoch 294/1000 
	 loss: 28.2482, MinusLogProbMetric: 28.2482, val_loss: 28.1737, val_MinusLogProbMetric: 28.1737

Epoch 294: val_loss improved from 28.42284 to 28.17365, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 28.2482 - MinusLogProbMetric: 28.2482 - val_loss: 28.1737 - val_MinusLogProbMetric: 28.1737 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 295/1000
2023-10-09 13:57:31.507 
Epoch 295/1000 
	 loss: 28.0867, MinusLogProbMetric: 28.0867, val_loss: 28.3869, val_MinusLogProbMetric: 28.3869

Epoch 295: val_loss did not improve from 28.17365
196/196 - 54s - loss: 28.0867 - MinusLogProbMetric: 28.0867 - val_loss: 28.3869 - val_MinusLogProbMetric: 28.3869 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 296/1000
2023-10-09 13:58:25.613 
Epoch 296/1000 
	 loss: 28.1331, MinusLogProbMetric: 28.1331, val_loss: 28.3004, val_MinusLogProbMetric: 28.3004

Epoch 296: val_loss did not improve from 28.17365
196/196 - 54s - loss: 28.1331 - MinusLogProbMetric: 28.1331 - val_loss: 28.3004 - val_MinusLogProbMetric: 28.3004 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 297/1000
2023-10-09 13:59:19.267 
Epoch 297/1000 
	 loss: 28.0855, MinusLogProbMetric: 28.0855, val_loss: 28.5105, val_MinusLogProbMetric: 28.5105

Epoch 297: val_loss did not improve from 28.17365
196/196 - 54s - loss: 28.0855 - MinusLogProbMetric: 28.0855 - val_loss: 28.5105 - val_MinusLogProbMetric: 28.5105 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 298/1000
2023-10-09 14:00:10.090 
Epoch 298/1000 
	 loss: 28.0658, MinusLogProbMetric: 28.0658, val_loss: 28.2417, val_MinusLogProbMetric: 28.2417

Epoch 298: val_loss did not improve from 28.17365
196/196 - 51s - loss: 28.0658 - MinusLogProbMetric: 28.0658 - val_loss: 28.2417 - val_MinusLogProbMetric: 28.2417 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 299/1000
2023-10-09 14:01:04.044 
Epoch 299/1000 
	 loss: 28.1724, MinusLogProbMetric: 28.1724, val_loss: 33.1256, val_MinusLogProbMetric: 33.1256

Epoch 299: val_loss did not improve from 28.17365
196/196 - 54s - loss: 28.1724 - MinusLogProbMetric: 28.1724 - val_loss: 33.1256 - val_MinusLogProbMetric: 33.1256 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 300/1000
2023-10-09 14:01:57.751 
Epoch 300/1000 
	 loss: 28.3404, MinusLogProbMetric: 28.3404, val_loss: 27.9965, val_MinusLogProbMetric: 27.9965

Epoch 300: val_loss improved from 28.17365 to 27.99652, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 28.3404 - MinusLogProbMetric: 28.3404 - val_loss: 27.9965 - val_MinusLogProbMetric: 27.9965 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 301/1000
2023-10-09 14:02:52.078 
Epoch 301/1000 
	 loss: 27.8986, MinusLogProbMetric: 27.8986, val_loss: 28.1437, val_MinusLogProbMetric: 28.1437

Epoch 301: val_loss did not improve from 27.99652
196/196 - 54s - loss: 27.8986 - MinusLogProbMetric: 27.8986 - val_loss: 28.1437 - val_MinusLogProbMetric: 28.1437 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 302/1000
2023-10-09 14:03:46.488 
Epoch 302/1000 
	 loss: 27.8520, MinusLogProbMetric: 27.8520, val_loss: 28.2546, val_MinusLogProbMetric: 28.2546

Epoch 302: val_loss did not improve from 27.99652
196/196 - 54s - loss: 27.8520 - MinusLogProbMetric: 27.8520 - val_loss: 28.2546 - val_MinusLogProbMetric: 28.2546 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 303/1000
2023-10-09 14:04:39.736 
Epoch 303/1000 
	 loss: 27.8364, MinusLogProbMetric: 27.8364, val_loss: 28.1365, val_MinusLogProbMetric: 28.1365

Epoch 303: val_loss did not improve from 27.99652
196/196 - 53s - loss: 27.8364 - MinusLogProbMetric: 27.8364 - val_loss: 28.1365 - val_MinusLogProbMetric: 28.1365 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 304/1000
2023-10-09 14:05:33.586 
Epoch 304/1000 
	 loss: 27.7731, MinusLogProbMetric: 27.7731, val_loss: 28.0607, val_MinusLogProbMetric: 28.0607

Epoch 304: val_loss did not improve from 27.99652
196/196 - 54s - loss: 27.7731 - MinusLogProbMetric: 27.7731 - val_loss: 28.0607 - val_MinusLogProbMetric: 28.0607 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 305/1000
2023-10-09 14:06:27.667 
Epoch 305/1000 
	 loss: 27.7476, MinusLogProbMetric: 27.7476, val_loss: 27.8465, val_MinusLogProbMetric: 27.8465

Epoch 305: val_loss improved from 27.99652 to 27.84653, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 27.7476 - MinusLogProbMetric: 27.7476 - val_loss: 27.8465 - val_MinusLogProbMetric: 27.8465 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 306/1000
2023-10-09 14:07:22.625 
Epoch 306/1000 
	 loss: 27.7368, MinusLogProbMetric: 27.7368, val_loss: 27.7650, val_MinusLogProbMetric: 27.7650

Epoch 306: val_loss improved from 27.84653 to 27.76495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 27.7368 - MinusLogProbMetric: 27.7368 - val_loss: 27.7650 - val_MinusLogProbMetric: 27.7650 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 307/1000
2023-10-09 14:08:16.098 
Epoch 307/1000 
	 loss: 27.6458, MinusLogProbMetric: 27.6458, val_loss: 27.8620, val_MinusLogProbMetric: 27.8620

Epoch 307: val_loss did not improve from 27.76495
196/196 - 53s - loss: 27.6458 - MinusLogProbMetric: 27.6458 - val_loss: 27.8620 - val_MinusLogProbMetric: 27.8620 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 308/1000
2023-10-09 14:09:09.557 
Epoch 308/1000 
	 loss: 27.6597, MinusLogProbMetric: 27.6597, val_loss: 27.6438, val_MinusLogProbMetric: 27.6438

Epoch 308: val_loss improved from 27.76495 to 27.64377, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 27.6597 - MinusLogProbMetric: 27.6597 - val_loss: 27.6438 - val_MinusLogProbMetric: 27.6438 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 309/1000
2023-10-09 14:10:04.169 
Epoch 309/1000 
	 loss: 27.6183, MinusLogProbMetric: 27.6183, val_loss: 27.5725, val_MinusLogProbMetric: 27.5725

Epoch 309: val_loss improved from 27.64377 to 27.57255, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 27.6183 - MinusLogProbMetric: 27.6183 - val_loss: 27.5725 - val_MinusLogProbMetric: 27.5725 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 310/1000
2023-10-09 14:10:58.993 
Epoch 310/1000 
	 loss: 27.8506, MinusLogProbMetric: 27.8506, val_loss: 28.9825, val_MinusLogProbMetric: 28.9825

Epoch 310: val_loss did not improve from 27.57255
196/196 - 54s - loss: 27.8506 - MinusLogProbMetric: 27.8506 - val_loss: 28.9825 - val_MinusLogProbMetric: 28.9825 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 311/1000
2023-10-09 14:11:53.024 
Epoch 311/1000 
	 loss: 27.5629, MinusLogProbMetric: 27.5629, val_loss: 27.6268, val_MinusLogProbMetric: 27.6268

Epoch 311: val_loss did not improve from 27.57255
196/196 - 54s - loss: 27.5629 - MinusLogProbMetric: 27.5629 - val_loss: 27.6268 - val_MinusLogProbMetric: 27.6268 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 312/1000
2023-10-09 14:12:47.056 
Epoch 312/1000 
	 loss: 27.4796, MinusLogProbMetric: 27.4796, val_loss: 27.4424, val_MinusLogProbMetric: 27.4424

Epoch 312: val_loss improved from 27.57255 to 27.44235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 27.4796 - MinusLogProbMetric: 27.4796 - val_loss: 27.4424 - val_MinusLogProbMetric: 27.4424 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 313/1000
2023-10-09 14:13:42.122 
Epoch 313/1000 
	 loss: 27.4307, MinusLogProbMetric: 27.4307, val_loss: 27.8835, val_MinusLogProbMetric: 27.8835

Epoch 313: val_loss did not improve from 27.44235
196/196 - 54s - loss: 27.4307 - MinusLogProbMetric: 27.4307 - val_loss: 27.8835 - val_MinusLogProbMetric: 27.8835 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 314/1000
2023-10-09 14:14:35.771 
Epoch 314/1000 
	 loss: 27.3881, MinusLogProbMetric: 27.3881, val_loss: 27.3909, val_MinusLogProbMetric: 27.3909

Epoch 314: val_loss improved from 27.44235 to 27.39085, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 27.3881 - MinusLogProbMetric: 27.3881 - val_loss: 27.3909 - val_MinusLogProbMetric: 27.3909 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 315/1000
2023-10-09 14:15:30.598 
Epoch 315/1000 
	 loss: 27.3491, MinusLogProbMetric: 27.3491, val_loss: 28.0297, val_MinusLogProbMetric: 28.0297

Epoch 315: val_loss did not improve from 27.39085
196/196 - 54s - loss: 27.3491 - MinusLogProbMetric: 27.3491 - val_loss: 28.0297 - val_MinusLogProbMetric: 28.0297 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 316/1000
2023-10-09 14:16:24.495 
Epoch 316/1000 
	 loss: 27.3411, MinusLogProbMetric: 27.3411, val_loss: 27.9501, val_MinusLogProbMetric: 27.9501

Epoch 316: val_loss did not improve from 27.39085
196/196 - 54s - loss: 27.3411 - MinusLogProbMetric: 27.3411 - val_loss: 27.9501 - val_MinusLogProbMetric: 27.9501 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 317/1000
2023-10-09 14:17:18.400 
Epoch 317/1000 
	 loss: 38.4921, MinusLogProbMetric: 38.4921, val_loss: 31.2292, val_MinusLogProbMetric: 31.2292

Epoch 317: val_loss did not improve from 27.39085
196/196 - 54s - loss: 38.4921 - MinusLogProbMetric: 38.4921 - val_loss: 31.2292 - val_MinusLogProbMetric: 31.2292 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 318/1000
2023-10-09 14:18:11.925 
Epoch 318/1000 
	 loss: 29.1848, MinusLogProbMetric: 29.1848, val_loss: 28.6403, val_MinusLogProbMetric: 28.6403

Epoch 318: val_loss did not improve from 27.39085
196/196 - 54s - loss: 29.1848 - MinusLogProbMetric: 29.1848 - val_loss: 28.6403 - val_MinusLogProbMetric: 28.6403 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 319/1000
2023-10-09 14:19:05.787 
Epoch 319/1000 
	 loss: 27.9905, MinusLogProbMetric: 27.9905, val_loss: 27.6472, val_MinusLogProbMetric: 27.6472

Epoch 319: val_loss did not improve from 27.39085
196/196 - 54s - loss: 27.9905 - MinusLogProbMetric: 27.9905 - val_loss: 27.6472 - val_MinusLogProbMetric: 27.6472 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 320/1000
2023-10-09 14:19:58.503 
Epoch 320/1000 
	 loss: 27.4536, MinusLogProbMetric: 27.4536, val_loss: 27.5093, val_MinusLogProbMetric: 27.5093

Epoch 320: val_loss did not improve from 27.39085
196/196 - 53s - loss: 27.4536 - MinusLogProbMetric: 27.4536 - val_loss: 27.5093 - val_MinusLogProbMetric: 27.5093 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 321/1000
2023-10-09 14:20:52.488 
Epoch 321/1000 
	 loss: 27.3194, MinusLogProbMetric: 27.3194, val_loss: 27.2654, val_MinusLogProbMetric: 27.2654

Epoch 321: val_loss improved from 27.39085 to 27.26535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 27.3194 - MinusLogProbMetric: 27.3194 - val_loss: 27.2654 - val_MinusLogProbMetric: 27.2654 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 322/1000
2023-10-09 14:21:47.617 
Epoch 322/1000 
	 loss: 27.2151, MinusLogProbMetric: 27.2151, val_loss: 27.2745, val_MinusLogProbMetric: 27.2745

Epoch 322: val_loss did not improve from 27.26535
196/196 - 54s - loss: 27.2151 - MinusLogProbMetric: 27.2151 - val_loss: 27.2745 - val_MinusLogProbMetric: 27.2745 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 323/1000
2023-10-09 14:22:41.929 
Epoch 323/1000 
	 loss: 27.1173, MinusLogProbMetric: 27.1173, val_loss: 27.2448, val_MinusLogProbMetric: 27.2448

Epoch 323: val_loss improved from 27.26535 to 27.24483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 27.1173 - MinusLogProbMetric: 27.1173 - val_loss: 27.2448 - val_MinusLogProbMetric: 27.2448 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 324/1000
2023-10-09 14:23:37.449 
Epoch 324/1000 
	 loss: 27.1181, MinusLogProbMetric: 27.1181, val_loss: 27.2142, val_MinusLogProbMetric: 27.2142

Epoch 324: val_loss improved from 27.24483 to 27.21418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 56s - loss: 27.1181 - MinusLogProbMetric: 27.1181 - val_loss: 27.2142 - val_MinusLogProbMetric: 27.2142 - lr: 1.1111e-04 - 56s/epoch - 283ms/step
Epoch 325/1000
2023-10-09 14:24:31.731 
Epoch 325/1000 
	 loss: 27.0842, MinusLogProbMetric: 27.0842, val_loss: 27.1852, val_MinusLogProbMetric: 27.1852

Epoch 325: val_loss improved from 27.21418 to 27.18520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 27.0842 - MinusLogProbMetric: 27.0842 - val_loss: 27.1852 - val_MinusLogProbMetric: 27.1852 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 326/1000
2023-10-09 14:25:25.684 
Epoch 326/1000 
	 loss: 27.0686, MinusLogProbMetric: 27.0686, val_loss: 27.5821, val_MinusLogProbMetric: 27.5821

Epoch 326: val_loss did not improve from 27.18520
196/196 - 53s - loss: 27.0686 - MinusLogProbMetric: 27.0686 - val_loss: 27.5821 - val_MinusLogProbMetric: 27.5821 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 327/1000
2023-10-09 14:26:19.764 
Epoch 327/1000 
	 loss: 27.0262, MinusLogProbMetric: 27.0262, val_loss: 26.9333, val_MinusLogProbMetric: 26.9333

Epoch 327: val_loss improved from 27.18520 to 26.93332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 27.0262 - MinusLogProbMetric: 27.0262 - val_loss: 26.9333 - val_MinusLogProbMetric: 26.9333 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 328/1000
2023-10-09 14:27:14.366 
Epoch 328/1000 
	 loss: 26.8858, MinusLogProbMetric: 26.8858, val_loss: 26.8704, val_MinusLogProbMetric: 26.8704

Epoch 328: val_loss improved from 26.93332 to 26.87039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 26.8858 - MinusLogProbMetric: 26.8858 - val_loss: 26.8704 - val_MinusLogProbMetric: 26.8704 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 329/1000
2023-10-09 14:28:09.068 
Epoch 329/1000 
	 loss: 26.7803, MinusLogProbMetric: 26.7803, val_loss: 27.1286, val_MinusLogProbMetric: 27.1286

Epoch 329: val_loss did not improve from 26.87039
196/196 - 54s - loss: 26.7803 - MinusLogProbMetric: 26.7803 - val_loss: 27.1286 - val_MinusLogProbMetric: 27.1286 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 330/1000
2023-10-09 14:29:03.241 
Epoch 330/1000 
	 loss: 26.7450, MinusLogProbMetric: 26.7450, val_loss: 26.7674, val_MinusLogProbMetric: 26.7674

Epoch 330: val_loss improved from 26.87039 to 26.76740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 26.7450 - MinusLogProbMetric: 26.7450 - val_loss: 26.7674 - val_MinusLogProbMetric: 26.7674 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 331/1000
2023-10-09 14:29:58.123 
Epoch 331/1000 
	 loss: 47.6234, MinusLogProbMetric: 47.6234, val_loss: 75.8763, val_MinusLogProbMetric: 75.8763

Epoch 331: val_loss did not improve from 26.76740
196/196 - 54s - loss: 47.6234 - MinusLogProbMetric: 47.6234 - val_loss: 75.8763 - val_MinusLogProbMetric: 75.8763 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 332/1000
2023-10-09 14:30:51.711 
Epoch 332/1000 
	 loss: 52.5601, MinusLogProbMetric: 52.5601, val_loss: 44.2672, val_MinusLogProbMetric: 44.2672

Epoch 332: val_loss did not improve from 26.76740
196/196 - 54s - loss: 52.5601 - MinusLogProbMetric: 52.5601 - val_loss: 44.2672 - val_MinusLogProbMetric: 44.2672 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 333/1000
2023-10-09 14:31:45.029 
Epoch 333/1000 
	 loss: 39.7405, MinusLogProbMetric: 39.7405, val_loss: 36.0104, val_MinusLogProbMetric: 36.0104

Epoch 333: val_loss did not improve from 26.76740
196/196 - 53s - loss: 39.7405 - MinusLogProbMetric: 39.7405 - val_loss: 36.0104 - val_MinusLogProbMetric: 36.0104 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 334/1000
2023-10-09 14:32:38.795 
Epoch 334/1000 
	 loss: 34.3019, MinusLogProbMetric: 34.3019, val_loss: 33.1083, val_MinusLogProbMetric: 33.1083

Epoch 334: val_loss did not improve from 26.76740
196/196 - 54s - loss: 34.3019 - MinusLogProbMetric: 34.3019 - val_loss: 33.1083 - val_MinusLogProbMetric: 33.1083 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 335/1000
2023-10-09 14:33:32.484 
Epoch 335/1000 
	 loss: 32.2428, MinusLogProbMetric: 32.2428, val_loss: 31.6812, val_MinusLogProbMetric: 31.6812

Epoch 335: val_loss did not improve from 26.76740
196/196 - 54s - loss: 32.2428 - MinusLogProbMetric: 32.2428 - val_loss: 31.6812 - val_MinusLogProbMetric: 31.6812 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 336/1000
2023-10-09 14:34:24.535 
Epoch 336/1000 
	 loss: 46.5189, MinusLogProbMetric: 46.5189, val_loss: 47.1861, val_MinusLogProbMetric: 47.1861

Epoch 336: val_loss did not improve from 26.76740
196/196 - 52s - loss: 46.5189 - MinusLogProbMetric: 46.5189 - val_loss: 47.1861 - val_MinusLogProbMetric: 47.1861 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 337/1000
2023-10-09 14:35:19.138 
Epoch 337/1000 
	 loss: 43.9634, MinusLogProbMetric: 43.9634, val_loss: 41.4555, val_MinusLogProbMetric: 41.4555

Epoch 337: val_loss did not improve from 26.76740
196/196 - 55s - loss: 43.9634 - MinusLogProbMetric: 43.9634 - val_loss: 41.4555 - val_MinusLogProbMetric: 41.4555 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 338/1000
2023-10-09 14:36:12.501 
Epoch 338/1000 
	 loss: 39.2569, MinusLogProbMetric: 39.2569, val_loss: 37.3463, val_MinusLogProbMetric: 37.3463

Epoch 338: val_loss did not improve from 26.76740
196/196 - 53s - loss: 39.2569 - MinusLogProbMetric: 39.2569 - val_loss: 37.3463 - val_MinusLogProbMetric: 37.3463 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 339/1000
2023-10-09 14:37:05.128 
Epoch 339/1000 
	 loss: 36.2058, MinusLogProbMetric: 36.2058, val_loss: 35.4625, val_MinusLogProbMetric: 35.4625

Epoch 339: val_loss did not improve from 26.76740
196/196 - 53s - loss: 36.2058 - MinusLogProbMetric: 36.2058 - val_loss: 35.4625 - val_MinusLogProbMetric: 35.4625 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 340/1000
2023-10-09 14:37:58.966 
Epoch 340/1000 
	 loss: 34.8786, MinusLogProbMetric: 34.8786, val_loss: 34.1359, val_MinusLogProbMetric: 34.1359

Epoch 340: val_loss did not improve from 26.76740
196/196 - 54s - loss: 34.8786 - MinusLogProbMetric: 34.8786 - val_loss: 34.1359 - val_MinusLogProbMetric: 34.1359 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 341/1000
2023-10-09 14:38:52.728 
Epoch 341/1000 
	 loss: 33.2622, MinusLogProbMetric: 33.2622, val_loss: 32.5891, val_MinusLogProbMetric: 32.5891

Epoch 341: val_loss did not improve from 26.76740
196/196 - 54s - loss: 33.2622 - MinusLogProbMetric: 33.2622 - val_loss: 32.5891 - val_MinusLogProbMetric: 32.5891 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 342/1000
2023-10-09 14:39:46.866 
Epoch 342/1000 
	 loss: 31.8899, MinusLogProbMetric: 31.8899, val_loss: 31.4137, val_MinusLogProbMetric: 31.4137

Epoch 342: val_loss did not improve from 26.76740
196/196 - 54s - loss: 31.8899 - MinusLogProbMetric: 31.8899 - val_loss: 31.4137 - val_MinusLogProbMetric: 31.4137 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 343/1000
2023-10-09 14:40:40.563 
Epoch 343/1000 
	 loss: 30.9038, MinusLogProbMetric: 30.9038, val_loss: 30.6240, val_MinusLogProbMetric: 30.6240

Epoch 343: val_loss did not improve from 26.76740
196/196 - 54s - loss: 30.9038 - MinusLogProbMetric: 30.9038 - val_loss: 30.6240 - val_MinusLogProbMetric: 30.6240 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 344/1000
2023-10-09 14:41:33.941 
Epoch 344/1000 
	 loss: 30.3530, MinusLogProbMetric: 30.3530, val_loss: 30.4607, val_MinusLogProbMetric: 30.4607

Epoch 344: val_loss did not improve from 26.76740
196/196 - 53s - loss: 30.3530 - MinusLogProbMetric: 30.3530 - val_loss: 30.4607 - val_MinusLogProbMetric: 30.4607 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 345/1000
2023-10-09 14:42:28.212 
Epoch 345/1000 
	 loss: 29.9637, MinusLogProbMetric: 29.9637, val_loss: 30.1349, val_MinusLogProbMetric: 30.1349

Epoch 345: val_loss did not improve from 26.76740
196/196 - 54s - loss: 29.9637 - MinusLogProbMetric: 29.9637 - val_loss: 30.1349 - val_MinusLogProbMetric: 30.1349 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 346/1000
2023-10-09 14:43:21.908 
Epoch 346/1000 
	 loss: 29.5996, MinusLogProbMetric: 29.5996, val_loss: 29.6097, val_MinusLogProbMetric: 29.6097

Epoch 346: val_loss did not improve from 26.76740
196/196 - 54s - loss: 29.5996 - MinusLogProbMetric: 29.5996 - val_loss: 29.6097 - val_MinusLogProbMetric: 29.6097 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 347/1000
2023-10-09 14:44:15.699 
Epoch 347/1000 
	 loss: 29.3599, MinusLogProbMetric: 29.3599, val_loss: 29.2497, val_MinusLogProbMetric: 29.2497

Epoch 347: val_loss did not improve from 26.76740
196/196 - 54s - loss: 29.3599 - MinusLogProbMetric: 29.3599 - val_loss: 29.2497 - val_MinusLogProbMetric: 29.2497 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 348/1000
2023-10-09 14:45:09.385 
Epoch 348/1000 
	 loss: 29.1245, MinusLogProbMetric: 29.1245, val_loss: 29.0314, val_MinusLogProbMetric: 29.0314

Epoch 348: val_loss did not improve from 26.76740
196/196 - 54s - loss: 29.1245 - MinusLogProbMetric: 29.1245 - val_loss: 29.0314 - val_MinusLogProbMetric: 29.0314 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 349/1000
2023-10-09 14:46:02.282 
Epoch 349/1000 
	 loss: 28.8750, MinusLogProbMetric: 28.8750, val_loss: 29.0834, val_MinusLogProbMetric: 29.0834

Epoch 349: val_loss did not improve from 26.76740
196/196 - 53s - loss: 28.8750 - MinusLogProbMetric: 28.8750 - val_loss: 29.0834 - val_MinusLogProbMetric: 29.0834 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 350/1000
2023-10-09 14:46:55.576 
Epoch 350/1000 
	 loss: 28.7020, MinusLogProbMetric: 28.7020, val_loss: 28.7613, val_MinusLogProbMetric: 28.7613

Epoch 350: val_loss did not improve from 26.76740
196/196 - 53s - loss: 28.7020 - MinusLogProbMetric: 28.7020 - val_loss: 28.7613 - val_MinusLogProbMetric: 28.7613 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 351/1000
2023-10-09 14:47:48.960 
Epoch 351/1000 
	 loss: 28.6177, MinusLogProbMetric: 28.6177, val_loss: 28.7296, val_MinusLogProbMetric: 28.7296

Epoch 351: val_loss did not improve from 26.76740
196/196 - 53s - loss: 28.6177 - MinusLogProbMetric: 28.6177 - val_loss: 28.7296 - val_MinusLogProbMetric: 28.7296 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 352/1000
2023-10-09 14:48:42.988 
Epoch 352/1000 
	 loss: 28.4319, MinusLogProbMetric: 28.4319, val_loss: 28.7414, val_MinusLogProbMetric: 28.7414

Epoch 352: val_loss did not improve from 26.76740
196/196 - 54s - loss: 28.4319 - MinusLogProbMetric: 28.4319 - val_loss: 28.7414 - val_MinusLogProbMetric: 28.7414 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 353/1000
2023-10-09 14:49:36.927 
Epoch 353/1000 
	 loss: 28.3048, MinusLogProbMetric: 28.3048, val_loss: 28.5332, val_MinusLogProbMetric: 28.5332

Epoch 353: val_loss did not improve from 26.76740
196/196 - 54s - loss: 28.3048 - MinusLogProbMetric: 28.3048 - val_loss: 28.5332 - val_MinusLogProbMetric: 28.5332 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 354/1000
2023-10-09 14:50:30.998 
Epoch 354/1000 
	 loss: 28.1899, MinusLogProbMetric: 28.1899, val_loss: 28.3424, val_MinusLogProbMetric: 28.3424

Epoch 354: val_loss did not improve from 26.76740
196/196 - 54s - loss: 28.1899 - MinusLogProbMetric: 28.1899 - val_loss: 28.3424 - val_MinusLogProbMetric: 28.3424 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 355/1000
2023-10-09 14:51:24.632 
Epoch 355/1000 
	 loss: 28.1024, MinusLogProbMetric: 28.1024, val_loss: 28.1275, val_MinusLogProbMetric: 28.1275

Epoch 355: val_loss did not improve from 26.76740
196/196 - 54s - loss: 28.1024 - MinusLogProbMetric: 28.1024 - val_loss: 28.1275 - val_MinusLogProbMetric: 28.1275 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 356/1000
2023-10-09 14:52:17.926 
Epoch 356/1000 
	 loss: 27.9981, MinusLogProbMetric: 27.9981, val_loss: 27.9468, val_MinusLogProbMetric: 27.9468

Epoch 356: val_loss did not improve from 26.76740
196/196 - 53s - loss: 27.9981 - MinusLogProbMetric: 27.9981 - val_loss: 27.9468 - val_MinusLogProbMetric: 27.9468 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 357/1000
2023-10-09 14:53:12.267 
Epoch 357/1000 
	 loss: 27.9457, MinusLogProbMetric: 27.9457, val_loss: 27.9325, val_MinusLogProbMetric: 27.9325

Epoch 357: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.9457 - MinusLogProbMetric: 27.9457 - val_loss: 27.9325 - val_MinusLogProbMetric: 27.9325 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 358/1000
2023-10-09 14:54:05.621 
Epoch 358/1000 
	 loss: 27.8106, MinusLogProbMetric: 27.8106, val_loss: 27.7819, val_MinusLogProbMetric: 27.7819

Epoch 358: val_loss did not improve from 26.76740
196/196 - 53s - loss: 27.8106 - MinusLogProbMetric: 27.8106 - val_loss: 27.7819 - val_MinusLogProbMetric: 27.7819 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 359/1000
2023-10-09 14:55:00.362 
Epoch 359/1000 
	 loss: 27.7142, MinusLogProbMetric: 27.7142, val_loss: 27.9331, val_MinusLogProbMetric: 27.9331

Epoch 359: val_loss did not improve from 26.76740
196/196 - 55s - loss: 27.7142 - MinusLogProbMetric: 27.7142 - val_loss: 27.9331 - val_MinusLogProbMetric: 27.9331 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 360/1000
2023-10-09 14:55:54.558 
Epoch 360/1000 
	 loss: 27.6590, MinusLogProbMetric: 27.6590, val_loss: 27.6963, val_MinusLogProbMetric: 27.6963

Epoch 360: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.6590 - MinusLogProbMetric: 27.6590 - val_loss: 27.6963 - val_MinusLogProbMetric: 27.6963 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 361/1000
2023-10-09 14:56:48.806 
Epoch 361/1000 
	 loss: 27.5235, MinusLogProbMetric: 27.5235, val_loss: 27.5485, val_MinusLogProbMetric: 27.5485

Epoch 361: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.5235 - MinusLogProbMetric: 27.5235 - val_loss: 27.5485 - val_MinusLogProbMetric: 27.5485 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 362/1000
2023-10-09 14:57:42.619 
Epoch 362/1000 
	 loss: 27.5322, MinusLogProbMetric: 27.5322, val_loss: 27.4354, val_MinusLogProbMetric: 27.4354

Epoch 362: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.5322 - MinusLogProbMetric: 27.5322 - val_loss: 27.4354 - val_MinusLogProbMetric: 27.4354 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 363/1000
2023-10-09 14:58:37.370 
Epoch 363/1000 
	 loss: 27.4010, MinusLogProbMetric: 27.4010, val_loss: 27.4869, val_MinusLogProbMetric: 27.4869

Epoch 363: val_loss did not improve from 26.76740
196/196 - 55s - loss: 27.4010 - MinusLogProbMetric: 27.4010 - val_loss: 27.4869 - val_MinusLogProbMetric: 27.4869 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 364/1000
2023-10-09 14:59:29.166 
Epoch 364/1000 
	 loss: 27.4036, MinusLogProbMetric: 27.4036, val_loss: 27.6377, val_MinusLogProbMetric: 27.6377

Epoch 364: val_loss did not improve from 26.76740
196/196 - 52s - loss: 27.4036 - MinusLogProbMetric: 27.4036 - val_loss: 27.6377 - val_MinusLogProbMetric: 27.6377 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 365/1000
2023-10-09 15:00:23.177 
Epoch 365/1000 
	 loss: 27.2891, MinusLogProbMetric: 27.2891, val_loss: 27.3107, val_MinusLogProbMetric: 27.3107

Epoch 365: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.2891 - MinusLogProbMetric: 27.2891 - val_loss: 27.3107 - val_MinusLogProbMetric: 27.3107 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 366/1000
2023-10-09 15:01:16.958 
Epoch 366/1000 
	 loss: 27.3610, MinusLogProbMetric: 27.3610, val_loss: 27.9243, val_MinusLogProbMetric: 27.9243

Epoch 366: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.3610 - MinusLogProbMetric: 27.3610 - val_loss: 27.9243 - val_MinusLogProbMetric: 27.9243 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 367/1000
2023-10-09 15:02:10.235 
Epoch 367/1000 
	 loss: 27.2275, MinusLogProbMetric: 27.2275, val_loss: 27.4844, val_MinusLogProbMetric: 27.4844

Epoch 367: val_loss did not improve from 26.76740
196/196 - 53s - loss: 27.2275 - MinusLogProbMetric: 27.2275 - val_loss: 27.4844 - val_MinusLogProbMetric: 27.4844 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 368/1000
2023-10-09 15:03:03.829 
Epoch 368/1000 
	 loss: 27.1375, MinusLogProbMetric: 27.1375, val_loss: 27.1718, val_MinusLogProbMetric: 27.1718

Epoch 368: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.1375 - MinusLogProbMetric: 27.1375 - val_loss: 27.1718 - val_MinusLogProbMetric: 27.1718 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 369/1000
2023-10-09 15:03:57.696 
Epoch 369/1000 
	 loss: 27.0564, MinusLogProbMetric: 27.0564, val_loss: 27.3094, val_MinusLogProbMetric: 27.3094

Epoch 369: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.0564 - MinusLogProbMetric: 27.0564 - val_loss: 27.3094 - val_MinusLogProbMetric: 27.3094 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 370/1000
2023-10-09 15:04:51.673 
Epoch 370/1000 
	 loss: 26.9799, MinusLogProbMetric: 26.9799, val_loss: 27.3882, val_MinusLogProbMetric: 27.3882

Epoch 370: val_loss did not improve from 26.76740
196/196 - 54s - loss: 26.9799 - MinusLogProbMetric: 26.9799 - val_loss: 27.3882 - val_MinusLogProbMetric: 27.3882 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 371/1000
2023-10-09 15:05:45.280 
Epoch 371/1000 
	 loss: 27.0265, MinusLogProbMetric: 27.0265, val_loss: 27.4407, val_MinusLogProbMetric: 27.4407

Epoch 371: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.0265 - MinusLogProbMetric: 27.0265 - val_loss: 27.4407 - val_MinusLogProbMetric: 27.4407 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 372/1000
2023-10-09 15:06:39.237 
Epoch 372/1000 
	 loss: 27.0665, MinusLogProbMetric: 27.0665, val_loss: 28.5639, val_MinusLogProbMetric: 28.5639

Epoch 372: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.0665 - MinusLogProbMetric: 27.0665 - val_loss: 28.5639 - val_MinusLogProbMetric: 28.5639 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 373/1000
2023-10-09 15:07:32.652 
Epoch 373/1000 
	 loss: 27.1430, MinusLogProbMetric: 27.1430, val_loss: 26.9618, val_MinusLogProbMetric: 26.9618

Epoch 373: val_loss did not improve from 26.76740
196/196 - 53s - loss: 27.1430 - MinusLogProbMetric: 27.1430 - val_loss: 26.9618 - val_MinusLogProbMetric: 26.9618 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 374/1000
2023-10-09 15:08:26.429 
Epoch 374/1000 
	 loss: 26.7818, MinusLogProbMetric: 26.7818, val_loss: 26.8903, val_MinusLogProbMetric: 26.8903

Epoch 374: val_loss did not improve from 26.76740
196/196 - 54s - loss: 26.7818 - MinusLogProbMetric: 26.7818 - val_loss: 26.8903 - val_MinusLogProbMetric: 26.8903 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 375/1000
2023-10-09 15:09:19.788 
Epoch 375/1000 
	 loss: 30.0184, MinusLogProbMetric: 30.0184, val_loss: 29.4961, val_MinusLogProbMetric: 29.4961

Epoch 375: val_loss did not improve from 26.76740
196/196 - 53s - loss: 30.0184 - MinusLogProbMetric: 30.0184 - val_loss: 29.4961 - val_MinusLogProbMetric: 29.4961 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 376/1000
2023-10-09 15:10:13.064 
Epoch 376/1000 
	 loss: 28.7999, MinusLogProbMetric: 28.7999, val_loss: 28.7064, val_MinusLogProbMetric: 28.7064

Epoch 376: val_loss did not improve from 26.76740
196/196 - 53s - loss: 28.7999 - MinusLogProbMetric: 28.7999 - val_loss: 28.7064 - val_MinusLogProbMetric: 28.7064 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 377/1000
2023-10-09 15:11:06.880 
Epoch 377/1000 
	 loss: 28.3258, MinusLogProbMetric: 28.3258, val_loss: 28.3717, val_MinusLogProbMetric: 28.3717

Epoch 377: val_loss did not improve from 26.76740
196/196 - 54s - loss: 28.3258 - MinusLogProbMetric: 28.3258 - val_loss: 28.3717 - val_MinusLogProbMetric: 28.3717 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 378/1000
2023-10-09 15:12:00.262 
Epoch 378/1000 
	 loss: 28.0596, MinusLogProbMetric: 28.0596, val_loss: 28.1328, val_MinusLogProbMetric: 28.1328

Epoch 378: val_loss did not improve from 26.76740
196/196 - 53s - loss: 28.0596 - MinusLogProbMetric: 28.0596 - val_loss: 28.1328 - val_MinusLogProbMetric: 28.1328 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 379/1000
2023-10-09 15:12:54.071 
Epoch 379/1000 
	 loss: 27.8521, MinusLogProbMetric: 27.8521, val_loss: 27.9754, val_MinusLogProbMetric: 27.9754

Epoch 379: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.8521 - MinusLogProbMetric: 27.8521 - val_loss: 27.9754 - val_MinusLogProbMetric: 27.9754 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 380/1000
2023-10-09 15:13:48.983 
Epoch 380/1000 
	 loss: 27.7098, MinusLogProbMetric: 27.7098, val_loss: 27.8005, val_MinusLogProbMetric: 27.8005

Epoch 380: val_loss did not improve from 26.76740
196/196 - 55s - loss: 27.7098 - MinusLogProbMetric: 27.7098 - val_loss: 27.8005 - val_MinusLogProbMetric: 27.8005 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 381/1000
2023-10-09 15:14:43.154 
Epoch 381/1000 
	 loss: 27.5032, MinusLogProbMetric: 27.5032, val_loss: 27.5951, val_MinusLogProbMetric: 27.5951

Epoch 381: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.5032 - MinusLogProbMetric: 27.5032 - val_loss: 27.5951 - val_MinusLogProbMetric: 27.5951 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 382/1000
2023-10-09 15:15:36.973 
Epoch 382/1000 
	 loss: 27.4441, MinusLogProbMetric: 27.4441, val_loss: 27.5164, val_MinusLogProbMetric: 27.5164

Epoch 382: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.4441 - MinusLogProbMetric: 27.4441 - val_loss: 27.5164 - val_MinusLogProbMetric: 27.5164 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 383/1000
2023-10-09 15:16:30.091 
Epoch 383/1000 
	 loss: 27.3830, MinusLogProbMetric: 27.3830, val_loss: 27.4842, val_MinusLogProbMetric: 27.4842

Epoch 383: val_loss did not improve from 26.76740
196/196 - 53s - loss: 27.3830 - MinusLogProbMetric: 27.3830 - val_loss: 27.4842 - val_MinusLogProbMetric: 27.4842 - lr: 5.5556e-05 - 53s/epoch - 271ms/step
Epoch 384/1000
2023-10-09 15:17:22.561 
Epoch 384/1000 
	 loss: 27.3208, MinusLogProbMetric: 27.3208, val_loss: 27.4567, val_MinusLogProbMetric: 27.4567

Epoch 384: val_loss did not improve from 26.76740
196/196 - 52s - loss: 27.3208 - MinusLogProbMetric: 27.3208 - val_loss: 27.4567 - val_MinusLogProbMetric: 27.4567 - lr: 5.5556e-05 - 52s/epoch - 268ms/step
Epoch 385/1000
2023-10-09 15:18:16.441 
Epoch 385/1000 
	 loss: 27.2756, MinusLogProbMetric: 27.2756, val_loss: 27.4116, val_MinusLogProbMetric: 27.4116

Epoch 385: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.2756 - MinusLogProbMetric: 27.2756 - val_loss: 27.4116 - val_MinusLogProbMetric: 27.4116 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 386/1000
2023-10-09 15:19:10.455 
Epoch 386/1000 
	 loss: 27.2317, MinusLogProbMetric: 27.2317, val_loss: 27.3995, val_MinusLogProbMetric: 27.3995

Epoch 386: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.2317 - MinusLogProbMetric: 27.2317 - val_loss: 27.3995 - val_MinusLogProbMetric: 27.3995 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 387/1000
2023-10-09 15:20:04.342 
Epoch 387/1000 
	 loss: 27.1900, MinusLogProbMetric: 27.1900, val_loss: 27.2958, val_MinusLogProbMetric: 27.2958

Epoch 387: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.1900 - MinusLogProbMetric: 27.1900 - val_loss: 27.2958 - val_MinusLogProbMetric: 27.2958 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 388/1000
2023-10-09 15:20:57.627 
Epoch 388/1000 
	 loss: 27.1368, MinusLogProbMetric: 27.1368, val_loss: 27.2941, val_MinusLogProbMetric: 27.2941

Epoch 388: val_loss did not improve from 26.76740
196/196 - 53s - loss: 27.1368 - MinusLogProbMetric: 27.1368 - val_loss: 27.2941 - val_MinusLogProbMetric: 27.2941 - lr: 5.5556e-05 - 53s/epoch - 272ms/step
Epoch 389/1000
2023-10-09 15:21:51.345 
Epoch 389/1000 
	 loss: 27.1038, MinusLogProbMetric: 27.1038, val_loss: 27.2802, val_MinusLogProbMetric: 27.2802

Epoch 389: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.1038 - MinusLogProbMetric: 27.1038 - val_loss: 27.2802 - val_MinusLogProbMetric: 27.2802 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 390/1000
2023-10-09 15:22:44.907 
Epoch 390/1000 
	 loss: 27.0427, MinusLogProbMetric: 27.0427, val_loss: 27.1793, val_MinusLogProbMetric: 27.1793

Epoch 390: val_loss did not improve from 26.76740
196/196 - 54s - loss: 27.0427 - MinusLogProbMetric: 27.0427 - val_loss: 27.1793 - val_MinusLogProbMetric: 27.1793 - lr: 5.5556e-05 - 54s/epoch - 273ms/step
Epoch 391/1000
2023-10-09 15:23:39.563 
Epoch 391/1000 
	 loss: 26.9966, MinusLogProbMetric: 26.9966, val_loss: 27.0847, val_MinusLogProbMetric: 27.0847

Epoch 391: val_loss did not improve from 26.76740
196/196 - 55s - loss: 26.9966 - MinusLogProbMetric: 26.9966 - val_loss: 27.0847 - val_MinusLogProbMetric: 27.0847 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 392/1000
2023-10-09 15:24:33.568 
Epoch 392/1000 
	 loss: 26.9658, MinusLogProbMetric: 26.9658, val_loss: 27.1385, val_MinusLogProbMetric: 27.1385

Epoch 392: val_loss did not improve from 26.76740
196/196 - 54s - loss: 26.9658 - MinusLogProbMetric: 26.9658 - val_loss: 27.1385 - val_MinusLogProbMetric: 27.1385 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 393/1000
2023-10-09 15:25:26.767 
Epoch 393/1000 
	 loss: 26.9207, MinusLogProbMetric: 26.9207, val_loss: 27.0410, val_MinusLogProbMetric: 27.0410

Epoch 393: val_loss did not improve from 26.76740
196/196 - 53s - loss: 26.9207 - MinusLogProbMetric: 26.9207 - val_loss: 27.0410 - val_MinusLogProbMetric: 27.0410 - lr: 5.5556e-05 - 53s/epoch - 271ms/step
Epoch 394/1000
2023-10-09 15:26:20.414 
Epoch 394/1000 
	 loss: 26.8803, MinusLogProbMetric: 26.8803, val_loss: 26.9627, val_MinusLogProbMetric: 26.9627

Epoch 394: val_loss did not improve from 26.76740
196/196 - 54s - loss: 26.8803 - MinusLogProbMetric: 26.8803 - val_loss: 26.9627 - val_MinusLogProbMetric: 26.9627 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 395/1000
2023-10-09 15:27:14.109 
Epoch 395/1000 
	 loss: 26.8444, MinusLogProbMetric: 26.8444, val_loss: 26.9824, val_MinusLogProbMetric: 26.9824

Epoch 395: val_loss did not improve from 26.76740
196/196 - 54s - loss: 26.8444 - MinusLogProbMetric: 26.8444 - val_loss: 26.9824 - val_MinusLogProbMetric: 26.9824 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 396/1000
2023-10-09 15:28:08.147 
Epoch 396/1000 
	 loss: 26.8124, MinusLogProbMetric: 26.8124, val_loss: 26.9202, val_MinusLogProbMetric: 26.9202

Epoch 396: val_loss did not improve from 26.76740
196/196 - 54s - loss: 26.8124 - MinusLogProbMetric: 26.8124 - val_loss: 26.9202 - val_MinusLogProbMetric: 26.9202 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 397/1000
2023-10-09 15:29:01.601 
Epoch 397/1000 
	 loss: 26.7758, MinusLogProbMetric: 26.7758, val_loss: 26.9754, val_MinusLogProbMetric: 26.9754

Epoch 397: val_loss did not improve from 26.76740
196/196 - 53s - loss: 26.7758 - MinusLogProbMetric: 26.7758 - val_loss: 26.9754 - val_MinusLogProbMetric: 26.9754 - lr: 5.5556e-05 - 53s/epoch - 273ms/step
Epoch 398/1000
2023-10-09 15:29:55.304 
Epoch 398/1000 
	 loss: 26.7454, MinusLogProbMetric: 26.7454, val_loss: 26.8671, val_MinusLogProbMetric: 26.8671

Epoch 398: val_loss did not improve from 26.76740
196/196 - 54s - loss: 26.7454 - MinusLogProbMetric: 26.7454 - val_loss: 26.8671 - val_MinusLogProbMetric: 26.8671 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 399/1000
2023-10-09 15:30:49.721 
Epoch 399/1000 
	 loss: 26.6970, MinusLogProbMetric: 26.6970, val_loss: 26.8066, val_MinusLogProbMetric: 26.8066

Epoch 399: val_loss did not improve from 26.76740
196/196 - 54s - loss: 26.6970 - MinusLogProbMetric: 26.6970 - val_loss: 26.8066 - val_MinusLogProbMetric: 26.8066 - lr: 5.5556e-05 - 54s/epoch - 278ms/step
Epoch 400/1000
2023-10-09 15:31:43.705 
Epoch 400/1000 
	 loss: 26.6759, MinusLogProbMetric: 26.6759, val_loss: 26.7714, val_MinusLogProbMetric: 26.7714

Epoch 400: val_loss did not improve from 26.76740
196/196 - 54s - loss: 26.6759 - MinusLogProbMetric: 26.6759 - val_loss: 26.7714 - val_MinusLogProbMetric: 26.7714 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 401/1000
2023-10-09 15:32:37.905 
Epoch 401/1000 
	 loss: 26.6382, MinusLogProbMetric: 26.6382, val_loss: 26.7423, val_MinusLogProbMetric: 26.7423

Epoch 401: val_loss improved from 26.76740 to 26.74229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 26.6382 - MinusLogProbMetric: 26.6382 - val_loss: 26.7423 - val_MinusLogProbMetric: 26.7423 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 402/1000
2023-10-09 15:33:32.726 
Epoch 402/1000 
	 loss: 26.6083, MinusLogProbMetric: 26.6083, val_loss: 26.7802, val_MinusLogProbMetric: 26.7802

Epoch 402: val_loss did not improve from 26.74229
196/196 - 54s - loss: 26.6083 - MinusLogProbMetric: 26.6083 - val_loss: 26.7802 - val_MinusLogProbMetric: 26.7802 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 403/1000
2023-10-09 15:34:27.035 
Epoch 403/1000 
	 loss: 26.5731, MinusLogProbMetric: 26.5731, val_loss: 26.6747, val_MinusLogProbMetric: 26.6747

Epoch 403: val_loss improved from 26.74229 to 26.67468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 26.5731 - MinusLogProbMetric: 26.5731 - val_loss: 26.6747 - val_MinusLogProbMetric: 26.6747 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 404/1000
2023-10-09 15:35:21.930 
Epoch 404/1000 
	 loss: 26.5408, MinusLogProbMetric: 26.5408, val_loss: 26.7116, val_MinusLogProbMetric: 26.7116

Epoch 404: val_loss did not improve from 26.67468
196/196 - 54s - loss: 26.5408 - MinusLogProbMetric: 26.5408 - val_loss: 26.7116 - val_MinusLogProbMetric: 26.7116 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 405/1000
2023-10-09 15:36:15.597 
Epoch 405/1000 
	 loss: 26.5122, MinusLogProbMetric: 26.5122, val_loss: 26.6276, val_MinusLogProbMetric: 26.6276

Epoch 405: val_loss improved from 26.67468 to 26.62757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 26.5122 - MinusLogProbMetric: 26.5122 - val_loss: 26.6276 - val_MinusLogProbMetric: 26.6276 - lr: 5.5556e-05 - 54s/epoch - 278ms/step
Epoch 406/1000
2023-10-09 15:37:10.570 
Epoch 406/1000 
	 loss: 26.4834, MinusLogProbMetric: 26.4834, val_loss: 26.5584, val_MinusLogProbMetric: 26.5584

Epoch 406: val_loss improved from 26.62757 to 26.55843, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 26.4834 - MinusLogProbMetric: 26.4834 - val_loss: 26.5584 - val_MinusLogProbMetric: 26.5584 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 407/1000
2023-10-09 15:38:05.724 
Epoch 407/1000 
	 loss: 26.4548, MinusLogProbMetric: 26.4548, val_loss: 26.5638, val_MinusLogProbMetric: 26.5638

Epoch 407: val_loss did not improve from 26.55843
196/196 - 54s - loss: 26.4548 - MinusLogProbMetric: 26.4548 - val_loss: 26.5638 - val_MinusLogProbMetric: 26.5638 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 408/1000
2023-10-09 15:38:59.731 
Epoch 408/1000 
	 loss: 26.4341, MinusLogProbMetric: 26.4341, val_loss: 26.6171, val_MinusLogProbMetric: 26.6171

Epoch 408: val_loss did not improve from 26.55843
196/196 - 54s - loss: 26.4341 - MinusLogProbMetric: 26.4341 - val_loss: 26.6171 - val_MinusLogProbMetric: 26.6171 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 409/1000
2023-10-09 15:39:53.678 
Epoch 409/1000 
	 loss: 26.4054, MinusLogProbMetric: 26.4054, val_loss: 26.6310, val_MinusLogProbMetric: 26.6310

Epoch 409: val_loss did not improve from 26.55843
196/196 - 54s - loss: 26.4054 - MinusLogProbMetric: 26.4054 - val_loss: 26.6310 - val_MinusLogProbMetric: 26.6310 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 410/1000
2023-10-09 15:40:47.365 
Epoch 410/1000 
	 loss: 26.3695, MinusLogProbMetric: 26.3695, val_loss: 26.5334, val_MinusLogProbMetric: 26.5334

Epoch 410: val_loss improved from 26.55843 to 26.53337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 26.3695 - MinusLogProbMetric: 26.3695 - val_loss: 26.5334 - val_MinusLogProbMetric: 26.5334 - lr: 5.5556e-05 - 54s/epoch - 278ms/step
Epoch 411/1000
2023-10-09 15:41:41.783 
Epoch 411/1000 
	 loss: 26.3498, MinusLogProbMetric: 26.3498, val_loss: 26.4557, val_MinusLogProbMetric: 26.4557

Epoch 411: val_loss improved from 26.53337 to 26.45572, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 26.3498 - MinusLogProbMetric: 26.3498 - val_loss: 26.4557 - val_MinusLogProbMetric: 26.4557 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 412/1000
2023-10-09 15:42:35.938 
Epoch 412/1000 
	 loss: 26.3035, MinusLogProbMetric: 26.3035, val_loss: 26.4470, val_MinusLogProbMetric: 26.4470

Epoch 412: val_loss improved from 26.45572 to 26.44697, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 26.3035 - MinusLogProbMetric: 26.3035 - val_loss: 26.4470 - val_MinusLogProbMetric: 26.4470 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 413/1000
2023-10-09 15:43:28.894 
Epoch 413/1000 
	 loss: 26.2917, MinusLogProbMetric: 26.2917, val_loss: 26.3582, val_MinusLogProbMetric: 26.3582

Epoch 413: val_loss improved from 26.44697 to 26.35824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 26.2917 - MinusLogProbMetric: 26.2917 - val_loss: 26.3582 - val_MinusLogProbMetric: 26.3582 - lr: 5.5556e-05 - 53s/epoch - 270ms/step
Epoch 414/1000
2023-10-09 15:44:23.441 
Epoch 414/1000 
	 loss: 26.2651, MinusLogProbMetric: 26.2651, val_loss: 26.3867, val_MinusLogProbMetric: 26.3867

Epoch 414: val_loss did not improve from 26.35824
196/196 - 54s - loss: 26.2651 - MinusLogProbMetric: 26.2651 - val_loss: 26.3867 - val_MinusLogProbMetric: 26.3867 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 415/1000
2023-10-09 15:45:17.174 
Epoch 415/1000 
	 loss: 26.2370, MinusLogProbMetric: 26.2370, val_loss: 26.3357, val_MinusLogProbMetric: 26.3357

Epoch 415: val_loss improved from 26.35824 to 26.33575, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 26.2370 - MinusLogProbMetric: 26.2370 - val_loss: 26.3357 - val_MinusLogProbMetric: 26.3357 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 416/1000
2023-10-09 15:46:11.570 
Epoch 416/1000 
	 loss: 26.2080, MinusLogProbMetric: 26.2080, val_loss: 26.3487, val_MinusLogProbMetric: 26.3487

Epoch 416: val_loss did not improve from 26.33575
196/196 - 53s - loss: 26.2080 - MinusLogProbMetric: 26.2080 - val_loss: 26.3487 - val_MinusLogProbMetric: 26.3487 - lr: 5.5556e-05 - 53s/epoch - 273ms/step
Epoch 417/1000
2023-10-09 15:47:05.471 
Epoch 417/1000 
	 loss: 26.1860, MinusLogProbMetric: 26.1860, val_loss: 26.3527, val_MinusLogProbMetric: 26.3527

Epoch 417: val_loss did not improve from 26.33575
196/196 - 54s - loss: 26.1860 - MinusLogProbMetric: 26.1860 - val_loss: 26.3527 - val_MinusLogProbMetric: 26.3527 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 418/1000
2023-10-09 15:47:59.821 
Epoch 418/1000 
	 loss: 26.1648, MinusLogProbMetric: 26.1648, val_loss: 26.3034, val_MinusLogProbMetric: 26.3034

Epoch 418: val_loss improved from 26.33575 to 26.30337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 26.1648 - MinusLogProbMetric: 26.1648 - val_loss: 26.3034 - val_MinusLogProbMetric: 26.3034 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 419/1000
2023-10-09 15:48:54.105 
Epoch 419/1000 
	 loss: 26.1611, MinusLogProbMetric: 26.1611, val_loss: 26.2504, val_MinusLogProbMetric: 26.2504

Epoch 419: val_loss improved from 26.30337 to 26.25036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 26.1611 - MinusLogProbMetric: 26.1611 - val_loss: 26.2504 - val_MinusLogProbMetric: 26.2504 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 420/1000
2023-10-09 15:49:48.785 
Epoch 420/1000 
	 loss: 26.1231, MinusLogProbMetric: 26.1231, val_loss: 26.2396, val_MinusLogProbMetric: 26.2396

Epoch 420: val_loss improved from 26.25036 to 26.23957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 26.1231 - MinusLogProbMetric: 26.1231 - val_loss: 26.2396 - val_MinusLogProbMetric: 26.2396 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 421/1000
2023-10-09 15:50:43.087 
Epoch 421/1000 
	 loss: 26.0935, MinusLogProbMetric: 26.0935, val_loss: 26.3060, val_MinusLogProbMetric: 26.3060

Epoch 421: val_loss did not improve from 26.23957
196/196 - 54s - loss: 26.0935 - MinusLogProbMetric: 26.0935 - val_loss: 26.3060 - val_MinusLogProbMetric: 26.3060 - lr: 5.5556e-05 - 54s/epoch - 273ms/step
Epoch 422/1000
2023-10-09 15:51:37.122 
Epoch 422/1000 
	 loss: 26.0636, MinusLogProbMetric: 26.0636, val_loss: 26.2519, val_MinusLogProbMetric: 26.2519

Epoch 422: val_loss did not improve from 26.23957
196/196 - 54s - loss: 26.0636 - MinusLogProbMetric: 26.0636 - val_loss: 26.2519 - val_MinusLogProbMetric: 26.2519 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 423/1000
2023-10-09 15:52:29.774 
Epoch 423/1000 
	 loss: 26.0543, MinusLogProbMetric: 26.0543, val_loss: 26.1862, val_MinusLogProbMetric: 26.1862

Epoch 423: val_loss improved from 26.23957 to 26.18618, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 26.0543 - MinusLogProbMetric: 26.0543 - val_loss: 26.1862 - val_MinusLogProbMetric: 26.1862 - lr: 5.5556e-05 - 53s/epoch - 273ms/step
Epoch 424/1000
2023-10-09 15:53:24.723 
Epoch 424/1000 
	 loss: 26.0098, MinusLogProbMetric: 26.0098, val_loss: 26.1204, val_MinusLogProbMetric: 26.1204

Epoch 424: val_loss improved from 26.18618 to 26.12044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 26.0098 - MinusLogProbMetric: 26.0098 - val_loss: 26.1204 - val_MinusLogProbMetric: 26.1204 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 425/1000
2023-10-09 15:54:19.490 
Epoch 425/1000 
	 loss: 25.9866, MinusLogProbMetric: 25.9866, val_loss: 26.1029, val_MinusLogProbMetric: 26.1029

Epoch 425: val_loss improved from 26.12044 to 26.10293, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 25.9866 - MinusLogProbMetric: 25.9866 - val_loss: 26.1029 - val_MinusLogProbMetric: 26.1029 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 426/1000
2023-10-09 15:55:14.497 
Epoch 426/1000 
	 loss: 25.9676, MinusLogProbMetric: 25.9676, val_loss: 26.1646, val_MinusLogProbMetric: 26.1646

Epoch 426: val_loss did not improve from 26.10293
196/196 - 54s - loss: 25.9676 - MinusLogProbMetric: 25.9676 - val_loss: 26.1646 - val_MinusLogProbMetric: 26.1646 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 427/1000
2023-10-09 15:56:08.406 
Epoch 427/1000 
	 loss: 25.9510, MinusLogProbMetric: 25.9510, val_loss: 26.0982, val_MinusLogProbMetric: 26.0982

Epoch 427: val_loss improved from 26.10293 to 26.09819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 25.9510 - MinusLogProbMetric: 25.9510 - val_loss: 26.0982 - val_MinusLogProbMetric: 26.0982 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 428/1000
2023-10-09 15:57:03.250 
Epoch 428/1000 
	 loss: 25.9333, MinusLogProbMetric: 25.9333, val_loss: 26.0474, val_MinusLogProbMetric: 26.0474

Epoch 428: val_loss improved from 26.09819 to 26.04744, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 25.9333 - MinusLogProbMetric: 25.9333 - val_loss: 26.0474 - val_MinusLogProbMetric: 26.0474 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 429/1000
2023-10-09 15:57:57.851 
Epoch 429/1000 
	 loss: 25.8985, MinusLogProbMetric: 25.8985, val_loss: 25.9971, val_MinusLogProbMetric: 25.9971

Epoch 429: val_loss improved from 26.04744 to 25.99711, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 25.8985 - MinusLogProbMetric: 25.8985 - val_loss: 25.9971 - val_MinusLogProbMetric: 25.9971 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 430/1000
2023-10-09 15:58:53.335 
Epoch 430/1000 
	 loss: 25.8630, MinusLogProbMetric: 25.8630, val_loss: 26.0199, val_MinusLogProbMetric: 26.0199

Epoch 430: val_loss did not improve from 25.99711
196/196 - 55s - loss: 25.8630 - MinusLogProbMetric: 25.8630 - val_loss: 26.0199 - val_MinusLogProbMetric: 26.0199 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 431/1000
2023-10-09 15:59:47.494 
Epoch 431/1000 
	 loss: 25.8354, MinusLogProbMetric: 25.8354, val_loss: 25.9367, val_MinusLogProbMetric: 25.9367

Epoch 431: val_loss improved from 25.99711 to 25.93669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 25.8354 - MinusLogProbMetric: 25.8354 - val_loss: 25.9367 - val_MinusLogProbMetric: 25.9367 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 432/1000
2023-10-09 16:00:42.771 
Epoch 432/1000 
	 loss: 25.8033, MinusLogProbMetric: 25.8033, val_loss: 25.9319, val_MinusLogProbMetric: 25.9319

Epoch 432: val_loss improved from 25.93669 to 25.93190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 25.8033 - MinusLogProbMetric: 25.8033 - val_loss: 25.9319 - val_MinusLogProbMetric: 25.9319 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 433/1000
2023-10-09 16:01:36.927 
Epoch 433/1000 
	 loss: 25.7578, MinusLogProbMetric: 25.7578, val_loss: 25.9015, val_MinusLogProbMetric: 25.9015

Epoch 433: val_loss improved from 25.93190 to 25.90148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 25.7578 - MinusLogProbMetric: 25.7578 - val_loss: 25.9015 - val_MinusLogProbMetric: 25.9015 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 434/1000
2023-10-09 16:02:31.406 
Epoch 434/1000 
	 loss: 25.7207, MinusLogProbMetric: 25.7207, val_loss: 25.7991, val_MinusLogProbMetric: 25.7991

Epoch 434: val_loss improved from 25.90148 to 25.79915, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 25.7207 - MinusLogProbMetric: 25.7207 - val_loss: 25.7991 - val_MinusLogProbMetric: 25.7991 - lr: 5.5556e-05 - 54s/epoch - 278ms/step
Epoch 435/1000
2023-10-09 16:03:25.212 
Epoch 435/1000 
	 loss: 25.6789, MinusLogProbMetric: 25.6789, val_loss: 25.8178, val_MinusLogProbMetric: 25.8178

Epoch 435: val_loss did not improve from 25.79915
196/196 - 53s - loss: 25.6789 - MinusLogProbMetric: 25.6789 - val_loss: 25.8178 - val_MinusLogProbMetric: 25.8178 - lr: 5.5556e-05 - 53s/epoch - 271ms/step
Epoch 436/1000
2023-10-09 16:04:18.701 
Epoch 436/1000 
	 loss: 25.6487, MinusLogProbMetric: 25.6487, val_loss: 25.7985, val_MinusLogProbMetric: 25.7985

Epoch 436: val_loss improved from 25.79915 to 25.79847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 25.6487 - MinusLogProbMetric: 25.6487 - val_loss: 25.7985 - val_MinusLogProbMetric: 25.7985 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 437/1000
2023-10-09 16:05:12.281 
Epoch 437/1000 
	 loss: 25.6369, MinusLogProbMetric: 25.6369, val_loss: 25.7736, val_MinusLogProbMetric: 25.7736

Epoch 437: val_loss improved from 25.79847 to 25.77363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 25.6369 - MinusLogProbMetric: 25.6369 - val_loss: 25.7736 - val_MinusLogProbMetric: 25.7736 - lr: 5.5556e-05 - 54s/epoch - 273ms/step
Epoch 438/1000
2023-10-09 16:06:06.390 
Epoch 438/1000 
	 loss: 25.6130, MinusLogProbMetric: 25.6130, val_loss: 25.7037, val_MinusLogProbMetric: 25.7037

Epoch 438: val_loss improved from 25.77363 to 25.70368, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 25.6130 - MinusLogProbMetric: 25.6130 - val_loss: 25.7037 - val_MinusLogProbMetric: 25.7037 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 439/1000
2023-10-09 16:07:00.895 
Epoch 439/1000 
	 loss: 25.5891, MinusLogProbMetric: 25.5891, val_loss: 25.7081, val_MinusLogProbMetric: 25.7081

Epoch 439: val_loss did not improve from 25.70368
196/196 - 54s - loss: 25.5891 - MinusLogProbMetric: 25.5891 - val_loss: 25.7081 - val_MinusLogProbMetric: 25.7081 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 440/1000
2023-10-09 16:07:53.945 
Epoch 440/1000 
	 loss: 25.5690, MinusLogProbMetric: 25.5690, val_loss: 25.8088, val_MinusLogProbMetric: 25.8088

Epoch 440: val_loss did not improve from 25.70368
196/196 - 53s - loss: 25.5690 - MinusLogProbMetric: 25.5690 - val_loss: 25.8088 - val_MinusLogProbMetric: 25.8088 - lr: 5.5556e-05 - 53s/epoch - 271ms/step
Epoch 441/1000
2023-10-09 16:08:48.166 
Epoch 441/1000 
	 loss: 25.5387, MinusLogProbMetric: 25.5387, val_loss: 25.6406, val_MinusLogProbMetric: 25.6406

Epoch 441: val_loss improved from 25.70368 to 25.64061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 25.5387 - MinusLogProbMetric: 25.5387 - val_loss: 25.6406 - val_MinusLogProbMetric: 25.6406 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 442/1000
2023-10-09 16:09:43.199 
Epoch 442/1000 
	 loss: 25.5141, MinusLogProbMetric: 25.5141, val_loss: 25.6656, val_MinusLogProbMetric: 25.6656

Epoch 442: val_loss did not improve from 25.64061
196/196 - 54s - loss: 25.5141 - MinusLogProbMetric: 25.5141 - val_loss: 25.6656 - val_MinusLogProbMetric: 25.6656 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 443/1000
2023-10-09 16:10:37.047 
Epoch 443/1000 
	 loss: 25.4929, MinusLogProbMetric: 25.4929, val_loss: 25.7596, val_MinusLogProbMetric: 25.7596

Epoch 443: val_loss did not improve from 25.64061
196/196 - 54s - loss: 25.4929 - MinusLogProbMetric: 25.4929 - val_loss: 25.7596 - val_MinusLogProbMetric: 25.7596 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 444/1000
2023-10-09 16:11:30.699 
Epoch 444/1000 
	 loss: 25.4783, MinusLogProbMetric: 25.4783, val_loss: 25.6231, val_MinusLogProbMetric: 25.6231

Epoch 444: val_loss improved from 25.64061 to 25.62307, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 25.4783 - MinusLogProbMetric: 25.4783 - val_loss: 25.6231 - val_MinusLogProbMetric: 25.6231 - lr: 5.5556e-05 - 54s/epoch - 278ms/step
Epoch 445/1000
2023-10-09 16:12:24.696 
Epoch 445/1000 
	 loss: 25.4652, MinusLogProbMetric: 25.4652, val_loss: 25.6757, val_MinusLogProbMetric: 25.6757

Epoch 445: val_loss did not improve from 25.62307
196/196 - 53s - loss: 25.4652 - MinusLogProbMetric: 25.4652 - val_loss: 25.6757 - val_MinusLogProbMetric: 25.6757 - lr: 5.5556e-05 - 53s/epoch - 271ms/step
Epoch 446/1000
2023-10-09 16:13:18.165 
Epoch 446/1000 
	 loss: 25.4531, MinusLogProbMetric: 25.4531, val_loss: 25.5565, val_MinusLogProbMetric: 25.5565

Epoch 446: val_loss improved from 25.62307 to 25.55646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 25.4531 - MinusLogProbMetric: 25.4531 - val_loss: 25.5565 - val_MinusLogProbMetric: 25.5565 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 447/1000
2023-10-09 16:14:12.524 
Epoch 447/1000 
	 loss: 25.4238, MinusLogProbMetric: 25.4238, val_loss: 25.5508, val_MinusLogProbMetric: 25.5508

Epoch 447: val_loss improved from 25.55646 to 25.55085, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 25.4238 - MinusLogProbMetric: 25.4238 - val_loss: 25.5508 - val_MinusLogProbMetric: 25.5508 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 448/1000
2023-10-09 16:15:07.152 
Epoch 448/1000 
	 loss: 25.4056, MinusLogProbMetric: 25.4056, val_loss: 25.5667, val_MinusLogProbMetric: 25.5667

Epoch 448: val_loss did not improve from 25.55085
196/196 - 54s - loss: 25.4056 - MinusLogProbMetric: 25.4056 - val_loss: 25.5667 - val_MinusLogProbMetric: 25.5667 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 449/1000
2023-10-09 16:16:01.467 
Epoch 449/1000 
	 loss: 25.3898, MinusLogProbMetric: 25.3898, val_loss: 25.5713, val_MinusLogProbMetric: 25.5713

Epoch 449: val_loss did not improve from 25.55085
196/196 - 54s - loss: 25.3898 - MinusLogProbMetric: 25.3898 - val_loss: 25.5713 - val_MinusLogProbMetric: 25.5713 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 450/1000
2023-10-09 16:16:55.204 
Epoch 450/1000 
	 loss: 25.3673, MinusLogProbMetric: 25.3673, val_loss: 25.5170, val_MinusLogProbMetric: 25.5170

Epoch 450: val_loss improved from 25.55085 to 25.51701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 25.3673 - MinusLogProbMetric: 25.3673 - val_loss: 25.5170 - val_MinusLogProbMetric: 25.5170 - lr: 5.5556e-05 - 55s/epoch - 278ms/step
Epoch 451/1000
2023-10-09 16:17:49.968 
Epoch 451/1000 
	 loss: 25.3534, MinusLogProbMetric: 25.3534, val_loss: 25.4690, val_MinusLogProbMetric: 25.4690

Epoch 451: val_loss improved from 25.51701 to 25.46900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 25.3534 - MinusLogProbMetric: 25.3534 - val_loss: 25.4690 - val_MinusLogProbMetric: 25.4690 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 452/1000
2023-10-09 16:18:44.369 
Epoch 452/1000 
	 loss: 25.3281, MinusLogProbMetric: 25.3281, val_loss: 25.4958, val_MinusLogProbMetric: 25.4958

Epoch 452: val_loss did not improve from 25.46900
196/196 - 54s - loss: 25.3281 - MinusLogProbMetric: 25.3281 - val_loss: 25.4958 - val_MinusLogProbMetric: 25.4958 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 453/1000
2023-10-09 16:19:38.420 
Epoch 453/1000 
	 loss: 25.3214, MinusLogProbMetric: 25.3214, val_loss: 25.4875, val_MinusLogProbMetric: 25.4875

Epoch 453: val_loss did not improve from 25.46900
196/196 - 54s - loss: 25.3214 - MinusLogProbMetric: 25.3214 - val_loss: 25.4875 - val_MinusLogProbMetric: 25.4875 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 454/1000
2023-10-09 16:20:31.756 
Epoch 454/1000 
	 loss: 25.2988, MinusLogProbMetric: 25.2988, val_loss: 25.4171, val_MinusLogProbMetric: 25.4171

Epoch 454: val_loss improved from 25.46900 to 25.41710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 25.2988 - MinusLogProbMetric: 25.2988 - val_loss: 25.4171 - val_MinusLogProbMetric: 25.4171 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 455/1000
2023-10-09 16:21:26.035 
Epoch 455/1000 
	 loss: 25.2892, MinusLogProbMetric: 25.2892, val_loss: 25.5334, val_MinusLogProbMetric: 25.5334

Epoch 455: val_loss did not improve from 25.41710
196/196 - 54s - loss: 25.2892 - MinusLogProbMetric: 25.2892 - val_loss: 25.5334 - val_MinusLogProbMetric: 25.5334 - lr: 5.5556e-05 - 54s/epoch - 273ms/step
Epoch 456/1000
2023-10-09 16:22:19.362 
Epoch 456/1000 
	 loss: 25.2666, MinusLogProbMetric: 25.2666, val_loss: 25.3639, val_MinusLogProbMetric: 25.3639

Epoch 456: val_loss improved from 25.41710 to 25.36388, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 25.2666 - MinusLogProbMetric: 25.2666 - val_loss: 25.3639 - val_MinusLogProbMetric: 25.3639 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 457/1000
2023-10-09 16:23:11.561 
Epoch 457/1000 
	 loss: 25.2376, MinusLogProbMetric: 25.2376, val_loss: 25.3995, val_MinusLogProbMetric: 25.3995

Epoch 457: val_loss did not improve from 25.36388
196/196 - 51s - loss: 25.2376 - MinusLogProbMetric: 25.2376 - val_loss: 25.3995 - val_MinusLogProbMetric: 25.3995 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 458/1000
2023-10-09 16:24:04.931 
Epoch 458/1000 
	 loss: 25.2335, MinusLogProbMetric: 25.2335, val_loss: 25.3645, val_MinusLogProbMetric: 25.3645

Epoch 458: val_loss did not improve from 25.36388
196/196 - 53s - loss: 25.2335 - MinusLogProbMetric: 25.2335 - val_loss: 25.3645 - val_MinusLogProbMetric: 25.3645 - lr: 5.5556e-05 - 53s/epoch - 272ms/step
Epoch 459/1000
2023-10-09 16:24:56.436 
Epoch 459/1000 
	 loss: 25.2240, MinusLogProbMetric: 25.2240, val_loss: 25.3750, val_MinusLogProbMetric: 25.3750

Epoch 459: val_loss did not improve from 25.36388
196/196 - 51s - loss: 25.2240 - MinusLogProbMetric: 25.2240 - val_loss: 25.3750 - val_MinusLogProbMetric: 25.3750 - lr: 5.5556e-05 - 51s/epoch - 263ms/step
Epoch 460/1000
2023-10-09 16:25:47.861 
Epoch 460/1000 
	 loss: 25.2041, MinusLogProbMetric: 25.2041, val_loss: 25.3412, val_MinusLogProbMetric: 25.3412

Epoch 460: val_loss improved from 25.36388 to 25.34118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 25.2041 - MinusLogProbMetric: 25.2041 - val_loss: 25.3412 - val_MinusLogProbMetric: 25.3412 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 461/1000
2023-10-09 16:26:39.192 
Epoch 461/1000 
	 loss: 25.1883, MinusLogProbMetric: 25.1883, val_loss: 25.4803, val_MinusLogProbMetric: 25.4803

Epoch 461: val_loss did not improve from 25.34118
196/196 - 50s - loss: 25.1883 - MinusLogProbMetric: 25.1883 - val_loss: 25.4803 - val_MinusLogProbMetric: 25.4803 - lr: 5.5556e-05 - 50s/epoch - 258ms/step
Epoch 462/1000
2023-10-09 16:27:28.803 
Epoch 462/1000 
	 loss: 25.1758, MinusLogProbMetric: 25.1758, val_loss: 25.2925, val_MinusLogProbMetric: 25.2925

Epoch 462: val_loss improved from 25.34118 to 25.29246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 25.1758 - MinusLogProbMetric: 25.1758 - val_loss: 25.2925 - val_MinusLogProbMetric: 25.2925 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 463/1000
2023-10-09 16:28:24.484 
Epoch 463/1000 
	 loss: 25.1679, MinusLogProbMetric: 25.1679, val_loss: 25.4787, val_MinusLogProbMetric: 25.4787

Epoch 463: val_loss did not improve from 25.29246
196/196 - 55s - loss: 25.1679 - MinusLogProbMetric: 25.1679 - val_loss: 25.4787 - val_MinusLogProbMetric: 25.4787 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 464/1000
2023-10-09 16:29:14.386 
Epoch 464/1000 
	 loss: 25.1392, MinusLogProbMetric: 25.1392, val_loss: 25.3622, val_MinusLogProbMetric: 25.3622

Epoch 464: val_loss did not improve from 25.29246
196/196 - 50s - loss: 25.1392 - MinusLogProbMetric: 25.1392 - val_loss: 25.3622 - val_MinusLogProbMetric: 25.3622 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 465/1000
2023-10-09 16:30:04.514 
Epoch 465/1000 
	 loss: 25.1407, MinusLogProbMetric: 25.1407, val_loss: 25.2857, val_MinusLogProbMetric: 25.2857

Epoch 465: val_loss improved from 25.29246 to 25.28574, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 25.1407 - MinusLogProbMetric: 25.1407 - val_loss: 25.2857 - val_MinusLogProbMetric: 25.2857 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 466/1000
2023-10-09 16:30:54.577 
Epoch 466/1000 
	 loss: 25.1286, MinusLogProbMetric: 25.1286, val_loss: 25.3155, val_MinusLogProbMetric: 25.3155

Epoch 466: val_loss did not improve from 25.28574
196/196 - 49s - loss: 25.1286 - MinusLogProbMetric: 25.1286 - val_loss: 25.3155 - val_MinusLogProbMetric: 25.3155 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 467/1000
2023-10-09 16:31:44.327 
Epoch 467/1000 
	 loss: 25.1007, MinusLogProbMetric: 25.1007, val_loss: 25.2680, val_MinusLogProbMetric: 25.2680

Epoch 467: val_loss improved from 25.28574 to 25.26804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 25.1007 - MinusLogProbMetric: 25.1007 - val_loss: 25.2680 - val_MinusLogProbMetric: 25.2680 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 468/1000
2023-10-09 16:32:33.767 
Epoch 468/1000 
	 loss: 25.0866, MinusLogProbMetric: 25.0866, val_loss: 25.2345, val_MinusLogProbMetric: 25.2345

Epoch 468: val_loss improved from 25.26804 to 25.23450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 25.0866 - MinusLogProbMetric: 25.0866 - val_loss: 25.2345 - val_MinusLogProbMetric: 25.2345 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 469/1000
2023-10-09 16:33:26.046 
Epoch 469/1000 
	 loss: 25.0707, MinusLogProbMetric: 25.0707, val_loss: 25.2213, val_MinusLogProbMetric: 25.2213

Epoch 469: val_loss improved from 25.23450 to 25.22132, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 25.0707 - MinusLogProbMetric: 25.0707 - val_loss: 25.2213 - val_MinusLogProbMetric: 25.2213 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 470/1000
2023-10-09 16:34:15.698 
Epoch 470/1000 
	 loss: 25.0614, MinusLogProbMetric: 25.0614, val_loss: 25.1916, val_MinusLogProbMetric: 25.1916

Epoch 470: val_loss improved from 25.22132 to 25.19164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 25.0614 - MinusLogProbMetric: 25.0614 - val_loss: 25.1916 - val_MinusLogProbMetric: 25.1916 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 471/1000
2023-10-09 16:35:04.289 
Epoch 471/1000 
	 loss: 25.0553, MinusLogProbMetric: 25.0553, val_loss: 25.1877, val_MinusLogProbMetric: 25.1877

Epoch 471: val_loss improved from 25.19164 to 25.18773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 25.0553 - MinusLogProbMetric: 25.0553 - val_loss: 25.1877 - val_MinusLogProbMetric: 25.1877 - lr: 5.5556e-05 - 49s/epoch - 248ms/step
Epoch 472/1000
2023-10-09 16:35:53.234 
Epoch 472/1000 
	 loss: 25.0335, MinusLogProbMetric: 25.0335, val_loss: 25.2013, val_MinusLogProbMetric: 25.2013

Epoch 472: val_loss did not improve from 25.18773
196/196 - 48s - loss: 25.0335 - MinusLogProbMetric: 25.0335 - val_loss: 25.2013 - val_MinusLogProbMetric: 25.2013 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 473/1000
2023-10-09 16:36:41.488 
Epoch 473/1000 
	 loss: 25.0137, MinusLogProbMetric: 25.0137, val_loss: 25.1415, val_MinusLogProbMetric: 25.1415

Epoch 473: val_loss improved from 25.18773 to 25.14148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 25.0137 - MinusLogProbMetric: 25.0137 - val_loss: 25.1415 - val_MinusLogProbMetric: 25.1415 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 474/1000
2023-10-09 16:37:31.199 
Epoch 474/1000 
	 loss: 24.9987, MinusLogProbMetric: 24.9987, val_loss: 25.1894, val_MinusLogProbMetric: 25.1894

Epoch 474: val_loss did not improve from 25.14148
196/196 - 49s - loss: 24.9987 - MinusLogProbMetric: 24.9987 - val_loss: 25.1894 - val_MinusLogProbMetric: 25.1894 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 475/1000
2023-10-09 16:38:20.573 
Epoch 475/1000 
	 loss: 24.9942, MinusLogProbMetric: 24.9942, val_loss: 25.1207, val_MinusLogProbMetric: 25.1207

Epoch 475: val_loss improved from 25.14148 to 25.12068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.9942 - MinusLogProbMetric: 24.9942 - val_loss: 25.1207 - val_MinusLogProbMetric: 25.1207 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 476/1000
2023-10-09 16:39:09.175 
Epoch 476/1000 
	 loss: 24.9898, MinusLogProbMetric: 24.9898, val_loss: 25.0772, val_MinusLogProbMetric: 25.0772

Epoch 476: val_loss improved from 25.12068 to 25.07723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 24.9898 - MinusLogProbMetric: 24.9898 - val_loss: 25.0772 - val_MinusLogProbMetric: 25.0772 - lr: 5.5556e-05 - 49s/epoch - 248ms/step
Epoch 477/1000
2023-10-09 16:39:58.876 
Epoch 477/1000 
	 loss: 24.9719, MinusLogProbMetric: 24.9719, val_loss: 25.1160, val_MinusLogProbMetric: 25.1160

Epoch 477: val_loss did not improve from 25.07723
196/196 - 49s - loss: 24.9719 - MinusLogProbMetric: 24.9719 - val_loss: 25.1160 - val_MinusLogProbMetric: 25.1160 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 478/1000
2023-10-09 16:40:47.915 
Epoch 478/1000 
	 loss: 24.9446, MinusLogProbMetric: 24.9446, val_loss: 25.0982, val_MinusLogProbMetric: 25.0982

Epoch 478: val_loss did not improve from 25.07723
196/196 - 49s - loss: 24.9446 - MinusLogProbMetric: 24.9446 - val_loss: 25.0982 - val_MinusLogProbMetric: 25.0982 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 479/1000
2023-10-09 16:41:36.397 
Epoch 479/1000 
	 loss: 24.9595, MinusLogProbMetric: 24.9595, val_loss: 25.0711, val_MinusLogProbMetric: 25.0711

Epoch 479: val_loss improved from 25.07723 to 25.07109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 24.9595 - MinusLogProbMetric: 24.9595 - val_loss: 25.0711 - val_MinusLogProbMetric: 25.0711 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 480/1000
2023-10-09 16:42:27.392 
Epoch 480/1000 
	 loss: 24.9347, MinusLogProbMetric: 24.9347, val_loss: 25.1159, val_MinusLogProbMetric: 25.1159

Epoch 480: val_loss did not improve from 25.07109
196/196 - 50s - loss: 24.9347 - MinusLogProbMetric: 24.9347 - val_loss: 25.1159 - val_MinusLogProbMetric: 25.1159 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 481/1000
2023-10-09 16:43:16.031 
Epoch 481/1000 
	 loss: 24.9224, MinusLogProbMetric: 24.9224, val_loss: 25.0455, val_MinusLogProbMetric: 25.0455

Epoch 481: val_loss improved from 25.07109 to 25.04548, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 24.9224 - MinusLogProbMetric: 24.9224 - val_loss: 25.0455 - val_MinusLogProbMetric: 25.0455 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 482/1000
2023-10-09 16:44:06.299 
Epoch 482/1000 
	 loss: 24.9080, MinusLogProbMetric: 24.9080, val_loss: 25.0547, val_MinusLogProbMetric: 25.0547

Epoch 482: val_loss did not improve from 25.04548
196/196 - 49s - loss: 24.9080 - MinusLogProbMetric: 24.9080 - val_loss: 25.0547 - val_MinusLogProbMetric: 25.0547 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 483/1000
2023-10-09 16:44:55.920 
Epoch 483/1000 
	 loss: 24.8891, MinusLogProbMetric: 24.8891, val_loss: 25.0106, val_MinusLogProbMetric: 25.0106

Epoch 483: val_loss improved from 25.04548 to 25.01064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.8891 - MinusLogProbMetric: 24.8891 - val_loss: 25.0106 - val_MinusLogProbMetric: 25.0106 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 484/1000
2023-10-09 16:45:45.161 
Epoch 484/1000 
	 loss: 24.8716, MinusLogProbMetric: 24.8716, val_loss: 24.9903, val_MinusLogProbMetric: 24.9903

Epoch 484: val_loss improved from 25.01064 to 24.99027, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 24.8716 - MinusLogProbMetric: 24.8716 - val_loss: 24.9903 - val_MinusLogProbMetric: 24.9903 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 485/1000
2023-10-09 16:46:34.153 
Epoch 485/1000 
	 loss: 24.8511, MinusLogProbMetric: 24.8511, val_loss: 25.0097, val_MinusLogProbMetric: 25.0097

Epoch 485: val_loss did not improve from 24.99027
196/196 - 48s - loss: 24.8511 - MinusLogProbMetric: 24.8511 - val_loss: 25.0097 - val_MinusLogProbMetric: 25.0097 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 486/1000
2023-10-09 16:47:25.476 
Epoch 486/1000 
	 loss: 24.8543, MinusLogProbMetric: 24.8543, val_loss: 24.9654, val_MinusLogProbMetric: 24.9654

Epoch 486: val_loss improved from 24.99027 to 24.96540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 24.8543 - MinusLogProbMetric: 24.8543 - val_loss: 24.9654 - val_MinusLogProbMetric: 24.9654 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 487/1000
2023-10-09 16:48:15.526 
Epoch 487/1000 
	 loss: 24.8464, MinusLogProbMetric: 24.8464, val_loss: 24.9989, val_MinusLogProbMetric: 24.9989

Epoch 487: val_loss did not improve from 24.96540
196/196 - 49s - loss: 24.8464 - MinusLogProbMetric: 24.8464 - val_loss: 24.9989 - val_MinusLogProbMetric: 24.9989 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 488/1000
2023-10-09 16:49:04.781 
Epoch 488/1000 
	 loss: 24.8376, MinusLogProbMetric: 24.8376, val_loss: 24.9819, val_MinusLogProbMetric: 24.9819

Epoch 488: val_loss did not improve from 24.96540
196/196 - 49s - loss: 24.8376 - MinusLogProbMetric: 24.8376 - val_loss: 24.9819 - val_MinusLogProbMetric: 24.9819 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 489/1000
2023-10-09 16:49:56.198 
Epoch 489/1000 
	 loss: 24.8319, MinusLogProbMetric: 24.8319, val_loss: 25.1329, val_MinusLogProbMetric: 25.1329

Epoch 489: val_loss did not improve from 24.96540
196/196 - 51s - loss: 24.8319 - MinusLogProbMetric: 24.8319 - val_loss: 25.1329 - val_MinusLogProbMetric: 25.1329 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 490/1000
2023-10-09 16:50:45.288 
Epoch 490/1000 
	 loss: 24.8153, MinusLogProbMetric: 24.8153, val_loss: 24.9079, val_MinusLogProbMetric: 24.9079

Epoch 490: val_loss improved from 24.96540 to 24.90792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.8153 - MinusLogProbMetric: 24.8153 - val_loss: 24.9079 - val_MinusLogProbMetric: 24.9079 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 491/1000
2023-10-09 16:51:34.987 
Epoch 491/1000 
	 loss: 24.7887, MinusLogProbMetric: 24.7887, val_loss: 24.9503, val_MinusLogProbMetric: 24.9503

Epoch 491: val_loss did not improve from 24.90792
196/196 - 49s - loss: 24.7887 - MinusLogProbMetric: 24.7887 - val_loss: 24.9503 - val_MinusLogProbMetric: 24.9503 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 492/1000
2023-10-09 16:52:23.268 
Epoch 492/1000 
	 loss: 24.7799, MinusLogProbMetric: 24.7799, val_loss: 25.0483, val_MinusLogProbMetric: 25.0483

Epoch 492: val_loss did not improve from 24.90792
196/196 - 48s - loss: 24.7799 - MinusLogProbMetric: 24.7799 - val_loss: 25.0483 - val_MinusLogProbMetric: 25.0483 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 493/1000
2023-10-09 16:53:12.243 
Epoch 493/1000 
	 loss: 24.7670, MinusLogProbMetric: 24.7670, val_loss: 24.9160, val_MinusLogProbMetric: 24.9160

Epoch 493: val_loss did not improve from 24.90792
196/196 - 49s - loss: 24.7670 - MinusLogProbMetric: 24.7670 - val_loss: 24.9160 - val_MinusLogProbMetric: 24.9160 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 494/1000
2023-10-09 16:54:00.825 
Epoch 494/1000 
	 loss: 24.7431, MinusLogProbMetric: 24.7431, val_loss: 24.9684, val_MinusLogProbMetric: 24.9684

Epoch 494: val_loss did not improve from 24.90792
196/196 - 49s - loss: 24.7431 - MinusLogProbMetric: 24.7431 - val_loss: 24.9684 - val_MinusLogProbMetric: 24.9684 - lr: 5.5556e-05 - 49s/epoch - 248ms/step
Epoch 495/1000
2023-10-09 16:54:49.508 
Epoch 495/1000 
	 loss: 24.7399, MinusLogProbMetric: 24.7399, val_loss: 25.1731, val_MinusLogProbMetric: 25.1731

Epoch 495: val_loss did not improve from 24.90792
196/196 - 49s - loss: 24.7399 - MinusLogProbMetric: 24.7399 - val_loss: 25.1731 - val_MinusLogProbMetric: 25.1731 - lr: 5.5556e-05 - 49s/epoch - 248ms/step
Epoch 496/1000
2023-10-09 16:55:39.455 
Epoch 496/1000 
	 loss: 24.7398, MinusLogProbMetric: 24.7398, val_loss: 24.8467, val_MinusLogProbMetric: 24.8467

Epoch 496: val_loss improved from 24.90792 to 24.84666, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 24.7398 - MinusLogProbMetric: 24.7398 - val_loss: 24.8467 - val_MinusLogProbMetric: 24.8467 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 497/1000
2023-10-09 16:56:29.306 
Epoch 497/1000 
	 loss: 24.7280, MinusLogProbMetric: 24.7280, val_loss: 24.9524, val_MinusLogProbMetric: 24.9524

Epoch 497: val_loss did not improve from 24.84666
196/196 - 49s - loss: 24.7280 - MinusLogProbMetric: 24.7280 - val_loss: 24.9524 - val_MinusLogProbMetric: 24.9524 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 498/1000
2023-10-09 16:57:18.916 
Epoch 498/1000 
	 loss: 24.7122, MinusLogProbMetric: 24.7122, val_loss: 24.8477, val_MinusLogProbMetric: 24.8477

Epoch 498: val_loss did not improve from 24.84666
196/196 - 50s - loss: 24.7122 - MinusLogProbMetric: 24.7122 - val_loss: 24.8477 - val_MinusLogProbMetric: 24.8477 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 499/1000
2023-10-09 16:58:11.464 
Epoch 499/1000 
	 loss: 24.6951, MinusLogProbMetric: 24.6951, val_loss: 24.9502, val_MinusLogProbMetric: 24.9502

Epoch 499: val_loss did not improve from 24.84666
196/196 - 53s - loss: 24.6951 - MinusLogProbMetric: 24.6951 - val_loss: 24.9502 - val_MinusLogProbMetric: 24.9502 - lr: 5.5556e-05 - 53s/epoch - 268ms/step
Epoch 500/1000
2023-10-09 16:59:00.922 
Epoch 500/1000 
	 loss: 24.6840, MinusLogProbMetric: 24.6840, val_loss: 24.9905, val_MinusLogProbMetric: 24.9905

Epoch 500: val_loss did not improve from 24.84666
196/196 - 49s - loss: 24.6840 - MinusLogProbMetric: 24.6840 - val_loss: 24.9905 - val_MinusLogProbMetric: 24.9905 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 501/1000
2023-10-09 16:59:51.633 
Epoch 501/1000 
	 loss: 24.6817, MinusLogProbMetric: 24.6817, val_loss: 24.8455, val_MinusLogProbMetric: 24.8455

Epoch 501: val_loss improved from 24.84666 to 24.84548, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 24.6817 - MinusLogProbMetric: 24.6817 - val_loss: 24.8455 - val_MinusLogProbMetric: 24.8455 - lr: 5.5556e-05 - 51s/epoch - 263ms/step
Epoch 502/1000
2023-10-09 17:00:41.043 
Epoch 502/1000 
	 loss: 24.6662, MinusLogProbMetric: 24.6662, val_loss: 24.7945, val_MinusLogProbMetric: 24.7945

Epoch 502: val_loss improved from 24.84548 to 24.79447, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 24.6662 - MinusLogProbMetric: 24.6662 - val_loss: 24.7945 - val_MinusLogProbMetric: 24.7945 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 503/1000
2023-10-09 17:01:29.743 
Epoch 503/1000 
	 loss: 24.6601, MinusLogProbMetric: 24.6601, val_loss: 24.8691, val_MinusLogProbMetric: 24.8691

Epoch 503: val_loss did not improve from 24.79447
196/196 - 48s - loss: 24.6601 - MinusLogProbMetric: 24.6601 - val_loss: 24.8691 - val_MinusLogProbMetric: 24.8691 - lr: 5.5556e-05 - 48s/epoch - 245ms/step
Epoch 504/1000
2023-10-09 17:02:21.308 
Epoch 504/1000 
	 loss: 24.6432, MinusLogProbMetric: 24.6432, val_loss: 25.0203, val_MinusLogProbMetric: 25.0203

Epoch 504: val_loss did not improve from 24.79447
196/196 - 52s - loss: 24.6432 - MinusLogProbMetric: 24.6432 - val_loss: 25.0203 - val_MinusLogProbMetric: 25.0203 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 505/1000
2023-10-09 17:03:08.426 
Epoch 505/1000 
	 loss: 24.6341, MinusLogProbMetric: 24.6341, val_loss: 24.7412, val_MinusLogProbMetric: 24.7412

Epoch 505: val_loss improved from 24.79447 to 24.74117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 48s - loss: 24.6341 - MinusLogProbMetric: 24.6341 - val_loss: 24.7412 - val_MinusLogProbMetric: 24.7412 - lr: 5.5556e-05 - 48s/epoch - 245ms/step
Epoch 506/1000
2023-10-09 17:04:00.827 
Epoch 506/1000 
	 loss: 24.6236, MinusLogProbMetric: 24.6236, val_loss: 24.7705, val_MinusLogProbMetric: 24.7705

Epoch 506: val_loss did not improve from 24.74117
196/196 - 51s - loss: 24.6236 - MinusLogProbMetric: 24.6236 - val_loss: 24.7705 - val_MinusLogProbMetric: 24.7705 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 507/1000
2023-10-09 17:04:51.095 
Epoch 507/1000 
	 loss: 24.6097, MinusLogProbMetric: 24.6097, val_loss: 24.7489, val_MinusLogProbMetric: 24.7489

Epoch 507: val_loss did not improve from 24.74117
196/196 - 50s - loss: 24.6097 - MinusLogProbMetric: 24.6097 - val_loss: 24.7489 - val_MinusLogProbMetric: 24.7489 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 508/1000
2023-10-09 17:05:39.312 
Epoch 508/1000 
	 loss: 24.5898, MinusLogProbMetric: 24.5898, val_loss: 24.8423, val_MinusLogProbMetric: 24.8423

Epoch 508: val_loss did not improve from 24.74117
196/196 - 48s - loss: 24.5898 - MinusLogProbMetric: 24.5898 - val_loss: 24.8423 - val_MinusLogProbMetric: 24.8423 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 509/1000
2023-10-09 17:06:26.604 
Epoch 509/1000 
	 loss: 24.5746, MinusLogProbMetric: 24.5746, val_loss: 24.7466, val_MinusLogProbMetric: 24.7466

Epoch 509: val_loss did not improve from 24.74117
196/196 - 47s - loss: 24.5746 - MinusLogProbMetric: 24.5746 - val_loss: 24.7466 - val_MinusLogProbMetric: 24.7466 - lr: 5.5556e-05 - 47s/epoch - 241ms/step
Epoch 510/1000
2023-10-09 17:07:13.592 
Epoch 510/1000 
	 loss: 24.5808, MinusLogProbMetric: 24.5808, val_loss: 24.6890, val_MinusLogProbMetric: 24.6890

Epoch 510: val_loss improved from 24.74117 to 24.68903, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 48s - loss: 24.5808 - MinusLogProbMetric: 24.5808 - val_loss: 24.6890 - val_MinusLogProbMetric: 24.6890 - lr: 5.5556e-05 - 48s/epoch - 244ms/step
Epoch 511/1000
2023-10-09 17:08:02.157 
Epoch 511/1000 
	 loss: 24.5723, MinusLogProbMetric: 24.5723, val_loss: 24.7380, val_MinusLogProbMetric: 24.7380

Epoch 511: val_loss did not improve from 24.68903
196/196 - 48s - loss: 24.5723 - MinusLogProbMetric: 24.5723 - val_loss: 24.7380 - val_MinusLogProbMetric: 24.7380 - lr: 5.5556e-05 - 48s/epoch - 243ms/step
Epoch 512/1000
2023-10-09 17:08:49.593 
Epoch 512/1000 
	 loss: 24.5427, MinusLogProbMetric: 24.5427, val_loss: 24.6966, val_MinusLogProbMetric: 24.6966

Epoch 512: val_loss did not improve from 24.68903
196/196 - 47s - loss: 24.5427 - MinusLogProbMetric: 24.5427 - val_loss: 24.6966 - val_MinusLogProbMetric: 24.6966 - lr: 5.5556e-05 - 47s/epoch - 242ms/step
Epoch 513/1000
2023-10-09 17:09:37.572 
Epoch 513/1000 
	 loss: 24.5351, MinusLogProbMetric: 24.5351, val_loss: 24.8105, val_MinusLogProbMetric: 24.8105

Epoch 513: val_loss did not improve from 24.68903
196/196 - 48s - loss: 24.5351 - MinusLogProbMetric: 24.5351 - val_loss: 24.8105 - val_MinusLogProbMetric: 24.8105 - lr: 5.5556e-05 - 48s/epoch - 245ms/step
Epoch 514/1000
2023-10-09 17:10:24.537 
Epoch 514/1000 
	 loss: 24.5305, MinusLogProbMetric: 24.5305, val_loss: 24.6296, val_MinusLogProbMetric: 24.6296

Epoch 514: val_loss improved from 24.68903 to 24.62964, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 48s - loss: 24.5305 - MinusLogProbMetric: 24.5305 - val_loss: 24.6296 - val_MinusLogProbMetric: 24.6296 - lr: 5.5556e-05 - 48s/epoch - 243ms/step
Epoch 515/1000
2023-10-09 17:11:12.031 
Epoch 515/1000 
	 loss: 24.5157, MinusLogProbMetric: 24.5157, val_loss: 24.6236, val_MinusLogProbMetric: 24.6236

Epoch 515: val_loss improved from 24.62964 to 24.62365, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 48s - loss: 24.5157 - MinusLogProbMetric: 24.5157 - val_loss: 24.6236 - val_MinusLogProbMetric: 24.6236 - lr: 5.5556e-05 - 48s/epoch - 243ms/step
Epoch 516/1000
2023-10-09 17:11:58.362 
Epoch 516/1000 
	 loss: 24.5145, MinusLogProbMetric: 24.5145, val_loss: 24.6248, val_MinusLogProbMetric: 24.6248

Epoch 516: val_loss did not improve from 24.62365
196/196 - 45s - loss: 24.5145 - MinusLogProbMetric: 24.5145 - val_loss: 24.6248 - val_MinusLogProbMetric: 24.6248 - lr: 5.5556e-05 - 45s/epoch - 232ms/step
Epoch 517/1000
2023-10-09 17:12:46.595 
Epoch 517/1000 
	 loss: 24.5109, MinusLogProbMetric: 24.5109, val_loss: 24.6320, val_MinusLogProbMetric: 24.6320

Epoch 517: val_loss did not improve from 24.62365
196/196 - 48s - loss: 24.5109 - MinusLogProbMetric: 24.5109 - val_loss: 24.6320 - val_MinusLogProbMetric: 24.6320 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 518/1000
2023-10-09 17:13:33.202 
Epoch 518/1000 
	 loss: 24.4709, MinusLogProbMetric: 24.4709, val_loss: 24.6169, val_MinusLogProbMetric: 24.6169

Epoch 518: val_loss improved from 24.62365 to 24.61689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 48s - loss: 24.4709 - MinusLogProbMetric: 24.4709 - val_loss: 24.6169 - val_MinusLogProbMetric: 24.6169 - lr: 5.5556e-05 - 48s/epoch - 243ms/step
Epoch 519/1000
2023-10-09 17:14:21.069 
Epoch 519/1000 
	 loss: 24.4693, MinusLogProbMetric: 24.4693, val_loss: 24.6816, val_MinusLogProbMetric: 24.6816

Epoch 519: val_loss did not improve from 24.61689
196/196 - 47s - loss: 24.4693 - MinusLogProbMetric: 24.4693 - val_loss: 24.6816 - val_MinusLogProbMetric: 24.6816 - lr: 5.5556e-05 - 47s/epoch - 239ms/step
Epoch 520/1000
2023-10-09 17:15:10.021 
Epoch 520/1000 
	 loss: 24.4719, MinusLogProbMetric: 24.4719, val_loss: 24.6112, val_MinusLogProbMetric: 24.6112

Epoch 520: val_loss improved from 24.61689 to 24.61124, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.4719 - MinusLogProbMetric: 24.4719 - val_loss: 24.6112 - val_MinusLogProbMetric: 24.6112 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 521/1000
2023-10-09 17:15:58.994 
Epoch 521/1000 
	 loss: 24.4587, MinusLogProbMetric: 24.4587, val_loss: 24.6407, val_MinusLogProbMetric: 24.6407

Epoch 521: val_loss did not improve from 24.61124
196/196 - 48s - loss: 24.4587 - MinusLogProbMetric: 24.4587 - val_loss: 24.6407 - val_MinusLogProbMetric: 24.6407 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 522/1000
2023-10-09 17:16:47.200 
Epoch 522/1000 
	 loss: 24.4360, MinusLogProbMetric: 24.4360, val_loss: 24.5673, val_MinusLogProbMetric: 24.5673

Epoch 522: val_loss improved from 24.61124 to 24.56732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 24.4360 - MinusLogProbMetric: 24.4360 - val_loss: 24.5673 - val_MinusLogProbMetric: 24.5673 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 523/1000
2023-10-09 17:17:36.286 
Epoch 523/1000 
	 loss: 24.4366, MinusLogProbMetric: 24.4366, val_loss: 24.7039, val_MinusLogProbMetric: 24.7039

Epoch 523: val_loss did not improve from 24.56732
196/196 - 48s - loss: 24.4366 - MinusLogProbMetric: 24.4366 - val_loss: 24.7039 - val_MinusLogProbMetric: 24.7039 - lr: 5.5556e-05 - 48s/epoch - 247ms/step
Epoch 524/1000
2023-10-09 17:18:24.091 
Epoch 524/1000 
	 loss: 24.4259, MinusLogProbMetric: 24.4259, val_loss: 24.6301, val_MinusLogProbMetric: 24.6301

Epoch 524: val_loss did not improve from 24.56732
196/196 - 48s - loss: 24.4259 - MinusLogProbMetric: 24.4259 - val_loss: 24.6301 - val_MinusLogProbMetric: 24.6301 - lr: 5.5556e-05 - 48s/epoch - 244ms/step
Epoch 525/1000
2023-10-09 17:19:12.084 
Epoch 525/1000 
	 loss: 24.4200, MinusLogProbMetric: 24.4200, val_loss: 24.5617, val_MinusLogProbMetric: 24.5617

Epoch 525: val_loss improved from 24.56732 to 24.56174, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 24.4200 - MinusLogProbMetric: 24.4200 - val_loss: 24.5617 - val_MinusLogProbMetric: 24.5617 - lr: 5.5556e-05 - 49s/epoch - 248ms/step
Epoch 526/1000
2023-10-09 17:20:00.822 
Epoch 526/1000 
	 loss: 24.3950, MinusLogProbMetric: 24.3950, val_loss: 24.5258, val_MinusLogProbMetric: 24.5258

Epoch 526: val_loss improved from 24.56174 to 24.52578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 24.3950 - MinusLogProbMetric: 24.3950 - val_loss: 24.5258 - val_MinusLogProbMetric: 24.5258 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 527/1000
2023-10-09 17:20:49.923 
Epoch 527/1000 
	 loss: 24.3780, MinusLogProbMetric: 24.3780, val_loss: 24.4978, val_MinusLogProbMetric: 24.4978

Epoch 527: val_loss improved from 24.52578 to 24.49779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 24.3780 - MinusLogProbMetric: 24.3780 - val_loss: 24.4978 - val_MinusLogProbMetric: 24.4978 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 528/1000
2023-10-09 17:21:39.835 
Epoch 528/1000 
	 loss: 24.3931, MinusLogProbMetric: 24.3931, val_loss: 24.4954, val_MinusLogProbMetric: 24.4954

Epoch 528: val_loss improved from 24.49779 to 24.49540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.3931 - MinusLogProbMetric: 24.3931 - val_loss: 24.4954 - val_MinusLogProbMetric: 24.4954 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 529/1000
2023-10-09 17:22:29.730 
Epoch 529/1000 
	 loss: 24.3772, MinusLogProbMetric: 24.3772, val_loss: 24.4775, val_MinusLogProbMetric: 24.4775

Epoch 529: val_loss improved from 24.49540 to 24.47753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.3772 - MinusLogProbMetric: 24.3772 - val_loss: 24.4775 - val_MinusLogProbMetric: 24.4775 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 530/1000
2023-10-09 17:23:21.555 
Epoch 530/1000 
	 loss: 24.3678, MinusLogProbMetric: 24.3678, val_loss: 24.4816, val_MinusLogProbMetric: 24.4816

Epoch 530: val_loss did not improve from 24.47753
196/196 - 51s - loss: 24.3678 - MinusLogProbMetric: 24.3678 - val_loss: 24.4816 - val_MinusLogProbMetric: 24.4816 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 531/1000
2023-10-09 17:24:09.171 
Epoch 531/1000 
	 loss: 24.3539, MinusLogProbMetric: 24.3539, val_loss: 24.5388, val_MinusLogProbMetric: 24.5388

Epoch 531: val_loss did not improve from 24.47753
196/196 - 48s - loss: 24.3539 - MinusLogProbMetric: 24.3539 - val_loss: 24.5388 - val_MinusLogProbMetric: 24.5388 - lr: 5.5556e-05 - 48s/epoch - 243ms/step
Epoch 532/1000
2023-10-09 17:25:01.925 
Epoch 532/1000 
	 loss: 24.3312, MinusLogProbMetric: 24.3312, val_loss: 24.4658, val_MinusLogProbMetric: 24.4658

Epoch 532: val_loss improved from 24.47753 to 24.46580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 24.3312 - MinusLogProbMetric: 24.3312 - val_loss: 24.4658 - val_MinusLogProbMetric: 24.4658 - lr: 5.5556e-05 - 53s/epoch - 273ms/step
Epoch 533/1000
2023-10-09 17:25:52.734 
Epoch 533/1000 
	 loss: 24.3200, MinusLogProbMetric: 24.3200, val_loss: 24.5385, val_MinusLogProbMetric: 24.5385

Epoch 533: val_loss did not improve from 24.46580
196/196 - 50s - loss: 24.3200 - MinusLogProbMetric: 24.3200 - val_loss: 24.5385 - val_MinusLogProbMetric: 24.5385 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 534/1000
2023-10-09 17:26:41.790 
Epoch 534/1000 
	 loss: 24.3191, MinusLogProbMetric: 24.3191, val_loss: 24.4614, val_MinusLogProbMetric: 24.4614

Epoch 534: val_loss improved from 24.46580 to 24.46135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.3191 - MinusLogProbMetric: 24.3191 - val_loss: 24.4614 - val_MinusLogProbMetric: 24.4614 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 535/1000
2023-10-09 17:27:31.521 
Epoch 535/1000 
	 loss: 24.3125, MinusLogProbMetric: 24.3125, val_loss: 24.4467, val_MinusLogProbMetric: 24.4467

Epoch 535: val_loss improved from 24.46135 to 24.44671, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.3125 - MinusLogProbMetric: 24.3125 - val_loss: 24.4467 - val_MinusLogProbMetric: 24.4467 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 536/1000
2023-10-09 17:28:22.008 
Epoch 536/1000 
	 loss: 24.2979, MinusLogProbMetric: 24.2979, val_loss: 24.4317, val_MinusLogProbMetric: 24.4317

Epoch 536: val_loss improved from 24.44671 to 24.43167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 24.2979 - MinusLogProbMetric: 24.2979 - val_loss: 24.4317 - val_MinusLogProbMetric: 24.4317 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 537/1000
2023-10-09 17:29:11.263 
Epoch 537/1000 
	 loss: 24.2818, MinusLogProbMetric: 24.2818, val_loss: 24.4296, val_MinusLogProbMetric: 24.4296

Epoch 537: val_loss improved from 24.43167 to 24.42963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 24.2818 - MinusLogProbMetric: 24.2818 - val_loss: 24.4296 - val_MinusLogProbMetric: 24.4296 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 538/1000
2023-10-09 17:30:02.705 
Epoch 538/1000 
	 loss: 24.2700, MinusLogProbMetric: 24.2700, val_loss: 24.5026, val_MinusLogProbMetric: 24.5026

Epoch 538: val_loss did not improve from 24.42963
196/196 - 51s - loss: 24.2700 - MinusLogProbMetric: 24.2700 - val_loss: 24.5026 - val_MinusLogProbMetric: 24.5026 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 539/1000
2023-10-09 17:30:51.621 
Epoch 539/1000 
	 loss: 24.2578, MinusLogProbMetric: 24.2578, val_loss: 24.4545, val_MinusLogProbMetric: 24.4545

Epoch 539: val_loss did not improve from 24.42963
196/196 - 49s - loss: 24.2578 - MinusLogProbMetric: 24.2578 - val_loss: 24.4545 - val_MinusLogProbMetric: 24.4545 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 540/1000
2023-10-09 17:31:40.339 
Epoch 540/1000 
	 loss: 24.2533, MinusLogProbMetric: 24.2533, val_loss: 24.3779, val_MinusLogProbMetric: 24.3779

Epoch 540: val_loss improved from 24.42963 to 24.37794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.2533 - MinusLogProbMetric: 24.2533 - val_loss: 24.3779 - val_MinusLogProbMetric: 24.3779 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 541/1000
2023-10-09 17:32:30.304 
Epoch 541/1000 
	 loss: 24.2298, MinusLogProbMetric: 24.2298, val_loss: 24.3298, val_MinusLogProbMetric: 24.3298

Epoch 541: val_loss improved from 24.37794 to 24.32975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.2298 - MinusLogProbMetric: 24.2298 - val_loss: 24.3298 - val_MinusLogProbMetric: 24.3298 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 542/1000
2023-10-09 17:33:19.982 
Epoch 542/1000 
	 loss: 24.2311, MinusLogProbMetric: 24.2311, val_loss: 24.4185, val_MinusLogProbMetric: 24.4185

Epoch 542: val_loss did not improve from 24.32975
196/196 - 49s - loss: 24.2311 - MinusLogProbMetric: 24.2311 - val_loss: 24.4185 - val_MinusLogProbMetric: 24.4185 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 543/1000
2023-10-09 17:34:10.326 
Epoch 543/1000 
	 loss: 24.2182, MinusLogProbMetric: 24.2182, val_loss: 24.4560, val_MinusLogProbMetric: 24.4560

Epoch 543: val_loss did not improve from 24.32975
196/196 - 50s - loss: 24.2182 - MinusLogProbMetric: 24.2182 - val_loss: 24.4560 - val_MinusLogProbMetric: 24.4560 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 544/1000
2023-10-09 17:34:59.039 
Epoch 544/1000 
	 loss: 24.2137, MinusLogProbMetric: 24.2137, val_loss: 24.3727, val_MinusLogProbMetric: 24.3727

Epoch 544: val_loss did not improve from 24.32975
196/196 - 49s - loss: 24.2137 - MinusLogProbMetric: 24.2137 - val_loss: 24.3727 - val_MinusLogProbMetric: 24.3727 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 545/1000
2023-10-09 17:35:47.891 
Epoch 545/1000 
	 loss: 24.2083, MinusLogProbMetric: 24.2083, val_loss: 24.4392, val_MinusLogProbMetric: 24.4392

Epoch 545: val_loss did not improve from 24.32975
196/196 - 49s - loss: 24.2083 - MinusLogProbMetric: 24.2083 - val_loss: 24.4392 - val_MinusLogProbMetric: 24.4392 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 546/1000
2023-10-09 17:36:36.447 
Epoch 546/1000 
	 loss: 24.2083, MinusLogProbMetric: 24.2083, val_loss: 24.3360, val_MinusLogProbMetric: 24.3360

Epoch 546: val_loss did not improve from 24.32975
196/196 - 49s - loss: 24.2083 - MinusLogProbMetric: 24.2083 - val_loss: 24.3360 - val_MinusLogProbMetric: 24.3360 - lr: 5.5556e-05 - 49s/epoch - 248ms/step
Epoch 547/1000
2023-10-09 17:37:24.953 
Epoch 547/1000 
	 loss: 24.1855, MinusLogProbMetric: 24.1855, val_loss: 24.3672, val_MinusLogProbMetric: 24.3672

Epoch 547: val_loss did not improve from 24.32975
196/196 - 49s - loss: 24.1855 - MinusLogProbMetric: 24.1855 - val_loss: 24.3672 - val_MinusLogProbMetric: 24.3672 - lr: 5.5556e-05 - 49s/epoch - 247ms/step
Epoch 548/1000
2023-10-09 17:38:13.930 
Epoch 548/1000 
	 loss: 24.1616, MinusLogProbMetric: 24.1616, val_loss: 24.2589, val_MinusLogProbMetric: 24.2589

Epoch 548: val_loss improved from 24.32975 to 24.25888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.1616 - MinusLogProbMetric: 24.1616 - val_loss: 24.2589 - val_MinusLogProbMetric: 24.2589 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 549/1000
2023-10-09 17:39:03.244 
Epoch 549/1000 
	 loss: 24.1891, MinusLogProbMetric: 24.1891, val_loss: 24.2985, val_MinusLogProbMetric: 24.2985

Epoch 549: val_loss did not improve from 24.25888
196/196 - 48s - loss: 24.1891 - MinusLogProbMetric: 24.1891 - val_loss: 24.2985 - val_MinusLogProbMetric: 24.2985 - lr: 5.5556e-05 - 48s/epoch - 247ms/step
Epoch 550/1000
2023-10-09 17:39:51.741 
Epoch 550/1000 
	 loss: 24.1605, MinusLogProbMetric: 24.1605, val_loss: 24.3422, val_MinusLogProbMetric: 24.3422

Epoch 550: val_loss did not improve from 24.25888
196/196 - 48s - loss: 24.1605 - MinusLogProbMetric: 24.1605 - val_loss: 24.3422 - val_MinusLogProbMetric: 24.3422 - lr: 5.5556e-05 - 48s/epoch - 247ms/step
Epoch 551/1000
2023-10-09 17:40:39.939 
Epoch 551/1000 
	 loss: 24.1517, MinusLogProbMetric: 24.1517, val_loss: 24.3803, val_MinusLogProbMetric: 24.3803

Epoch 551: val_loss did not improve from 24.25888
196/196 - 48s - loss: 24.1517 - MinusLogProbMetric: 24.1517 - val_loss: 24.3803 - val_MinusLogProbMetric: 24.3803 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 552/1000
2023-10-09 17:41:29.559 
Epoch 552/1000 
	 loss: 24.1413, MinusLogProbMetric: 24.1413, val_loss: 24.2467, val_MinusLogProbMetric: 24.2467

Epoch 552: val_loss improved from 24.25888 to 24.24670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 24.1413 - MinusLogProbMetric: 24.1413 - val_loss: 24.2467 - val_MinusLogProbMetric: 24.2467 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 553/1000
2023-10-09 17:42:19.325 
Epoch 553/1000 
	 loss: 24.1264, MinusLogProbMetric: 24.1264, val_loss: 24.2577, val_MinusLogProbMetric: 24.2577

Epoch 553: val_loss did not improve from 24.24670
196/196 - 49s - loss: 24.1264 - MinusLogProbMetric: 24.1264 - val_loss: 24.2577 - val_MinusLogProbMetric: 24.2577 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 554/1000
2023-10-09 17:43:08.575 
Epoch 554/1000 
	 loss: 24.1057, MinusLogProbMetric: 24.1057, val_loss: 24.2489, val_MinusLogProbMetric: 24.2489

Epoch 554: val_loss did not improve from 24.24670
196/196 - 49s - loss: 24.1057 - MinusLogProbMetric: 24.1057 - val_loss: 24.2489 - val_MinusLogProbMetric: 24.2489 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 555/1000
2023-10-09 17:43:57.422 
Epoch 555/1000 
	 loss: 24.1371, MinusLogProbMetric: 24.1371, val_loss: 24.2100, val_MinusLogProbMetric: 24.2100

Epoch 555: val_loss improved from 24.24670 to 24.20998, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.1371 - MinusLogProbMetric: 24.1371 - val_loss: 24.2100 - val_MinusLogProbMetric: 24.2100 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 556/1000
2023-10-09 17:44:47.234 
Epoch 556/1000 
	 loss: 24.1264, MinusLogProbMetric: 24.1264, val_loss: 24.2086, val_MinusLogProbMetric: 24.2086

Epoch 556: val_loss improved from 24.20998 to 24.20859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.1264 - MinusLogProbMetric: 24.1264 - val_loss: 24.2086 - val_MinusLogProbMetric: 24.2086 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 557/1000
2023-10-09 17:45:36.768 
Epoch 557/1000 
	 loss: 24.0920, MinusLogProbMetric: 24.0920, val_loss: 24.3445, val_MinusLogProbMetric: 24.3445

Epoch 557: val_loss did not improve from 24.20859
196/196 - 49s - loss: 24.0920 - MinusLogProbMetric: 24.0920 - val_loss: 24.3445 - val_MinusLogProbMetric: 24.3445 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 558/1000
2023-10-09 17:46:25.307 
Epoch 558/1000 
	 loss: 24.0737, MinusLogProbMetric: 24.0737, val_loss: 24.2474, val_MinusLogProbMetric: 24.2474

Epoch 558: val_loss did not improve from 24.20859
196/196 - 49s - loss: 24.0737 - MinusLogProbMetric: 24.0737 - val_loss: 24.2474 - val_MinusLogProbMetric: 24.2474 - lr: 5.5556e-05 - 49s/epoch - 248ms/step
Epoch 559/1000
2023-10-09 17:47:14.052 
Epoch 559/1000 
	 loss: 24.0781, MinusLogProbMetric: 24.0781, val_loss: 24.1886, val_MinusLogProbMetric: 24.1886

Epoch 559: val_loss improved from 24.20859 to 24.18863, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.0781 - MinusLogProbMetric: 24.0781 - val_loss: 24.1886 - val_MinusLogProbMetric: 24.1886 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 560/1000
2023-10-09 17:48:03.917 
Epoch 560/1000 
	 loss: 24.0786, MinusLogProbMetric: 24.0786, val_loss: 24.2261, val_MinusLogProbMetric: 24.2261

Epoch 560: val_loss did not improve from 24.18863
196/196 - 49s - loss: 24.0786 - MinusLogProbMetric: 24.0786 - val_loss: 24.2261 - val_MinusLogProbMetric: 24.2261 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 561/1000
2023-10-09 17:48:53.267 
Epoch 561/1000 
	 loss: 24.0613, MinusLogProbMetric: 24.0613, val_loss: 24.1801, val_MinusLogProbMetric: 24.1801

Epoch 561: val_loss improved from 24.18863 to 24.18015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 24.0613 - MinusLogProbMetric: 24.0613 - val_loss: 24.1801 - val_MinusLogProbMetric: 24.1801 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 562/1000
2023-10-09 17:49:44.051 
Epoch 562/1000 
	 loss: 24.0442, MinusLogProbMetric: 24.0442, val_loss: 24.2288, val_MinusLogProbMetric: 24.2288

Epoch 562: val_loss did not improve from 24.18015
196/196 - 50s - loss: 24.0442 - MinusLogProbMetric: 24.0442 - val_loss: 24.2288 - val_MinusLogProbMetric: 24.2288 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 563/1000
2023-10-09 17:50:33.175 
Epoch 563/1000 
	 loss: 24.0584, MinusLogProbMetric: 24.0584, val_loss: 24.4249, val_MinusLogProbMetric: 24.4249

Epoch 563: val_loss did not improve from 24.18015
196/196 - 49s - loss: 24.0584 - MinusLogProbMetric: 24.0584 - val_loss: 24.4249 - val_MinusLogProbMetric: 24.4249 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 564/1000
2023-10-09 17:51:21.597 
Epoch 564/1000 
	 loss: 24.0229, MinusLogProbMetric: 24.0229, val_loss: 24.4742, val_MinusLogProbMetric: 24.4742

Epoch 564: val_loss did not improve from 24.18015
196/196 - 48s - loss: 24.0229 - MinusLogProbMetric: 24.0229 - val_loss: 24.4742 - val_MinusLogProbMetric: 24.4742 - lr: 5.5556e-05 - 48s/epoch - 247ms/step
Epoch 565/1000
2023-10-09 17:52:10.132 
Epoch 565/1000 
	 loss: 24.0374, MinusLogProbMetric: 24.0374, val_loss: 24.2273, val_MinusLogProbMetric: 24.2273

Epoch 565: val_loss did not improve from 24.18015
196/196 - 49s - loss: 24.0374 - MinusLogProbMetric: 24.0374 - val_loss: 24.2273 - val_MinusLogProbMetric: 24.2273 - lr: 5.5556e-05 - 49s/epoch - 248ms/step
Epoch 566/1000
2023-10-09 17:53:01.742 
Epoch 566/1000 
	 loss: 24.0157, MinusLogProbMetric: 24.0157, val_loss: 24.1001, val_MinusLogProbMetric: 24.1001

Epoch 566: val_loss improved from 24.18015 to 24.10007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 24.0157 - MinusLogProbMetric: 24.0157 - val_loss: 24.1001 - val_MinusLogProbMetric: 24.1001 - lr: 5.5556e-05 - 52s/epoch - 268ms/step
Epoch 567/1000
2023-10-09 17:53:51.624 
Epoch 567/1000 
	 loss: 24.0298, MinusLogProbMetric: 24.0298, val_loss: 24.1335, val_MinusLogProbMetric: 24.1335

Epoch 567: val_loss did not improve from 24.10007
196/196 - 49s - loss: 24.0298 - MinusLogProbMetric: 24.0298 - val_loss: 24.1335 - val_MinusLogProbMetric: 24.1335 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 568/1000
2023-10-09 17:54:40.296 
Epoch 568/1000 
	 loss: 23.9837, MinusLogProbMetric: 23.9837, val_loss: 24.1999, val_MinusLogProbMetric: 24.1999

Epoch 568: val_loss did not improve from 24.10007
196/196 - 49s - loss: 23.9837 - MinusLogProbMetric: 23.9837 - val_loss: 24.1999 - val_MinusLogProbMetric: 24.1999 - lr: 5.5556e-05 - 49s/epoch - 248ms/step
Epoch 569/1000
2023-10-09 17:55:28.602 
Epoch 569/1000 
	 loss: 23.9884, MinusLogProbMetric: 23.9884, val_loss: 24.1980, val_MinusLogProbMetric: 24.1980

Epoch 569: val_loss did not improve from 24.10007
196/196 - 48s - loss: 23.9884 - MinusLogProbMetric: 23.9884 - val_loss: 24.1980 - val_MinusLogProbMetric: 24.1980 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 570/1000
2023-10-09 17:56:17.114 
Epoch 570/1000 
	 loss: 23.9869, MinusLogProbMetric: 23.9869, val_loss: 24.1088, val_MinusLogProbMetric: 24.1088

Epoch 570: val_loss did not improve from 24.10007
196/196 - 49s - loss: 23.9869 - MinusLogProbMetric: 23.9869 - val_loss: 24.1088 - val_MinusLogProbMetric: 24.1088 - lr: 5.5556e-05 - 49s/epoch - 248ms/step
Epoch 571/1000
2023-10-09 17:57:06.377 
Epoch 571/1000 
	 loss: 23.9501, MinusLogProbMetric: 23.9501, val_loss: 24.1203, val_MinusLogProbMetric: 24.1203

Epoch 571: val_loss did not improve from 24.10007
196/196 - 49s - loss: 23.9501 - MinusLogProbMetric: 23.9501 - val_loss: 24.1203 - val_MinusLogProbMetric: 24.1203 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 572/1000
2023-10-09 17:57:55.994 
Epoch 572/1000 
	 loss: 23.9410, MinusLogProbMetric: 23.9410, val_loss: 24.1761, val_MinusLogProbMetric: 24.1761

Epoch 572: val_loss did not improve from 24.10007
196/196 - 50s - loss: 23.9410 - MinusLogProbMetric: 23.9410 - val_loss: 24.1761 - val_MinusLogProbMetric: 24.1761 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 573/1000
2023-10-09 17:58:46.551 
Epoch 573/1000 
	 loss: 23.9610, MinusLogProbMetric: 23.9610, val_loss: 24.2044, val_MinusLogProbMetric: 24.2044

Epoch 573: val_loss did not improve from 24.10007
196/196 - 51s - loss: 23.9610 - MinusLogProbMetric: 23.9610 - val_loss: 24.2044 - val_MinusLogProbMetric: 24.2044 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 574/1000
2023-10-09 17:59:36.032 
Epoch 574/1000 
	 loss: 23.9292, MinusLogProbMetric: 23.9292, val_loss: 24.1503, val_MinusLogProbMetric: 24.1503

Epoch 574: val_loss did not improve from 24.10007
196/196 - 49s - loss: 23.9292 - MinusLogProbMetric: 23.9292 - val_loss: 24.1503 - val_MinusLogProbMetric: 24.1503 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 575/1000
2023-10-09 18:00:26.627 
Epoch 575/1000 
	 loss: 23.9392, MinusLogProbMetric: 23.9392, val_loss: 24.0588, val_MinusLogProbMetric: 24.0588

Epoch 575: val_loss improved from 24.10007 to 24.05882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 23.9392 - MinusLogProbMetric: 23.9392 - val_loss: 24.0588 - val_MinusLogProbMetric: 24.0588 - lr: 5.5556e-05 - 51s/epoch - 263ms/step
Epoch 576/1000
2023-10-09 18:01:17.183 
Epoch 576/1000 
	 loss: 23.9137, MinusLogProbMetric: 23.9137, val_loss: 24.1847, val_MinusLogProbMetric: 24.1847

Epoch 576: val_loss did not improve from 24.05882
196/196 - 50s - loss: 23.9137 - MinusLogProbMetric: 23.9137 - val_loss: 24.1847 - val_MinusLogProbMetric: 24.1847 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 577/1000
2023-10-09 18:02:07.515 
Epoch 577/1000 
	 loss: 23.9202, MinusLogProbMetric: 23.9202, val_loss: 24.0133, val_MinusLogProbMetric: 24.0133

Epoch 577: val_loss improved from 24.05882 to 24.01332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 23.9202 - MinusLogProbMetric: 23.9202 - val_loss: 24.0133 - val_MinusLogProbMetric: 24.0133 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 578/1000
2023-10-09 18:02:57.423 
Epoch 578/1000 
	 loss: 23.8795, MinusLogProbMetric: 23.8795, val_loss: 24.0045, val_MinusLogProbMetric: 24.0045

Epoch 578: val_loss improved from 24.01332 to 24.00453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 23.8795 - MinusLogProbMetric: 23.8795 - val_loss: 24.0045 - val_MinusLogProbMetric: 24.0045 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 579/1000
2023-10-09 18:03:48.636 
Epoch 579/1000 
	 loss: 23.8902, MinusLogProbMetric: 23.8902, val_loss: 24.0599, val_MinusLogProbMetric: 24.0599

Epoch 579: val_loss did not improve from 24.00453
196/196 - 50s - loss: 23.8902 - MinusLogProbMetric: 23.8902 - val_loss: 24.0599 - val_MinusLogProbMetric: 24.0599 - lr: 5.5556e-05 - 50s/epoch - 258ms/step
Epoch 580/1000
2023-10-09 18:04:37.667 
Epoch 580/1000 
	 loss: 23.8852, MinusLogProbMetric: 23.8852, val_loss: 24.0587, val_MinusLogProbMetric: 24.0587

Epoch 580: val_loss did not improve from 24.00453
196/196 - 49s - loss: 23.8852 - MinusLogProbMetric: 23.8852 - val_loss: 24.0587 - val_MinusLogProbMetric: 24.0587 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 581/1000
2023-10-09 18:05:25.875 
Epoch 581/1000 
	 loss: 23.8792, MinusLogProbMetric: 23.8792, val_loss: 23.9716, val_MinusLogProbMetric: 23.9716

Epoch 581: val_loss improved from 24.00453 to 23.97158, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 23.8792 - MinusLogProbMetric: 23.8792 - val_loss: 23.9716 - val_MinusLogProbMetric: 23.9716 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 582/1000
2023-10-09 18:06:14.864 
Epoch 582/1000 
	 loss: 23.8755, MinusLogProbMetric: 23.8755, val_loss: 24.1273, val_MinusLogProbMetric: 24.1273

Epoch 582: val_loss did not improve from 23.97158
196/196 - 48s - loss: 23.8755 - MinusLogProbMetric: 23.8755 - val_loss: 24.1273 - val_MinusLogProbMetric: 24.1273 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 583/1000
2023-10-09 18:07:04.417 
Epoch 583/1000 
	 loss: 23.8804, MinusLogProbMetric: 23.8804, val_loss: 24.0034, val_MinusLogProbMetric: 24.0034

Epoch 583: val_loss did not improve from 23.97158
196/196 - 50s - loss: 23.8804 - MinusLogProbMetric: 23.8804 - val_loss: 24.0034 - val_MinusLogProbMetric: 24.0034 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 584/1000
2023-10-09 18:07:54.407 
Epoch 584/1000 
	 loss: 23.8483, MinusLogProbMetric: 23.8483, val_loss: 24.0388, val_MinusLogProbMetric: 24.0388

Epoch 584: val_loss did not improve from 23.97158
196/196 - 50s - loss: 23.8483 - MinusLogProbMetric: 23.8483 - val_loss: 24.0388 - val_MinusLogProbMetric: 24.0388 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 585/1000
2023-10-09 18:08:43.326 
Epoch 585/1000 
	 loss: 23.8232, MinusLogProbMetric: 23.8232, val_loss: 23.9846, val_MinusLogProbMetric: 23.9846

Epoch 585: val_loss did not improve from 23.97158
196/196 - 49s - loss: 23.8232 - MinusLogProbMetric: 23.8232 - val_loss: 23.9846 - val_MinusLogProbMetric: 23.9846 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 586/1000
2023-10-09 18:09:31.669 
Epoch 586/1000 
	 loss: 23.8384, MinusLogProbMetric: 23.8384, val_loss: 23.9141, val_MinusLogProbMetric: 23.9141

Epoch 586: val_loss improved from 23.97158 to 23.91406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 23.8384 - MinusLogProbMetric: 23.8384 - val_loss: 23.9141 - val_MinusLogProbMetric: 23.9141 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 587/1000
2023-10-09 18:10:22.437 
Epoch 587/1000 
	 loss: 23.8218, MinusLogProbMetric: 23.8218, val_loss: 24.0330, val_MinusLogProbMetric: 24.0330

Epoch 587: val_loss did not improve from 23.91406
196/196 - 50s - loss: 23.8218 - MinusLogProbMetric: 23.8218 - val_loss: 24.0330 - val_MinusLogProbMetric: 24.0330 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 588/1000
2023-10-09 18:11:13.029 
Epoch 588/1000 
	 loss: 23.8121, MinusLogProbMetric: 23.8121, val_loss: 23.9764, val_MinusLogProbMetric: 23.9764

Epoch 588: val_loss did not improve from 23.91406
196/196 - 51s - loss: 23.8121 - MinusLogProbMetric: 23.8121 - val_loss: 23.9764 - val_MinusLogProbMetric: 23.9764 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 589/1000
2023-10-09 18:12:02.058 
Epoch 589/1000 
	 loss: 23.8102, MinusLogProbMetric: 23.8102, val_loss: 23.9425, val_MinusLogProbMetric: 23.9425

Epoch 589: val_loss did not improve from 23.91406
196/196 - 49s - loss: 23.8102 - MinusLogProbMetric: 23.8102 - val_loss: 23.9425 - val_MinusLogProbMetric: 23.9425 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 590/1000
2023-10-09 18:12:51.139 
Epoch 590/1000 
	 loss: 23.7989, MinusLogProbMetric: 23.7989, val_loss: 24.0852, val_MinusLogProbMetric: 24.0852

Epoch 590: val_loss did not improve from 23.91406
196/196 - 49s - loss: 23.7989 - MinusLogProbMetric: 23.7989 - val_loss: 24.0852 - val_MinusLogProbMetric: 24.0852 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 591/1000
2023-10-09 18:13:39.198 
Epoch 591/1000 
	 loss: 23.7912, MinusLogProbMetric: 23.7912, val_loss: 24.0281, val_MinusLogProbMetric: 24.0281

Epoch 591: val_loss did not improve from 23.91406
196/196 - 48s - loss: 23.7912 - MinusLogProbMetric: 23.7912 - val_loss: 24.0281 - val_MinusLogProbMetric: 24.0281 - lr: 5.5556e-05 - 48s/epoch - 245ms/step
Epoch 592/1000
2023-10-09 18:14:27.579 
Epoch 592/1000 
	 loss: 23.8021, MinusLogProbMetric: 23.8021, val_loss: 23.9509, val_MinusLogProbMetric: 23.9509

Epoch 592: val_loss did not improve from 23.91406
196/196 - 48s - loss: 23.8021 - MinusLogProbMetric: 23.8021 - val_loss: 23.9509 - val_MinusLogProbMetric: 23.9509 - lr: 5.5556e-05 - 48s/epoch - 247ms/step
Epoch 593/1000
2023-10-09 18:15:16.625 
Epoch 593/1000 
	 loss: 23.7883, MinusLogProbMetric: 23.7883, val_loss: 23.8835, val_MinusLogProbMetric: 23.8835

Epoch 593: val_loss improved from 23.91406 to 23.88352, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 23.7883 - MinusLogProbMetric: 23.7883 - val_loss: 23.8835 - val_MinusLogProbMetric: 23.8835 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 594/1000
2023-10-09 18:16:06.102 
Epoch 594/1000 
	 loss: 23.7683, MinusLogProbMetric: 23.7683, val_loss: 23.8564, val_MinusLogProbMetric: 23.8564

Epoch 594: val_loss improved from 23.88352 to 23.85642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 23.7683 - MinusLogProbMetric: 23.7683 - val_loss: 23.8564 - val_MinusLogProbMetric: 23.8564 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 595/1000
2023-10-09 18:16:55.556 
Epoch 595/1000 
	 loss: 23.7502, MinusLogProbMetric: 23.7502, val_loss: 23.9720, val_MinusLogProbMetric: 23.9720

Epoch 595: val_loss did not improve from 23.85642
196/196 - 49s - loss: 23.7502 - MinusLogProbMetric: 23.7502 - val_loss: 23.9720 - val_MinusLogProbMetric: 23.9720 - lr: 5.5556e-05 - 49s/epoch - 248ms/step
Epoch 596/1000
2023-10-09 18:17:46.703 
Epoch 596/1000 
	 loss: 23.7753, MinusLogProbMetric: 23.7753, val_loss: 24.0371, val_MinusLogProbMetric: 24.0371

Epoch 596: val_loss did not improve from 23.85642
196/196 - 51s - loss: 23.7753 - MinusLogProbMetric: 23.7753 - val_loss: 24.0371 - val_MinusLogProbMetric: 24.0371 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 597/1000
2023-10-09 18:18:35.547 
Epoch 597/1000 
	 loss: 23.7322, MinusLogProbMetric: 23.7322, val_loss: 23.8272, val_MinusLogProbMetric: 23.8272

Epoch 597: val_loss improved from 23.85642 to 23.82717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 23.7322 - MinusLogProbMetric: 23.7322 - val_loss: 23.8272 - val_MinusLogProbMetric: 23.8272 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 598/1000
2023-10-09 18:19:26.903 
Epoch 598/1000 
	 loss: 23.7268, MinusLogProbMetric: 23.7268, val_loss: 23.9055, val_MinusLogProbMetric: 23.9055

Epoch 598: val_loss did not improve from 23.82717
196/196 - 51s - loss: 23.7268 - MinusLogProbMetric: 23.7268 - val_loss: 23.9055 - val_MinusLogProbMetric: 23.9055 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 599/1000
2023-10-09 18:20:16.656 
Epoch 599/1000 
	 loss: 23.7201, MinusLogProbMetric: 23.7201, val_loss: 23.8284, val_MinusLogProbMetric: 23.8284

Epoch 599: val_loss did not improve from 23.82717
196/196 - 50s - loss: 23.7201 - MinusLogProbMetric: 23.7201 - val_loss: 23.8284 - val_MinusLogProbMetric: 23.8284 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 600/1000
2023-10-09 18:21:06.581 
Epoch 600/1000 
	 loss: 23.7266, MinusLogProbMetric: 23.7266, val_loss: 23.8463, val_MinusLogProbMetric: 23.8463

Epoch 600: val_loss did not improve from 23.82717
196/196 - 50s - loss: 23.7266 - MinusLogProbMetric: 23.7266 - val_loss: 23.8463 - val_MinusLogProbMetric: 23.8463 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 601/1000
2023-10-09 18:21:55.565 
Epoch 601/1000 
	 loss: 23.6975, MinusLogProbMetric: 23.6975, val_loss: 23.8521, val_MinusLogProbMetric: 23.8521

Epoch 601: val_loss did not improve from 23.82717
196/196 - 49s - loss: 23.6975 - MinusLogProbMetric: 23.6975 - val_loss: 23.8521 - val_MinusLogProbMetric: 23.8521 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 602/1000
2023-10-09 18:22:45.643 
Epoch 602/1000 
	 loss: 23.7097, MinusLogProbMetric: 23.7097, val_loss: 23.7756, val_MinusLogProbMetric: 23.7756

Epoch 602: val_loss improved from 23.82717 to 23.77562, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 23.7097 - MinusLogProbMetric: 23.7097 - val_loss: 23.7756 - val_MinusLogProbMetric: 23.7756 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 603/1000
2023-10-09 18:23:36.882 
Epoch 603/1000 
	 loss: 23.7135, MinusLogProbMetric: 23.7135, val_loss: 23.9167, val_MinusLogProbMetric: 23.9167

Epoch 603: val_loss did not improve from 23.77562
196/196 - 50s - loss: 23.7135 - MinusLogProbMetric: 23.7135 - val_loss: 23.9167 - val_MinusLogProbMetric: 23.9167 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 604/1000
2023-10-09 18:24:24.977 
Epoch 604/1000 
	 loss: 23.6799, MinusLogProbMetric: 23.6799, val_loss: 23.8440, val_MinusLogProbMetric: 23.8440

Epoch 604: val_loss did not improve from 23.77562
196/196 - 48s - loss: 23.6799 - MinusLogProbMetric: 23.6799 - val_loss: 23.8440 - val_MinusLogProbMetric: 23.8440 - lr: 5.5556e-05 - 48s/epoch - 245ms/step
Epoch 605/1000
2023-10-09 18:25:14.275 
Epoch 605/1000 
	 loss: 23.6644, MinusLogProbMetric: 23.6644, val_loss: 23.8829, val_MinusLogProbMetric: 23.8829

Epoch 605: val_loss did not improve from 23.77562
196/196 - 49s - loss: 23.6644 - MinusLogProbMetric: 23.6644 - val_loss: 23.8829 - val_MinusLogProbMetric: 23.8829 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 606/1000
2023-10-09 18:26:06.381 
Epoch 606/1000 
	 loss: 23.6666, MinusLogProbMetric: 23.6666, val_loss: 23.8505, val_MinusLogProbMetric: 23.8505

Epoch 606: val_loss did not improve from 23.77562
196/196 - 52s - loss: 23.6666 - MinusLogProbMetric: 23.6666 - val_loss: 23.8505 - val_MinusLogProbMetric: 23.8505 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 607/1000
2023-10-09 18:26:56.569 
Epoch 607/1000 
	 loss: 23.6405, MinusLogProbMetric: 23.6405, val_loss: 23.7778, val_MinusLogProbMetric: 23.7778

Epoch 607: val_loss did not improve from 23.77562
196/196 - 50s - loss: 23.6405 - MinusLogProbMetric: 23.6405 - val_loss: 23.7778 - val_MinusLogProbMetric: 23.7778 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 608/1000
2023-10-09 18:27:45.260 
Epoch 608/1000 
	 loss: 23.6583, MinusLogProbMetric: 23.6583, val_loss: 23.7651, val_MinusLogProbMetric: 23.7651

Epoch 608: val_loss improved from 23.77562 to 23.76506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 23.6583 - MinusLogProbMetric: 23.6583 - val_loss: 23.7651 - val_MinusLogProbMetric: 23.7651 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 609/1000
2023-10-09 18:28:36.983 
Epoch 609/1000 
	 loss: 23.6364, MinusLogProbMetric: 23.6364, val_loss: 23.7849, val_MinusLogProbMetric: 23.7849

Epoch 609: val_loss did not improve from 23.76506
196/196 - 51s - loss: 23.6364 - MinusLogProbMetric: 23.6364 - val_loss: 23.7849 - val_MinusLogProbMetric: 23.7849 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 610/1000
2023-10-09 18:29:27.329 
Epoch 610/1000 
	 loss: 23.6411, MinusLogProbMetric: 23.6411, val_loss: 23.7566, val_MinusLogProbMetric: 23.7566

Epoch 610: val_loss improved from 23.76506 to 23.75658, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 23.6411 - MinusLogProbMetric: 23.6411 - val_loss: 23.7566 - val_MinusLogProbMetric: 23.7566 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 611/1000
2023-10-09 18:30:16.966 
Epoch 611/1000 
	 loss: 23.6256, MinusLogProbMetric: 23.6256, val_loss: 23.8326, val_MinusLogProbMetric: 23.8326

Epoch 611: val_loss did not improve from 23.75658
196/196 - 49s - loss: 23.6256 - MinusLogProbMetric: 23.6256 - val_loss: 23.8326 - val_MinusLogProbMetric: 23.8326 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 612/1000
2023-10-09 18:31:06.302 
Epoch 612/1000 
	 loss: 23.6182, MinusLogProbMetric: 23.6182, val_loss: 23.8225, val_MinusLogProbMetric: 23.8225

Epoch 612: val_loss did not improve from 23.75658
196/196 - 49s - loss: 23.6182 - MinusLogProbMetric: 23.6182 - val_loss: 23.8225 - val_MinusLogProbMetric: 23.8225 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 613/1000
2023-10-09 18:31:57.426 
Epoch 613/1000 
	 loss: 23.6248, MinusLogProbMetric: 23.6248, val_loss: 23.7257, val_MinusLogProbMetric: 23.7257

Epoch 613: val_loss improved from 23.75658 to 23.72570, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 23.6248 - MinusLogProbMetric: 23.6248 - val_loss: 23.7257 - val_MinusLogProbMetric: 23.7257 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 614/1000
2023-10-09 18:32:49.576 
Epoch 614/1000 
	 loss: 23.6215, MinusLogProbMetric: 23.6215, val_loss: 23.8152, val_MinusLogProbMetric: 23.8152

Epoch 614: val_loss did not improve from 23.72570
196/196 - 51s - loss: 23.6215 - MinusLogProbMetric: 23.6215 - val_loss: 23.8152 - val_MinusLogProbMetric: 23.8152 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 615/1000
2023-10-09 18:33:40.737 
Epoch 615/1000 
	 loss: 23.5981, MinusLogProbMetric: 23.5981, val_loss: 23.8334, val_MinusLogProbMetric: 23.8334

Epoch 615: val_loss did not improve from 23.72570
196/196 - 51s - loss: 23.5981 - MinusLogProbMetric: 23.5981 - val_loss: 23.8334 - val_MinusLogProbMetric: 23.8334 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 616/1000
2023-10-09 18:34:31.200 
Epoch 616/1000 
	 loss: 23.6040, MinusLogProbMetric: 23.6040, val_loss: 23.9866, val_MinusLogProbMetric: 23.9866

Epoch 616: val_loss did not improve from 23.72570
196/196 - 50s - loss: 23.6040 - MinusLogProbMetric: 23.6040 - val_loss: 23.9866 - val_MinusLogProbMetric: 23.9866 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 617/1000
2023-10-09 18:35:22.873 
Epoch 617/1000 
	 loss: 23.5864, MinusLogProbMetric: 23.5864, val_loss: 23.6794, val_MinusLogProbMetric: 23.6794

Epoch 617: val_loss improved from 23.72570 to 23.67937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 23.5864 - MinusLogProbMetric: 23.5864 - val_loss: 23.6794 - val_MinusLogProbMetric: 23.6794 - lr: 5.5556e-05 - 52s/epoch - 268ms/step
Epoch 618/1000
2023-10-09 18:36:12.760 
Epoch 618/1000 
	 loss: 23.5743, MinusLogProbMetric: 23.5743, val_loss: 23.6549, val_MinusLogProbMetric: 23.6549

Epoch 618: val_loss improved from 23.67937 to 23.65490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 23.5743 - MinusLogProbMetric: 23.5743 - val_loss: 23.6549 - val_MinusLogProbMetric: 23.6549 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 619/1000
2023-10-09 18:37:06.685 
Epoch 619/1000 
	 loss: 23.5646, MinusLogProbMetric: 23.5646, val_loss: 23.7530, val_MinusLogProbMetric: 23.7530

Epoch 619: val_loss did not improve from 23.65490
196/196 - 53s - loss: 23.5646 - MinusLogProbMetric: 23.5646 - val_loss: 23.7530 - val_MinusLogProbMetric: 23.7530 - lr: 5.5556e-05 - 53s/epoch - 271ms/step
Epoch 620/1000
2023-10-09 18:37:56.652 
Epoch 620/1000 
	 loss: 23.5523, MinusLogProbMetric: 23.5523, val_loss: 23.7214, val_MinusLogProbMetric: 23.7214

Epoch 620: val_loss did not improve from 23.65490
196/196 - 50s - loss: 23.5523 - MinusLogProbMetric: 23.5523 - val_loss: 23.7214 - val_MinusLogProbMetric: 23.7214 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 621/1000
2023-10-09 18:38:48.858 
Epoch 621/1000 
	 loss: 23.5462, MinusLogProbMetric: 23.5462, val_loss: 23.6626, val_MinusLogProbMetric: 23.6626

Epoch 621: val_loss did not improve from 23.65490
196/196 - 52s - loss: 23.5462 - MinusLogProbMetric: 23.5462 - val_loss: 23.6626 - val_MinusLogProbMetric: 23.6626 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 622/1000
2023-10-09 18:39:40.464 
Epoch 622/1000 
	 loss: 23.5332, MinusLogProbMetric: 23.5332, val_loss: 23.6573, val_MinusLogProbMetric: 23.6573

Epoch 622: val_loss did not improve from 23.65490
196/196 - 52s - loss: 23.5332 - MinusLogProbMetric: 23.5332 - val_loss: 23.6573 - val_MinusLogProbMetric: 23.6573 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 623/1000
2023-10-09 18:40:31.533 
Epoch 623/1000 
	 loss: 23.5368, MinusLogProbMetric: 23.5368, val_loss: 23.6730, val_MinusLogProbMetric: 23.6730

Epoch 623: val_loss did not improve from 23.65490
196/196 - 51s - loss: 23.5368 - MinusLogProbMetric: 23.5368 - val_loss: 23.6730 - val_MinusLogProbMetric: 23.6730 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 624/1000
2023-10-09 18:41:22.068 
Epoch 624/1000 
	 loss: 23.5086, MinusLogProbMetric: 23.5086, val_loss: 23.8689, val_MinusLogProbMetric: 23.8689

Epoch 624: val_loss did not improve from 23.65490
196/196 - 51s - loss: 23.5086 - MinusLogProbMetric: 23.5086 - val_loss: 23.8689 - val_MinusLogProbMetric: 23.8689 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 625/1000
2023-10-09 18:42:13.361 
Epoch 625/1000 
	 loss: 23.5119, MinusLogProbMetric: 23.5119, val_loss: 23.6304, val_MinusLogProbMetric: 23.6304

Epoch 625: val_loss improved from 23.65490 to 23.63036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 23.5119 - MinusLogProbMetric: 23.5119 - val_loss: 23.6304 - val_MinusLogProbMetric: 23.6304 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 626/1000
2023-10-09 18:43:05.129 
Epoch 626/1000 
	 loss: 23.5088, MinusLogProbMetric: 23.5088, val_loss: 23.5973, val_MinusLogProbMetric: 23.5973

Epoch 626: val_loss improved from 23.63036 to 23.59726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 23.5088 - MinusLogProbMetric: 23.5088 - val_loss: 23.5973 - val_MinusLogProbMetric: 23.5973 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 627/1000
2023-10-09 18:43:57.846 
Epoch 627/1000 
	 loss: 23.5023, MinusLogProbMetric: 23.5023, val_loss: 23.6181, val_MinusLogProbMetric: 23.6181

Epoch 627: val_loss did not improve from 23.59726
196/196 - 52s - loss: 23.5023 - MinusLogProbMetric: 23.5023 - val_loss: 23.6181 - val_MinusLogProbMetric: 23.6181 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 628/1000
2023-10-09 18:44:51.475 
Epoch 628/1000 
	 loss: 23.5049, MinusLogProbMetric: 23.5049, val_loss: 23.8038, val_MinusLogProbMetric: 23.8038

Epoch 628: val_loss did not improve from 23.59726
196/196 - 54s - loss: 23.5049 - MinusLogProbMetric: 23.5049 - val_loss: 23.8038 - val_MinusLogProbMetric: 23.8038 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 629/1000
2023-10-09 18:45:41.212 
Epoch 629/1000 
	 loss: 23.5035, MinusLogProbMetric: 23.5035, val_loss: 23.6229, val_MinusLogProbMetric: 23.6229

Epoch 629: val_loss did not improve from 23.59726
196/196 - 50s - loss: 23.5035 - MinusLogProbMetric: 23.5035 - val_loss: 23.6229 - val_MinusLogProbMetric: 23.6229 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 630/1000
2023-10-09 18:46:32.582 
Epoch 630/1000 
	 loss: 23.5150, MinusLogProbMetric: 23.5150, val_loss: 23.6027, val_MinusLogProbMetric: 23.6027

Epoch 630: val_loss did not improve from 23.59726
196/196 - 51s - loss: 23.5150 - MinusLogProbMetric: 23.5150 - val_loss: 23.6027 - val_MinusLogProbMetric: 23.6027 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 631/1000
2023-10-09 18:47:24.572 
Epoch 631/1000 
	 loss: 23.4570, MinusLogProbMetric: 23.4570, val_loss: 23.6052, val_MinusLogProbMetric: 23.6052

Epoch 631: val_loss did not improve from 23.59726
196/196 - 52s - loss: 23.4570 - MinusLogProbMetric: 23.4570 - val_loss: 23.6052 - val_MinusLogProbMetric: 23.6052 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 632/1000
2023-10-09 18:48:14.883 
Epoch 632/1000 
	 loss: 23.4558, MinusLogProbMetric: 23.4558, val_loss: 23.5711, val_MinusLogProbMetric: 23.5711

Epoch 632: val_loss improved from 23.59726 to 23.57108, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 23.4558 - MinusLogProbMetric: 23.4558 - val_loss: 23.5711 - val_MinusLogProbMetric: 23.5711 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 633/1000
2023-10-09 18:49:05.004 
Epoch 633/1000 
	 loss: 23.4459, MinusLogProbMetric: 23.4459, val_loss: 23.5863, val_MinusLogProbMetric: 23.5863

Epoch 633: val_loss did not improve from 23.57108
196/196 - 49s - loss: 23.4459 - MinusLogProbMetric: 23.4459 - val_loss: 23.5863 - val_MinusLogProbMetric: 23.5863 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 634/1000
2023-10-09 18:49:58.342 
Epoch 634/1000 
	 loss: 23.4434, MinusLogProbMetric: 23.4434, val_loss: 23.5579, val_MinusLogProbMetric: 23.5579

Epoch 634: val_loss improved from 23.57108 to 23.55788, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 23.4434 - MinusLogProbMetric: 23.4434 - val_loss: 23.5579 - val_MinusLogProbMetric: 23.5579 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 635/1000
2023-10-09 18:50:50.938 
Epoch 635/1000 
	 loss: 23.4251, MinusLogProbMetric: 23.4251, val_loss: 23.6390, val_MinusLogProbMetric: 23.6390

Epoch 635: val_loss did not improve from 23.55788
196/196 - 52s - loss: 23.4251 - MinusLogProbMetric: 23.4251 - val_loss: 23.6390 - val_MinusLogProbMetric: 23.6390 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 636/1000
2023-10-09 18:51:41.742 
Epoch 636/1000 
	 loss: 23.4471, MinusLogProbMetric: 23.4471, val_loss: 23.6470, val_MinusLogProbMetric: 23.6470

Epoch 636: val_loss did not improve from 23.55788
196/196 - 51s - loss: 23.4471 - MinusLogProbMetric: 23.4471 - val_loss: 23.6470 - val_MinusLogProbMetric: 23.6470 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 637/1000
2023-10-09 18:52:30.876 
Epoch 637/1000 
	 loss: 23.4138, MinusLogProbMetric: 23.4138, val_loss: 23.5869, val_MinusLogProbMetric: 23.5869

Epoch 637: val_loss did not improve from 23.55788
196/196 - 49s - loss: 23.4138 - MinusLogProbMetric: 23.4138 - val_loss: 23.5869 - val_MinusLogProbMetric: 23.5869 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 638/1000
2023-10-09 18:53:20.528 
Epoch 638/1000 
	 loss: 23.4256, MinusLogProbMetric: 23.4256, val_loss: 23.5704, val_MinusLogProbMetric: 23.5704

Epoch 638: val_loss did not improve from 23.55788
196/196 - 50s - loss: 23.4256 - MinusLogProbMetric: 23.4256 - val_loss: 23.5704 - val_MinusLogProbMetric: 23.5704 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 639/1000
2023-10-09 18:54:14.472 
Epoch 639/1000 
	 loss: 23.4025, MinusLogProbMetric: 23.4025, val_loss: 23.5684, val_MinusLogProbMetric: 23.5684

Epoch 639: val_loss did not improve from 23.55788
196/196 - 54s - loss: 23.4025 - MinusLogProbMetric: 23.4025 - val_loss: 23.5684 - val_MinusLogProbMetric: 23.5684 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 640/1000
2023-10-09 18:55:08.642 
Epoch 640/1000 
	 loss: 23.4122, MinusLogProbMetric: 23.4122, val_loss: 23.5084, val_MinusLogProbMetric: 23.5084

Epoch 640: val_loss improved from 23.55788 to 23.50839, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 55s - loss: 23.4122 - MinusLogProbMetric: 23.4122 - val_loss: 23.5084 - val_MinusLogProbMetric: 23.5084 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 641/1000
2023-10-09 18:55:59.878 
Epoch 641/1000 
	 loss: 23.3963, MinusLogProbMetric: 23.3963, val_loss: 23.6649, val_MinusLogProbMetric: 23.6649

Epoch 641: val_loss did not improve from 23.50839
196/196 - 50s - loss: 23.3963 - MinusLogProbMetric: 23.3963 - val_loss: 23.6649 - val_MinusLogProbMetric: 23.6649 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 642/1000
2023-10-09 18:56:50.914 
Epoch 642/1000 
	 loss: 23.3851, MinusLogProbMetric: 23.3851, val_loss: 23.5326, val_MinusLogProbMetric: 23.5326

Epoch 642: val_loss did not improve from 23.50839
196/196 - 51s - loss: 23.3851 - MinusLogProbMetric: 23.3851 - val_loss: 23.5326 - val_MinusLogProbMetric: 23.5326 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 643/1000
2023-10-09 18:57:42.873 
Epoch 643/1000 
	 loss: 23.3703, MinusLogProbMetric: 23.3703, val_loss: 23.5071, val_MinusLogProbMetric: 23.5071

Epoch 643: val_loss improved from 23.50839 to 23.50710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 23.3703 - MinusLogProbMetric: 23.3703 - val_loss: 23.5071 - val_MinusLogProbMetric: 23.5071 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 644/1000
2023-10-09 18:58:36.642 
Epoch 644/1000 
	 loss: 23.3959, MinusLogProbMetric: 23.3959, val_loss: 23.5954, val_MinusLogProbMetric: 23.5954

Epoch 644: val_loss did not improve from 23.50710
196/196 - 53s - loss: 23.3959 - MinusLogProbMetric: 23.3959 - val_loss: 23.5954 - val_MinusLogProbMetric: 23.5954 - lr: 5.5556e-05 - 53s/epoch - 270ms/step
Epoch 645/1000
2023-10-09 18:59:28.476 
Epoch 645/1000 
	 loss: 23.3912, MinusLogProbMetric: 23.3912, val_loss: 23.4932, val_MinusLogProbMetric: 23.4932

Epoch 645: val_loss improved from 23.50710 to 23.49316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 23.3912 - MinusLogProbMetric: 23.3912 - val_loss: 23.4932 - val_MinusLogProbMetric: 23.4932 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 646/1000
2023-10-09 19:00:21.191 
Epoch 646/1000 
	 loss: 23.3543, MinusLogProbMetric: 23.3543, val_loss: 23.4687, val_MinusLogProbMetric: 23.4687

Epoch 646: val_loss improved from 23.49316 to 23.46870, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 23.3543 - MinusLogProbMetric: 23.3543 - val_loss: 23.4687 - val_MinusLogProbMetric: 23.4687 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 647/1000
2023-10-09 19:01:12.894 
Epoch 647/1000 
	 loss: 23.3578, MinusLogProbMetric: 23.3578, val_loss: 23.4728, val_MinusLogProbMetric: 23.4728

Epoch 647: val_loss did not improve from 23.46870
196/196 - 51s - loss: 23.3578 - MinusLogProbMetric: 23.3578 - val_loss: 23.4728 - val_MinusLogProbMetric: 23.4728 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 648/1000
2023-10-09 19:02:03.068 
Epoch 648/1000 
	 loss: 23.3536, MinusLogProbMetric: 23.3536, val_loss: 23.5085, val_MinusLogProbMetric: 23.5085

Epoch 648: val_loss did not improve from 23.46870
196/196 - 50s - loss: 23.3536 - MinusLogProbMetric: 23.3536 - val_loss: 23.5085 - val_MinusLogProbMetric: 23.5085 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 649/1000
2023-10-09 19:02:52.912 
Epoch 649/1000 
	 loss: 23.3480, MinusLogProbMetric: 23.3480, val_loss: 23.5326, val_MinusLogProbMetric: 23.5326

Epoch 649: val_loss did not improve from 23.46870
196/196 - 50s - loss: 23.3480 - MinusLogProbMetric: 23.3480 - val_loss: 23.5326 - val_MinusLogProbMetric: 23.5326 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 650/1000
2023-10-09 19:03:42.103 
Epoch 650/1000 
	 loss: 23.3336, MinusLogProbMetric: 23.3336, val_loss: 23.4279, val_MinusLogProbMetric: 23.4279

Epoch 650: val_loss improved from 23.46870 to 23.42791, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 23.3336 - MinusLogProbMetric: 23.3336 - val_loss: 23.4279 - val_MinusLogProbMetric: 23.4279 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 651/1000
2023-10-09 19:04:35.967 
Epoch 651/1000 
	 loss: 23.3443, MinusLogProbMetric: 23.3443, val_loss: 23.4502, val_MinusLogProbMetric: 23.4502

Epoch 651: val_loss did not improve from 23.42791
196/196 - 53s - loss: 23.3443 - MinusLogProbMetric: 23.3443 - val_loss: 23.4502 - val_MinusLogProbMetric: 23.4502 - lr: 5.5556e-05 - 53s/epoch - 270ms/step
Epoch 652/1000
2023-10-09 19:05:27.561 
Epoch 652/1000 
	 loss: 23.3137, MinusLogProbMetric: 23.3137, val_loss: 23.5190, val_MinusLogProbMetric: 23.5190

Epoch 652: val_loss did not improve from 23.42791
196/196 - 52s - loss: 23.3137 - MinusLogProbMetric: 23.3137 - val_loss: 23.5190 - val_MinusLogProbMetric: 23.5190 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 653/1000
2023-10-09 19:06:19.993 
Epoch 653/1000 
	 loss: 23.3184, MinusLogProbMetric: 23.3184, val_loss: 23.6089, val_MinusLogProbMetric: 23.6089

Epoch 653: val_loss did not improve from 23.42791
196/196 - 52s - loss: 23.3184 - MinusLogProbMetric: 23.3184 - val_loss: 23.6089 - val_MinusLogProbMetric: 23.6089 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 654/1000
2023-10-09 19:07:14.857 
Epoch 654/1000 
	 loss: 23.3015, MinusLogProbMetric: 23.3015, val_loss: 23.4415, val_MinusLogProbMetric: 23.4415

Epoch 654: val_loss did not improve from 23.42791
196/196 - 55s - loss: 23.3015 - MinusLogProbMetric: 23.3015 - val_loss: 23.4415 - val_MinusLogProbMetric: 23.4415 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 655/1000
2023-10-09 19:08:05.947 
Epoch 655/1000 
	 loss: 23.3110, MinusLogProbMetric: 23.3110, val_loss: 23.5020, val_MinusLogProbMetric: 23.5020

Epoch 655: val_loss did not improve from 23.42791
196/196 - 51s - loss: 23.3110 - MinusLogProbMetric: 23.3110 - val_loss: 23.5020 - val_MinusLogProbMetric: 23.5020 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 656/1000
2023-10-09 19:08:56.883 
Epoch 656/1000 
	 loss: 23.3096, MinusLogProbMetric: 23.3096, val_loss: 23.4121, val_MinusLogProbMetric: 23.4121

Epoch 656: val_loss improved from 23.42791 to 23.41212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 23.3096 - MinusLogProbMetric: 23.3096 - val_loss: 23.4121 - val_MinusLogProbMetric: 23.4121 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 657/1000
2023-10-09 19:09:47.184 
Epoch 657/1000 
	 loss: 23.2617, MinusLogProbMetric: 23.2617, val_loss: 23.4175, val_MinusLogProbMetric: 23.4175

Epoch 657: val_loss did not improve from 23.41212
196/196 - 50s - loss: 23.2617 - MinusLogProbMetric: 23.2617 - val_loss: 23.4175 - val_MinusLogProbMetric: 23.4175 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 658/1000
2023-10-09 19:10:37.143 
Epoch 658/1000 
	 loss: 23.2736, MinusLogProbMetric: 23.2736, val_loss: 23.3585, val_MinusLogProbMetric: 23.3585

Epoch 658: val_loss improved from 23.41212 to 23.35847, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 23.2736 - MinusLogProbMetric: 23.2736 - val_loss: 23.3585 - val_MinusLogProbMetric: 23.3585 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 659/1000
2023-10-09 19:11:27.708 
Epoch 659/1000 
	 loss: 23.2569, MinusLogProbMetric: 23.2569, val_loss: 23.3550, val_MinusLogProbMetric: 23.3550

Epoch 659: val_loss improved from 23.35847 to 23.35503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 23.2569 - MinusLogProbMetric: 23.2569 - val_loss: 23.3550 - val_MinusLogProbMetric: 23.3550 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 660/1000
2023-10-09 19:12:19.811 
Epoch 660/1000 
	 loss: 23.2559, MinusLogProbMetric: 23.2559, val_loss: 23.4171, val_MinusLogProbMetric: 23.4171

Epoch 660: val_loss did not improve from 23.35503
196/196 - 51s - loss: 23.2559 - MinusLogProbMetric: 23.2559 - val_loss: 23.4171 - val_MinusLogProbMetric: 23.4171 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 661/1000
2023-10-09 19:13:11.612 
Epoch 661/1000 
	 loss: 23.2459, MinusLogProbMetric: 23.2459, val_loss: 23.3755, val_MinusLogProbMetric: 23.3755

Epoch 661: val_loss did not improve from 23.35503
196/196 - 52s - loss: 23.2459 - MinusLogProbMetric: 23.2459 - val_loss: 23.3755 - val_MinusLogProbMetric: 23.3755 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 662/1000
2023-10-09 19:14:01.513 
Epoch 662/1000 
	 loss: 23.2497, MinusLogProbMetric: 23.2497, val_loss: 23.3585, val_MinusLogProbMetric: 23.3585

Epoch 662: val_loss did not improve from 23.35503
196/196 - 50s - loss: 23.2497 - MinusLogProbMetric: 23.2497 - val_loss: 23.3585 - val_MinusLogProbMetric: 23.3585 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 663/1000
2023-10-09 19:14:51.887 
Epoch 663/1000 
	 loss: 23.2208, MinusLogProbMetric: 23.2208, val_loss: 23.3963, val_MinusLogProbMetric: 23.3963

Epoch 663: val_loss did not improve from 23.35503
196/196 - 50s - loss: 23.2208 - MinusLogProbMetric: 23.2208 - val_loss: 23.3963 - val_MinusLogProbMetric: 23.3963 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 664/1000
2023-10-09 19:15:41.749 
Epoch 664/1000 
	 loss: 23.2444, MinusLogProbMetric: 23.2444, val_loss: 23.3808, val_MinusLogProbMetric: 23.3808

Epoch 664: val_loss did not improve from 23.35503
196/196 - 50s - loss: 23.2444 - MinusLogProbMetric: 23.2444 - val_loss: 23.3808 - val_MinusLogProbMetric: 23.3808 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 665/1000
2023-10-09 19:16:32.972 
Epoch 665/1000 
	 loss: 23.2192, MinusLogProbMetric: 23.2192, val_loss: 23.3536, val_MinusLogProbMetric: 23.3536

Epoch 665: val_loss improved from 23.35503 to 23.35357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 23.2192 - MinusLogProbMetric: 23.2192 - val_loss: 23.3536 - val_MinusLogProbMetric: 23.3536 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 666/1000
2023-10-09 19:17:25.768 
Epoch 666/1000 
	 loss: 23.1949, MinusLogProbMetric: 23.1949, val_loss: 23.3550, val_MinusLogProbMetric: 23.3550

Epoch 666: val_loss did not improve from 23.35357
196/196 - 52s - loss: 23.1949 - MinusLogProbMetric: 23.1949 - val_loss: 23.3550 - val_MinusLogProbMetric: 23.3550 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 667/1000
2023-10-09 19:18:16.299 
Epoch 667/1000 
	 loss: 23.1965, MinusLogProbMetric: 23.1965, val_loss: 23.3198, val_MinusLogProbMetric: 23.3198

Epoch 667: val_loss improved from 23.35357 to 23.31981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 23.1965 - MinusLogProbMetric: 23.1965 - val_loss: 23.3198 - val_MinusLogProbMetric: 23.3198 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 668/1000
2023-10-09 19:19:09.314 
Epoch 668/1000 
	 loss: 23.1950, MinusLogProbMetric: 23.1950, val_loss: 23.3823, val_MinusLogProbMetric: 23.3823

Epoch 668: val_loss did not improve from 23.31981
196/196 - 52s - loss: 23.1950 - MinusLogProbMetric: 23.1950 - val_loss: 23.3823 - val_MinusLogProbMetric: 23.3823 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 669/1000
2023-10-09 19:20:04.062 
Epoch 669/1000 
	 loss: 23.1950, MinusLogProbMetric: 23.1950, val_loss: 23.3459, val_MinusLogProbMetric: 23.3459

Epoch 669: val_loss did not improve from 23.31981
196/196 - 55s - loss: 23.1950 - MinusLogProbMetric: 23.1950 - val_loss: 23.3459 - val_MinusLogProbMetric: 23.3459 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 670/1000
2023-10-09 19:20:54.493 
Epoch 670/1000 
	 loss: 23.1849, MinusLogProbMetric: 23.1849, val_loss: 23.4066, val_MinusLogProbMetric: 23.4066

Epoch 670: val_loss did not improve from 23.31981
196/196 - 50s - loss: 23.1849 - MinusLogProbMetric: 23.1849 - val_loss: 23.4066 - val_MinusLogProbMetric: 23.4066 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 671/1000
2023-10-09 19:21:44.949 
Epoch 671/1000 
	 loss: 23.1904, MinusLogProbMetric: 23.1904, val_loss: 23.4399, val_MinusLogProbMetric: 23.4399

Epoch 671: val_loss did not improve from 23.31981
196/196 - 50s - loss: 23.1904 - MinusLogProbMetric: 23.1904 - val_loss: 23.4399 - val_MinusLogProbMetric: 23.4399 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 672/1000
2023-10-09 19:22:33.904 
Epoch 672/1000 
	 loss: 23.2208, MinusLogProbMetric: 23.2208, val_loss: 23.3056, val_MinusLogProbMetric: 23.3056

Epoch 672: val_loss improved from 23.31981 to 23.30557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 23.2208 - MinusLogProbMetric: 23.2208 - val_loss: 23.3056 - val_MinusLogProbMetric: 23.3056 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 673/1000
2023-10-09 19:23:26.400 
Epoch 673/1000 
	 loss: 23.1669, MinusLogProbMetric: 23.1669, val_loss: 23.4018, val_MinusLogProbMetric: 23.4018

Epoch 673: val_loss did not improve from 23.30557
196/196 - 52s - loss: 23.1669 - MinusLogProbMetric: 23.1669 - val_loss: 23.4018 - val_MinusLogProbMetric: 23.4018 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 674/1000
2023-10-09 19:24:17.185 
Epoch 674/1000 
	 loss: 23.1796, MinusLogProbMetric: 23.1796, val_loss: 23.2852, val_MinusLogProbMetric: 23.2852

Epoch 674: val_loss improved from 23.30557 to 23.28520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 23.1796 - MinusLogProbMetric: 23.1796 - val_loss: 23.2852 - val_MinusLogProbMetric: 23.2852 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 675/1000
2023-10-09 19:25:08.068 
Epoch 675/1000 
	 loss: 23.1705, MinusLogProbMetric: 23.1705, val_loss: 23.2821, val_MinusLogProbMetric: 23.2821

Epoch 675: val_loss improved from 23.28520 to 23.28207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 23.1705 - MinusLogProbMetric: 23.1705 - val_loss: 23.2821 - val_MinusLogProbMetric: 23.2821 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 676/1000
2023-10-09 19:26:00.682 
Epoch 676/1000 
	 loss: 23.1756, MinusLogProbMetric: 23.1756, val_loss: 23.2436, val_MinusLogProbMetric: 23.2436

Epoch 676: val_loss improved from 23.28207 to 23.24360, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 23.1756 - MinusLogProbMetric: 23.1756 - val_loss: 23.2436 - val_MinusLogProbMetric: 23.2436 - lr: 5.5556e-05 - 53s/epoch - 268ms/step
Epoch 677/1000
2023-10-09 19:26:52.132 
Epoch 677/1000 
	 loss: 23.1596, MinusLogProbMetric: 23.1596, val_loss: 23.3080, val_MinusLogProbMetric: 23.3080

Epoch 677: val_loss did not improve from 23.24360
196/196 - 51s - loss: 23.1596 - MinusLogProbMetric: 23.1596 - val_loss: 23.3080 - val_MinusLogProbMetric: 23.3080 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 678/1000
2023-10-09 19:27:41.701 
Epoch 678/1000 
	 loss: 23.1283, MinusLogProbMetric: 23.1283, val_loss: 23.2116, val_MinusLogProbMetric: 23.2116

Epoch 678: val_loss improved from 23.24360 to 23.21158, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 23.1283 - MinusLogProbMetric: 23.1283 - val_loss: 23.2116 - val_MinusLogProbMetric: 23.2116 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 679/1000
2023-10-09 19:28:33.310 
Epoch 679/1000 
	 loss: 23.1453, MinusLogProbMetric: 23.1453, val_loss: 23.3595, val_MinusLogProbMetric: 23.3595

Epoch 679: val_loss did not improve from 23.21158
196/196 - 51s - loss: 23.1453 - MinusLogProbMetric: 23.1453 - val_loss: 23.3595 - val_MinusLogProbMetric: 23.3595 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 680/1000
2023-10-09 19:29:24.124 
Epoch 680/1000 
	 loss: 23.1265, MinusLogProbMetric: 23.1265, val_loss: 23.2754, val_MinusLogProbMetric: 23.2754

Epoch 680: val_loss did not improve from 23.21158
196/196 - 51s - loss: 23.1265 - MinusLogProbMetric: 23.1265 - val_loss: 23.2754 - val_MinusLogProbMetric: 23.2754 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 681/1000
2023-10-09 19:30:16.407 
Epoch 681/1000 
	 loss: 23.1354, MinusLogProbMetric: 23.1354, val_loss: 23.5982, val_MinusLogProbMetric: 23.5982

Epoch 681: val_loss did not improve from 23.21158
196/196 - 52s - loss: 23.1354 - MinusLogProbMetric: 23.1354 - val_loss: 23.5982 - val_MinusLogProbMetric: 23.5982 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 682/1000
2023-10-09 19:31:07.163 
Epoch 682/1000 
	 loss: 23.1180, MinusLogProbMetric: 23.1180, val_loss: 23.2083, val_MinusLogProbMetric: 23.2083

Epoch 682: val_loss improved from 23.21158 to 23.20835, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 23.1180 - MinusLogProbMetric: 23.1180 - val_loss: 23.2083 - val_MinusLogProbMetric: 23.2083 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 683/1000
2023-10-09 19:31:58.177 
Epoch 683/1000 
	 loss: 23.0983, MinusLogProbMetric: 23.0983, val_loss: 23.2327, val_MinusLogProbMetric: 23.2327

Epoch 683: val_loss did not improve from 23.20835
196/196 - 50s - loss: 23.0983 - MinusLogProbMetric: 23.0983 - val_loss: 23.2327 - val_MinusLogProbMetric: 23.2327 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 684/1000
2023-10-09 19:32:51.324 
Epoch 684/1000 
	 loss: 23.0912, MinusLogProbMetric: 23.0912, val_loss: 23.2170, val_MinusLogProbMetric: 23.2170

Epoch 684: val_loss did not improve from 23.20835
196/196 - 53s - loss: 23.0912 - MinusLogProbMetric: 23.0912 - val_loss: 23.2170 - val_MinusLogProbMetric: 23.2170 - lr: 5.5556e-05 - 53s/epoch - 271ms/step
Epoch 685/1000
2023-10-09 19:33:41.874 
Epoch 685/1000 
	 loss: 23.0992, MinusLogProbMetric: 23.0992, val_loss: 23.2652, val_MinusLogProbMetric: 23.2652

Epoch 685: val_loss did not improve from 23.20835
196/196 - 51s - loss: 23.0992 - MinusLogProbMetric: 23.0992 - val_loss: 23.2652 - val_MinusLogProbMetric: 23.2652 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 686/1000
2023-10-09 19:34:33.181 
Epoch 686/1000 
	 loss: 23.0849, MinusLogProbMetric: 23.0849, val_loss: 23.3002, val_MinusLogProbMetric: 23.3002

Epoch 686: val_loss did not improve from 23.20835
196/196 - 51s - loss: 23.0849 - MinusLogProbMetric: 23.0849 - val_loss: 23.3002 - val_MinusLogProbMetric: 23.3002 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 687/1000
2023-10-09 19:35:23.491 
Epoch 687/1000 
	 loss: 23.0704, MinusLogProbMetric: 23.0704, val_loss: 23.2615, val_MinusLogProbMetric: 23.2615

Epoch 687: val_loss did not improve from 23.20835
196/196 - 50s - loss: 23.0704 - MinusLogProbMetric: 23.0704 - val_loss: 23.2615 - val_MinusLogProbMetric: 23.2615 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 688/1000
2023-10-09 19:36:13.229 
Epoch 688/1000 
	 loss: 23.0578, MinusLogProbMetric: 23.0578, val_loss: 23.2211, val_MinusLogProbMetric: 23.2211

Epoch 688: val_loss did not improve from 23.20835
196/196 - 50s - loss: 23.0578 - MinusLogProbMetric: 23.0578 - val_loss: 23.2211 - val_MinusLogProbMetric: 23.2211 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 689/1000
2023-10-09 19:37:02.657 
Epoch 689/1000 
	 loss: 23.0495, MinusLogProbMetric: 23.0495, val_loss: 23.2912, val_MinusLogProbMetric: 23.2912

Epoch 689: val_loss did not improve from 23.20835
196/196 - 49s - loss: 23.0495 - MinusLogProbMetric: 23.0495 - val_loss: 23.2912 - val_MinusLogProbMetric: 23.2912 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 690/1000
2023-10-09 19:37:53.081 
Epoch 690/1000 
	 loss: 23.0567, MinusLogProbMetric: 23.0567, val_loss: 23.2411, val_MinusLogProbMetric: 23.2411

Epoch 690: val_loss did not improve from 23.20835
196/196 - 50s - loss: 23.0567 - MinusLogProbMetric: 23.0567 - val_loss: 23.2411 - val_MinusLogProbMetric: 23.2411 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 691/1000
2023-10-09 19:38:43.719 
Epoch 691/1000 
	 loss: 23.0482, MinusLogProbMetric: 23.0482, val_loss: 23.2072, val_MinusLogProbMetric: 23.2072

Epoch 691: val_loss improved from 23.20835 to 23.20719, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 23.0482 - MinusLogProbMetric: 23.0482 - val_loss: 23.2072 - val_MinusLogProbMetric: 23.2072 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 692/1000
2023-10-09 19:39:35.823 
Epoch 692/1000 
	 loss: 23.0406, MinusLogProbMetric: 23.0406, val_loss: 23.2193, val_MinusLogProbMetric: 23.2193

Epoch 692: val_loss did not improve from 23.20719
196/196 - 51s - loss: 23.0406 - MinusLogProbMetric: 23.0406 - val_loss: 23.2193 - val_MinusLogProbMetric: 23.2193 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 693/1000
2023-10-09 19:40:26.345 
Epoch 693/1000 
	 loss: 23.0572, MinusLogProbMetric: 23.0572, val_loss: 23.1772, val_MinusLogProbMetric: 23.1772

Epoch 693: val_loss improved from 23.20719 to 23.17720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 23.0572 - MinusLogProbMetric: 23.0572 - val_loss: 23.1772 - val_MinusLogProbMetric: 23.1772 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 694/1000
2023-10-09 19:41:16.594 
Epoch 694/1000 
	 loss: 23.0448, MinusLogProbMetric: 23.0448, val_loss: 23.2114, val_MinusLogProbMetric: 23.2114

Epoch 694: val_loss did not improve from 23.17720
196/196 - 50s - loss: 23.0448 - MinusLogProbMetric: 23.0448 - val_loss: 23.2114 - val_MinusLogProbMetric: 23.2114 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 695/1000
2023-10-09 19:42:05.480 
Epoch 695/1000 
	 loss: 23.0254, MinusLogProbMetric: 23.0254, val_loss: 23.1699, val_MinusLogProbMetric: 23.1699

Epoch 695: val_loss improved from 23.17720 to 23.16987, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 23.0254 - MinusLogProbMetric: 23.0254 - val_loss: 23.1699 - val_MinusLogProbMetric: 23.1699 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 696/1000
2023-10-09 19:42:55.680 
Epoch 696/1000 
	 loss: 23.0140, MinusLogProbMetric: 23.0140, val_loss: 23.1098, val_MinusLogProbMetric: 23.1098

Epoch 696: val_loss improved from 23.16987 to 23.10976, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 23.0140 - MinusLogProbMetric: 23.0140 - val_loss: 23.1098 - val_MinusLogProbMetric: 23.1098 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 697/1000
2023-10-09 19:43:47.500 
Epoch 697/1000 
	 loss: 23.0399, MinusLogProbMetric: 23.0399, val_loss: 23.2460, val_MinusLogProbMetric: 23.2460

Epoch 697: val_loss did not improve from 23.10976
196/196 - 51s - loss: 23.0399 - MinusLogProbMetric: 23.0399 - val_loss: 23.2460 - val_MinusLogProbMetric: 23.2460 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 698/1000
2023-10-09 19:44:37.777 
Epoch 698/1000 
	 loss: 23.0094, MinusLogProbMetric: 23.0094, val_loss: 23.1515, val_MinusLogProbMetric: 23.1515

Epoch 698: val_loss did not improve from 23.10976
196/196 - 50s - loss: 23.0094 - MinusLogProbMetric: 23.0094 - val_loss: 23.1515 - val_MinusLogProbMetric: 23.1515 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 699/1000
2023-10-09 19:45:30.162 
Epoch 699/1000 
	 loss: 23.0125, MinusLogProbMetric: 23.0125, val_loss: 23.2912, val_MinusLogProbMetric: 23.2912

Epoch 699: val_loss did not improve from 23.10976
196/196 - 52s - loss: 23.0125 - MinusLogProbMetric: 23.0125 - val_loss: 23.2912 - val_MinusLogProbMetric: 23.2912 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 700/1000
2023-10-09 19:46:20.849 
Epoch 700/1000 
	 loss: 23.0254, MinusLogProbMetric: 23.0254, val_loss: 23.1387, val_MinusLogProbMetric: 23.1387

Epoch 700: val_loss did not improve from 23.10976
196/196 - 51s - loss: 23.0254 - MinusLogProbMetric: 23.0254 - val_loss: 23.1387 - val_MinusLogProbMetric: 23.1387 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 701/1000
2023-10-09 19:47:14.292 
Epoch 701/1000 
	 loss: 22.9831, MinusLogProbMetric: 22.9831, val_loss: 23.2363, val_MinusLogProbMetric: 23.2363

Epoch 701: val_loss did not improve from 23.10976
196/196 - 53s - loss: 22.9831 - MinusLogProbMetric: 22.9831 - val_loss: 23.2363 - val_MinusLogProbMetric: 23.2363 - lr: 5.5556e-05 - 53s/epoch - 273ms/step
Epoch 702/1000
2023-10-09 19:48:03.673 
Epoch 702/1000 
	 loss: 22.9815, MinusLogProbMetric: 22.9815, val_loss: 23.1632, val_MinusLogProbMetric: 23.1632

Epoch 702: val_loss did not improve from 23.10976
196/196 - 49s - loss: 22.9815 - MinusLogProbMetric: 22.9815 - val_loss: 23.1632 - val_MinusLogProbMetric: 23.1632 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 703/1000
2023-10-09 19:48:54.374 
Epoch 703/1000 
	 loss: 22.9685, MinusLogProbMetric: 22.9685, val_loss: 23.0689, val_MinusLogProbMetric: 23.0689

Epoch 703: val_loss improved from 23.10976 to 23.06890, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.9685 - MinusLogProbMetric: 22.9685 - val_loss: 23.0689 - val_MinusLogProbMetric: 23.0689 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 704/1000
2023-10-09 19:49:48.480 
Epoch 704/1000 
	 loss: 22.9760, MinusLogProbMetric: 22.9760, val_loss: 23.1382, val_MinusLogProbMetric: 23.1382

Epoch 704: val_loss did not improve from 23.06890
196/196 - 53s - loss: 22.9760 - MinusLogProbMetric: 22.9760 - val_loss: 23.1382 - val_MinusLogProbMetric: 23.1382 - lr: 5.5556e-05 - 53s/epoch - 272ms/step
Epoch 705/1000
2023-10-09 19:50:40.603 
Epoch 705/1000 
	 loss: 22.9592, MinusLogProbMetric: 22.9592, val_loss: 23.1556, val_MinusLogProbMetric: 23.1556

Epoch 705: val_loss did not improve from 23.06890
196/196 - 52s - loss: 22.9592 - MinusLogProbMetric: 22.9592 - val_loss: 23.1556 - val_MinusLogProbMetric: 23.1556 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 706/1000
2023-10-09 19:51:31.786 
Epoch 706/1000 
	 loss: 22.9592, MinusLogProbMetric: 22.9592, val_loss: 23.1108, val_MinusLogProbMetric: 23.1108

Epoch 706: val_loss did not improve from 23.06890
196/196 - 51s - loss: 22.9592 - MinusLogProbMetric: 22.9592 - val_loss: 23.1108 - val_MinusLogProbMetric: 23.1108 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 707/1000
2023-10-09 19:52:24.097 
Epoch 707/1000 
	 loss: 22.9489, MinusLogProbMetric: 22.9489, val_loss: 23.3169, val_MinusLogProbMetric: 23.3169

Epoch 707: val_loss did not improve from 23.06890
196/196 - 52s - loss: 22.9489 - MinusLogProbMetric: 22.9489 - val_loss: 23.3169 - val_MinusLogProbMetric: 23.3169 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 708/1000
2023-10-09 19:53:14.214 
Epoch 708/1000 
	 loss: 22.9432, MinusLogProbMetric: 22.9432, val_loss: 23.0862, val_MinusLogProbMetric: 23.0862

Epoch 708: val_loss did not improve from 23.06890
196/196 - 50s - loss: 22.9432 - MinusLogProbMetric: 22.9432 - val_loss: 23.0862 - val_MinusLogProbMetric: 23.0862 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 709/1000
2023-10-09 19:54:03.359 
Epoch 709/1000 
	 loss: 22.9229, MinusLogProbMetric: 22.9229, val_loss: 23.0315, val_MinusLogProbMetric: 23.0315

Epoch 709: val_loss improved from 23.06890 to 23.03153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 22.9229 - MinusLogProbMetric: 22.9229 - val_loss: 23.0315 - val_MinusLogProbMetric: 23.0315 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 710/1000
2023-10-09 19:54:54.857 
Epoch 710/1000 
	 loss: 22.9623, MinusLogProbMetric: 22.9623, val_loss: 23.0699, val_MinusLogProbMetric: 23.0699

Epoch 710: val_loss did not improve from 23.03153
196/196 - 51s - loss: 22.9623 - MinusLogProbMetric: 22.9623 - val_loss: 23.0699 - val_MinusLogProbMetric: 23.0699 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 711/1000
2023-10-09 19:55:47.210 
Epoch 711/1000 
	 loss: 22.9179, MinusLogProbMetric: 22.9179, val_loss: 23.0132, val_MinusLogProbMetric: 23.0132

Epoch 711: val_loss improved from 23.03153 to 23.01317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 22.9179 - MinusLogProbMetric: 22.9179 - val_loss: 23.0132 - val_MinusLogProbMetric: 23.0132 - lr: 5.5556e-05 - 53s/epoch - 271ms/step
Epoch 712/1000
2023-10-09 19:56:38.354 
Epoch 712/1000 
	 loss: 22.9076, MinusLogProbMetric: 22.9076, val_loss: 23.2453, val_MinusLogProbMetric: 23.2453

Epoch 712: val_loss did not improve from 23.01317
196/196 - 50s - loss: 22.9076 - MinusLogProbMetric: 22.9076 - val_loss: 23.2453 - val_MinusLogProbMetric: 23.2453 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 713/1000
2023-10-09 19:57:29.189 
Epoch 713/1000 
	 loss: 22.9253, MinusLogProbMetric: 22.9253, val_loss: 23.0470, val_MinusLogProbMetric: 23.0470

Epoch 713: val_loss did not improve from 23.01317
196/196 - 51s - loss: 22.9253 - MinusLogProbMetric: 22.9253 - val_loss: 23.0470 - val_MinusLogProbMetric: 23.0470 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 714/1000
2023-10-09 19:58:20.123 
Epoch 714/1000 
	 loss: 22.9156, MinusLogProbMetric: 22.9156, val_loss: 23.0045, val_MinusLogProbMetric: 23.0045

Epoch 714: val_loss improved from 23.01317 to 23.00452, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.9156 - MinusLogProbMetric: 22.9156 - val_loss: 23.0045 - val_MinusLogProbMetric: 23.0045 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 715/1000
2023-10-09 19:59:12.087 
Epoch 715/1000 
	 loss: 22.8966, MinusLogProbMetric: 22.8966, val_loss: 23.0808, val_MinusLogProbMetric: 23.0808

Epoch 715: val_loss did not improve from 23.00452
196/196 - 51s - loss: 22.8966 - MinusLogProbMetric: 22.8966 - val_loss: 23.0808 - val_MinusLogProbMetric: 23.0808 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 716/1000
2023-10-09 20:00:01.920 
Epoch 716/1000 
	 loss: 22.8916, MinusLogProbMetric: 22.8916, val_loss: 23.0149, val_MinusLogProbMetric: 23.0149

Epoch 716: val_loss did not improve from 23.00452
196/196 - 50s - loss: 22.8916 - MinusLogProbMetric: 22.8916 - val_loss: 23.0149 - val_MinusLogProbMetric: 23.0149 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 717/1000
2023-10-09 20:00:52.105 
Epoch 717/1000 
	 loss: 22.8847, MinusLogProbMetric: 22.8847, val_loss: 22.9776, val_MinusLogProbMetric: 22.9776

Epoch 717: val_loss improved from 23.00452 to 22.97762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.8847 - MinusLogProbMetric: 22.8847 - val_loss: 22.9776 - val_MinusLogProbMetric: 22.9776 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 718/1000
2023-10-09 20:01:44.116 
Epoch 718/1000 
	 loss: 22.8805, MinusLogProbMetric: 22.8805, val_loss: 23.1398, val_MinusLogProbMetric: 23.1398

Epoch 718: val_loss did not improve from 22.97762
196/196 - 51s - loss: 22.8805 - MinusLogProbMetric: 22.8805 - val_loss: 23.1398 - val_MinusLogProbMetric: 23.1398 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 719/1000
2023-10-09 20:02:36.047 
Epoch 719/1000 
	 loss: 22.8897, MinusLogProbMetric: 22.8897, val_loss: 23.0520, val_MinusLogProbMetric: 23.0520

Epoch 719: val_loss did not improve from 22.97762
196/196 - 52s - loss: 22.8897 - MinusLogProbMetric: 22.8897 - val_loss: 23.0520 - val_MinusLogProbMetric: 23.0520 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 720/1000
2023-10-09 20:03:25.664 
Epoch 720/1000 
	 loss: 22.8640, MinusLogProbMetric: 22.8640, val_loss: 23.0582, val_MinusLogProbMetric: 23.0582

Epoch 720: val_loss did not improve from 22.97762
196/196 - 50s - loss: 22.8640 - MinusLogProbMetric: 22.8640 - val_loss: 23.0582 - val_MinusLogProbMetric: 23.0582 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 721/1000
2023-10-09 20:04:15.525 
Epoch 721/1000 
	 loss: 22.8613, MinusLogProbMetric: 22.8613, val_loss: 22.9679, val_MinusLogProbMetric: 22.9679

Epoch 721: val_loss improved from 22.97762 to 22.96785, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.8613 - MinusLogProbMetric: 22.8613 - val_loss: 22.9679 - val_MinusLogProbMetric: 22.9679 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 722/1000
2023-10-09 20:05:06.783 
Epoch 722/1000 
	 loss: 22.8653, MinusLogProbMetric: 22.8653, val_loss: 22.9712, val_MinusLogProbMetric: 22.9712

Epoch 722: val_loss did not improve from 22.96785
196/196 - 50s - loss: 22.8653 - MinusLogProbMetric: 22.8653 - val_loss: 22.9712 - val_MinusLogProbMetric: 22.9712 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 723/1000
2023-10-09 20:05:58.941 
Epoch 723/1000 
	 loss: 22.8446, MinusLogProbMetric: 22.8446, val_loss: 23.0999, val_MinusLogProbMetric: 23.0999

Epoch 723: val_loss did not improve from 22.96785
196/196 - 52s - loss: 22.8446 - MinusLogProbMetric: 22.8446 - val_loss: 23.0999 - val_MinusLogProbMetric: 23.0999 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 724/1000
2023-10-09 20:06:48.738 
Epoch 724/1000 
	 loss: 22.8614, MinusLogProbMetric: 22.8614, val_loss: 22.9546, val_MinusLogProbMetric: 22.9546

Epoch 724: val_loss improved from 22.96785 to 22.95462, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.8614 - MinusLogProbMetric: 22.8614 - val_loss: 22.9546 - val_MinusLogProbMetric: 22.9546 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 725/1000
2023-10-09 20:07:39.413 
Epoch 725/1000 
	 loss: 22.8466, MinusLogProbMetric: 22.8466, val_loss: 22.9918, val_MinusLogProbMetric: 22.9918

Epoch 725: val_loss did not improve from 22.95462
196/196 - 50s - loss: 22.8466 - MinusLogProbMetric: 22.8466 - val_loss: 22.9918 - val_MinusLogProbMetric: 22.9918 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 726/1000
2023-10-09 20:08:31.899 
Epoch 726/1000 
	 loss: 22.8601, MinusLogProbMetric: 22.8601, val_loss: 22.9998, val_MinusLogProbMetric: 22.9998

Epoch 726: val_loss did not improve from 22.95462
196/196 - 52s - loss: 22.8601 - MinusLogProbMetric: 22.8601 - val_loss: 22.9998 - val_MinusLogProbMetric: 22.9998 - lr: 5.5556e-05 - 52s/epoch - 268ms/step
Epoch 727/1000
2023-10-09 20:09:22.596 
Epoch 727/1000 
	 loss: 22.8467, MinusLogProbMetric: 22.8467, val_loss: 23.0333, val_MinusLogProbMetric: 23.0333

Epoch 727: val_loss did not improve from 22.95462
196/196 - 51s - loss: 22.8467 - MinusLogProbMetric: 22.8467 - val_loss: 23.0333 - val_MinusLogProbMetric: 23.0333 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 728/1000
2023-10-09 20:10:12.687 
Epoch 728/1000 
	 loss: 22.8333, MinusLogProbMetric: 22.8333, val_loss: 23.0898, val_MinusLogProbMetric: 23.0898

Epoch 728: val_loss did not improve from 22.95462
196/196 - 50s - loss: 22.8333 - MinusLogProbMetric: 22.8333 - val_loss: 23.0898 - val_MinusLogProbMetric: 23.0898 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 729/1000
2023-10-09 20:11:01.502 
Epoch 729/1000 
	 loss: 22.8486, MinusLogProbMetric: 22.8486, val_loss: 23.0187, val_MinusLogProbMetric: 23.0187

Epoch 729: val_loss did not improve from 22.95462
196/196 - 49s - loss: 22.8486 - MinusLogProbMetric: 22.8486 - val_loss: 23.0187 - val_MinusLogProbMetric: 23.0187 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 730/1000
2023-10-09 20:11:53.336 
Epoch 730/1000 
	 loss: 22.8114, MinusLogProbMetric: 22.8114, val_loss: 22.9795, val_MinusLogProbMetric: 22.9795

Epoch 730: val_loss did not improve from 22.95462
196/196 - 52s - loss: 22.8114 - MinusLogProbMetric: 22.8114 - val_loss: 22.9795 - val_MinusLogProbMetric: 22.9795 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 731/1000
2023-10-09 20:12:42.942 
Epoch 731/1000 
	 loss: 22.8213, MinusLogProbMetric: 22.8213, val_loss: 22.9616, val_MinusLogProbMetric: 22.9616

Epoch 731: val_loss did not improve from 22.95462
196/196 - 50s - loss: 22.8213 - MinusLogProbMetric: 22.8213 - val_loss: 22.9616 - val_MinusLogProbMetric: 22.9616 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 732/1000
2023-10-09 20:13:35.412 
Epoch 732/1000 
	 loss: 22.8180, MinusLogProbMetric: 22.8180, val_loss: 22.9012, val_MinusLogProbMetric: 22.9012

Epoch 732: val_loss improved from 22.95462 to 22.90119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 22.8180 - MinusLogProbMetric: 22.8180 - val_loss: 22.9012 - val_MinusLogProbMetric: 22.9012 - lr: 5.5556e-05 - 53s/epoch - 272ms/step
Epoch 733/1000
2023-10-09 20:14:26.534 
Epoch 733/1000 
	 loss: 22.8060, MinusLogProbMetric: 22.8060, val_loss: 23.0115, val_MinusLogProbMetric: 23.0115

Epoch 733: val_loss did not improve from 22.90119
196/196 - 50s - loss: 22.8060 - MinusLogProbMetric: 22.8060 - val_loss: 23.0115 - val_MinusLogProbMetric: 23.0115 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 734/1000
2023-10-09 20:15:17.292 
Epoch 734/1000 
	 loss: 22.7886, MinusLogProbMetric: 22.7886, val_loss: 22.9441, val_MinusLogProbMetric: 22.9441

Epoch 734: val_loss did not improve from 22.90119
196/196 - 51s - loss: 22.7886 - MinusLogProbMetric: 22.7886 - val_loss: 22.9441 - val_MinusLogProbMetric: 22.9441 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 735/1000
2023-10-09 20:16:07.852 
Epoch 735/1000 
	 loss: 22.7807, MinusLogProbMetric: 22.7807, val_loss: 23.0603, val_MinusLogProbMetric: 23.0603

Epoch 735: val_loss did not improve from 22.90119
196/196 - 51s - loss: 22.7807 - MinusLogProbMetric: 22.7807 - val_loss: 23.0603 - val_MinusLogProbMetric: 23.0603 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 736/1000
2023-10-09 20:16:58.744 
Epoch 736/1000 
	 loss: 22.7802, MinusLogProbMetric: 22.7802, val_loss: 22.9066, val_MinusLogProbMetric: 22.9066

Epoch 736: val_loss did not improve from 22.90119
196/196 - 51s - loss: 22.7802 - MinusLogProbMetric: 22.7802 - val_loss: 22.9066 - val_MinusLogProbMetric: 22.9066 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 737/1000
2023-10-09 20:17:47.995 
Epoch 737/1000 
	 loss: 22.7749, MinusLogProbMetric: 22.7749, val_loss: 23.1001, val_MinusLogProbMetric: 23.1001

Epoch 737: val_loss did not improve from 22.90119
196/196 - 49s - loss: 22.7749 - MinusLogProbMetric: 22.7749 - val_loss: 23.1001 - val_MinusLogProbMetric: 23.1001 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 738/1000
2023-10-09 20:18:42.733 
Epoch 738/1000 
	 loss: 22.8142, MinusLogProbMetric: 22.8142, val_loss: 23.2129, val_MinusLogProbMetric: 23.2129

Epoch 738: val_loss did not improve from 22.90119
196/196 - 55s - loss: 22.8142 - MinusLogProbMetric: 22.8142 - val_loss: 23.2129 - val_MinusLogProbMetric: 23.2129 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 739/1000
2023-10-09 20:19:33.518 
Epoch 739/1000 
	 loss: 22.7785, MinusLogProbMetric: 22.7785, val_loss: 22.9380, val_MinusLogProbMetric: 22.9380

Epoch 739: val_loss did not improve from 22.90119
196/196 - 51s - loss: 22.7785 - MinusLogProbMetric: 22.7785 - val_loss: 22.9380 - val_MinusLogProbMetric: 22.9380 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 740/1000
2023-10-09 20:20:23.038 
Epoch 740/1000 
	 loss: 22.7704, MinusLogProbMetric: 22.7704, val_loss: 22.8566, val_MinusLogProbMetric: 22.8566

Epoch 740: val_loss improved from 22.90119 to 22.85659, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 22.7704 - MinusLogProbMetric: 22.7704 - val_loss: 22.8566 - val_MinusLogProbMetric: 22.8566 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 741/1000
2023-10-09 20:21:14.360 
Epoch 741/1000 
	 loss: 22.7519, MinusLogProbMetric: 22.7519, val_loss: 23.0319, val_MinusLogProbMetric: 23.0319

Epoch 741: val_loss did not improve from 22.85659
196/196 - 51s - loss: 22.7519 - MinusLogProbMetric: 22.7519 - val_loss: 23.0319 - val_MinusLogProbMetric: 23.0319 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 742/1000
2023-10-09 20:22:06.834 
Epoch 742/1000 
	 loss: 22.7695, MinusLogProbMetric: 22.7695, val_loss: 23.1266, val_MinusLogProbMetric: 23.1266

Epoch 742: val_loss did not improve from 22.85659
196/196 - 52s - loss: 22.7695 - MinusLogProbMetric: 22.7695 - val_loss: 23.1266 - val_MinusLogProbMetric: 23.1266 - lr: 5.5556e-05 - 52s/epoch - 268ms/step
Epoch 743/1000
2023-10-09 20:22:56.994 
Epoch 743/1000 
	 loss: 22.7356, MinusLogProbMetric: 22.7356, val_loss: 22.9709, val_MinusLogProbMetric: 22.9709

Epoch 743: val_loss did not improve from 22.85659
196/196 - 50s - loss: 22.7356 - MinusLogProbMetric: 22.7356 - val_loss: 22.9709 - val_MinusLogProbMetric: 22.9709 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 744/1000
2023-10-09 20:23:48.273 
Epoch 744/1000 
	 loss: 22.7352, MinusLogProbMetric: 22.7352, val_loss: 22.8945, val_MinusLogProbMetric: 22.8945

Epoch 744: val_loss did not improve from 22.85659
196/196 - 51s - loss: 22.7352 - MinusLogProbMetric: 22.7352 - val_loss: 22.8945 - val_MinusLogProbMetric: 22.8945 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 745/1000
2023-10-09 20:24:37.020 
Epoch 745/1000 
	 loss: 22.7493, MinusLogProbMetric: 22.7493, val_loss: 22.8649, val_MinusLogProbMetric: 22.8649

Epoch 745: val_loss did not improve from 22.85659
196/196 - 49s - loss: 22.7493 - MinusLogProbMetric: 22.7493 - val_loss: 22.8649 - val_MinusLogProbMetric: 22.8649 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 746/1000
2023-10-09 20:25:27.097 
Epoch 746/1000 
	 loss: 22.7262, MinusLogProbMetric: 22.7262, val_loss: 22.9054, val_MinusLogProbMetric: 22.9054

Epoch 746: val_loss did not improve from 22.85659
196/196 - 50s - loss: 22.7262 - MinusLogProbMetric: 22.7262 - val_loss: 22.9054 - val_MinusLogProbMetric: 22.9054 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 747/1000
2023-10-09 20:26:19.934 
Epoch 747/1000 
	 loss: 22.7293, MinusLogProbMetric: 22.7293, val_loss: 22.8400, val_MinusLogProbMetric: 22.8400

Epoch 747: val_loss improved from 22.85659 to 22.84005, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 22.7293 - MinusLogProbMetric: 22.7293 - val_loss: 22.8400 - val_MinusLogProbMetric: 22.8400 - lr: 5.5556e-05 - 54s/epoch - 273ms/step
Epoch 748/1000
2023-10-09 20:27:11.530 
Epoch 748/1000 
	 loss: 22.7090, MinusLogProbMetric: 22.7090, val_loss: 22.8334, val_MinusLogProbMetric: 22.8334

Epoch 748: val_loss improved from 22.84005 to 22.83338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.7090 - MinusLogProbMetric: 22.7090 - val_loss: 22.8334 - val_MinusLogProbMetric: 22.8334 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 749/1000
2023-10-09 20:28:02.544 
Epoch 749/1000 
	 loss: 22.7083, MinusLogProbMetric: 22.7083, val_loss: 22.8900, val_MinusLogProbMetric: 22.8900

Epoch 749: val_loss did not improve from 22.83338
196/196 - 50s - loss: 22.7083 - MinusLogProbMetric: 22.7083 - val_loss: 22.8900 - val_MinusLogProbMetric: 22.8900 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 750/1000
2023-10-09 20:28:54.849 
Epoch 750/1000 
	 loss: 22.7142, MinusLogProbMetric: 22.7142, val_loss: 22.8608, val_MinusLogProbMetric: 22.8608

Epoch 750: val_loss did not improve from 22.83338
196/196 - 52s - loss: 22.7142 - MinusLogProbMetric: 22.7142 - val_loss: 22.8608 - val_MinusLogProbMetric: 22.8608 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 751/1000
2023-10-09 20:29:47.384 
Epoch 751/1000 
	 loss: 22.6937, MinusLogProbMetric: 22.6937, val_loss: 23.0360, val_MinusLogProbMetric: 23.0360

Epoch 751: val_loss did not improve from 22.83338
196/196 - 53s - loss: 22.6937 - MinusLogProbMetric: 22.6937 - val_loss: 23.0360 - val_MinusLogProbMetric: 23.0360 - lr: 5.5556e-05 - 53s/epoch - 268ms/step
Epoch 752/1000
2023-10-09 20:30:38.707 
Epoch 752/1000 
	 loss: 22.6714, MinusLogProbMetric: 22.6714, val_loss: 22.7857, val_MinusLogProbMetric: 22.7857

Epoch 752: val_loss improved from 22.83338 to 22.78565, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.6714 - MinusLogProbMetric: 22.6714 - val_loss: 22.7857 - val_MinusLogProbMetric: 22.7857 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 753/1000
2023-10-09 20:31:29.688 
Epoch 753/1000 
	 loss: 22.6845, MinusLogProbMetric: 22.6845, val_loss: 22.8571, val_MinusLogProbMetric: 22.8571

Epoch 753: val_loss did not improve from 22.78565
196/196 - 50s - loss: 22.6845 - MinusLogProbMetric: 22.6845 - val_loss: 22.8571 - val_MinusLogProbMetric: 22.8571 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 754/1000
2023-10-09 20:32:20.133 
Epoch 754/1000 
	 loss: 22.6852, MinusLogProbMetric: 22.6852, val_loss: 22.8138, val_MinusLogProbMetric: 22.8138

Epoch 754: val_loss did not improve from 22.78565
196/196 - 50s - loss: 22.6852 - MinusLogProbMetric: 22.6852 - val_loss: 22.8138 - val_MinusLogProbMetric: 22.8138 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 755/1000
2023-10-09 20:33:11.814 
Epoch 755/1000 
	 loss: 22.6460, MinusLogProbMetric: 22.6460, val_loss: 22.7859, val_MinusLogProbMetric: 22.7859

Epoch 755: val_loss did not improve from 22.78565
196/196 - 52s - loss: 22.6460 - MinusLogProbMetric: 22.6460 - val_loss: 22.7859 - val_MinusLogProbMetric: 22.7859 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 756/1000
2023-10-09 20:34:01.292 
Epoch 756/1000 
	 loss: 22.6635, MinusLogProbMetric: 22.6635, val_loss: 22.8382, val_MinusLogProbMetric: 22.8382

Epoch 756: val_loss did not improve from 22.78565
196/196 - 49s - loss: 22.6635 - MinusLogProbMetric: 22.6635 - val_loss: 22.8382 - val_MinusLogProbMetric: 22.8382 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 757/1000
2023-10-09 20:34:50.309 
Epoch 757/1000 
	 loss: 22.6619, MinusLogProbMetric: 22.6619, val_loss: 22.8027, val_MinusLogProbMetric: 22.8027

Epoch 757: val_loss did not improve from 22.78565
196/196 - 49s - loss: 22.6619 - MinusLogProbMetric: 22.6619 - val_loss: 22.8027 - val_MinusLogProbMetric: 22.8027 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 758/1000
2023-10-09 20:35:41.552 
Epoch 758/1000 
	 loss: 22.6541, MinusLogProbMetric: 22.6541, val_loss: 22.8734, val_MinusLogProbMetric: 22.8734

Epoch 758: val_loss did not improve from 22.78565
196/196 - 51s - loss: 22.6541 - MinusLogProbMetric: 22.6541 - val_loss: 22.8734 - val_MinusLogProbMetric: 22.8734 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 759/1000
2023-10-09 20:36:33.075 
Epoch 759/1000 
	 loss: 22.6612, MinusLogProbMetric: 22.6612, val_loss: 22.7518, val_MinusLogProbMetric: 22.7518

Epoch 759: val_loss improved from 22.78565 to 22.75177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.6612 - MinusLogProbMetric: 22.6612 - val_loss: 22.7518 - val_MinusLogProbMetric: 22.7518 - lr: 5.5556e-05 - 52s/epoch - 268ms/step
Epoch 760/1000
2023-10-09 20:37:25.094 
Epoch 760/1000 
	 loss: 22.6422, MinusLogProbMetric: 22.6422, val_loss: 22.8039, val_MinusLogProbMetric: 22.8039

Epoch 760: val_loss did not improve from 22.75177
196/196 - 51s - loss: 22.6422 - MinusLogProbMetric: 22.6422 - val_loss: 22.8039 - val_MinusLogProbMetric: 22.8039 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 761/1000
2023-10-09 20:38:14.763 
Epoch 761/1000 
	 loss: 22.6340, MinusLogProbMetric: 22.6340, val_loss: 22.7810, val_MinusLogProbMetric: 22.7810

Epoch 761: val_loss did not improve from 22.75177
196/196 - 50s - loss: 22.6340 - MinusLogProbMetric: 22.6340 - val_loss: 22.7810 - val_MinusLogProbMetric: 22.7810 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 762/1000
2023-10-09 20:39:05.510 
Epoch 762/1000 
	 loss: 22.6414, MinusLogProbMetric: 22.6414, val_loss: 22.7465, val_MinusLogProbMetric: 22.7465

Epoch 762: val_loss improved from 22.75177 to 22.74649, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.6414 - MinusLogProbMetric: 22.6414 - val_loss: 22.7465 - val_MinusLogProbMetric: 22.7465 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 763/1000
2023-10-09 20:39:57.114 
Epoch 763/1000 
	 loss: 22.6272, MinusLogProbMetric: 22.6272, val_loss: 22.7715, val_MinusLogProbMetric: 22.7715

Epoch 763: val_loss did not improve from 22.74649
196/196 - 51s - loss: 22.6272 - MinusLogProbMetric: 22.6272 - val_loss: 22.7715 - val_MinusLogProbMetric: 22.7715 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 764/1000
2023-10-09 20:40:49.960 
Epoch 764/1000 
	 loss: 22.5989, MinusLogProbMetric: 22.5989, val_loss: 22.7417, val_MinusLogProbMetric: 22.7417

Epoch 764: val_loss improved from 22.74649 to 22.74167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 22.5989 - MinusLogProbMetric: 22.5989 - val_loss: 22.7417 - val_MinusLogProbMetric: 22.7417 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 765/1000
2023-10-09 20:41:40.336 
Epoch 765/1000 
	 loss: 22.6011, MinusLogProbMetric: 22.6011, val_loss: 22.8622, val_MinusLogProbMetric: 22.8622

Epoch 765: val_loss did not improve from 22.74167
196/196 - 50s - loss: 22.6011 - MinusLogProbMetric: 22.6011 - val_loss: 22.8622 - val_MinusLogProbMetric: 22.8622 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 766/1000
2023-10-09 20:42:32.016 
Epoch 766/1000 
	 loss: 22.6142, MinusLogProbMetric: 22.6142, val_loss: 22.6765, val_MinusLogProbMetric: 22.6765

Epoch 766: val_loss improved from 22.74167 to 22.67654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 22.6142 - MinusLogProbMetric: 22.6142 - val_loss: 22.6765 - val_MinusLogProbMetric: 22.6765 - lr: 5.5556e-05 - 53s/epoch - 268ms/step
Epoch 767/1000
2023-10-09 20:43:25.510 
Epoch 767/1000 
	 loss: 22.5939, MinusLogProbMetric: 22.5939, val_loss: 22.7377, val_MinusLogProbMetric: 22.7377

Epoch 767: val_loss did not improve from 22.67654
196/196 - 53s - loss: 22.5939 - MinusLogProbMetric: 22.5939 - val_loss: 22.7377 - val_MinusLogProbMetric: 22.7377 - lr: 5.5556e-05 - 53s/epoch - 268ms/step
Epoch 768/1000
2023-10-09 20:44:14.602 
Epoch 768/1000 
	 loss: 22.5744, MinusLogProbMetric: 22.5744, val_loss: 22.7221, val_MinusLogProbMetric: 22.7221

Epoch 768: val_loss did not improve from 22.67654
196/196 - 49s - loss: 22.5744 - MinusLogProbMetric: 22.5744 - val_loss: 22.7221 - val_MinusLogProbMetric: 22.7221 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 769/1000
2023-10-09 20:45:05.222 
Epoch 769/1000 
	 loss: 22.5739, MinusLogProbMetric: 22.5739, val_loss: 22.7380, val_MinusLogProbMetric: 22.7380

Epoch 769: val_loss did not improve from 22.67654
196/196 - 51s - loss: 22.5739 - MinusLogProbMetric: 22.5739 - val_loss: 22.7380 - val_MinusLogProbMetric: 22.7380 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 770/1000
2023-10-09 20:45:57.487 
Epoch 770/1000 
	 loss: 22.5889, MinusLogProbMetric: 22.5889, val_loss: 22.7105, val_MinusLogProbMetric: 22.7105

Epoch 770: val_loss did not improve from 22.67654
196/196 - 52s - loss: 22.5889 - MinusLogProbMetric: 22.5889 - val_loss: 22.7105 - val_MinusLogProbMetric: 22.7105 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 771/1000
2023-10-09 20:46:49.480 
Epoch 771/1000 
	 loss: 22.5835, MinusLogProbMetric: 22.5835, val_loss: 22.7033, val_MinusLogProbMetric: 22.7033

Epoch 771: val_loss did not improve from 22.67654
196/196 - 52s - loss: 22.5835 - MinusLogProbMetric: 22.5835 - val_loss: 22.7033 - val_MinusLogProbMetric: 22.7033 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 772/1000
2023-10-09 20:47:42.707 
Epoch 772/1000 
	 loss: 22.5707, MinusLogProbMetric: 22.5707, val_loss: 22.6397, val_MinusLogProbMetric: 22.6397

Epoch 772: val_loss improved from 22.67654 to 22.63967, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 22.5707 - MinusLogProbMetric: 22.5707 - val_loss: 22.6397 - val_MinusLogProbMetric: 22.6397 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 773/1000
2023-10-09 20:48:32.987 
Epoch 773/1000 
	 loss: 22.5531, MinusLogProbMetric: 22.5531, val_loss: 22.8794, val_MinusLogProbMetric: 22.8794

Epoch 773: val_loss did not improve from 22.63967
196/196 - 50s - loss: 22.5531 - MinusLogProbMetric: 22.5531 - val_loss: 22.8794 - val_MinusLogProbMetric: 22.8794 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 774/1000
2023-10-09 20:49:23.343 
Epoch 774/1000 
	 loss: 22.5468, MinusLogProbMetric: 22.5468, val_loss: 22.7868, val_MinusLogProbMetric: 22.7868

Epoch 774: val_loss did not improve from 22.63967
196/196 - 50s - loss: 22.5468 - MinusLogProbMetric: 22.5468 - val_loss: 22.7868 - val_MinusLogProbMetric: 22.7868 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 775/1000
2023-10-09 20:50:16.120 
Epoch 775/1000 
	 loss: 22.5751, MinusLogProbMetric: 22.5751, val_loss: 22.9809, val_MinusLogProbMetric: 22.9809

Epoch 775: val_loss did not improve from 22.63967
196/196 - 53s - loss: 22.5751 - MinusLogProbMetric: 22.5751 - val_loss: 22.9809 - val_MinusLogProbMetric: 22.9809 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 776/1000
2023-10-09 20:51:06.613 
Epoch 776/1000 
	 loss: 22.5391, MinusLogProbMetric: 22.5391, val_loss: 22.7194, val_MinusLogProbMetric: 22.7194

Epoch 776: val_loss did not improve from 22.63967
196/196 - 50s - loss: 22.5391 - MinusLogProbMetric: 22.5391 - val_loss: 22.7194 - val_MinusLogProbMetric: 22.7194 - lr: 5.5556e-05 - 50s/epoch - 258ms/step
Epoch 777/1000
2023-10-09 20:51:57.958 
Epoch 777/1000 
	 loss: 22.5300, MinusLogProbMetric: 22.5300, val_loss: 22.6753, val_MinusLogProbMetric: 22.6753

Epoch 777: val_loss did not improve from 22.63967
196/196 - 51s - loss: 22.5300 - MinusLogProbMetric: 22.5300 - val_loss: 22.6753 - val_MinusLogProbMetric: 22.6753 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 778/1000
2023-10-09 20:52:48.244 
Epoch 778/1000 
	 loss: 22.5416, MinusLogProbMetric: 22.5416, val_loss: 22.6674, val_MinusLogProbMetric: 22.6674

Epoch 778: val_loss did not improve from 22.63967
196/196 - 50s - loss: 22.5416 - MinusLogProbMetric: 22.5416 - val_loss: 22.6674 - val_MinusLogProbMetric: 22.6674 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 779/1000
2023-10-09 20:53:38.882 
Epoch 779/1000 
	 loss: 22.5313, MinusLogProbMetric: 22.5313, val_loss: 22.6090, val_MinusLogProbMetric: 22.6090

Epoch 779: val_loss improved from 22.63967 to 22.60901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.5313 - MinusLogProbMetric: 22.5313 - val_loss: 22.6090 - val_MinusLogProbMetric: 22.6090 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 780/1000
2023-10-09 20:54:29.451 
Epoch 780/1000 
	 loss: 22.5019, MinusLogProbMetric: 22.5019, val_loss: 22.6473, val_MinusLogProbMetric: 22.6473

Epoch 780: val_loss did not improve from 22.60901
196/196 - 50s - loss: 22.5019 - MinusLogProbMetric: 22.5019 - val_loss: 22.6473 - val_MinusLogProbMetric: 22.6473 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 781/1000
2023-10-09 20:55:20.125 
Epoch 781/1000 
	 loss: 22.5320, MinusLogProbMetric: 22.5320, val_loss: 22.6425, val_MinusLogProbMetric: 22.6425

Epoch 781: val_loss did not improve from 22.60901
196/196 - 51s - loss: 22.5320 - MinusLogProbMetric: 22.5320 - val_loss: 22.6425 - val_MinusLogProbMetric: 22.6425 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 782/1000
2023-10-09 20:56:09.767 
Epoch 782/1000 
	 loss: 22.5023, MinusLogProbMetric: 22.5023, val_loss: 22.6032, val_MinusLogProbMetric: 22.6032

Epoch 782: val_loss improved from 22.60901 to 22.60321, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 22.5023 - MinusLogProbMetric: 22.5023 - val_loss: 22.6032 - val_MinusLogProbMetric: 22.6032 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 783/1000
2023-10-09 20:57:01.238 
Epoch 783/1000 
	 loss: 22.4982, MinusLogProbMetric: 22.4982, val_loss: 22.6238, val_MinusLogProbMetric: 22.6238

Epoch 783: val_loss did not improve from 22.60321
196/196 - 51s - loss: 22.4982 - MinusLogProbMetric: 22.4982 - val_loss: 22.6238 - val_MinusLogProbMetric: 22.6238 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 784/1000
2023-10-09 20:57:51.971 
Epoch 784/1000 
	 loss: 22.4813, MinusLogProbMetric: 22.4813, val_loss: 22.8407, val_MinusLogProbMetric: 22.8407

Epoch 784: val_loss did not improve from 22.60321
196/196 - 51s - loss: 22.4813 - MinusLogProbMetric: 22.4813 - val_loss: 22.8407 - val_MinusLogProbMetric: 22.8407 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 785/1000
2023-10-09 20:58:42.916 
Epoch 785/1000 
	 loss: 22.4864, MinusLogProbMetric: 22.4864, val_loss: 22.5953, val_MinusLogProbMetric: 22.5953

Epoch 785: val_loss improved from 22.60321 to 22.59533, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.4864 - MinusLogProbMetric: 22.4864 - val_loss: 22.5953 - val_MinusLogProbMetric: 22.5953 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 786/1000
2023-10-09 20:59:35.283 
Epoch 786/1000 
	 loss: 22.4898, MinusLogProbMetric: 22.4898, val_loss: 22.9856, val_MinusLogProbMetric: 22.9856

Epoch 786: val_loss did not improve from 22.59533
196/196 - 52s - loss: 22.4898 - MinusLogProbMetric: 22.4898 - val_loss: 22.9856 - val_MinusLogProbMetric: 22.9856 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 787/1000
2023-10-09 21:00:25.438 
Epoch 787/1000 
	 loss: 22.4903, MinusLogProbMetric: 22.4903, val_loss: 22.5664, val_MinusLogProbMetric: 22.5664

Epoch 787: val_loss improved from 22.59533 to 22.56644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.4903 - MinusLogProbMetric: 22.4903 - val_loss: 22.5664 - val_MinusLogProbMetric: 22.5664 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 788/1000
2023-10-09 21:01:18.681 
Epoch 788/1000 
	 loss: 22.4636, MinusLogProbMetric: 22.4636, val_loss: 22.6551, val_MinusLogProbMetric: 22.6551

Epoch 788: val_loss did not improve from 22.56644
196/196 - 52s - loss: 22.4636 - MinusLogProbMetric: 22.4636 - val_loss: 22.6551 - val_MinusLogProbMetric: 22.6551 - lr: 5.5556e-05 - 52s/epoch - 268ms/step
Epoch 789/1000
2023-10-09 21:02:07.747 
Epoch 789/1000 
	 loss: 22.4870, MinusLogProbMetric: 22.4870, val_loss: 22.7405, val_MinusLogProbMetric: 22.7405

Epoch 789: val_loss did not improve from 22.56644
196/196 - 49s - loss: 22.4870 - MinusLogProbMetric: 22.4870 - val_loss: 22.7405 - val_MinusLogProbMetric: 22.7405 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 790/1000
2023-10-09 21:02:58.832 
Epoch 790/1000 
	 loss: 22.4698, MinusLogProbMetric: 22.4698, val_loss: 22.5959, val_MinusLogProbMetric: 22.5959

Epoch 790: val_loss did not improve from 22.56644
196/196 - 51s - loss: 22.4698 - MinusLogProbMetric: 22.4698 - val_loss: 22.5959 - val_MinusLogProbMetric: 22.5959 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 791/1000
2023-10-09 21:03:50.973 
Epoch 791/1000 
	 loss: 22.4704, MinusLogProbMetric: 22.4704, val_loss: 22.5360, val_MinusLogProbMetric: 22.5360

Epoch 791: val_loss improved from 22.56644 to 22.53602, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 22.4704 - MinusLogProbMetric: 22.4704 - val_loss: 22.5360 - val_MinusLogProbMetric: 22.5360 - lr: 5.5556e-05 - 53s/epoch - 270ms/step
Epoch 792/1000
2023-10-09 21:04:43.494 
Epoch 792/1000 
	 loss: 22.4669, MinusLogProbMetric: 22.4669, val_loss: 22.5883, val_MinusLogProbMetric: 22.5883

Epoch 792: val_loss did not improve from 22.53602
196/196 - 52s - loss: 22.4669 - MinusLogProbMetric: 22.4669 - val_loss: 22.5883 - val_MinusLogProbMetric: 22.5883 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 793/1000
2023-10-09 21:05:33.112 
Epoch 793/1000 
	 loss: 22.4661, MinusLogProbMetric: 22.4661, val_loss: 22.5855, val_MinusLogProbMetric: 22.5855

Epoch 793: val_loss did not improve from 22.53602
196/196 - 50s - loss: 22.4661 - MinusLogProbMetric: 22.4661 - val_loss: 22.5855 - val_MinusLogProbMetric: 22.5855 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 794/1000
2023-10-09 21:06:25.019 
Epoch 794/1000 
	 loss: 22.4232, MinusLogProbMetric: 22.4232, val_loss: 22.6636, val_MinusLogProbMetric: 22.6636

Epoch 794: val_loss did not improve from 22.53602
196/196 - 52s - loss: 22.4232 - MinusLogProbMetric: 22.4232 - val_loss: 22.6636 - val_MinusLogProbMetric: 22.6636 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 795/1000
2023-10-09 21:07:14.698 
Epoch 795/1000 
	 loss: 22.4363, MinusLogProbMetric: 22.4363, val_loss: 22.5020, val_MinusLogProbMetric: 22.5020

Epoch 795: val_loss improved from 22.53602 to 22.50201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 22.4363 - MinusLogProbMetric: 22.4363 - val_loss: 22.5020 - val_MinusLogProbMetric: 22.5020 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 796/1000
2023-10-09 21:08:04.754 
Epoch 796/1000 
	 loss: 22.4205, MinusLogProbMetric: 22.4205, val_loss: 22.5315, val_MinusLogProbMetric: 22.5315

Epoch 796: val_loss did not improve from 22.50201
196/196 - 49s - loss: 22.4205 - MinusLogProbMetric: 22.4205 - val_loss: 22.5315 - val_MinusLogProbMetric: 22.5315 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 797/1000
2023-10-09 21:08:57.052 
Epoch 797/1000 
	 loss: 22.4160, MinusLogProbMetric: 22.4160, val_loss: 22.5138, val_MinusLogProbMetric: 22.5138

Epoch 797: val_loss did not improve from 22.50201
196/196 - 52s - loss: 22.4160 - MinusLogProbMetric: 22.4160 - val_loss: 22.5138 - val_MinusLogProbMetric: 22.5138 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 798/1000
2023-10-09 21:09:46.640 
Epoch 798/1000 
	 loss: 22.4196, MinusLogProbMetric: 22.4196, val_loss: 22.5697, val_MinusLogProbMetric: 22.5697

Epoch 798: val_loss did not improve from 22.50201
196/196 - 50s - loss: 22.4196 - MinusLogProbMetric: 22.4196 - val_loss: 22.5697 - val_MinusLogProbMetric: 22.5697 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 799/1000
2023-10-09 21:10:37.082 
Epoch 799/1000 
	 loss: 22.4149, MinusLogProbMetric: 22.4149, val_loss: 22.5569, val_MinusLogProbMetric: 22.5569

Epoch 799: val_loss did not improve from 22.50201
196/196 - 50s - loss: 22.4149 - MinusLogProbMetric: 22.4149 - val_loss: 22.5569 - val_MinusLogProbMetric: 22.5569 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 800/1000
2023-10-09 21:11:29.882 
Epoch 800/1000 
	 loss: 22.4044, MinusLogProbMetric: 22.4044, val_loss: 22.5548, val_MinusLogProbMetric: 22.5548

Epoch 800: val_loss did not improve from 22.50201
196/196 - 53s - loss: 22.4044 - MinusLogProbMetric: 22.4044 - val_loss: 22.5548 - val_MinusLogProbMetric: 22.5548 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 801/1000
2023-10-09 21:12:21.043 
Epoch 801/1000 
	 loss: 22.3813, MinusLogProbMetric: 22.3813, val_loss: 22.5454, val_MinusLogProbMetric: 22.5454

Epoch 801: val_loss did not improve from 22.50201
196/196 - 51s - loss: 22.3813 - MinusLogProbMetric: 22.3813 - val_loss: 22.5454 - val_MinusLogProbMetric: 22.5454 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 802/1000
2023-10-09 21:13:11.198 
Epoch 802/1000 
	 loss: 22.4031, MinusLogProbMetric: 22.4031, val_loss: 22.4955, val_MinusLogProbMetric: 22.4955

Epoch 802: val_loss improved from 22.50201 to 22.49553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.4031 - MinusLogProbMetric: 22.4031 - val_loss: 22.4955 - val_MinusLogProbMetric: 22.4955 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 803/1000
2023-10-09 21:14:03.781 
Epoch 803/1000 
	 loss: 22.3862, MinusLogProbMetric: 22.3862, val_loss: 22.5037, val_MinusLogProbMetric: 22.5037

Epoch 803: val_loss did not improve from 22.49553
196/196 - 52s - loss: 22.3862 - MinusLogProbMetric: 22.3862 - val_loss: 22.5037 - val_MinusLogProbMetric: 22.5037 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 804/1000
2023-10-09 21:14:55.352 
Epoch 804/1000 
	 loss: 22.4129, MinusLogProbMetric: 22.4129, val_loss: 22.5366, val_MinusLogProbMetric: 22.5366

Epoch 804: val_loss did not improve from 22.49553
196/196 - 52s - loss: 22.4129 - MinusLogProbMetric: 22.4129 - val_loss: 22.5366 - val_MinusLogProbMetric: 22.5366 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 805/1000
2023-10-09 21:15:47.815 
Epoch 805/1000 
	 loss: 22.3823, MinusLogProbMetric: 22.3823, val_loss: 22.4269, val_MinusLogProbMetric: 22.4269

Epoch 805: val_loss improved from 22.49553 to 22.42686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 22.3823 - MinusLogProbMetric: 22.3823 - val_loss: 22.4269 - val_MinusLogProbMetric: 22.4269 - lr: 5.5556e-05 - 53s/epoch - 271ms/step
Epoch 806/1000
2023-10-09 21:16:40.926 
Epoch 806/1000 
	 loss: 22.4084, MinusLogProbMetric: 22.4084, val_loss: 22.4794, val_MinusLogProbMetric: 22.4794

Epoch 806: val_loss did not improve from 22.42686
196/196 - 52s - loss: 22.4084 - MinusLogProbMetric: 22.4084 - val_loss: 22.4794 - val_MinusLogProbMetric: 22.4794 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 807/1000
2023-10-09 21:17:33.733 
Epoch 807/1000 
	 loss: 22.3908, MinusLogProbMetric: 22.3908, val_loss: 22.4725, val_MinusLogProbMetric: 22.4725

Epoch 807: val_loss did not improve from 22.42686
196/196 - 53s - loss: 22.3908 - MinusLogProbMetric: 22.3908 - val_loss: 22.4725 - val_MinusLogProbMetric: 22.4725 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 808/1000
2023-10-09 21:18:25.570 
Epoch 808/1000 
	 loss: 22.3636, MinusLogProbMetric: 22.3636, val_loss: 22.4783, val_MinusLogProbMetric: 22.4783

Epoch 808: val_loss did not improve from 22.42686
196/196 - 52s - loss: 22.3636 - MinusLogProbMetric: 22.3636 - val_loss: 22.4783 - val_MinusLogProbMetric: 22.4783 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 809/1000
2023-10-09 21:19:17.344 
Epoch 809/1000 
	 loss: 22.3701, MinusLogProbMetric: 22.3701, val_loss: 22.6264, val_MinusLogProbMetric: 22.6264

Epoch 809: val_loss did not improve from 22.42686
196/196 - 52s - loss: 22.3701 - MinusLogProbMetric: 22.3701 - val_loss: 22.6264 - val_MinusLogProbMetric: 22.6264 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 810/1000
2023-10-09 21:20:09.229 
Epoch 810/1000 
	 loss: 22.3487, MinusLogProbMetric: 22.3487, val_loss: 22.4230, val_MinusLogProbMetric: 22.4230

Epoch 810: val_loss improved from 22.42686 to 22.42301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 22.3487 - MinusLogProbMetric: 22.3487 - val_loss: 22.4230 - val_MinusLogProbMetric: 22.4230 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 811/1000
2023-10-09 21:21:01.772 
Epoch 811/1000 
	 loss: 22.3354, MinusLogProbMetric: 22.3354, val_loss: 22.5483, val_MinusLogProbMetric: 22.5483

Epoch 811: val_loss did not improve from 22.42301
196/196 - 52s - loss: 22.3354 - MinusLogProbMetric: 22.3354 - val_loss: 22.5483 - val_MinusLogProbMetric: 22.5483 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 812/1000
2023-10-09 21:21:53.092 
Epoch 812/1000 
	 loss: 22.3290, MinusLogProbMetric: 22.3290, val_loss: 22.4668, val_MinusLogProbMetric: 22.4668

Epoch 812: val_loss did not improve from 22.42301
196/196 - 51s - loss: 22.3290 - MinusLogProbMetric: 22.3290 - val_loss: 22.4668 - val_MinusLogProbMetric: 22.4668 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 813/1000
2023-10-09 21:22:44.035 
Epoch 813/1000 
	 loss: 22.3332, MinusLogProbMetric: 22.3332, val_loss: 22.5198, val_MinusLogProbMetric: 22.5198

Epoch 813: val_loss did not improve from 22.42301
196/196 - 51s - loss: 22.3332 - MinusLogProbMetric: 22.3332 - val_loss: 22.5198 - val_MinusLogProbMetric: 22.5198 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 814/1000
2023-10-09 21:23:33.323 
Epoch 814/1000 
	 loss: 22.3369, MinusLogProbMetric: 22.3369, val_loss: 22.4552, val_MinusLogProbMetric: 22.4552

Epoch 814: val_loss did not improve from 22.42301
196/196 - 49s - loss: 22.3369 - MinusLogProbMetric: 22.3369 - val_loss: 22.4552 - val_MinusLogProbMetric: 22.4552 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 815/1000
2023-10-09 21:24:23.290 
Epoch 815/1000 
	 loss: 22.3339, MinusLogProbMetric: 22.3339, val_loss: 22.4164, val_MinusLogProbMetric: 22.4164

Epoch 815: val_loss improved from 22.42301 to 22.41643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.3339 - MinusLogProbMetric: 22.3339 - val_loss: 22.4164 - val_MinusLogProbMetric: 22.4164 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 816/1000
2023-10-09 21:25:15.285 
Epoch 816/1000 
	 loss: 22.3160, MinusLogProbMetric: 22.3160, val_loss: 22.4248, val_MinusLogProbMetric: 22.4248

Epoch 816: val_loss did not improve from 22.41643
196/196 - 51s - loss: 22.3160 - MinusLogProbMetric: 22.3160 - val_loss: 22.4248 - val_MinusLogProbMetric: 22.4248 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 817/1000
2023-10-09 21:26:06.485 
Epoch 817/1000 
	 loss: 22.3156, MinusLogProbMetric: 22.3156, val_loss: 22.4093, val_MinusLogProbMetric: 22.4093

Epoch 817: val_loss improved from 22.41643 to 22.40933, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.3156 - MinusLogProbMetric: 22.3156 - val_loss: 22.4093 - val_MinusLogProbMetric: 22.4093 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 818/1000
2023-10-09 21:26:58.849 
Epoch 818/1000 
	 loss: 22.3184, MinusLogProbMetric: 22.3184, val_loss: 22.5221, val_MinusLogProbMetric: 22.5221

Epoch 818: val_loss did not improve from 22.40933
196/196 - 52s - loss: 22.3184 - MinusLogProbMetric: 22.3184 - val_loss: 22.5221 - val_MinusLogProbMetric: 22.5221 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 819/1000
2023-10-09 21:27:49.097 
Epoch 819/1000 
	 loss: 22.3167, MinusLogProbMetric: 22.3167, val_loss: 22.4908, val_MinusLogProbMetric: 22.4908

Epoch 819: val_loss did not improve from 22.40933
196/196 - 50s - loss: 22.3167 - MinusLogProbMetric: 22.3167 - val_loss: 22.4908 - val_MinusLogProbMetric: 22.4908 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 820/1000
2023-10-09 21:28:41.872 
Epoch 820/1000 
	 loss: 22.3290, MinusLogProbMetric: 22.3290, val_loss: 22.4431, val_MinusLogProbMetric: 22.4431

Epoch 820: val_loss did not improve from 22.40933
196/196 - 53s - loss: 22.3290 - MinusLogProbMetric: 22.3290 - val_loss: 22.4431 - val_MinusLogProbMetric: 22.4431 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 821/1000
2023-10-09 21:29:32.306 
Epoch 821/1000 
	 loss: 22.2674, MinusLogProbMetric: 22.2674, val_loss: 22.4365, val_MinusLogProbMetric: 22.4365

Epoch 821: val_loss did not improve from 22.40933
196/196 - 50s - loss: 22.2674 - MinusLogProbMetric: 22.2674 - val_loss: 22.4365 - val_MinusLogProbMetric: 22.4365 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 822/1000
2023-10-09 21:30:22.550 
Epoch 822/1000 
	 loss: 22.2816, MinusLogProbMetric: 22.2816, val_loss: 22.4036, val_MinusLogProbMetric: 22.4036

Epoch 822: val_loss improved from 22.40933 to 22.40361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.2816 - MinusLogProbMetric: 22.2816 - val_loss: 22.4036 - val_MinusLogProbMetric: 22.4036 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 823/1000
2023-10-09 21:31:14.351 
Epoch 823/1000 
	 loss: 22.2957, MinusLogProbMetric: 22.2957, val_loss: 22.5715, val_MinusLogProbMetric: 22.5715

Epoch 823: val_loss did not improve from 22.40361
196/196 - 51s - loss: 22.2957 - MinusLogProbMetric: 22.2957 - val_loss: 22.5715 - val_MinusLogProbMetric: 22.5715 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 824/1000
2023-10-09 21:32:05.072 
Epoch 824/1000 
	 loss: 22.2813, MinusLogProbMetric: 22.2813, val_loss: 22.3576, val_MinusLogProbMetric: 22.3576

Epoch 824: val_loss improved from 22.40361 to 22.35759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.2813 - MinusLogProbMetric: 22.2813 - val_loss: 22.3576 - val_MinusLogProbMetric: 22.3576 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 825/1000
2023-10-09 21:32:56.089 
Epoch 825/1000 
	 loss: 22.2736, MinusLogProbMetric: 22.2736, val_loss: 22.4056, val_MinusLogProbMetric: 22.4056

Epoch 825: val_loss did not improve from 22.35759
196/196 - 50s - loss: 22.2736 - MinusLogProbMetric: 22.2736 - val_loss: 22.4056 - val_MinusLogProbMetric: 22.4056 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 826/1000
2023-10-09 21:33:46.537 
Epoch 826/1000 
	 loss: 22.2730, MinusLogProbMetric: 22.2730, val_loss: 22.4288, val_MinusLogProbMetric: 22.4288

Epoch 826: val_loss did not improve from 22.35759
196/196 - 50s - loss: 22.2730 - MinusLogProbMetric: 22.2730 - val_loss: 22.4288 - val_MinusLogProbMetric: 22.4288 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 827/1000
2023-10-09 21:34:38.557 
Epoch 827/1000 
	 loss: 22.2540, MinusLogProbMetric: 22.2540, val_loss: 22.3696, val_MinusLogProbMetric: 22.3696

Epoch 827: val_loss did not improve from 22.35759
196/196 - 52s - loss: 22.2540 - MinusLogProbMetric: 22.2540 - val_loss: 22.3696 - val_MinusLogProbMetric: 22.3696 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 828/1000
2023-10-09 21:35:31.302 
Epoch 828/1000 
	 loss: 22.2588, MinusLogProbMetric: 22.2588, val_loss: 22.4942, val_MinusLogProbMetric: 22.4942

Epoch 828: val_loss did not improve from 22.35759
196/196 - 53s - loss: 22.2588 - MinusLogProbMetric: 22.2588 - val_loss: 22.4942 - val_MinusLogProbMetric: 22.4942 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 829/1000
2023-10-09 21:36:22.169 
Epoch 829/1000 
	 loss: 22.2629, MinusLogProbMetric: 22.2629, val_loss: 22.3172, val_MinusLogProbMetric: 22.3172

Epoch 829: val_loss improved from 22.35759 to 22.31721, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.2629 - MinusLogProbMetric: 22.2629 - val_loss: 22.3172 - val_MinusLogProbMetric: 22.3172 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 830/1000
2023-10-09 21:37:12.599 
Epoch 830/1000 
	 loss: 22.2332, MinusLogProbMetric: 22.2332, val_loss: 22.4362, val_MinusLogProbMetric: 22.4362

Epoch 830: val_loss did not improve from 22.31721
196/196 - 50s - loss: 22.2332 - MinusLogProbMetric: 22.2332 - val_loss: 22.4362 - val_MinusLogProbMetric: 22.4362 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 831/1000
2023-10-09 21:38:04.083 
Epoch 831/1000 
	 loss: 22.2343, MinusLogProbMetric: 22.2343, val_loss: 22.4607, val_MinusLogProbMetric: 22.4607

Epoch 831: val_loss did not improve from 22.31721
196/196 - 51s - loss: 22.2343 - MinusLogProbMetric: 22.2343 - val_loss: 22.4607 - val_MinusLogProbMetric: 22.4607 - lr: 5.5556e-05 - 51s/epoch - 263ms/step
Epoch 832/1000
2023-10-09 21:38:54.715 
Epoch 832/1000 
	 loss: 22.2203, MinusLogProbMetric: 22.2203, val_loss: 22.4319, val_MinusLogProbMetric: 22.4319

Epoch 832: val_loss did not improve from 22.31721
196/196 - 51s - loss: 22.2203 - MinusLogProbMetric: 22.2203 - val_loss: 22.4319 - val_MinusLogProbMetric: 22.4319 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 833/1000
2023-10-09 21:39:45.697 
Epoch 833/1000 
	 loss: 22.2497, MinusLogProbMetric: 22.2497, val_loss: 22.3643, val_MinusLogProbMetric: 22.3643

Epoch 833: val_loss did not improve from 22.31721
196/196 - 51s - loss: 22.2497 - MinusLogProbMetric: 22.2497 - val_loss: 22.3643 - val_MinusLogProbMetric: 22.3643 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 834/1000
2023-10-09 21:40:36.438 
Epoch 834/1000 
	 loss: 22.2357, MinusLogProbMetric: 22.2357, val_loss: 22.4316, val_MinusLogProbMetric: 22.4316

Epoch 834: val_loss did not improve from 22.31721
196/196 - 51s - loss: 22.2357 - MinusLogProbMetric: 22.2357 - val_loss: 22.4316 - val_MinusLogProbMetric: 22.4316 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 835/1000
2023-10-09 21:41:28.913 
Epoch 835/1000 
	 loss: 22.2153, MinusLogProbMetric: 22.2153, val_loss: 22.4440, val_MinusLogProbMetric: 22.4440

Epoch 835: val_loss did not improve from 22.31721
196/196 - 52s - loss: 22.2153 - MinusLogProbMetric: 22.2153 - val_loss: 22.4440 - val_MinusLogProbMetric: 22.4440 - lr: 5.5556e-05 - 52s/epoch - 268ms/step
Epoch 836/1000
2023-10-09 21:42:19.673 
Epoch 836/1000 
	 loss: 22.2034, MinusLogProbMetric: 22.2034, val_loss: 22.2951, val_MinusLogProbMetric: 22.2951

Epoch 836: val_loss improved from 22.31721 to 22.29508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.2034 - MinusLogProbMetric: 22.2034 - val_loss: 22.2951 - val_MinusLogProbMetric: 22.2951 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 837/1000
2023-10-09 21:43:10.513 
Epoch 837/1000 
	 loss: 22.2072, MinusLogProbMetric: 22.2072, val_loss: 22.2820, val_MinusLogProbMetric: 22.2820

Epoch 837: val_loss improved from 22.29508 to 22.28202, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.2072 - MinusLogProbMetric: 22.2072 - val_loss: 22.2820 - val_MinusLogProbMetric: 22.2820 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 838/1000
2023-10-09 21:44:00.843 
Epoch 838/1000 
	 loss: 22.2050, MinusLogProbMetric: 22.2050, val_loss: 22.3907, val_MinusLogProbMetric: 22.3907

Epoch 838: val_loss did not improve from 22.28202
196/196 - 50s - loss: 22.2050 - MinusLogProbMetric: 22.2050 - val_loss: 22.3907 - val_MinusLogProbMetric: 22.3907 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 839/1000
2023-10-09 21:44:52.104 
Epoch 839/1000 
	 loss: 22.1776, MinusLogProbMetric: 22.1776, val_loss: 22.2889, val_MinusLogProbMetric: 22.2889

Epoch 839: val_loss did not improve from 22.28202
196/196 - 51s - loss: 22.1776 - MinusLogProbMetric: 22.1776 - val_loss: 22.2889 - val_MinusLogProbMetric: 22.2889 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 840/1000
2023-10-09 21:45:43.614 
Epoch 840/1000 
	 loss: 22.2062, MinusLogProbMetric: 22.2062, val_loss: 22.5570, val_MinusLogProbMetric: 22.5570

Epoch 840: val_loss did not improve from 22.28202
196/196 - 52s - loss: 22.2062 - MinusLogProbMetric: 22.2062 - val_loss: 22.5570 - val_MinusLogProbMetric: 22.5570 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 841/1000
2023-10-09 21:46:33.546 
Epoch 841/1000 
	 loss: 22.1942, MinusLogProbMetric: 22.1942, val_loss: 22.2734, val_MinusLogProbMetric: 22.2734

Epoch 841: val_loss improved from 22.28202 to 22.27337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.1942 - MinusLogProbMetric: 22.1942 - val_loss: 22.2734 - val_MinusLogProbMetric: 22.2734 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 842/1000
2023-10-09 21:47:26.118 
Epoch 842/1000 
	 loss: 22.1711, MinusLogProbMetric: 22.1711, val_loss: 22.2601, val_MinusLogProbMetric: 22.2601

Epoch 842: val_loss improved from 22.27337 to 22.26010, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.1711 - MinusLogProbMetric: 22.1711 - val_loss: 22.2601 - val_MinusLogProbMetric: 22.2601 - lr: 5.5556e-05 - 52s/epoch - 268ms/step
Epoch 843/1000
2023-10-09 21:48:17.407 
Epoch 843/1000 
	 loss: 22.1789, MinusLogProbMetric: 22.1789, val_loss: 22.3482, val_MinusLogProbMetric: 22.3482

Epoch 843: val_loss did not improve from 22.26010
196/196 - 51s - loss: 22.1789 - MinusLogProbMetric: 22.1789 - val_loss: 22.3482 - val_MinusLogProbMetric: 22.3482 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 844/1000
2023-10-09 21:49:08.379 
Epoch 844/1000 
	 loss: 22.1713, MinusLogProbMetric: 22.1713, val_loss: 22.3506, val_MinusLogProbMetric: 22.3506

Epoch 844: val_loss did not improve from 22.26010
196/196 - 51s - loss: 22.1713 - MinusLogProbMetric: 22.1713 - val_loss: 22.3506 - val_MinusLogProbMetric: 22.3506 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 845/1000
2023-10-09 21:50:00.374 
Epoch 845/1000 
	 loss: 22.1628, MinusLogProbMetric: 22.1628, val_loss: 22.3183, val_MinusLogProbMetric: 22.3183

Epoch 845: val_loss did not improve from 22.26010
196/196 - 52s - loss: 22.1628 - MinusLogProbMetric: 22.1628 - val_loss: 22.3183 - val_MinusLogProbMetric: 22.3183 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 846/1000
2023-10-09 21:50:51.393 
Epoch 846/1000 
	 loss: 22.1679, MinusLogProbMetric: 22.1679, val_loss: 22.3494, val_MinusLogProbMetric: 22.3494

Epoch 846: val_loss did not improve from 22.26010
196/196 - 51s - loss: 22.1679 - MinusLogProbMetric: 22.1679 - val_loss: 22.3494 - val_MinusLogProbMetric: 22.3494 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 847/1000
2023-10-09 21:51:43.137 
Epoch 847/1000 
	 loss: 22.1438, MinusLogProbMetric: 22.1438, val_loss: 22.2885, val_MinusLogProbMetric: 22.2885

Epoch 847: val_loss did not improve from 22.26010
196/196 - 52s - loss: 22.1438 - MinusLogProbMetric: 22.1438 - val_loss: 22.2885 - val_MinusLogProbMetric: 22.2885 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 848/1000
2023-10-09 21:52:33.144 
Epoch 848/1000 
	 loss: 22.1458, MinusLogProbMetric: 22.1458, val_loss: 22.4067, val_MinusLogProbMetric: 22.4067

Epoch 848: val_loss did not improve from 22.26010
196/196 - 50s - loss: 22.1458 - MinusLogProbMetric: 22.1458 - val_loss: 22.4067 - val_MinusLogProbMetric: 22.4067 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 849/1000
2023-10-09 21:53:24.199 
Epoch 849/1000 
	 loss: 22.1482, MinusLogProbMetric: 22.1482, val_loss: 22.4785, val_MinusLogProbMetric: 22.4785

Epoch 849: val_loss did not improve from 22.26010
196/196 - 51s - loss: 22.1482 - MinusLogProbMetric: 22.1482 - val_loss: 22.4785 - val_MinusLogProbMetric: 22.4785 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 850/1000
2023-10-09 21:54:15.482 
Epoch 850/1000 
	 loss: 22.1349, MinusLogProbMetric: 22.1349, val_loss: 22.2923, val_MinusLogProbMetric: 22.2923

Epoch 850: val_loss did not improve from 22.26010
196/196 - 51s - loss: 22.1349 - MinusLogProbMetric: 22.1349 - val_loss: 22.2923 - val_MinusLogProbMetric: 22.2923 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 851/1000
2023-10-09 21:55:05.897 
Epoch 851/1000 
	 loss: 22.1393, MinusLogProbMetric: 22.1393, val_loss: 22.2469, val_MinusLogProbMetric: 22.2469

Epoch 851: val_loss improved from 22.26010 to 22.24694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.1393 - MinusLogProbMetric: 22.1393 - val_loss: 22.2469 - val_MinusLogProbMetric: 22.2469 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 852/1000
2023-10-09 21:55:57.047 
Epoch 852/1000 
	 loss: 22.1348, MinusLogProbMetric: 22.1348, val_loss: 22.2345, val_MinusLogProbMetric: 22.2345

Epoch 852: val_loss improved from 22.24694 to 22.23453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.1348 - MinusLogProbMetric: 22.1348 - val_loss: 22.2345 - val_MinusLogProbMetric: 22.2345 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 853/1000
2023-10-09 21:56:47.428 
Epoch 853/1000 
	 loss: 22.1227, MinusLogProbMetric: 22.1227, val_loss: 22.5212, val_MinusLogProbMetric: 22.5212

Epoch 853: val_loss did not improve from 22.23453
196/196 - 50s - loss: 22.1227 - MinusLogProbMetric: 22.1227 - val_loss: 22.5212 - val_MinusLogProbMetric: 22.5212 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 854/1000
2023-10-09 21:57:38.794 
Epoch 854/1000 
	 loss: 22.0963, MinusLogProbMetric: 22.0963, val_loss: 22.2286, val_MinusLogProbMetric: 22.2286

Epoch 854: val_loss improved from 22.23453 to 22.22865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.0963 - MinusLogProbMetric: 22.0963 - val_loss: 22.2286 - val_MinusLogProbMetric: 22.2286 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 855/1000
2023-10-09 21:58:34.297 
Epoch 855/1000 
	 loss: 22.1339, MinusLogProbMetric: 22.1339, val_loss: 22.2409, val_MinusLogProbMetric: 22.2409

Epoch 855: val_loss did not improve from 22.22865
196/196 - 55s - loss: 22.1339 - MinusLogProbMetric: 22.1339 - val_loss: 22.2409 - val_MinusLogProbMetric: 22.2409 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 856/1000
2023-10-09 21:59:22.946 
Epoch 856/1000 
	 loss: 22.1294, MinusLogProbMetric: 22.1294, val_loss: 22.2035, val_MinusLogProbMetric: 22.2035

Epoch 856: val_loss improved from 22.22865 to 22.20354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 49s - loss: 22.1294 - MinusLogProbMetric: 22.1294 - val_loss: 22.2035 - val_MinusLogProbMetric: 22.2035 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 857/1000
2023-10-09 22:00:13.952 
Epoch 857/1000 
	 loss: 22.7568, MinusLogProbMetric: 22.7568, val_loss: 22.3315, val_MinusLogProbMetric: 22.3315

Epoch 857: val_loss did not improve from 22.20354
196/196 - 50s - loss: 22.7568 - MinusLogProbMetric: 22.7568 - val_loss: 22.3315 - val_MinusLogProbMetric: 22.3315 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 858/1000
2023-10-09 22:01:03.555 
Epoch 858/1000 
	 loss: 22.1160, MinusLogProbMetric: 22.1160, val_loss: 22.2754, val_MinusLogProbMetric: 22.2754

Epoch 858: val_loss did not improve from 22.20354
196/196 - 50s - loss: 22.1160 - MinusLogProbMetric: 22.1160 - val_loss: 22.2754 - val_MinusLogProbMetric: 22.2754 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 859/1000
2023-10-09 22:01:56.012 
Epoch 859/1000 
	 loss: 22.0765, MinusLogProbMetric: 22.0765, val_loss: 22.3560, val_MinusLogProbMetric: 22.3560

Epoch 859: val_loss did not improve from 22.20354
196/196 - 52s - loss: 22.0765 - MinusLogProbMetric: 22.0765 - val_loss: 22.3560 - val_MinusLogProbMetric: 22.3560 - lr: 5.5556e-05 - 52s/epoch - 268ms/step
Epoch 860/1000
2023-10-09 22:02:47.276 
Epoch 860/1000 
	 loss: 22.0897, MinusLogProbMetric: 22.0897, val_loss: 22.1934, val_MinusLogProbMetric: 22.1934

Epoch 860: val_loss improved from 22.20354 to 22.19340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.0897 - MinusLogProbMetric: 22.0897 - val_loss: 22.1934 - val_MinusLogProbMetric: 22.1934 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 861/1000
2023-10-09 22:03:38.573 
Epoch 861/1000 
	 loss: 22.0945, MinusLogProbMetric: 22.0945, val_loss: 22.2926, val_MinusLogProbMetric: 22.2926

Epoch 861: val_loss did not improve from 22.19340
196/196 - 51s - loss: 22.0945 - MinusLogProbMetric: 22.0945 - val_loss: 22.2926 - val_MinusLogProbMetric: 22.2926 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 862/1000
2023-10-09 22:04:29.674 
Epoch 862/1000 
	 loss: 22.0745, MinusLogProbMetric: 22.0745, val_loss: 22.3455, val_MinusLogProbMetric: 22.3455

Epoch 862: val_loss did not improve from 22.19340
196/196 - 51s - loss: 22.0745 - MinusLogProbMetric: 22.0745 - val_loss: 22.3455 - val_MinusLogProbMetric: 22.3455 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 863/1000
2023-10-09 22:05:20.397 
Epoch 863/1000 
	 loss: 22.0649, MinusLogProbMetric: 22.0649, val_loss: 22.2072, val_MinusLogProbMetric: 22.2072

Epoch 863: val_loss did not improve from 22.19340
196/196 - 51s - loss: 22.0649 - MinusLogProbMetric: 22.0649 - val_loss: 22.2072 - val_MinusLogProbMetric: 22.2072 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 864/1000
2023-10-09 22:06:10.826 
Epoch 864/1000 
	 loss: 22.0758, MinusLogProbMetric: 22.0758, val_loss: 22.2386, val_MinusLogProbMetric: 22.2386

Epoch 864: val_loss did not improve from 22.19340
196/196 - 50s - loss: 22.0758 - MinusLogProbMetric: 22.0758 - val_loss: 22.2386 - val_MinusLogProbMetric: 22.2386 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 865/1000
2023-10-09 22:07:00.608 
Epoch 865/1000 
	 loss: 22.0525, MinusLogProbMetric: 22.0525, val_loss: 22.3276, val_MinusLogProbMetric: 22.3276

Epoch 865: val_loss did not improve from 22.19340
196/196 - 50s - loss: 22.0525 - MinusLogProbMetric: 22.0525 - val_loss: 22.3276 - val_MinusLogProbMetric: 22.3276 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 866/1000
2023-10-09 22:07:52.160 
Epoch 866/1000 
	 loss: 22.0429, MinusLogProbMetric: 22.0429, val_loss: 22.1625, val_MinusLogProbMetric: 22.1625

Epoch 866: val_loss improved from 22.19340 to 22.16248, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.0429 - MinusLogProbMetric: 22.0429 - val_loss: 22.1625 - val_MinusLogProbMetric: 22.1625 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 867/1000
2023-10-09 22:08:43.271 
Epoch 867/1000 
	 loss: 22.0399, MinusLogProbMetric: 22.0399, val_loss: 22.1675, val_MinusLogProbMetric: 22.1675

Epoch 867: val_loss did not improve from 22.16248
196/196 - 50s - loss: 22.0399 - MinusLogProbMetric: 22.0399 - val_loss: 22.1675 - val_MinusLogProbMetric: 22.1675 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 868/1000
2023-10-09 22:09:34.706 
Epoch 868/1000 
	 loss: 22.0457, MinusLogProbMetric: 22.0457, val_loss: 22.2439, val_MinusLogProbMetric: 22.2439

Epoch 868: val_loss did not improve from 22.16248
196/196 - 51s - loss: 22.0457 - MinusLogProbMetric: 22.0457 - val_loss: 22.2439 - val_MinusLogProbMetric: 22.2439 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 869/1000
2023-10-09 22:10:25.835 
Epoch 869/1000 
	 loss: 22.0393, MinusLogProbMetric: 22.0393, val_loss: 22.1483, val_MinusLogProbMetric: 22.1483

Epoch 869: val_loss improved from 22.16248 to 22.14826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 22.0393 - MinusLogProbMetric: 22.0393 - val_loss: 22.1483 - val_MinusLogProbMetric: 22.1483 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 870/1000
2023-10-09 22:11:16.584 
Epoch 870/1000 
	 loss: 22.0340, MinusLogProbMetric: 22.0340, val_loss: 22.1530, val_MinusLogProbMetric: 22.1530

Epoch 870: val_loss did not improve from 22.14826
196/196 - 50s - loss: 22.0340 - MinusLogProbMetric: 22.0340 - val_loss: 22.1530 - val_MinusLogProbMetric: 22.1530 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 871/1000
2023-10-09 22:12:09.612 
Epoch 871/1000 
	 loss: 22.0384, MinusLogProbMetric: 22.0384, val_loss: 22.1234, val_MinusLogProbMetric: 22.1234

Epoch 871: val_loss improved from 22.14826 to 22.12341, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 22.0384 - MinusLogProbMetric: 22.0384 - val_loss: 22.1234 - val_MinusLogProbMetric: 22.1234 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 872/1000
2023-10-09 22:13:01.381 
Epoch 872/1000 
	 loss: 22.0345, MinusLogProbMetric: 22.0345, val_loss: 22.1300, val_MinusLogProbMetric: 22.1300

Epoch 872: val_loss did not improve from 22.12341
196/196 - 51s - loss: 22.0345 - MinusLogProbMetric: 22.0345 - val_loss: 22.1300 - val_MinusLogProbMetric: 22.1300 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 873/1000
2023-10-09 22:13:54.113 
Epoch 873/1000 
	 loss: 22.0318, MinusLogProbMetric: 22.0318, val_loss: 22.5094, val_MinusLogProbMetric: 22.5094

Epoch 873: val_loss did not improve from 22.12341
196/196 - 53s - loss: 22.0318 - MinusLogProbMetric: 22.0318 - val_loss: 22.5094 - val_MinusLogProbMetric: 22.5094 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 874/1000
2023-10-09 22:14:44.458 
Epoch 874/1000 
	 loss: 22.0053, MinusLogProbMetric: 22.0053, val_loss: 22.1544, val_MinusLogProbMetric: 22.1544

Epoch 874: val_loss did not improve from 22.12341
196/196 - 50s - loss: 22.0053 - MinusLogProbMetric: 22.0053 - val_loss: 22.1544 - val_MinusLogProbMetric: 22.1544 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 875/1000
2023-10-09 22:15:35.001 
Epoch 875/1000 
	 loss: 22.0434, MinusLogProbMetric: 22.0434, val_loss: 22.1408, val_MinusLogProbMetric: 22.1408

Epoch 875: val_loss did not improve from 22.12341
196/196 - 51s - loss: 22.0434 - MinusLogProbMetric: 22.0434 - val_loss: 22.1408 - val_MinusLogProbMetric: 22.1408 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 876/1000
2023-10-09 22:16:24.437 
Epoch 876/1000 
	 loss: 21.9833, MinusLogProbMetric: 21.9833, val_loss: 22.2688, val_MinusLogProbMetric: 22.2688

Epoch 876: val_loss did not improve from 22.12341
196/196 - 49s - loss: 21.9833 - MinusLogProbMetric: 21.9833 - val_loss: 22.2688 - val_MinusLogProbMetric: 22.2688 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 877/1000
2023-10-09 22:17:15.099 
Epoch 877/1000 
	 loss: 21.9997, MinusLogProbMetric: 21.9997, val_loss: 22.1352, val_MinusLogProbMetric: 22.1352

Epoch 877: val_loss did not improve from 22.12341
196/196 - 51s - loss: 21.9997 - MinusLogProbMetric: 21.9997 - val_loss: 22.1352 - val_MinusLogProbMetric: 22.1352 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 878/1000
2023-10-09 22:18:04.920 
Epoch 878/1000 
	 loss: 22.0098, MinusLogProbMetric: 22.0098, val_loss: 22.1782, val_MinusLogProbMetric: 22.1782

Epoch 878: val_loss did not improve from 22.12341
196/196 - 50s - loss: 22.0098 - MinusLogProbMetric: 22.0098 - val_loss: 22.1782 - val_MinusLogProbMetric: 22.1782 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 879/1000
2023-10-09 22:18:55.671 
Epoch 879/1000 
	 loss: 22.0010, MinusLogProbMetric: 22.0010, val_loss: 22.0618, val_MinusLogProbMetric: 22.0618

Epoch 879: val_loss improved from 22.12341 to 22.06181, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 22.0010 - MinusLogProbMetric: 22.0010 - val_loss: 22.0618 - val_MinusLogProbMetric: 22.0618 - lr: 5.5556e-05 - 51s/epoch - 263ms/step
Epoch 880/1000
2023-10-09 22:19:45.957 
Epoch 880/1000 
	 loss: 21.9717, MinusLogProbMetric: 21.9717, val_loss: 22.1032, val_MinusLogProbMetric: 22.1032

Epoch 880: val_loss did not improve from 22.06181
196/196 - 50s - loss: 21.9717 - MinusLogProbMetric: 21.9717 - val_loss: 22.1032 - val_MinusLogProbMetric: 22.1032 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 881/1000
2023-10-09 22:20:38.608 
Epoch 881/1000 
	 loss: 21.9752, MinusLogProbMetric: 21.9752, val_loss: 22.1572, val_MinusLogProbMetric: 22.1572

Epoch 881: val_loss did not improve from 22.06181
196/196 - 53s - loss: 21.9752 - MinusLogProbMetric: 21.9752 - val_loss: 22.1572 - val_MinusLogProbMetric: 22.1572 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 882/1000
2023-10-09 22:21:29.177 
Epoch 882/1000 
	 loss: 21.9687, MinusLogProbMetric: 21.9687, val_loss: 22.0994, val_MinusLogProbMetric: 22.0994

Epoch 882: val_loss did not improve from 22.06181
196/196 - 51s - loss: 21.9687 - MinusLogProbMetric: 21.9687 - val_loss: 22.0994 - val_MinusLogProbMetric: 22.0994 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 883/1000
2023-10-09 22:22:21.769 
Epoch 883/1000 
	 loss: 21.9731, MinusLogProbMetric: 21.9731, val_loss: 22.1873, val_MinusLogProbMetric: 22.1873

Epoch 883: val_loss did not improve from 22.06181
196/196 - 53s - loss: 21.9731 - MinusLogProbMetric: 21.9731 - val_loss: 22.1873 - val_MinusLogProbMetric: 22.1873 - lr: 5.5556e-05 - 53s/epoch - 268ms/step
Epoch 884/1000
2023-10-09 22:23:12.178 
Epoch 884/1000 
	 loss: 21.9528, MinusLogProbMetric: 21.9528, val_loss: 22.0566, val_MinusLogProbMetric: 22.0566

Epoch 884: val_loss improved from 22.06181 to 22.05660, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 21.9528 - MinusLogProbMetric: 21.9528 - val_loss: 22.0566 - val_MinusLogProbMetric: 22.0566 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 885/1000
2023-10-09 22:24:02.245 
Epoch 885/1000 
	 loss: 21.9439, MinusLogProbMetric: 21.9439, val_loss: 22.0585, val_MinusLogProbMetric: 22.0585

Epoch 885: val_loss did not improve from 22.05660
196/196 - 49s - loss: 21.9439 - MinusLogProbMetric: 21.9439 - val_loss: 22.0585 - val_MinusLogProbMetric: 22.0585 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 886/1000
2023-10-09 22:24:54.253 
Epoch 886/1000 
	 loss: 21.9550, MinusLogProbMetric: 21.9550, val_loss: 22.2548, val_MinusLogProbMetric: 22.2548

Epoch 886: val_loss did not improve from 22.05660
196/196 - 52s - loss: 21.9550 - MinusLogProbMetric: 21.9550 - val_loss: 22.2548 - val_MinusLogProbMetric: 22.2548 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 887/1000
2023-10-09 22:25:46.858 
Epoch 887/1000 
	 loss: 21.9725, MinusLogProbMetric: 21.9725, val_loss: 22.2026, val_MinusLogProbMetric: 22.2026

Epoch 887: val_loss did not improve from 22.05660
196/196 - 53s - loss: 21.9725 - MinusLogProbMetric: 21.9725 - val_loss: 22.2026 - val_MinusLogProbMetric: 22.2026 - lr: 5.5556e-05 - 53s/epoch - 268ms/step
Epoch 888/1000
2023-10-09 22:26:37.857 
Epoch 888/1000 
	 loss: 21.9597, MinusLogProbMetric: 21.9597, val_loss: 22.0859, val_MinusLogProbMetric: 22.0859

Epoch 888: val_loss did not improve from 22.05660
196/196 - 51s - loss: 21.9597 - MinusLogProbMetric: 21.9597 - val_loss: 22.0859 - val_MinusLogProbMetric: 22.0859 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 889/1000
2023-10-09 22:27:28.720 
Epoch 889/1000 
	 loss: 21.9513, MinusLogProbMetric: 21.9513, val_loss: 22.2197, val_MinusLogProbMetric: 22.2197

Epoch 889: val_loss did not improve from 22.05660
196/196 - 51s - loss: 21.9513 - MinusLogProbMetric: 21.9513 - val_loss: 22.2197 - val_MinusLogProbMetric: 22.2197 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 890/1000
2023-10-09 22:28:19.626 
Epoch 890/1000 
	 loss: 21.9254, MinusLogProbMetric: 21.9254, val_loss: 22.1300, val_MinusLogProbMetric: 22.1300

Epoch 890: val_loss did not improve from 22.05660
196/196 - 51s - loss: 21.9254 - MinusLogProbMetric: 21.9254 - val_loss: 22.1300 - val_MinusLogProbMetric: 22.1300 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 891/1000
2023-10-09 22:29:10.234 
Epoch 891/1000 
	 loss: 21.9267, MinusLogProbMetric: 21.9267, val_loss: 22.0932, val_MinusLogProbMetric: 22.0932

Epoch 891: val_loss did not improve from 22.05660
196/196 - 51s - loss: 21.9267 - MinusLogProbMetric: 21.9267 - val_loss: 22.0932 - val_MinusLogProbMetric: 22.0932 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 892/1000
2023-10-09 22:30:01.148 
Epoch 892/1000 
	 loss: 21.9443, MinusLogProbMetric: 21.9443, val_loss: 22.1143, val_MinusLogProbMetric: 22.1143

Epoch 892: val_loss did not improve from 22.05660
196/196 - 51s - loss: 21.9443 - MinusLogProbMetric: 21.9443 - val_loss: 22.1143 - val_MinusLogProbMetric: 22.1143 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 893/1000
2023-10-09 22:30:52.757 
Epoch 893/1000 
	 loss: 21.9186, MinusLogProbMetric: 21.9186, val_loss: 22.0223, val_MinusLogProbMetric: 22.0223

Epoch 893: val_loss improved from 22.05660 to 22.02234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 21.9186 - MinusLogProbMetric: 21.9186 - val_loss: 22.0223 - val_MinusLogProbMetric: 22.0223 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 894/1000
2023-10-09 22:31:43.993 
Epoch 894/1000 
	 loss: 21.9298, MinusLogProbMetric: 21.9298, val_loss: 22.0397, val_MinusLogProbMetric: 22.0397

Epoch 894: val_loss did not improve from 22.02234
196/196 - 50s - loss: 21.9298 - MinusLogProbMetric: 21.9298 - val_loss: 22.0397 - val_MinusLogProbMetric: 22.0397 - lr: 5.5556e-05 - 50s/epoch - 258ms/step
Epoch 895/1000
2023-10-09 22:32:36.070 
Epoch 895/1000 
	 loss: 21.9075, MinusLogProbMetric: 21.9075, val_loss: 22.1002, val_MinusLogProbMetric: 22.1002

Epoch 895: val_loss did not improve from 22.02234
196/196 - 52s - loss: 21.9075 - MinusLogProbMetric: 21.9075 - val_loss: 22.1002 - val_MinusLogProbMetric: 22.1002 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 896/1000
2023-10-09 22:33:25.311 
Epoch 896/1000 
	 loss: 21.9163, MinusLogProbMetric: 21.9163, val_loss: 22.0680, val_MinusLogProbMetric: 22.0680

Epoch 896: val_loss did not improve from 22.02234
196/196 - 49s - loss: 21.9163 - MinusLogProbMetric: 21.9163 - val_loss: 22.0680 - val_MinusLogProbMetric: 22.0680 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 897/1000
2023-10-09 22:34:15.919 
Epoch 897/1000 
	 loss: 21.9069, MinusLogProbMetric: 21.9069, val_loss: 22.0640, val_MinusLogProbMetric: 22.0640

Epoch 897: val_loss did not improve from 22.02234
196/196 - 51s - loss: 21.9069 - MinusLogProbMetric: 21.9069 - val_loss: 22.0640 - val_MinusLogProbMetric: 22.0640 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 898/1000
2023-10-09 22:35:08.005 
Epoch 898/1000 
	 loss: 21.8896, MinusLogProbMetric: 21.8896, val_loss: 22.1927, val_MinusLogProbMetric: 22.1927

Epoch 898: val_loss did not improve from 22.02234
196/196 - 52s - loss: 21.8896 - MinusLogProbMetric: 21.8896 - val_loss: 22.1927 - val_MinusLogProbMetric: 22.1927 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 899/1000
2023-10-09 22:35:57.765 
Epoch 899/1000 
	 loss: 21.9247, MinusLogProbMetric: 21.9247, val_loss: 22.0360, val_MinusLogProbMetric: 22.0360

Epoch 899: val_loss did not improve from 22.02234
196/196 - 50s - loss: 21.9247 - MinusLogProbMetric: 21.9247 - val_loss: 22.0360 - val_MinusLogProbMetric: 22.0360 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 900/1000
2023-10-09 22:36:48.005 
Epoch 900/1000 
	 loss: 21.9049, MinusLogProbMetric: 21.9049, val_loss: 22.1979, val_MinusLogProbMetric: 22.1979

Epoch 900: val_loss did not improve from 22.02234
196/196 - 50s - loss: 21.9049 - MinusLogProbMetric: 21.9049 - val_loss: 22.1979 - val_MinusLogProbMetric: 22.1979 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 901/1000
2023-10-09 22:37:38.752 
Epoch 901/1000 
	 loss: 21.8964, MinusLogProbMetric: 21.8964, val_loss: 21.9900, val_MinusLogProbMetric: 21.9900

Epoch 901: val_loss improved from 22.02234 to 21.99001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 21.8964 - MinusLogProbMetric: 21.8964 - val_loss: 21.9900 - val_MinusLogProbMetric: 21.9900 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 902/1000
2023-10-09 22:38:31.421 
Epoch 902/1000 
	 loss: 21.9031, MinusLogProbMetric: 21.9031, val_loss: 21.9458, val_MinusLogProbMetric: 21.9458

Epoch 902: val_loss improved from 21.99001 to 21.94582, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 21.9031 - MinusLogProbMetric: 21.9031 - val_loss: 21.9458 - val_MinusLogProbMetric: 21.9458 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 903/1000
2023-10-09 22:39:21.589 
Epoch 903/1000 
	 loss: 21.8774, MinusLogProbMetric: 21.8774, val_loss: 21.9641, val_MinusLogProbMetric: 21.9641

Epoch 903: val_loss did not improve from 21.94582
196/196 - 49s - loss: 21.8774 - MinusLogProbMetric: 21.8774 - val_loss: 21.9641 - val_MinusLogProbMetric: 21.9641 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 904/1000
2023-10-09 22:40:11.916 
Epoch 904/1000 
	 loss: 21.8706, MinusLogProbMetric: 21.8706, val_loss: 21.9988, val_MinusLogProbMetric: 21.9988

Epoch 904: val_loss did not improve from 21.94582
196/196 - 50s - loss: 21.8706 - MinusLogProbMetric: 21.8706 - val_loss: 21.9988 - val_MinusLogProbMetric: 21.9988 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 905/1000
2023-10-09 22:41:06.296 
Epoch 905/1000 
	 loss: 21.8818, MinusLogProbMetric: 21.8818, val_loss: 22.1629, val_MinusLogProbMetric: 22.1629

Epoch 905: val_loss did not improve from 21.94582
196/196 - 54s - loss: 21.8818 - MinusLogProbMetric: 21.8818 - val_loss: 22.1629 - val_MinusLogProbMetric: 22.1629 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 906/1000
2023-10-09 22:41:58.173 
Epoch 906/1000 
	 loss: 21.8683, MinusLogProbMetric: 21.8683, val_loss: 22.0077, val_MinusLogProbMetric: 22.0077

Epoch 906: val_loss did not improve from 21.94582
196/196 - 52s - loss: 21.8683 - MinusLogProbMetric: 21.8683 - val_loss: 22.0077 - val_MinusLogProbMetric: 22.0077 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 907/1000
2023-10-09 22:42:47.592 
Epoch 907/1000 
	 loss: 21.8719, MinusLogProbMetric: 21.8719, val_loss: 21.9587, val_MinusLogProbMetric: 21.9587

Epoch 907: val_loss did not improve from 21.94582
196/196 - 49s - loss: 21.8719 - MinusLogProbMetric: 21.8719 - val_loss: 21.9587 - val_MinusLogProbMetric: 21.9587 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 908/1000
2023-10-09 22:43:38.125 
Epoch 908/1000 
	 loss: 21.8666, MinusLogProbMetric: 21.8666, val_loss: 22.1140, val_MinusLogProbMetric: 22.1140

Epoch 908: val_loss did not improve from 21.94582
196/196 - 51s - loss: 21.8666 - MinusLogProbMetric: 21.8666 - val_loss: 22.1140 - val_MinusLogProbMetric: 22.1140 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 909/1000
2023-10-09 22:44:30.610 
Epoch 909/1000 
	 loss: 21.9009, MinusLogProbMetric: 21.9009, val_loss: 22.0077, val_MinusLogProbMetric: 22.0077

Epoch 909: val_loss did not improve from 21.94582
196/196 - 52s - loss: 21.9009 - MinusLogProbMetric: 21.9009 - val_loss: 22.0077 - val_MinusLogProbMetric: 22.0077 - lr: 5.5556e-05 - 52s/epoch - 268ms/step
Epoch 910/1000
2023-10-09 22:45:22.924 
Epoch 910/1000 
	 loss: 21.8638, MinusLogProbMetric: 21.8638, val_loss: 22.0396, val_MinusLogProbMetric: 22.0396

Epoch 910: val_loss did not improve from 21.94582
196/196 - 52s - loss: 21.8638 - MinusLogProbMetric: 21.8638 - val_loss: 22.0396 - val_MinusLogProbMetric: 22.0396 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 911/1000
2023-10-09 22:46:13.812 
Epoch 911/1000 
	 loss: 21.8469, MinusLogProbMetric: 21.8469, val_loss: 22.0412, val_MinusLogProbMetric: 22.0412

Epoch 911: val_loss did not improve from 21.94582
196/196 - 51s - loss: 21.8469 - MinusLogProbMetric: 21.8469 - val_loss: 22.0412 - val_MinusLogProbMetric: 22.0412 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 912/1000
2023-10-09 22:47:03.921 
Epoch 912/1000 
	 loss: 21.8490, MinusLogProbMetric: 21.8490, val_loss: 21.9104, val_MinusLogProbMetric: 21.9104

Epoch 912: val_loss improved from 21.94582 to 21.91038, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 21.8490 - MinusLogProbMetric: 21.8490 - val_loss: 21.9104 - val_MinusLogProbMetric: 21.9104 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 913/1000
2023-10-09 22:47:55.025 
Epoch 913/1000 
	 loss: 21.8338, MinusLogProbMetric: 21.8338, val_loss: 21.9894, val_MinusLogProbMetric: 21.9894

Epoch 913: val_loss did not improve from 21.91038
196/196 - 50s - loss: 21.8338 - MinusLogProbMetric: 21.8338 - val_loss: 21.9894 - val_MinusLogProbMetric: 21.9894 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 914/1000
2023-10-09 22:48:46.593 
Epoch 914/1000 
	 loss: 21.8184, MinusLogProbMetric: 21.8184, val_loss: 21.8843, val_MinusLogProbMetric: 21.8843

Epoch 914: val_loss improved from 21.91038 to 21.88428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 21.8184 - MinusLogProbMetric: 21.8184 - val_loss: 21.8843 - val_MinusLogProbMetric: 21.8843 - lr: 5.5556e-05 - 52s/epoch - 268ms/step
Epoch 915/1000
2023-10-09 22:49:38.864 
Epoch 915/1000 
	 loss: 21.8302, MinusLogProbMetric: 21.8302, val_loss: 21.9359, val_MinusLogProbMetric: 21.9359

Epoch 915: val_loss did not improve from 21.88428
196/196 - 51s - loss: 21.8302 - MinusLogProbMetric: 21.8302 - val_loss: 21.9359 - val_MinusLogProbMetric: 21.9359 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 916/1000
2023-10-09 22:50:29.913 
Epoch 916/1000 
	 loss: 21.8608, MinusLogProbMetric: 21.8608, val_loss: 22.4912, val_MinusLogProbMetric: 22.4912

Epoch 916: val_loss did not improve from 21.88428
196/196 - 51s - loss: 21.8608 - MinusLogProbMetric: 21.8608 - val_loss: 22.4912 - val_MinusLogProbMetric: 22.4912 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 917/1000
2023-10-09 22:51:20.359 
Epoch 917/1000 
	 loss: 21.8337, MinusLogProbMetric: 21.8337, val_loss: 21.9097, val_MinusLogProbMetric: 21.9097

Epoch 917: val_loss did not improve from 21.88428
196/196 - 50s - loss: 21.8337 - MinusLogProbMetric: 21.8337 - val_loss: 21.9097 - val_MinusLogProbMetric: 21.9097 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 918/1000
2023-10-09 22:52:12.601 
Epoch 918/1000 
	 loss: 21.8231, MinusLogProbMetric: 21.8231, val_loss: 21.9658, val_MinusLogProbMetric: 21.9658

Epoch 918: val_loss did not improve from 21.88428
196/196 - 52s - loss: 21.8231 - MinusLogProbMetric: 21.8231 - val_loss: 21.9658 - val_MinusLogProbMetric: 21.9658 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 919/1000
2023-10-09 22:53:02.092 
Epoch 919/1000 
	 loss: 21.8192, MinusLogProbMetric: 21.8192, val_loss: 21.9002, val_MinusLogProbMetric: 21.9002

Epoch 919: val_loss did not improve from 21.88428
196/196 - 49s - loss: 21.8192 - MinusLogProbMetric: 21.8192 - val_loss: 21.9002 - val_MinusLogProbMetric: 21.9002 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 920/1000
2023-10-09 22:53:51.517 
Epoch 920/1000 
	 loss: 21.8367, MinusLogProbMetric: 21.8367, val_loss: 21.9128, val_MinusLogProbMetric: 21.9128

Epoch 920: val_loss did not improve from 21.88428
196/196 - 49s - loss: 21.8367 - MinusLogProbMetric: 21.8367 - val_loss: 21.9128 - val_MinusLogProbMetric: 21.9128 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 921/1000
2023-10-09 22:54:40.743 
Epoch 921/1000 
	 loss: 21.8148, MinusLogProbMetric: 21.8148, val_loss: 22.0015, val_MinusLogProbMetric: 22.0015

Epoch 921: val_loss did not improve from 21.88428
196/196 - 49s - loss: 21.8148 - MinusLogProbMetric: 21.8148 - val_loss: 22.0015 - val_MinusLogProbMetric: 22.0015 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 922/1000
2023-10-09 22:55:31.264 
Epoch 922/1000 
	 loss: 21.7909, MinusLogProbMetric: 21.7909, val_loss: 21.9683, val_MinusLogProbMetric: 21.9683

Epoch 922: val_loss did not improve from 21.88428
196/196 - 51s - loss: 21.7909 - MinusLogProbMetric: 21.7909 - val_loss: 21.9683 - val_MinusLogProbMetric: 21.9683 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 923/1000
2023-10-09 22:56:21.232 
Epoch 923/1000 
	 loss: 21.7927, MinusLogProbMetric: 21.7927, val_loss: 21.9183, val_MinusLogProbMetric: 21.9183

Epoch 923: val_loss did not improve from 21.88428
196/196 - 50s - loss: 21.7927 - MinusLogProbMetric: 21.7927 - val_loss: 21.9183 - val_MinusLogProbMetric: 21.9183 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 924/1000
2023-10-09 22:57:11.754 
Epoch 924/1000 
	 loss: 21.7772, MinusLogProbMetric: 21.7772, val_loss: 21.9719, val_MinusLogProbMetric: 21.9719

Epoch 924: val_loss did not improve from 21.88428
196/196 - 51s - loss: 21.7772 - MinusLogProbMetric: 21.7772 - val_loss: 21.9719 - val_MinusLogProbMetric: 21.9719 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 925/1000
2023-10-09 22:58:05.451 
Epoch 925/1000 
	 loss: 21.7763, MinusLogProbMetric: 21.7763, val_loss: 21.9059, val_MinusLogProbMetric: 21.9059

Epoch 925: val_loss did not improve from 21.88428
196/196 - 54s - loss: 21.7763 - MinusLogProbMetric: 21.7763 - val_loss: 21.9059 - val_MinusLogProbMetric: 21.9059 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 926/1000
2023-10-09 22:58:56.097 
Epoch 926/1000 
	 loss: 21.8042, MinusLogProbMetric: 21.8042, val_loss: 21.9147, val_MinusLogProbMetric: 21.9147

Epoch 926: val_loss did not improve from 21.88428
196/196 - 51s - loss: 21.8042 - MinusLogProbMetric: 21.8042 - val_loss: 21.9147 - val_MinusLogProbMetric: 21.9147 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 927/1000
2023-10-09 22:59:47.806 
Epoch 927/1000 
	 loss: 21.7822, MinusLogProbMetric: 21.7822, val_loss: 22.0293, val_MinusLogProbMetric: 22.0293

Epoch 927: val_loss did not improve from 21.88428
196/196 - 52s - loss: 21.7822 - MinusLogProbMetric: 21.7822 - val_loss: 22.0293 - val_MinusLogProbMetric: 22.0293 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 928/1000
2023-10-09 23:00:39.687 
Epoch 928/1000 
	 loss: 21.8069, MinusLogProbMetric: 21.8069, val_loss: 21.9823, val_MinusLogProbMetric: 21.9823

Epoch 928: val_loss did not improve from 21.88428
196/196 - 52s - loss: 21.8069 - MinusLogProbMetric: 21.8069 - val_loss: 21.9823 - val_MinusLogProbMetric: 21.9823 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 929/1000
2023-10-09 23:01:33.172 
Epoch 929/1000 
	 loss: 21.8135, MinusLogProbMetric: 21.8135, val_loss: 21.9954, val_MinusLogProbMetric: 21.9954

Epoch 929: val_loss did not improve from 21.88428
196/196 - 53s - loss: 21.8135 - MinusLogProbMetric: 21.8135 - val_loss: 21.9954 - val_MinusLogProbMetric: 21.9954 - lr: 5.5556e-05 - 53s/epoch - 273ms/step
Epoch 930/1000
2023-10-09 23:02:23.974 
Epoch 930/1000 
	 loss: 21.7792, MinusLogProbMetric: 21.7792, val_loss: 21.8712, val_MinusLogProbMetric: 21.8712

Epoch 930: val_loss improved from 21.88428 to 21.87119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 21.7792 - MinusLogProbMetric: 21.7792 - val_loss: 21.8712 - val_MinusLogProbMetric: 21.8712 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 931/1000
2023-10-09 23:03:16.762 
Epoch 931/1000 
	 loss: 21.7671, MinusLogProbMetric: 21.7671, val_loss: 21.8340, val_MinusLogProbMetric: 21.8340

Epoch 931: val_loss improved from 21.87119 to 21.83404, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 21.7671 - MinusLogProbMetric: 21.7671 - val_loss: 21.8340 - val_MinusLogProbMetric: 21.8340 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 932/1000
2023-10-09 23:04:10.017 
Epoch 932/1000 
	 loss: 21.7471, MinusLogProbMetric: 21.7471, val_loss: 21.9267, val_MinusLogProbMetric: 21.9267

Epoch 932: val_loss did not improve from 21.83404
196/196 - 53s - loss: 21.7471 - MinusLogProbMetric: 21.7471 - val_loss: 21.9267 - val_MinusLogProbMetric: 21.9267 - lr: 5.5556e-05 - 53s/epoch - 268ms/step
Epoch 933/1000
2023-10-09 23:05:00.795 
Epoch 933/1000 
	 loss: 21.7380, MinusLogProbMetric: 21.7380, val_loss: 21.8917, val_MinusLogProbMetric: 21.8917

Epoch 933: val_loss did not improve from 21.83404
196/196 - 51s - loss: 21.7380 - MinusLogProbMetric: 21.7380 - val_loss: 21.8917 - val_MinusLogProbMetric: 21.8917 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 934/1000
2023-10-09 23:05:52.291 
Epoch 934/1000 
	 loss: 21.7452, MinusLogProbMetric: 21.7452, val_loss: 22.0648, val_MinusLogProbMetric: 22.0648

Epoch 934: val_loss did not improve from 21.83404
196/196 - 51s - loss: 21.7452 - MinusLogProbMetric: 21.7452 - val_loss: 22.0648 - val_MinusLogProbMetric: 22.0648 - lr: 5.5556e-05 - 51s/epoch - 263ms/step
Epoch 935/1000
2023-10-09 23:06:46.746 
Epoch 935/1000 
	 loss: 21.7580, MinusLogProbMetric: 21.7580, val_loss: 21.8404, val_MinusLogProbMetric: 21.8404

Epoch 935: val_loss did not improve from 21.83404
196/196 - 54s - loss: 21.7580 - MinusLogProbMetric: 21.7580 - val_loss: 21.8404 - val_MinusLogProbMetric: 21.8404 - lr: 5.5556e-05 - 54s/epoch - 278ms/step
Epoch 936/1000
2023-10-09 23:07:36.246 
Epoch 936/1000 
	 loss: 21.7748, MinusLogProbMetric: 21.7748, val_loss: 22.1613, val_MinusLogProbMetric: 22.1613

Epoch 936: val_loss did not improve from 21.83404
196/196 - 49s - loss: 21.7748 - MinusLogProbMetric: 21.7748 - val_loss: 22.1613 - val_MinusLogProbMetric: 22.1613 - lr: 5.5556e-05 - 49s/epoch - 253ms/step
Epoch 937/1000
2023-10-09 23:08:25.528 
Epoch 937/1000 
	 loss: 21.7402, MinusLogProbMetric: 21.7402, val_loss: 22.0583, val_MinusLogProbMetric: 22.0583

Epoch 937: val_loss did not improve from 21.83404
196/196 - 49s - loss: 21.7402 - MinusLogProbMetric: 21.7402 - val_loss: 22.0583 - val_MinusLogProbMetric: 22.0583 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 938/1000
2023-10-09 23:09:15.342 
Epoch 938/1000 
	 loss: 21.7566, MinusLogProbMetric: 21.7566, val_loss: 21.8606, val_MinusLogProbMetric: 21.8606

Epoch 938: val_loss did not improve from 21.83404
196/196 - 50s - loss: 21.7566 - MinusLogProbMetric: 21.7566 - val_loss: 21.8606 - val_MinusLogProbMetric: 21.8606 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 939/1000
2023-10-09 23:10:06.270 
Epoch 939/1000 
	 loss: 21.7111, MinusLogProbMetric: 21.7111, val_loss: 21.8063, val_MinusLogProbMetric: 21.8063

Epoch 939: val_loss improved from 21.83404 to 21.80631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 21.7111 - MinusLogProbMetric: 21.7111 - val_loss: 21.8063 - val_MinusLogProbMetric: 21.8063 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 940/1000
2023-10-09 23:10:57.798 
Epoch 940/1000 
	 loss: 21.7237, MinusLogProbMetric: 21.7237, val_loss: 21.8677, val_MinusLogProbMetric: 21.8677

Epoch 940: val_loss did not improve from 21.80631
196/196 - 51s - loss: 21.7237 - MinusLogProbMetric: 21.7237 - val_loss: 21.8677 - val_MinusLogProbMetric: 21.8677 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 941/1000
2023-10-09 23:11:48.515 
Epoch 941/1000 
	 loss: 21.7226, MinusLogProbMetric: 21.7226, val_loss: 21.8613, val_MinusLogProbMetric: 21.8613

Epoch 941: val_loss did not improve from 21.80631
196/196 - 51s - loss: 21.7226 - MinusLogProbMetric: 21.7226 - val_loss: 21.8613 - val_MinusLogProbMetric: 21.8613 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 942/1000
2023-10-09 23:12:39.599 
Epoch 942/1000 
	 loss: 21.7226, MinusLogProbMetric: 21.7226, val_loss: 21.8304, val_MinusLogProbMetric: 21.8304

Epoch 942: val_loss did not improve from 21.80631
196/196 - 51s - loss: 21.7226 - MinusLogProbMetric: 21.7226 - val_loss: 21.8304 - val_MinusLogProbMetric: 21.8304 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 943/1000
2023-10-09 23:13:31.643 
Epoch 943/1000 
	 loss: 21.7166, MinusLogProbMetric: 21.7166, val_loss: 21.8984, val_MinusLogProbMetric: 21.8984

Epoch 943: val_loss did not improve from 21.80631
196/196 - 52s - loss: 21.7166 - MinusLogProbMetric: 21.7166 - val_loss: 21.8984 - val_MinusLogProbMetric: 21.8984 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 944/1000
2023-10-09 23:14:21.875 
Epoch 944/1000 
	 loss: 21.7053, MinusLogProbMetric: 21.7053, val_loss: 21.9214, val_MinusLogProbMetric: 21.9214

Epoch 944: val_loss did not improve from 21.80631
196/196 - 50s - loss: 21.7053 - MinusLogProbMetric: 21.7053 - val_loss: 21.9214 - val_MinusLogProbMetric: 21.9214 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 945/1000
2023-10-09 23:15:11.750 
Epoch 945/1000 
	 loss: 21.6986, MinusLogProbMetric: 21.6986, val_loss: 21.9131, val_MinusLogProbMetric: 21.9131

Epoch 945: val_loss did not improve from 21.80631
196/196 - 50s - loss: 21.6986 - MinusLogProbMetric: 21.6986 - val_loss: 21.9131 - val_MinusLogProbMetric: 21.9131 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 946/1000
2023-10-09 23:16:00.479 
Epoch 946/1000 
	 loss: 21.6826, MinusLogProbMetric: 21.6826, val_loss: 21.9143, val_MinusLogProbMetric: 21.9143

Epoch 946: val_loss did not improve from 21.80631
196/196 - 49s - loss: 21.6826 - MinusLogProbMetric: 21.6826 - val_loss: 21.9143 - val_MinusLogProbMetric: 21.9143 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 947/1000
2023-10-09 23:16:53.797 
Epoch 947/1000 
	 loss: 21.7185, MinusLogProbMetric: 21.7185, val_loss: 21.8851, val_MinusLogProbMetric: 21.8851

Epoch 947: val_loss did not improve from 21.80631
196/196 - 53s - loss: 21.7185 - MinusLogProbMetric: 21.7185 - val_loss: 21.8851 - val_MinusLogProbMetric: 21.8851 - lr: 5.5556e-05 - 53s/epoch - 272ms/step
Epoch 948/1000
2023-10-09 23:17:46.008 
Epoch 948/1000 
	 loss: 21.7027, MinusLogProbMetric: 21.7027, val_loss: 22.0348, val_MinusLogProbMetric: 22.0348

Epoch 948: val_loss did not improve from 21.80631
196/196 - 52s - loss: 21.7027 - MinusLogProbMetric: 21.7027 - val_loss: 22.0348 - val_MinusLogProbMetric: 22.0348 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 949/1000
2023-10-09 23:18:36.385 
Epoch 949/1000 
	 loss: 21.7195, MinusLogProbMetric: 21.7195, val_loss: 21.8730, val_MinusLogProbMetric: 21.8730

Epoch 949: val_loss did not improve from 21.80631
196/196 - 50s - loss: 21.7195 - MinusLogProbMetric: 21.7195 - val_loss: 21.8730 - val_MinusLogProbMetric: 21.8730 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 950/1000
2023-10-09 23:19:27.100 
Epoch 950/1000 
	 loss: 21.7188, MinusLogProbMetric: 21.7188, val_loss: 21.8508, val_MinusLogProbMetric: 21.8508

Epoch 950: val_loss did not improve from 21.80631
196/196 - 51s - loss: 21.7188 - MinusLogProbMetric: 21.7188 - val_loss: 21.8508 - val_MinusLogProbMetric: 21.8508 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 951/1000
2023-10-09 23:20:16.876 
Epoch 951/1000 
	 loss: 21.7036, MinusLogProbMetric: 21.7036, val_loss: 21.8500, val_MinusLogProbMetric: 21.8500

Epoch 951: val_loss did not improve from 21.80631
196/196 - 50s - loss: 21.7036 - MinusLogProbMetric: 21.7036 - val_loss: 21.8500 - val_MinusLogProbMetric: 21.8500 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 952/1000
2023-10-09 23:21:06.619 
Epoch 952/1000 
	 loss: 21.7735, MinusLogProbMetric: 21.7735, val_loss: 21.7761, val_MinusLogProbMetric: 21.7761

Epoch 952: val_loss improved from 21.80631 to 21.77607, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 50s - loss: 21.7735 - MinusLogProbMetric: 21.7735 - val_loss: 21.7761 - val_MinusLogProbMetric: 21.7761 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 953/1000
2023-10-09 23:21:58.433 
Epoch 953/1000 
	 loss: 21.6894, MinusLogProbMetric: 21.6894, val_loss: 21.8365, val_MinusLogProbMetric: 21.8365

Epoch 953: val_loss did not improve from 21.77607
196/196 - 51s - loss: 21.6894 - MinusLogProbMetric: 21.6894 - val_loss: 21.8365 - val_MinusLogProbMetric: 21.8365 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 954/1000
2023-10-09 23:22:48.154 
Epoch 954/1000 
	 loss: 21.6811, MinusLogProbMetric: 21.6811, val_loss: 21.8180, val_MinusLogProbMetric: 21.8180

Epoch 954: val_loss did not improve from 21.77607
196/196 - 50s - loss: 21.6811 - MinusLogProbMetric: 21.6811 - val_loss: 21.8180 - val_MinusLogProbMetric: 21.8180 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 955/1000
2023-10-09 23:23:37.511 
Epoch 955/1000 
	 loss: 21.6614, MinusLogProbMetric: 21.6614, val_loss: 21.8469, val_MinusLogProbMetric: 21.8469

Epoch 955: val_loss did not improve from 21.77607
196/196 - 49s - loss: 21.6614 - MinusLogProbMetric: 21.6614 - val_loss: 21.8469 - val_MinusLogProbMetric: 21.8469 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 956/1000
2023-10-09 23:24:31.159 
Epoch 956/1000 
	 loss: 21.6641, MinusLogProbMetric: 21.6641, val_loss: 21.9227, val_MinusLogProbMetric: 21.9227

Epoch 956: val_loss did not improve from 21.77607
196/196 - 54s - loss: 21.6641 - MinusLogProbMetric: 21.6641 - val_loss: 21.9227 - val_MinusLogProbMetric: 21.9227 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 957/1000
2023-10-09 23:25:23.219 
Epoch 957/1000 
	 loss: 21.6581, MinusLogProbMetric: 21.6581, val_loss: 21.8050, val_MinusLogProbMetric: 21.8050

Epoch 957: val_loss did not improve from 21.77607
196/196 - 52s - loss: 21.6581 - MinusLogProbMetric: 21.6581 - val_loss: 21.8050 - val_MinusLogProbMetric: 21.8050 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 958/1000
2023-10-09 23:26:13.516 
Epoch 958/1000 
	 loss: 21.6590, MinusLogProbMetric: 21.6590, val_loss: 21.9248, val_MinusLogProbMetric: 21.9248

Epoch 958: val_loss did not improve from 21.77607
196/196 - 50s - loss: 21.6590 - MinusLogProbMetric: 21.6590 - val_loss: 21.9248 - val_MinusLogProbMetric: 21.9248 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 959/1000
2023-10-09 23:27:04.483 
Epoch 959/1000 
	 loss: 21.6497, MinusLogProbMetric: 21.6497, val_loss: 21.7652, val_MinusLogProbMetric: 21.7652

Epoch 959: val_loss improved from 21.77607 to 21.76525, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 21.6497 - MinusLogProbMetric: 21.6497 - val_loss: 21.7652 - val_MinusLogProbMetric: 21.7652 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 960/1000
2023-10-09 23:27:57.012 
Epoch 960/1000 
	 loss: 21.6363, MinusLogProbMetric: 21.6363, val_loss: 21.7820, val_MinusLogProbMetric: 21.7820

Epoch 960: val_loss did not improve from 21.76525
196/196 - 52s - loss: 21.6363 - MinusLogProbMetric: 21.6363 - val_loss: 21.7820 - val_MinusLogProbMetric: 21.7820 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 961/1000
2023-10-09 23:28:47.128 
Epoch 961/1000 
	 loss: 21.6405, MinusLogProbMetric: 21.6405, val_loss: 21.7526, val_MinusLogProbMetric: 21.7526

Epoch 961: val_loss improved from 21.76525 to 21.75262, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 21.6405 - MinusLogProbMetric: 21.6405 - val_loss: 21.7526 - val_MinusLogProbMetric: 21.7526 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 962/1000
2023-10-09 23:29:41.019 
Epoch 962/1000 
	 loss: 21.6286, MinusLogProbMetric: 21.6286, val_loss: 21.7187, val_MinusLogProbMetric: 21.7187

Epoch 962: val_loss improved from 21.75262 to 21.71871, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 54s - loss: 21.6286 - MinusLogProbMetric: 21.6286 - val_loss: 21.7187 - val_MinusLogProbMetric: 21.7187 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 963/1000
2023-10-09 23:30:33.225 
Epoch 963/1000 
	 loss: 21.6301, MinusLogProbMetric: 21.6301, val_loss: 21.8633, val_MinusLogProbMetric: 21.8633

Epoch 963: val_loss did not improve from 21.71871
196/196 - 51s - loss: 21.6301 - MinusLogProbMetric: 21.6301 - val_loss: 21.8633 - val_MinusLogProbMetric: 21.8633 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 964/1000
2023-10-09 23:31:23.099 
Epoch 964/1000 
	 loss: 21.6437, MinusLogProbMetric: 21.6437, val_loss: 21.6809, val_MinusLogProbMetric: 21.6809

Epoch 964: val_loss improved from 21.71871 to 21.68092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 51s - loss: 21.6437 - MinusLogProbMetric: 21.6437 - val_loss: 21.6809 - val_MinusLogProbMetric: 21.6809 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 965/1000
2023-10-09 23:32:12.930 
Epoch 965/1000 
	 loss: 21.6168, MinusLogProbMetric: 21.6168, val_loss: 21.7212, val_MinusLogProbMetric: 21.7212

Epoch 965: val_loss did not improve from 21.68092
196/196 - 49s - loss: 21.6168 - MinusLogProbMetric: 21.6168 - val_loss: 21.7212 - val_MinusLogProbMetric: 21.7212 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 966/1000
2023-10-09 23:33:03.007 
Epoch 966/1000 
	 loss: 21.6197, MinusLogProbMetric: 21.6197, val_loss: 21.8231, val_MinusLogProbMetric: 21.8231

Epoch 966: val_loss did not improve from 21.68092
196/196 - 50s - loss: 21.6197 - MinusLogProbMetric: 21.6197 - val_loss: 21.8231 - val_MinusLogProbMetric: 21.8231 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 967/1000
2023-10-09 23:33:54.896 
Epoch 967/1000 
	 loss: 21.6291, MinusLogProbMetric: 21.6291, val_loss: 21.6848, val_MinusLogProbMetric: 21.6848

Epoch 967: val_loss did not improve from 21.68092
196/196 - 52s - loss: 21.6291 - MinusLogProbMetric: 21.6291 - val_loss: 21.6848 - val_MinusLogProbMetric: 21.6848 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 968/1000
2023-10-09 23:34:46.035 
Epoch 968/1000 
	 loss: 21.6101, MinusLogProbMetric: 21.6101, val_loss: 21.8094, val_MinusLogProbMetric: 21.8094

Epoch 968: val_loss did not improve from 21.68092
196/196 - 51s - loss: 21.6101 - MinusLogProbMetric: 21.6101 - val_loss: 21.8094 - val_MinusLogProbMetric: 21.8094 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 969/1000
2023-10-09 23:35:39.998 
Epoch 969/1000 
	 loss: 21.6181, MinusLogProbMetric: 21.6181, val_loss: 21.8535, val_MinusLogProbMetric: 21.8535

Epoch 969: val_loss did not improve from 21.68092
196/196 - 54s - loss: 21.6181 - MinusLogProbMetric: 21.6181 - val_loss: 21.8535 - val_MinusLogProbMetric: 21.8535 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 970/1000
2023-10-09 23:36:31.501 
Epoch 970/1000 
	 loss: 21.6079, MinusLogProbMetric: 21.6079, val_loss: 21.7647, val_MinusLogProbMetric: 21.7647

Epoch 970: val_loss did not improve from 21.68092
196/196 - 51s - loss: 21.6079 - MinusLogProbMetric: 21.6079 - val_loss: 21.7647 - val_MinusLogProbMetric: 21.7647 - lr: 5.5556e-05 - 51s/epoch - 263ms/step
Epoch 971/1000
2023-10-09 23:37:21.907 
Epoch 971/1000 
	 loss: 21.6051, MinusLogProbMetric: 21.6051, val_loss: 21.7682, val_MinusLogProbMetric: 21.7682

Epoch 971: val_loss did not improve from 21.68092
196/196 - 50s - loss: 21.6051 - MinusLogProbMetric: 21.6051 - val_loss: 21.7682 - val_MinusLogProbMetric: 21.7682 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 972/1000
2023-10-09 23:38:13.751 
Epoch 972/1000 
	 loss: 21.5957, MinusLogProbMetric: 21.5957, val_loss: 21.7299, val_MinusLogProbMetric: 21.7299

Epoch 972: val_loss did not improve from 21.68092
196/196 - 52s - loss: 21.5957 - MinusLogProbMetric: 21.5957 - val_loss: 21.7299 - val_MinusLogProbMetric: 21.7299 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 973/1000
2023-10-09 23:39:06.114 
Epoch 973/1000 
	 loss: 21.5889, MinusLogProbMetric: 21.5889, val_loss: 21.8150, val_MinusLogProbMetric: 21.8150

Epoch 973: val_loss did not improve from 21.68092
196/196 - 52s - loss: 21.5889 - MinusLogProbMetric: 21.5889 - val_loss: 21.8150 - val_MinusLogProbMetric: 21.8150 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 974/1000
2023-10-09 23:39:58.709 
Epoch 974/1000 
	 loss: 21.6098, MinusLogProbMetric: 21.6098, val_loss: 21.8880, val_MinusLogProbMetric: 21.8880

Epoch 974: val_loss did not improve from 21.68092
196/196 - 53s - loss: 21.6098 - MinusLogProbMetric: 21.6098 - val_loss: 21.8880 - val_MinusLogProbMetric: 21.8880 - lr: 5.5556e-05 - 53s/epoch - 268ms/step
Epoch 975/1000
2023-10-09 23:40:50.733 
Epoch 975/1000 
	 loss: 21.6037, MinusLogProbMetric: 21.6037, val_loss: 21.7886, val_MinusLogProbMetric: 21.7886

Epoch 975: val_loss did not improve from 21.68092
196/196 - 52s - loss: 21.6037 - MinusLogProbMetric: 21.6037 - val_loss: 21.7886 - val_MinusLogProbMetric: 21.7886 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 976/1000
2023-10-09 23:41:41.502 
Epoch 976/1000 
	 loss: 21.5893, MinusLogProbMetric: 21.5893, val_loss: 21.7390, val_MinusLogProbMetric: 21.7390

Epoch 976: val_loss did not improve from 21.68092
196/196 - 51s - loss: 21.5893 - MinusLogProbMetric: 21.5893 - val_loss: 21.7390 - val_MinusLogProbMetric: 21.7390 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 977/1000
2023-10-09 23:42:34.116 
Epoch 977/1000 
	 loss: 21.5854, MinusLogProbMetric: 21.5854, val_loss: 21.6671, val_MinusLogProbMetric: 21.6671

Epoch 977: val_loss improved from 21.68092 to 21.66715, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 21.5854 - MinusLogProbMetric: 21.5854 - val_loss: 21.6671 - val_MinusLogProbMetric: 21.6671 - lr: 5.5556e-05 - 53s/epoch - 272ms/step
Epoch 978/1000
2023-10-09 23:43:25.774 
Epoch 978/1000 
	 loss: 21.5832, MinusLogProbMetric: 21.5832, val_loss: 21.7275, val_MinusLogProbMetric: 21.7275

Epoch 978: val_loss did not improve from 21.66715
196/196 - 51s - loss: 21.5832 - MinusLogProbMetric: 21.5832 - val_loss: 21.7275 - val_MinusLogProbMetric: 21.7275 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 979/1000
2023-10-09 23:44:17.444 
Epoch 979/1000 
	 loss: 21.5828, MinusLogProbMetric: 21.5828, val_loss: 21.6802, val_MinusLogProbMetric: 21.6802

Epoch 979: val_loss did not improve from 21.66715
196/196 - 52s - loss: 21.5828 - MinusLogProbMetric: 21.5828 - val_loss: 21.6802 - val_MinusLogProbMetric: 21.6802 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 980/1000
2023-10-09 23:45:08.362 
Epoch 980/1000 
	 loss: 21.5771, MinusLogProbMetric: 21.5771, val_loss: 21.7680, val_MinusLogProbMetric: 21.7680

Epoch 980: val_loss did not improve from 21.66715
196/196 - 51s - loss: 21.5771 - MinusLogProbMetric: 21.5771 - val_loss: 21.7680 - val_MinusLogProbMetric: 21.7680 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 981/1000
2023-10-09 23:46:00.524 
Epoch 981/1000 
	 loss: 21.5953, MinusLogProbMetric: 21.5953, val_loss: 21.8228, val_MinusLogProbMetric: 21.8228

Epoch 981: val_loss did not improve from 21.66715
196/196 - 52s - loss: 21.5953 - MinusLogProbMetric: 21.5953 - val_loss: 21.8228 - val_MinusLogProbMetric: 21.8228 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 982/1000
2023-10-09 23:46:52.596 
Epoch 982/1000 
	 loss: 21.5971, MinusLogProbMetric: 21.5971, val_loss: 21.8522, val_MinusLogProbMetric: 21.8522

Epoch 982: val_loss did not improve from 21.66715
196/196 - 52s - loss: 21.5971 - MinusLogProbMetric: 21.5971 - val_loss: 21.8522 - val_MinusLogProbMetric: 21.8522 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 983/1000
2023-10-09 23:47:45.393 
Epoch 983/1000 
	 loss: 21.5674, MinusLogProbMetric: 21.5674, val_loss: 21.6874, val_MinusLogProbMetric: 21.6874

Epoch 983: val_loss did not improve from 21.66715
196/196 - 53s - loss: 21.5674 - MinusLogProbMetric: 21.5674 - val_loss: 21.6874 - val_MinusLogProbMetric: 21.6874 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 984/1000
2023-10-09 23:48:37.656 
Epoch 984/1000 
	 loss: 21.5703, MinusLogProbMetric: 21.5703, val_loss: 21.7463, val_MinusLogProbMetric: 21.7463

Epoch 984: val_loss did not improve from 21.66715
196/196 - 52s - loss: 21.5703 - MinusLogProbMetric: 21.5703 - val_loss: 21.7463 - val_MinusLogProbMetric: 21.7463 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 985/1000
2023-10-09 23:49:29.721 
Epoch 985/1000 
	 loss: 21.5603, MinusLogProbMetric: 21.5603, val_loss: 21.9306, val_MinusLogProbMetric: 21.9306

Epoch 985: val_loss did not improve from 21.66715
196/196 - 52s - loss: 21.5603 - MinusLogProbMetric: 21.5603 - val_loss: 21.9306 - val_MinusLogProbMetric: 21.9306 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 986/1000
2023-10-09 23:50:19.919 
Epoch 986/1000 
	 loss: 21.5531, MinusLogProbMetric: 21.5531, val_loss: 21.7009, val_MinusLogProbMetric: 21.7009

Epoch 986: val_loss did not improve from 21.66715
196/196 - 50s - loss: 21.5531 - MinusLogProbMetric: 21.5531 - val_loss: 21.7009 - val_MinusLogProbMetric: 21.7009 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 987/1000
2023-10-09 23:51:09.888 
Epoch 987/1000 
	 loss: 21.5919, MinusLogProbMetric: 21.5919, val_loss: 21.7666, val_MinusLogProbMetric: 21.7666

Epoch 987: val_loss did not improve from 21.66715
196/196 - 50s - loss: 21.5919 - MinusLogProbMetric: 21.5919 - val_loss: 21.7666 - val_MinusLogProbMetric: 21.7666 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 988/1000
2023-10-09 23:52:01.802 
Epoch 988/1000 
	 loss: 21.5417, MinusLogProbMetric: 21.5417, val_loss: 21.6826, val_MinusLogProbMetric: 21.6826

Epoch 988: val_loss did not improve from 21.66715
196/196 - 52s - loss: 21.5417 - MinusLogProbMetric: 21.5417 - val_loss: 21.6826 - val_MinusLogProbMetric: 21.6826 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 989/1000
2023-10-09 23:52:53.295 
Epoch 989/1000 
	 loss: 21.5642, MinusLogProbMetric: 21.5642, val_loss: 21.7199, val_MinusLogProbMetric: 21.7199

Epoch 989: val_loss did not improve from 21.66715
196/196 - 51s - loss: 21.5642 - MinusLogProbMetric: 21.5642 - val_loss: 21.7199 - val_MinusLogProbMetric: 21.7199 - lr: 5.5556e-05 - 51s/epoch - 263ms/step
Epoch 990/1000
2023-10-09 23:53:45.384 
Epoch 990/1000 
	 loss: 21.5673, MinusLogProbMetric: 21.5673, val_loss: 21.6618, val_MinusLogProbMetric: 21.6618

Epoch 990: val_loss improved from 21.66715 to 21.66177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 21.5673 - MinusLogProbMetric: 21.5673 - val_loss: 21.6618 - val_MinusLogProbMetric: 21.6618 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 991/1000
2023-10-09 23:54:37.434 
Epoch 991/1000 
	 loss: 21.5593, MinusLogProbMetric: 21.5593, val_loss: 21.6034, val_MinusLogProbMetric: 21.6034

Epoch 991: val_loss improved from 21.66177 to 21.60340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 52s - loss: 21.5593 - MinusLogProbMetric: 21.5593 - val_loss: 21.6034 - val_MinusLogProbMetric: 21.6034 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 992/1000
2023-10-09 23:55:28.131 
Epoch 992/1000 
	 loss: 21.5280, MinusLogProbMetric: 21.5280, val_loss: 21.7319, val_MinusLogProbMetric: 21.7319

Epoch 992: val_loss did not improve from 21.60340
196/196 - 50s - loss: 21.5280 - MinusLogProbMetric: 21.5280 - val_loss: 21.7319 - val_MinusLogProbMetric: 21.7319 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 993/1000
2023-10-09 23:56:20.426 
Epoch 993/1000 
	 loss: 21.5334, MinusLogProbMetric: 21.5334, val_loss: 21.7899, val_MinusLogProbMetric: 21.7899

Epoch 993: val_loss did not improve from 21.60340
196/196 - 52s - loss: 21.5334 - MinusLogProbMetric: 21.5334 - val_loss: 21.7899 - val_MinusLogProbMetric: 21.7899 - lr: 5.5556e-05 - 52s/epoch - 267ms/step
Epoch 994/1000
2023-10-09 23:57:12.284 
Epoch 994/1000 
	 loss: 21.5198, MinusLogProbMetric: 21.5198, val_loss: 21.7272, val_MinusLogProbMetric: 21.7272

Epoch 994: val_loss did not improve from 21.60340
196/196 - 52s - loss: 21.5198 - MinusLogProbMetric: 21.5198 - val_loss: 21.7272 - val_MinusLogProbMetric: 21.7272 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 995/1000
2023-10-09 23:58:05.557 
Epoch 995/1000 
	 loss: 21.5457, MinusLogProbMetric: 21.5457, val_loss: 21.6409, val_MinusLogProbMetric: 21.6409

Epoch 995: val_loss did not improve from 21.60340
196/196 - 53s - loss: 21.5457 - MinusLogProbMetric: 21.5457 - val_loss: 21.6409 - val_MinusLogProbMetric: 21.6409 - lr: 5.5556e-05 - 53s/epoch - 272ms/step
Epoch 996/1000
2023-10-09 23:58:55.143 
Epoch 996/1000 
	 loss: 21.5105, MinusLogProbMetric: 21.5105, val_loss: 21.7065, val_MinusLogProbMetric: 21.7065

Epoch 996: val_loss did not improve from 21.60340
196/196 - 50s - loss: 21.5105 - MinusLogProbMetric: 21.5105 - val_loss: 21.7065 - val_MinusLogProbMetric: 21.7065 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 997/1000
2023-10-09 23:59:47.892 
Epoch 997/1000 
	 loss: 21.5083, MinusLogProbMetric: 21.5083, val_loss: 21.6340, val_MinusLogProbMetric: 21.6340

Epoch 997: val_loss did not improve from 21.60340
196/196 - 53s - loss: 21.5083 - MinusLogProbMetric: 21.5083 - val_loss: 21.6340 - val_MinusLogProbMetric: 21.6340 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 998/1000
2023-10-10 00:00:40.091 
Epoch 998/1000 
	 loss: 21.5306, MinusLogProbMetric: 21.5306, val_loss: 21.6020, val_MinusLogProbMetric: 21.6020

Epoch 998: val_loss improved from 21.60340 to 21.60203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_245/weights/best_weights.h5
196/196 - 53s - loss: 21.5306 - MinusLogProbMetric: 21.5306 - val_loss: 21.6020 - val_MinusLogProbMetric: 21.6020 - lr: 5.5556e-05 - 53s/epoch - 270ms/step
Epoch 999/1000
2023-10-10 00:01:32.414 
Epoch 999/1000 
	 loss: 21.5019, MinusLogProbMetric: 21.5019, val_loss: 21.6460, val_MinusLogProbMetric: 21.6460

Epoch 999: val_loss did not improve from 21.60203
196/196 - 52s - loss: 21.5019 - MinusLogProbMetric: 21.5019 - val_loss: 21.6460 - val_MinusLogProbMetric: 21.6460 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 1000/1000
2023-10-10 00:02:23.042 
Epoch 1000/1000 
	 loss: 21.5256, MinusLogProbMetric: 21.5256, val_loss: 21.8363, val_MinusLogProbMetric: 21.8363

Epoch 1000: val_loss did not improve from 21.60203
196/196 - 51s - loss: 21.5256 - MinusLogProbMetric: 21.5256 - val_loss: 21.8363 - val_MinusLogProbMetric: 21.8363 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
LR metric calculation completed in 23.513156896922737 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 14.095350353047252 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 10.463184694992378 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
FN metric calculation completed in 11.38232716685161 seconds.
Training succeeded with seed 0.
Model trained in 52250.77 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 60.77 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 61.31 s.
===========
Run 245/720 done in 52593.04 s.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

===========
Generating train data for run 263.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f13c15da050>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f13eb8c7100>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f13eb8c7100>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d68a87e20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0d1cdaec50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0d1cdaf1c0>, <keras.callbacks.ModelCheckpoint object at 0x7f0d1cdaf280>, <keras.callbacks.EarlyStopping object at 0x7f0d1cdaf4f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0d1cdaf520>, <keras.callbacks.TerminateOnNaN object at 0x7f0d1cdaf160>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_263/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-10 00:03:33.331232
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 37: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 00:06:39.062 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 2507.0190, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 185s - loss: nan - MinusLogProbMetric: 2507.0190 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 185s/epoch - 945ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 263.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f0cedf62620>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0ceda57fa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0ceda57fa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b714a3eb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0ced9d6d40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0ced9d72b0>, <keras.callbacks.ModelCheckpoint object at 0x7f0ced9d7370>, <keras.callbacks.EarlyStopping object at 0x7f0ced9d75e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0ced9d7610>, <keras.callbacks.TerminateOnNaN object at 0x7f0ced9d7250>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_263/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-10 00:06:49.409920
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 153: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 00:10:31.999 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1538.9000, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 222s - loss: nan - MinusLogProbMetric: 1538.9000 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 222s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 263.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_65"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_66 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f0cecc53e20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0cecd82710>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0cecd82710>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0ce4422710>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0cecc83610>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0cecc83b80>, <keras.callbacks.ModelCheckpoint object at 0x7f0cecc83c40>, <keras.callbacks.EarlyStopping object at 0x7f0cecc83eb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0cecc83ee0>, <keras.callbacks.TerminateOnNaN object at 0x7f0cecc83b20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_263/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-10 00:10:40.248340
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
2023-10-10 00:13:59.827 
Epoch 1/1000 
	 loss: 1580.4425, MinusLogProbMetric: 1580.4425, val_loss: 792.3264, val_MinusLogProbMetric: 792.3264

Epoch 1: val_loss improved from inf to 792.32642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 200s - loss: 1580.4425 - MinusLogProbMetric: 1580.4425 - val_loss: 792.3264 - val_MinusLogProbMetric: 792.3264 - lr: 1.1111e-04 - 200s/epoch - 1s/step
Epoch 2/1000
2023-10-10 00:15:03.365 
Epoch 2/1000 
	 loss: 581.2645, MinusLogProbMetric: 581.2645, val_loss: 564.0687, val_MinusLogProbMetric: 564.0687

Epoch 2: val_loss improved from 792.32642 to 564.06873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 581.2645 - MinusLogProbMetric: 581.2645 - val_loss: 564.0687 - val_MinusLogProbMetric: 564.0687 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 3/1000
2023-10-10 00:16:06.076 
Epoch 3/1000 
	 loss: 381.2946, MinusLogProbMetric: 381.2946, val_loss: 326.0013, val_MinusLogProbMetric: 326.0013

Epoch 3: val_loss improved from 564.06873 to 326.00131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 381.2946 - MinusLogProbMetric: 381.2946 - val_loss: 326.0013 - val_MinusLogProbMetric: 326.0013 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 4/1000
2023-10-10 00:17:09.679 
Epoch 4/1000 
	 loss: 308.2100, MinusLogProbMetric: 308.2100, val_loss: 277.5617, val_MinusLogProbMetric: 277.5617

Epoch 4: val_loss improved from 326.00131 to 277.56168, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 308.2100 - MinusLogProbMetric: 308.2100 - val_loss: 277.5617 - val_MinusLogProbMetric: 277.5617 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 5/1000
2023-10-10 00:18:12.656 
Epoch 5/1000 
	 loss: 266.2536, MinusLogProbMetric: 266.2536, val_loss: 251.2143, val_MinusLogProbMetric: 251.2143

Epoch 5: val_loss improved from 277.56168 to 251.21426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 266.2536 - MinusLogProbMetric: 266.2536 - val_loss: 251.2143 - val_MinusLogProbMetric: 251.2143 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 6/1000
2023-10-10 00:19:15.795 
Epoch 6/1000 
	 loss: 245.6089, MinusLogProbMetric: 245.6089, val_loss: 234.2059, val_MinusLogProbMetric: 234.2059

Epoch 6: val_loss improved from 251.21426 to 234.20592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 245.6089 - MinusLogProbMetric: 245.6089 - val_loss: 234.2059 - val_MinusLogProbMetric: 234.2059 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 7/1000
2023-10-10 00:20:20.490 
Epoch 7/1000 
	 loss: 224.7827, MinusLogProbMetric: 224.7827, val_loss: 219.0136, val_MinusLogProbMetric: 219.0136

Epoch 7: val_loss improved from 234.20592 to 219.01363, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 224.7827 - MinusLogProbMetric: 224.7827 - val_loss: 219.0136 - val_MinusLogProbMetric: 219.0136 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 8/1000
2023-10-10 00:21:24.371 
Epoch 8/1000 
	 loss: 209.0664, MinusLogProbMetric: 209.0664, val_loss: 201.5945, val_MinusLogProbMetric: 201.5945

Epoch 8: val_loss improved from 219.01363 to 201.59453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 209.0664 - MinusLogProbMetric: 209.0664 - val_loss: 201.5945 - val_MinusLogProbMetric: 201.5945 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 9/1000
2023-10-10 00:22:26.983 
Epoch 9/1000 
	 loss: 204.3835, MinusLogProbMetric: 204.3835, val_loss: 200.4253, val_MinusLogProbMetric: 200.4253

Epoch 9: val_loss improved from 201.59453 to 200.42529, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 204.3835 - MinusLogProbMetric: 204.3835 - val_loss: 200.4253 - val_MinusLogProbMetric: 200.4253 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 10/1000
2023-10-10 00:23:30.079 
Epoch 10/1000 
	 loss: 232.0244, MinusLogProbMetric: 232.0244, val_loss: 263.6378, val_MinusLogProbMetric: 263.6378

Epoch 10: val_loss did not improve from 200.42529
196/196 - 62s - loss: 232.0244 - MinusLogProbMetric: 232.0244 - val_loss: 263.6378 - val_MinusLogProbMetric: 263.6378 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 11/1000
2023-10-10 00:24:33.324 
Epoch 11/1000 
	 loss: 222.3779, MinusLogProbMetric: 222.3779, val_loss: 206.1086, val_MinusLogProbMetric: 206.1086

Epoch 11: val_loss did not improve from 200.42529
196/196 - 63s - loss: 222.3779 - MinusLogProbMetric: 222.3779 - val_loss: 206.1086 - val_MinusLogProbMetric: 206.1086 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 12/1000
2023-10-10 00:25:35.430 
Epoch 12/1000 
	 loss: 196.5812, MinusLogProbMetric: 196.5812, val_loss: 188.4585, val_MinusLogProbMetric: 188.4585

Epoch 12: val_loss improved from 200.42529 to 188.45851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 196.5812 - MinusLogProbMetric: 196.5812 - val_loss: 188.4585 - val_MinusLogProbMetric: 188.4585 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 13/1000
2023-10-10 00:26:39.599 
Epoch 13/1000 
	 loss: 184.5102, MinusLogProbMetric: 184.5102, val_loss: 179.2659, val_MinusLogProbMetric: 179.2659

Epoch 13: val_loss improved from 188.45851 to 179.26595, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 184.5102 - MinusLogProbMetric: 184.5102 - val_loss: 179.2659 - val_MinusLogProbMetric: 179.2659 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 14/1000
2023-10-10 00:27:43.876 
Epoch 14/1000 
	 loss: 173.0407, MinusLogProbMetric: 173.0407, val_loss: 172.5107, val_MinusLogProbMetric: 172.5107

Epoch 14: val_loss improved from 179.26595 to 172.51073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 173.0407 - MinusLogProbMetric: 173.0407 - val_loss: 172.5107 - val_MinusLogProbMetric: 172.5107 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 15/1000
2023-10-10 00:28:47.349 
Epoch 15/1000 
	 loss: 191.6110, MinusLogProbMetric: 191.6110, val_loss: 202.5277, val_MinusLogProbMetric: 202.5277

Epoch 15: val_loss did not improve from 172.51073
196/196 - 62s - loss: 191.6110 - MinusLogProbMetric: 191.6110 - val_loss: 202.5277 - val_MinusLogProbMetric: 202.5277 - lr: 1.1111e-04 - 62s/epoch - 319ms/step
Epoch 16/1000
2023-10-10 00:29:49.772 
Epoch 16/1000 
	 loss: 189.0214, MinusLogProbMetric: 189.0214, val_loss: 176.0586, val_MinusLogProbMetric: 176.0586

Epoch 16: val_loss did not improve from 172.51073
196/196 - 62s - loss: 189.0214 - MinusLogProbMetric: 189.0214 - val_loss: 176.0586 - val_MinusLogProbMetric: 176.0586 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 17/1000
2023-10-10 00:30:50.783 
Epoch 17/1000 
	 loss: 165.1619, MinusLogProbMetric: 165.1619, val_loss: 159.0707, val_MinusLogProbMetric: 159.0707

Epoch 17: val_loss improved from 172.51073 to 159.07069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 165.1619 - MinusLogProbMetric: 165.1619 - val_loss: 159.0707 - val_MinusLogProbMetric: 159.0707 - lr: 1.1111e-04 - 62s/epoch - 317ms/step
Epoch 18/1000
2023-10-10 00:31:53.866 
Epoch 18/1000 
	 loss: 159.1945, MinusLogProbMetric: 159.1945, val_loss: 172.2355, val_MinusLogProbMetric: 172.2355

Epoch 18: val_loss did not improve from 159.07069
196/196 - 62s - loss: 159.1945 - MinusLogProbMetric: 159.1945 - val_loss: 172.2355 - val_MinusLogProbMetric: 172.2355 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 19/1000
2023-10-10 00:32:57.639 
Epoch 19/1000 
	 loss: 154.4164, MinusLogProbMetric: 154.4164, val_loss: 146.8297, val_MinusLogProbMetric: 146.8297

Epoch 19: val_loss improved from 159.07069 to 146.82974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 154.4164 - MinusLogProbMetric: 154.4164 - val_loss: 146.8297 - val_MinusLogProbMetric: 146.8297 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 20/1000
2023-10-10 00:34:01.282 
Epoch 20/1000 
	 loss: 145.0565, MinusLogProbMetric: 145.0565, val_loss: 142.4051, val_MinusLogProbMetric: 142.4051

Epoch 20: val_loss improved from 146.82974 to 142.40511, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 145.0565 - MinusLogProbMetric: 145.0565 - val_loss: 142.4051 - val_MinusLogProbMetric: 142.4051 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 21/1000
2023-10-10 00:35:04.269 
Epoch 21/1000 
	 loss: 139.9329, MinusLogProbMetric: 139.9329, val_loss: 137.2568, val_MinusLogProbMetric: 137.2568

Epoch 21: val_loss improved from 142.40511 to 137.25677, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 139.9329 - MinusLogProbMetric: 139.9329 - val_loss: 137.2568 - val_MinusLogProbMetric: 137.2568 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 22/1000
2023-10-10 00:36:08.285 
Epoch 22/1000 
	 loss: 138.1648, MinusLogProbMetric: 138.1648, val_loss: 134.2511, val_MinusLogProbMetric: 134.2511

Epoch 22: val_loss improved from 137.25677 to 134.25113, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 138.1648 - MinusLogProbMetric: 138.1648 - val_loss: 134.2511 - val_MinusLogProbMetric: 134.2511 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 23/1000
2023-10-10 00:37:11.782 
Epoch 23/1000 
	 loss: 145.6240, MinusLogProbMetric: 145.6240, val_loss: 140.9027, val_MinusLogProbMetric: 140.9027

Epoch 23: val_loss did not improve from 134.25113
196/196 - 62s - loss: 145.6240 - MinusLogProbMetric: 145.6240 - val_loss: 140.9027 - val_MinusLogProbMetric: 140.9027 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 24/1000
2023-10-10 00:38:13.910 
Epoch 24/1000 
	 loss: 134.3876, MinusLogProbMetric: 134.3876, val_loss: 129.2363, val_MinusLogProbMetric: 129.2363

Epoch 24: val_loss improved from 134.25113 to 129.23634, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 134.3876 - MinusLogProbMetric: 134.3876 - val_loss: 129.2363 - val_MinusLogProbMetric: 129.2363 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 25/1000
2023-10-10 00:39:17.199 
Epoch 25/1000 
	 loss: 127.2996, MinusLogProbMetric: 127.2996, val_loss: 126.6790, val_MinusLogProbMetric: 126.6790

Epoch 25: val_loss improved from 129.23634 to 126.67896, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 127.2996 - MinusLogProbMetric: 127.2996 - val_loss: 126.6790 - val_MinusLogProbMetric: 126.6790 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 26/1000
2023-10-10 00:40:20.023 
Epoch 26/1000 
	 loss: 124.9789, MinusLogProbMetric: 124.9789, val_loss: 123.8680, val_MinusLogProbMetric: 123.8680

Epoch 26: val_loss improved from 126.67896 to 123.86797, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 124.9789 - MinusLogProbMetric: 124.9789 - val_loss: 123.8680 - val_MinusLogProbMetric: 123.8680 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 27/1000
2023-10-10 00:41:23.007 
Epoch 27/1000 
	 loss: 121.5429, MinusLogProbMetric: 121.5429, val_loss: 120.2405, val_MinusLogProbMetric: 120.2405

Epoch 27: val_loss improved from 123.86797 to 120.24048, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 121.5429 - MinusLogProbMetric: 121.5429 - val_loss: 120.2405 - val_MinusLogProbMetric: 120.2405 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 28/1000
2023-10-10 00:42:27.241 
Epoch 28/1000 
	 loss: 121.6975, MinusLogProbMetric: 121.6975, val_loss: 123.0368, val_MinusLogProbMetric: 123.0368

Epoch 28: val_loss did not improve from 120.24048
196/196 - 63s - loss: 121.6975 - MinusLogProbMetric: 121.6975 - val_loss: 123.0368 - val_MinusLogProbMetric: 123.0368 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 29/1000
2023-10-10 00:43:29.640 
Epoch 29/1000 
	 loss: 122.5427, MinusLogProbMetric: 122.5427, val_loss: 119.4554, val_MinusLogProbMetric: 119.4554

Epoch 29: val_loss improved from 120.24048 to 119.45540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 122.5427 - MinusLogProbMetric: 122.5427 - val_loss: 119.4554 - val_MinusLogProbMetric: 119.4554 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 30/1000
2023-10-10 00:44:33.057 
Epoch 30/1000 
	 loss: 116.0995, MinusLogProbMetric: 116.0995, val_loss: 114.1732, val_MinusLogProbMetric: 114.1732

Epoch 30: val_loss improved from 119.45540 to 114.17316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 116.0995 - MinusLogProbMetric: 116.0995 - val_loss: 114.1732 - val_MinusLogProbMetric: 114.1732 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 31/1000
2023-10-10 00:45:36.150 
Epoch 31/1000 
	 loss: 115.5474, MinusLogProbMetric: 115.5474, val_loss: 115.1403, val_MinusLogProbMetric: 115.1403

Epoch 31: val_loss did not improve from 114.17316
196/196 - 62s - loss: 115.5474 - MinusLogProbMetric: 115.5474 - val_loss: 115.1403 - val_MinusLogProbMetric: 115.1403 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 32/1000
2023-10-10 00:46:38.139 
Epoch 32/1000 
	 loss: 110.9464, MinusLogProbMetric: 110.9464, val_loss: 109.2083, val_MinusLogProbMetric: 109.2083

Epoch 32: val_loss improved from 114.17316 to 109.20827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 110.9464 - MinusLogProbMetric: 110.9464 - val_loss: 109.2083 - val_MinusLogProbMetric: 109.2083 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 33/1000
2023-10-10 00:47:42.061 
Epoch 33/1000 
	 loss: 108.2538, MinusLogProbMetric: 108.2538, val_loss: 107.0182, val_MinusLogProbMetric: 107.0182

Epoch 33: val_loss improved from 109.20827 to 107.01823, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 108.2538 - MinusLogProbMetric: 108.2538 - val_loss: 107.0182 - val_MinusLogProbMetric: 107.0182 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 34/1000
2023-10-10 00:48:44.652 
Epoch 34/1000 
	 loss: 108.2180, MinusLogProbMetric: 108.2180, val_loss: 118.3097, val_MinusLogProbMetric: 118.3097

Epoch 34: val_loss did not improve from 107.01823
196/196 - 62s - loss: 108.2180 - MinusLogProbMetric: 108.2180 - val_loss: 118.3097 - val_MinusLogProbMetric: 118.3097 - lr: 1.1111e-04 - 62s/epoch - 315ms/step
Epoch 35/1000
2023-10-10 00:49:46.305 
Epoch 35/1000 
	 loss: 107.9929, MinusLogProbMetric: 107.9929, val_loss: 104.2531, val_MinusLogProbMetric: 104.2531

Epoch 35: val_loss improved from 107.01823 to 104.25307, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 107.9929 - MinusLogProbMetric: 107.9929 - val_loss: 104.2531 - val_MinusLogProbMetric: 104.2531 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 36/1000
2023-10-10 00:50:51.398 
Epoch 36/1000 
	 loss: 104.3111, MinusLogProbMetric: 104.3111, val_loss: 101.5560, val_MinusLogProbMetric: 101.5560

Epoch 36: val_loss improved from 104.25307 to 101.55597, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 104.3111 - MinusLogProbMetric: 104.3111 - val_loss: 101.5560 - val_MinusLogProbMetric: 101.5560 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 37/1000
2023-10-10 00:51:56.601 
Epoch 37/1000 
	 loss: 109.7625, MinusLogProbMetric: 109.7625, val_loss: 100.7323, val_MinusLogProbMetric: 100.7323

Epoch 37: val_loss improved from 101.55597 to 100.73232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 109.7625 - MinusLogProbMetric: 109.7625 - val_loss: 100.7323 - val_MinusLogProbMetric: 100.7323 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 38/1000
2023-10-10 00:52:59.474 
Epoch 38/1000 
	 loss: 99.9958, MinusLogProbMetric: 99.9958, val_loss: 99.3439, val_MinusLogProbMetric: 99.3439

Epoch 38: val_loss improved from 100.73232 to 99.34387, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 99.9958 - MinusLogProbMetric: 99.9958 - val_loss: 99.3439 - val_MinusLogProbMetric: 99.3439 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 39/1000
2023-10-10 00:54:02.181 
Epoch 39/1000 
	 loss: 100.3316, MinusLogProbMetric: 100.3316, val_loss: 98.4240, val_MinusLogProbMetric: 98.4240

Epoch 39: val_loss improved from 99.34387 to 98.42400, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 100.3316 - MinusLogProbMetric: 100.3316 - val_loss: 98.4240 - val_MinusLogProbMetric: 98.4240 - lr: 1.1111e-04 - 63s/epoch - 320ms/step
Epoch 40/1000
2023-10-10 00:55:05.179 
Epoch 40/1000 
	 loss: 96.5416, MinusLogProbMetric: 96.5416, val_loss: 96.7910, val_MinusLogProbMetric: 96.7910

Epoch 40: val_loss improved from 98.42400 to 96.79102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 96.5416 - MinusLogProbMetric: 96.5416 - val_loss: 96.7910 - val_MinusLogProbMetric: 96.7910 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 41/1000
2023-10-10 00:56:08.070 
Epoch 41/1000 
	 loss: 96.1744, MinusLogProbMetric: 96.1744, val_loss: 96.1046, val_MinusLogProbMetric: 96.1046

Epoch 41: val_loss improved from 96.79102 to 96.10457, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 96.1744 - MinusLogProbMetric: 96.1744 - val_loss: 96.1046 - val_MinusLogProbMetric: 96.1046 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 42/1000
2023-10-10 00:57:11.868 
Epoch 42/1000 
	 loss: 95.2421, MinusLogProbMetric: 95.2421, val_loss: 93.3521, val_MinusLogProbMetric: 93.3521

Epoch 42: val_loss improved from 96.10457 to 93.35205, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 95.2421 - MinusLogProbMetric: 95.2421 - val_loss: 93.3521 - val_MinusLogProbMetric: 93.3521 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 43/1000
2023-10-10 00:58:15.273 
Epoch 43/1000 
	 loss: 94.5993, MinusLogProbMetric: 94.5993, val_loss: 95.3200, val_MinusLogProbMetric: 95.3200

Epoch 43: val_loss did not improve from 93.35205
196/196 - 62s - loss: 94.5993 - MinusLogProbMetric: 94.5993 - val_loss: 95.3200 - val_MinusLogProbMetric: 95.3200 - lr: 1.1111e-04 - 62s/epoch - 318ms/step
Epoch 44/1000
2023-10-10 00:59:18.112 
Epoch 44/1000 
	 loss: 101.6413, MinusLogProbMetric: 101.6413, val_loss: 96.6077, val_MinusLogProbMetric: 96.6077

Epoch 44: val_loss did not improve from 93.35205
196/196 - 63s - loss: 101.6413 - MinusLogProbMetric: 101.6413 - val_loss: 96.6077 - val_MinusLogProbMetric: 96.6077 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 45/1000
2023-10-10 01:00:20.028 
Epoch 45/1000 
	 loss: 97.3244, MinusLogProbMetric: 97.3244, val_loss: 114.2441, val_MinusLogProbMetric: 114.2441

Epoch 45: val_loss did not improve from 93.35205
196/196 - 62s - loss: 97.3244 - MinusLogProbMetric: 97.3244 - val_loss: 114.2441 - val_MinusLogProbMetric: 114.2441 - lr: 1.1111e-04 - 62s/epoch - 316ms/step
Epoch 46/1000
2023-10-10 01:01:22.663 
Epoch 46/1000 
	 loss: 95.2918, MinusLogProbMetric: 95.2918, val_loss: 90.2619, val_MinusLogProbMetric: 90.2619

Epoch 46: val_loss improved from 93.35205 to 90.26192, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 95.2918 - MinusLogProbMetric: 95.2918 - val_loss: 90.2619 - val_MinusLogProbMetric: 90.2619 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 47/1000
2023-10-10 01:02:26.699 
Epoch 47/1000 
	 loss: 91.1961, MinusLogProbMetric: 91.1961, val_loss: 95.0500, val_MinusLogProbMetric: 95.0500

Epoch 47: val_loss did not improve from 90.26192
196/196 - 63s - loss: 91.1961 - MinusLogProbMetric: 91.1961 - val_loss: 95.0500 - val_MinusLogProbMetric: 95.0500 - lr: 1.1111e-04 - 63s/epoch - 321ms/step
Epoch 48/1000
2023-10-10 01:03:29.453 
Epoch 48/1000 
	 loss: 90.5455, MinusLogProbMetric: 90.5455, val_loss: 87.0794, val_MinusLogProbMetric: 87.0794

Epoch 48: val_loss improved from 90.26192 to 87.07941, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 90.5455 - MinusLogProbMetric: 90.5455 - val_loss: 87.0794 - val_MinusLogProbMetric: 87.0794 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 49/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 120: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 01:04:10.550 
Epoch 49/1000 
	 loss: nan, MinusLogProbMetric: 91.7443, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 49: val_loss did not improve from 87.07941
196/196 - 40s - loss: nan - MinusLogProbMetric: 91.7443 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 40s/epoch - 204ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 263.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_76"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_77 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f0c882b8b80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b7d323610>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b7d323610>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b7d310a30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b71ab0310>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b71ab0880>, <keras.callbacks.ModelCheckpoint object at 0x7f0b71ab0940>, <keras.callbacks.EarlyStopping object at 0x7f0b71ab0bb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b71ab0be0>, <keras.callbacks.TerminateOnNaN object at 0x7f0b71ab0820>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-10 01:04:20.803577
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 179: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 01:07:33.817 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 92.5093, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 193s - loss: nan - MinusLogProbMetric: 92.5093 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 193s/epoch - 984ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 263.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_87"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_88 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f0b61f19f30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b61332200>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b61332200>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b613e4910>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b613e5bd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b613e6140>, <keras.callbacks.ModelCheckpoint object at 0x7f0b613e6200>, <keras.callbacks.EarlyStopping object at 0x7f0b613e6470>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b613e64a0>, <keras.callbacks.TerminateOnNaN object at 0x7f0b613e60e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-10 01:07:43.028516
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
2023-10-10 01:11:00.602 
Epoch 1/1000 
	 loss: 84.2282, MinusLogProbMetric: 84.2282, val_loss: 82.9205, val_MinusLogProbMetric: 82.9205

Epoch 1: val_loss improved from inf to 82.92049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 198s - loss: 84.2282 - MinusLogProbMetric: 84.2282 - val_loss: 82.9205 - val_MinusLogProbMetric: 82.9205 - lr: 1.2346e-05 - 198s/epoch - 1s/step
Epoch 2/1000
2023-10-10 01:12:04.053 
Epoch 2/1000 
	 loss: 84.2195, MinusLogProbMetric: 84.2195, val_loss: 85.1801, val_MinusLogProbMetric: 85.1801

Epoch 2: val_loss did not improve from 82.92049
196/196 - 62s - loss: 84.2195 - MinusLogProbMetric: 84.2195 - val_loss: 85.1801 - val_MinusLogProbMetric: 85.1801 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 3/1000
2023-10-10 01:13:05.639 
Epoch 3/1000 
	 loss: 78.8548, MinusLogProbMetric: 78.8548, val_loss: 76.3517, val_MinusLogProbMetric: 76.3517

Epoch 3: val_loss improved from 82.92049 to 76.35172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 78.8548 - MinusLogProbMetric: 78.8548 - val_loss: 76.3517 - val_MinusLogProbMetric: 76.3517 - lr: 1.2346e-05 - 63s/epoch - 320ms/step
Epoch 4/1000
2023-10-10 01:14:10.328 
Epoch 4/1000 
	 loss: 74.0507, MinusLogProbMetric: 74.0507, val_loss: 72.1082, val_MinusLogProbMetric: 72.1082

Epoch 4: val_loss improved from 76.35172 to 72.10818, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 74.0507 - MinusLogProbMetric: 74.0507 - val_loss: 72.1082 - val_MinusLogProbMetric: 72.1082 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 5/1000
2023-10-10 01:15:16.266 
Epoch 5/1000 
	 loss: 71.4642, MinusLogProbMetric: 71.4642, val_loss: 71.0889, val_MinusLogProbMetric: 71.0889

Epoch 5: val_loss improved from 72.10818 to 71.08894, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 71.4642 - MinusLogProbMetric: 71.4642 - val_loss: 71.0889 - val_MinusLogProbMetric: 71.0889 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 6/1000
2023-10-10 01:16:20.382 
Epoch 6/1000 
	 loss: 74.3768, MinusLogProbMetric: 74.3768, val_loss: 70.2021, val_MinusLogProbMetric: 70.2021

Epoch 6: val_loss improved from 71.08894 to 70.20208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 74.3768 - MinusLogProbMetric: 74.3768 - val_loss: 70.2021 - val_MinusLogProbMetric: 70.2021 - lr: 1.2346e-05 - 64s/epoch - 327ms/step
Epoch 7/1000
2023-10-10 01:17:24.876 
Epoch 7/1000 
	 loss: 70.1678, MinusLogProbMetric: 70.1678, val_loss: 67.0990, val_MinusLogProbMetric: 67.0990

Epoch 7: val_loss improved from 70.20208 to 67.09902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 70.1678 - MinusLogProbMetric: 70.1678 - val_loss: 67.0990 - val_MinusLogProbMetric: 67.0990 - lr: 1.2346e-05 - 64s/epoch - 329ms/step
Epoch 8/1000
2023-10-10 01:18:29.229 
Epoch 8/1000 
	 loss: 66.4830, MinusLogProbMetric: 66.4830, val_loss: 65.8485, val_MinusLogProbMetric: 65.8485

Epoch 8: val_loss improved from 67.09902 to 65.84848, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 66.4830 - MinusLogProbMetric: 66.4830 - val_loss: 65.8485 - val_MinusLogProbMetric: 65.8485 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 9/1000
2023-10-10 01:19:32.489 
Epoch 9/1000 
	 loss: 65.4360, MinusLogProbMetric: 65.4360, val_loss: 70.6182, val_MinusLogProbMetric: 70.6182

Epoch 9: val_loss did not improve from 65.84848
196/196 - 62s - loss: 65.4360 - MinusLogProbMetric: 65.4360 - val_loss: 70.6182 - val_MinusLogProbMetric: 70.6182 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 10/1000
2023-10-10 01:20:35.229 
Epoch 10/1000 
	 loss: 65.5695, MinusLogProbMetric: 65.5695, val_loss: 62.0362, val_MinusLogProbMetric: 62.0362

Epoch 10: val_loss improved from 65.84848 to 62.03618, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 65.5695 - MinusLogProbMetric: 65.5695 - val_loss: 62.0362 - val_MinusLogProbMetric: 62.0362 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 11/1000
2023-10-10 01:21:39.524 
Epoch 11/1000 
	 loss: 65.3194, MinusLogProbMetric: 65.3194, val_loss: 63.3377, val_MinusLogProbMetric: 63.3377

Epoch 11: val_loss did not improve from 62.03618
196/196 - 63s - loss: 65.3194 - MinusLogProbMetric: 65.3194 - val_loss: 63.3377 - val_MinusLogProbMetric: 63.3377 - lr: 1.2346e-05 - 63s/epoch - 322ms/step
Epoch 12/1000
2023-10-10 01:22:42.425 
Epoch 12/1000 
	 loss: 60.7386, MinusLogProbMetric: 60.7386, val_loss: 60.7048, val_MinusLogProbMetric: 60.7048

Epoch 12: val_loss improved from 62.03618 to 60.70483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 60.7386 - MinusLogProbMetric: 60.7386 - val_loss: 60.7048 - val_MinusLogProbMetric: 60.7048 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 13/1000
2023-10-10 01:23:45.707 
Epoch 13/1000 
	 loss: 59.6308, MinusLogProbMetric: 59.6308, val_loss: 59.1709, val_MinusLogProbMetric: 59.1709

Epoch 13: val_loss improved from 60.70483 to 59.17091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 59.6308 - MinusLogProbMetric: 59.6308 - val_loss: 59.1709 - val_MinusLogProbMetric: 59.1709 - lr: 1.2346e-05 - 63s/epoch - 324ms/step
Epoch 14/1000
2023-10-10 01:24:51.283 
Epoch 14/1000 
	 loss: 70.8252, MinusLogProbMetric: 70.8252, val_loss: 73.9472, val_MinusLogProbMetric: 73.9472

Epoch 14: val_loss did not improve from 59.17091
196/196 - 64s - loss: 70.8252 - MinusLogProbMetric: 70.8252 - val_loss: 73.9472 - val_MinusLogProbMetric: 73.9472 - lr: 1.2346e-05 - 64s/epoch - 329ms/step
Epoch 15/1000
2023-10-10 01:25:53.846 
Epoch 15/1000 
	 loss: 63.6603, MinusLogProbMetric: 63.6603, val_loss: 58.8601, val_MinusLogProbMetric: 58.8601

Epoch 15: val_loss improved from 59.17091 to 58.86007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 63.6603 - MinusLogProbMetric: 63.6603 - val_loss: 58.8601 - val_MinusLogProbMetric: 58.8601 - lr: 1.2346e-05 - 64s/epoch - 325ms/step
Epoch 16/1000
2023-10-10 01:26:57.532 
Epoch 16/1000 
	 loss: 57.9110, MinusLogProbMetric: 57.9110, val_loss: 56.5237, val_MinusLogProbMetric: 56.5237

Epoch 16: val_loss improved from 58.86007 to 56.52374, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 57.9110 - MinusLogProbMetric: 57.9110 - val_loss: 56.5237 - val_MinusLogProbMetric: 56.5237 - lr: 1.2346e-05 - 64s/epoch - 325ms/step
Epoch 17/1000
2023-10-10 01:28:03.483 
Epoch 17/1000 
	 loss: 56.9529, MinusLogProbMetric: 56.9529, val_loss: 56.3139, val_MinusLogProbMetric: 56.3139

Epoch 17: val_loss improved from 56.52374 to 56.31395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 56.9529 - MinusLogProbMetric: 56.9529 - val_loss: 56.3139 - val_MinusLogProbMetric: 56.3139 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 18/1000
2023-10-10 01:29:07.837 
Epoch 18/1000 
	 loss: 55.3066, MinusLogProbMetric: 55.3066, val_loss: 56.2562, val_MinusLogProbMetric: 56.2562

Epoch 18: val_loss improved from 56.31395 to 56.25622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 55.3066 - MinusLogProbMetric: 55.3066 - val_loss: 56.2562 - val_MinusLogProbMetric: 56.2562 - lr: 1.2346e-05 - 64s/epoch - 329ms/step
Epoch 19/1000
2023-10-10 01:30:11.498 
Epoch 19/1000 
	 loss: 54.5595, MinusLogProbMetric: 54.5595, val_loss: 60.9999, val_MinusLogProbMetric: 60.9999

Epoch 19: val_loss did not improve from 56.25622
196/196 - 62s - loss: 54.5595 - MinusLogProbMetric: 54.5595 - val_loss: 60.9999 - val_MinusLogProbMetric: 60.9999 - lr: 1.2346e-05 - 62s/epoch - 319ms/step
Epoch 20/1000
2023-10-10 01:31:13.852 
Epoch 20/1000 
	 loss: 55.0777, MinusLogProbMetric: 55.0777, val_loss: 54.0877, val_MinusLogProbMetric: 54.0877

Epoch 20: val_loss improved from 56.25622 to 54.08770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 55.0777 - MinusLogProbMetric: 55.0777 - val_loss: 54.0877 - val_MinusLogProbMetric: 54.0877 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 21/1000
2023-10-10 01:32:17.321 
Epoch 21/1000 
	 loss: 53.4397, MinusLogProbMetric: 53.4397, val_loss: 57.7293, val_MinusLogProbMetric: 57.7293

Epoch 21: val_loss did not improve from 54.08770
196/196 - 62s - loss: 53.4397 - MinusLogProbMetric: 53.4397 - val_loss: 57.7293 - val_MinusLogProbMetric: 57.7293 - lr: 1.2346e-05 - 62s/epoch - 319ms/step
Epoch 22/1000
2023-10-10 01:33:19.678 
Epoch 22/1000 
	 loss: 53.0103, MinusLogProbMetric: 53.0103, val_loss: 51.9426, val_MinusLogProbMetric: 51.9426

Epoch 22: val_loss improved from 54.08770 to 51.94263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 53.0103 - MinusLogProbMetric: 53.0103 - val_loss: 51.9426 - val_MinusLogProbMetric: 51.9426 - lr: 1.2346e-05 - 63s/epoch - 324ms/step
Epoch 23/1000
2023-10-10 01:34:23.334 
Epoch 23/1000 
	 loss: 51.2671, MinusLogProbMetric: 51.2671, val_loss: 50.9709, val_MinusLogProbMetric: 50.9709

Epoch 23: val_loss improved from 51.94263 to 50.97091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 51.2671 - MinusLogProbMetric: 51.2671 - val_loss: 50.9709 - val_MinusLogProbMetric: 50.9709 - lr: 1.2346e-05 - 64s/epoch - 325ms/step
Epoch 24/1000
2023-10-10 01:35:27.404 
Epoch 24/1000 
	 loss: 50.9417, MinusLogProbMetric: 50.9417, val_loss: 49.8480, val_MinusLogProbMetric: 49.8480

Epoch 24: val_loss improved from 50.97091 to 49.84801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 50.9417 - MinusLogProbMetric: 50.9417 - val_loss: 49.8480 - val_MinusLogProbMetric: 49.8480 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 25/1000
2023-10-10 01:36:30.758 
Epoch 25/1000 
	 loss: 49.8403, MinusLogProbMetric: 49.8403, val_loss: 53.7974, val_MinusLogProbMetric: 53.7974

Epoch 25: val_loss did not improve from 49.84801
196/196 - 62s - loss: 49.8403 - MinusLogProbMetric: 49.8403 - val_loss: 53.7974 - val_MinusLogProbMetric: 53.7974 - lr: 1.2346e-05 - 62s/epoch - 318ms/step
Epoch 26/1000
2023-10-10 01:37:32.214 
Epoch 26/1000 
	 loss: 51.1688, MinusLogProbMetric: 51.1688, val_loss: 49.0221, val_MinusLogProbMetric: 49.0221

Epoch 26: val_loss improved from 49.84801 to 49.02208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 51.1688 - MinusLogProbMetric: 51.1688 - val_loss: 49.0221 - val_MinusLogProbMetric: 49.0221 - lr: 1.2346e-05 - 63s/epoch - 319ms/step
Epoch 27/1000
2023-10-10 01:38:35.830 
Epoch 27/1000 
	 loss: 50.7591, MinusLogProbMetric: 50.7591, val_loss: 50.0142, val_MinusLogProbMetric: 50.0142

Epoch 27: val_loss did not improve from 49.02208
196/196 - 62s - loss: 50.7591 - MinusLogProbMetric: 50.7591 - val_loss: 50.0142 - val_MinusLogProbMetric: 50.0142 - lr: 1.2346e-05 - 62s/epoch - 319ms/step
Epoch 28/1000
2023-10-10 01:39:38.541 
Epoch 28/1000 
	 loss: 49.4159, MinusLogProbMetric: 49.4159, val_loss: 49.4760, val_MinusLogProbMetric: 49.4760

Epoch 28: val_loss did not improve from 49.02208
196/196 - 63s - loss: 49.4159 - MinusLogProbMetric: 49.4159 - val_loss: 49.4760 - val_MinusLogProbMetric: 49.4760 - lr: 1.2346e-05 - 63s/epoch - 320ms/step
Epoch 29/1000
2023-10-10 01:40:41.394 
Epoch 29/1000 
	 loss: 49.1861, MinusLogProbMetric: 49.1861, val_loss: 48.1062, val_MinusLogProbMetric: 48.1062

Epoch 29: val_loss improved from 49.02208 to 48.10618, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 49.1861 - MinusLogProbMetric: 49.1861 - val_loss: 48.1062 - val_MinusLogProbMetric: 48.1062 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 30/1000
2023-10-10 01:41:43.236 
Epoch 30/1000 
	 loss: 47.7093, MinusLogProbMetric: 47.7093, val_loss: 46.6643, val_MinusLogProbMetric: 46.6643

Epoch 30: val_loss improved from 48.10618 to 46.66433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 47.7093 - MinusLogProbMetric: 47.7093 - val_loss: 46.6643 - val_MinusLogProbMetric: 46.6643 - lr: 1.2346e-05 - 62s/epoch - 316ms/step
Epoch 31/1000
2023-10-10 01:42:46.167 
Epoch 31/1000 
	 loss: 54.4946, MinusLogProbMetric: 54.4946, val_loss: 57.0397, val_MinusLogProbMetric: 57.0397

Epoch 31: val_loss did not improve from 46.66433
196/196 - 62s - loss: 54.4946 - MinusLogProbMetric: 54.4946 - val_loss: 57.0397 - val_MinusLogProbMetric: 57.0397 - lr: 1.2346e-05 - 62s/epoch - 315ms/step
Epoch 32/1000
2023-10-10 01:43:49.588 
Epoch 32/1000 
	 loss: 52.0817, MinusLogProbMetric: 52.0817, val_loss: 49.6870, val_MinusLogProbMetric: 49.6870

Epoch 32: val_loss did not improve from 46.66433
196/196 - 63s - loss: 52.0817 - MinusLogProbMetric: 52.0817 - val_loss: 49.6870 - val_MinusLogProbMetric: 49.6870 - lr: 1.2346e-05 - 63s/epoch - 324ms/step
Epoch 33/1000
2023-10-10 01:44:52.010 
Epoch 33/1000 
	 loss: 50.0348, MinusLogProbMetric: 50.0348, val_loss: 47.9104, val_MinusLogProbMetric: 47.9104

Epoch 33: val_loss did not improve from 46.66433
196/196 - 62s - loss: 50.0348 - MinusLogProbMetric: 50.0348 - val_loss: 47.9104 - val_MinusLogProbMetric: 47.9104 - lr: 1.2346e-05 - 62s/epoch - 318ms/step
Epoch 34/1000
2023-10-10 01:45:53.937 
Epoch 34/1000 
	 loss: 67.1073, MinusLogProbMetric: 67.1073, val_loss: 73.7542, val_MinusLogProbMetric: 73.7542

Epoch 34: val_loss did not improve from 46.66433
196/196 - 62s - loss: 67.1073 - MinusLogProbMetric: 67.1073 - val_loss: 73.7542 - val_MinusLogProbMetric: 73.7542 - lr: 1.2346e-05 - 62s/epoch - 316ms/step
Epoch 35/1000
2023-10-10 01:46:56.190 
Epoch 35/1000 
	 loss: 65.1278, MinusLogProbMetric: 65.1278, val_loss: 57.8766, val_MinusLogProbMetric: 57.8766

Epoch 35: val_loss did not improve from 46.66433
196/196 - 62s - loss: 65.1278 - MinusLogProbMetric: 65.1278 - val_loss: 57.8766 - val_MinusLogProbMetric: 57.8766 - lr: 1.2346e-05 - 62s/epoch - 318ms/step
Epoch 36/1000
2023-10-10 01:47:58.066 
Epoch 36/1000 
	 loss: 57.7281, MinusLogProbMetric: 57.7281, val_loss: 56.6638, val_MinusLogProbMetric: 56.6638

Epoch 36: val_loss did not improve from 46.66433
196/196 - 62s - loss: 57.7281 - MinusLogProbMetric: 57.7281 - val_loss: 56.6638 - val_MinusLogProbMetric: 56.6638 - lr: 1.2346e-05 - 62s/epoch - 316ms/step
Epoch 37/1000
2023-10-10 01:49:00.876 
Epoch 37/1000 
	 loss: 54.8101, MinusLogProbMetric: 54.8101, val_loss: 53.0403, val_MinusLogProbMetric: 53.0403

Epoch 37: val_loss did not improve from 46.66433
196/196 - 63s - loss: 54.8101 - MinusLogProbMetric: 54.8101 - val_loss: 53.0403 - val_MinusLogProbMetric: 53.0403 - lr: 1.2346e-05 - 63s/epoch - 320ms/step
Epoch 38/1000
2023-10-10 01:50:03.680 
Epoch 38/1000 
	 loss: 53.0207, MinusLogProbMetric: 53.0207, val_loss: 54.3925, val_MinusLogProbMetric: 54.3925

Epoch 38: val_loss did not improve from 46.66433
196/196 - 63s - loss: 53.0207 - MinusLogProbMetric: 53.0207 - val_loss: 54.3925 - val_MinusLogProbMetric: 54.3925 - lr: 1.2346e-05 - 63s/epoch - 320ms/step
Epoch 39/1000
2023-10-10 01:51:05.706 
Epoch 39/1000 
	 loss: 53.8502, MinusLogProbMetric: 53.8502, val_loss: 50.8659, val_MinusLogProbMetric: 50.8659

Epoch 39: val_loss did not improve from 46.66433
196/196 - 62s - loss: 53.8502 - MinusLogProbMetric: 53.8502 - val_loss: 50.8659 - val_MinusLogProbMetric: 50.8659 - lr: 1.2346e-05 - 62s/epoch - 316ms/step
Epoch 40/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 69: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 01:51:30.211 
Epoch 40/1000 
	 loss: nan, MinusLogProbMetric: 50.3036, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 40: val_loss did not improve from 46.66433
196/196 - 25s - loss: nan - MinusLogProbMetric: 50.3036 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 25s/epoch - 125ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 263.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_98"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_99 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f0b68796b60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c94302da0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c94302da0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c88181f30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0d1c2fa200>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0d1c2f99c0>, <keras.callbacks.ModelCheckpoint object at 0x7f0d1c2f9960>, <keras.callbacks.EarlyStopping object at 0x7f0d1c2f88e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0d1c2f9000>, <keras.callbacks.TerminateOnNaN object at 0x7f0d1c2f8d30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-10 01:51:38.458449
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
2023-10-10 01:54:53.677 
Epoch 1/1000 
	 loss: 46.6281, MinusLogProbMetric: 46.6281, val_loss: 50.2089, val_MinusLogProbMetric: 50.2089

Epoch 1: val_loss improved from inf to 50.20894, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 196s - loss: 46.6281 - MinusLogProbMetric: 46.6281 - val_loss: 50.2089 - val_MinusLogProbMetric: 50.2089 - lr: 4.1152e-06 - 196s/epoch - 1s/step
Epoch 2/1000
2023-10-10 01:55:58.000 
Epoch 2/1000 
	 loss: 45.8558, MinusLogProbMetric: 45.8558, val_loss: 45.3098, val_MinusLogProbMetric: 45.3098

Epoch 2: val_loss improved from 50.20894 to 45.30979, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 45.8558 - MinusLogProbMetric: 45.8558 - val_loss: 45.3098 - val_MinusLogProbMetric: 45.3098 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 3/1000
2023-10-10 01:57:00.281 
Epoch 3/1000 
	 loss: 47.5956, MinusLogProbMetric: 47.5956, val_loss: 45.3093, val_MinusLogProbMetric: 45.3093

Epoch 3: val_loss improved from 45.30979 to 45.30930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 47.5956 - MinusLogProbMetric: 47.5956 - val_loss: 45.3093 - val_MinusLogProbMetric: 45.3093 - lr: 4.1152e-06 - 62s/epoch - 318ms/step
Epoch 4/1000
2023-10-10 01:58:04.004 
Epoch 4/1000 
	 loss: 44.8901, MinusLogProbMetric: 44.8901, val_loss: 44.7718, val_MinusLogProbMetric: 44.7718

Epoch 4: val_loss improved from 45.30930 to 44.77179, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 44.8901 - MinusLogProbMetric: 44.8901 - val_loss: 44.7718 - val_MinusLogProbMetric: 44.7718 - lr: 4.1152e-06 - 64s/epoch - 324ms/step
Epoch 5/1000
2023-10-10 01:59:06.570 
Epoch 5/1000 
	 loss: 45.3897, MinusLogProbMetric: 45.3897, val_loss: 47.1169, val_MinusLogProbMetric: 47.1169

Epoch 5: val_loss did not improve from 44.77179
196/196 - 62s - loss: 45.3897 - MinusLogProbMetric: 45.3897 - val_loss: 47.1169 - val_MinusLogProbMetric: 47.1169 - lr: 4.1152e-06 - 62s/epoch - 314ms/step
Epoch 6/1000
2023-10-10 02:00:09.384 
Epoch 6/1000 
	 loss: 45.1315, MinusLogProbMetric: 45.1315, val_loss: 44.0870, val_MinusLogProbMetric: 44.0870

Epoch 6: val_loss improved from 44.77179 to 44.08702, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 45.1315 - MinusLogProbMetric: 45.1315 - val_loss: 44.0870 - val_MinusLogProbMetric: 44.0870 - lr: 4.1152e-06 - 64s/epoch - 326ms/step
Epoch 7/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
2023-10-10 02:01:13.869 
Epoch 7/1000 
	 loss: 48.6166, MinusLogProbMetric: 48.6166, val_loss: 44.8810, val_MinusLogProbMetric: 44.8810

Epoch 7: val_loss did not improve from 44.08702
196/196 - 63s - loss: 48.6166 - MinusLogProbMetric: 48.6166 - val_loss: 44.8810 - val_MinusLogProbMetric: 44.8810 - lr: 4.1152e-06 - 63s/epoch - 323ms/step
Epoch 8/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 90: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 02:01:44.902 
Epoch 8/1000 
	 loss: nan, MinusLogProbMetric: 45.1126, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 8: val_loss did not improve from 44.08702
196/196 - 31s - loss: nan - MinusLogProbMetric: 45.1126 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 31s/epoch - 158ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.3717421124828526e-06.
===========
Generating train data for run 263.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_109"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_110 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  1074400   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f0bd8159ea0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0bd814da20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0bd814da20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c2037a020>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0c204a7a30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0c204a7fa0>, <keras.callbacks.ModelCheckpoint object at 0x7f0c204a7f40>, <keras.callbacks.EarlyStopping object at 0x7f0c204a7f70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0c20450130>, <keras.callbacks.TerminateOnNaN object at 0x7f0c20450310>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-10 02:01:54.620052
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
2023-10-10 02:05:13.968 
Epoch 1/1000 
	 loss: 44.0058, MinusLogProbMetric: 44.0058, val_loss: 43.8414, val_MinusLogProbMetric: 43.8414

Epoch 1: val_loss improved from inf to 43.84139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 200s - loss: 44.0058 - MinusLogProbMetric: 44.0058 - val_loss: 43.8414 - val_MinusLogProbMetric: 43.8414 - lr: 1.3717e-06 - 200s/epoch - 1s/step
Epoch 2/1000
2023-10-10 02:06:17.419 
Epoch 2/1000 
	 loss: 43.4947, MinusLogProbMetric: 43.4947, val_loss: 43.6683, val_MinusLogProbMetric: 43.6683

Epoch 2: val_loss improved from 43.84139 to 43.66833, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 43.4947 - MinusLogProbMetric: 43.4947 - val_loss: 43.6683 - val_MinusLogProbMetric: 43.6683 - lr: 1.3717e-06 - 63s/epoch - 321ms/step
Epoch 3/1000
2023-10-10 02:07:22.944 
Epoch 3/1000 
	 loss: 43.4450, MinusLogProbMetric: 43.4450, val_loss: 43.6481, val_MinusLogProbMetric: 43.6481

Epoch 3: val_loss improved from 43.66833 to 43.64813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 43.4450 - MinusLogProbMetric: 43.4450 - val_loss: 43.6481 - val_MinusLogProbMetric: 43.6481 - lr: 1.3717e-06 - 66s/epoch - 336ms/step
Epoch 4/1000
2023-10-10 02:08:26.720 
Epoch 4/1000 
	 loss: 43.3461, MinusLogProbMetric: 43.3461, val_loss: 43.4261, val_MinusLogProbMetric: 43.4261

Epoch 4: val_loss improved from 43.64813 to 43.42612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 43.3461 - MinusLogProbMetric: 43.3461 - val_loss: 43.4261 - val_MinusLogProbMetric: 43.4261 - lr: 1.3717e-06 - 64s/epoch - 324ms/step
Epoch 5/1000
2023-10-10 02:09:30.474 
Epoch 5/1000 
	 loss: 43.4081, MinusLogProbMetric: 43.4081, val_loss: 43.3764, val_MinusLogProbMetric: 43.3764

Epoch 5: val_loss improved from 43.42612 to 43.37645, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 43.4081 - MinusLogProbMetric: 43.4081 - val_loss: 43.3764 - val_MinusLogProbMetric: 43.3764 - lr: 1.3717e-06 - 64s/epoch - 325ms/step
Epoch 6/1000
2023-10-10 02:10:34.072 
Epoch 6/1000 
	 loss: 43.1414, MinusLogProbMetric: 43.1414, val_loss: 43.5107, val_MinusLogProbMetric: 43.5107

Epoch 6: val_loss did not improve from 43.37645
196/196 - 62s - loss: 43.1414 - MinusLogProbMetric: 43.1414 - val_loss: 43.5107 - val_MinusLogProbMetric: 43.5107 - lr: 1.3717e-06 - 62s/epoch - 319ms/step
Epoch 7/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 163: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 02:11:27.065 
Epoch 7/1000 
	 loss: nan, MinusLogProbMetric: 43.1802, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 7: val_loss did not improve from 43.37645
196/196 - 53s - loss: nan - MinusLogProbMetric: 43.1802 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 53s/epoch - 270ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.572473708276175e-07.
===========
Generating train data for run 263.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_120"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_121 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f0b7c16f520>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b687f1b70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b687f1b70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b9862eb00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b01629bd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b0162a140>, <keras.callbacks.ModelCheckpoint object at 0x7f0b0162a200>, <keras.callbacks.EarlyStopping object at 0x7f0b0162a470>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b0162a4a0>, <keras.callbacks.TerminateOnNaN object at 0x7f0b0162a0e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-10 02:11:36.116182
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
2023-10-10 02:14:54.212 
Epoch 1/1000 
	 loss: 43.1917, MinusLogProbMetric: 43.1917, val_loss: 43.4124, val_MinusLogProbMetric: 43.4124

Epoch 1: val_loss improved from inf to 43.41239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 199s - loss: 43.1917 - MinusLogProbMetric: 43.1917 - val_loss: 43.4124 - val_MinusLogProbMetric: 43.4124 - lr: 4.5725e-07 - 199s/epoch - 1s/step
Epoch 2/1000
2023-10-10 02:15:58.566 
Epoch 2/1000 
	 loss: 43.0484, MinusLogProbMetric: 43.0484, val_loss: 43.1512, val_MinusLogProbMetric: 43.1512

Epoch 2: val_loss improved from 43.41239 to 43.15118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 43.0484 - MinusLogProbMetric: 43.0484 - val_loss: 43.1512 - val_MinusLogProbMetric: 43.1512 - lr: 4.5725e-07 - 64s/epoch - 326ms/step
Epoch 3/1000
2023-10-10 02:17:00.849 
Epoch 3/1000 
	 loss: 43.0267, MinusLogProbMetric: 43.0267, val_loss: 43.2065, val_MinusLogProbMetric: 43.2065

Epoch 3: val_loss did not improve from 43.15118
196/196 - 61s - loss: 43.0267 - MinusLogProbMetric: 43.0267 - val_loss: 43.2065 - val_MinusLogProbMetric: 43.2065 - lr: 4.5725e-07 - 61s/epoch - 312ms/step
Epoch 4/1000
2023-10-10 02:18:03.596 
Epoch 4/1000 
	 loss: 43.3693, MinusLogProbMetric: 43.3693, val_loss: 43.1727, val_MinusLogProbMetric: 43.1727

Epoch 4: val_loss did not improve from 43.15118
196/196 - 63s - loss: 43.3693 - MinusLogProbMetric: 43.3693 - val_loss: 43.1727 - val_MinusLogProbMetric: 43.1727 - lr: 4.5725e-07 - 63s/epoch - 320ms/step
Epoch 5/1000
2023-10-10 02:19:06.118 
Epoch 5/1000 
	 loss: 42.8757, MinusLogProbMetric: 42.8757, val_loss: 43.0782, val_MinusLogProbMetric: 43.0782

Epoch 5: val_loss improved from 43.15118 to 43.07822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.8757 - MinusLogProbMetric: 42.8757 - val_loss: 43.0782 - val_MinusLogProbMetric: 43.0782 - lr: 4.5725e-07 - 64s/epoch - 326ms/step
Epoch 6/1000
2023-10-10 02:20:10.942 
Epoch 6/1000 
	 loss: 43.0409, MinusLogProbMetric: 43.0409, val_loss: 43.2604, val_MinusLogProbMetric: 43.2604

Epoch 6: val_loss did not improve from 43.07822
196/196 - 63s - loss: 43.0409 - MinusLogProbMetric: 43.0409 - val_loss: 43.2604 - val_MinusLogProbMetric: 43.2604 - lr: 4.5725e-07 - 63s/epoch - 324ms/step
Epoch 7/1000
2023-10-10 02:21:15.233 
Epoch 7/1000 
	 loss: 42.9105, MinusLogProbMetric: 42.9105, val_loss: 43.0612, val_MinusLogProbMetric: 43.0612

Epoch 7: val_loss improved from 43.07822 to 43.06121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 42.9105 - MinusLogProbMetric: 42.9105 - val_loss: 43.0612 - val_MinusLogProbMetric: 43.0612 - lr: 4.5725e-07 - 65s/epoch - 334ms/step
Epoch 8/1000
2023-10-10 02:22:18.415 
Epoch 8/1000 
	 loss: 42.8359, MinusLogProbMetric: 42.8359, val_loss: 43.0041, val_MinusLogProbMetric: 43.0041

Epoch 8: val_loss improved from 43.06121 to 43.00414, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.8359 - MinusLogProbMetric: 42.8359 - val_loss: 43.0041 - val_MinusLogProbMetric: 43.0041 - lr: 4.5725e-07 - 64s/epoch - 325ms/step
Epoch 9/1000
2023-10-10 02:23:23.239 
Epoch 9/1000 
	 loss: 42.8038, MinusLogProbMetric: 42.8038, val_loss: 42.9637, val_MinusLogProbMetric: 42.9637

Epoch 9: val_loss improved from 43.00414 to 42.96368, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.8038 - MinusLogProbMetric: 42.8038 - val_loss: 42.9637 - val_MinusLogProbMetric: 42.9637 - lr: 4.5725e-07 - 64s/epoch - 328ms/step
Epoch 10/1000
2023-10-10 02:24:26.428 
Epoch 10/1000 
	 loss: 42.7439, MinusLogProbMetric: 42.7439, val_loss: 42.9409, val_MinusLogProbMetric: 42.9409

Epoch 10: val_loss improved from 42.96368 to 42.94091, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.7439 - MinusLogProbMetric: 42.7439 - val_loss: 42.9409 - val_MinusLogProbMetric: 42.9409 - lr: 4.5725e-07 - 63s/epoch - 323ms/step
Epoch 11/1000
2023-10-10 02:25:29.305 
Epoch 11/1000 
	 loss: 42.8455, MinusLogProbMetric: 42.8455, val_loss: 42.9293, val_MinusLogProbMetric: 42.9293

Epoch 11: val_loss improved from 42.94091 to 42.92931, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.8455 - MinusLogProbMetric: 42.8455 - val_loss: 42.9293 - val_MinusLogProbMetric: 42.9293 - lr: 4.5725e-07 - 63s/epoch - 320ms/step
Epoch 12/1000
2023-10-10 02:26:33.401 
Epoch 12/1000 
	 loss: 42.6923, MinusLogProbMetric: 42.6923, val_loss: 42.8938, val_MinusLogProbMetric: 42.8938

Epoch 12: val_loss improved from 42.92931 to 42.89383, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.6923 - MinusLogProbMetric: 42.6923 - val_loss: 42.8938 - val_MinusLogProbMetric: 42.8938 - lr: 4.5725e-07 - 64s/epoch - 328ms/step
Epoch 13/1000
2023-10-10 02:27:36.917 
Epoch 13/1000 
	 loss: 42.8981, MinusLogProbMetric: 42.8981, val_loss: 43.0042, val_MinusLogProbMetric: 43.0042

Epoch 13: val_loss did not improve from 42.89383
196/196 - 62s - loss: 42.8981 - MinusLogProbMetric: 42.8981 - val_loss: 43.0042 - val_MinusLogProbMetric: 43.0042 - lr: 4.5725e-07 - 62s/epoch - 318ms/step
Epoch 14/1000
2023-10-10 02:28:38.577 
Epoch 14/1000 
	 loss: 42.7378, MinusLogProbMetric: 42.7378, val_loss: 43.2530, val_MinusLogProbMetric: 43.2530

Epoch 14: val_loss did not improve from 42.89383
196/196 - 62s - loss: 42.7378 - MinusLogProbMetric: 42.7378 - val_loss: 43.2530 - val_MinusLogProbMetric: 43.2530 - lr: 4.5725e-07 - 62s/epoch - 315ms/step
Epoch 15/1000
2023-10-10 02:29:40.087 
Epoch 15/1000 
	 loss: 42.8223, MinusLogProbMetric: 42.8223, val_loss: 42.9056, val_MinusLogProbMetric: 42.9056

Epoch 15: val_loss did not improve from 42.89383
196/196 - 62s - loss: 42.8223 - MinusLogProbMetric: 42.8223 - val_loss: 42.9056 - val_MinusLogProbMetric: 42.9056 - lr: 4.5725e-07 - 62s/epoch - 314ms/step
Epoch 16/1000
2023-10-10 02:30:42.675 
Epoch 16/1000 
	 loss: 42.6950, MinusLogProbMetric: 42.6950, val_loss: 42.8377, val_MinusLogProbMetric: 42.8377

Epoch 16: val_loss improved from 42.89383 to 42.83769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.6950 - MinusLogProbMetric: 42.6950 - val_loss: 42.8377 - val_MinusLogProbMetric: 42.8377 - lr: 4.5725e-07 - 64s/epoch - 325ms/step
Epoch 17/1000
2023-10-10 02:31:46.866 
Epoch 17/1000 
	 loss: 42.5956, MinusLogProbMetric: 42.5956, val_loss: 42.7953, val_MinusLogProbMetric: 42.7953

Epoch 17: val_loss improved from 42.83769 to 42.79533, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.5956 - MinusLogProbMetric: 42.5956 - val_loss: 42.7953 - val_MinusLogProbMetric: 42.7953 - lr: 4.5725e-07 - 64s/epoch - 328ms/step
Epoch 18/1000
2023-10-10 02:32:51.552 
Epoch 18/1000 
	 loss: 42.5869, MinusLogProbMetric: 42.5869, val_loss: 42.7852, val_MinusLogProbMetric: 42.7852

Epoch 18: val_loss improved from 42.79533 to 42.78521, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 42.5869 - MinusLogProbMetric: 42.5869 - val_loss: 42.7852 - val_MinusLogProbMetric: 42.7852 - lr: 4.5725e-07 - 65s/epoch - 330ms/step
Epoch 19/1000
2023-10-10 02:33:56.050 
Epoch 19/1000 
	 loss: 42.5607, MinusLogProbMetric: 42.5607, val_loss: 42.7945, val_MinusLogProbMetric: 42.7945

Epoch 19: val_loss did not improve from 42.78521
196/196 - 63s - loss: 42.5607 - MinusLogProbMetric: 42.5607 - val_loss: 42.7945 - val_MinusLogProbMetric: 42.7945 - lr: 4.5725e-07 - 63s/epoch - 323ms/step
Epoch 20/1000
2023-10-10 02:34:58.996 
Epoch 20/1000 
	 loss: 42.5290, MinusLogProbMetric: 42.5290, val_loss: 42.7229, val_MinusLogProbMetric: 42.7229

Epoch 20: val_loss improved from 42.78521 to 42.72285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.5290 - MinusLogProbMetric: 42.5290 - val_loss: 42.7229 - val_MinusLogProbMetric: 42.7229 - lr: 4.5725e-07 - 64s/epoch - 327ms/step
Epoch 21/1000
2023-10-10 02:36:01.878 
Epoch 21/1000 
	 loss: 42.5002, MinusLogProbMetric: 42.5002, val_loss: 42.7229, val_MinusLogProbMetric: 42.7229

Epoch 21: val_loss did not improve from 42.72285
196/196 - 62s - loss: 42.5002 - MinusLogProbMetric: 42.5002 - val_loss: 42.7229 - val_MinusLogProbMetric: 42.7229 - lr: 4.5725e-07 - 62s/epoch - 315ms/step
Epoch 22/1000
2023-10-10 02:37:04.321 
Epoch 22/1000 
	 loss: 42.4934, MinusLogProbMetric: 42.4934, val_loss: 42.7014, val_MinusLogProbMetric: 42.7014

Epoch 22: val_loss improved from 42.72285 to 42.70140, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.4934 - MinusLogProbMetric: 42.4934 - val_loss: 42.7014 - val_MinusLogProbMetric: 42.7014 - lr: 4.5725e-07 - 64s/epoch - 324ms/step
Epoch 23/1000
2023-10-10 02:38:08.005 
Epoch 23/1000 
	 loss: 42.4755, MinusLogProbMetric: 42.4755, val_loss: 42.6764, val_MinusLogProbMetric: 42.6764

Epoch 23: val_loss improved from 42.70140 to 42.67644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.4755 - MinusLogProbMetric: 42.4755 - val_loss: 42.6764 - val_MinusLogProbMetric: 42.6764 - lr: 4.5725e-07 - 64s/epoch - 325ms/step
Epoch 24/1000
2023-10-10 02:39:12.558 
Epoch 24/1000 
	 loss: 42.4708, MinusLogProbMetric: 42.4708, val_loss: 42.7595, val_MinusLogProbMetric: 42.7595

Epoch 24: val_loss did not improve from 42.67644
196/196 - 63s - loss: 42.4708 - MinusLogProbMetric: 42.4708 - val_loss: 42.7595 - val_MinusLogProbMetric: 42.7595 - lr: 4.5725e-07 - 63s/epoch - 323ms/step
Epoch 25/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 113: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 02:39:51.925 
Epoch 25/1000 
	 loss: nan, MinusLogProbMetric: 42.5224, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 25: val_loss did not improve from 42.67644
196/196 - 39s - loss: nan - MinusLogProbMetric: 42.5224 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 39s/epoch - 201ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.524157902758725e-07.
===========
Generating train data for run 263.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_131"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_132 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f0b84e8fca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b70ac2a70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b70ac2a70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b7c20b910>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b84b8be50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b84ba0400>, <keras.callbacks.ModelCheckpoint object at 0x7f0b84ba04c0>, <keras.callbacks.EarlyStopping object at 0x7f0b84ba0730>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b84ba0760>, <keras.callbacks.TerminateOnNaN object at 0x7f0b84ba03a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-10 02:40:00.219077
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
2023-10-10 02:43:18.842 
Epoch 1/1000 
	 loss: 42.4523, MinusLogProbMetric: 42.4523, val_loss: 42.6531, val_MinusLogProbMetric: 42.6531

Epoch 1: val_loss improved from inf to 42.65309, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 200s - loss: 42.4523 - MinusLogProbMetric: 42.4523 - val_loss: 42.6531 - val_MinusLogProbMetric: 42.6531 - lr: 1.5242e-07 - 200s/epoch - 1s/step
Epoch 2/1000
2023-10-10 02:44:23.086 
Epoch 2/1000 
	 loss: 42.4435, MinusLogProbMetric: 42.4435, val_loss: 42.6734, val_MinusLogProbMetric: 42.6734

Epoch 2: val_loss did not improve from 42.65309
196/196 - 63s - loss: 42.4435 - MinusLogProbMetric: 42.4435 - val_loss: 42.6734 - val_MinusLogProbMetric: 42.6734 - lr: 1.5242e-07 - 63s/epoch - 320ms/step
Epoch 3/1000
2023-10-10 02:45:25.866 
Epoch 3/1000 
	 loss: 42.4190, MinusLogProbMetric: 42.4190, val_loss: 42.6274, val_MinusLogProbMetric: 42.6274

Epoch 3: val_loss improved from 42.65309 to 42.62740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.4190 - MinusLogProbMetric: 42.4190 - val_loss: 42.6274 - val_MinusLogProbMetric: 42.6274 - lr: 1.5242e-07 - 64s/epoch - 326ms/step
Epoch 4/1000
2023-10-10 02:46:31.023 
Epoch 4/1000 
	 loss: 42.3922, MinusLogProbMetric: 42.3922, val_loss: 42.5934, val_MinusLogProbMetric: 42.5934

Epoch 4: val_loss improved from 42.62740 to 42.59338, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 42.3922 - MinusLogProbMetric: 42.3922 - val_loss: 42.5934 - val_MinusLogProbMetric: 42.5934 - lr: 1.5242e-07 - 65s/epoch - 332ms/step
Epoch 5/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 166: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 02:47:25.368 
Epoch 5/1000 
	 loss: nan, MinusLogProbMetric: 42.4039, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 5: val_loss did not improve from 42.59338
196/196 - 53s - loss: nan - MinusLogProbMetric: 42.4039 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 53s/epoch - 272ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.0805263425290834e-08.
===========
Generating train data for run 263.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_142"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_143 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f0ced863940>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b8455c5e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b8455c5e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b6172d3c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0ced8d9c60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0ced8da1d0>, <keras.callbacks.ModelCheckpoint object at 0x7f0ced8da290>, <keras.callbacks.EarlyStopping object at 0x7f0ced8da500>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0ced8da530>, <keras.callbacks.TerminateOnNaN object at 0x7f0ced8da170>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-10 02:47:34.609622
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
2023-10-10 02:50:50.446 
Epoch 1/1000 
	 loss: 42.3726, MinusLogProbMetric: 42.3726, val_loss: 42.6152, val_MinusLogProbMetric: 42.6152

Epoch 1: val_loss improved from inf to 42.61518, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 197s - loss: 42.3726 - MinusLogProbMetric: 42.3726 - val_loss: 42.6152 - val_MinusLogProbMetric: 42.6152 - lr: 5.0805e-08 - 197s/epoch - 1s/step
Epoch 2/1000
2023-10-10 02:51:55.041 
Epoch 2/1000 
	 loss: 42.3716, MinusLogProbMetric: 42.3716, val_loss: 42.5816, val_MinusLogProbMetric: 42.5816

Epoch 2: val_loss improved from 42.61518 to 42.58155, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.3716 - MinusLogProbMetric: 42.3716 - val_loss: 42.5816 - val_MinusLogProbMetric: 42.5816 - lr: 5.0805e-08 - 64s/epoch - 328ms/step
Epoch 3/1000
2023-10-10 02:52:56.759 
Epoch 3/1000 
	 loss: 42.3554, MinusLogProbMetric: 42.3554, val_loss: 42.5707, val_MinusLogProbMetric: 42.5707

Epoch 3: val_loss improved from 42.58155 to 42.57074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 42.3554 - MinusLogProbMetric: 42.3554 - val_loss: 42.5707 - val_MinusLogProbMetric: 42.5707 - lr: 5.0805e-08 - 62s/epoch - 315ms/step
Epoch 4/1000
2023-10-10 02:53:59.990 
Epoch 4/1000 
	 loss: 42.3649, MinusLogProbMetric: 42.3649, val_loss: 42.5855, val_MinusLogProbMetric: 42.5855

Epoch 4: val_loss did not improve from 42.57074
196/196 - 62s - loss: 42.3649 - MinusLogProbMetric: 42.3649 - val_loss: 42.5855 - val_MinusLogProbMetric: 42.5855 - lr: 5.0805e-08 - 62s/epoch - 317ms/step
Epoch 5/1000
2023-10-10 02:55:03.264 
Epoch 5/1000 
	 loss: 42.3483, MinusLogProbMetric: 42.3483, val_loss: 42.5598, val_MinusLogProbMetric: 42.5598

Epoch 5: val_loss improved from 42.57074 to 42.55981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.3483 - MinusLogProbMetric: 42.3483 - val_loss: 42.5598 - val_MinusLogProbMetric: 42.5598 - lr: 5.0805e-08 - 64s/epoch - 329ms/step
Epoch 6/1000
2023-10-10 02:56:07.968 
Epoch 6/1000 
	 loss: 42.3390, MinusLogProbMetric: 42.3390, val_loss: 42.5564, val_MinusLogProbMetric: 42.5564

Epoch 6: val_loss improved from 42.55981 to 42.55644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 42.3390 - MinusLogProbMetric: 42.3390 - val_loss: 42.5564 - val_MinusLogProbMetric: 42.5564 - lr: 5.0805e-08 - 65s/epoch - 330ms/step
Epoch 7/1000
2023-10-10 02:57:12.282 
Epoch 7/1000 
	 loss: 42.3325, MinusLogProbMetric: 42.3325, val_loss: 42.5542, val_MinusLogProbMetric: 42.5542

Epoch 7: val_loss improved from 42.55644 to 42.55418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.3325 - MinusLogProbMetric: 42.3325 - val_loss: 42.5542 - val_MinusLogProbMetric: 42.5542 - lr: 5.0805e-08 - 64s/epoch - 327ms/step
Epoch 8/1000
2023-10-10 02:58:17.775 
Epoch 8/1000 
	 loss: 42.3308, MinusLogProbMetric: 42.3308, val_loss: 42.5455, val_MinusLogProbMetric: 42.5455

Epoch 8: val_loss improved from 42.55418 to 42.54553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 42.3308 - MinusLogProbMetric: 42.3308 - val_loss: 42.5455 - val_MinusLogProbMetric: 42.5455 - lr: 5.0805e-08 - 66s/epoch - 335ms/step
Epoch 9/1000
2023-10-10 02:59:22.115 
Epoch 9/1000 
	 loss: 42.3303, MinusLogProbMetric: 42.3303, val_loss: 42.5664, val_MinusLogProbMetric: 42.5664

Epoch 9: val_loss did not improve from 42.54553
196/196 - 63s - loss: 42.3303 - MinusLogProbMetric: 42.3303 - val_loss: 42.5664 - val_MinusLogProbMetric: 42.5664 - lr: 5.0805e-08 - 63s/epoch - 322ms/step
Epoch 10/1000
2023-10-10 03:00:25.435 
Epoch 10/1000 
	 loss: 42.3533, MinusLogProbMetric: 42.3533, val_loss: 42.5763, val_MinusLogProbMetric: 42.5763

Epoch 10: val_loss did not improve from 42.54553
196/196 - 63s - loss: 42.3533 - MinusLogProbMetric: 42.3533 - val_loss: 42.5763 - val_MinusLogProbMetric: 42.5763 - lr: 5.0805e-08 - 63s/epoch - 323ms/step
Epoch 11/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 171: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 03:01:21.233 
Epoch 11/1000 
	 loss: nan, MinusLogProbMetric: 42.3604, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 11: val_loss did not improve from 42.54553
196/196 - 56s - loss: nan - MinusLogProbMetric: 42.3604 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 56s/epoch - 285ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.6935087808430278e-08.
===========
Generating train data for run 263.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_263/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_263
self.data_kwargs: {'seed': 377}
self.x_data: [[5.416377   7.151266   6.5657053  ... 3.613302   2.6304417  8.037386  ]
 [4.297167   5.832213   0.12875912 ... 1.4317983  6.415448   1.3533646 ]
 [6.2353234  7.1795187  6.219724   ... 4.5891814  2.66546    7.551563  ]
 ...
 [1.8237456  4.102903   7.5608215  ... 7.136859   2.564185   1.5539532 ]
 [2.2305896  4.6568003  8.321339   ... 7.0577617  2.6189713  1.518045  ]
 [5.653481   7.1263475  6.929123   ... 4.292566   2.643216   7.404443  ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_153"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_154 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  1074400   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,074,400
Trainable params: 1,074,400
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f0b28413220>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b01dd6f50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b01dd6f50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b61e02ef0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b091cc160>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b091cc6d0>, <keras.callbacks.ModelCheckpoint object at 0x7f0b091cc790>, <keras.callbacks.EarlyStopping object at 0x7f0b091cca00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b091cca30>, <keras.callbacks.TerminateOnNaN object at 0x7f0b091cc670>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.1831715 ,  3.2834203 ,  8.308094  , ...,  7.3133397 ,
         2.7002556 ,  1.7567806 ],
       [ 6.1308765 ,  7.147402  ,  6.0903115 , ...,  4.022617  ,
         2.6618268 ,  7.627545  ],
       [ 2.7429156 ,  3.6509063 ,  7.295548  , ...,  7.086021  ,
         2.8938296 ,  1.8224454 ],
       ...,
       [ 2.1098776 ,  4.1337423 ,  8.041495  , ...,  7.2043395 ,
         2.3724499 ,  1.8449893 ],
       [ 4.759076  ,  5.6016526 , -0.17659053, ...,  1.2483898 ,
         6.6213574 ,  1.391019  ],
       [ 4.6478305 ,  5.6608987 ,  0.05425403, ...,  1.0307544 ,
         6.027231  ,  1.3217621 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 263/720 with hyperparameters:
timestamp = 2023-10-10 03:01:30.145316
ndims = 32
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1074400
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.416377    7.151266    6.5657053   5.44807     3.870332    6.1800294
  2.9008856   8.441634    9.391678    4.1002183   7.382605    5.3625755
  5.5868373   9.949682    0.82923275  1.9832962  -0.10069095  8.219267
  8.539603    9.221239    9.5321455   8.239499    4.945965    8.152974
 -0.16393006  6.303131    0.6036788   9.50227     6.1976576   3.613302
  2.6304417   8.037386  ]
Epoch 1/1000
2023-10-10 03:04:52.847 
Epoch 1/1000 
	 loss: 42.3245, MinusLogProbMetric: 42.3245, val_loss: 42.5443, val_MinusLogProbMetric: 42.5443

Epoch 1: val_loss improved from inf to 42.54428, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 204s - loss: 42.3245 - MinusLogProbMetric: 42.3245 - val_loss: 42.5443 - val_MinusLogProbMetric: 42.5443 - lr: 1.6935e-08 - 204s/epoch - 1s/step
Epoch 2/1000
2023-10-10 03:05:57.852 
Epoch 2/1000 
	 loss: 42.3216, MinusLogProbMetric: 42.3216, val_loss: 42.5442, val_MinusLogProbMetric: 42.5442

Epoch 2: val_loss improved from 42.54428 to 42.54424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.3216 - MinusLogProbMetric: 42.3216 - val_loss: 42.5442 - val_MinusLogProbMetric: 42.5442 - lr: 1.6935e-08 - 64s/epoch - 327ms/step
Epoch 3/1000
2023-10-10 03:07:01.621 
Epoch 3/1000 
	 loss: 42.3191, MinusLogProbMetric: 42.3191, val_loss: 42.5406, val_MinusLogProbMetric: 42.5406

Epoch 3: val_loss improved from 42.54424 to 42.54060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.3191 - MinusLogProbMetric: 42.3191 - val_loss: 42.5406 - val_MinusLogProbMetric: 42.5406 - lr: 1.6935e-08 - 64s/epoch - 327ms/step
Epoch 4/1000
2023-10-10 03:08:04.924 
Epoch 4/1000 
	 loss: 42.3176, MinusLogProbMetric: 42.3176, val_loss: 42.5451, val_MinusLogProbMetric: 42.5451

Epoch 4: val_loss did not improve from 42.54060
196/196 - 62s - loss: 42.3176 - MinusLogProbMetric: 42.3176 - val_loss: 42.5451 - val_MinusLogProbMetric: 42.5451 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 5/1000
2023-10-10 03:09:07.016 
Epoch 5/1000 
	 loss: 42.3178, MinusLogProbMetric: 42.3178, val_loss: 42.5404, val_MinusLogProbMetric: 42.5404

Epoch 5: val_loss improved from 42.54060 to 42.54039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.3178 - MinusLogProbMetric: 42.3178 - val_loss: 42.5404 - val_MinusLogProbMetric: 42.5404 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 6/1000
2023-10-10 03:10:10.221 
Epoch 6/1000 
	 loss: 42.3156, MinusLogProbMetric: 42.3156, val_loss: 42.5427, val_MinusLogProbMetric: 42.5427

Epoch 6: val_loss did not improve from 42.54039
196/196 - 62s - loss: 42.3156 - MinusLogProbMetric: 42.3156 - val_loss: 42.5427 - val_MinusLogProbMetric: 42.5427 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 7/1000
2023-10-10 03:11:14.024 
Epoch 7/1000 
	 loss: 42.3162, MinusLogProbMetric: 42.3162, val_loss: 42.5407, val_MinusLogProbMetric: 42.5407

Epoch 7: val_loss did not improve from 42.54039
196/196 - 64s - loss: 42.3162 - MinusLogProbMetric: 42.3162 - val_loss: 42.5407 - val_MinusLogProbMetric: 42.5407 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 8/1000
2023-10-10 03:12:16.253 
Epoch 8/1000 
	 loss: 42.3172, MinusLogProbMetric: 42.3172, val_loss: 42.5416, val_MinusLogProbMetric: 42.5416

Epoch 8: val_loss did not improve from 42.54039
196/196 - 62s - loss: 42.3172 - MinusLogProbMetric: 42.3172 - val_loss: 42.5416 - val_MinusLogProbMetric: 42.5416 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 9/1000
2023-10-10 03:13:19.937 
Epoch 9/1000 
	 loss: 42.3139, MinusLogProbMetric: 42.3139, val_loss: 42.5344, val_MinusLogProbMetric: 42.5344

Epoch 9: val_loss improved from 42.54039 to 42.53436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 42.3139 - MinusLogProbMetric: 42.3139 - val_loss: 42.5344 - val_MinusLogProbMetric: 42.5344 - lr: 1.6935e-08 - 65s/epoch - 331ms/step
Epoch 10/1000
2023-10-10 03:14:24.445 
Epoch 10/1000 
	 loss: 42.3116, MinusLogProbMetric: 42.3116, val_loss: 42.5352, val_MinusLogProbMetric: 42.5352

Epoch 10: val_loss did not improve from 42.53436
196/196 - 63s - loss: 42.3116 - MinusLogProbMetric: 42.3116 - val_loss: 42.5352 - val_MinusLogProbMetric: 42.5352 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 11/1000
2023-10-10 03:15:28.671 
Epoch 11/1000 
	 loss: 42.3153, MinusLogProbMetric: 42.3153, val_loss: 42.5474, val_MinusLogProbMetric: 42.5474

Epoch 11: val_loss did not improve from 42.53436
196/196 - 64s - loss: 42.3153 - MinusLogProbMetric: 42.3153 - val_loss: 42.5474 - val_MinusLogProbMetric: 42.5474 - lr: 1.6935e-08 - 64s/epoch - 328ms/step
Epoch 12/1000
2023-10-10 03:16:30.348 
Epoch 12/1000 
	 loss: 42.3204, MinusLogProbMetric: 42.3204, val_loss: 42.5433, val_MinusLogProbMetric: 42.5433

Epoch 12: val_loss did not improve from 42.53436
196/196 - 62s - loss: 42.3204 - MinusLogProbMetric: 42.3204 - val_loss: 42.5433 - val_MinusLogProbMetric: 42.5433 - lr: 1.6935e-08 - 62s/epoch - 315ms/step
Epoch 13/1000
2023-10-10 03:17:33.268 
Epoch 13/1000 
	 loss: 42.3156, MinusLogProbMetric: 42.3156, val_loss: 42.5378, val_MinusLogProbMetric: 42.5378

Epoch 13: val_loss did not improve from 42.53436
196/196 - 63s - loss: 42.3156 - MinusLogProbMetric: 42.3156 - val_loss: 42.5378 - val_MinusLogProbMetric: 42.5378 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 14/1000
2023-10-10 03:18:35.971 
Epoch 14/1000 
	 loss: 42.3119, MinusLogProbMetric: 42.3119, val_loss: 42.5357, val_MinusLogProbMetric: 42.5357

Epoch 14: val_loss did not improve from 42.53436
196/196 - 63s - loss: 42.3119 - MinusLogProbMetric: 42.3119 - val_loss: 42.5357 - val_MinusLogProbMetric: 42.5357 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 15/1000
2023-10-10 03:19:38.186 
Epoch 15/1000 
	 loss: 42.3134, MinusLogProbMetric: 42.3134, val_loss: 42.5422, val_MinusLogProbMetric: 42.5422

Epoch 15: val_loss did not improve from 42.53436
196/196 - 62s - loss: 42.3134 - MinusLogProbMetric: 42.3134 - val_loss: 42.5422 - val_MinusLogProbMetric: 42.5422 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 16/1000
2023-10-10 03:20:41.136 
Epoch 16/1000 
	 loss: 42.3135, MinusLogProbMetric: 42.3135, val_loss: 42.5340, val_MinusLogProbMetric: 42.5340

Epoch 16: val_loss improved from 42.53436 to 42.53397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.3135 - MinusLogProbMetric: 42.3135 - val_loss: 42.5340 - val_MinusLogProbMetric: 42.5340 - lr: 1.6935e-08 - 64s/epoch - 327ms/step
Epoch 17/1000
2023-10-10 03:21:46.966 
Epoch 17/1000 
	 loss: 42.3094, MinusLogProbMetric: 42.3094, val_loss: 42.5373, val_MinusLogProbMetric: 42.5373

Epoch 17: val_loss did not improve from 42.53397
196/196 - 65s - loss: 42.3094 - MinusLogProbMetric: 42.3094 - val_loss: 42.5373 - val_MinusLogProbMetric: 42.5373 - lr: 1.6935e-08 - 65s/epoch - 330ms/step
Epoch 18/1000
2023-10-10 03:22:50.144 
Epoch 18/1000 
	 loss: 42.3089, MinusLogProbMetric: 42.3089, val_loss: 42.5348, val_MinusLogProbMetric: 42.5348

Epoch 18: val_loss did not improve from 42.53397
196/196 - 63s - loss: 42.3089 - MinusLogProbMetric: 42.3089 - val_loss: 42.5348 - val_MinusLogProbMetric: 42.5348 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 19/1000
2023-10-10 03:23:54.275 
Epoch 19/1000 
	 loss: 42.3044, MinusLogProbMetric: 42.3044, val_loss: 42.5316, val_MinusLogProbMetric: 42.5316

Epoch 19: val_loss improved from 42.53397 to 42.53164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 42.3044 - MinusLogProbMetric: 42.3044 - val_loss: 42.5316 - val_MinusLogProbMetric: 42.5316 - lr: 1.6935e-08 - 65s/epoch - 332ms/step
Epoch 20/1000
2023-10-10 03:24:58.664 
Epoch 20/1000 
	 loss: 42.3035, MinusLogProbMetric: 42.3035, val_loss: 42.5320, val_MinusLogProbMetric: 42.5320

Epoch 20: val_loss did not improve from 42.53164
196/196 - 63s - loss: 42.3035 - MinusLogProbMetric: 42.3035 - val_loss: 42.5320 - val_MinusLogProbMetric: 42.5320 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 21/1000
2023-10-10 03:26:02.812 
Epoch 21/1000 
	 loss: 42.3041, MinusLogProbMetric: 42.3041, val_loss: 42.5275, val_MinusLogProbMetric: 42.5275

Epoch 21: val_loss improved from 42.53164 to 42.52753, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 42.3041 - MinusLogProbMetric: 42.3041 - val_loss: 42.5275 - val_MinusLogProbMetric: 42.5275 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 22/1000
2023-10-10 03:27:07.383 
Epoch 22/1000 
	 loss: 42.3053, MinusLogProbMetric: 42.3053, val_loss: 42.5281, val_MinusLogProbMetric: 42.5281

Epoch 22: val_loss did not improve from 42.52753
196/196 - 64s - loss: 42.3053 - MinusLogProbMetric: 42.3053 - val_loss: 42.5281 - val_MinusLogProbMetric: 42.5281 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 23/1000
2023-10-10 03:28:10.375 
Epoch 23/1000 
	 loss: 42.3047, MinusLogProbMetric: 42.3047, val_loss: 42.5267, val_MinusLogProbMetric: 42.5267

Epoch 23: val_loss improved from 42.52753 to 42.52666, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.3047 - MinusLogProbMetric: 42.3047 - val_loss: 42.5267 - val_MinusLogProbMetric: 42.5267 - lr: 1.6935e-08 - 64s/epoch - 327ms/step
Epoch 24/1000
2023-10-10 03:29:16.704 
Epoch 24/1000 
	 loss: 42.3023, MinusLogProbMetric: 42.3023, val_loss: 42.5236, val_MinusLogProbMetric: 42.5236

Epoch 24: val_loss improved from 42.52666 to 42.52357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 42.3023 - MinusLogProbMetric: 42.3023 - val_loss: 42.5236 - val_MinusLogProbMetric: 42.5236 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 25/1000
2023-10-10 03:30:22.362 
Epoch 25/1000 
	 loss: 42.3019, MinusLogProbMetric: 42.3019, val_loss: 42.5257, val_MinusLogProbMetric: 42.5257

Epoch 25: val_loss did not improve from 42.52357
196/196 - 65s - loss: 42.3019 - MinusLogProbMetric: 42.3019 - val_loss: 42.5257 - val_MinusLogProbMetric: 42.5257 - lr: 1.6935e-08 - 65s/epoch - 329ms/step
Epoch 26/1000
2023-10-10 03:31:25.217 
Epoch 26/1000 
	 loss: 42.3071, MinusLogProbMetric: 42.3071, val_loss: 42.5297, val_MinusLogProbMetric: 42.5297

Epoch 26: val_loss did not improve from 42.52357
196/196 - 63s - loss: 42.3071 - MinusLogProbMetric: 42.3071 - val_loss: 42.5297 - val_MinusLogProbMetric: 42.5297 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 27/1000
2023-10-10 03:32:28.390 
Epoch 27/1000 
	 loss: 42.3036, MinusLogProbMetric: 42.3036, val_loss: 42.5292, val_MinusLogProbMetric: 42.5292

Epoch 27: val_loss did not improve from 42.52357
196/196 - 63s - loss: 42.3036 - MinusLogProbMetric: 42.3036 - val_loss: 42.5292 - val_MinusLogProbMetric: 42.5292 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 28/1000
2023-10-10 03:33:32.210 
Epoch 28/1000 
	 loss: 42.3013, MinusLogProbMetric: 42.3013, val_loss: 42.5278, val_MinusLogProbMetric: 42.5278

Epoch 28: val_loss did not improve from 42.52357
196/196 - 64s - loss: 42.3013 - MinusLogProbMetric: 42.3013 - val_loss: 42.5278 - val_MinusLogProbMetric: 42.5278 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 29/1000
2023-10-10 03:34:34.707 
Epoch 29/1000 
	 loss: 42.3023, MinusLogProbMetric: 42.3023, val_loss: 42.5254, val_MinusLogProbMetric: 42.5254

Epoch 29: val_loss did not improve from 42.52357
196/196 - 62s - loss: 42.3023 - MinusLogProbMetric: 42.3023 - val_loss: 42.5254 - val_MinusLogProbMetric: 42.5254 - lr: 1.6935e-08 - 62s/epoch - 319ms/step
Epoch 30/1000
2023-10-10 03:35:37.187 
Epoch 30/1000 
	 loss: 42.3028, MinusLogProbMetric: 42.3028, val_loss: 42.5316, val_MinusLogProbMetric: 42.5316

Epoch 30: val_loss did not improve from 42.52357
196/196 - 62s - loss: 42.3028 - MinusLogProbMetric: 42.3028 - val_loss: 42.5316 - val_MinusLogProbMetric: 42.5316 - lr: 1.6935e-08 - 62s/epoch - 319ms/step
Epoch 31/1000
2023-10-10 03:36:40.622 
Epoch 31/1000 
	 loss: 42.3029, MinusLogProbMetric: 42.3029, val_loss: 42.5348, val_MinusLogProbMetric: 42.5348

Epoch 31: val_loss did not improve from 42.52357
196/196 - 63s - loss: 42.3029 - MinusLogProbMetric: 42.3029 - val_loss: 42.5348 - val_MinusLogProbMetric: 42.5348 - lr: 1.6935e-08 - 63s/epoch - 324ms/step
Epoch 32/1000
2023-10-10 03:37:42.786 
Epoch 32/1000 
	 loss: 42.3041, MinusLogProbMetric: 42.3041, val_loss: 42.5250, val_MinusLogProbMetric: 42.5250

Epoch 32: val_loss did not improve from 42.52357
196/196 - 62s - loss: 42.3041 - MinusLogProbMetric: 42.3041 - val_loss: 42.5250 - val_MinusLogProbMetric: 42.5250 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 33/1000
2023-10-10 03:38:47.313 
Epoch 33/1000 
	 loss: 42.2997, MinusLogProbMetric: 42.2997, val_loss: 42.5245, val_MinusLogProbMetric: 42.5245

Epoch 33: val_loss did not improve from 42.52357
196/196 - 65s - loss: 42.2997 - MinusLogProbMetric: 42.2997 - val_loss: 42.5245 - val_MinusLogProbMetric: 42.5245 - lr: 1.6935e-08 - 65s/epoch - 329ms/step
Epoch 34/1000
2023-10-10 03:39:47.668 
Epoch 34/1000 
	 loss: 42.2982, MinusLogProbMetric: 42.2982, val_loss: 42.5239, val_MinusLogProbMetric: 42.5239

Epoch 34: val_loss did not improve from 42.52357
196/196 - 60s - loss: 42.2982 - MinusLogProbMetric: 42.2982 - val_loss: 42.5239 - val_MinusLogProbMetric: 42.5239 - lr: 1.6935e-08 - 60s/epoch - 308ms/step
Epoch 35/1000
2023-10-10 03:40:50.534 
Epoch 35/1000 
	 loss: 42.2960, MinusLogProbMetric: 42.2960, val_loss: 42.5236, val_MinusLogProbMetric: 42.5236

Epoch 35: val_loss did not improve from 42.52357
196/196 - 63s - loss: 42.2960 - MinusLogProbMetric: 42.2960 - val_loss: 42.5236 - val_MinusLogProbMetric: 42.5236 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 36/1000
2023-10-10 03:41:53.296 
Epoch 36/1000 
	 loss: 42.2965, MinusLogProbMetric: 42.2965, val_loss: 42.5218, val_MinusLogProbMetric: 42.5218

Epoch 36: val_loss improved from 42.52357 to 42.52180, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2965 - MinusLogProbMetric: 42.2965 - val_loss: 42.5218 - val_MinusLogProbMetric: 42.5218 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 37/1000
2023-10-10 03:42:56.665 
Epoch 37/1000 
	 loss: 42.2956, MinusLogProbMetric: 42.2956, val_loss: 42.5265, val_MinusLogProbMetric: 42.5265

Epoch 37: val_loss did not improve from 42.52180
196/196 - 62s - loss: 42.2956 - MinusLogProbMetric: 42.2956 - val_loss: 42.5265 - val_MinusLogProbMetric: 42.5265 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 38/1000
2023-10-10 03:44:00.419 
Epoch 38/1000 
	 loss: 42.2933, MinusLogProbMetric: 42.2933, val_loss: 42.5187, val_MinusLogProbMetric: 42.5187

Epoch 38: val_loss improved from 42.52180 to 42.51873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 42.2933 - MinusLogProbMetric: 42.2933 - val_loss: 42.5187 - val_MinusLogProbMetric: 42.5187 - lr: 1.6935e-08 - 65s/epoch - 331ms/step
Epoch 39/1000
2023-10-10 03:45:04.189 
Epoch 39/1000 
	 loss: 42.2943, MinusLogProbMetric: 42.2943, val_loss: 42.5198, val_MinusLogProbMetric: 42.5198

Epoch 39: val_loss did not improve from 42.51873
196/196 - 63s - loss: 42.2943 - MinusLogProbMetric: 42.2943 - val_loss: 42.5198 - val_MinusLogProbMetric: 42.5198 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 40/1000
2023-10-10 03:46:05.151 
Epoch 40/1000 
	 loss: 42.2948, MinusLogProbMetric: 42.2948, val_loss: 42.5197, val_MinusLogProbMetric: 42.5197

Epoch 40: val_loss did not improve from 42.51873
196/196 - 61s - loss: 42.2948 - MinusLogProbMetric: 42.2948 - val_loss: 42.5197 - val_MinusLogProbMetric: 42.5197 - lr: 1.6935e-08 - 61s/epoch - 311ms/step
Epoch 41/1000
2023-10-10 03:47:05.654 
Epoch 41/1000 
	 loss: 42.2945, MinusLogProbMetric: 42.2945, val_loss: 42.5291, val_MinusLogProbMetric: 42.5291

Epoch 41: val_loss did not improve from 42.51873
196/196 - 60s - loss: 42.2945 - MinusLogProbMetric: 42.2945 - val_loss: 42.5291 - val_MinusLogProbMetric: 42.5291 - lr: 1.6935e-08 - 60s/epoch - 309ms/step
Epoch 42/1000
2023-10-10 03:48:07.869 
Epoch 42/1000 
	 loss: 42.2971, MinusLogProbMetric: 42.2971, val_loss: 42.5220, val_MinusLogProbMetric: 42.5220

Epoch 42: val_loss did not improve from 42.51873
196/196 - 62s - loss: 42.2971 - MinusLogProbMetric: 42.2971 - val_loss: 42.5220 - val_MinusLogProbMetric: 42.5220 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 43/1000
2023-10-10 03:49:10.450 
Epoch 43/1000 
	 loss: 42.2948, MinusLogProbMetric: 42.2948, val_loss: 42.5209, val_MinusLogProbMetric: 42.5209

Epoch 43: val_loss did not improve from 42.51873
196/196 - 63s - loss: 42.2948 - MinusLogProbMetric: 42.2948 - val_loss: 42.5209 - val_MinusLogProbMetric: 42.5209 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 44/1000
2023-10-10 03:50:13.962 
Epoch 44/1000 
	 loss: 42.2932, MinusLogProbMetric: 42.2932, val_loss: 42.5215, val_MinusLogProbMetric: 42.5215

Epoch 44: val_loss did not improve from 42.51873
196/196 - 64s - loss: 42.2932 - MinusLogProbMetric: 42.2932 - val_loss: 42.5215 - val_MinusLogProbMetric: 42.5215 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 45/1000
2023-10-10 03:51:17.452 
Epoch 45/1000 
	 loss: 42.2929, MinusLogProbMetric: 42.2929, val_loss: 42.5208, val_MinusLogProbMetric: 42.5208

Epoch 45: val_loss did not improve from 42.51873
196/196 - 63s - loss: 42.2929 - MinusLogProbMetric: 42.2929 - val_loss: 42.5208 - val_MinusLogProbMetric: 42.5208 - lr: 1.6935e-08 - 63s/epoch - 324ms/step
Epoch 46/1000
2023-10-10 03:52:21.206 
Epoch 46/1000 
	 loss: 42.2913, MinusLogProbMetric: 42.2913, val_loss: 42.5198, val_MinusLogProbMetric: 42.5198

Epoch 46: val_loss did not improve from 42.51873
196/196 - 64s - loss: 42.2913 - MinusLogProbMetric: 42.2913 - val_loss: 42.5198 - val_MinusLogProbMetric: 42.5198 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 47/1000
2023-10-10 03:53:24.180 
Epoch 47/1000 
	 loss: 42.2911, MinusLogProbMetric: 42.2911, val_loss: 42.5200, val_MinusLogProbMetric: 42.5200

Epoch 47: val_loss did not improve from 42.51873
196/196 - 63s - loss: 42.2911 - MinusLogProbMetric: 42.2911 - val_loss: 42.5200 - val_MinusLogProbMetric: 42.5200 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 48/1000
2023-10-10 03:54:26.858 
Epoch 48/1000 
	 loss: 42.2893, MinusLogProbMetric: 42.2893, val_loss: 42.5140, val_MinusLogProbMetric: 42.5140

Epoch 48: val_loss improved from 42.51873 to 42.51396, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2893 - MinusLogProbMetric: 42.2893 - val_loss: 42.5140 - val_MinusLogProbMetric: 42.5140 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 49/1000
2023-10-10 03:55:30.892 
Epoch 49/1000 
	 loss: 42.2940, MinusLogProbMetric: 42.2940, val_loss: 42.5211, val_MinusLogProbMetric: 42.5211

Epoch 49: val_loss did not improve from 42.51396
196/196 - 63s - loss: 42.2940 - MinusLogProbMetric: 42.2940 - val_loss: 42.5211 - val_MinusLogProbMetric: 42.5211 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 50/1000
2023-10-10 03:56:34.243 
Epoch 50/1000 
	 loss: 42.2910, MinusLogProbMetric: 42.2910, val_loss: 42.5199, val_MinusLogProbMetric: 42.5199

Epoch 50: val_loss did not improve from 42.51396
196/196 - 63s - loss: 42.2910 - MinusLogProbMetric: 42.2910 - val_loss: 42.5199 - val_MinusLogProbMetric: 42.5199 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 51/1000
2023-10-10 03:57:37.687 
Epoch 51/1000 
	 loss: 42.2951, MinusLogProbMetric: 42.2951, val_loss: 42.5169, val_MinusLogProbMetric: 42.5169

Epoch 51: val_loss did not improve from 42.51396
196/196 - 63s - loss: 42.2951 - MinusLogProbMetric: 42.2951 - val_loss: 42.5169 - val_MinusLogProbMetric: 42.5169 - lr: 1.6935e-08 - 63s/epoch - 324ms/step
Epoch 52/1000
2023-10-10 03:58:41.392 
Epoch 52/1000 
	 loss: 42.2912, MinusLogProbMetric: 42.2912, val_loss: 42.5147, val_MinusLogProbMetric: 42.5147

Epoch 52: val_loss did not improve from 42.51396
196/196 - 64s - loss: 42.2912 - MinusLogProbMetric: 42.2912 - val_loss: 42.5147 - val_MinusLogProbMetric: 42.5147 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 53/1000
2023-10-10 03:59:44.812 
Epoch 53/1000 
	 loss: 42.2918, MinusLogProbMetric: 42.2918, val_loss: 42.5160, val_MinusLogProbMetric: 42.5160

Epoch 53: val_loss did not improve from 42.51396
196/196 - 63s - loss: 42.2918 - MinusLogProbMetric: 42.2918 - val_loss: 42.5160 - val_MinusLogProbMetric: 42.5160 - lr: 1.6935e-08 - 63s/epoch - 324ms/step
Epoch 54/1000
2023-10-10 04:00:47.731 
Epoch 54/1000 
	 loss: 42.2890, MinusLogProbMetric: 42.2890, val_loss: 42.5157, val_MinusLogProbMetric: 42.5157

Epoch 54: val_loss did not improve from 42.51396
196/196 - 63s - loss: 42.2890 - MinusLogProbMetric: 42.2890 - val_loss: 42.5157 - val_MinusLogProbMetric: 42.5157 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 55/1000
2023-10-10 04:01:50.630 
Epoch 55/1000 
	 loss: 42.2880, MinusLogProbMetric: 42.2880, val_loss: 42.5107, val_MinusLogProbMetric: 42.5107

Epoch 55: val_loss improved from 42.51396 to 42.51075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2880 - MinusLogProbMetric: 42.2880 - val_loss: 42.5107 - val_MinusLogProbMetric: 42.5107 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 56/1000
2023-10-10 04:02:54.689 
Epoch 56/1000 
	 loss: 42.2853, MinusLogProbMetric: 42.2853, val_loss: 42.5126, val_MinusLogProbMetric: 42.5126

Epoch 56: val_loss did not improve from 42.51075
196/196 - 63s - loss: 42.2853 - MinusLogProbMetric: 42.2853 - val_loss: 42.5126 - val_MinusLogProbMetric: 42.5126 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 57/1000
2023-10-10 04:03:57.960 
Epoch 57/1000 
	 loss: 42.2866, MinusLogProbMetric: 42.2866, val_loss: 42.5130, val_MinusLogProbMetric: 42.5130

Epoch 57: val_loss did not improve from 42.51075
196/196 - 63s - loss: 42.2866 - MinusLogProbMetric: 42.2866 - val_loss: 42.5130 - val_MinusLogProbMetric: 42.5130 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 58/1000
2023-10-10 04:05:01.121 
Epoch 58/1000 
	 loss: 42.2857, MinusLogProbMetric: 42.2857, val_loss: 42.5113, val_MinusLogProbMetric: 42.5113

Epoch 58: val_loss did not improve from 42.51075
196/196 - 63s - loss: 42.2857 - MinusLogProbMetric: 42.2857 - val_loss: 42.5113 - val_MinusLogProbMetric: 42.5113 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 59/1000
2023-10-10 04:06:03.936 
Epoch 59/1000 
	 loss: 42.2875, MinusLogProbMetric: 42.2875, val_loss: 42.5089, val_MinusLogProbMetric: 42.5089

Epoch 59: val_loss improved from 42.51075 to 42.50890, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2875 - MinusLogProbMetric: 42.2875 - val_loss: 42.5089 - val_MinusLogProbMetric: 42.5089 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 60/1000
2023-10-10 04:07:08.302 
Epoch 60/1000 
	 loss: 42.2854, MinusLogProbMetric: 42.2854, val_loss: 42.5065, val_MinusLogProbMetric: 42.5065

Epoch 60: val_loss improved from 42.50890 to 42.50646, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2854 - MinusLogProbMetric: 42.2854 - val_loss: 42.5065 - val_MinusLogProbMetric: 42.5065 - lr: 1.6935e-08 - 64s/epoch - 328ms/step
Epoch 61/1000
2023-10-10 04:08:12.305 
Epoch 61/1000 
	 loss: 42.2860, MinusLogProbMetric: 42.2860, val_loss: 42.5088, val_MinusLogProbMetric: 42.5088

Epoch 61: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2860 - MinusLogProbMetric: 42.2860 - val_loss: 42.5088 - val_MinusLogProbMetric: 42.5088 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 62/1000
2023-10-10 04:09:15.233 
Epoch 62/1000 
	 loss: 42.2864, MinusLogProbMetric: 42.2864, val_loss: 42.5122, val_MinusLogProbMetric: 42.5122

Epoch 62: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2864 - MinusLogProbMetric: 42.2864 - val_loss: 42.5122 - val_MinusLogProbMetric: 42.5122 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 63/1000
2023-10-10 04:10:18.084 
Epoch 63/1000 
	 loss: 42.2840, MinusLogProbMetric: 42.2840, val_loss: 42.5090, val_MinusLogProbMetric: 42.5090

Epoch 63: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2840 - MinusLogProbMetric: 42.2840 - val_loss: 42.5090 - val_MinusLogProbMetric: 42.5090 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 64/1000
2023-10-10 04:11:21.011 
Epoch 64/1000 
	 loss: 42.2862, MinusLogProbMetric: 42.2862, val_loss: 42.5191, val_MinusLogProbMetric: 42.5191

Epoch 64: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2862 - MinusLogProbMetric: 42.2862 - val_loss: 42.5191 - val_MinusLogProbMetric: 42.5191 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 65/1000
2023-10-10 04:12:24.004 
Epoch 65/1000 
	 loss: 42.2876, MinusLogProbMetric: 42.2876, val_loss: 42.5115, val_MinusLogProbMetric: 42.5115

Epoch 65: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2876 - MinusLogProbMetric: 42.2876 - val_loss: 42.5115 - val_MinusLogProbMetric: 42.5115 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 66/1000
2023-10-10 04:13:27.333 
Epoch 66/1000 
	 loss: 42.2846, MinusLogProbMetric: 42.2846, val_loss: 42.5086, val_MinusLogProbMetric: 42.5086

Epoch 66: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2846 - MinusLogProbMetric: 42.2846 - val_loss: 42.5086 - val_MinusLogProbMetric: 42.5086 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 67/1000
2023-10-10 04:14:30.182 
Epoch 67/1000 
	 loss: 42.2842, MinusLogProbMetric: 42.2842, val_loss: 42.5099, val_MinusLogProbMetric: 42.5099

Epoch 67: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2842 - MinusLogProbMetric: 42.2842 - val_loss: 42.5099 - val_MinusLogProbMetric: 42.5099 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 68/1000
2023-10-10 04:15:32.778 
Epoch 68/1000 
	 loss: 42.2826, MinusLogProbMetric: 42.2826, val_loss: 42.5167, val_MinusLogProbMetric: 42.5167

Epoch 68: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2826 - MinusLogProbMetric: 42.2826 - val_loss: 42.5167 - val_MinusLogProbMetric: 42.5167 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 69/1000
2023-10-10 04:16:35.074 
Epoch 69/1000 
	 loss: 42.2883, MinusLogProbMetric: 42.2883, val_loss: 42.5202, val_MinusLogProbMetric: 42.5202

Epoch 69: val_loss did not improve from 42.50646
196/196 - 62s - loss: 42.2883 - MinusLogProbMetric: 42.2883 - val_loss: 42.5202 - val_MinusLogProbMetric: 42.5202 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 70/1000
2023-10-10 04:17:38.421 
Epoch 70/1000 
	 loss: 42.2886, MinusLogProbMetric: 42.2886, val_loss: 42.5160, val_MinusLogProbMetric: 42.5160

Epoch 70: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2886 - MinusLogProbMetric: 42.2886 - val_loss: 42.5160 - val_MinusLogProbMetric: 42.5160 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 71/1000
2023-10-10 04:18:41.101 
Epoch 71/1000 
	 loss: 42.2845, MinusLogProbMetric: 42.2845, val_loss: 42.5113, val_MinusLogProbMetric: 42.5113

Epoch 71: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2845 - MinusLogProbMetric: 42.2845 - val_loss: 42.5113 - val_MinusLogProbMetric: 42.5113 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 72/1000
2023-10-10 04:19:43.172 
Epoch 72/1000 
	 loss: 42.2864, MinusLogProbMetric: 42.2864, val_loss: 42.5193, val_MinusLogProbMetric: 42.5193

Epoch 72: val_loss did not improve from 42.50646
196/196 - 62s - loss: 42.2864 - MinusLogProbMetric: 42.2864 - val_loss: 42.5193 - val_MinusLogProbMetric: 42.5193 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 73/1000
2023-10-10 04:20:46.823 
Epoch 73/1000 
	 loss: 42.2859, MinusLogProbMetric: 42.2859, val_loss: 42.5114, val_MinusLogProbMetric: 42.5114

Epoch 73: val_loss did not improve from 42.50646
196/196 - 64s - loss: 42.2859 - MinusLogProbMetric: 42.2859 - val_loss: 42.5114 - val_MinusLogProbMetric: 42.5114 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 74/1000
2023-10-10 04:21:47.860 
Epoch 74/1000 
	 loss: 42.2827, MinusLogProbMetric: 42.2827, val_loss: 42.5092, val_MinusLogProbMetric: 42.5092

Epoch 74: val_loss did not improve from 42.50646
196/196 - 61s - loss: 42.2827 - MinusLogProbMetric: 42.2827 - val_loss: 42.5092 - val_MinusLogProbMetric: 42.5092 - lr: 1.6935e-08 - 61s/epoch - 311ms/step
Epoch 75/1000
2023-10-10 04:22:50.524 
Epoch 75/1000 
	 loss: 42.2823, MinusLogProbMetric: 42.2823, val_loss: 42.5090, val_MinusLogProbMetric: 42.5090

Epoch 75: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2823 - MinusLogProbMetric: 42.2823 - val_loss: 42.5090 - val_MinusLogProbMetric: 42.5090 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 76/1000
2023-10-10 04:23:53.399 
Epoch 76/1000 
	 loss: 42.2869, MinusLogProbMetric: 42.2869, val_loss: 42.5147, val_MinusLogProbMetric: 42.5147

Epoch 76: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2869 - MinusLogProbMetric: 42.2869 - val_loss: 42.5147 - val_MinusLogProbMetric: 42.5147 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 77/1000
2023-10-10 04:24:55.644 
Epoch 77/1000 
	 loss: 42.2873, MinusLogProbMetric: 42.2873, val_loss: 42.5122, val_MinusLogProbMetric: 42.5122

Epoch 77: val_loss did not improve from 42.50646
196/196 - 62s - loss: 42.2873 - MinusLogProbMetric: 42.2873 - val_loss: 42.5122 - val_MinusLogProbMetric: 42.5122 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 78/1000
2023-10-10 04:25:59.589 
Epoch 78/1000 
	 loss: 42.2859, MinusLogProbMetric: 42.2859, val_loss: 42.5125, val_MinusLogProbMetric: 42.5125

Epoch 78: val_loss did not improve from 42.50646
196/196 - 64s - loss: 42.2859 - MinusLogProbMetric: 42.2859 - val_loss: 42.5125 - val_MinusLogProbMetric: 42.5125 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 79/1000
2023-10-10 04:27:02.126 
Epoch 79/1000 
	 loss: 42.2838, MinusLogProbMetric: 42.2838, val_loss: 42.5081, val_MinusLogProbMetric: 42.5081

Epoch 79: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2838 - MinusLogProbMetric: 42.2838 - val_loss: 42.5081 - val_MinusLogProbMetric: 42.5081 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 80/1000
2023-10-10 04:28:04.813 
Epoch 80/1000 
	 loss: 42.2826, MinusLogProbMetric: 42.2826, val_loss: 42.5074, val_MinusLogProbMetric: 42.5074

Epoch 80: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2826 - MinusLogProbMetric: 42.2826 - val_loss: 42.5074 - val_MinusLogProbMetric: 42.5074 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 81/1000
2023-10-10 04:29:07.921 
Epoch 81/1000 
	 loss: 42.2881, MinusLogProbMetric: 42.2881, val_loss: 42.5202, val_MinusLogProbMetric: 42.5202

Epoch 81: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2881 - MinusLogProbMetric: 42.2881 - val_loss: 42.5202 - val_MinusLogProbMetric: 42.5202 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 82/1000
2023-10-10 04:30:10.037 
Epoch 82/1000 
	 loss: 42.2897, MinusLogProbMetric: 42.2897, val_loss: 42.5143, val_MinusLogProbMetric: 42.5143

Epoch 82: val_loss did not improve from 42.50646
196/196 - 62s - loss: 42.2897 - MinusLogProbMetric: 42.2897 - val_loss: 42.5143 - val_MinusLogProbMetric: 42.5143 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 83/1000
2023-10-10 04:31:10.897 
Epoch 83/1000 
	 loss: 42.2875, MinusLogProbMetric: 42.2875, val_loss: 42.5119, val_MinusLogProbMetric: 42.5119

Epoch 83: val_loss did not improve from 42.50646
196/196 - 61s - loss: 42.2875 - MinusLogProbMetric: 42.2875 - val_loss: 42.5119 - val_MinusLogProbMetric: 42.5119 - lr: 1.6935e-08 - 61s/epoch - 311ms/step
Epoch 84/1000
2023-10-10 04:32:14.507 
Epoch 84/1000 
	 loss: 42.2926, MinusLogProbMetric: 42.2926, val_loss: 42.5176, val_MinusLogProbMetric: 42.5176

Epoch 84: val_loss did not improve from 42.50646
196/196 - 64s - loss: 42.2926 - MinusLogProbMetric: 42.2926 - val_loss: 42.5176 - val_MinusLogProbMetric: 42.5176 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 85/1000
2023-10-10 04:33:18.837 
Epoch 85/1000 
	 loss: 42.2915, MinusLogProbMetric: 42.2915, val_loss: 42.5163, val_MinusLogProbMetric: 42.5163

Epoch 85: val_loss did not improve from 42.50646
196/196 - 64s - loss: 42.2915 - MinusLogProbMetric: 42.2915 - val_loss: 42.5163 - val_MinusLogProbMetric: 42.5163 - lr: 1.6935e-08 - 64s/epoch - 328ms/step
Epoch 86/1000
2023-10-10 04:34:22.916 
Epoch 86/1000 
	 loss: 42.2875, MinusLogProbMetric: 42.2875, val_loss: 42.5136, val_MinusLogProbMetric: 42.5136

Epoch 86: val_loss did not improve from 42.50646
196/196 - 64s - loss: 42.2875 - MinusLogProbMetric: 42.2875 - val_loss: 42.5136 - val_MinusLogProbMetric: 42.5136 - lr: 1.6935e-08 - 64s/epoch - 327ms/step
Epoch 87/1000
2023-10-10 04:35:26.046 
Epoch 87/1000 
	 loss: 42.2882, MinusLogProbMetric: 42.2882, val_loss: 42.5120, val_MinusLogProbMetric: 42.5120

Epoch 87: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2882 - MinusLogProbMetric: 42.2882 - val_loss: 42.5120 - val_MinusLogProbMetric: 42.5120 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 88/1000
2023-10-10 04:36:29.110 
Epoch 88/1000 
	 loss: 42.2833, MinusLogProbMetric: 42.2833, val_loss: 42.5110, val_MinusLogProbMetric: 42.5110

Epoch 88: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2833 - MinusLogProbMetric: 42.2833 - val_loss: 42.5110 - val_MinusLogProbMetric: 42.5110 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 89/1000
2023-10-10 04:37:31.816 
Epoch 89/1000 
	 loss: 42.2843, MinusLogProbMetric: 42.2843, val_loss: 42.5115, val_MinusLogProbMetric: 42.5115

Epoch 89: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2843 - MinusLogProbMetric: 42.2843 - val_loss: 42.5115 - val_MinusLogProbMetric: 42.5115 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 90/1000
2023-10-10 04:38:34.850 
Epoch 90/1000 
	 loss: 42.2833, MinusLogProbMetric: 42.2833, val_loss: 42.5106, val_MinusLogProbMetric: 42.5106

Epoch 90: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2833 - MinusLogProbMetric: 42.2833 - val_loss: 42.5106 - val_MinusLogProbMetric: 42.5106 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 91/1000
2023-10-10 04:39:38.663 
Epoch 91/1000 
	 loss: 42.2828, MinusLogProbMetric: 42.2828, val_loss: 42.5135, val_MinusLogProbMetric: 42.5135

Epoch 91: val_loss did not improve from 42.50646
196/196 - 64s - loss: 42.2828 - MinusLogProbMetric: 42.2828 - val_loss: 42.5135 - val_MinusLogProbMetric: 42.5135 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 92/1000
2023-10-10 04:40:40.620 
Epoch 92/1000 
	 loss: 42.2836, MinusLogProbMetric: 42.2836, val_loss: 42.5125, val_MinusLogProbMetric: 42.5125

Epoch 92: val_loss did not improve from 42.50646
196/196 - 62s - loss: 42.2836 - MinusLogProbMetric: 42.2836 - val_loss: 42.5125 - val_MinusLogProbMetric: 42.5125 - lr: 1.6935e-08 - 62s/epoch - 316ms/step
Epoch 93/1000
2023-10-10 04:41:40.017 
Epoch 93/1000 
	 loss: 42.2804, MinusLogProbMetric: 42.2804, val_loss: 42.5093, val_MinusLogProbMetric: 42.5093

Epoch 93: val_loss did not improve from 42.50646
196/196 - 59s - loss: 42.2804 - MinusLogProbMetric: 42.2804 - val_loss: 42.5093 - val_MinusLogProbMetric: 42.5093 - lr: 1.6935e-08 - 59s/epoch - 303ms/step
Epoch 94/1000
2023-10-10 04:42:44.550 
Epoch 94/1000 
	 loss: 42.2806, MinusLogProbMetric: 42.2806, val_loss: 42.5090, val_MinusLogProbMetric: 42.5090

Epoch 94: val_loss did not improve from 42.50646
196/196 - 65s - loss: 42.2806 - MinusLogProbMetric: 42.2806 - val_loss: 42.5090 - val_MinusLogProbMetric: 42.5090 - lr: 1.6935e-08 - 65s/epoch - 329ms/step
Epoch 95/1000
2023-10-10 04:43:47.711 
Epoch 95/1000 
	 loss: 42.2787, MinusLogProbMetric: 42.2787, val_loss: 42.5088, val_MinusLogProbMetric: 42.5088

Epoch 95: val_loss did not improve from 42.50646
196/196 - 63s - loss: 42.2787 - MinusLogProbMetric: 42.2787 - val_loss: 42.5088 - val_MinusLogProbMetric: 42.5088 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 96/1000
2023-10-10 04:44:48.660 
Epoch 96/1000 
	 loss: 42.2800, MinusLogProbMetric: 42.2800, val_loss: 42.5071, val_MinusLogProbMetric: 42.5071

Epoch 96: val_loss did not improve from 42.50646
196/196 - 61s - loss: 42.2800 - MinusLogProbMetric: 42.2800 - val_loss: 42.5071 - val_MinusLogProbMetric: 42.5071 - lr: 1.6935e-08 - 61s/epoch - 311ms/step
Epoch 97/1000
2023-10-10 04:45:48.628 
Epoch 97/1000 
	 loss: 42.2788, MinusLogProbMetric: 42.2788, val_loss: 42.5060, val_MinusLogProbMetric: 42.5060

Epoch 97: val_loss improved from 42.50646 to 42.50599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 61s - loss: 42.2788 - MinusLogProbMetric: 42.2788 - val_loss: 42.5060 - val_MinusLogProbMetric: 42.5060 - lr: 1.6935e-08 - 61s/epoch - 312ms/step
Epoch 98/1000
2023-10-10 04:46:53.329 
Epoch 98/1000 
	 loss: 42.2760, MinusLogProbMetric: 42.2760, val_loss: 42.5035, val_MinusLogProbMetric: 42.5035

Epoch 98: val_loss improved from 42.50599 to 42.50348, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 42.2760 - MinusLogProbMetric: 42.2760 - val_loss: 42.5035 - val_MinusLogProbMetric: 42.5035 - lr: 1.6935e-08 - 65s/epoch - 330ms/step
Epoch 99/1000
2023-10-10 04:47:56.560 
Epoch 99/1000 
	 loss: 42.2766, MinusLogProbMetric: 42.2766, val_loss: 42.5043, val_MinusLogProbMetric: 42.5043

Epoch 99: val_loss did not improve from 42.50348
196/196 - 62s - loss: 42.2766 - MinusLogProbMetric: 42.2766 - val_loss: 42.5043 - val_MinusLogProbMetric: 42.5043 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 100/1000
2023-10-10 04:48:58.400 
Epoch 100/1000 
	 loss: 42.2767, MinusLogProbMetric: 42.2767, val_loss: 42.5058, val_MinusLogProbMetric: 42.5058

Epoch 100: val_loss did not improve from 42.50348
196/196 - 62s - loss: 42.2767 - MinusLogProbMetric: 42.2767 - val_loss: 42.5058 - val_MinusLogProbMetric: 42.5058 - lr: 1.6935e-08 - 62s/epoch - 315ms/step
Epoch 101/1000
2023-10-10 04:50:00.998 
Epoch 101/1000 
	 loss: 42.2729, MinusLogProbMetric: 42.2729, val_loss: 42.5048, val_MinusLogProbMetric: 42.5048

Epoch 101: val_loss did not improve from 42.50348
196/196 - 63s - loss: 42.2729 - MinusLogProbMetric: 42.2729 - val_loss: 42.5048 - val_MinusLogProbMetric: 42.5048 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 102/1000
2023-10-10 04:51:04.210 
Epoch 102/1000 
	 loss: 42.2760, MinusLogProbMetric: 42.2760, val_loss: 42.5017, val_MinusLogProbMetric: 42.5017

Epoch 102: val_loss improved from 42.50348 to 42.50170, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2760 - MinusLogProbMetric: 42.2760 - val_loss: 42.5017 - val_MinusLogProbMetric: 42.5017 - lr: 1.6935e-08 - 64s/epoch - 328ms/step
Epoch 103/1000
2023-10-10 04:52:08.195 
Epoch 103/1000 
	 loss: 42.2757, MinusLogProbMetric: 42.2757, val_loss: 42.5035, val_MinusLogProbMetric: 42.5035

Epoch 103: val_loss did not improve from 42.50170
196/196 - 63s - loss: 42.2757 - MinusLogProbMetric: 42.2757 - val_loss: 42.5035 - val_MinusLogProbMetric: 42.5035 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 104/1000
2023-10-10 04:53:11.282 
Epoch 104/1000 
	 loss: 42.2738, MinusLogProbMetric: 42.2738, val_loss: 42.5031, val_MinusLogProbMetric: 42.5031

Epoch 104: val_loss did not improve from 42.50170
196/196 - 63s - loss: 42.2738 - MinusLogProbMetric: 42.2738 - val_loss: 42.5031 - val_MinusLogProbMetric: 42.5031 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 105/1000
2023-10-10 04:54:12.959 
Epoch 105/1000 
	 loss: 42.2735, MinusLogProbMetric: 42.2735, val_loss: 42.5012, val_MinusLogProbMetric: 42.5012

Epoch 105: val_loss improved from 42.50170 to 42.50116, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.2735 - MinusLogProbMetric: 42.2735 - val_loss: 42.5012 - val_MinusLogProbMetric: 42.5012 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 106/1000
2023-10-10 04:55:16.602 
Epoch 106/1000 
	 loss: 42.2738, MinusLogProbMetric: 42.2738, val_loss: 42.5002, val_MinusLogProbMetric: 42.5002

Epoch 106: val_loss improved from 42.50116 to 42.50022, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2738 - MinusLogProbMetric: 42.2738 - val_loss: 42.5002 - val_MinusLogProbMetric: 42.5002 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 107/1000
2023-10-10 04:56:20.402 
Epoch 107/1000 
	 loss: 42.2740, MinusLogProbMetric: 42.2740, val_loss: 42.5011, val_MinusLogProbMetric: 42.5011

Epoch 107: val_loss did not improve from 42.50022
196/196 - 63s - loss: 42.2740 - MinusLogProbMetric: 42.2740 - val_loss: 42.5011 - val_MinusLogProbMetric: 42.5011 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 108/1000
2023-10-10 04:57:23.150 
Epoch 108/1000 
	 loss: 42.2721, MinusLogProbMetric: 42.2721, val_loss: 42.5005, val_MinusLogProbMetric: 42.5005

Epoch 108: val_loss did not improve from 42.50022
196/196 - 63s - loss: 42.2721 - MinusLogProbMetric: 42.2721 - val_loss: 42.5005 - val_MinusLogProbMetric: 42.5005 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 109/1000
2023-10-10 04:58:26.334 
Epoch 109/1000 
	 loss: 42.2713, MinusLogProbMetric: 42.2713, val_loss: 42.4978, val_MinusLogProbMetric: 42.4978

Epoch 109: val_loss improved from 42.50022 to 42.49777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2713 - MinusLogProbMetric: 42.2713 - val_loss: 42.4978 - val_MinusLogProbMetric: 42.4978 - lr: 1.6935e-08 - 64s/epoch - 329ms/step
Epoch 110/1000
2023-10-10 04:59:30.071 
Epoch 110/1000 
	 loss: 42.2703, MinusLogProbMetric: 42.2703, val_loss: 42.4980, val_MinusLogProbMetric: 42.4980

Epoch 110: val_loss did not improve from 42.49777
196/196 - 62s - loss: 42.2703 - MinusLogProbMetric: 42.2703 - val_loss: 42.4980 - val_MinusLogProbMetric: 42.4980 - lr: 1.6935e-08 - 62s/epoch - 319ms/step
Epoch 111/1000
2023-10-10 05:00:32.547 
Epoch 111/1000 
	 loss: 42.2708, MinusLogProbMetric: 42.2708, val_loss: 42.4992, val_MinusLogProbMetric: 42.4992

Epoch 111: val_loss did not improve from 42.49777
196/196 - 62s - loss: 42.2708 - MinusLogProbMetric: 42.2708 - val_loss: 42.4992 - val_MinusLogProbMetric: 42.4992 - lr: 1.6935e-08 - 62s/epoch - 319ms/step
Epoch 112/1000
2023-10-10 05:01:35.915 
Epoch 112/1000 
	 loss: 42.2707, MinusLogProbMetric: 42.2707, val_loss: 42.4957, val_MinusLogProbMetric: 42.4957

Epoch 112: val_loss improved from 42.49777 to 42.49569, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2707 - MinusLogProbMetric: 42.2707 - val_loss: 42.4957 - val_MinusLogProbMetric: 42.4957 - lr: 1.6935e-08 - 64s/epoch - 329ms/step
Epoch 113/1000
2023-10-10 05:02:40.328 
Epoch 113/1000 
	 loss: 42.2702, MinusLogProbMetric: 42.2702, val_loss: 42.4963, val_MinusLogProbMetric: 42.4963

Epoch 113: val_loss did not improve from 42.49569
196/196 - 63s - loss: 42.2702 - MinusLogProbMetric: 42.2702 - val_loss: 42.4963 - val_MinusLogProbMetric: 42.4963 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 114/1000
2023-10-10 05:03:43.364 
Epoch 114/1000 
	 loss: 42.2681, MinusLogProbMetric: 42.2681, val_loss: 42.4963, val_MinusLogProbMetric: 42.4963

Epoch 114: val_loss did not improve from 42.49569
196/196 - 63s - loss: 42.2681 - MinusLogProbMetric: 42.2681 - val_loss: 42.4963 - val_MinusLogProbMetric: 42.4963 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 115/1000
2023-10-10 05:04:46.257 
Epoch 115/1000 
	 loss: 42.2695, MinusLogProbMetric: 42.2695, val_loss: 42.4949, val_MinusLogProbMetric: 42.4949

Epoch 115: val_loss improved from 42.49569 to 42.49495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2695 - MinusLogProbMetric: 42.2695 - val_loss: 42.4949 - val_MinusLogProbMetric: 42.4949 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 116/1000
2023-10-10 05:05:47.971 
Epoch 116/1000 
	 loss: 42.2690, MinusLogProbMetric: 42.2690, val_loss: 42.4958, val_MinusLogProbMetric: 42.4958

Epoch 116: val_loss did not improve from 42.49495
196/196 - 61s - loss: 42.2690 - MinusLogProbMetric: 42.2690 - val_loss: 42.4958 - val_MinusLogProbMetric: 42.4958 - lr: 1.6935e-08 - 61s/epoch - 310ms/step
Epoch 117/1000
2023-10-10 05:06:51.137 
Epoch 117/1000 
	 loss: 42.2677, MinusLogProbMetric: 42.2677, val_loss: 42.4955, val_MinusLogProbMetric: 42.4955

Epoch 117: val_loss did not improve from 42.49495
196/196 - 63s - loss: 42.2677 - MinusLogProbMetric: 42.2677 - val_loss: 42.4955 - val_MinusLogProbMetric: 42.4955 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 118/1000
2023-10-10 05:07:54.017 
Epoch 118/1000 
	 loss: 42.2669, MinusLogProbMetric: 42.2669, val_loss: 42.4948, val_MinusLogProbMetric: 42.4948

Epoch 118: val_loss improved from 42.49495 to 42.49480, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2669 - MinusLogProbMetric: 42.2669 - val_loss: 42.4948 - val_MinusLogProbMetric: 42.4948 - lr: 1.6935e-08 - 64s/epoch - 327ms/step
Epoch 119/1000
2023-10-10 05:08:57.298 
Epoch 119/1000 
	 loss: 42.2689, MinusLogProbMetric: 42.2689, val_loss: 42.4951, val_MinusLogProbMetric: 42.4951

Epoch 119: val_loss did not improve from 42.49480
196/196 - 62s - loss: 42.2689 - MinusLogProbMetric: 42.2689 - val_loss: 42.4951 - val_MinusLogProbMetric: 42.4951 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 120/1000
2023-10-10 05:09:59.668 
Epoch 120/1000 
	 loss: 42.2686, MinusLogProbMetric: 42.2686, val_loss: 42.4928, val_MinusLogProbMetric: 42.4928

Epoch 120: val_loss improved from 42.49480 to 42.49281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2686 - MinusLogProbMetric: 42.2686 - val_loss: 42.4928 - val_MinusLogProbMetric: 42.4928 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 121/1000
2023-10-10 05:11:03.139 
Epoch 121/1000 
	 loss: 42.2684, MinusLogProbMetric: 42.2684, val_loss: 42.4958, val_MinusLogProbMetric: 42.4958

Epoch 121: val_loss did not improve from 42.49281
196/196 - 62s - loss: 42.2684 - MinusLogProbMetric: 42.2684 - val_loss: 42.4958 - val_MinusLogProbMetric: 42.4958 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 122/1000
2023-10-10 05:12:04.713 
Epoch 122/1000 
	 loss: 42.2653, MinusLogProbMetric: 42.2653, val_loss: 42.4941, val_MinusLogProbMetric: 42.4941

Epoch 122: val_loss did not improve from 42.49281
196/196 - 62s - loss: 42.2653 - MinusLogProbMetric: 42.2653 - val_loss: 42.4941 - val_MinusLogProbMetric: 42.4941 - lr: 1.6935e-08 - 62s/epoch - 314ms/step
Epoch 123/1000
2023-10-10 05:13:08.510 
Epoch 123/1000 
	 loss: 42.2673, MinusLogProbMetric: 42.2673, val_loss: 42.4954, val_MinusLogProbMetric: 42.4954

Epoch 123: val_loss did not improve from 42.49281
196/196 - 64s - loss: 42.2673 - MinusLogProbMetric: 42.2673 - val_loss: 42.4954 - val_MinusLogProbMetric: 42.4954 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 124/1000
2023-10-10 05:14:12.070 
Epoch 124/1000 
	 loss: 42.2676, MinusLogProbMetric: 42.2676, val_loss: 42.4940, val_MinusLogProbMetric: 42.4940

Epoch 124: val_loss did not improve from 42.49281
196/196 - 64s - loss: 42.2676 - MinusLogProbMetric: 42.2676 - val_loss: 42.4940 - val_MinusLogProbMetric: 42.4940 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 125/1000
2023-10-10 05:15:13.293 
Epoch 125/1000 
	 loss: 42.2674, MinusLogProbMetric: 42.2674, val_loss: 42.4968, val_MinusLogProbMetric: 42.4968

Epoch 125: val_loss did not improve from 42.49281
196/196 - 61s - loss: 42.2674 - MinusLogProbMetric: 42.2674 - val_loss: 42.4968 - val_MinusLogProbMetric: 42.4968 - lr: 1.6935e-08 - 61s/epoch - 312ms/step
Epoch 126/1000
2023-10-10 05:16:15.221 
Epoch 126/1000 
	 loss: 42.2672, MinusLogProbMetric: 42.2672, val_loss: 42.4993, val_MinusLogProbMetric: 42.4993

Epoch 126: val_loss did not improve from 42.49281
196/196 - 62s - loss: 42.2672 - MinusLogProbMetric: 42.2672 - val_loss: 42.4993 - val_MinusLogProbMetric: 42.4993 - lr: 1.6935e-08 - 62s/epoch - 316ms/step
Epoch 127/1000
2023-10-10 05:17:17.908 
Epoch 127/1000 
	 loss: 42.2666, MinusLogProbMetric: 42.2666, val_loss: 42.4940, val_MinusLogProbMetric: 42.4940

Epoch 127: val_loss did not improve from 42.49281
196/196 - 63s - loss: 42.2666 - MinusLogProbMetric: 42.2666 - val_loss: 42.4940 - val_MinusLogProbMetric: 42.4940 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 128/1000
2023-10-10 05:18:20.908 
Epoch 128/1000 
	 loss: 42.2659, MinusLogProbMetric: 42.2659, val_loss: 42.4913, val_MinusLogProbMetric: 42.4913

Epoch 128: val_loss improved from 42.49281 to 42.49128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2659 - MinusLogProbMetric: 42.2659 - val_loss: 42.4913 - val_MinusLogProbMetric: 42.4913 - lr: 1.6935e-08 - 64s/epoch - 327ms/step
Epoch 129/1000
2023-10-10 05:19:23.579 
Epoch 129/1000 
	 loss: 42.2681, MinusLogProbMetric: 42.2681, val_loss: 42.4969, val_MinusLogProbMetric: 42.4969

Epoch 129: val_loss did not improve from 42.49128
196/196 - 62s - loss: 42.2681 - MinusLogProbMetric: 42.2681 - val_loss: 42.4969 - val_MinusLogProbMetric: 42.4969 - lr: 1.6935e-08 - 62s/epoch - 314ms/step
Epoch 130/1000
2023-10-10 05:20:26.819 
Epoch 130/1000 
	 loss: 42.2656, MinusLogProbMetric: 42.2656, val_loss: 42.4956, val_MinusLogProbMetric: 42.4956

Epoch 130: val_loss did not improve from 42.49128
196/196 - 63s - loss: 42.2656 - MinusLogProbMetric: 42.2656 - val_loss: 42.4956 - val_MinusLogProbMetric: 42.4956 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 131/1000
2023-10-10 05:21:30.309 
Epoch 131/1000 
	 loss: 42.2673, MinusLogProbMetric: 42.2673, val_loss: 42.4987, val_MinusLogProbMetric: 42.4987

Epoch 131: val_loss did not improve from 42.49128
196/196 - 63s - loss: 42.2673 - MinusLogProbMetric: 42.2673 - val_loss: 42.4987 - val_MinusLogProbMetric: 42.4987 - lr: 1.6935e-08 - 63s/epoch - 324ms/step
Epoch 132/1000
2023-10-10 05:22:33.697 
Epoch 132/1000 
	 loss: 42.2690, MinusLogProbMetric: 42.2690, val_loss: 42.4937, val_MinusLogProbMetric: 42.4937

Epoch 132: val_loss did not improve from 42.49128
196/196 - 63s - loss: 42.2690 - MinusLogProbMetric: 42.2690 - val_loss: 42.4937 - val_MinusLogProbMetric: 42.4937 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 133/1000
2023-10-10 05:23:35.657 
Epoch 133/1000 
	 loss: 42.2655, MinusLogProbMetric: 42.2655, val_loss: 42.4959, val_MinusLogProbMetric: 42.4959

Epoch 133: val_loss did not improve from 42.49128
196/196 - 62s - loss: 42.2655 - MinusLogProbMetric: 42.2655 - val_loss: 42.4959 - val_MinusLogProbMetric: 42.4959 - lr: 1.6935e-08 - 62s/epoch - 316ms/step
Epoch 134/1000
2023-10-10 05:24:38.173 
Epoch 134/1000 
	 loss: 42.2642, MinusLogProbMetric: 42.2642, val_loss: 42.4976, val_MinusLogProbMetric: 42.4976

Epoch 134: val_loss did not improve from 42.49128
196/196 - 63s - loss: 42.2642 - MinusLogProbMetric: 42.2642 - val_loss: 42.4976 - val_MinusLogProbMetric: 42.4976 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 135/1000
2023-10-10 05:25:40.269 
Epoch 135/1000 
	 loss: 42.2653, MinusLogProbMetric: 42.2653, val_loss: 42.4939, val_MinusLogProbMetric: 42.4939

Epoch 135: val_loss did not improve from 42.49128
196/196 - 62s - loss: 42.2653 - MinusLogProbMetric: 42.2653 - val_loss: 42.4939 - val_MinusLogProbMetric: 42.4939 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 136/1000
2023-10-10 05:26:40.370 
Epoch 136/1000 
	 loss: 42.2637, MinusLogProbMetric: 42.2637, val_loss: 42.4920, val_MinusLogProbMetric: 42.4920

Epoch 136: val_loss did not improve from 42.49128
196/196 - 60s - loss: 42.2637 - MinusLogProbMetric: 42.2637 - val_loss: 42.4920 - val_MinusLogProbMetric: 42.4920 - lr: 1.6935e-08 - 60s/epoch - 307ms/step
Epoch 137/1000
2023-10-10 05:27:42.767 
Epoch 137/1000 
	 loss: 42.2696, MinusLogProbMetric: 42.2696, val_loss: 42.4962, val_MinusLogProbMetric: 42.4962

Epoch 137: val_loss did not improve from 42.49128
196/196 - 62s - loss: 42.2696 - MinusLogProbMetric: 42.2696 - val_loss: 42.4962 - val_MinusLogProbMetric: 42.4962 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 138/1000
2023-10-10 05:28:47.017 
Epoch 138/1000 
	 loss: 42.2690, MinusLogProbMetric: 42.2690, val_loss: 42.4960, val_MinusLogProbMetric: 42.4960

Epoch 138: val_loss did not improve from 42.49128
196/196 - 64s - loss: 42.2690 - MinusLogProbMetric: 42.2690 - val_loss: 42.4960 - val_MinusLogProbMetric: 42.4960 - lr: 1.6935e-08 - 64s/epoch - 328ms/step
Epoch 139/1000
2023-10-10 05:29:49.805 
Epoch 139/1000 
	 loss: 42.2666, MinusLogProbMetric: 42.2666, val_loss: 42.4915, val_MinusLogProbMetric: 42.4915

Epoch 139: val_loss did not improve from 42.49128
196/196 - 63s - loss: 42.2666 - MinusLogProbMetric: 42.2666 - val_loss: 42.4915 - val_MinusLogProbMetric: 42.4915 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 140/1000
2023-10-10 05:30:52.948 
Epoch 140/1000 
	 loss: 42.2672, MinusLogProbMetric: 42.2672, val_loss: 42.4969, val_MinusLogProbMetric: 42.4969

Epoch 140: val_loss did not improve from 42.49128
196/196 - 63s - loss: 42.2672 - MinusLogProbMetric: 42.2672 - val_loss: 42.4969 - val_MinusLogProbMetric: 42.4969 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 141/1000
2023-10-10 05:31:55.671 
Epoch 141/1000 
	 loss: 42.2633, MinusLogProbMetric: 42.2633, val_loss: 42.4941, val_MinusLogProbMetric: 42.4941

Epoch 141: val_loss did not improve from 42.49128
196/196 - 63s - loss: 42.2633 - MinusLogProbMetric: 42.2633 - val_loss: 42.4941 - val_MinusLogProbMetric: 42.4941 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 142/1000
2023-10-10 05:32:59.131 
Epoch 142/1000 
	 loss: 42.2628, MinusLogProbMetric: 42.2628, val_loss: 42.4933, val_MinusLogProbMetric: 42.4933

Epoch 142: val_loss did not improve from 42.49128
196/196 - 63s - loss: 42.2628 - MinusLogProbMetric: 42.2628 - val_loss: 42.4933 - val_MinusLogProbMetric: 42.4933 - lr: 1.6935e-08 - 63s/epoch - 324ms/step
Epoch 143/1000
2023-10-10 05:34:01.398 
Epoch 143/1000 
	 loss: 42.2644, MinusLogProbMetric: 42.2644, val_loss: 42.4906, val_MinusLogProbMetric: 42.4906

Epoch 143: val_loss improved from 42.49128 to 42.49056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.2644 - MinusLogProbMetric: 42.2644 - val_loss: 42.4906 - val_MinusLogProbMetric: 42.4906 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 144/1000
2023-10-10 05:35:05.230 
Epoch 144/1000 
	 loss: 42.2600, MinusLogProbMetric: 42.2600, val_loss: 42.4929, val_MinusLogProbMetric: 42.4929

Epoch 144: val_loss did not improve from 42.49056
196/196 - 63s - loss: 42.2600 - MinusLogProbMetric: 42.2600 - val_loss: 42.4929 - val_MinusLogProbMetric: 42.4929 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 145/1000
2023-10-10 05:36:07.352 
Epoch 145/1000 
	 loss: 42.2618, MinusLogProbMetric: 42.2618, val_loss: 42.4911, val_MinusLogProbMetric: 42.4911

Epoch 145: val_loss did not improve from 42.49056
196/196 - 62s - loss: 42.2618 - MinusLogProbMetric: 42.2618 - val_loss: 42.4911 - val_MinusLogProbMetric: 42.4911 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 146/1000
2023-10-10 05:37:09.847 
Epoch 146/1000 
	 loss: 42.2628, MinusLogProbMetric: 42.2628, val_loss: 42.4911, val_MinusLogProbMetric: 42.4911

Epoch 146: val_loss did not improve from 42.49056
196/196 - 62s - loss: 42.2628 - MinusLogProbMetric: 42.2628 - val_loss: 42.4911 - val_MinusLogProbMetric: 42.4911 - lr: 1.6935e-08 - 62s/epoch - 319ms/step
Epoch 147/1000
2023-10-10 05:38:13.364 
Epoch 147/1000 
	 loss: 42.2622, MinusLogProbMetric: 42.2622, val_loss: 42.4913, val_MinusLogProbMetric: 42.4913

Epoch 147: val_loss did not improve from 42.49056
196/196 - 64s - loss: 42.2622 - MinusLogProbMetric: 42.2622 - val_loss: 42.4913 - val_MinusLogProbMetric: 42.4913 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 148/1000
2023-10-10 05:39:16.557 
Epoch 148/1000 
	 loss: 42.2647, MinusLogProbMetric: 42.2647, val_loss: 42.4937, val_MinusLogProbMetric: 42.4937

Epoch 148: val_loss did not improve from 42.49056
196/196 - 63s - loss: 42.2647 - MinusLogProbMetric: 42.2647 - val_loss: 42.4937 - val_MinusLogProbMetric: 42.4937 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 149/1000
2023-10-10 05:40:18.970 
Epoch 149/1000 
	 loss: 42.2659, MinusLogProbMetric: 42.2659, val_loss: 42.4898, val_MinusLogProbMetric: 42.4898

Epoch 149: val_loss improved from 42.49056 to 42.48983, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.2659 - MinusLogProbMetric: 42.2659 - val_loss: 42.4898 - val_MinusLogProbMetric: 42.4898 - lr: 1.6935e-08 - 63s/epoch - 324ms/step
Epoch 150/1000
2023-10-10 05:41:21.923 
Epoch 150/1000 
	 loss: 42.2637, MinusLogProbMetric: 42.2637, val_loss: 42.4912, val_MinusLogProbMetric: 42.4912

Epoch 150: val_loss did not improve from 42.48983
196/196 - 62s - loss: 42.2637 - MinusLogProbMetric: 42.2637 - val_loss: 42.4912 - val_MinusLogProbMetric: 42.4912 - lr: 1.6935e-08 - 62s/epoch - 316ms/step
Epoch 151/1000
2023-10-10 05:42:24.730 
Epoch 151/1000 
	 loss: 42.2626, MinusLogProbMetric: 42.2626, val_loss: 42.4916, val_MinusLogProbMetric: 42.4916

Epoch 151: val_loss did not improve from 42.48983
196/196 - 63s - loss: 42.2626 - MinusLogProbMetric: 42.2626 - val_loss: 42.4916 - val_MinusLogProbMetric: 42.4916 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 152/1000
2023-10-10 05:43:27.551 
Epoch 152/1000 
	 loss: 42.2613, MinusLogProbMetric: 42.2613, val_loss: 42.4876, val_MinusLogProbMetric: 42.4876

Epoch 152: val_loss improved from 42.48983 to 42.48757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2613 - MinusLogProbMetric: 42.2613 - val_loss: 42.4876 - val_MinusLogProbMetric: 42.4876 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 153/1000
2023-10-10 05:44:30.714 
Epoch 153/1000 
	 loss: 42.2596, MinusLogProbMetric: 42.2596, val_loss: 42.4927, val_MinusLogProbMetric: 42.4927

Epoch 153: val_loss did not improve from 42.48757
196/196 - 62s - loss: 42.2596 - MinusLogProbMetric: 42.2596 - val_loss: 42.4927 - val_MinusLogProbMetric: 42.4927 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 154/1000
2023-10-10 05:45:32.028 
Epoch 154/1000 
	 loss: 42.2604, MinusLogProbMetric: 42.2604, val_loss: 42.4869, val_MinusLogProbMetric: 42.4869

Epoch 154: val_loss improved from 42.48757 to 42.48691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 62s - loss: 42.2604 - MinusLogProbMetric: 42.2604 - val_loss: 42.4869 - val_MinusLogProbMetric: 42.4869 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 155/1000
2023-10-10 05:46:36.268 
Epoch 155/1000 
	 loss: 42.2609, MinusLogProbMetric: 42.2609, val_loss: 42.4908, val_MinusLogProbMetric: 42.4908

Epoch 155: val_loss did not improve from 42.48691
196/196 - 63s - loss: 42.2609 - MinusLogProbMetric: 42.2609 - val_loss: 42.4908 - val_MinusLogProbMetric: 42.4908 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 156/1000
2023-10-10 05:47:39.203 
Epoch 156/1000 
	 loss: 42.2616, MinusLogProbMetric: 42.2616, val_loss: 42.4891, val_MinusLogProbMetric: 42.4891

Epoch 156: val_loss did not improve from 42.48691
196/196 - 63s - loss: 42.2616 - MinusLogProbMetric: 42.2616 - val_loss: 42.4891 - val_MinusLogProbMetric: 42.4891 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 157/1000
2023-10-10 05:48:41.398 
Epoch 157/1000 
	 loss: 42.2591, MinusLogProbMetric: 42.2591, val_loss: 42.4866, val_MinusLogProbMetric: 42.4866

Epoch 157: val_loss improved from 42.48691 to 42.48659, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.2591 - MinusLogProbMetric: 42.2591 - val_loss: 42.4866 - val_MinusLogProbMetric: 42.4866 - lr: 1.6935e-08 - 63s/epoch - 324ms/step
Epoch 158/1000
2023-10-10 05:49:45.452 
Epoch 158/1000 
	 loss: 42.2590, MinusLogProbMetric: 42.2590, val_loss: 42.4906, val_MinusLogProbMetric: 42.4906

Epoch 158: val_loss did not improve from 42.48659
196/196 - 63s - loss: 42.2590 - MinusLogProbMetric: 42.2590 - val_loss: 42.4906 - val_MinusLogProbMetric: 42.4906 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 159/1000
2023-10-10 05:50:48.732 
Epoch 159/1000 
	 loss: 42.2603, MinusLogProbMetric: 42.2603, val_loss: 42.4907, val_MinusLogProbMetric: 42.4907

Epoch 159: val_loss did not improve from 42.48659
196/196 - 63s - loss: 42.2603 - MinusLogProbMetric: 42.2603 - val_loss: 42.4907 - val_MinusLogProbMetric: 42.4907 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 160/1000
2023-10-10 05:51:52.093 
Epoch 160/1000 
	 loss: 42.2590, MinusLogProbMetric: 42.2590, val_loss: 42.4887, val_MinusLogProbMetric: 42.4887

Epoch 160: val_loss did not improve from 42.48659
196/196 - 63s - loss: 42.2590 - MinusLogProbMetric: 42.2590 - val_loss: 42.4887 - val_MinusLogProbMetric: 42.4887 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 161/1000
2023-10-10 05:52:56.517 
Epoch 161/1000 
	 loss: 42.2556, MinusLogProbMetric: 42.2556, val_loss: 42.4878, val_MinusLogProbMetric: 42.4878

Epoch 161: val_loss did not improve from 42.48659
196/196 - 64s - loss: 42.2556 - MinusLogProbMetric: 42.2556 - val_loss: 42.4878 - val_MinusLogProbMetric: 42.4878 - lr: 1.6935e-08 - 64s/epoch - 329ms/step
Epoch 162/1000
2023-10-10 05:54:00.886 
Epoch 162/1000 
	 loss: 42.2565, MinusLogProbMetric: 42.2565, val_loss: 42.4862, val_MinusLogProbMetric: 42.4862

Epoch 162: val_loss improved from 42.48659 to 42.48624, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 42.2565 - MinusLogProbMetric: 42.2565 - val_loss: 42.4862 - val_MinusLogProbMetric: 42.4862 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 163/1000
2023-10-10 05:55:04.926 
Epoch 163/1000 
	 loss: 42.2579, MinusLogProbMetric: 42.2579, val_loss: 42.4862, val_MinusLogProbMetric: 42.4862

Epoch 163: val_loss improved from 42.48624 to 42.48622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.2579 - MinusLogProbMetric: 42.2579 - val_loss: 42.4862 - val_MinusLogProbMetric: 42.4862 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 164/1000
2023-10-10 05:56:08.312 
Epoch 164/1000 
	 loss: 42.2567, MinusLogProbMetric: 42.2567, val_loss: 42.4886, val_MinusLogProbMetric: 42.4886

Epoch 164: val_loss did not improve from 42.48622
196/196 - 63s - loss: 42.2567 - MinusLogProbMetric: 42.2567 - val_loss: 42.4886 - val_MinusLogProbMetric: 42.4886 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 165/1000
2023-10-10 05:57:12.270 
Epoch 165/1000 
	 loss: 42.2556, MinusLogProbMetric: 42.2556, val_loss: 42.4877, val_MinusLogProbMetric: 42.4877

Epoch 165: val_loss did not improve from 42.48622
196/196 - 64s - loss: 42.2556 - MinusLogProbMetric: 42.2556 - val_loss: 42.4877 - val_MinusLogProbMetric: 42.4877 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 166/1000
2023-10-10 05:58:14.493 
Epoch 166/1000 
	 loss: 42.2544, MinusLogProbMetric: 42.2544, val_loss: 42.4860, val_MinusLogProbMetric: 42.4860

Epoch 166: val_loss improved from 42.48622 to 42.48597, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.2544 - MinusLogProbMetric: 42.2544 - val_loss: 42.4860 - val_MinusLogProbMetric: 42.4860 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 167/1000
2023-10-10 05:59:19.281 
Epoch 167/1000 
	 loss: 42.2578, MinusLogProbMetric: 42.2578, val_loss: 42.4891, val_MinusLogProbMetric: 42.4891

Epoch 167: val_loss did not improve from 42.48597
196/196 - 64s - loss: 42.2578 - MinusLogProbMetric: 42.2578 - val_loss: 42.4891 - val_MinusLogProbMetric: 42.4891 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 168/1000
2023-10-10 06:00:22.483 
Epoch 168/1000 
	 loss: 42.2600, MinusLogProbMetric: 42.2600, val_loss: 42.4886, val_MinusLogProbMetric: 42.4886

Epoch 168: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2600 - MinusLogProbMetric: 42.2600 - val_loss: 42.4886 - val_MinusLogProbMetric: 42.4886 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 169/1000
2023-10-10 06:01:25.693 
Epoch 169/1000 
	 loss: 42.2582, MinusLogProbMetric: 42.2582, val_loss: 42.4902, val_MinusLogProbMetric: 42.4902

Epoch 169: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2582 - MinusLogProbMetric: 42.2582 - val_loss: 42.4902 - val_MinusLogProbMetric: 42.4902 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 170/1000
2023-10-10 06:02:28.701 
Epoch 170/1000 
	 loss: 42.2668, MinusLogProbMetric: 42.2668, val_loss: 42.4956, val_MinusLogProbMetric: 42.4956

Epoch 170: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2668 - MinusLogProbMetric: 42.2668 - val_loss: 42.4956 - val_MinusLogProbMetric: 42.4956 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 171/1000
2023-10-10 06:03:31.734 
Epoch 171/1000 
	 loss: 42.2670, MinusLogProbMetric: 42.2670, val_loss: 42.4919, val_MinusLogProbMetric: 42.4919

Epoch 171: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2670 - MinusLogProbMetric: 42.2670 - val_loss: 42.4919 - val_MinusLogProbMetric: 42.4919 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 172/1000
2023-10-10 06:04:34.251 
Epoch 172/1000 
	 loss: 42.2630, MinusLogProbMetric: 42.2630, val_loss: 42.4909, val_MinusLogProbMetric: 42.4909

Epoch 172: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2630 - MinusLogProbMetric: 42.2630 - val_loss: 42.4909 - val_MinusLogProbMetric: 42.4909 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 173/1000
2023-10-10 06:05:37.138 
Epoch 173/1000 
	 loss: 42.2650, MinusLogProbMetric: 42.2650, val_loss: 42.4973, val_MinusLogProbMetric: 42.4973

Epoch 173: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2650 - MinusLogProbMetric: 42.2650 - val_loss: 42.4973 - val_MinusLogProbMetric: 42.4973 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 174/1000
2023-10-10 06:06:40.406 
Epoch 174/1000 
	 loss: 42.2702, MinusLogProbMetric: 42.2702, val_loss: 42.4941, val_MinusLogProbMetric: 42.4941

Epoch 174: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2702 - MinusLogProbMetric: 42.2702 - val_loss: 42.4941 - val_MinusLogProbMetric: 42.4941 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 175/1000
2023-10-10 06:07:43.071 
Epoch 175/1000 
	 loss: 42.2634, MinusLogProbMetric: 42.2634, val_loss: 42.4947, val_MinusLogProbMetric: 42.4947

Epoch 175: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2634 - MinusLogProbMetric: 42.2634 - val_loss: 42.4947 - val_MinusLogProbMetric: 42.4947 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 176/1000
2023-10-10 06:08:45.478 
Epoch 176/1000 
	 loss: 42.2626, MinusLogProbMetric: 42.2626, val_loss: 42.4924, val_MinusLogProbMetric: 42.4924

Epoch 176: val_loss did not improve from 42.48597
196/196 - 62s - loss: 42.2626 - MinusLogProbMetric: 42.2626 - val_loss: 42.4924 - val_MinusLogProbMetric: 42.4924 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 177/1000
2023-10-10 06:09:48.516 
Epoch 177/1000 
	 loss: 42.2618, MinusLogProbMetric: 42.2618, val_loss: 42.4920, val_MinusLogProbMetric: 42.4920

Epoch 177: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2618 - MinusLogProbMetric: 42.2618 - val_loss: 42.4920 - val_MinusLogProbMetric: 42.4920 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 178/1000
2023-10-10 06:10:51.691 
Epoch 178/1000 
	 loss: 42.2599, MinusLogProbMetric: 42.2599, val_loss: 42.5026, val_MinusLogProbMetric: 42.5026

Epoch 178: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2599 - MinusLogProbMetric: 42.2599 - val_loss: 42.5026 - val_MinusLogProbMetric: 42.5026 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 179/1000
2023-10-10 06:11:53.860 
Epoch 179/1000 
	 loss: 42.2713, MinusLogProbMetric: 42.2713, val_loss: 42.4991, val_MinusLogProbMetric: 42.4991

Epoch 179: val_loss did not improve from 42.48597
196/196 - 62s - loss: 42.2713 - MinusLogProbMetric: 42.2713 - val_loss: 42.4991 - val_MinusLogProbMetric: 42.4991 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 180/1000
2023-10-10 06:12:57.067 
Epoch 180/1000 
	 loss: 42.2690, MinusLogProbMetric: 42.2690, val_loss: 42.4988, val_MinusLogProbMetric: 42.4988

Epoch 180: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2690 - MinusLogProbMetric: 42.2690 - val_loss: 42.4988 - val_MinusLogProbMetric: 42.4988 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 181/1000
2023-10-10 06:14:00.087 
Epoch 181/1000 
	 loss: 42.2688, MinusLogProbMetric: 42.2688, val_loss: 42.4974, val_MinusLogProbMetric: 42.4974

Epoch 181: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2688 - MinusLogProbMetric: 42.2688 - val_loss: 42.4974 - val_MinusLogProbMetric: 42.4974 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 182/1000
2023-10-10 06:15:03.635 
Epoch 182/1000 
	 loss: 42.2671, MinusLogProbMetric: 42.2671, val_loss: 42.4948, val_MinusLogProbMetric: 42.4948

Epoch 182: val_loss did not improve from 42.48597
196/196 - 64s - loss: 42.2671 - MinusLogProbMetric: 42.2671 - val_loss: 42.4948 - val_MinusLogProbMetric: 42.4948 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 183/1000
2023-10-10 06:16:06.156 
Epoch 183/1000 
	 loss: 42.2660, MinusLogProbMetric: 42.2660, val_loss: 42.4929, val_MinusLogProbMetric: 42.4929

Epoch 183: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2660 - MinusLogProbMetric: 42.2660 - val_loss: 42.4929 - val_MinusLogProbMetric: 42.4929 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 184/1000
2023-10-10 06:17:09.446 
Epoch 184/1000 
	 loss: 42.2662, MinusLogProbMetric: 42.2662, val_loss: 42.4951, val_MinusLogProbMetric: 42.4951

Epoch 184: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2662 - MinusLogProbMetric: 42.2662 - val_loss: 42.4951 - val_MinusLogProbMetric: 42.4951 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 185/1000
2023-10-10 06:18:11.239 
Epoch 185/1000 
	 loss: 42.2648, MinusLogProbMetric: 42.2648, val_loss: 42.4927, val_MinusLogProbMetric: 42.4927

Epoch 185: val_loss did not improve from 42.48597
196/196 - 62s - loss: 42.2648 - MinusLogProbMetric: 42.2648 - val_loss: 42.4927 - val_MinusLogProbMetric: 42.4927 - lr: 1.6935e-08 - 62s/epoch - 315ms/step
Epoch 186/1000
2023-10-10 06:19:14.094 
Epoch 186/1000 
	 loss: 42.2635, MinusLogProbMetric: 42.2635, val_loss: 42.4907, val_MinusLogProbMetric: 42.4907

Epoch 186: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2635 - MinusLogProbMetric: 42.2635 - val_loss: 42.4907 - val_MinusLogProbMetric: 42.4907 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 187/1000
2023-10-10 06:20:16.832 
Epoch 187/1000 
	 loss: 42.2629, MinusLogProbMetric: 42.2629, val_loss: 42.4871, val_MinusLogProbMetric: 42.4871

Epoch 187: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2629 - MinusLogProbMetric: 42.2629 - val_loss: 42.4871 - val_MinusLogProbMetric: 42.4871 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 188/1000
2023-10-10 06:21:19.193 
Epoch 188/1000 
	 loss: 42.2623, MinusLogProbMetric: 42.2623, val_loss: 42.4919, val_MinusLogProbMetric: 42.4919

Epoch 188: val_loss did not improve from 42.48597
196/196 - 62s - loss: 42.2623 - MinusLogProbMetric: 42.2623 - val_loss: 42.4919 - val_MinusLogProbMetric: 42.4919 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 189/1000
2023-10-10 06:22:21.191 
Epoch 189/1000 
	 loss: 42.2649, MinusLogProbMetric: 42.2649, val_loss: 42.4900, val_MinusLogProbMetric: 42.4900

Epoch 189: val_loss did not improve from 42.48597
196/196 - 62s - loss: 42.2649 - MinusLogProbMetric: 42.2649 - val_loss: 42.4900 - val_MinusLogProbMetric: 42.4900 - lr: 1.6935e-08 - 62s/epoch - 316ms/step
Epoch 190/1000
2023-10-10 06:23:23.955 
Epoch 190/1000 
	 loss: 42.2615, MinusLogProbMetric: 42.2615, val_loss: 42.4914, val_MinusLogProbMetric: 42.4914

Epoch 190: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2615 - MinusLogProbMetric: 42.2615 - val_loss: 42.4914 - val_MinusLogProbMetric: 42.4914 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 191/1000
2023-10-10 06:24:27.045 
Epoch 191/1000 
	 loss: 42.2578, MinusLogProbMetric: 42.2578, val_loss: 42.4875, val_MinusLogProbMetric: 42.4875

Epoch 191: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2578 - MinusLogProbMetric: 42.2578 - val_loss: 42.4875 - val_MinusLogProbMetric: 42.4875 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 192/1000
2023-10-10 06:25:30.391 
Epoch 192/1000 
	 loss: 42.2573, MinusLogProbMetric: 42.2573, val_loss: 42.4880, val_MinusLogProbMetric: 42.4880

Epoch 192: val_loss did not improve from 42.48597
196/196 - 63s - loss: 42.2573 - MinusLogProbMetric: 42.2573 - val_loss: 42.4880 - val_MinusLogProbMetric: 42.4880 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 193/1000
2023-10-10 06:26:32.808 
Epoch 193/1000 
	 loss: 42.2597, MinusLogProbMetric: 42.2597, val_loss: 42.4897, val_MinusLogProbMetric: 42.4897

Epoch 193: val_loss did not improve from 42.48597
196/196 - 62s - loss: 42.2597 - MinusLogProbMetric: 42.2597 - val_loss: 42.4897 - val_MinusLogProbMetric: 42.4897 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 194/1000
2023-10-10 06:27:36.163 
Epoch 194/1000 
	 loss: 42.2581, MinusLogProbMetric: 42.2581, val_loss: 42.4852, val_MinusLogProbMetric: 42.4852

Epoch 194: val_loss improved from 42.48597 to 42.48515, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2581 - MinusLogProbMetric: 42.2581 - val_loss: 42.4852 - val_MinusLogProbMetric: 42.4852 - lr: 1.6935e-08 - 64s/epoch - 329ms/step
Epoch 195/1000
2023-10-10 06:28:39.489 
Epoch 195/1000 
	 loss: 42.2571, MinusLogProbMetric: 42.2571, val_loss: 42.4899, val_MinusLogProbMetric: 42.4899

Epoch 195: val_loss did not improve from 42.48515
196/196 - 62s - loss: 42.2571 - MinusLogProbMetric: 42.2571 - val_loss: 42.4899 - val_MinusLogProbMetric: 42.4899 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 196/1000
2023-10-10 06:29:43.991 
Epoch 196/1000 
	 loss: 42.2562, MinusLogProbMetric: 42.2562, val_loss: 42.4860, val_MinusLogProbMetric: 42.4860

Epoch 196: val_loss did not improve from 42.48515
196/196 - 64s - loss: 42.2562 - MinusLogProbMetric: 42.2562 - val_loss: 42.4860 - val_MinusLogProbMetric: 42.4860 - lr: 1.6935e-08 - 64s/epoch - 329ms/step
Epoch 197/1000
2023-10-10 06:30:46.504 
Epoch 197/1000 
	 loss: 42.2567, MinusLogProbMetric: 42.2567, val_loss: 42.4865, val_MinusLogProbMetric: 42.4865

Epoch 197: val_loss did not improve from 42.48515
196/196 - 63s - loss: 42.2567 - MinusLogProbMetric: 42.2567 - val_loss: 42.4865 - val_MinusLogProbMetric: 42.4865 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 198/1000
2023-10-10 06:31:49.732 
Epoch 198/1000 
	 loss: 42.2558, MinusLogProbMetric: 42.2558, val_loss: 42.4878, val_MinusLogProbMetric: 42.4878

Epoch 198: val_loss did not improve from 42.48515
196/196 - 63s - loss: 42.2558 - MinusLogProbMetric: 42.2558 - val_loss: 42.4878 - val_MinusLogProbMetric: 42.4878 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 199/1000
2023-10-10 06:32:52.766 
Epoch 199/1000 
	 loss: 42.2537, MinusLogProbMetric: 42.2537, val_loss: 42.4849, val_MinusLogProbMetric: 42.4849

Epoch 199: val_loss improved from 42.48515 to 42.48491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2537 - MinusLogProbMetric: 42.2537 - val_loss: 42.4849 - val_MinusLogProbMetric: 42.4849 - lr: 1.6935e-08 - 64s/epoch - 327ms/step
Epoch 200/1000
2023-10-10 06:33:56.136 
Epoch 200/1000 
	 loss: 42.2544, MinusLogProbMetric: 42.2544, val_loss: 42.4873, val_MinusLogProbMetric: 42.4873

Epoch 200: val_loss did not improve from 42.48491
196/196 - 62s - loss: 42.2544 - MinusLogProbMetric: 42.2544 - val_loss: 42.4873 - val_MinusLogProbMetric: 42.4873 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 201/1000
2023-10-10 06:34:58.666 
Epoch 201/1000 
	 loss: 42.2558, MinusLogProbMetric: 42.2558, val_loss: 42.4841, val_MinusLogProbMetric: 42.4841

Epoch 201: val_loss improved from 42.48491 to 42.48407, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2558 - MinusLogProbMetric: 42.2558 - val_loss: 42.4841 - val_MinusLogProbMetric: 42.4841 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 202/1000
2023-10-10 06:36:02.012 
Epoch 202/1000 
	 loss: 42.2573, MinusLogProbMetric: 42.2573, val_loss: 42.4816, val_MinusLogProbMetric: 42.4816

Epoch 202: val_loss improved from 42.48407 to 42.48162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2573 - MinusLogProbMetric: 42.2573 - val_loss: 42.4816 - val_MinusLogProbMetric: 42.4816 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 203/1000
2023-10-10 06:37:06.145 
Epoch 203/1000 
	 loss: 42.2558, MinusLogProbMetric: 42.2558, val_loss: 42.4839, val_MinusLogProbMetric: 42.4839

Epoch 203: val_loss did not improve from 42.48162
196/196 - 63s - loss: 42.2558 - MinusLogProbMetric: 42.2558 - val_loss: 42.4839 - val_MinusLogProbMetric: 42.4839 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 204/1000
2023-10-10 06:38:09.475 
Epoch 204/1000 
	 loss: 42.2555, MinusLogProbMetric: 42.2555, val_loss: 42.4853, val_MinusLogProbMetric: 42.4853

Epoch 204: val_loss did not improve from 42.48162
196/196 - 63s - loss: 42.2555 - MinusLogProbMetric: 42.2555 - val_loss: 42.4853 - val_MinusLogProbMetric: 42.4853 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 205/1000
2023-10-10 06:39:12.225 
Epoch 205/1000 
	 loss: 42.2526, MinusLogProbMetric: 42.2526, val_loss: 42.4842, val_MinusLogProbMetric: 42.4842

Epoch 205: val_loss did not improve from 42.48162
196/196 - 63s - loss: 42.2526 - MinusLogProbMetric: 42.2526 - val_loss: 42.4842 - val_MinusLogProbMetric: 42.4842 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 206/1000
2023-10-10 06:40:14.845 
Epoch 206/1000 
	 loss: 42.2526, MinusLogProbMetric: 42.2526, val_loss: 42.4836, val_MinusLogProbMetric: 42.4836

Epoch 206: val_loss did not improve from 42.48162
196/196 - 63s - loss: 42.2526 - MinusLogProbMetric: 42.2526 - val_loss: 42.4836 - val_MinusLogProbMetric: 42.4836 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 207/1000
2023-10-10 06:41:17.069 
Epoch 207/1000 
	 loss: 42.2524, MinusLogProbMetric: 42.2524, val_loss: 42.4825, val_MinusLogProbMetric: 42.4825

Epoch 207: val_loss did not improve from 42.48162
196/196 - 62s - loss: 42.2524 - MinusLogProbMetric: 42.2524 - val_loss: 42.4825 - val_MinusLogProbMetric: 42.4825 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 208/1000
2023-10-10 06:42:19.490 
Epoch 208/1000 
	 loss: 42.2544, MinusLogProbMetric: 42.2544, val_loss: 42.4896, val_MinusLogProbMetric: 42.4896

Epoch 208: val_loss did not improve from 42.48162
196/196 - 62s - loss: 42.2544 - MinusLogProbMetric: 42.2544 - val_loss: 42.4896 - val_MinusLogProbMetric: 42.4896 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 209/1000
2023-10-10 06:43:22.708 
Epoch 209/1000 
	 loss: 42.2651, MinusLogProbMetric: 42.2651, val_loss: 42.4957, val_MinusLogProbMetric: 42.4957

Epoch 209: val_loss did not improve from 42.48162
196/196 - 63s - loss: 42.2651 - MinusLogProbMetric: 42.2651 - val_loss: 42.4957 - val_MinusLogProbMetric: 42.4957 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 210/1000
2023-10-10 06:44:25.275 
Epoch 210/1000 
	 loss: 42.2605, MinusLogProbMetric: 42.2605, val_loss: 42.4901, val_MinusLogProbMetric: 42.4901

Epoch 210: val_loss did not improve from 42.48162
196/196 - 63s - loss: 42.2605 - MinusLogProbMetric: 42.2605 - val_loss: 42.4901 - val_MinusLogProbMetric: 42.4901 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 211/1000
2023-10-10 06:45:28.790 
Epoch 211/1000 
	 loss: 42.2588, MinusLogProbMetric: 42.2588, val_loss: 42.4855, val_MinusLogProbMetric: 42.4855

Epoch 211: val_loss did not improve from 42.48162
196/196 - 64s - loss: 42.2588 - MinusLogProbMetric: 42.2588 - val_loss: 42.4855 - val_MinusLogProbMetric: 42.4855 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 212/1000
2023-10-10 06:46:31.941 
Epoch 212/1000 
	 loss: 42.2573, MinusLogProbMetric: 42.2573, val_loss: 42.4866, val_MinusLogProbMetric: 42.4866

Epoch 212: val_loss did not improve from 42.48162
196/196 - 63s - loss: 42.2573 - MinusLogProbMetric: 42.2573 - val_loss: 42.4866 - val_MinusLogProbMetric: 42.4866 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 213/1000
2023-10-10 06:47:33.545 
Epoch 213/1000 
	 loss: 42.2562, MinusLogProbMetric: 42.2562, val_loss: 42.4832, val_MinusLogProbMetric: 42.4832

Epoch 213: val_loss did not improve from 42.48162
196/196 - 62s - loss: 42.2562 - MinusLogProbMetric: 42.2562 - val_loss: 42.4832 - val_MinusLogProbMetric: 42.4832 - lr: 1.6935e-08 - 62s/epoch - 314ms/step
Epoch 214/1000
2023-10-10 06:48:36.606 
Epoch 214/1000 
	 loss: 42.2529, MinusLogProbMetric: 42.2529, val_loss: 42.4810, val_MinusLogProbMetric: 42.4810

Epoch 214: val_loss improved from 42.48162 to 42.48097, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2529 - MinusLogProbMetric: 42.2529 - val_loss: 42.4810 - val_MinusLogProbMetric: 42.4810 - lr: 1.6935e-08 - 64s/epoch - 328ms/step
Epoch 215/1000
2023-10-10 06:49:42.040 
Epoch 215/1000 
	 loss: 42.2531, MinusLogProbMetric: 42.2531, val_loss: 42.4843, val_MinusLogProbMetric: 42.4843

Epoch 215: val_loss did not improve from 42.48097
196/196 - 64s - loss: 42.2531 - MinusLogProbMetric: 42.2531 - val_loss: 42.4843 - val_MinusLogProbMetric: 42.4843 - lr: 1.6935e-08 - 64s/epoch - 328ms/step
Epoch 216/1000
2023-10-10 06:50:44.528 
Epoch 216/1000 
	 loss: 42.2519, MinusLogProbMetric: 42.2519, val_loss: 42.4822, val_MinusLogProbMetric: 42.4822

Epoch 216: val_loss did not improve from 42.48097
196/196 - 62s - loss: 42.2519 - MinusLogProbMetric: 42.2519 - val_loss: 42.4822 - val_MinusLogProbMetric: 42.4822 - lr: 1.6935e-08 - 62s/epoch - 319ms/step
Epoch 217/1000
2023-10-10 06:51:47.481 
Epoch 217/1000 
	 loss: 42.2500, MinusLogProbMetric: 42.2500, val_loss: 42.4814, val_MinusLogProbMetric: 42.4814

Epoch 217: val_loss did not improve from 42.48097
196/196 - 63s - loss: 42.2500 - MinusLogProbMetric: 42.2500 - val_loss: 42.4814 - val_MinusLogProbMetric: 42.4814 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 218/1000
2023-10-10 06:52:50.783 
Epoch 218/1000 
	 loss: 42.2509, MinusLogProbMetric: 42.2509, val_loss: 42.4825, val_MinusLogProbMetric: 42.4825

Epoch 218: val_loss did not improve from 42.48097
196/196 - 63s - loss: 42.2509 - MinusLogProbMetric: 42.2509 - val_loss: 42.4825 - val_MinusLogProbMetric: 42.4825 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 219/1000
2023-10-10 06:53:55.233 
Epoch 219/1000 
	 loss: 42.2518, MinusLogProbMetric: 42.2518, val_loss: 42.4780, val_MinusLogProbMetric: 42.4780

Epoch 219: val_loss improved from 42.48097 to 42.47801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 42.2518 - MinusLogProbMetric: 42.2518 - val_loss: 42.4780 - val_MinusLogProbMetric: 42.4780 - lr: 1.6935e-08 - 66s/epoch - 335ms/step
Epoch 220/1000
2023-10-10 06:55:00.234 
Epoch 220/1000 
	 loss: 42.2527, MinusLogProbMetric: 42.2527, val_loss: 42.4806, val_MinusLogProbMetric: 42.4806

Epoch 220: val_loss did not improve from 42.47801
196/196 - 64s - loss: 42.2527 - MinusLogProbMetric: 42.2527 - val_loss: 42.4806 - val_MinusLogProbMetric: 42.4806 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 221/1000
2023-10-10 06:56:03.348 
Epoch 221/1000 
	 loss: 42.2515, MinusLogProbMetric: 42.2515, val_loss: 42.4796, val_MinusLogProbMetric: 42.4796

Epoch 221: val_loss did not improve from 42.47801
196/196 - 63s - loss: 42.2515 - MinusLogProbMetric: 42.2515 - val_loss: 42.4796 - val_MinusLogProbMetric: 42.4796 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 222/1000
2023-10-10 06:57:04.572 
Epoch 222/1000 
	 loss: 42.2513, MinusLogProbMetric: 42.2513, val_loss: 42.4800, val_MinusLogProbMetric: 42.4800

Epoch 222: val_loss did not improve from 42.47801
196/196 - 61s - loss: 42.2513 - MinusLogProbMetric: 42.2513 - val_loss: 42.4800 - val_MinusLogProbMetric: 42.4800 - lr: 1.6935e-08 - 61s/epoch - 312ms/step
Epoch 223/1000
2023-10-10 06:58:05.887 
Epoch 223/1000 
	 loss: 42.2495, MinusLogProbMetric: 42.2495, val_loss: 42.4782, val_MinusLogProbMetric: 42.4782

Epoch 223: val_loss did not improve from 42.47801
196/196 - 61s - loss: 42.2495 - MinusLogProbMetric: 42.2495 - val_loss: 42.4782 - val_MinusLogProbMetric: 42.4782 - lr: 1.6935e-08 - 61s/epoch - 313ms/step
Epoch 224/1000
2023-10-10 06:59:08.118 
Epoch 224/1000 
	 loss: 42.2478, MinusLogProbMetric: 42.2478, val_loss: 42.4762, val_MinusLogProbMetric: 42.4762

Epoch 224: val_loss improved from 42.47801 to 42.47618, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.2478 - MinusLogProbMetric: 42.2478 - val_loss: 42.4762 - val_MinusLogProbMetric: 42.4762 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 225/1000
2023-10-10 07:00:10.938 
Epoch 225/1000 
	 loss: 42.2471, MinusLogProbMetric: 42.2471, val_loss: 42.4760, val_MinusLogProbMetric: 42.4760

Epoch 225: val_loss improved from 42.47618 to 42.47605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.2471 - MinusLogProbMetric: 42.2471 - val_loss: 42.4760 - val_MinusLogProbMetric: 42.4760 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 226/1000
2023-10-10 07:01:17.018 
Epoch 226/1000 
	 loss: 42.2474, MinusLogProbMetric: 42.2474, val_loss: 42.4756, val_MinusLogProbMetric: 42.4756

Epoch 226: val_loss improved from 42.47605 to 42.47562, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 42.2474 - MinusLogProbMetric: 42.2474 - val_loss: 42.4756 - val_MinusLogProbMetric: 42.4756 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 227/1000
2023-10-10 07:02:20.736 
Epoch 227/1000 
	 loss: 42.2468, MinusLogProbMetric: 42.2468, val_loss: 42.4763, val_MinusLogProbMetric: 42.4763

Epoch 227: val_loss did not improve from 42.47562
196/196 - 63s - loss: 42.2468 - MinusLogProbMetric: 42.2468 - val_loss: 42.4763 - val_MinusLogProbMetric: 42.4763 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 228/1000
2023-10-10 07:03:24.200 
Epoch 228/1000 
	 loss: 42.2498, MinusLogProbMetric: 42.2498, val_loss: 42.4766, val_MinusLogProbMetric: 42.4766

Epoch 228: val_loss did not improve from 42.47562
196/196 - 63s - loss: 42.2498 - MinusLogProbMetric: 42.2498 - val_loss: 42.4766 - val_MinusLogProbMetric: 42.4766 - lr: 1.6935e-08 - 63s/epoch - 324ms/step
Epoch 229/1000
2023-10-10 07:04:27.052 
Epoch 229/1000 
	 loss: 42.2496, MinusLogProbMetric: 42.2496, val_loss: 42.4779, val_MinusLogProbMetric: 42.4779

Epoch 229: val_loss did not improve from 42.47562
196/196 - 63s - loss: 42.2496 - MinusLogProbMetric: 42.2496 - val_loss: 42.4779 - val_MinusLogProbMetric: 42.4779 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 230/1000
2023-10-10 07:05:29.576 
Epoch 230/1000 
	 loss: 42.2494, MinusLogProbMetric: 42.2494, val_loss: 42.4772, val_MinusLogProbMetric: 42.4772

Epoch 230: val_loss did not improve from 42.47562
196/196 - 63s - loss: 42.2494 - MinusLogProbMetric: 42.2494 - val_loss: 42.4772 - val_MinusLogProbMetric: 42.4772 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 231/1000
2023-10-10 07:06:33.064 
Epoch 231/1000 
	 loss: 42.2476, MinusLogProbMetric: 42.2476, val_loss: 42.4764, val_MinusLogProbMetric: 42.4764

Epoch 231: val_loss did not improve from 42.47562
196/196 - 63s - loss: 42.2476 - MinusLogProbMetric: 42.2476 - val_loss: 42.4764 - val_MinusLogProbMetric: 42.4764 - lr: 1.6935e-08 - 63s/epoch - 324ms/step
Epoch 232/1000
2023-10-10 07:07:36.301 
Epoch 232/1000 
	 loss: 42.2484, MinusLogProbMetric: 42.2484, val_loss: 42.4736, val_MinusLogProbMetric: 42.4736

Epoch 232: val_loss improved from 42.47562 to 42.47364, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2484 - MinusLogProbMetric: 42.2484 - val_loss: 42.4736 - val_MinusLogProbMetric: 42.4736 - lr: 1.6935e-08 - 64s/epoch - 329ms/step
Epoch 233/1000
2023-10-10 07:08:40.465 
Epoch 233/1000 
	 loss: 42.2480, MinusLogProbMetric: 42.2480, val_loss: 42.4770, val_MinusLogProbMetric: 42.4770

Epoch 233: val_loss did not improve from 42.47364
196/196 - 63s - loss: 42.2480 - MinusLogProbMetric: 42.2480 - val_loss: 42.4770 - val_MinusLogProbMetric: 42.4770 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 234/1000
2023-10-10 07:09:43.374 
Epoch 234/1000 
	 loss: 42.2459, MinusLogProbMetric: 42.2459, val_loss: 42.4763, val_MinusLogProbMetric: 42.4763

Epoch 234: val_loss did not improve from 42.47364
196/196 - 63s - loss: 42.2459 - MinusLogProbMetric: 42.2459 - val_loss: 42.4763 - val_MinusLogProbMetric: 42.4763 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 235/1000
2023-10-10 07:10:46.671 
Epoch 235/1000 
	 loss: 42.2490, MinusLogProbMetric: 42.2490, val_loss: 42.4795, val_MinusLogProbMetric: 42.4795

Epoch 235: val_loss did not improve from 42.47364
196/196 - 63s - loss: 42.2490 - MinusLogProbMetric: 42.2490 - val_loss: 42.4795 - val_MinusLogProbMetric: 42.4795 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 236/1000
2023-10-10 07:11:49.690 
Epoch 236/1000 
	 loss: 42.2476, MinusLogProbMetric: 42.2476, val_loss: 42.4768, val_MinusLogProbMetric: 42.4768

Epoch 236: val_loss did not improve from 42.47364
196/196 - 63s - loss: 42.2476 - MinusLogProbMetric: 42.2476 - val_loss: 42.4768 - val_MinusLogProbMetric: 42.4768 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 237/1000
2023-10-10 07:12:52.465 
Epoch 237/1000 
	 loss: 42.2449, MinusLogProbMetric: 42.2449, val_loss: 42.4766, val_MinusLogProbMetric: 42.4766

Epoch 237: val_loss did not improve from 42.47364
196/196 - 63s - loss: 42.2449 - MinusLogProbMetric: 42.2449 - val_loss: 42.4766 - val_MinusLogProbMetric: 42.4766 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 238/1000
2023-10-10 07:13:54.784 
Epoch 238/1000 
	 loss: 42.2467, MinusLogProbMetric: 42.2467, val_loss: 42.4771, val_MinusLogProbMetric: 42.4771

Epoch 238: val_loss did not improve from 42.47364
196/196 - 62s - loss: 42.2467 - MinusLogProbMetric: 42.2467 - val_loss: 42.4771 - val_MinusLogProbMetric: 42.4771 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 239/1000
2023-10-10 07:14:57.980 
Epoch 239/1000 
	 loss: 42.2452, MinusLogProbMetric: 42.2452, val_loss: 42.4743, val_MinusLogProbMetric: 42.4743

Epoch 239: val_loss did not improve from 42.47364
196/196 - 63s - loss: 42.2452 - MinusLogProbMetric: 42.2452 - val_loss: 42.4743 - val_MinusLogProbMetric: 42.4743 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 240/1000
2023-10-10 07:15:59.843 
Epoch 240/1000 
	 loss: 42.2458, MinusLogProbMetric: 42.2458, val_loss: 42.4755, val_MinusLogProbMetric: 42.4755

Epoch 240: val_loss did not improve from 42.47364
196/196 - 62s - loss: 42.2458 - MinusLogProbMetric: 42.2458 - val_loss: 42.4755 - val_MinusLogProbMetric: 42.4755 - lr: 1.6935e-08 - 62s/epoch - 316ms/step
Epoch 241/1000
2023-10-10 07:17:02.563 
Epoch 241/1000 
	 loss: 42.2460, MinusLogProbMetric: 42.2460, val_loss: 42.4766, val_MinusLogProbMetric: 42.4766

Epoch 241: val_loss did not improve from 42.47364
196/196 - 63s - loss: 42.2460 - MinusLogProbMetric: 42.2460 - val_loss: 42.4766 - val_MinusLogProbMetric: 42.4766 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 242/1000
2023-10-10 07:18:05.235 
Epoch 242/1000 
	 loss: 42.2455, MinusLogProbMetric: 42.2455, val_loss: 42.4761, val_MinusLogProbMetric: 42.4761

Epoch 242: val_loss did not improve from 42.47364
196/196 - 63s - loss: 42.2455 - MinusLogProbMetric: 42.2455 - val_loss: 42.4761 - val_MinusLogProbMetric: 42.4761 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 243/1000
2023-10-10 07:19:08.414 
Epoch 243/1000 
	 loss: 42.2456, MinusLogProbMetric: 42.2456, val_loss: 42.4775, val_MinusLogProbMetric: 42.4775

Epoch 243: val_loss did not improve from 42.47364
196/196 - 63s - loss: 42.2456 - MinusLogProbMetric: 42.2456 - val_loss: 42.4775 - val_MinusLogProbMetric: 42.4775 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 244/1000
2023-10-10 07:20:13.003 
Epoch 244/1000 
	 loss: 42.2453, MinusLogProbMetric: 42.2453, val_loss: 42.4737, val_MinusLogProbMetric: 42.4737

Epoch 244: val_loss did not improve from 42.47364
196/196 - 65s - loss: 42.2453 - MinusLogProbMetric: 42.2453 - val_loss: 42.4737 - val_MinusLogProbMetric: 42.4737 - lr: 1.6935e-08 - 65s/epoch - 330ms/step
Epoch 245/1000
2023-10-10 07:21:16.809 
Epoch 245/1000 
	 loss: 42.2443, MinusLogProbMetric: 42.2443, val_loss: 42.4713, val_MinusLogProbMetric: 42.4713

Epoch 245: val_loss improved from 42.47364 to 42.47125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 42.2443 - MinusLogProbMetric: 42.2443 - val_loss: 42.4713 - val_MinusLogProbMetric: 42.4713 - lr: 1.6935e-08 - 65s/epoch - 331ms/step
Epoch 246/1000
2023-10-10 07:22:19.840 
Epoch 246/1000 
	 loss: 42.2433, MinusLogProbMetric: 42.2433, val_loss: 42.4729, val_MinusLogProbMetric: 42.4729

Epoch 246: val_loss did not improve from 42.47125
196/196 - 62s - loss: 42.2433 - MinusLogProbMetric: 42.2433 - val_loss: 42.4729 - val_MinusLogProbMetric: 42.4729 - lr: 1.6935e-08 - 62s/epoch - 316ms/step
Epoch 247/1000
2023-10-10 07:23:22.151 
Epoch 247/1000 
	 loss: 42.2448, MinusLogProbMetric: 42.2448, val_loss: 42.4725, val_MinusLogProbMetric: 42.4725

Epoch 247: val_loss did not improve from 42.47125
196/196 - 62s - loss: 42.2448 - MinusLogProbMetric: 42.2448 - val_loss: 42.4725 - val_MinusLogProbMetric: 42.4725 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 248/1000
2023-10-10 07:24:24.671 
Epoch 248/1000 
	 loss: 42.2453, MinusLogProbMetric: 42.2453, val_loss: 42.4739, val_MinusLogProbMetric: 42.4739

Epoch 248: val_loss did not improve from 42.47125
196/196 - 63s - loss: 42.2453 - MinusLogProbMetric: 42.2453 - val_loss: 42.4739 - val_MinusLogProbMetric: 42.4739 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 249/1000
2023-10-10 07:25:28.450 
Epoch 249/1000 
	 loss: 42.2449, MinusLogProbMetric: 42.2449, val_loss: 42.4721, val_MinusLogProbMetric: 42.4721

Epoch 249: val_loss did not improve from 42.47125
196/196 - 64s - loss: 42.2449 - MinusLogProbMetric: 42.2449 - val_loss: 42.4721 - val_MinusLogProbMetric: 42.4721 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 250/1000
2023-10-10 07:26:31.129 
Epoch 250/1000 
	 loss: 42.2457, MinusLogProbMetric: 42.2457, val_loss: 42.4695, val_MinusLogProbMetric: 42.4695

Epoch 250: val_loss improved from 42.47125 to 42.46946, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2457 - MinusLogProbMetric: 42.2457 - val_loss: 42.4695 - val_MinusLogProbMetric: 42.4695 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 251/1000
2023-10-10 07:27:35.760 
Epoch 251/1000 
	 loss: 42.2441, MinusLogProbMetric: 42.2441, val_loss: 42.4702, val_MinusLogProbMetric: 42.4702

Epoch 251: val_loss did not improve from 42.46946
196/196 - 64s - loss: 42.2441 - MinusLogProbMetric: 42.2441 - val_loss: 42.4702 - val_MinusLogProbMetric: 42.4702 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 252/1000
2023-10-10 07:28:38.916 
Epoch 252/1000 
	 loss: 42.2426, MinusLogProbMetric: 42.2426, val_loss: 42.4698, val_MinusLogProbMetric: 42.4698

Epoch 252: val_loss did not improve from 42.46946
196/196 - 63s - loss: 42.2426 - MinusLogProbMetric: 42.2426 - val_loss: 42.4698 - val_MinusLogProbMetric: 42.4698 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 253/1000
2023-10-10 07:29:44.606 
Epoch 253/1000 
	 loss: 42.2430, MinusLogProbMetric: 42.2430, val_loss: 42.4680, val_MinusLogProbMetric: 42.4680

Epoch 253: val_loss improved from 42.46946 to 42.46799, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 42.2430 - MinusLogProbMetric: 42.2430 - val_loss: 42.4680 - val_MinusLogProbMetric: 42.4680 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 254/1000
2023-10-10 07:30:48.570 
Epoch 254/1000 
	 loss: 42.2424, MinusLogProbMetric: 42.2424, val_loss: 42.4744, val_MinusLogProbMetric: 42.4744

Epoch 254: val_loss did not improve from 42.46799
196/196 - 63s - loss: 42.2424 - MinusLogProbMetric: 42.2424 - val_loss: 42.4744 - val_MinusLogProbMetric: 42.4744 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 255/1000
2023-10-10 07:31:50.401 
Epoch 255/1000 
	 loss: 42.2409, MinusLogProbMetric: 42.2409, val_loss: 42.4717, val_MinusLogProbMetric: 42.4717

Epoch 255: val_loss did not improve from 42.46799
196/196 - 62s - loss: 42.2409 - MinusLogProbMetric: 42.2409 - val_loss: 42.4717 - val_MinusLogProbMetric: 42.4717 - lr: 1.6935e-08 - 62s/epoch - 315ms/step
Epoch 256/1000
2023-10-10 07:32:53.927 
Epoch 256/1000 
	 loss: 42.2419, MinusLogProbMetric: 42.2419, val_loss: 42.4726, val_MinusLogProbMetric: 42.4726

Epoch 256: val_loss did not improve from 42.46799
196/196 - 64s - loss: 42.2419 - MinusLogProbMetric: 42.2419 - val_loss: 42.4726 - val_MinusLogProbMetric: 42.4726 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 257/1000
2023-10-10 07:33:56.534 
Epoch 257/1000 
	 loss: 42.2417, MinusLogProbMetric: 42.2417, val_loss: 42.4711, val_MinusLogProbMetric: 42.4711

Epoch 257: val_loss did not improve from 42.46799
196/196 - 63s - loss: 42.2417 - MinusLogProbMetric: 42.2417 - val_loss: 42.4711 - val_MinusLogProbMetric: 42.4711 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 258/1000
2023-10-10 07:34:59.049 
Epoch 258/1000 
	 loss: 42.2408, MinusLogProbMetric: 42.2408, val_loss: 42.4670, val_MinusLogProbMetric: 42.4670

Epoch 258: val_loss improved from 42.46799 to 42.46701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2408 - MinusLogProbMetric: 42.2408 - val_loss: 42.4670 - val_MinusLogProbMetric: 42.4670 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 259/1000
2023-10-10 07:36:02.994 
Epoch 259/1000 
	 loss: 42.2422, MinusLogProbMetric: 42.2422, val_loss: 42.4715, val_MinusLogProbMetric: 42.4715

Epoch 259: val_loss did not improve from 42.46701
196/196 - 63s - loss: 42.2422 - MinusLogProbMetric: 42.2422 - val_loss: 42.4715 - val_MinusLogProbMetric: 42.4715 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 260/1000
2023-10-10 07:37:06.164 
Epoch 260/1000 
	 loss: 42.2442, MinusLogProbMetric: 42.2442, val_loss: 42.4792, val_MinusLogProbMetric: 42.4792

Epoch 260: val_loss did not improve from 42.46701
196/196 - 63s - loss: 42.2442 - MinusLogProbMetric: 42.2442 - val_loss: 42.4792 - val_MinusLogProbMetric: 42.4792 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 261/1000
2023-10-10 07:38:08.646 
Epoch 261/1000 
	 loss: 42.2461, MinusLogProbMetric: 42.2461, val_loss: 42.4737, val_MinusLogProbMetric: 42.4737

Epoch 261: val_loss did not improve from 42.46701
196/196 - 62s - loss: 42.2461 - MinusLogProbMetric: 42.2461 - val_loss: 42.4737 - val_MinusLogProbMetric: 42.4737 - lr: 1.6935e-08 - 62s/epoch - 319ms/step
Epoch 262/1000
2023-10-10 07:39:11.302 
Epoch 262/1000 
	 loss: 42.2444, MinusLogProbMetric: 42.2444, val_loss: 42.4739, val_MinusLogProbMetric: 42.4739

Epoch 262: val_loss did not improve from 42.46701
196/196 - 63s - loss: 42.2444 - MinusLogProbMetric: 42.2444 - val_loss: 42.4739 - val_MinusLogProbMetric: 42.4739 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 263/1000
2023-10-10 07:40:15.796 
Epoch 263/1000 
	 loss: 42.2434, MinusLogProbMetric: 42.2434, val_loss: 42.4740, val_MinusLogProbMetric: 42.4740

Epoch 263: val_loss did not improve from 42.46701
196/196 - 64s - loss: 42.2434 - MinusLogProbMetric: 42.2434 - val_loss: 42.4740 - val_MinusLogProbMetric: 42.4740 - lr: 1.6935e-08 - 64s/epoch - 329ms/step
Epoch 264/1000
2023-10-10 07:41:19.193 
Epoch 264/1000 
	 loss: 42.2430, MinusLogProbMetric: 42.2430, val_loss: 42.4685, val_MinusLogProbMetric: 42.4685

Epoch 264: val_loss did not improve from 42.46701
196/196 - 63s - loss: 42.2430 - MinusLogProbMetric: 42.2430 - val_loss: 42.4685 - val_MinusLogProbMetric: 42.4685 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 265/1000
2023-10-10 07:42:22.032 
Epoch 265/1000 
	 loss: 42.2409, MinusLogProbMetric: 42.2409, val_loss: 42.4693, val_MinusLogProbMetric: 42.4693

Epoch 265: val_loss did not improve from 42.46701
196/196 - 63s - loss: 42.2409 - MinusLogProbMetric: 42.2409 - val_loss: 42.4693 - val_MinusLogProbMetric: 42.4693 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 266/1000
2023-10-10 07:43:24.312 
Epoch 266/1000 
	 loss: 42.2435, MinusLogProbMetric: 42.2435, val_loss: 42.4690, val_MinusLogProbMetric: 42.4690

Epoch 266: val_loss did not improve from 42.46701
196/196 - 62s - loss: 42.2435 - MinusLogProbMetric: 42.2435 - val_loss: 42.4690 - val_MinusLogProbMetric: 42.4690 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 267/1000
2023-10-10 07:44:27.172 
Epoch 267/1000 
	 loss: 42.2400, MinusLogProbMetric: 42.2400, val_loss: 42.4666, val_MinusLogProbMetric: 42.4666

Epoch 267: val_loss improved from 42.46701 to 42.46656, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2400 - MinusLogProbMetric: 42.2400 - val_loss: 42.4666 - val_MinusLogProbMetric: 42.4666 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 268/1000
2023-10-10 07:45:30.084 
Epoch 268/1000 
	 loss: 42.2429, MinusLogProbMetric: 42.2429, val_loss: 42.4714, val_MinusLogProbMetric: 42.4714

Epoch 268: val_loss did not improve from 42.46656
196/196 - 62s - loss: 42.2429 - MinusLogProbMetric: 42.2429 - val_loss: 42.4714 - val_MinusLogProbMetric: 42.4714 - lr: 1.6935e-08 - 62s/epoch - 316ms/step
Epoch 269/1000
2023-10-10 07:46:32.649 
Epoch 269/1000 
	 loss: 42.2456, MinusLogProbMetric: 42.2456, val_loss: 42.4704, val_MinusLogProbMetric: 42.4704

Epoch 269: val_loss did not improve from 42.46656
196/196 - 63s - loss: 42.2456 - MinusLogProbMetric: 42.2456 - val_loss: 42.4704 - val_MinusLogProbMetric: 42.4704 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 270/1000
2023-10-10 07:47:35.599 
Epoch 270/1000 
	 loss: 42.2421, MinusLogProbMetric: 42.2421, val_loss: 42.4670, val_MinusLogProbMetric: 42.4670

Epoch 270: val_loss did not improve from 42.46656
196/196 - 63s - loss: 42.2421 - MinusLogProbMetric: 42.2421 - val_loss: 42.4670 - val_MinusLogProbMetric: 42.4670 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 271/1000
2023-10-10 07:48:38.695 
Epoch 271/1000 
	 loss: 42.2405, MinusLogProbMetric: 42.2405, val_loss: 42.4680, val_MinusLogProbMetric: 42.4680

Epoch 271: val_loss did not improve from 42.46656
196/196 - 63s - loss: 42.2405 - MinusLogProbMetric: 42.2405 - val_loss: 42.4680 - val_MinusLogProbMetric: 42.4680 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 272/1000
2023-10-10 07:49:41.480 
Epoch 272/1000 
	 loss: 42.2395, MinusLogProbMetric: 42.2395, val_loss: 42.4703, val_MinusLogProbMetric: 42.4703

Epoch 272: val_loss did not improve from 42.46656
196/196 - 63s - loss: 42.2395 - MinusLogProbMetric: 42.2395 - val_loss: 42.4703 - val_MinusLogProbMetric: 42.4703 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 273/1000
2023-10-10 07:50:43.610 
Epoch 273/1000 
	 loss: 42.2398, MinusLogProbMetric: 42.2398, val_loss: 42.4676, val_MinusLogProbMetric: 42.4676

Epoch 273: val_loss did not improve from 42.46656
196/196 - 62s - loss: 42.2398 - MinusLogProbMetric: 42.2398 - val_loss: 42.4676 - val_MinusLogProbMetric: 42.4676 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 274/1000
2023-10-10 07:51:47.600 
Epoch 274/1000 
	 loss: 42.2391, MinusLogProbMetric: 42.2391, val_loss: 42.4704, val_MinusLogProbMetric: 42.4704

Epoch 274: val_loss did not improve from 42.46656
196/196 - 64s - loss: 42.2391 - MinusLogProbMetric: 42.2391 - val_loss: 42.4704 - val_MinusLogProbMetric: 42.4704 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 275/1000
2023-10-10 07:52:50.064 
Epoch 275/1000 
	 loss: 42.2406, MinusLogProbMetric: 42.2406, val_loss: 42.4671, val_MinusLogProbMetric: 42.4671

Epoch 275: val_loss did not improve from 42.46656
196/196 - 62s - loss: 42.2406 - MinusLogProbMetric: 42.2406 - val_loss: 42.4671 - val_MinusLogProbMetric: 42.4671 - lr: 1.6935e-08 - 62s/epoch - 319ms/step
Epoch 276/1000
2023-10-10 07:53:52.334 
Epoch 276/1000 
	 loss: 42.2392, MinusLogProbMetric: 42.2392, val_loss: 42.4659, val_MinusLogProbMetric: 42.4659

Epoch 276: val_loss improved from 42.46656 to 42.46585, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.2392 - MinusLogProbMetric: 42.2392 - val_loss: 42.4659 - val_MinusLogProbMetric: 42.4659 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 277/1000
2023-10-10 07:54:56.301 
Epoch 277/1000 
	 loss: 42.2382, MinusLogProbMetric: 42.2382, val_loss: 42.4669, val_MinusLogProbMetric: 42.4669

Epoch 277: val_loss did not improve from 42.46585
196/196 - 63s - loss: 42.2382 - MinusLogProbMetric: 42.2382 - val_loss: 42.4669 - val_MinusLogProbMetric: 42.4669 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 278/1000
2023-10-10 07:55:58.516 
Epoch 278/1000 
	 loss: 42.2401, MinusLogProbMetric: 42.2401, val_loss: 42.4643, val_MinusLogProbMetric: 42.4643

Epoch 278: val_loss improved from 42.46585 to 42.46431, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.2401 - MinusLogProbMetric: 42.2401 - val_loss: 42.4643 - val_MinusLogProbMetric: 42.4643 - lr: 1.6935e-08 - 63s/epoch - 324ms/step
Epoch 279/1000
2023-10-10 07:57:03.492 
Epoch 279/1000 
	 loss: 42.2378, MinusLogProbMetric: 42.2378, val_loss: 42.4667, val_MinusLogProbMetric: 42.4667

Epoch 279: val_loss did not improve from 42.46431
196/196 - 64s - loss: 42.2378 - MinusLogProbMetric: 42.2378 - val_loss: 42.4667 - val_MinusLogProbMetric: 42.4667 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 280/1000
2023-10-10 07:58:05.640 
Epoch 280/1000 
	 loss: 42.2381, MinusLogProbMetric: 42.2381, val_loss: 42.4658, val_MinusLogProbMetric: 42.4658

Epoch 280: val_loss did not improve from 42.46431
196/196 - 62s - loss: 42.2381 - MinusLogProbMetric: 42.2381 - val_loss: 42.4658 - val_MinusLogProbMetric: 42.4658 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 281/1000
2023-10-10 07:59:08.280 
Epoch 281/1000 
	 loss: 42.2368, MinusLogProbMetric: 42.2368, val_loss: 42.4702, val_MinusLogProbMetric: 42.4702

Epoch 281: val_loss did not improve from 42.46431
196/196 - 63s - loss: 42.2368 - MinusLogProbMetric: 42.2368 - val_loss: 42.4702 - val_MinusLogProbMetric: 42.4702 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 282/1000
2023-10-10 08:00:11.121 
Epoch 282/1000 
	 loss: 42.2371, MinusLogProbMetric: 42.2371, val_loss: 42.4695, val_MinusLogProbMetric: 42.4695

Epoch 282: val_loss did not improve from 42.46431
196/196 - 63s - loss: 42.2371 - MinusLogProbMetric: 42.2371 - val_loss: 42.4695 - val_MinusLogProbMetric: 42.4695 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 283/1000
2023-10-10 08:01:13.889 
Epoch 283/1000 
	 loss: 42.2366, MinusLogProbMetric: 42.2366, val_loss: 42.4669, val_MinusLogProbMetric: 42.4669

Epoch 283: val_loss did not improve from 42.46431
196/196 - 63s - loss: 42.2366 - MinusLogProbMetric: 42.2366 - val_loss: 42.4669 - val_MinusLogProbMetric: 42.4669 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 284/1000
2023-10-10 08:02:16.399 
Epoch 284/1000 
	 loss: 42.2387, MinusLogProbMetric: 42.2387, val_loss: 42.4694, val_MinusLogProbMetric: 42.4694

Epoch 284: val_loss did not improve from 42.46431
196/196 - 63s - loss: 42.2387 - MinusLogProbMetric: 42.2387 - val_loss: 42.4694 - val_MinusLogProbMetric: 42.4694 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 285/1000
2023-10-10 08:03:17.771 
Epoch 285/1000 
	 loss: 42.2416, MinusLogProbMetric: 42.2416, val_loss: 42.4693, val_MinusLogProbMetric: 42.4693

Epoch 285: val_loss did not improve from 42.46431
196/196 - 61s - loss: 42.2416 - MinusLogProbMetric: 42.2416 - val_loss: 42.4693 - val_MinusLogProbMetric: 42.4693 - lr: 1.6935e-08 - 61s/epoch - 313ms/step
Epoch 286/1000
2023-10-10 08:04:20.506 
Epoch 286/1000 
	 loss: 42.2401, MinusLogProbMetric: 42.2401, val_loss: 42.4704, val_MinusLogProbMetric: 42.4704

Epoch 286: val_loss did not improve from 42.46431
196/196 - 63s - loss: 42.2401 - MinusLogProbMetric: 42.2401 - val_loss: 42.4704 - val_MinusLogProbMetric: 42.4704 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 287/1000
2023-10-10 08:05:24.436 
Epoch 287/1000 
	 loss: 42.2428, MinusLogProbMetric: 42.2428, val_loss: 42.4704, val_MinusLogProbMetric: 42.4704

Epoch 287: val_loss did not improve from 42.46431
196/196 - 64s - loss: 42.2428 - MinusLogProbMetric: 42.2428 - val_loss: 42.4704 - val_MinusLogProbMetric: 42.4704 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 288/1000
2023-10-10 08:06:28.444 
Epoch 288/1000 
	 loss: 42.2408, MinusLogProbMetric: 42.2408, val_loss: 42.4664, val_MinusLogProbMetric: 42.4664

Epoch 288: val_loss did not improve from 42.46431
196/196 - 64s - loss: 42.2408 - MinusLogProbMetric: 42.2408 - val_loss: 42.4664 - val_MinusLogProbMetric: 42.4664 - lr: 1.6935e-08 - 64s/epoch - 327ms/step
Epoch 289/1000
2023-10-10 08:07:31.035 
Epoch 289/1000 
	 loss: 42.2388, MinusLogProbMetric: 42.2388, val_loss: 42.4674, val_MinusLogProbMetric: 42.4674

Epoch 289: val_loss did not improve from 42.46431
196/196 - 63s - loss: 42.2388 - MinusLogProbMetric: 42.2388 - val_loss: 42.4674 - val_MinusLogProbMetric: 42.4674 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 290/1000
2023-10-10 08:08:33.085 
Epoch 290/1000 
	 loss: 42.2387, MinusLogProbMetric: 42.2387, val_loss: 42.4644, val_MinusLogProbMetric: 42.4644

Epoch 290: val_loss did not improve from 42.46431
196/196 - 62s - loss: 42.2387 - MinusLogProbMetric: 42.2387 - val_loss: 42.4644 - val_MinusLogProbMetric: 42.4644 - lr: 1.6935e-08 - 62s/epoch - 317ms/step
Epoch 291/1000
2023-10-10 08:09:35.783 
Epoch 291/1000 
	 loss: 42.2370, MinusLogProbMetric: 42.2370, val_loss: 42.4638, val_MinusLogProbMetric: 42.4638

Epoch 291: val_loss improved from 42.46431 to 42.46383, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2370 - MinusLogProbMetric: 42.2370 - val_loss: 42.4638 - val_MinusLogProbMetric: 42.4638 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 292/1000
2023-10-10 08:10:39.317 
Epoch 292/1000 
	 loss: 42.2362, MinusLogProbMetric: 42.2362, val_loss: 42.4668, val_MinusLogProbMetric: 42.4668

Epoch 292: val_loss did not improve from 42.46383
196/196 - 62s - loss: 42.2362 - MinusLogProbMetric: 42.2362 - val_loss: 42.4668 - val_MinusLogProbMetric: 42.4668 - lr: 1.6935e-08 - 62s/epoch - 319ms/step
Epoch 293/1000
2023-10-10 08:11:42.107 
Epoch 293/1000 
	 loss: 42.2378, MinusLogProbMetric: 42.2378, val_loss: 42.4624, val_MinusLogProbMetric: 42.4624

Epoch 293: val_loss improved from 42.46383 to 42.46243, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2378 - MinusLogProbMetric: 42.2378 - val_loss: 42.4624 - val_MinusLogProbMetric: 42.4624 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 294/1000
2023-10-10 08:12:45.196 
Epoch 294/1000 
	 loss: 42.2367, MinusLogProbMetric: 42.2367, val_loss: 42.4595, val_MinusLogProbMetric: 42.4595

Epoch 294: val_loss improved from 42.46243 to 42.45953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.2367 - MinusLogProbMetric: 42.2367 - val_loss: 42.4595 - val_MinusLogProbMetric: 42.4595 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 295/1000
2023-10-10 08:13:48.894 
Epoch 295/1000 
	 loss: 42.2366, MinusLogProbMetric: 42.2366, val_loss: 42.4671, val_MinusLogProbMetric: 42.4671

Epoch 295: val_loss did not improve from 42.45953
196/196 - 63s - loss: 42.2366 - MinusLogProbMetric: 42.2366 - val_loss: 42.4671 - val_MinusLogProbMetric: 42.4671 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 296/1000
2023-10-10 08:14:51.744 
Epoch 296/1000 
	 loss: 42.2354, MinusLogProbMetric: 42.2354, val_loss: 42.4638, val_MinusLogProbMetric: 42.4638

Epoch 296: val_loss did not improve from 42.45953
196/196 - 63s - loss: 42.2354 - MinusLogProbMetric: 42.2354 - val_loss: 42.4638 - val_MinusLogProbMetric: 42.4638 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 297/1000
2023-10-10 08:15:54.406 
Epoch 297/1000 
	 loss: 42.2354, MinusLogProbMetric: 42.2354, val_loss: 42.4612, val_MinusLogProbMetric: 42.4612

Epoch 297: val_loss did not improve from 42.45953
196/196 - 63s - loss: 42.2354 - MinusLogProbMetric: 42.2354 - val_loss: 42.4612 - val_MinusLogProbMetric: 42.4612 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 298/1000
2023-10-10 08:16:58.987 
Epoch 298/1000 
	 loss: 42.2356, MinusLogProbMetric: 42.2356, val_loss: 42.4601, val_MinusLogProbMetric: 42.4601

Epoch 298: val_loss did not improve from 42.45953
196/196 - 65s - loss: 42.2356 - MinusLogProbMetric: 42.2356 - val_loss: 42.4601 - val_MinusLogProbMetric: 42.4601 - lr: 1.6935e-08 - 65s/epoch - 329ms/step
Epoch 299/1000
2023-10-10 08:17:59.616 
Epoch 299/1000 
	 loss: 42.2353, MinusLogProbMetric: 42.2353, val_loss: 42.4598, val_MinusLogProbMetric: 42.4598

Epoch 299: val_loss did not improve from 42.45953
196/196 - 61s - loss: 42.2353 - MinusLogProbMetric: 42.2353 - val_loss: 42.4598 - val_MinusLogProbMetric: 42.4598 - lr: 1.6935e-08 - 61s/epoch - 309ms/step
Epoch 300/1000
2023-10-10 08:19:02.579 
Epoch 300/1000 
	 loss: 42.2345, MinusLogProbMetric: 42.2345, val_loss: 42.4615, val_MinusLogProbMetric: 42.4615

Epoch 300: val_loss did not improve from 42.45953
196/196 - 63s - loss: 42.2345 - MinusLogProbMetric: 42.2345 - val_loss: 42.4615 - val_MinusLogProbMetric: 42.4615 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 301/1000
2023-10-10 08:20:05.538 
Epoch 301/1000 
	 loss: 42.2344, MinusLogProbMetric: 42.2344, val_loss: 42.4621, val_MinusLogProbMetric: 42.4621

Epoch 301: val_loss did not improve from 42.45953
196/196 - 63s - loss: 42.2344 - MinusLogProbMetric: 42.2344 - val_loss: 42.4621 - val_MinusLogProbMetric: 42.4621 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 302/1000
2023-10-10 08:21:08.004 
Epoch 302/1000 
	 loss: 42.2359, MinusLogProbMetric: 42.2359, val_loss: 42.4603, val_MinusLogProbMetric: 42.4603

Epoch 302: val_loss did not improve from 42.45953
196/196 - 62s - loss: 42.2359 - MinusLogProbMetric: 42.2359 - val_loss: 42.4603 - val_MinusLogProbMetric: 42.4603 - lr: 1.6935e-08 - 62s/epoch - 319ms/step
Epoch 303/1000
2023-10-10 08:22:11.224 
Epoch 303/1000 
	 loss: 42.2363, MinusLogProbMetric: 42.2363, val_loss: 42.4621, val_MinusLogProbMetric: 42.4621

Epoch 303: val_loss did not improve from 42.45953
196/196 - 63s - loss: 42.2363 - MinusLogProbMetric: 42.2363 - val_loss: 42.4621 - val_MinusLogProbMetric: 42.4621 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 304/1000
2023-10-10 08:23:13.773 
Epoch 304/1000 
	 loss: 42.2359, MinusLogProbMetric: 42.2359, val_loss: 42.4638, val_MinusLogProbMetric: 42.4638

Epoch 304: val_loss did not improve from 42.45953
196/196 - 63s - loss: 42.2359 - MinusLogProbMetric: 42.2359 - val_loss: 42.4638 - val_MinusLogProbMetric: 42.4638 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 305/1000
2023-10-10 08:24:16.887 
Epoch 305/1000 
	 loss: 42.2343, MinusLogProbMetric: 42.2343, val_loss: 42.4632, val_MinusLogProbMetric: 42.4632

Epoch 305: val_loss did not improve from 42.45953
196/196 - 63s - loss: 42.2343 - MinusLogProbMetric: 42.2343 - val_loss: 42.4632 - val_MinusLogProbMetric: 42.4632 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 306/1000
2023-10-10 08:25:19.443 
Epoch 306/1000 
	 loss: 42.2359, MinusLogProbMetric: 42.2359, val_loss: 42.4654, val_MinusLogProbMetric: 42.4654

Epoch 306: val_loss did not improve from 42.45953
196/196 - 63s - loss: 42.2359 - MinusLogProbMetric: 42.2359 - val_loss: 42.4654 - val_MinusLogProbMetric: 42.4654 - lr: 1.6935e-08 - 63s/epoch - 319ms/step
Epoch 307/1000
2023-10-10 08:26:22.685 
Epoch 307/1000 
	 loss: 42.2349, MinusLogProbMetric: 42.2349, val_loss: 42.4631, val_MinusLogProbMetric: 42.4631

Epoch 307: val_loss did not improve from 42.45953
196/196 - 63s - loss: 42.2349 - MinusLogProbMetric: 42.2349 - val_loss: 42.4631 - val_MinusLogProbMetric: 42.4631 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 308/1000
2023-10-10 08:27:25.085 
Epoch 308/1000 
	 loss: 42.2360, MinusLogProbMetric: 42.2360, val_loss: 42.4625, val_MinusLogProbMetric: 42.4625

Epoch 308: val_loss did not improve from 42.45953
196/196 - 62s - loss: 42.2360 - MinusLogProbMetric: 42.2360 - val_loss: 42.4625 - val_MinusLogProbMetric: 42.4625 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 309/1000
2023-10-10 08:28:27.944 
Epoch 309/1000 
	 loss: 42.2375, MinusLogProbMetric: 42.2375, val_loss: 42.4645, val_MinusLogProbMetric: 42.4645

Epoch 309: val_loss did not improve from 42.45953
196/196 - 63s - loss: 42.2375 - MinusLogProbMetric: 42.2375 - val_loss: 42.4645 - val_MinusLogProbMetric: 42.4645 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 310/1000
2023-10-10 08:29:31.285 
Epoch 310/1000 
	 loss: 42.2358, MinusLogProbMetric: 42.2358, val_loss: 42.4651, val_MinusLogProbMetric: 42.4651

Epoch 310: val_loss did not improve from 42.45953
196/196 - 63s - loss: 42.2358 - MinusLogProbMetric: 42.2358 - val_loss: 42.4651 - val_MinusLogProbMetric: 42.4651 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 311/1000
2023-10-10 08:30:32.869 
Epoch 311/1000 
	 loss: 42.2339, MinusLogProbMetric: 42.2339, val_loss: 42.4632, val_MinusLogProbMetric: 42.4632

Epoch 311: val_loss did not improve from 42.45953
196/196 - 62s - loss: 42.2339 - MinusLogProbMetric: 42.2339 - val_loss: 42.4632 - val_MinusLogProbMetric: 42.4632 - lr: 1.6935e-08 - 62s/epoch - 314ms/step
Epoch 312/1000
2023-10-10 08:31:37.147 
Epoch 312/1000 
	 loss: 42.2342, MinusLogProbMetric: 42.2342, val_loss: 42.4629, val_MinusLogProbMetric: 42.4629

Epoch 312: val_loss did not improve from 42.45953
196/196 - 64s - loss: 42.2342 - MinusLogProbMetric: 42.2342 - val_loss: 42.4629 - val_MinusLogProbMetric: 42.4629 - lr: 1.6935e-08 - 64s/epoch - 328ms/step
Epoch 313/1000
2023-10-10 08:32:38.821 
Epoch 313/1000 
	 loss: 42.2350, MinusLogProbMetric: 42.2350, val_loss: 42.4629, val_MinusLogProbMetric: 42.4629

Epoch 313: val_loss did not improve from 42.45953
196/196 - 62s - loss: 42.2350 - MinusLogProbMetric: 42.2350 - val_loss: 42.4629 - val_MinusLogProbMetric: 42.4629 - lr: 1.6935e-08 - 62s/epoch - 315ms/step
Epoch 314/1000
2023-10-10 08:33:40.712 
Epoch 314/1000 
	 loss: 42.2336, MinusLogProbMetric: 42.2336, val_loss: 42.4598, val_MinusLogProbMetric: 42.4598

Epoch 314: val_loss did not improve from 42.45953
196/196 - 62s - loss: 42.2336 - MinusLogProbMetric: 42.2336 - val_loss: 42.4598 - val_MinusLogProbMetric: 42.4598 - lr: 1.6935e-08 - 62s/epoch - 316ms/step
Epoch 315/1000
2023-10-10 08:34:44.242 
Epoch 315/1000 
	 loss: 42.2337, MinusLogProbMetric: 42.2337, val_loss: 42.4633, val_MinusLogProbMetric: 42.4633

Epoch 315: val_loss did not improve from 42.45953
196/196 - 64s - loss: 42.2337 - MinusLogProbMetric: 42.2337 - val_loss: 42.4633 - val_MinusLogProbMetric: 42.4633 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 316/1000
2023-10-10 08:35:47.248 
Epoch 316/1000 
	 loss: 42.2333, MinusLogProbMetric: 42.2333, val_loss: 42.4617, val_MinusLogProbMetric: 42.4617

Epoch 316: val_loss did not improve from 42.45953
196/196 - 63s - loss: 42.2333 - MinusLogProbMetric: 42.2333 - val_loss: 42.4617 - val_MinusLogProbMetric: 42.4617 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 317/1000
2023-10-10 08:36:50.970 
Epoch 317/1000 
	 loss: 42.2310, MinusLogProbMetric: 42.2310, val_loss: 42.4615, val_MinusLogProbMetric: 42.4615

Epoch 317: val_loss did not improve from 42.45953
196/196 - 64s - loss: 42.2310 - MinusLogProbMetric: 42.2310 - val_loss: 42.4615 - val_MinusLogProbMetric: 42.4615 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 318/1000
2023-10-10 08:37:53.385 
Epoch 318/1000 
	 loss: 42.2311, MinusLogProbMetric: 42.2311, val_loss: 42.4591, val_MinusLogProbMetric: 42.4591

Epoch 318: val_loss improved from 42.45953 to 42.45914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2311 - MinusLogProbMetric: 42.2311 - val_loss: 42.4591 - val_MinusLogProbMetric: 42.4591 - lr: 1.6935e-08 - 64s/epoch - 324ms/step
Epoch 319/1000
2023-10-10 08:38:58.679 
Epoch 319/1000 
	 loss: 42.2322, MinusLogProbMetric: 42.2322, val_loss: 42.4580, val_MinusLogProbMetric: 42.4580

Epoch 319: val_loss improved from 42.45914 to 42.45796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 42.2322 - MinusLogProbMetric: 42.2322 - val_loss: 42.4580 - val_MinusLogProbMetric: 42.4580 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 320/1000
2023-10-10 08:40:02.799 
Epoch 320/1000 
	 loss: 42.2304, MinusLogProbMetric: 42.2304, val_loss: 42.4615, val_MinusLogProbMetric: 42.4615

Epoch 320: val_loss did not improve from 42.45796
196/196 - 63s - loss: 42.2304 - MinusLogProbMetric: 42.2304 - val_loss: 42.4615 - val_MinusLogProbMetric: 42.4615 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 321/1000
2023-10-10 08:41:06.738 
Epoch 321/1000 
	 loss: 42.2315, MinusLogProbMetric: 42.2315, val_loss: 42.4576, val_MinusLogProbMetric: 42.4576

Epoch 321: val_loss improved from 42.45796 to 42.45761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 42.2315 - MinusLogProbMetric: 42.2315 - val_loss: 42.4576 - val_MinusLogProbMetric: 42.4576 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 322/1000
2023-10-10 08:42:10.580 
Epoch 322/1000 
	 loss: 42.2295, MinusLogProbMetric: 42.2295, val_loss: 42.4586, val_MinusLogProbMetric: 42.4586

Epoch 322: val_loss did not improve from 42.45761
196/196 - 62s - loss: 42.2295 - MinusLogProbMetric: 42.2295 - val_loss: 42.4586 - val_MinusLogProbMetric: 42.4586 - lr: 1.6935e-08 - 62s/epoch - 319ms/step
Epoch 323/1000
2023-10-10 08:43:13.659 
Epoch 323/1000 
	 loss: 42.2317, MinusLogProbMetric: 42.2317, val_loss: 42.4613, val_MinusLogProbMetric: 42.4613

Epoch 323: val_loss did not improve from 42.45761
196/196 - 63s - loss: 42.2317 - MinusLogProbMetric: 42.2317 - val_loss: 42.4613 - val_MinusLogProbMetric: 42.4613 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 324/1000
2023-10-10 08:44:15.889 
Epoch 324/1000 
	 loss: 42.2321, MinusLogProbMetric: 42.2321, val_loss: 42.4568, val_MinusLogProbMetric: 42.4568

Epoch 324: val_loss improved from 42.45761 to 42.45679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.2321 - MinusLogProbMetric: 42.2321 - val_loss: 42.4568 - val_MinusLogProbMetric: 42.4568 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 325/1000
2023-10-10 08:45:18.474 
Epoch 325/1000 
	 loss: 42.2324, MinusLogProbMetric: 42.2324, val_loss: 42.4599, val_MinusLogProbMetric: 42.4599

Epoch 325: val_loss did not improve from 42.45679
196/196 - 62s - loss: 42.2324 - MinusLogProbMetric: 42.2324 - val_loss: 42.4599 - val_MinusLogProbMetric: 42.4599 - lr: 1.6935e-08 - 62s/epoch - 315ms/step
Epoch 326/1000
2023-10-10 08:46:21.103 
Epoch 326/1000 
	 loss: 42.2317, MinusLogProbMetric: 42.2317, val_loss: 42.4589, val_MinusLogProbMetric: 42.4589

Epoch 326: val_loss did not improve from 42.45679
196/196 - 63s - loss: 42.2317 - MinusLogProbMetric: 42.2317 - val_loss: 42.4589 - val_MinusLogProbMetric: 42.4589 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 327/1000
2023-10-10 08:47:24.812 
Epoch 327/1000 
	 loss: 42.2282, MinusLogProbMetric: 42.2282, val_loss: 42.4576, val_MinusLogProbMetric: 42.4576

Epoch 327: val_loss did not improve from 42.45679
196/196 - 64s - loss: 42.2282 - MinusLogProbMetric: 42.2282 - val_loss: 42.4576 - val_MinusLogProbMetric: 42.4576 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 328/1000
2023-10-10 08:48:27.098 
Epoch 328/1000 
	 loss: 42.2315, MinusLogProbMetric: 42.2315, val_loss: 42.4599, val_MinusLogProbMetric: 42.4599

Epoch 328: val_loss did not improve from 42.45679
196/196 - 62s - loss: 42.2315 - MinusLogProbMetric: 42.2315 - val_loss: 42.4599 - val_MinusLogProbMetric: 42.4599 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 329/1000
2023-10-10 08:49:30.994 
Epoch 329/1000 
	 loss: 42.2333, MinusLogProbMetric: 42.2333, val_loss: 42.4621, val_MinusLogProbMetric: 42.4621

Epoch 329: val_loss did not improve from 42.45679
196/196 - 64s - loss: 42.2333 - MinusLogProbMetric: 42.2333 - val_loss: 42.4621 - val_MinusLogProbMetric: 42.4621 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 330/1000
2023-10-10 08:50:34.176 
Epoch 330/1000 
	 loss: 42.2340, MinusLogProbMetric: 42.2340, val_loss: 42.4603, val_MinusLogProbMetric: 42.4603

Epoch 330: val_loss did not improve from 42.45679
196/196 - 63s - loss: 42.2340 - MinusLogProbMetric: 42.2340 - val_loss: 42.4603 - val_MinusLogProbMetric: 42.4603 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 331/1000
2023-10-10 08:51:37.313 
Epoch 331/1000 
	 loss: 42.2324, MinusLogProbMetric: 42.2324, val_loss: 42.4603, val_MinusLogProbMetric: 42.4603

Epoch 331: val_loss did not improve from 42.45679
196/196 - 63s - loss: 42.2324 - MinusLogProbMetric: 42.2324 - val_loss: 42.4603 - val_MinusLogProbMetric: 42.4603 - lr: 1.6935e-08 - 63s/epoch - 322ms/step
Epoch 332/1000
2023-10-10 08:52:39.055 
Epoch 332/1000 
	 loss: 42.2315, MinusLogProbMetric: 42.2315, val_loss: 42.4565, val_MinusLogProbMetric: 42.4565

Epoch 332: val_loss improved from 42.45679 to 42.45654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 63s - loss: 42.2315 - MinusLogProbMetric: 42.2315 - val_loss: 42.4565 - val_MinusLogProbMetric: 42.4565 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 333/1000
2023-10-10 08:53:41.981 
Epoch 333/1000 
	 loss: 42.2312, MinusLogProbMetric: 42.2312, val_loss: 42.4569, val_MinusLogProbMetric: 42.4569

Epoch 333: val_loss did not improve from 42.45654
196/196 - 62s - loss: 42.2312 - MinusLogProbMetric: 42.2312 - val_loss: 42.4569 - val_MinusLogProbMetric: 42.4569 - lr: 1.6935e-08 - 62s/epoch - 315ms/step
Epoch 334/1000
2023-10-10 08:54:45.247 
Epoch 334/1000 
	 loss: 42.2308, MinusLogProbMetric: 42.2308, val_loss: 42.4542, val_MinusLogProbMetric: 42.4542

Epoch 334: val_loss improved from 42.45654 to 42.45421, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 64s - loss: 42.2308 - MinusLogProbMetric: 42.2308 - val_loss: 42.4542 - val_MinusLogProbMetric: 42.4542 - lr: 1.6935e-08 - 64s/epoch - 328ms/step
Epoch 335/1000
2023-10-10 08:55:49.139 
Epoch 335/1000 
	 loss: 42.2314, MinusLogProbMetric: 42.2314, val_loss: 42.4578, val_MinusLogProbMetric: 42.4578

Epoch 335: val_loss did not improve from 42.45421
196/196 - 63s - loss: 42.2314 - MinusLogProbMetric: 42.2314 - val_loss: 42.4578 - val_MinusLogProbMetric: 42.4578 - lr: 1.6935e-08 - 63s/epoch - 320ms/step
Epoch 336/1000
2023-10-10 08:56:50.937 
Epoch 336/1000 
	 loss: 42.2305, MinusLogProbMetric: 42.2305, val_loss: 42.4596, val_MinusLogProbMetric: 42.4596

Epoch 336: val_loss did not improve from 42.45421
196/196 - 62s - loss: 42.2305 - MinusLogProbMetric: 42.2305 - val_loss: 42.4596 - val_MinusLogProbMetric: 42.4596 - lr: 1.6935e-08 - 62s/epoch - 315ms/step
Epoch 337/1000
2023-10-10 08:57:52.626 
Epoch 337/1000 
	 loss: 42.2294, MinusLogProbMetric: 42.2294, val_loss: 42.4573, val_MinusLogProbMetric: 42.4573

Epoch 337: val_loss did not improve from 42.45421
196/196 - 62s - loss: 42.2294 - MinusLogProbMetric: 42.2294 - val_loss: 42.4573 - val_MinusLogProbMetric: 42.4573 - lr: 1.6935e-08 - 62s/epoch - 315ms/step
Epoch 338/1000
2023-10-10 08:58:53.780 
Epoch 338/1000 
	 loss: 42.2280, MinusLogProbMetric: 42.2280, val_loss: 42.4593, val_MinusLogProbMetric: 42.4593

Epoch 338: val_loss did not improve from 42.45421
196/196 - 61s - loss: 42.2280 - MinusLogProbMetric: 42.2280 - val_loss: 42.4593 - val_MinusLogProbMetric: 42.4593 - lr: 1.6935e-08 - 61s/epoch - 312ms/step
Epoch 339/1000
2023-10-10 08:59:55.567 
Epoch 339/1000 
	 loss: 42.2333, MinusLogProbMetric: 42.2333, val_loss: 42.4621, val_MinusLogProbMetric: 42.4621

Epoch 339: val_loss did not improve from 42.45421
196/196 - 62s - loss: 42.2333 - MinusLogProbMetric: 42.2333 - val_loss: 42.4621 - val_MinusLogProbMetric: 42.4621 - lr: 1.6935e-08 - 62s/epoch - 315ms/step
Epoch 340/1000
2023-10-10 09:00:57.847 
Epoch 340/1000 
	 loss: 42.2323, MinusLogProbMetric: 42.2323, val_loss: 42.4618, val_MinusLogProbMetric: 42.4618

Epoch 340: val_loss did not improve from 42.45421
196/196 - 62s - loss: 42.2323 - MinusLogProbMetric: 42.2323 - val_loss: 42.4618 - val_MinusLogProbMetric: 42.4618 - lr: 1.6935e-08 - 62s/epoch - 318ms/step
Epoch 341/1000
2023-10-10 09:02:08.676 
Epoch 341/1000 
	 loss: 42.2327, MinusLogProbMetric: 42.2327, val_loss: 42.4610, val_MinusLogProbMetric: 42.4610

Epoch 341: val_loss did not improve from 42.45421
196/196 - 71s - loss: 42.2327 - MinusLogProbMetric: 42.2327 - val_loss: 42.4610 - val_MinusLogProbMetric: 42.4610 - lr: 1.6935e-08 - 71s/epoch - 361ms/step
Epoch 342/1000
2023-10-10 09:03:19.339 
Epoch 342/1000 
	 loss: 42.2311, MinusLogProbMetric: 42.2311, val_loss: 42.4601, val_MinusLogProbMetric: 42.4601

Epoch 342: val_loss did not improve from 42.45421
196/196 - 71s - loss: 42.2311 - MinusLogProbMetric: 42.2311 - val_loss: 42.4601 - val_MinusLogProbMetric: 42.4601 - lr: 1.6935e-08 - 71s/epoch - 361ms/step
Epoch 343/1000
2023-10-10 09:04:28.732 
Epoch 343/1000 
	 loss: 42.2289, MinusLogProbMetric: 42.2289, val_loss: 42.4574, val_MinusLogProbMetric: 42.4574

Epoch 343: val_loss did not improve from 42.45421
196/196 - 69s - loss: 42.2289 - MinusLogProbMetric: 42.2289 - val_loss: 42.4574 - val_MinusLogProbMetric: 42.4574 - lr: 1.6935e-08 - 69s/epoch - 354ms/step
Epoch 344/1000
2023-10-10 09:05:37.684 
Epoch 344/1000 
	 loss: 42.2307, MinusLogProbMetric: 42.2307, val_loss: 42.4585, val_MinusLogProbMetric: 42.4585

Epoch 344: val_loss did not improve from 42.45421
196/196 - 69s - loss: 42.2307 - MinusLogProbMetric: 42.2307 - val_loss: 42.4585 - val_MinusLogProbMetric: 42.4585 - lr: 1.6935e-08 - 69s/epoch - 352ms/step
Epoch 345/1000
2023-10-10 09:06:46.388 
Epoch 345/1000 
	 loss: 42.2289, MinusLogProbMetric: 42.2289, val_loss: 42.4582, val_MinusLogProbMetric: 42.4582

Epoch 345: val_loss did not improve from 42.45421
196/196 - 69s - loss: 42.2289 - MinusLogProbMetric: 42.2289 - val_loss: 42.4582 - val_MinusLogProbMetric: 42.4582 - lr: 1.6935e-08 - 69s/epoch - 350ms/step
Epoch 346/1000
2023-10-10 09:08:02.574 
Epoch 346/1000 
	 loss: 42.2297, MinusLogProbMetric: 42.2297, val_loss: 42.4585, val_MinusLogProbMetric: 42.4585

Epoch 346: val_loss did not improve from 42.45421
196/196 - 76s - loss: 42.2297 - MinusLogProbMetric: 42.2297 - val_loss: 42.4585 - val_MinusLogProbMetric: 42.4585 - lr: 1.6935e-08 - 76s/epoch - 389ms/step
Epoch 347/1000
2023-10-10 09:09:12.946 
Epoch 347/1000 
	 loss: 42.2290, MinusLogProbMetric: 42.2290, val_loss: 42.4579, val_MinusLogProbMetric: 42.4579

Epoch 347: val_loss did not improve from 42.45421
196/196 - 70s - loss: 42.2290 - MinusLogProbMetric: 42.2290 - val_loss: 42.4579 - val_MinusLogProbMetric: 42.4579 - lr: 1.6935e-08 - 70s/epoch - 359ms/step
Epoch 348/1000
2023-10-10 09:10:23.148 
Epoch 348/1000 
	 loss: 42.2292, MinusLogProbMetric: 42.2292, val_loss: 42.4588, val_MinusLogProbMetric: 42.4588

Epoch 348: val_loss did not improve from 42.45421
196/196 - 70s - loss: 42.2292 - MinusLogProbMetric: 42.2292 - val_loss: 42.4588 - val_MinusLogProbMetric: 42.4588 - lr: 1.6935e-08 - 70s/epoch - 358ms/step
Epoch 349/1000
2023-10-10 09:11:31.974 
Epoch 349/1000 
	 loss: 42.2274, MinusLogProbMetric: 42.2274, val_loss: 42.4586, val_MinusLogProbMetric: 42.4586

Epoch 349: val_loss did not improve from 42.45421
196/196 - 69s - loss: 42.2274 - MinusLogProbMetric: 42.2274 - val_loss: 42.4586 - val_MinusLogProbMetric: 42.4586 - lr: 1.6935e-08 - 69s/epoch - 351ms/step
Epoch 350/1000
2023-10-10 09:12:41.014 
Epoch 350/1000 
	 loss: 42.2324, MinusLogProbMetric: 42.2324, val_loss: 42.4588, val_MinusLogProbMetric: 42.4588

Epoch 350: val_loss did not improve from 42.45421
196/196 - 69s - loss: 42.2324 - MinusLogProbMetric: 42.2324 - val_loss: 42.4588 - val_MinusLogProbMetric: 42.4588 - lr: 1.6935e-08 - 69s/epoch - 352ms/step
Epoch 351/1000
2023-10-10 09:13:50.579 
Epoch 351/1000 
	 loss: 42.2315, MinusLogProbMetric: 42.2315, val_loss: 42.4585, val_MinusLogProbMetric: 42.4585

Epoch 351: val_loss did not improve from 42.45421
196/196 - 70s - loss: 42.2315 - MinusLogProbMetric: 42.2315 - val_loss: 42.4585 - val_MinusLogProbMetric: 42.4585 - lr: 1.6935e-08 - 70s/epoch - 355ms/step
Epoch 352/1000
2023-10-10 09:15:02.649 
Epoch 352/1000 
	 loss: 42.2307, MinusLogProbMetric: 42.2307, val_loss: 42.4595, val_MinusLogProbMetric: 42.4595

Epoch 352: val_loss did not improve from 42.45421
196/196 - 72s - loss: 42.2307 - MinusLogProbMetric: 42.2307 - val_loss: 42.4595 - val_MinusLogProbMetric: 42.4595 - lr: 1.6935e-08 - 72s/epoch - 368ms/step
Epoch 353/1000
2023-10-10 09:16:12.588 
Epoch 353/1000 
	 loss: 42.2301, MinusLogProbMetric: 42.2301, val_loss: 42.4595, val_MinusLogProbMetric: 42.4595

Epoch 353: val_loss did not improve from 42.45421
196/196 - 70s - loss: 42.2301 - MinusLogProbMetric: 42.2301 - val_loss: 42.4595 - val_MinusLogProbMetric: 42.4595 - lr: 1.6935e-08 - 70s/epoch - 357ms/step
Epoch 354/1000
2023-10-10 09:17:22.907 
Epoch 354/1000 
	 loss: 42.2272, MinusLogProbMetric: 42.2272, val_loss: 42.4580, val_MinusLogProbMetric: 42.4580

Epoch 354: val_loss did not improve from 42.45421
196/196 - 70s - loss: 42.2272 - MinusLogProbMetric: 42.2272 - val_loss: 42.4580 - val_MinusLogProbMetric: 42.4580 - lr: 1.6935e-08 - 70s/epoch - 359ms/step
Epoch 355/1000
2023-10-10 09:18:35.150 
Epoch 355/1000 
	 loss: 42.2304, MinusLogProbMetric: 42.2304, val_loss: 42.4574, val_MinusLogProbMetric: 42.4574

Epoch 355: val_loss did not improve from 42.45421
196/196 - 72s - loss: 42.2304 - MinusLogProbMetric: 42.2304 - val_loss: 42.4574 - val_MinusLogProbMetric: 42.4574 - lr: 1.6935e-08 - 72s/epoch - 369ms/step
Epoch 356/1000
2023-10-10 09:19:49.508 
Epoch 356/1000 
	 loss: 42.2290, MinusLogProbMetric: 42.2290, val_loss: 42.4556, val_MinusLogProbMetric: 42.4556

Epoch 356: val_loss did not improve from 42.45421
196/196 - 74s - loss: 42.2290 - MinusLogProbMetric: 42.2290 - val_loss: 42.4556 - val_MinusLogProbMetric: 42.4556 - lr: 1.6935e-08 - 74s/epoch - 379ms/step
Epoch 357/1000
2023-10-10 09:21:00.054 
Epoch 357/1000 
	 loss: 42.2290, MinusLogProbMetric: 42.2290, val_loss: 42.4558, val_MinusLogProbMetric: 42.4558

Epoch 357: val_loss did not improve from 42.45421
196/196 - 71s - loss: 42.2290 - MinusLogProbMetric: 42.2290 - val_loss: 42.4558 - val_MinusLogProbMetric: 42.4558 - lr: 1.6935e-08 - 71s/epoch - 360ms/step
Epoch 358/1000
2023-10-10 09:22:10.122 
Epoch 358/1000 
	 loss: 42.2290, MinusLogProbMetric: 42.2290, val_loss: 42.4584, val_MinusLogProbMetric: 42.4584

Epoch 358: val_loss did not improve from 42.45421
196/196 - 70s - loss: 42.2290 - MinusLogProbMetric: 42.2290 - val_loss: 42.4584 - val_MinusLogProbMetric: 42.4584 - lr: 1.6935e-08 - 70s/epoch - 357ms/step
Epoch 359/1000
2023-10-10 09:23:19.913 
Epoch 359/1000 
	 loss: 42.2279, MinusLogProbMetric: 42.2279, val_loss: 42.4598, val_MinusLogProbMetric: 42.4598

Epoch 359: val_loss did not improve from 42.45421
196/196 - 70s - loss: 42.2279 - MinusLogProbMetric: 42.2279 - val_loss: 42.4598 - val_MinusLogProbMetric: 42.4598 - lr: 1.6935e-08 - 70s/epoch - 356ms/step
Epoch 360/1000
2023-10-10 09:24:29.130 
Epoch 360/1000 
	 loss: 42.2286, MinusLogProbMetric: 42.2286, val_loss: 42.4573, val_MinusLogProbMetric: 42.4573

Epoch 360: val_loss did not improve from 42.45421
196/196 - 69s - loss: 42.2286 - MinusLogProbMetric: 42.2286 - val_loss: 42.4573 - val_MinusLogProbMetric: 42.4573 - lr: 1.6935e-08 - 69s/epoch - 353ms/step
Epoch 361/1000
2023-10-10 09:25:39.332 
Epoch 361/1000 
	 loss: 42.2259, MinusLogProbMetric: 42.2259, val_loss: 42.4569, val_MinusLogProbMetric: 42.4569

Epoch 361: val_loss did not improve from 42.45421
196/196 - 70s - loss: 42.2259 - MinusLogProbMetric: 42.2259 - val_loss: 42.4569 - val_MinusLogProbMetric: 42.4569 - lr: 1.6935e-08 - 70s/epoch - 358ms/step
Epoch 362/1000
2023-10-10 09:26:49.166 
Epoch 362/1000 
	 loss: 42.2259, MinusLogProbMetric: 42.2259, val_loss: 42.4581, val_MinusLogProbMetric: 42.4581

Epoch 362: val_loss did not improve from 42.45421
196/196 - 70s - loss: 42.2259 - MinusLogProbMetric: 42.2259 - val_loss: 42.4581 - val_MinusLogProbMetric: 42.4581 - lr: 1.6935e-08 - 70s/epoch - 356ms/step
Epoch 363/1000
2023-10-10 09:27:59.110 
Epoch 363/1000 
	 loss: 42.2248, MinusLogProbMetric: 42.2248, val_loss: 42.4591, val_MinusLogProbMetric: 42.4591

Epoch 363: val_loss did not improve from 42.45421
196/196 - 70s - loss: 42.2248 - MinusLogProbMetric: 42.2248 - val_loss: 42.4591 - val_MinusLogProbMetric: 42.4591 - lr: 1.6935e-08 - 70s/epoch - 357ms/step
Epoch 364/1000
2023-10-10 09:29:09.080 
Epoch 364/1000 
	 loss: 42.2270, MinusLogProbMetric: 42.2270, val_loss: 42.4590, val_MinusLogProbMetric: 42.4590

Epoch 364: val_loss did not improve from 42.45421
196/196 - 70s - loss: 42.2270 - MinusLogProbMetric: 42.2270 - val_loss: 42.4590 - val_MinusLogProbMetric: 42.4590 - lr: 1.6935e-08 - 70s/epoch - 357ms/step
Epoch 365/1000
2023-10-10 09:30:18.247 
Epoch 365/1000 
	 loss: 42.2286, MinusLogProbMetric: 42.2286, val_loss: 42.4567, val_MinusLogProbMetric: 42.4567

Epoch 365: val_loss did not improve from 42.45421
196/196 - 69s - loss: 42.2286 - MinusLogProbMetric: 42.2286 - val_loss: 42.4567 - val_MinusLogProbMetric: 42.4567 - lr: 1.6935e-08 - 69s/epoch - 353ms/step
Epoch 366/1000
2023-10-10 09:31:34.495 
Epoch 366/1000 
	 loss: 42.2294, MinusLogProbMetric: 42.2294, val_loss: 42.4574, val_MinusLogProbMetric: 42.4574

Epoch 366: val_loss did not improve from 42.45421
196/196 - 76s - loss: 42.2294 - MinusLogProbMetric: 42.2294 - val_loss: 42.4574 - val_MinusLogProbMetric: 42.4574 - lr: 1.6935e-08 - 76s/epoch - 389ms/step
Epoch 367/1000
2023-10-10 09:32:48.298 
Epoch 367/1000 
	 loss: 42.2281, MinusLogProbMetric: 42.2281, val_loss: 42.4545, val_MinusLogProbMetric: 42.4545

Epoch 367: val_loss did not improve from 42.45421
196/196 - 74s - loss: 42.2281 - MinusLogProbMetric: 42.2281 - val_loss: 42.4545 - val_MinusLogProbMetric: 42.4545 - lr: 1.6935e-08 - 74s/epoch - 377ms/step
Epoch 368/1000
2023-10-10 09:33:58.506 
Epoch 368/1000 
	 loss: 42.2268, MinusLogProbMetric: 42.2268, val_loss: 42.4542, val_MinusLogProbMetric: 42.4542

Epoch 368: val_loss improved from 42.45421 to 42.45419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 71s - loss: 42.2268 - MinusLogProbMetric: 42.2268 - val_loss: 42.4542 - val_MinusLogProbMetric: 42.4542 - lr: 1.6935e-08 - 71s/epoch - 362ms/step
Epoch 369/1000
2023-10-10 09:35:08.679 
Epoch 369/1000 
	 loss: 42.2261, MinusLogProbMetric: 42.2261, val_loss: 42.4560, val_MinusLogProbMetric: 42.4560

Epoch 369: val_loss did not improve from 42.45419
196/196 - 69s - loss: 42.2261 - MinusLogProbMetric: 42.2261 - val_loss: 42.4560 - val_MinusLogProbMetric: 42.4560 - lr: 1.6935e-08 - 69s/epoch - 354ms/step
Epoch 370/1000
2023-10-10 09:36:17.869 
Epoch 370/1000 
	 loss: 42.2262, MinusLogProbMetric: 42.2262, val_loss: 42.4534, val_MinusLogProbMetric: 42.4534

Epoch 370: val_loss improved from 42.45419 to 42.45340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 70s - loss: 42.2262 - MinusLogProbMetric: 42.2262 - val_loss: 42.4534 - val_MinusLogProbMetric: 42.4534 - lr: 1.6935e-08 - 70s/epoch - 359ms/step
Epoch 371/1000
2023-10-10 09:37:30.702 
Epoch 371/1000 
	 loss: 42.2248, MinusLogProbMetric: 42.2248, val_loss: 42.4566, val_MinusLogProbMetric: 42.4566

Epoch 371: val_loss did not improve from 42.45340
196/196 - 72s - loss: 42.2248 - MinusLogProbMetric: 42.2248 - val_loss: 42.4566 - val_MinusLogProbMetric: 42.4566 - lr: 1.6935e-08 - 72s/epoch - 365ms/step
Epoch 372/1000
2023-10-10 09:38:40.468 
Epoch 372/1000 
	 loss: 42.2259, MinusLogProbMetric: 42.2259, val_loss: 42.4549, val_MinusLogProbMetric: 42.4549

Epoch 372: val_loss did not improve from 42.45340
196/196 - 70s - loss: 42.2259 - MinusLogProbMetric: 42.2259 - val_loss: 42.4549 - val_MinusLogProbMetric: 42.4549 - lr: 1.6935e-08 - 70s/epoch - 356ms/step
Epoch 373/1000
2023-10-10 09:39:50.623 
Epoch 373/1000 
	 loss: 42.2240, MinusLogProbMetric: 42.2240, val_loss: 42.4535, val_MinusLogProbMetric: 42.4535

Epoch 373: val_loss did not improve from 42.45340
196/196 - 70s - loss: 42.2240 - MinusLogProbMetric: 42.2240 - val_loss: 42.4535 - val_MinusLogProbMetric: 42.4535 - lr: 1.6935e-08 - 70s/epoch - 358ms/step
Epoch 374/1000
2023-10-10 09:40:59.901 
Epoch 374/1000 
	 loss: 42.2250, MinusLogProbMetric: 42.2250, val_loss: 42.4524, val_MinusLogProbMetric: 42.4524

Epoch 374: val_loss improved from 42.45340 to 42.45236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 70s - loss: 42.2250 - MinusLogProbMetric: 42.2250 - val_loss: 42.4524 - val_MinusLogProbMetric: 42.4524 - lr: 1.6935e-08 - 70s/epoch - 360ms/step
Epoch 375/1000
2023-10-10 09:42:10.974 
Epoch 375/1000 
	 loss: 42.2230, MinusLogProbMetric: 42.2230, val_loss: 42.4532, val_MinusLogProbMetric: 42.4532

Epoch 375: val_loss did not improve from 42.45236
196/196 - 70s - loss: 42.2230 - MinusLogProbMetric: 42.2230 - val_loss: 42.4532 - val_MinusLogProbMetric: 42.4532 - lr: 1.6935e-08 - 70s/epoch - 356ms/step
Epoch 376/1000
2023-10-10 09:43:21.521 
Epoch 376/1000 
	 loss: 42.2233, MinusLogProbMetric: 42.2233, val_loss: 42.4522, val_MinusLogProbMetric: 42.4522

Epoch 376: val_loss improved from 42.45236 to 42.45216, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 72s - loss: 42.2233 - MinusLogProbMetric: 42.2233 - val_loss: 42.4522 - val_MinusLogProbMetric: 42.4522 - lr: 1.6935e-08 - 72s/epoch - 367ms/step
Epoch 377/1000
2023-10-10 09:44:31.737 
Epoch 377/1000 
	 loss: 42.2257, MinusLogProbMetric: 42.2257, val_loss: 42.4605, val_MinusLogProbMetric: 42.4605

Epoch 377: val_loss did not improve from 42.45216
196/196 - 69s - loss: 42.2257 - MinusLogProbMetric: 42.2257 - val_loss: 42.4605 - val_MinusLogProbMetric: 42.4605 - lr: 1.6935e-08 - 69s/epoch - 351ms/step
Epoch 378/1000
2023-10-10 09:45:42.029 
Epoch 378/1000 
	 loss: 42.2271, MinusLogProbMetric: 42.2271, val_loss: 42.4559, val_MinusLogProbMetric: 42.4559

Epoch 378: val_loss did not improve from 42.45216
196/196 - 70s - loss: 42.2271 - MinusLogProbMetric: 42.2271 - val_loss: 42.4559 - val_MinusLogProbMetric: 42.4559 - lr: 1.6935e-08 - 70s/epoch - 359ms/step
Epoch 379/1000
2023-10-10 09:46:54.037 
Epoch 379/1000 
	 loss: 42.2242, MinusLogProbMetric: 42.2242, val_loss: 42.4546, val_MinusLogProbMetric: 42.4546

Epoch 379: val_loss did not improve from 42.45216
196/196 - 72s - loss: 42.2242 - MinusLogProbMetric: 42.2242 - val_loss: 42.4546 - val_MinusLogProbMetric: 42.4546 - lr: 1.6935e-08 - 72s/epoch - 367ms/step
Epoch 380/1000
2023-10-10 09:48:04.703 
Epoch 380/1000 
	 loss: 42.2255, MinusLogProbMetric: 42.2255, val_loss: 42.4531, val_MinusLogProbMetric: 42.4531

Epoch 380: val_loss did not improve from 42.45216
196/196 - 71s - loss: 42.2255 - MinusLogProbMetric: 42.2255 - val_loss: 42.4531 - val_MinusLogProbMetric: 42.4531 - lr: 1.6935e-08 - 71s/epoch - 361ms/step
Epoch 381/1000
2023-10-10 09:49:20.486 
Epoch 381/1000 
	 loss: 42.2255, MinusLogProbMetric: 42.2255, val_loss: 42.4494, val_MinusLogProbMetric: 42.4494

Epoch 381: val_loss improved from 42.45216 to 42.44938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 77s - loss: 42.2255 - MinusLogProbMetric: 42.2255 - val_loss: 42.4494 - val_MinusLogProbMetric: 42.4494 - lr: 1.6935e-08 - 77s/epoch - 395ms/step
Epoch 382/1000
2023-10-10 09:50:35.971 
Epoch 382/1000 
	 loss: 42.2252, MinusLogProbMetric: 42.2252, val_loss: 42.4505, val_MinusLogProbMetric: 42.4505

Epoch 382: val_loss did not improve from 42.44938
196/196 - 74s - loss: 42.2252 - MinusLogProbMetric: 42.2252 - val_loss: 42.4505 - val_MinusLogProbMetric: 42.4505 - lr: 1.6935e-08 - 74s/epoch - 377ms/step
Epoch 383/1000
2023-10-10 09:51:47.098 
Epoch 383/1000 
	 loss: 42.2245, MinusLogProbMetric: 42.2245, val_loss: 42.4545, val_MinusLogProbMetric: 42.4545

Epoch 383: val_loss did not improve from 42.44938
196/196 - 71s - loss: 42.2245 - MinusLogProbMetric: 42.2245 - val_loss: 42.4545 - val_MinusLogProbMetric: 42.4545 - lr: 1.6935e-08 - 71s/epoch - 363ms/step
Epoch 384/1000
2023-10-10 09:52:56.040 
Epoch 384/1000 
	 loss: 42.2227, MinusLogProbMetric: 42.2227, val_loss: 42.4523, val_MinusLogProbMetric: 42.4523

Epoch 384: val_loss did not improve from 42.44938
196/196 - 69s - loss: 42.2227 - MinusLogProbMetric: 42.2227 - val_loss: 42.4523 - val_MinusLogProbMetric: 42.4523 - lr: 1.6935e-08 - 69s/epoch - 352ms/step
Epoch 385/1000
2023-10-10 09:54:04.823 
Epoch 385/1000 
	 loss: 42.2226, MinusLogProbMetric: 42.2226, val_loss: 42.4511, val_MinusLogProbMetric: 42.4511

Epoch 385: val_loss did not improve from 42.44938
196/196 - 69s - loss: 42.2226 - MinusLogProbMetric: 42.2226 - val_loss: 42.4511 - val_MinusLogProbMetric: 42.4511 - lr: 1.6935e-08 - 69s/epoch - 351ms/step
Epoch 386/1000
2023-10-10 09:55:13.680 
Epoch 386/1000 
	 loss: 42.2209, MinusLogProbMetric: 42.2209, val_loss: 42.4532, val_MinusLogProbMetric: 42.4532

Epoch 386: val_loss did not improve from 42.44938
196/196 - 69s - loss: 42.2209 - MinusLogProbMetric: 42.2209 - val_loss: 42.4532 - val_MinusLogProbMetric: 42.4532 - lr: 1.6935e-08 - 69s/epoch - 351ms/step
Epoch 387/1000
2023-10-10 09:56:22.930 
Epoch 387/1000 
	 loss: 42.2221, MinusLogProbMetric: 42.2221, val_loss: 42.4457, val_MinusLogProbMetric: 42.4457

Epoch 387: val_loss improved from 42.44938 to 42.44575, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 71s - loss: 42.2221 - MinusLogProbMetric: 42.2221 - val_loss: 42.4457 - val_MinusLogProbMetric: 42.4457 - lr: 1.6935e-08 - 71s/epoch - 361ms/step
Epoch 388/1000
2023-10-10 09:57:37.538 
Epoch 388/1000 
	 loss: 42.2229, MinusLogProbMetric: 42.2229, val_loss: 42.4511, val_MinusLogProbMetric: 42.4511

Epoch 388: val_loss did not improve from 42.44575
196/196 - 73s - loss: 42.2229 - MinusLogProbMetric: 42.2229 - val_loss: 42.4511 - val_MinusLogProbMetric: 42.4511 - lr: 1.6935e-08 - 73s/epoch - 373ms/step
Epoch 389/1000
2023-10-10 09:58:48.959 
Epoch 389/1000 
	 loss: 42.2210, MinusLogProbMetric: 42.2210, val_loss: 42.4529, val_MinusLogProbMetric: 42.4529

Epoch 389: val_loss did not improve from 42.44575
196/196 - 71s - loss: 42.2210 - MinusLogProbMetric: 42.2210 - val_loss: 42.4529 - val_MinusLogProbMetric: 42.4529 - lr: 1.6935e-08 - 71s/epoch - 364ms/step
Epoch 390/1000
2023-10-10 09:59:56.646 
Epoch 390/1000 
	 loss: 42.2203, MinusLogProbMetric: 42.2203, val_loss: 42.4476, val_MinusLogProbMetric: 42.4476

Epoch 390: val_loss did not improve from 42.44575
196/196 - 68s - loss: 42.2203 - MinusLogProbMetric: 42.2203 - val_loss: 42.4476 - val_MinusLogProbMetric: 42.4476 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 391/1000
2023-10-10 10:01:04.014 
Epoch 391/1000 
	 loss: 42.2195, MinusLogProbMetric: 42.2195, val_loss: 42.4496, val_MinusLogProbMetric: 42.4496

Epoch 391: val_loss did not improve from 42.44575
196/196 - 67s - loss: 42.2195 - MinusLogProbMetric: 42.2195 - val_loss: 42.4496 - val_MinusLogProbMetric: 42.4496 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 392/1000
2023-10-10 10:02:13.641 
Epoch 392/1000 
	 loss: 42.2216, MinusLogProbMetric: 42.2216, val_loss: 42.4493, val_MinusLogProbMetric: 42.4493

Epoch 392: val_loss did not improve from 42.44575
196/196 - 70s - loss: 42.2216 - MinusLogProbMetric: 42.2216 - val_loss: 42.4493 - val_MinusLogProbMetric: 42.4493 - lr: 1.6935e-08 - 70s/epoch - 355ms/step
Epoch 393/1000
2023-10-10 10:03:22.596 
Epoch 393/1000 
	 loss: 42.2208, MinusLogProbMetric: 42.2208, val_loss: 42.4531, val_MinusLogProbMetric: 42.4531

Epoch 393: val_loss did not improve from 42.44575
196/196 - 69s - loss: 42.2208 - MinusLogProbMetric: 42.2208 - val_loss: 42.4531 - val_MinusLogProbMetric: 42.4531 - lr: 1.6935e-08 - 69s/epoch - 352ms/step
Epoch 394/1000
2023-10-10 10:04:29.385 
Epoch 394/1000 
	 loss: 42.2205, MinusLogProbMetric: 42.2205, val_loss: 42.4498, val_MinusLogProbMetric: 42.4498

Epoch 394: val_loss did not improve from 42.44575
196/196 - 67s - loss: 42.2205 - MinusLogProbMetric: 42.2205 - val_loss: 42.4498 - val_MinusLogProbMetric: 42.4498 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 395/1000
2023-10-10 10:05:36.707 
Epoch 395/1000 
	 loss: 42.2187, MinusLogProbMetric: 42.2187, val_loss: 42.4478, val_MinusLogProbMetric: 42.4478

Epoch 395: val_loss did not improve from 42.44575
196/196 - 67s - loss: 42.2187 - MinusLogProbMetric: 42.2187 - val_loss: 42.4478 - val_MinusLogProbMetric: 42.4478 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 396/1000
2023-10-10 10:06:43.490 
Epoch 396/1000 
	 loss: 42.2191, MinusLogProbMetric: 42.2191, val_loss: 42.4514, val_MinusLogProbMetric: 42.4514

Epoch 396: val_loss did not improve from 42.44575
196/196 - 67s - loss: 42.2191 - MinusLogProbMetric: 42.2191 - val_loss: 42.4514 - val_MinusLogProbMetric: 42.4514 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 397/1000
2023-10-10 10:07:52.945 
Epoch 397/1000 
	 loss: 42.2209, MinusLogProbMetric: 42.2209, val_loss: 42.4477, val_MinusLogProbMetric: 42.4477

Epoch 397: val_loss did not improve from 42.44575
196/196 - 69s - loss: 42.2209 - MinusLogProbMetric: 42.2209 - val_loss: 42.4477 - val_MinusLogProbMetric: 42.4477 - lr: 1.6935e-08 - 69s/epoch - 354ms/step
Epoch 398/1000
2023-10-10 10:09:02.345 
Epoch 398/1000 
	 loss: 42.2181, MinusLogProbMetric: 42.2181, val_loss: 42.4476, val_MinusLogProbMetric: 42.4476

Epoch 398: val_loss did not improve from 42.44575
196/196 - 69s - loss: 42.2181 - MinusLogProbMetric: 42.2181 - val_loss: 42.4476 - val_MinusLogProbMetric: 42.4476 - lr: 1.6935e-08 - 69s/epoch - 354ms/step
Epoch 399/1000
2023-10-10 10:10:11.069 
Epoch 399/1000 
	 loss: 42.2187, MinusLogProbMetric: 42.2187, val_loss: 42.4474, val_MinusLogProbMetric: 42.4474

Epoch 399: val_loss did not improve from 42.44575
196/196 - 69s - loss: 42.2187 - MinusLogProbMetric: 42.2187 - val_loss: 42.4474 - val_MinusLogProbMetric: 42.4474 - lr: 1.6935e-08 - 69s/epoch - 351ms/step
Epoch 400/1000
2023-10-10 10:11:19.987 
Epoch 400/1000 
	 loss: 42.2199, MinusLogProbMetric: 42.2199, val_loss: 42.4495, val_MinusLogProbMetric: 42.4495

Epoch 400: val_loss did not improve from 42.44575
196/196 - 69s - loss: 42.2199 - MinusLogProbMetric: 42.2199 - val_loss: 42.4495 - val_MinusLogProbMetric: 42.4495 - lr: 1.6935e-08 - 69s/epoch - 352ms/step
Epoch 401/1000
2023-10-10 10:12:27.714 
Epoch 401/1000 
	 loss: 42.2259, MinusLogProbMetric: 42.2259, val_loss: 42.4529, val_MinusLogProbMetric: 42.4529

Epoch 401: val_loss did not improve from 42.44575
196/196 - 68s - loss: 42.2259 - MinusLogProbMetric: 42.2259 - val_loss: 42.4529 - val_MinusLogProbMetric: 42.4529 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 402/1000
2023-10-10 10:13:35.757 
Epoch 402/1000 
	 loss: 42.2267, MinusLogProbMetric: 42.2267, val_loss: 42.4518, val_MinusLogProbMetric: 42.4518

Epoch 402: val_loss did not improve from 42.44575
196/196 - 68s - loss: 42.2267 - MinusLogProbMetric: 42.2267 - val_loss: 42.4518 - val_MinusLogProbMetric: 42.4518 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 403/1000
2023-10-10 10:14:45.200 
Epoch 403/1000 
	 loss: 42.2241, MinusLogProbMetric: 42.2241, val_loss: 42.4534, val_MinusLogProbMetric: 42.4534

Epoch 403: val_loss did not improve from 42.44575
196/196 - 69s - loss: 42.2241 - MinusLogProbMetric: 42.2241 - val_loss: 42.4534 - val_MinusLogProbMetric: 42.4534 - lr: 1.6935e-08 - 69s/epoch - 354ms/step
Epoch 404/1000
2023-10-10 10:15:54.479 
Epoch 404/1000 
	 loss: 42.2224, MinusLogProbMetric: 42.2224, val_loss: 42.4519, val_MinusLogProbMetric: 42.4519

Epoch 404: val_loss did not improve from 42.44575
196/196 - 69s - loss: 42.2224 - MinusLogProbMetric: 42.2224 - val_loss: 42.4519 - val_MinusLogProbMetric: 42.4519 - lr: 1.6935e-08 - 69s/epoch - 353ms/step
Epoch 405/1000
2023-10-10 10:17:03.187 
Epoch 405/1000 
	 loss: 42.2214, MinusLogProbMetric: 42.2214, val_loss: 42.4469, val_MinusLogProbMetric: 42.4469

Epoch 405: val_loss did not improve from 42.44575
196/196 - 69s - loss: 42.2214 - MinusLogProbMetric: 42.2214 - val_loss: 42.4469 - val_MinusLogProbMetric: 42.4469 - lr: 1.6935e-08 - 69s/epoch - 351ms/step
Epoch 406/1000
2023-10-10 10:18:12.054 
Epoch 406/1000 
	 loss: 42.2225, MinusLogProbMetric: 42.2225, val_loss: 42.4456, val_MinusLogProbMetric: 42.4456

Epoch 406: val_loss improved from 42.44575 to 42.44564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 70s - loss: 42.2225 - MinusLogProbMetric: 42.2225 - val_loss: 42.4456 - val_MinusLogProbMetric: 42.4456 - lr: 1.6935e-08 - 70s/epoch - 357ms/step
Epoch 407/1000
2023-10-10 10:19:21.694 
Epoch 407/1000 
	 loss: 42.2195, MinusLogProbMetric: 42.2195, val_loss: 42.4481, val_MinusLogProbMetric: 42.4481

Epoch 407: val_loss did not improve from 42.44564
196/196 - 68s - loss: 42.2195 - MinusLogProbMetric: 42.2195 - val_loss: 42.4481 - val_MinusLogProbMetric: 42.4481 - lr: 1.6935e-08 - 68s/epoch - 349ms/step
Epoch 408/1000
2023-10-10 10:20:28.970 
Epoch 408/1000 
	 loss: 42.2198, MinusLogProbMetric: 42.2198, val_loss: 42.4482, val_MinusLogProbMetric: 42.4482

Epoch 408: val_loss did not improve from 42.44564
196/196 - 67s - loss: 42.2198 - MinusLogProbMetric: 42.2198 - val_loss: 42.4482 - val_MinusLogProbMetric: 42.4482 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 409/1000
2023-10-10 10:21:37.897 
Epoch 409/1000 
	 loss: 42.2211, MinusLogProbMetric: 42.2211, val_loss: 42.4422, val_MinusLogProbMetric: 42.4422

Epoch 409: val_loss improved from 42.44564 to 42.44224, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 70s - loss: 42.2211 - MinusLogProbMetric: 42.2211 - val_loss: 42.4422 - val_MinusLogProbMetric: 42.4422 - lr: 1.6935e-08 - 70s/epoch - 357ms/step
Epoch 410/1000
2023-10-10 10:22:47.305 
Epoch 410/1000 
	 loss: 42.2212, MinusLogProbMetric: 42.2212, val_loss: 42.4466, val_MinusLogProbMetric: 42.4466

Epoch 410: val_loss did not improve from 42.44224
196/196 - 68s - loss: 42.2212 - MinusLogProbMetric: 42.2212 - val_loss: 42.4466 - val_MinusLogProbMetric: 42.4466 - lr: 1.6935e-08 - 68s/epoch - 349ms/step
Epoch 411/1000
2023-10-10 10:23:57.181 
Epoch 411/1000 
	 loss: 42.2191, MinusLogProbMetric: 42.2191, val_loss: 42.4454, val_MinusLogProbMetric: 42.4454

Epoch 411: val_loss did not improve from 42.44224
196/196 - 70s - loss: 42.2191 - MinusLogProbMetric: 42.2191 - val_loss: 42.4454 - val_MinusLogProbMetric: 42.4454 - lr: 1.6935e-08 - 70s/epoch - 357ms/step
Epoch 412/1000
2023-10-10 10:25:07.676 
Epoch 412/1000 
	 loss: 42.2193, MinusLogProbMetric: 42.2193, val_loss: 42.4440, val_MinusLogProbMetric: 42.4440

Epoch 412: val_loss did not improve from 42.44224
196/196 - 70s - loss: 42.2193 - MinusLogProbMetric: 42.2193 - val_loss: 42.4440 - val_MinusLogProbMetric: 42.4440 - lr: 1.6935e-08 - 70s/epoch - 360ms/step
Epoch 413/1000
2023-10-10 10:26:16.777 
Epoch 413/1000 
	 loss: 42.2206, MinusLogProbMetric: 42.2206, val_loss: 42.4444, val_MinusLogProbMetric: 42.4444

Epoch 413: val_loss did not improve from 42.44224
196/196 - 69s - loss: 42.2206 - MinusLogProbMetric: 42.2206 - val_loss: 42.4444 - val_MinusLogProbMetric: 42.4444 - lr: 1.6935e-08 - 69s/epoch - 352ms/step
Epoch 414/1000
2023-10-10 10:27:25.868 
Epoch 414/1000 
	 loss: 42.2199, MinusLogProbMetric: 42.2199, val_loss: 42.4439, val_MinusLogProbMetric: 42.4439

Epoch 414: val_loss did not improve from 42.44224
196/196 - 69s - loss: 42.2199 - MinusLogProbMetric: 42.2199 - val_loss: 42.4439 - val_MinusLogProbMetric: 42.4439 - lr: 1.6935e-08 - 69s/epoch - 353ms/step
Epoch 415/1000
2023-10-10 10:28:35.029 
Epoch 415/1000 
	 loss: 42.2197, MinusLogProbMetric: 42.2197, val_loss: 42.4466, val_MinusLogProbMetric: 42.4466

Epoch 415: val_loss did not improve from 42.44224
196/196 - 69s - loss: 42.2197 - MinusLogProbMetric: 42.2197 - val_loss: 42.4466 - val_MinusLogProbMetric: 42.4466 - lr: 1.6935e-08 - 69s/epoch - 353ms/step
Epoch 416/1000
2023-10-10 10:29:42.878 
Epoch 416/1000 
	 loss: 42.2183, MinusLogProbMetric: 42.2183, val_loss: 42.4458, val_MinusLogProbMetric: 42.4458

Epoch 416: val_loss did not improve from 42.44224
196/196 - 68s - loss: 42.2183 - MinusLogProbMetric: 42.2183 - val_loss: 42.4458 - val_MinusLogProbMetric: 42.4458 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 417/1000
2023-10-10 10:30:50.883 
Epoch 417/1000 
	 loss: 42.2166, MinusLogProbMetric: 42.2166, val_loss: 42.4461, val_MinusLogProbMetric: 42.4461

Epoch 417: val_loss did not improve from 42.44224
196/196 - 68s - loss: 42.2166 - MinusLogProbMetric: 42.2166 - val_loss: 42.4461 - val_MinusLogProbMetric: 42.4461 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 418/1000
2023-10-10 10:31:58.927 
Epoch 418/1000 
	 loss: 42.2170, MinusLogProbMetric: 42.2170, val_loss: 42.4453, val_MinusLogProbMetric: 42.4453

Epoch 418: val_loss did not improve from 42.44224
196/196 - 68s - loss: 42.2170 - MinusLogProbMetric: 42.2170 - val_loss: 42.4453 - val_MinusLogProbMetric: 42.4453 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 419/1000
2023-10-10 10:33:06.892 
Epoch 419/1000 
	 loss: 42.2180, MinusLogProbMetric: 42.2180, val_loss: 42.4463, val_MinusLogProbMetric: 42.4463

Epoch 419: val_loss did not improve from 42.44224
196/196 - 68s - loss: 42.2180 - MinusLogProbMetric: 42.2180 - val_loss: 42.4463 - val_MinusLogProbMetric: 42.4463 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 420/1000
2023-10-10 10:34:14.542 
Epoch 420/1000 
	 loss: 42.2181, MinusLogProbMetric: 42.2181, val_loss: 42.4443, val_MinusLogProbMetric: 42.4443

Epoch 420: val_loss did not improve from 42.44224
196/196 - 68s - loss: 42.2181 - MinusLogProbMetric: 42.2181 - val_loss: 42.4443 - val_MinusLogProbMetric: 42.4443 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 421/1000
2023-10-10 10:35:22.402 
Epoch 421/1000 
	 loss: 42.2175, MinusLogProbMetric: 42.2175, val_loss: 42.4438, val_MinusLogProbMetric: 42.4438

Epoch 421: val_loss did not improve from 42.44224
196/196 - 68s - loss: 42.2175 - MinusLogProbMetric: 42.2175 - val_loss: 42.4438 - val_MinusLogProbMetric: 42.4438 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 422/1000
2023-10-10 10:36:32.292 
Epoch 422/1000 
	 loss: 42.2203, MinusLogProbMetric: 42.2203, val_loss: 42.4477, val_MinusLogProbMetric: 42.4477

Epoch 422: val_loss did not improve from 42.44224
196/196 - 70s - loss: 42.2203 - MinusLogProbMetric: 42.2203 - val_loss: 42.4477 - val_MinusLogProbMetric: 42.4477 - lr: 1.6935e-08 - 70s/epoch - 357ms/step
Epoch 423/1000
2023-10-10 10:37:39.678 
Epoch 423/1000 
	 loss: 42.2224, MinusLogProbMetric: 42.2224, val_loss: 42.4446, val_MinusLogProbMetric: 42.4446

Epoch 423: val_loss did not improve from 42.44224
196/196 - 67s - loss: 42.2224 - MinusLogProbMetric: 42.2224 - val_loss: 42.4446 - val_MinusLogProbMetric: 42.4446 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 424/1000
2023-10-10 10:38:47.265 
Epoch 424/1000 
	 loss: 42.2194, MinusLogProbMetric: 42.2194, val_loss: 42.4459, val_MinusLogProbMetric: 42.4459

Epoch 424: val_loss did not improve from 42.44224
196/196 - 68s - loss: 42.2194 - MinusLogProbMetric: 42.2194 - val_loss: 42.4459 - val_MinusLogProbMetric: 42.4459 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 425/1000
2023-10-10 10:39:54.118 
Epoch 425/1000 
	 loss: 42.2193, MinusLogProbMetric: 42.2193, val_loss: 42.4458, val_MinusLogProbMetric: 42.4458

Epoch 425: val_loss did not improve from 42.44224
196/196 - 67s - loss: 42.2193 - MinusLogProbMetric: 42.2193 - val_loss: 42.4458 - val_MinusLogProbMetric: 42.4458 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 426/1000
2023-10-10 10:41:02.560 
Epoch 426/1000 
	 loss: 42.2192, MinusLogProbMetric: 42.2192, val_loss: 42.4433, val_MinusLogProbMetric: 42.4433

Epoch 426: val_loss did not improve from 42.44224
196/196 - 68s - loss: 42.2192 - MinusLogProbMetric: 42.2192 - val_loss: 42.4433 - val_MinusLogProbMetric: 42.4433 - lr: 1.6935e-08 - 68s/epoch - 349ms/step
Epoch 427/1000
2023-10-10 10:42:09.990 
Epoch 427/1000 
	 loss: 42.2202, MinusLogProbMetric: 42.2202, val_loss: 42.4457, val_MinusLogProbMetric: 42.4457

Epoch 427: val_loss did not improve from 42.44224
196/196 - 67s - loss: 42.2202 - MinusLogProbMetric: 42.2202 - val_loss: 42.4457 - val_MinusLogProbMetric: 42.4457 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 428/1000
2023-10-10 10:43:18.351 
Epoch 428/1000 
	 loss: 42.2199, MinusLogProbMetric: 42.2199, val_loss: 42.4450, val_MinusLogProbMetric: 42.4450

Epoch 428: val_loss did not improve from 42.44224
196/196 - 68s - loss: 42.2199 - MinusLogProbMetric: 42.2199 - val_loss: 42.4450 - val_MinusLogProbMetric: 42.4450 - lr: 1.6935e-08 - 68s/epoch - 349ms/step
Epoch 429/1000
2023-10-10 10:44:24.914 
Epoch 429/1000 
	 loss: 42.2215, MinusLogProbMetric: 42.2215, val_loss: 42.4459, val_MinusLogProbMetric: 42.4459

Epoch 429: val_loss did not improve from 42.44224
196/196 - 67s - loss: 42.2215 - MinusLogProbMetric: 42.2215 - val_loss: 42.4459 - val_MinusLogProbMetric: 42.4459 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 430/1000
2023-10-10 10:45:31.663 
Epoch 430/1000 
	 loss: 42.2212, MinusLogProbMetric: 42.2212, val_loss: 42.4452, val_MinusLogProbMetric: 42.4452

Epoch 430: val_loss did not improve from 42.44224
196/196 - 67s - loss: 42.2212 - MinusLogProbMetric: 42.2212 - val_loss: 42.4452 - val_MinusLogProbMetric: 42.4452 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 431/1000
2023-10-10 10:46:39.364 
Epoch 431/1000 
	 loss: 42.2187, MinusLogProbMetric: 42.2187, val_loss: 42.4462, val_MinusLogProbMetric: 42.4462

Epoch 431: val_loss did not improve from 42.44224
196/196 - 68s - loss: 42.2187 - MinusLogProbMetric: 42.2187 - val_loss: 42.4462 - val_MinusLogProbMetric: 42.4462 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 432/1000
2023-10-10 10:47:46.307 
Epoch 432/1000 
	 loss: 42.2192, MinusLogProbMetric: 42.2192, val_loss: 42.4429, val_MinusLogProbMetric: 42.4429

Epoch 432: val_loss did not improve from 42.44224
196/196 - 67s - loss: 42.2192 - MinusLogProbMetric: 42.2192 - val_loss: 42.4429 - val_MinusLogProbMetric: 42.4429 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 433/1000
2023-10-10 10:48:53.413 
Epoch 433/1000 
	 loss: 42.2164, MinusLogProbMetric: 42.2164, val_loss: 42.4417, val_MinusLogProbMetric: 42.4417

Epoch 433: val_loss improved from 42.44224 to 42.44175, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.2164 - MinusLogProbMetric: 42.2164 - val_loss: 42.4417 - val_MinusLogProbMetric: 42.4417 - lr: 1.6935e-08 - 68s/epoch - 348ms/step
Epoch 434/1000
2023-10-10 10:50:01.523 
Epoch 434/1000 
	 loss: 42.2172, MinusLogProbMetric: 42.2172, val_loss: 42.4418, val_MinusLogProbMetric: 42.4418

Epoch 434: val_loss did not improve from 42.44175
196/196 - 67s - loss: 42.2172 - MinusLogProbMetric: 42.2172 - val_loss: 42.4418 - val_MinusLogProbMetric: 42.4418 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 435/1000
2023-10-10 10:51:08.537 
Epoch 435/1000 
	 loss: 42.2178, MinusLogProbMetric: 42.2178, val_loss: 42.4406, val_MinusLogProbMetric: 42.4406

Epoch 435: val_loss improved from 42.44175 to 42.44056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.2178 - MinusLogProbMetric: 42.2178 - val_loss: 42.4406 - val_MinusLogProbMetric: 42.4406 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 436/1000
2023-10-10 10:52:17.082 
Epoch 436/1000 
	 loss: 42.2155, MinusLogProbMetric: 42.2155, val_loss: 42.4429, val_MinusLogProbMetric: 42.4429

Epoch 436: val_loss did not improve from 42.44056
196/196 - 67s - loss: 42.2155 - MinusLogProbMetric: 42.2155 - val_loss: 42.4429 - val_MinusLogProbMetric: 42.4429 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 437/1000
2023-10-10 10:53:23.854 
Epoch 437/1000 
	 loss: 42.2163, MinusLogProbMetric: 42.2163, val_loss: 42.4434, val_MinusLogProbMetric: 42.4434

Epoch 437: val_loss did not improve from 42.44056
196/196 - 67s - loss: 42.2163 - MinusLogProbMetric: 42.2163 - val_loss: 42.4434 - val_MinusLogProbMetric: 42.4434 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 438/1000
2023-10-10 10:54:30.107 
Epoch 438/1000 
	 loss: 42.2156, MinusLogProbMetric: 42.2156, val_loss: 42.4437, val_MinusLogProbMetric: 42.4437

Epoch 438: val_loss did not improve from 42.44056
196/196 - 66s - loss: 42.2156 - MinusLogProbMetric: 42.2156 - val_loss: 42.4437 - val_MinusLogProbMetric: 42.4437 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 439/1000
2023-10-10 10:55:36.842 
Epoch 439/1000 
	 loss: 42.2146, MinusLogProbMetric: 42.2146, val_loss: 42.4478, val_MinusLogProbMetric: 42.4478

Epoch 439: val_loss did not improve from 42.44056
196/196 - 67s - loss: 42.2146 - MinusLogProbMetric: 42.2146 - val_loss: 42.4478 - val_MinusLogProbMetric: 42.4478 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 440/1000
2023-10-10 10:56:42.793 
Epoch 440/1000 
	 loss: 42.2141, MinusLogProbMetric: 42.2141, val_loss: 42.4428, val_MinusLogProbMetric: 42.4428

Epoch 440: val_loss did not improve from 42.44056
196/196 - 66s - loss: 42.2141 - MinusLogProbMetric: 42.2141 - val_loss: 42.4428 - val_MinusLogProbMetric: 42.4428 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 441/1000
2023-10-10 10:57:49.732 
Epoch 441/1000 
	 loss: 42.2146, MinusLogProbMetric: 42.2146, val_loss: 42.4411, val_MinusLogProbMetric: 42.4411

Epoch 441: val_loss did not improve from 42.44056
196/196 - 67s - loss: 42.2146 - MinusLogProbMetric: 42.2146 - val_loss: 42.4411 - val_MinusLogProbMetric: 42.4411 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 442/1000
2023-10-10 10:58:56.183 
Epoch 442/1000 
	 loss: 42.2179, MinusLogProbMetric: 42.2179, val_loss: 42.4475, val_MinusLogProbMetric: 42.4475

Epoch 442: val_loss did not improve from 42.44056
196/196 - 66s - loss: 42.2179 - MinusLogProbMetric: 42.2179 - val_loss: 42.4475 - val_MinusLogProbMetric: 42.4475 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 443/1000
2023-10-10 11:00:03.471 
Epoch 443/1000 
	 loss: 42.2210, MinusLogProbMetric: 42.2210, val_loss: 42.4463, val_MinusLogProbMetric: 42.4463

Epoch 443: val_loss did not improve from 42.44056
196/196 - 67s - loss: 42.2210 - MinusLogProbMetric: 42.2210 - val_loss: 42.4463 - val_MinusLogProbMetric: 42.4463 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 444/1000
2023-10-10 11:01:09.414 
Epoch 444/1000 
	 loss: 42.2165, MinusLogProbMetric: 42.2165, val_loss: 42.4458, val_MinusLogProbMetric: 42.4458

Epoch 444: val_loss did not improve from 42.44056
196/196 - 66s - loss: 42.2165 - MinusLogProbMetric: 42.2165 - val_loss: 42.4458 - val_MinusLogProbMetric: 42.4458 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 445/1000
2023-10-10 11:02:16.899 
Epoch 445/1000 
	 loss: 42.2151, MinusLogProbMetric: 42.2151, val_loss: 42.4445, val_MinusLogProbMetric: 42.4445

Epoch 445: val_loss did not improve from 42.44056
196/196 - 67s - loss: 42.2151 - MinusLogProbMetric: 42.2151 - val_loss: 42.4445 - val_MinusLogProbMetric: 42.4445 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 446/1000
2023-10-10 11:03:23.006 
Epoch 446/1000 
	 loss: 42.2168, MinusLogProbMetric: 42.2168, val_loss: 42.4478, val_MinusLogProbMetric: 42.4478

Epoch 446: val_loss did not improve from 42.44056
196/196 - 66s - loss: 42.2168 - MinusLogProbMetric: 42.2168 - val_loss: 42.4478 - val_MinusLogProbMetric: 42.4478 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 447/1000
2023-10-10 11:04:29.354 
Epoch 447/1000 
	 loss: 42.2164, MinusLogProbMetric: 42.2164, val_loss: 42.4427, val_MinusLogProbMetric: 42.4427

Epoch 447: val_loss did not improve from 42.44056
196/196 - 66s - loss: 42.2164 - MinusLogProbMetric: 42.2164 - val_loss: 42.4427 - val_MinusLogProbMetric: 42.4427 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 448/1000
2023-10-10 11:05:35.605 
Epoch 448/1000 
	 loss: 42.2159, MinusLogProbMetric: 42.2159, val_loss: 42.4431, val_MinusLogProbMetric: 42.4431

Epoch 448: val_loss did not improve from 42.44056
196/196 - 66s - loss: 42.2159 - MinusLogProbMetric: 42.2159 - val_loss: 42.4431 - val_MinusLogProbMetric: 42.4431 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 449/1000
2023-10-10 11:06:42.557 
Epoch 449/1000 
	 loss: 42.2161, MinusLogProbMetric: 42.2161, val_loss: 42.4457, val_MinusLogProbMetric: 42.4457

Epoch 449: val_loss did not improve from 42.44056
196/196 - 67s - loss: 42.2161 - MinusLogProbMetric: 42.2161 - val_loss: 42.4457 - val_MinusLogProbMetric: 42.4457 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 450/1000
2023-10-10 11:07:50.812 
Epoch 450/1000 
	 loss: 42.2158, MinusLogProbMetric: 42.2158, val_loss: 42.4462, val_MinusLogProbMetric: 42.4462

Epoch 450: val_loss did not improve from 42.44056
196/196 - 68s - loss: 42.2158 - MinusLogProbMetric: 42.2158 - val_loss: 42.4462 - val_MinusLogProbMetric: 42.4462 - lr: 1.6935e-08 - 68s/epoch - 348ms/step
Epoch 451/1000
2023-10-10 11:08:57.554 
Epoch 451/1000 
	 loss: 42.2181, MinusLogProbMetric: 42.2181, val_loss: 42.4426, val_MinusLogProbMetric: 42.4426

Epoch 451: val_loss did not improve from 42.44056
196/196 - 67s - loss: 42.2181 - MinusLogProbMetric: 42.2181 - val_loss: 42.4426 - val_MinusLogProbMetric: 42.4426 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 452/1000
2023-10-10 11:10:04.946 
Epoch 452/1000 
	 loss: 42.2161, MinusLogProbMetric: 42.2161, val_loss: 42.4444, val_MinusLogProbMetric: 42.4444

Epoch 452: val_loss did not improve from 42.44056
196/196 - 67s - loss: 42.2161 - MinusLogProbMetric: 42.2161 - val_loss: 42.4444 - val_MinusLogProbMetric: 42.4444 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 453/1000
2023-10-10 11:11:12.307 
Epoch 453/1000 
	 loss: 42.2145, MinusLogProbMetric: 42.2145, val_loss: 42.4433, val_MinusLogProbMetric: 42.4433

Epoch 453: val_loss did not improve from 42.44056
196/196 - 67s - loss: 42.2145 - MinusLogProbMetric: 42.2145 - val_loss: 42.4433 - val_MinusLogProbMetric: 42.4433 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 454/1000
2023-10-10 11:12:18.842 
Epoch 454/1000 
	 loss: 42.2131, MinusLogProbMetric: 42.2131, val_loss: 42.4405, val_MinusLogProbMetric: 42.4405

Epoch 454: val_loss improved from 42.44056 to 42.44050, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 42.2131 - MinusLogProbMetric: 42.2131 - val_loss: 42.4405 - val_MinusLogProbMetric: 42.4405 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 455/1000
2023-10-10 11:13:26.376 
Epoch 455/1000 
	 loss: 42.2129, MinusLogProbMetric: 42.2129, val_loss: 42.4434, val_MinusLogProbMetric: 42.4434

Epoch 455: val_loss did not improve from 42.44050
196/196 - 67s - loss: 42.2129 - MinusLogProbMetric: 42.2129 - val_loss: 42.4434 - val_MinusLogProbMetric: 42.4434 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 456/1000
2023-10-10 11:14:32.890 
Epoch 456/1000 
	 loss: 42.2126, MinusLogProbMetric: 42.2126, val_loss: 42.4411, val_MinusLogProbMetric: 42.4411

Epoch 456: val_loss did not improve from 42.44050
196/196 - 67s - loss: 42.2126 - MinusLogProbMetric: 42.2126 - val_loss: 42.4411 - val_MinusLogProbMetric: 42.4411 - lr: 1.6935e-08 - 67s/epoch - 339ms/step
Epoch 457/1000
2023-10-10 11:15:39.198 
Epoch 457/1000 
	 loss: 42.2113, MinusLogProbMetric: 42.2113, val_loss: 42.4401, val_MinusLogProbMetric: 42.4401

Epoch 457: val_loss improved from 42.44050 to 42.44011, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 42.2113 - MinusLogProbMetric: 42.2113 - val_loss: 42.4401 - val_MinusLogProbMetric: 42.4401 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 458/1000
2023-10-10 11:16:47.091 
Epoch 458/1000 
	 loss: 42.2143, MinusLogProbMetric: 42.2143, val_loss: 42.4447, val_MinusLogProbMetric: 42.4447

Epoch 458: val_loss did not improve from 42.44011
196/196 - 67s - loss: 42.2143 - MinusLogProbMetric: 42.2143 - val_loss: 42.4447 - val_MinusLogProbMetric: 42.4447 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 459/1000
2023-10-10 11:17:54.645 
Epoch 459/1000 
	 loss: 42.2156, MinusLogProbMetric: 42.2156, val_loss: 42.4450, val_MinusLogProbMetric: 42.4450

Epoch 459: val_loss did not improve from 42.44011
196/196 - 68s - loss: 42.2156 - MinusLogProbMetric: 42.2156 - val_loss: 42.4450 - val_MinusLogProbMetric: 42.4450 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 460/1000
2023-10-10 11:19:01.192 
Epoch 460/1000 
	 loss: 42.2135, MinusLogProbMetric: 42.2135, val_loss: 42.4408, val_MinusLogProbMetric: 42.4408

Epoch 460: val_loss did not improve from 42.44011
196/196 - 67s - loss: 42.2135 - MinusLogProbMetric: 42.2135 - val_loss: 42.4408 - val_MinusLogProbMetric: 42.4408 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 461/1000
2023-10-10 11:20:07.898 
Epoch 461/1000 
	 loss: 42.2132, MinusLogProbMetric: 42.2132, val_loss: 42.4404, val_MinusLogProbMetric: 42.4404

Epoch 461: val_loss did not improve from 42.44011
196/196 - 67s - loss: 42.2132 - MinusLogProbMetric: 42.2132 - val_loss: 42.4404 - val_MinusLogProbMetric: 42.4404 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 462/1000
2023-10-10 11:21:14.751 
Epoch 462/1000 
	 loss: 42.2118, MinusLogProbMetric: 42.2118, val_loss: 42.4399, val_MinusLogProbMetric: 42.4399

Epoch 462: val_loss improved from 42.44011 to 42.43986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.2118 - MinusLogProbMetric: 42.2118 - val_loss: 42.4399 - val_MinusLogProbMetric: 42.4399 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 463/1000
2023-10-10 11:22:22.358 
Epoch 463/1000 
	 loss: 42.2116, MinusLogProbMetric: 42.2116, val_loss: 42.4397, val_MinusLogProbMetric: 42.4397

Epoch 463: val_loss improved from 42.43986 to 42.43972, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.2116 - MinusLogProbMetric: 42.2116 - val_loss: 42.4397 - val_MinusLogProbMetric: 42.4397 - lr: 1.6935e-08 - 68s/epoch - 344ms/step
Epoch 464/1000
2023-10-10 11:23:30.046 
Epoch 464/1000 
	 loss: 42.2115, MinusLogProbMetric: 42.2115, val_loss: 42.4399, val_MinusLogProbMetric: 42.4399

Epoch 464: val_loss did not improve from 42.43972
196/196 - 67s - loss: 42.2115 - MinusLogProbMetric: 42.2115 - val_loss: 42.4399 - val_MinusLogProbMetric: 42.4399 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 465/1000
2023-10-10 11:24:37.756 
Epoch 465/1000 
	 loss: 42.2112, MinusLogProbMetric: 42.2112, val_loss: 42.4420, val_MinusLogProbMetric: 42.4420

Epoch 465: val_loss did not improve from 42.43972
196/196 - 68s - loss: 42.2112 - MinusLogProbMetric: 42.2112 - val_loss: 42.4420 - val_MinusLogProbMetric: 42.4420 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 466/1000
2023-10-10 11:25:44.405 
Epoch 466/1000 
	 loss: 42.2099, MinusLogProbMetric: 42.2099, val_loss: 42.4362, val_MinusLogProbMetric: 42.4362

Epoch 466: val_loss improved from 42.43972 to 42.43616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.2099 - MinusLogProbMetric: 42.2099 - val_loss: 42.4362 - val_MinusLogProbMetric: 42.4362 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 467/1000
2023-10-10 11:26:52.362 
Epoch 467/1000 
	 loss: 42.2102, MinusLogProbMetric: 42.2102, val_loss: 42.4377, val_MinusLogProbMetric: 42.4377

Epoch 467: val_loss did not improve from 42.43616
196/196 - 67s - loss: 42.2102 - MinusLogProbMetric: 42.2102 - val_loss: 42.4377 - val_MinusLogProbMetric: 42.4377 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 468/1000
2023-10-10 11:27:59.298 
Epoch 468/1000 
	 loss: 42.2106, MinusLogProbMetric: 42.2106, val_loss: 42.4397, val_MinusLogProbMetric: 42.4397

Epoch 468: val_loss did not improve from 42.43616
196/196 - 67s - loss: 42.2106 - MinusLogProbMetric: 42.2106 - val_loss: 42.4397 - val_MinusLogProbMetric: 42.4397 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 469/1000
2023-10-10 11:29:06.277 
Epoch 469/1000 
	 loss: 42.2101, MinusLogProbMetric: 42.2101, val_loss: 42.4402, val_MinusLogProbMetric: 42.4402

Epoch 469: val_loss did not improve from 42.43616
196/196 - 67s - loss: 42.2101 - MinusLogProbMetric: 42.2101 - val_loss: 42.4402 - val_MinusLogProbMetric: 42.4402 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 470/1000
2023-10-10 11:30:13.164 
Epoch 470/1000 
	 loss: 42.2101, MinusLogProbMetric: 42.2101, val_loss: 42.4397, val_MinusLogProbMetric: 42.4397

Epoch 470: val_loss did not improve from 42.43616
196/196 - 67s - loss: 42.2101 - MinusLogProbMetric: 42.2101 - val_loss: 42.4397 - val_MinusLogProbMetric: 42.4397 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 471/1000
2023-10-10 11:31:20.427 
Epoch 471/1000 
	 loss: 42.2119, MinusLogProbMetric: 42.2119, val_loss: 42.4359, val_MinusLogProbMetric: 42.4359

Epoch 471: val_loss improved from 42.43616 to 42.43592, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.2119 - MinusLogProbMetric: 42.2119 - val_loss: 42.4359 - val_MinusLogProbMetric: 42.4359 - lr: 1.6935e-08 - 68s/epoch - 349ms/step
Epoch 472/1000
2023-10-10 11:32:28.400 
Epoch 472/1000 
	 loss: 42.2091, MinusLogProbMetric: 42.2091, val_loss: 42.4385, val_MinusLogProbMetric: 42.4385

Epoch 472: val_loss did not improve from 42.43592
196/196 - 67s - loss: 42.2091 - MinusLogProbMetric: 42.2091 - val_loss: 42.4385 - val_MinusLogProbMetric: 42.4385 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 473/1000
2023-10-10 11:33:34.751 
Epoch 473/1000 
	 loss: 42.2092, MinusLogProbMetric: 42.2092, val_loss: 42.4396, val_MinusLogProbMetric: 42.4396

Epoch 473: val_loss did not improve from 42.43592
196/196 - 66s - loss: 42.2092 - MinusLogProbMetric: 42.2092 - val_loss: 42.4396 - val_MinusLogProbMetric: 42.4396 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 474/1000
2023-10-10 11:34:42.102 
Epoch 474/1000 
	 loss: 42.2097, MinusLogProbMetric: 42.2097, val_loss: 42.4399, val_MinusLogProbMetric: 42.4399

Epoch 474: val_loss did not improve from 42.43592
196/196 - 67s - loss: 42.2097 - MinusLogProbMetric: 42.2097 - val_loss: 42.4399 - val_MinusLogProbMetric: 42.4399 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 475/1000
2023-10-10 11:35:49.246 
Epoch 475/1000 
	 loss: 42.2117, MinusLogProbMetric: 42.2117, val_loss: 42.4406, val_MinusLogProbMetric: 42.4406

Epoch 475: val_loss did not improve from 42.43592
196/196 - 67s - loss: 42.2117 - MinusLogProbMetric: 42.2117 - val_loss: 42.4406 - val_MinusLogProbMetric: 42.4406 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 476/1000
2023-10-10 11:36:55.006 
Epoch 476/1000 
	 loss: 42.2107, MinusLogProbMetric: 42.2107, val_loss: 42.4368, val_MinusLogProbMetric: 42.4368

Epoch 476: val_loss did not improve from 42.43592
196/196 - 66s - loss: 42.2107 - MinusLogProbMetric: 42.2107 - val_loss: 42.4368 - val_MinusLogProbMetric: 42.4368 - lr: 1.6935e-08 - 66s/epoch - 335ms/step
Epoch 477/1000
2023-10-10 11:38:01.494 
Epoch 477/1000 
	 loss: 42.2112, MinusLogProbMetric: 42.2112, val_loss: 42.4452, val_MinusLogProbMetric: 42.4452

Epoch 477: val_loss did not improve from 42.43592
196/196 - 66s - loss: 42.2112 - MinusLogProbMetric: 42.2112 - val_loss: 42.4452 - val_MinusLogProbMetric: 42.4452 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 478/1000
2023-10-10 11:39:07.962 
Epoch 478/1000 
	 loss: 42.2109, MinusLogProbMetric: 42.2109, val_loss: 42.4395, val_MinusLogProbMetric: 42.4395

Epoch 478: val_loss did not improve from 42.43592
196/196 - 66s - loss: 42.2109 - MinusLogProbMetric: 42.2109 - val_loss: 42.4395 - val_MinusLogProbMetric: 42.4395 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 479/1000
2023-10-10 11:40:14.323 
Epoch 479/1000 
	 loss: 42.2115, MinusLogProbMetric: 42.2115, val_loss: 42.4420, val_MinusLogProbMetric: 42.4420

Epoch 479: val_loss did not improve from 42.43592
196/196 - 66s - loss: 42.2115 - MinusLogProbMetric: 42.2115 - val_loss: 42.4420 - val_MinusLogProbMetric: 42.4420 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 480/1000
2023-10-10 11:41:21.058 
Epoch 480/1000 
	 loss: 42.2128, MinusLogProbMetric: 42.2128, val_loss: 42.4379, val_MinusLogProbMetric: 42.4379

Epoch 480: val_loss did not improve from 42.43592
196/196 - 67s - loss: 42.2128 - MinusLogProbMetric: 42.2128 - val_loss: 42.4379 - val_MinusLogProbMetric: 42.4379 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 481/1000
2023-10-10 11:42:27.433 
Epoch 481/1000 
	 loss: 42.2103, MinusLogProbMetric: 42.2103, val_loss: 42.4362, val_MinusLogProbMetric: 42.4362

Epoch 481: val_loss did not improve from 42.43592
196/196 - 66s - loss: 42.2103 - MinusLogProbMetric: 42.2103 - val_loss: 42.4362 - val_MinusLogProbMetric: 42.4362 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 482/1000
2023-10-10 11:43:34.381 
Epoch 482/1000 
	 loss: 42.2097, MinusLogProbMetric: 42.2097, val_loss: 42.4387, val_MinusLogProbMetric: 42.4387

Epoch 482: val_loss did not improve from 42.43592
196/196 - 67s - loss: 42.2097 - MinusLogProbMetric: 42.2097 - val_loss: 42.4387 - val_MinusLogProbMetric: 42.4387 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 483/1000
2023-10-10 11:44:40.925 
Epoch 483/1000 
	 loss: 42.2089, MinusLogProbMetric: 42.2089, val_loss: 42.4414, val_MinusLogProbMetric: 42.4414

Epoch 483: val_loss did not improve from 42.43592
196/196 - 67s - loss: 42.2089 - MinusLogProbMetric: 42.2089 - val_loss: 42.4414 - val_MinusLogProbMetric: 42.4414 - lr: 1.6935e-08 - 67s/epoch - 339ms/step
Epoch 484/1000
2023-10-10 11:45:46.953 
Epoch 484/1000 
	 loss: 42.2070, MinusLogProbMetric: 42.2070, val_loss: 42.4411, val_MinusLogProbMetric: 42.4411

Epoch 484: val_loss did not improve from 42.43592
196/196 - 66s - loss: 42.2070 - MinusLogProbMetric: 42.2070 - val_loss: 42.4411 - val_MinusLogProbMetric: 42.4411 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 485/1000
2023-10-10 11:46:52.810 
Epoch 485/1000 
	 loss: 42.2078, MinusLogProbMetric: 42.2078, val_loss: 42.4365, val_MinusLogProbMetric: 42.4365

Epoch 485: val_loss did not improve from 42.43592
196/196 - 66s - loss: 42.2078 - MinusLogProbMetric: 42.2078 - val_loss: 42.4365 - val_MinusLogProbMetric: 42.4365 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 486/1000
2023-10-10 11:48:01.798 
Epoch 486/1000 
	 loss: 42.2081, MinusLogProbMetric: 42.2081, val_loss: 42.4375, val_MinusLogProbMetric: 42.4375

Epoch 486: val_loss did not improve from 42.43592
196/196 - 69s - loss: 42.2081 - MinusLogProbMetric: 42.2081 - val_loss: 42.4375 - val_MinusLogProbMetric: 42.4375 - lr: 1.6935e-08 - 69s/epoch - 352ms/step
Epoch 487/1000
2023-10-10 11:49:11.698 
Epoch 487/1000 
	 loss: 42.2119, MinusLogProbMetric: 42.2119, val_loss: 42.4381, val_MinusLogProbMetric: 42.4381

Epoch 487: val_loss did not improve from 42.43592
196/196 - 70s - loss: 42.2119 - MinusLogProbMetric: 42.2119 - val_loss: 42.4381 - val_MinusLogProbMetric: 42.4381 - lr: 1.6935e-08 - 70s/epoch - 357ms/step
Epoch 488/1000
2023-10-10 11:50:18.422 
Epoch 488/1000 
	 loss: 42.2097, MinusLogProbMetric: 42.2097, val_loss: 42.4381, val_MinusLogProbMetric: 42.4381

Epoch 488: val_loss did not improve from 42.43592
196/196 - 67s - loss: 42.2097 - MinusLogProbMetric: 42.2097 - val_loss: 42.4381 - val_MinusLogProbMetric: 42.4381 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 489/1000
2023-10-10 11:51:25.186 
Epoch 489/1000 
	 loss: 42.2084, MinusLogProbMetric: 42.2084, val_loss: 42.4400, val_MinusLogProbMetric: 42.4400

Epoch 489: val_loss did not improve from 42.43592
196/196 - 67s - loss: 42.2084 - MinusLogProbMetric: 42.2084 - val_loss: 42.4400 - val_MinusLogProbMetric: 42.4400 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 490/1000
2023-10-10 11:52:33.259 
Epoch 490/1000 
	 loss: 42.2098, MinusLogProbMetric: 42.2098, val_loss: 42.4399, val_MinusLogProbMetric: 42.4399

Epoch 490: val_loss did not improve from 42.43592
196/196 - 68s - loss: 42.2098 - MinusLogProbMetric: 42.2098 - val_loss: 42.4399 - val_MinusLogProbMetric: 42.4399 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 491/1000
2023-10-10 11:53:39.273 
Epoch 491/1000 
	 loss: 42.2098, MinusLogProbMetric: 42.2098, val_loss: 42.4404, val_MinusLogProbMetric: 42.4404

Epoch 491: val_loss did not improve from 42.43592
196/196 - 66s - loss: 42.2098 - MinusLogProbMetric: 42.2098 - val_loss: 42.4404 - val_MinusLogProbMetric: 42.4404 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 492/1000
2023-10-10 11:54:45.654 
Epoch 492/1000 
	 loss: 42.2085, MinusLogProbMetric: 42.2085, val_loss: 42.4412, val_MinusLogProbMetric: 42.4412

Epoch 492: val_loss did not improve from 42.43592
196/196 - 66s - loss: 42.2085 - MinusLogProbMetric: 42.2085 - val_loss: 42.4412 - val_MinusLogProbMetric: 42.4412 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 493/1000
2023-10-10 11:55:51.121 
Epoch 493/1000 
	 loss: 42.2100, MinusLogProbMetric: 42.2100, val_loss: 42.4375, val_MinusLogProbMetric: 42.4375

Epoch 493: val_loss did not improve from 42.43592
196/196 - 65s - loss: 42.2100 - MinusLogProbMetric: 42.2100 - val_loss: 42.4375 - val_MinusLogProbMetric: 42.4375 - lr: 1.6935e-08 - 65s/epoch - 334ms/step
Epoch 494/1000
2023-10-10 11:56:57.572 
Epoch 494/1000 
	 loss: 42.2087, MinusLogProbMetric: 42.2087, val_loss: 42.4348, val_MinusLogProbMetric: 42.4348

Epoch 494: val_loss improved from 42.43592 to 42.43476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.2087 - MinusLogProbMetric: 42.2087 - val_loss: 42.4348 - val_MinusLogProbMetric: 42.4348 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 495/1000
2023-10-10 11:58:05.520 
Epoch 495/1000 
	 loss: 42.2122, MinusLogProbMetric: 42.2122, val_loss: 42.4451, val_MinusLogProbMetric: 42.4451

Epoch 495: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2122 - MinusLogProbMetric: 42.2122 - val_loss: 42.4451 - val_MinusLogProbMetric: 42.4451 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 496/1000
2023-10-10 11:59:11.540 
Epoch 496/1000 
	 loss: 42.2155, MinusLogProbMetric: 42.2155, val_loss: 42.4505, val_MinusLogProbMetric: 42.4505

Epoch 496: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2155 - MinusLogProbMetric: 42.2155 - val_loss: 42.4505 - val_MinusLogProbMetric: 42.4505 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 497/1000
2023-10-10 12:00:18.395 
Epoch 497/1000 
	 loss: 42.2178, MinusLogProbMetric: 42.2178, val_loss: 42.4485, val_MinusLogProbMetric: 42.4485

Epoch 497: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2178 - MinusLogProbMetric: 42.2178 - val_loss: 42.4485 - val_MinusLogProbMetric: 42.4485 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 498/1000
2023-10-10 12:01:25.150 
Epoch 498/1000 
	 loss: 42.2186, MinusLogProbMetric: 42.2186, val_loss: 42.4592, val_MinusLogProbMetric: 42.4592

Epoch 498: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2186 - MinusLogProbMetric: 42.2186 - val_loss: 42.4592 - val_MinusLogProbMetric: 42.4592 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 499/1000
2023-10-10 12:02:31.087 
Epoch 499/1000 
	 loss: 42.2249, MinusLogProbMetric: 42.2249, val_loss: 42.4505, val_MinusLogProbMetric: 42.4505

Epoch 499: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2249 - MinusLogProbMetric: 42.2249 - val_loss: 42.4505 - val_MinusLogProbMetric: 42.4505 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 500/1000
2023-10-10 12:03:36.926 
Epoch 500/1000 
	 loss: 42.2189, MinusLogProbMetric: 42.2189, val_loss: 42.4539, val_MinusLogProbMetric: 42.4539

Epoch 500: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2189 - MinusLogProbMetric: 42.2189 - val_loss: 42.4539 - val_MinusLogProbMetric: 42.4539 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 501/1000
2023-10-10 12:04:43.507 
Epoch 501/1000 
	 loss: 42.2206, MinusLogProbMetric: 42.2206, val_loss: 42.4459, val_MinusLogProbMetric: 42.4459

Epoch 501: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2206 - MinusLogProbMetric: 42.2206 - val_loss: 42.4459 - val_MinusLogProbMetric: 42.4459 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 502/1000
2023-10-10 12:05:49.848 
Epoch 502/1000 
	 loss: 42.2182, MinusLogProbMetric: 42.2182, val_loss: 42.4445, val_MinusLogProbMetric: 42.4445

Epoch 502: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2182 - MinusLogProbMetric: 42.2182 - val_loss: 42.4445 - val_MinusLogProbMetric: 42.4445 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 503/1000
2023-10-10 12:06:56.371 
Epoch 503/1000 
	 loss: 42.2145, MinusLogProbMetric: 42.2145, val_loss: 42.4475, val_MinusLogProbMetric: 42.4475

Epoch 503: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2145 - MinusLogProbMetric: 42.2145 - val_loss: 42.4475 - val_MinusLogProbMetric: 42.4475 - lr: 1.6935e-08 - 67s/epoch - 339ms/step
Epoch 504/1000
2023-10-10 12:08:03.287 
Epoch 504/1000 
	 loss: 42.2154, MinusLogProbMetric: 42.2154, val_loss: 42.4452, val_MinusLogProbMetric: 42.4452

Epoch 504: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2154 - MinusLogProbMetric: 42.2154 - val_loss: 42.4452 - val_MinusLogProbMetric: 42.4452 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 505/1000
2023-10-10 12:09:10.306 
Epoch 505/1000 
	 loss: 42.2146, MinusLogProbMetric: 42.2146, val_loss: 42.4431, val_MinusLogProbMetric: 42.4431

Epoch 505: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2146 - MinusLogProbMetric: 42.2146 - val_loss: 42.4431 - val_MinusLogProbMetric: 42.4431 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 506/1000
2023-10-10 12:10:18.071 
Epoch 506/1000 
	 loss: 42.2133, MinusLogProbMetric: 42.2133, val_loss: 42.4411, val_MinusLogProbMetric: 42.4411

Epoch 506: val_loss did not improve from 42.43476
196/196 - 68s - loss: 42.2133 - MinusLogProbMetric: 42.2133 - val_loss: 42.4411 - val_MinusLogProbMetric: 42.4411 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 507/1000
2023-10-10 12:11:23.702 
Epoch 507/1000 
	 loss: 42.2126, MinusLogProbMetric: 42.2126, val_loss: 42.4418, val_MinusLogProbMetric: 42.4418

Epoch 507: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2126 - MinusLogProbMetric: 42.2126 - val_loss: 42.4418 - val_MinusLogProbMetric: 42.4418 - lr: 1.6935e-08 - 66s/epoch - 335ms/step
Epoch 508/1000
2023-10-10 12:12:31.194 
Epoch 508/1000 
	 loss: 42.2117, MinusLogProbMetric: 42.2117, val_loss: 42.4407, val_MinusLogProbMetric: 42.4407

Epoch 508: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2117 - MinusLogProbMetric: 42.2117 - val_loss: 42.4407 - val_MinusLogProbMetric: 42.4407 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 509/1000
2023-10-10 12:13:37.861 
Epoch 509/1000 
	 loss: 42.2138, MinusLogProbMetric: 42.2138, val_loss: 42.4395, val_MinusLogProbMetric: 42.4395

Epoch 509: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2138 - MinusLogProbMetric: 42.2138 - val_loss: 42.4395 - val_MinusLogProbMetric: 42.4395 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 510/1000
2023-10-10 12:14:46.681 
Epoch 510/1000 
	 loss: 42.2111, MinusLogProbMetric: 42.2111, val_loss: 42.4407, val_MinusLogProbMetric: 42.4407

Epoch 510: val_loss did not improve from 42.43476
196/196 - 69s - loss: 42.2111 - MinusLogProbMetric: 42.2111 - val_loss: 42.4407 - val_MinusLogProbMetric: 42.4407 - lr: 1.6935e-08 - 69s/epoch - 351ms/step
Epoch 511/1000
2023-10-10 12:15:54.150 
Epoch 511/1000 
	 loss: 42.2113, MinusLogProbMetric: 42.2113, val_loss: 42.4377, val_MinusLogProbMetric: 42.4377

Epoch 511: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2113 - MinusLogProbMetric: 42.2113 - val_loss: 42.4377 - val_MinusLogProbMetric: 42.4377 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 512/1000
2023-10-10 12:17:01.398 
Epoch 512/1000 
	 loss: 42.2091, MinusLogProbMetric: 42.2091, val_loss: 42.4383, val_MinusLogProbMetric: 42.4383

Epoch 512: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2091 - MinusLogProbMetric: 42.2091 - val_loss: 42.4383 - val_MinusLogProbMetric: 42.4383 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 513/1000
2023-10-10 12:18:08.698 
Epoch 513/1000 
	 loss: 42.2084, MinusLogProbMetric: 42.2084, val_loss: 42.4371, val_MinusLogProbMetric: 42.4371

Epoch 513: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2084 - MinusLogProbMetric: 42.2084 - val_loss: 42.4371 - val_MinusLogProbMetric: 42.4371 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 514/1000
2023-10-10 12:19:15.900 
Epoch 514/1000 
	 loss: 42.2077, MinusLogProbMetric: 42.2077, val_loss: 42.4357, val_MinusLogProbMetric: 42.4357

Epoch 514: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2077 - MinusLogProbMetric: 42.2077 - val_loss: 42.4357 - val_MinusLogProbMetric: 42.4357 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 515/1000
2023-10-10 12:20:22.164 
Epoch 515/1000 
	 loss: 42.2075, MinusLogProbMetric: 42.2075, val_loss: 42.4371, val_MinusLogProbMetric: 42.4371

Epoch 515: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2075 - MinusLogProbMetric: 42.2075 - val_loss: 42.4371 - val_MinusLogProbMetric: 42.4371 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 516/1000
2023-10-10 12:21:28.274 
Epoch 516/1000 
	 loss: 42.2072, MinusLogProbMetric: 42.2072, val_loss: 42.4365, val_MinusLogProbMetric: 42.4365

Epoch 516: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2072 - MinusLogProbMetric: 42.2072 - val_loss: 42.4365 - val_MinusLogProbMetric: 42.4365 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 517/1000
2023-10-10 12:22:35.017 
Epoch 517/1000 
	 loss: 42.2068, MinusLogProbMetric: 42.2068, val_loss: 42.4366, val_MinusLogProbMetric: 42.4366

Epoch 517: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2068 - MinusLogProbMetric: 42.2068 - val_loss: 42.4366 - val_MinusLogProbMetric: 42.4366 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 518/1000
2023-10-10 12:23:41.321 
Epoch 518/1000 
	 loss: 42.2095, MinusLogProbMetric: 42.2095, val_loss: 42.4417, val_MinusLogProbMetric: 42.4417

Epoch 518: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2095 - MinusLogProbMetric: 42.2095 - val_loss: 42.4417 - val_MinusLogProbMetric: 42.4417 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 519/1000
2023-10-10 12:24:48.243 
Epoch 519/1000 
	 loss: 42.2092, MinusLogProbMetric: 42.2092, val_loss: 42.4403, val_MinusLogProbMetric: 42.4403

Epoch 519: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2092 - MinusLogProbMetric: 42.2092 - val_loss: 42.4403 - val_MinusLogProbMetric: 42.4403 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 520/1000
2023-10-10 12:25:56.137 
Epoch 520/1000 
	 loss: 42.2092, MinusLogProbMetric: 42.2092, val_loss: 42.4380, val_MinusLogProbMetric: 42.4380

Epoch 520: val_loss did not improve from 42.43476
196/196 - 68s - loss: 42.2092 - MinusLogProbMetric: 42.2092 - val_loss: 42.4380 - val_MinusLogProbMetric: 42.4380 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 521/1000
2023-10-10 12:27:03.432 
Epoch 521/1000 
	 loss: 42.2076, MinusLogProbMetric: 42.2076, val_loss: 42.4392, val_MinusLogProbMetric: 42.4392

Epoch 521: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2076 - MinusLogProbMetric: 42.2076 - val_loss: 42.4392 - val_MinusLogProbMetric: 42.4392 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 522/1000
2023-10-10 12:28:11.179 
Epoch 522/1000 
	 loss: 42.2092, MinusLogProbMetric: 42.2092, val_loss: 42.4356, val_MinusLogProbMetric: 42.4356

Epoch 522: val_loss did not improve from 42.43476
196/196 - 68s - loss: 42.2092 - MinusLogProbMetric: 42.2092 - val_loss: 42.4356 - val_MinusLogProbMetric: 42.4356 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 523/1000
2023-10-10 12:29:17.359 
Epoch 523/1000 
	 loss: 42.2108, MinusLogProbMetric: 42.2108, val_loss: 42.4381, val_MinusLogProbMetric: 42.4381

Epoch 523: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2108 - MinusLogProbMetric: 42.2108 - val_loss: 42.4381 - val_MinusLogProbMetric: 42.4381 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 524/1000
2023-10-10 12:30:24.362 
Epoch 524/1000 
	 loss: 42.2092, MinusLogProbMetric: 42.2092, val_loss: 42.4396, val_MinusLogProbMetric: 42.4396

Epoch 524: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2092 - MinusLogProbMetric: 42.2092 - val_loss: 42.4396 - val_MinusLogProbMetric: 42.4396 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 525/1000
2023-10-10 12:31:30.304 
Epoch 525/1000 
	 loss: 42.2098, MinusLogProbMetric: 42.2098, val_loss: 42.4393, val_MinusLogProbMetric: 42.4393

Epoch 525: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2098 - MinusLogProbMetric: 42.2098 - val_loss: 42.4393 - val_MinusLogProbMetric: 42.4393 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 526/1000
2023-10-10 12:32:37.446 
Epoch 526/1000 
	 loss: 42.2083, MinusLogProbMetric: 42.2083, val_loss: 42.4389, val_MinusLogProbMetric: 42.4389

Epoch 526: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2083 - MinusLogProbMetric: 42.2083 - val_loss: 42.4389 - val_MinusLogProbMetric: 42.4389 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 527/1000
2023-10-10 12:33:44.404 
Epoch 527/1000 
	 loss: 42.2075, MinusLogProbMetric: 42.2075, val_loss: 42.4396, val_MinusLogProbMetric: 42.4396

Epoch 527: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2075 - MinusLogProbMetric: 42.2075 - val_loss: 42.4396 - val_MinusLogProbMetric: 42.4396 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 528/1000
2023-10-10 12:34:51.378 
Epoch 528/1000 
	 loss: 42.2075, MinusLogProbMetric: 42.2075, val_loss: 42.4388, val_MinusLogProbMetric: 42.4388

Epoch 528: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2075 - MinusLogProbMetric: 42.2075 - val_loss: 42.4388 - val_MinusLogProbMetric: 42.4388 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 529/1000
2023-10-10 12:35:59.331 
Epoch 529/1000 
	 loss: 42.2056, MinusLogProbMetric: 42.2056, val_loss: 42.4359, val_MinusLogProbMetric: 42.4359

Epoch 529: val_loss did not improve from 42.43476
196/196 - 68s - loss: 42.2056 - MinusLogProbMetric: 42.2056 - val_loss: 42.4359 - val_MinusLogProbMetric: 42.4359 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 530/1000
2023-10-10 12:37:06.258 
Epoch 530/1000 
	 loss: 42.2086, MinusLogProbMetric: 42.2086, val_loss: 42.4433, val_MinusLogProbMetric: 42.4433

Epoch 530: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2086 - MinusLogProbMetric: 42.2086 - val_loss: 42.4433 - val_MinusLogProbMetric: 42.4433 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 531/1000
2023-10-10 12:38:13.695 
Epoch 531/1000 
	 loss: 42.2136, MinusLogProbMetric: 42.2136, val_loss: 42.4418, val_MinusLogProbMetric: 42.4418

Epoch 531: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2136 - MinusLogProbMetric: 42.2136 - val_loss: 42.4418 - val_MinusLogProbMetric: 42.4418 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 532/1000
2023-10-10 12:39:19.808 
Epoch 532/1000 
	 loss: 42.2144, MinusLogProbMetric: 42.2144, val_loss: 42.4425, val_MinusLogProbMetric: 42.4425

Epoch 532: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2144 - MinusLogProbMetric: 42.2144 - val_loss: 42.4425 - val_MinusLogProbMetric: 42.4425 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 533/1000
2023-10-10 12:40:27.720 
Epoch 533/1000 
	 loss: 42.2141, MinusLogProbMetric: 42.2141, val_loss: 42.4406, val_MinusLogProbMetric: 42.4406

Epoch 533: val_loss did not improve from 42.43476
196/196 - 68s - loss: 42.2141 - MinusLogProbMetric: 42.2141 - val_loss: 42.4406 - val_MinusLogProbMetric: 42.4406 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 534/1000
2023-10-10 12:41:34.686 
Epoch 534/1000 
	 loss: 42.2113, MinusLogProbMetric: 42.2113, val_loss: 42.4386, val_MinusLogProbMetric: 42.4386

Epoch 534: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2113 - MinusLogProbMetric: 42.2113 - val_loss: 42.4386 - val_MinusLogProbMetric: 42.4386 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 535/1000
2023-10-10 12:42:41.382 
Epoch 535/1000 
	 loss: 42.2109, MinusLogProbMetric: 42.2109, val_loss: 42.4401, val_MinusLogProbMetric: 42.4401

Epoch 535: val_loss did not improve from 42.43476
196/196 - 67s - loss: 42.2109 - MinusLogProbMetric: 42.2109 - val_loss: 42.4401 - val_MinusLogProbMetric: 42.4401 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 536/1000
2023-10-10 12:43:47.340 
Epoch 536/1000 
	 loss: 42.2122, MinusLogProbMetric: 42.2122, val_loss: 42.4388, val_MinusLogProbMetric: 42.4388

Epoch 536: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2122 - MinusLogProbMetric: 42.2122 - val_loss: 42.4388 - val_MinusLogProbMetric: 42.4388 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 537/1000
2023-10-10 12:44:53.452 
Epoch 537/1000 
	 loss: 42.2108, MinusLogProbMetric: 42.2108, val_loss: 42.4385, val_MinusLogProbMetric: 42.4385

Epoch 537: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2108 - MinusLogProbMetric: 42.2108 - val_loss: 42.4385 - val_MinusLogProbMetric: 42.4385 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 538/1000
2023-10-10 12:46:01.343 
Epoch 538/1000 
	 loss: 42.2088, MinusLogProbMetric: 42.2088, val_loss: 42.4362, val_MinusLogProbMetric: 42.4362

Epoch 538: val_loss did not improve from 42.43476
196/196 - 68s - loss: 42.2088 - MinusLogProbMetric: 42.2088 - val_loss: 42.4362 - val_MinusLogProbMetric: 42.4362 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 539/1000
2023-10-10 12:47:07.600 
Epoch 539/1000 
	 loss: 42.2092, MinusLogProbMetric: 42.2092, val_loss: 42.4371, val_MinusLogProbMetric: 42.4371

Epoch 539: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2092 - MinusLogProbMetric: 42.2092 - val_loss: 42.4371 - val_MinusLogProbMetric: 42.4371 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 540/1000
2023-10-10 12:48:13.728 
Epoch 540/1000 
	 loss: 42.2073, MinusLogProbMetric: 42.2073, val_loss: 42.4383, val_MinusLogProbMetric: 42.4383

Epoch 540: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2073 - MinusLogProbMetric: 42.2073 - val_loss: 42.4383 - val_MinusLogProbMetric: 42.4383 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 541/1000
2023-10-10 12:49:19.994 
Epoch 541/1000 
	 loss: 42.2081, MinusLogProbMetric: 42.2081, val_loss: 42.4353, val_MinusLogProbMetric: 42.4353

Epoch 541: val_loss did not improve from 42.43476
196/196 - 66s - loss: 42.2081 - MinusLogProbMetric: 42.2081 - val_loss: 42.4353 - val_MinusLogProbMetric: 42.4353 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 542/1000
2023-10-10 12:50:26.230 
Epoch 542/1000 
	 loss: 42.2080, MinusLogProbMetric: 42.2080, val_loss: 42.4321, val_MinusLogProbMetric: 42.4321

Epoch 542: val_loss improved from 42.43476 to 42.43214, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 42.2080 - MinusLogProbMetric: 42.2080 - val_loss: 42.4321 - val_MinusLogProbMetric: 42.4321 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 543/1000
2023-10-10 12:51:33.663 
Epoch 543/1000 
	 loss: 42.2063, MinusLogProbMetric: 42.2063, val_loss: 42.4345, val_MinusLogProbMetric: 42.4345

Epoch 543: val_loss did not improve from 42.43214
196/196 - 66s - loss: 42.2063 - MinusLogProbMetric: 42.2063 - val_loss: 42.4345 - val_MinusLogProbMetric: 42.4345 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 544/1000
2023-10-10 12:52:40.819 
Epoch 544/1000 
	 loss: 42.2063, MinusLogProbMetric: 42.2063, val_loss: 42.4340, val_MinusLogProbMetric: 42.4340

Epoch 544: val_loss did not improve from 42.43214
196/196 - 67s - loss: 42.2063 - MinusLogProbMetric: 42.2063 - val_loss: 42.4340 - val_MinusLogProbMetric: 42.4340 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 545/1000
2023-10-10 12:53:47.797 
Epoch 545/1000 
	 loss: 42.2062, MinusLogProbMetric: 42.2062, val_loss: 42.4340, val_MinusLogProbMetric: 42.4340

Epoch 545: val_loss did not improve from 42.43214
196/196 - 67s - loss: 42.2062 - MinusLogProbMetric: 42.2062 - val_loss: 42.4340 - val_MinusLogProbMetric: 42.4340 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 546/1000
2023-10-10 12:54:54.556 
Epoch 546/1000 
	 loss: 42.2061, MinusLogProbMetric: 42.2061, val_loss: 42.4343, val_MinusLogProbMetric: 42.4343

Epoch 546: val_loss did not improve from 42.43214
196/196 - 67s - loss: 42.2061 - MinusLogProbMetric: 42.2061 - val_loss: 42.4343 - val_MinusLogProbMetric: 42.4343 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 547/1000
2023-10-10 12:56:02.015 
Epoch 547/1000 
	 loss: 42.2065, MinusLogProbMetric: 42.2065, val_loss: 42.4359, val_MinusLogProbMetric: 42.4359

Epoch 547: val_loss did not improve from 42.43214
196/196 - 67s - loss: 42.2065 - MinusLogProbMetric: 42.2065 - val_loss: 42.4359 - val_MinusLogProbMetric: 42.4359 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 548/1000
2023-10-10 12:57:09.063 
Epoch 548/1000 
	 loss: 42.2075, MinusLogProbMetric: 42.2075, val_loss: 42.4369, val_MinusLogProbMetric: 42.4369

Epoch 548: val_loss did not improve from 42.43214
196/196 - 67s - loss: 42.2075 - MinusLogProbMetric: 42.2075 - val_loss: 42.4369 - val_MinusLogProbMetric: 42.4369 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 549/1000
2023-10-10 12:58:16.160 
Epoch 549/1000 
	 loss: 42.2069, MinusLogProbMetric: 42.2069, val_loss: 42.4319, val_MinusLogProbMetric: 42.4319

Epoch 549: val_loss improved from 42.43214 to 42.43193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.2069 - MinusLogProbMetric: 42.2069 - val_loss: 42.4319 - val_MinusLogProbMetric: 42.4319 - lr: 1.6935e-08 - 68s/epoch - 348ms/step
Epoch 550/1000
2023-10-10 12:59:23.458 
Epoch 550/1000 
	 loss: 42.2054, MinusLogProbMetric: 42.2054, val_loss: 42.4357, val_MinusLogProbMetric: 42.4357

Epoch 550: val_loss did not improve from 42.43193
196/196 - 66s - loss: 42.2054 - MinusLogProbMetric: 42.2054 - val_loss: 42.4357 - val_MinusLogProbMetric: 42.4357 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 551/1000
2023-10-10 13:00:29.814 
Epoch 551/1000 
	 loss: 42.2043, MinusLogProbMetric: 42.2043, val_loss: 42.4343, val_MinusLogProbMetric: 42.4343

Epoch 551: val_loss did not improve from 42.43193
196/196 - 66s - loss: 42.2043 - MinusLogProbMetric: 42.2043 - val_loss: 42.4343 - val_MinusLogProbMetric: 42.4343 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 552/1000
2023-10-10 13:01:36.470 
Epoch 552/1000 
	 loss: 42.2045, MinusLogProbMetric: 42.2045, val_loss: 42.4306, val_MinusLogProbMetric: 42.4306

Epoch 552: val_loss improved from 42.43193 to 42.43062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.2045 - MinusLogProbMetric: 42.2045 - val_loss: 42.4306 - val_MinusLogProbMetric: 42.4306 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 553/1000
2023-10-10 13:02:44.126 
Epoch 553/1000 
	 loss: 42.2047, MinusLogProbMetric: 42.2047, val_loss: 42.4311, val_MinusLogProbMetric: 42.4311

Epoch 553: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2047 - MinusLogProbMetric: 42.2047 - val_loss: 42.4311 - val_MinusLogProbMetric: 42.4311 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 554/1000
2023-10-10 13:03:51.475 
Epoch 554/1000 
	 loss: 42.2081, MinusLogProbMetric: 42.2081, val_loss: 42.4458, val_MinusLogProbMetric: 42.4458

Epoch 554: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2081 - MinusLogProbMetric: 42.2081 - val_loss: 42.4458 - val_MinusLogProbMetric: 42.4458 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 555/1000
2023-10-10 13:04:58.577 
Epoch 555/1000 
	 loss: 42.2184, MinusLogProbMetric: 42.2184, val_loss: 42.4499, val_MinusLogProbMetric: 42.4499

Epoch 555: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2184 - MinusLogProbMetric: 42.2184 - val_loss: 42.4499 - val_MinusLogProbMetric: 42.4499 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 556/1000
2023-10-10 13:06:04.907 
Epoch 556/1000 
	 loss: 42.2173, MinusLogProbMetric: 42.2173, val_loss: 42.4440, val_MinusLogProbMetric: 42.4440

Epoch 556: val_loss did not improve from 42.43062
196/196 - 66s - loss: 42.2173 - MinusLogProbMetric: 42.2173 - val_loss: 42.4440 - val_MinusLogProbMetric: 42.4440 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 557/1000
2023-10-10 13:07:11.538 
Epoch 557/1000 
	 loss: 42.2146, MinusLogProbMetric: 42.2146, val_loss: 42.4437, val_MinusLogProbMetric: 42.4437

Epoch 557: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2146 - MinusLogProbMetric: 42.2146 - val_loss: 42.4437 - val_MinusLogProbMetric: 42.4437 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 558/1000
2023-10-10 13:08:18.639 
Epoch 558/1000 
	 loss: 42.2128, MinusLogProbMetric: 42.2128, val_loss: 42.4395, val_MinusLogProbMetric: 42.4395

Epoch 558: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2128 - MinusLogProbMetric: 42.2128 - val_loss: 42.4395 - val_MinusLogProbMetric: 42.4395 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 559/1000
2023-10-10 13:09:25.308 
Epoch 559/1000 
	 loss: 42.2122, MinusLogProbMetric: 42.2122, val_loss: 42.4488, val_MinusLogProbMetric: 42.4488

Epoch 559: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2122 - MinusLogProbMetric: 42.2122 - val_loss: 42.4488 - val_MinusLogProbMetric: 42.4488 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 560/1000
2023-10-10 13:10:32.240 
Epoch 560/1000 
	 loss: 42.2174, MinusLogProbMetric: 42.2174, val_loss: 42.4435, val_MinusLogProbMetric: 42.4435

Epoch 560: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2174 - MinusLogProbMetric: 42.2174 - val_loss: 42.4435 - val_MinusLogProbMetric: 42.4435 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 561/1000
2023-10-10 13:11:39.111 
Epoch 561/1000 
	 loss: 42.2138, MinusLogProbMetric: 42.2138, val_loss: 42.4424, val_MinusLogProbMetric: 42.4424

Epoch 561: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2138 - MinusLogProbMetric: 42.2138 - val_loss: 42.4424 - val_MinusLogProbMetric: 42.4424 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 562/1000
2023-10-10 13:12:45.717 
Epoch 562/1000 
	 loss: 42.2118, MinusLogProbMetric: 42.2118, val_loss: 42.4394, val_MinusLogProbMetric: 42.4394

Epoch 562: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2118 - MinusLogProbMetric: 42.2118 - val_loss: 42.4394 - val_MinusLogProbMetric: 42.4394 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 563/1000
2023-10-10 13:13:53.256 
Epoch 563/1000 
	 loss: 42.2117, MinusLogProbMetric: 42.2117, val_loss: 42.4379, val_MinusLogProbMetric: 42.4379

Epoch 563: val_loss did not improve from 42.43062
196/196 - 68s - loss: 42.2117 - MinusLogProbMetric: 42.2117 - val_loss: 42.4379 - val_MinusLogProbMetric: 42.4379 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 564/1000
2023-10-10 13:14:59.326 
Epoch 564/1000 
	 loss: 42.2084, MinusLogProbMetric: 42.2084, val_loss: 42.4363, val_MinusLogProbMetric: 42.4363

Epoch 564: val_loss did not improve from 42.43062
196/196 - 66s - loss: 42.2084 - MinusLogProbMetric: 42.2084 - val_loss: 42.4363 - val_MinusLogProbMetric: 42.4363 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 565/1000
2023-10-10 13:16:05.883 
Epoch 565/1000 
	 loss: 42.2086, MinusLogProbMetric: 42.2086, val_loss: 42.4357, val_MinusLogProbMetric: 42.4357

Epoch 565: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2086 - MinusLogProbMetric: 42.2086 - val_loss: 42.4357 - val_MinusLogProbMetric: 42.4357 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 566/1000
2023-10-10 13:17:11.804 
Epoch 566/1000 
	 loss: 42.2084, MinusLogProbMetric: 42.2084, val_loss: 42.4378, val_MinusLogProbMetric: 42.4378

Epoch 566: val_loss did not improve from 42.43062
196/196 - 66s - loss: 42.2084 - MinusLogProbMetric: 42.2084 - val_loss: 42.4378 - val_MinusLogProbMetric: 42.4378 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 567/1000
2023-10-10 13:18:18.121 
Epoch 567/1000 
	 loss: 42.2089, MinusLogProbMetric: 42.2089, val_loss: 42.4388, val_MinusLogProbMetric: 42.4388

Epoch 567: val_loss did not improve from 42.43062
196/196 - 66s - loss: 42.2089 - MinusLogProbMetric: 42.2089 - val_loss: 42.4388 - val_MinusLogProbMetric: 42.4388 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 568/1000
2023-10-10 13:19:25.495 
Epoch 568/1000 
	 loss: 42.2067, MinusLogProbMetric: 42.2067, val_loss: 42.4372, val_MinusLogProbMetric: 42.4372

Epoch 568: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2067 - MinusLogProbMetric: 42.2067 - val_loss: 42.4372 - val_MinusLogProbMetric: 42.4372 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 569/1000
2023-10-10 13:20:31.807 
Epoch 569/1000 
	 loss: 42.2078, MinusLogProbMetric: 42.2078, val_loss: 42.4356, val_MinusLogProbMetric: 42.4356

Epoch 569: val_loss did not improve from 42.43062
196/196 - 66s - loss: 42.2078 - MinusLogProbMetric: 42.2078 - val_loss: 42.4356 - val_MinusLogProbMetric: 42.4356 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 570/1000
2023-10-10 13:21:38.466 
Epoch 570/1000 
	 loss: 42.2065, MinusLogProbMetric: 42.2065, val_loss: 42.4334, val_MinusLogProbMetric: 42.4334

Epoch 570: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2065 - MinusLogProbMetric: 42.2065 - val_loss: 42.4334 - val_MinusLogProbMetric: 42.4334 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 571/1000
2023-10-10 13:22:46.461 
Epoch 571/1000 
	 loss: 42.2061, MinusLogProbMetric: 42.2061, val_loss: 42.4354, val_MinusLogProbMetric: 42.4354

Epoch 571: val_loss did not improve from 42.43062
196/196 - 68s - loss: 42.2061 - MinusLogProbMetric: 42.2061 - val_loss: 42.4354 - val_MinusLogProbMetric: 42.4354 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 572/1000
2023-10-10 13:23:52.991 
Epoch 572/1000 
	 loss: 42.2132, MinusLogProbMetric: 42.2132, val_loss: 42.4443, val_MinusLogProbMetric: 42.4443

Epoch 572: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2132 - MinusLogProbMetric: 42.2132 - val_loss: 42.4443 - val_MinusLogProbMetric: 42.4443 - lr: 1.6935e-08 - 67s/epoch - 339ms/step
Epoch 573/1000
2023-10-10 13:25:00.653 
Epoch 573/1000 
	 loss: 42.2148, MinusLogProbMetric: 42.2148, val_loss: 42.4410, val_MinusLogProbMetric: 42.4410

Epoch 573: val_loss did not improve from 42.43062
196/196 - 68s - loss: 42.2148 - MinusLogProbMetric: 42.2148 - val_loss: 42.4410 - val_MinusLogProbMetric: 42.4410 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 574/1000
2023-10-10 13:26:07.622 
Epoch 574/1000 
	 loss: 42.2109, MinusLogProbMetric: 42.2109, val_loss: 42.4377, val_MinusLogProbMetric: 42.4377

Epoch 574: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2109 - MinusLogProbMetric: 42.2109 - val_loss: 42.4377 - val_MinusLogProbMetric: 42.4377 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 575/1000
2023-10-10 13:27:14.627 
Epoch 575/1000 
	 loss: 42.2101, MinusLogProbMetric: 42.2101, val_loss: 42.4370, val_MinusLogProbMetric: 42.4370

Epoch 575: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2101 - MinusLogProbMetric: 42.2101 - val_loss: 42.4370 - val_MinusLogProbMetric: 42.4370 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 576/1000
2023-10-10 13:28:20.670 
Epoch 576/1000 
	 loss: 42.2098, MinusLogProbMetric: 42.2098, val_loss: 42.4429, val_MinusLogProbMetric: 42.4429

Epoch 576: val_loss did not improve from 42.43062
196/196 - 66s - loss: 42.2098 - MinusLogProbMetric: 42.2098 - val_loss: 42.4429 - val_MinusLogProbMetric: 42.4429 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 577/1000
2023-10-10 13:29:27.531 
Epoch 577/1000 
	 loss: 42.2079, MinusLogProbMetric: 42.2079, val_loss: 42.4386, val_MinusLogProbMetric: 42.4386

Epoch 577: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2079 - MinusLogProbMetric: 42.2079 - val_loss: 42.4386 - val_MinusLogProbMetric: 42.4386 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 578/1000
2023-10-10 13:30:34.042 
Epoch 578/1000 
	 loss: 42.2070, MinusLogProbMetric: 42.2070, val_loss: 42.4366, val_MinusLogProbMetric: 42.4366

Epoch 578: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2070 - MinusLogProbMetric: 42.2070 - val_loss: 42.4366 - val_MinusLogProbMetric: 42.4366 - lr: 1.6935e-08 - 67s/epoch - 339ms/step
Epoch 579/1000
2023-10-10 13:31:41.292 
Epoch 579/1000 
	 loss: 42.2081, MinusLogProbMetric: 42.2081, val_loss: 42.4344, val_MinusLogProbMetric: 42.4344

Epoch 579: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2081 - MinusLogProbMetric: 42.2081 - val_loss: 42.4344 - val_MinusLogProbMetric: 42.4344 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 580/1000
2023-10-10 13:32:48.366 
Epoch 580/1000 
	 loss: 42.2068, MinusLogProbMetric: 42.2068, val_loss: 42.4378, val_MinusLogProbMetric: 42.4378

Epoch 580: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2068 - MinusLogProbMetric: 42.2068 - val_loss: 42.4378 - val_MinusLogProbMetric: 42.4378 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 581/1000
2023-10-10 13:33:54.766 
Epoch 581/1000 
	 loss: 42.2067, MinusLogProbMetric: 42.2067, val_loss: 42.4335, val_MinusLogProbMetric: 42.4335

Epoch 581: val_loss did not improve from 42.43062
196/196 - 66s - loss: 42.2067 - MinusLogProbMetric: 42.2067 - val_loss: 42.4335 - val_MinusLogProbMetric: 42.4335 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 582/1000
2023-10-10 13:35:01.541 
Epoch 582/1000 
	 loss: 42.2072, MinusLogProbMetric: 42.2072, val_loss: 42.4354, val_MinusLogProbMetric: 42.4354

Epoch 582: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2072 - MinusLogProbMetric: 42.2072 - val_loss: 42.4354 - val_MinusLogProbMetric: 42.4354 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 583/1000
2023-10-10 13:36:07.104 
Epoch 583/1000 
	 loss: 42.2064, MinusLogProbMetric: 42.2064, val_loss: 42.4340, val_MinusLogProbMetric: 42.4340

Epoch 583: val_loss did not improve from 42.43062
196/196 - 66s - loss: 42.2064 - MinusLogProbMetric: 42.2064 - val_loss: 42.4340 - val_MinusLogProbMetric: 42.4340 - lr: 1.6935e-08 - 66s/epoch - 334ms/step
Epoch 584/1000
2023-10-10 13:37:14.149 
Epoch 584/1000 
	 loss: 42.2038, MinusLogProbMetric: 42.2038, val_loss: 42.4332, val_MinusLogProbMetric: 42.4332

Epoch 584: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2038 - MinusLogProbMetric: 42.2038 - val_loss: 42.4332 - val_MinusLogProbMetric: 42.4332 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 585/1000
2023-10-10 13:38:21.040 
Epoch 585/1000 
	 loss: 42.2040, MinusLogProbMetric: 42.2040, val_loss: 42.4332, val_MinusLogProbMetric: 42.4332

Epoch 585: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2040 - MinusLogProbMetric: 42.2040 - val_loss: 42.4332 - val_MinusLogProbMetric: 42.4332 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 586/1000
2023-10-10 13:39:26.826 
Epoch 586/1000 
	 loss: 42.2025, MinusLogProbMetric: 42.2025, val_loss: 42.4342, val_MinusLogProbMetric: 42.4342

Epoch 586: val_loss did not improve from 42.43062
196/196 - 66s - loss: 42.2025 - MinusLogProbMetric: 42.2025 - val_loss: 42.4342 - val_MinusLogProbMetric: 42.4342 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 587/1000
2023-10-10 13:40:33.187 
Epoch 587/1000 
	 loss: 42.2040, MinusLogProbMetric: 42.2040, val_loss: 42.4310, val_MinusLogProbMetric: 42.4310

Epoch 587: val_loss did not improve from 42.43062
196/196 - 66s - loss: 42.2040 - MinusLogProbMetric: 42.2040 - val_loss: 42.4310 - val_MinusLogProbMetric: 42.4310 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 588/1000
2023-10-10 13:41:40.138 
Epoch 588/1000 
	 loss: 42.2036, MinusLogProbMetric: 42.2036, val_loss: 42.4326, val_MinusLogProbMetric: 42.4326

Epoch 588: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2036 - MinusLogProbMetric: 42.2036 - val_loss: 42.4326 - val_MinusLogProbMetric: 42.4326 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 589/1000
2023-10-10 13:42:47.058 
Epoch 589/1000 
	 loss: 42.2024, MinusLogProbMetric: 42.2024, val_loss: 42.4332, val_MinusLogProbMetric: 42.4332

Epoch 589: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2024 - MinusLogProbMetric: 42.2024 - val_loss: 42.4332 - val_MinusLogProbMetric: 42.4332 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 590/1000
2023-10-10 13:43:53.844 
Epoch 590/1000 
	 loss: 42.2028, MinusLogProbMetric: 42.2028, val_loss: 42.4316, val_MinusLogProbMetric: 42.4316

Epoch 590: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2028 - MinusLogProbMetric: 42.2028 - val_loss: 42.4316 - val_MinusLogProbMetric: 42.4316 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 591/1000
2023-10-10 13:45:00.664 
Epoch 591/1000 
	 loss: 42.2013, MinusLogProbMetric: 42.2013, val_loss: 42.4354, val_MinusLogProbMetric: 42.4354

Epoch 591: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2013 - MinusLogProbMetric: 42.2013 - val_loss: 42.4354 - val_MinusLogProbMetric: 42.4354 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 592/1000
2023-10-10 13:46:07.699 
Epoch 592/1000 
	 loss: 42.2037, MinusLogProbMetric: 42.2037, val_loss: 42.4349, val_MinusLogProbMetric: 42.4349

Epoch 592: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2037 - MinusLogProbMetric: 42.2037 - val_loss: 42.4349 - val_MinusLogProbMetric: 42.4349 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 593/1000
2023-10-10 13:47:14.556 
Epoch 593/1000 
	 loss: 42.2034, MinusLogProbMetric: 42.2034, val_loss: 42.4330, val_MinusLogProbMetric: 42.4330

Epoch 593: val_loss did not improve from 42.43062
196/196 - 67s - loss: 42.2034 - MinusLogProbMetric: 42.2034 - val_loss: 42.4330 - val_MinusLogProbMetric: 42.4330 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 594/1000
2023-10-10 13:48:20.654 
Epoch 594/1000 
	 loss: 42.2062, MinusLogProbMetric: 42.2062, val_loss: 42.4313, val_MinusLogProbMetric: 42.4313

Epoch 594: val_loss did not improve from 42.43062
196/196 - 66s - loss: 42.2062 - MinusLogProbMetric: 42.2062 - val_loss: 42.4313 - val_MinusLogProbMetric: 42.4313 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 595/1000
2023-10-10 13:49:26.737 
Epoch 595/1000 
	 loss: 42.2039, MinusLogProbMetric: 42.2039, val_loss: 42.4304, val_MinusLogProbMetric: 42.4304

Epoch 595: val_loss improved from 42.43062 to 42.43044, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 42.2039 - MinusLogProbMetric: 42.2039 - val_loss: 42.4304 - val_MinusLogProbMetric: 42.4304 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 596/1000
2023-10-10 13:50:34.889 
Epoch 596/1000 
	 loss: 42.2039, MinusLogProbMetric: 42.2039, val_loss: 42.4387, val_MinusLogProbMetric: 42.4387

Epoch 596: val_loss did not improve from 42.43044
196/196 - 67s - loss: 42.2039 - MinusLogProbMetric: 42.2039 - val_loss: 42.4387 - val_MinusLogProbMetric: 42.4387 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 597/1000
2023-10-10 13:51:42.385 
Epoch 597/1000 
	 loss: 42.2067, MinusLogProbMetric: 42.2067, val_loss: 42.4357, val_MinusLogProbMetric: 42.4357

Epoch 597: val_loss did not improve from 42.43044
196/196 - 67s - loss: 42.2067 - MinusLogProbMetric: 42.2067 - val_loss: 42.4357 - val_MinusLogProbMetric: 42.4357 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 598/1000
2023-10-10 13:52:49.159 
Epoch 598/1000 
	 loss: 42.2058, MinusLogProbMetric: 42.2058, val_loss: 42.4328, val_MinusLogProbMetric: 42.4328

Epoch 598: val_loss did not improve from 42.43044
196/196 - 67s - loss: 42.2058 - MinusLogProbMetric: 42.2058 - val_loss: 42.4328 - val_MinusLogProbMetric: 42.4328 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 599/1000
2023-10-10 13:53:56.026 
Epoch 599/1000 
	 loss: 42.2043, MinusLogProbMetric: 42.2043, val_loss: 42.4335, val_MinusLogProbMetric: 42.4335

Epoch 599: val_loss did not improve from 42.43044
196/196 - 67s - loss: 42.2043 - MinusLogProbMetric: 42.2043 - val_loss: 42.4335 - val_MinusLogProbMetric: 42.4335 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 600/1000
2023-10-10 13:55:01.757 
Epoch 600/1000 
	 loss: 42.2044, MinusLogProbMetric: 42.2044, val_loss: 42.4337, val_MinusLogProbMetric: 42.4337

Epoch 600: val_loss did not improve from 42.43044
196/196 - 66s - loss: 42.2044 - MinusLogProbMetric: 42.2044 - val_loss: 42.4337 - val_MinusLogProbMetric: 42.4337 - lr: 1.6935e-08 - 66s/epoch - 335ms/step
Epoch 601/1000
2023-10-10 13:56:08.507 
Epoch 601/1000 
	 loss: 42.2043, MinusLogProbMetric: 42.2043, val_loss: 42.4334, val_MinusLogProbMetric: 42.4334

Epoch 601: val_loss did not improve from 42.43044
196/196 - 67s - loss: 42.2043 - MinusLogProbMetric: 42.2043 - val_loss: 42.4334 - val_MinusLogProbMetric: 42.4334 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 602/1000
2023-10-10 13:57:15.241 
Epoch 602/1000 
	 loss: 42.2027, MinusLogProbMetric: 42.2027, val_loss: 42.4299, val_MinusLogProbMetric: 42.4299

Epoch 602: val_loss improved from 42.43044 to 42.42991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.2027 - MinusLogProbMetric: 42.2027 - val_loss: 42.4299 - val_MinusLogProbMetric: 42.4299 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 603/1000
2023-10-10 13:58:23.034 
Epoch 603/1000 
	 loss: 42.2017, MinusLogProbMetric: 42.2017, val_loss: 42.4313, val_MinusLogProbMetric: 42.4313

Epoch 603: val_loss did not improve from 42.42991
196/196 - 67s - loss: 42.2017 - MinusLogProbMetric: 42.2017 - val_loss: 42.4313 - val_MinusLogProbMetric: 42.4313 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 604/1000
2023-10-10 13:59:29.397 
Epoch 604/1000 
	 loss: 42.2016, MinusLogProbMetric: 42.2016, val_loss: 42.4308, val_MinusLogProbMetric: 42.4308

Epoch 604: val_loss did not improve from 42.42991
196/196 - 66s - loss: 42.2016 - MinusLogProbMetric: 42.2016 - val_loss: 42.4308 - val_MinusLogProbMetric: 42.4308 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 605/1000
2023-10-10 14:00:36.003 
Epoch 605/1000 
	 loss: 42.2008, MinusLogProbMetric: 42.2008, val_loss: 42.4290, val_MinusLogProbMetric: 42.4290

Epoch 605: val_loss improved from 42.42991 to 42.42899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.2008 - MinusLogProbMetric: 42.2008 - val_loss: 42.4290 - val_MinusLogProbMetric: 42.4290 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 606/1000
2023-10-10 14:01:44.007 
Epoch 606/1000 
	 loss: 42.2019, MinusLogProbMetric: 42.2019, val_loss: 42.4273, val_MinusLogProbMetric: 42.4273

Epoch 606: val_loss improved from 42.42899 to 42.42729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.2019 - MinusLogProbMetric: 42.2019 - val_loss: 42.4273 - val_MinusLogProbMetric: 42.4273 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 607/1000
2023-10-10 14:02:51.241 
Epoch 607/1000 
	 loss: 42.2021, MinusLogProbMetric: 42.2021, val_loss: 42.4316, val_MinusLogProbMetric: 42.4316

Epoch 607: val_loss did not improve from 42.42729
196/196 - 66s - loss: 42.2021 - MinusLogProbMetric: 42.2021 - val_loss: 42.4316 - val_MinusLogProbMetric: 42.4316 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 608/1000
2023-10-10 14:03:58.265 
Epoch 608/1000 
	 loss: 42.2013, MinusLogProbMetric: 42.2013, val_loss: 42.4295, val_MinusLogProbMetric: 42.4295

Epoch 608: val_loss did not improve from 42.42729
196/196 - 67s - loss: 42.2013 - MinusLogProbMetric: 42.2013 - val_loss: 42.4295 - val_MinusLogProbMetric: 42.4295 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 609/1000
2023-10-10 14:05:04.655 
Epoch 609/1000 
	 loss: 42.1996, MinusLogProbMetric: 42.1996, val_loss: 42.4238, val_MinusLogProbMetric: 42.4238

Epoch 609: val_loss improved from 42.42729 to 42.42382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.1996 - MinusLogProbMetric: 42.1996 - val_loss: 42.4238 - val_MinusLogProbMetric: 42.4238 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 610/1000
2023-10-10 14:06:12.753 
Epoch 610/1000 
	 loss: 42.2004, MinusLogProbMetric: 42.2004, val_loss: 42.4271, val_MinusLogProbMetric: 42.4271

Epoch 610: val_loss did not improve from 42.42382
196/196 - 67s - loss: 42.2004 - MinusLogProbMetric: 42.2004 - val_loss: 42.4271 - val_MinusLogProbMetric: 42.4271 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 611/1000
2023-10-10 14:07:19.299 
Epoch 611/1000 
	 loss: 42.1999, MinusLogProbMetric: 42.1999, val_loss: 42.4260, val_MinusLogProbMetric: 42.4260

Epoch 611: val_loss did not improve from 42.42382
196/196 - 67s - loss: 42.1999 - MinusLogProbMetric: 42.1999 - val_loss: 42.4260 - val_MinusLogProbMetric: 42.4260 - lr: 1.6935e-08 - 67s/epoch - 339ms/step
Epoch 612/1000
2023-10-10 14:08:28.253 
Epoch 612/1000 
	 loss: 42.2022, MinusLogProbMetric: 42.2022, val_loss: 42.4322, val_MinusLogProbMetric: 42.4322

Epoch 612: val_loss did not improve from 42.42382
196/196 - 69s - loss: 42.2022 - MinusLogProbMetric: 42.2022 - val_loss: 42.4322 - val_MinusLogProbMetric: 42.4322 - lr: 1.6935e-08 - 69s/epoch - 352ms/step
Epoch 613/1000
2023-10-10 14:09:39.941 
Epoch 613/1000 
	 loss: 42.2043, MinusLogProbMetric: 42.2043, val_loss: 42.4311, val_MinusLogProbMetric: 42.4311

Epoch 613: val_loss did not improve from 42.42382
196/196 - 72s - loss: 42.2043 - MinusLogProbMetric: 42.2043 - val_loss: 42.4311 - val_MinusLogProbMetric: 42.4311 - lr: 1.6935e-08 - 72s/epoch - 366ms/step
Epoch 614/1000
2023-10-10 14:10:49.668 
Epoch 614/1000 
	 loss: 42.2014, MinusLogProbMetric: 42.2014, val_loss: 42.4318, val_MinusLogProbMetric: 42.4318

Epoch 614: val_loss did not improve from 42.42382
196/196 - 70s - loss: 42.2014 - MinusLogProbMetric: 42.2014 - val_loss: 42.4318 - val_MinusLogProbMetric: 42.4318 - lr: 1.6935e-08 - 70s/epoch - 356ms/step
Epoch 615/1000
2023-10-10 14:11:58.670 
Epoch 615/1000 
	 loss: 42.2004, MinusLogProbMetric: 42.2004, val_loss: 42.4255, val_MinusLogProbMetric: 42.4255

Epoch 615: val_loss did not improve from 42.42382
196/196 - 69s - loss: 42.2004 - MinusLogProbMetric: 42.2004 - val_loss: 42.4255 - val_MinusLogProbMetric: 42.4255 - lr: 1.6935e-08 - 69s/epoch - 352ms/step
Epoch 616/1000
2023-10-10 14:13:08.200 
Epoch 616/1000 
	 loss: 42.2000, MinusLogProbMetric: 42.2000, val_loss: 42.4274, val_MinusLogProbMetric: 42.4274

Epoch 616: val_loss did not improve from 42.42382
196/196 - 70s - loss: 42.2000 - MinusLogProbMetric: 42.2000 - val_loss: 42.4274 - val_MinusLogProbMetric: 42.4274 - lr: 1.6935e-08 - 70s/epoch - 355ms/step
Epoch 617/1000
2023-10-10 14:14:16.474 
Epoch 617/1000 
	 loss: 42.2014, MinusLogProbMetric: 42.2014, val_loss: 42.4319, val_MinusLogProbMetric: 42.4319

Epoch 617: val_loss did not improve from 42.42382
196/196 - 68s - loss: 42.2014 - MinusLogProbMetric: 42.2014 - val_loss: 42.4319 - val_MinusLogProbMetric: 42.4319 - lr: 1.6935e-08 - 68s/epoch - 348ms/step
Epoch 618/1000
2023-10-10 14:15:23.758 
Epoch 618/1000 
	 loss: 42.2059, MinusLogProbMetric: 42.2059, val_loss: 42.4287, val_MinusLogProbMetric: 42.4287

Epoch 618: val_loss did not improve from 42.42382
196/196 - 67s - loss: 42.2059 - MinusLogProbMetric: 42.2059 - val_loss: 42.4287 - val_MinusLogProbMetric: 42.4287 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 619/1000
2023-10-10 14:16:31.937 
Epoch 619/1000 
	 loss: 42.2020, MinusLogProbMetric: 42.2020, val_loss: 42.4305, val_MinusLogProbMetric: 42.4305

Epoch 619: val_loss did not improve from 42.42382
196/196 - 68s - loss: 42.2020 - MinusLogProbMetric: 42.2020 - val_loss: 42.4305 - val_MinusLogProbMetric: 42.4305 - lr: 1.6935e-08 - 68s/epoch - 348ms/step
Epoch 620/1000
2023-10-10 14:17:39.269 
Epoch 620/1000 
	 loss: 42.2013, MinusLogProbMetric: 42.2013, val_loss: 42.4253, val_MinusLogProbMetric: 42.4253

Epoch 620: val_loss did not improve from 42.42382
196/196 - 67s - loss: 42.2013 - MinusLogProbMetric: 42.2013 - val_loss: 42.4253 - val_MinusLogProbMetric: 42.4253 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 621/1000
2023-10-10 14:18:46.355 
Epoch 621/1000 
	 loss: 42.2030, MinusLogProbMetric: 42.2030, val_loss: 42.4263, val_MinusLogProbMetric: 42.4263

Epoch 621: val_loss did not improve from 42.42382
196/196 - 67s - loss: 42.2030 - MinusLogProbMetric: 42.2030 - val_loss: 42.4263 - val_MinusLogProbMetric: 42.4263 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 622/1000
2023-10-10 14:19:52.595 
Epoch 622/1000 
	 loss: 42.1998, MinusLogProbMetric: 42.1998, val_loss: 42.4289, val_MinusLogProbMetric: 42.4289

Epoch 622: val_loss did not improve from 42.42382
196/196 - 66s - loss: 42.1998 - MinusLogProbMetric: 42.1998 - val_loss: 42.4289 - val_MinusLogProbMetric: 42.4289 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 623/1000
2023-10-10 14:20:58.486 
Epoch 623/1000 
	 loss: 42.1985, MinusLogProbMetric: 42.1985, val_loss: 42.4243, val_MinusLogProbMetric: 42.4243

Epoch 623: val_loss did not improve from 42.42382
196/196 - 66s - loss: 42.1985 - MinusLogProbMetric: 42.1985 - val_loss: 42.4243 - val_MinusLogProbMetric: 42.4243 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 624/1000
2023-10-10 14:22:04.893 
Epoch 624/1000 
	 loss: 42.1983, MinusLogProbMetric: 42.1983, val_loss: 42.4267, val_MinusLogProbMetric: 42.4267

Epoch 624: val_loss did not improve from 42.42382
196/196 - 66s - loss: 42.1983 - MinusLogProbMetric: 42.1983 - val_loss: 42.4267 - val_MinusLogProbMetric: 42.4267 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 625/1000
2023-10-10 14:23:12.099 
Epoch 625/1000 
	 loss: 42.1991, MinusLogProbMetric: 42.1991, val_loss: 42.4246, val_MinusLogProbMetric: 42.4246

Epoch 625: val_loss did not improve from 42.42382
196/196 - 67s - loss: 42.1991 - MinusLogProbMetric: 42.1991 - val_loss: 42.4246 - val_MinusLogProbMetric: 42.4246 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 626/1000
2023-10-10 14:24:18.440 
Epoch 626/1000 
	 loss: 42.1992, MinusLogProbMetric: 42.1992, val_loss: 42.4254, val_MinusLogProbMetric: 42.4254

Epoch 626: val_loss did not improve from 42.42382
196/196 - 66s - loss: 42.1992 - MinusLogProbMetric: 42.1992 - val_loss: 42.4254 - val_MinusLogProbMetric: 42.4254 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 627/1000
2023-10-10 14:25:25.300 
Epoch 627/1000 
	 loss: 42.1976, MinusLogProbMetric: 42.1976, val_loss: 42.4243, val_MinusLogProbMetric: 42.4243

Epoch 627: val_loss did not improve from 42.42382
196/196 - 67s - loss: 42.1976 - MinusLogProbMetric: 42.1976 - val_loss: 42.4243 - val_MinusLogProbMetric: 42.4243 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 628/1000
2023-10-10 14:26:31.695 
Epoch 628/1000 
	 loss: 42.1968, MinusLogProbMetric: 42.1968, val_loss: 42.4235, val_MinusLogProbMetric: 42.4235

Epoch 628: val_loss improved from 42.42382 to 42.42353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.1968 - MinusLogProbMetric: 42.1968 - val_loss: 42.4235 - val_MinusLogProbMetric: 42.4235 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 629/1000
2023-10-10 14:27:38.907 
Epoch 629/1000 
	 loss: 42.1972, MinusLogProbMetric: 42.1972, val_loss: 42.4243, val_MinusLogProbMetric: 42.4243

Epoch 629: val_loss did not improve from 42.42353
196/196 - 66s - loss: 42.1972 - MinusLogProbMetric: 42.1972 - val_loss: 42.4243 - val_MinusLogProbMetric: 42.4243 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 630/1000
2023-10-10 14:28:46.290 
Epoch 630/1000 
	 loss: 42.1966, MinusLogProbMetric: 42.1966, val_loss: 42.4230, val_MinusLogProbMetric: 42.4230

Epoch 630: val_loss improved from 42.42353 to 42.42304, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.1966 - MinusLogProbMetric: 42.1966 - val_loss: 42.4230 - val_MinusLogProbMetric: 42.4230 - lr: 1.6935e-08 - 68s/epoch - 349ms/step
Epoch 631/1000
2023-10-10 14:29:54.722 
Epoch 631/1000 
	 loss: 42.1982, MinusLogProbMetric: 42.1982, val_loss: 42.4240, val_MinusLogProbMetric: 42.4240

Epoch 631: val_loss did not improve from 42.42304
196/196 - 67s - loss: 42.1982 - MinusLogProbMetric: 42.1982 - val_loss: 42.4240 - val_MinusLogProbMetric: 42.4240 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 632/1000
2023-10-10 14:31:02.216 
Epoch 632/1000 
	 loss: 42.2004, MinusLogProbMetric: 42.2004, val_loss: 42.4230, val_MinusLogProbMetric: 42.4230

Epoch 632: val_loss did not improve from 42.42304
196/196 - 67s - loss: 42.2004 - MinusLogProbMetric: 42.2004 - val_loss: 42.4230 - val_MinusLogProbMetric: 42.4230 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 633/1000
2023-10-10 14:32:09.520 
Epoch 633/1000 
	 loss: 42.1999, MinusLogProbMetric: 42.1999, val_loss: 42.4277, val_MinusLogProbMetric: 42.4277

Epoch 633: val_loss did not improve from 42.42304
196/196 - 67s - loss: 42.1999 - MinusLogProbMetric: 42.1999 - val_loss: 42.4277 - val_MinusLogProbMetric: 42.4277 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 634/1000
2023-10-10 14:33:16.170 
Epoch 634/1000 
	 loss: 42.1986, MinusLogProbMetric: 42.1986, val_loss: 42.4238, val_MinusLogProbMetric: 42.4238

Epoch 634: val_loss did not improve from 42.42304
196/196 - 67s - loss: 42.1986 - MinusLogProbMetric: 42.1986 - val_loss: 42.4238 - val_MinusLogProbMetric: 42.4238 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 635/1000
2023-10-10 14:34:22.083 
Epoch 635/1000 
	 loss: 42.1966, MinusLogProbMetric: 42.1966, val_loss: 42.4248, val_MinusLogProbMetric: 42.4248

Epoch 635: val_loss did not improve from 42.42304
196/196 - 66s - loss: 42.1966 - MinusLogProbMetric: 42.1966 - val_loss: 42.4248 - val_MinusLogProbMetric: 42.4248 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 636/1000
2023-10-10 14:35:29.152 
Epoch 636/1000 
	 loss: 42.1979, MinusLogProbMetric: 42.1979, val_loss: 42.4240, val_MinusLogProbMetric: 42.4240

Epoch 636: val_loss did not improve from 42.42304
196/196 - 67s - loss: 42.1979 - MinusLogProbMetric: 42.1979 - val_loss: 42.4240 - val_MinusLogProbMetric: 42.4240 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 637/1000
2023-10-10 14:36:36.622 
Epoch 637/1000 
	 loss: 42.1970, MinusLogProbMetric: 42.1970, val_loss: 42.4239, val_MinusLogProbMetric: 42.4239

Epoch 637: val_loss did not improve from 42.42304
196/196 - 67s - loss: 42.1970 - MinusLogProbMetric: 42.1970 - val_loss: 42.4239 - val_MinusLogProbMetric: 42.4239 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 638/1000
2023-10-10 14:37:43.915 
Epoch 638/1000 
	 loss: 42.1971, MinusLogProbMetric: 42.1971, val_loss: 42.4211, val_MinusLogProbMetric: 42.4211

Epoch 638: val_loss improved from 42.42304 to 42.42110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.1971 - MinusLogProbMetric: 42.1971 - val_loss: 42.4211 - val_MinusLogProbMetric: 42.4211 - lr: 1.6935e-08 - 68s/epoch - 349ms/step
Epoch 639/1000
2023-10-10 14:38:51.396 
Epoch 639/1000 
	 loss: 42.1963, MinusLogProbMetric: 42.1963, val_loss: 42.4232, val_MinusLogProbMetric: 42.4232

Epoch 639: val_loss did not improve from 42.42110
196/196 - 66s - loss: 42.1963 - MinusLogProbMetric: 42.1963 - val_loss: 42.4232 - val_MinusLogProbMetric: 42.4232 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 640/1000
2023-10-10 14:39:58.283 
Epoch 640/1000 
	 loss: 42.1964, MinusLogProbMetric: 42.1964, val_loss: 42.4249, val_MinusLogProbMetric: 42.4249

Epoch 640: val_loss did not improve from 42.42110
196/196 - 67s - loss: 42.1964 - MinusLogProbMetric: 42.1964 - val_loss: 42.4249 - val_MinusLogProbMetric: 42.4249 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 641/1000
2023-10-10 14:41:05.694 
Epoch 641/1000 
	 loss: 42.1967, MinusLogProbMetric: 42.1967, val_loss: 42.4237, val_MinusLogProbMetric: 42.4237

Epoch 641: val_loss did not improve from 42.42110
196/196 - 67s - loss: 42.1967 - MinusLogProbMetric: 42.1967 - val_loss: 42.4237 - val_MinusLogProbMetric: 42.4237 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 642/1000
2023-10-10 14:42:12.519 
Epoch 642/1000 
	 loss: 42.1964, MinusLogProbMetric: 42.1964, val_loss: 42.4225, val_MinusLogProbMetric: 42.4225

Epoch 642: val_loss did not improve from 42.42110
196/196 - 67s - loss: 42.1964 - MinusLogProbMetric: 42.1964 - val_loss: 42.4225 - val_MinusLogProbMetric: 42.4225 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 643/1000
2023-10-10 14:43:19.185 
Epoch 643/1000 
	 loss: 42.1968, MinusLogProbMetric: 42.1968, val_loss: 42.4240, val_MinusLogProbMetric: 42.4240

Epoch 643: val_loss did not improve from 42.42110
196/196 - 67s - loss: 42.1968 - MinusLogProbMetric: 42.1968 - val_loss: 42.4240 - val_MinusLogProbMetric: 42.4240 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 644/1000
2023-10-10 14:44:26.357 
Epoch 644/1000 
	 loss: 42.1941, MinusLogProbMetric: 42.1941, val_loss: 42.4220, val_MinusLogProbMetric: 42.4220

Epoch 644: val_loss did not improve from 42.42110
196/196 - 67s - loss: 42.1941 - MinusLogProbMetric: 42.1941 - val_loss: 42.4220 - val_MinusLogProbMetric: 42.4220 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 645/1000
2023-10-10 14:45:33.805 
Epoch 645/1000 
	 loss: 42.1949, MinusLogProbMetric: 42.1949, val_loss: 42.4210, val_MinusLogProbMetric: 42.4210

Epoch 645: val_loss improved from 42.42110 to 42.42095, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.1949 - MinusLogProbMetric: 42.1949 - val_loss: 42.4210 - val_MinusLogProbMetric: 42.4210 - lr: 1.6935e-08 - 68s/epoch - 349ms/step
Epoch 646/1000
2023-10-10 14:46:42.492 
Epoch 646/1000 
	 loss: 42.1938, MinusLogProbMetric: 42.1938, val_loss: 42.4236, val_MinusLogProbMetric: 42.4236

Epoch 646: val_loss did not improve from 42.42095
196/196 - 68s - loss: 42.1938 - MinusLogProbMetric: 42.1938 - val_loss: 42.4236 - val_MinusLogProbMetric: 42.4236 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 647/1000
2023-10-10 14:47:50.455 
Epoch 647/1000 
	 loss: 42.1952, MinusLogProbMetric: 42.1952, val_loss: 42.4229, val_MinusLogProbMetric: 42.4229

Epoch 647: val_loss did not improve from 42.42095
196/196 - 68s - loss: 42.1952 - MinusLogProbMetric: 42.1952 - val_loss: 42.4229 - val_MinusLogProbMetric: 42.4229 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 648/1000
2023-10-10 14:48:59.514 
Epoch 648/1000 
	 loss: 42.1941, MinusLogProbMetric: 42.1941, val_loss: 42.4237, val_MinusLogProbMetric: 42.4237

Epoch 648: val_loss did not improve from 42.42095
196/196 - 69s - loss: 42.1941 - MinusLogProbMetric: 42.1941 - val_loss: 42.4237 - val_MinusLogProbMetric: 42.4237 - lr: 1.6935e-08 - 69s/epoch - 352ms/step
Epoch 649/1000
2023-10-10 14:50:07.302 
Epoch 649/1000 
	 loss: 42.1953, MinusLogProbMetric: 42.1953, val_loss: 42.4220, val_MinusLogProbMetric: 42.4220

Epoch 649: val_loss did not improve from 42.42095
196/196 - 68s - loss: 42.1953 - MinusLogProbMetric: 42.1953 - val_loss: 42.4220 - val_MinusLogProbMetric: 42.4220 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 650/1000
2023-10-10 14:51:13.482 
Epoch 650/1000 
	 loss: 42.1928, MinusLogProbMetric: 42.1928, val_loss: 42.4217, val_MinusLogProbMetric: 42.4217

Epoch 650: val_loss did not improve from 42.42095
196/196 - 66s - loss: 42.1928 - MinusLogProbMetric: 42.1928 - val_loss: 42.4217 - val_MinusLogProbMetric: 42.4217 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 651/1000
2023-10-10 14:52:19.805 
Epoch 651/1000 
	 loss: 42.1940, MinusLogProbMetric: 42.1940, val_loss: 42.4224, val_MinusLogProbMetric: 42.4224

Epoch 651: val_loss did not improve from 42.42095
196/196 - 66s - loss: 42.1940 - MinusLogProbMetric: 42.1940 - val_loss: 42.4224 - val_MinusLogProbMetric: 42.4224 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 652/1000
2023-10-10 14:53:26.870 
Epoch 652/1000 
	 loss: 42.1937, MinusLogProbMetric: 42.1937, val_loss: 42.4235, val_MinusLogProbMetric: 42.4235

Epoch 652: val_loss did not improve from 42.42095
196/196 - 67s - loss: 42.1937 - MinusLogProbMetric: 42.1937 - val_loss: 42.4235 - val_MinusLogProbMetric: 42.4235 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 653/1000
2023-10-10 14:54:34.764 
Epoch 653/1000 
	 loss: 42.1935, MinusLogProbMetric: 42.1935, val_loss: 42.4230, val_MinusLogProbMetric: 42.4230

Epoch 653: val_loss did not improve from 42.42095
196/196 - 68s - loss: 42.1935 - MinusLogProbMetric: 42.1935 - val_loss: 42.4230 - val_MinusLogProbMetric: 42.4230 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 654/1000
2023-10-10 14:55:42.122 
Epoch 654/1000 
	 loss: 42.1928, MinusLogProbMetric: 42.1928, val_loss: 42.4222, val_MinusLogProbMetric: 42.4222

Epoch 654: val_loss did not improve from 42.42095
196/196 - 67s - loss: 42.1928 - MinusLogProbMetric: 42.1928 - val_loss: 42.4222 - val_MinusLogProbMetric: 42.4222 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 655/1000
2023-10-10 14:56:49.907 
Epoch 655/1000 
	 loss: 42.1985, MinusLogProbMetric: 42.1985, val_loss: 42.4295, val_MinusLogProbMetric: 42.4295

Epoch 655: val_loss did not improve from 42.42095
196/196 - 68s - loss: 42.1985 - MinusLogProbMetric: 42.1985 - val_loss: 42.4295 - val_MinusLogProbMetric: 42.4295 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 656/1000
2023-10-10 14:57:55.172 
Epoch 656/1000 
	 loss: 42.1988, MinusLogProbMetric: 42.1988, val_loss: 42.4294, val_MinusLogProbMetric: 42.4294

Epoch 656: val_loss did not improve from 42.42095
196/196 - 65s - loss: 42.1988 - MinusLogProbMetric: 42.1988 - val_loss: 42.4294 - val_MinusLogProbMetric: 42.4294 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 657/1000
2023-10-10 14:59:01.631 
Epoch 657/1000 
	 loss: 42.1970, MinusLogProbMetric: 42.1970, val_loss: 42.4234, val_MinusLogProbMetric: 42.4234

Epoch 657: val_loss did not improve from 42.42095
196/196 - 66s - loss: 42.1970 - MinusLogProbMetric: 42.1970 - val_loss: 42.4234 - val_MinusLogProbMetric: 42.4234 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 658/1000
2023-10-10 15:00:08.450 
Epoch 658/1000 
	 loss: 42.1965, MinusLogProbMetric: 42.1965, val_loss: 42.4235, val_MinusLogProbMetric: 42.4235

Epoch 658: val_loss did not improve from 42.42095
196/196 - 67s - loss: 42.1965 - MinusLogProbMetric: 42.1965 - val_loss: 42.4235 - val_MinusLogProbMetric: 42.4235 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 659/1000
2023-10-10 15:01:14.940 
Epoch 659/1000 
	 loss: 42.1953, MinusLogProbMetric: 42.1953, val_loss: 42.4226, val_MinusLogProbMetric: 42.4226

Epoch 659: val_loss did not improve from 42.42095
196/196 - 66s - loss: 42.1953 - MinusLogProbMetric: 42.1953 - val_loss: 42.4226 - val_MinusLogProbMetric: 42.4226 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 660/1000
2023-10-10 15:02:20.127 
Epoch 660/1000 
	 loss: 42.1950, MinusLogProbMetric: 42.1950, val_loss: 42.4210, val_MinusLogProbMetric: 42.4210

Epoch 660: val_loss did not improve from 42.42095
196/196 - 65s - loss: 42.1950 - MinusLogProbMetric: 42.1950 - val_loss: 42.4210 - val_MinusLogProbMetric: 42.4210 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 661/1000
2023-10-10 15:03:26.517 
Epoch 661/1000 
	 loss: 42.1934, MinusLogProbMetric: 42.1934, val_loss: 42.4207, val_MinusLogProbMetric: 42.4207

Epoch 661: val_loss improved from 42.42095 to 42.42067, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.1934 - MinusLogProbMetric: 42.1934 - val_loss: 42.4207 - val_MinusLogProbMetric: 42.4207 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 662/1000
2023-10-10 15:04:36.312 
Epoch 662/1000 
	 loss: 42.1938, MinusLogProbMetric: 42.1938, val_loss: 42.4245, val_MinusLogProbMetric: 42.4245

Epoch 662: val_loss did not improve from 42.42067
196/196 - 68s - loss: 42.1938 - MinusLogProbMetric: 42.1938 - val_loss: 42.4245 - val_MinusLogProbMetric: 42.4245 - lr: 1.6935e-08 - 68s/epoch - 348ms/step
Epoch 663/1000
2023-10-10 15:05:41.657 
Epoch 663/1000 
	 loss: 42.1917, MinusLogProbMetric: 42.1917, val_loss: 42.4232, val_MinusLogProbMetric: 42.4232

Epoch 663: val_loss did not improve from 42.42067
196/196 - 65s - loss: 42.1917 - MinusLogProbMetric: 42.1917 - val_loss: 42.4232 - val_MinusLogProbMetric: 42.4232 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 664/1000
2023-10-10 15:06:48.482 
Epoch 664/1000 
	 loss: 42.1918, MinusLogProbMetric: 42.1918, val_loss: 42.4206, val_MinusLogProbMetric: 42.4206

Epoch 664: val_loss improved from 42.42067 to 42.42062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.1918 - MinusLogProbMetric: 42.1918 - val_loss: 42.4206 - val_MinusLogProbMetric: 42.4206 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 665/1000
2023-10-10 15:07:57.328 
Epoch 665/1000 
	 loss: 42.1930, MinusLogProbMetric: 42.1930, val_loss: 42.4156, val_MinusLogProbMetric: 42.4156

Epoch 665: val_loss improved from 42.42062 to 42.41557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 69s - loss: 42.1930 - MinusLogProbMetric: 42.1930 - val_loss: 42.4156 - val_MinusLogProbMetric: 42.4156 - lr: 1.6935e-08 - 69s/epoch - 353ms/step
Epoch 666/1000
2023-10-10 15:09:04.576 
Epoch 666/1000 
	 loss: 42.1926, MinusLogProbMetric: 42.1926, val_loss: 42.4174, val_MinusLogProbMetric: 42.4174

Epoch 666: val_loss did not improve from 42.41557
196/196 - 66s - loss: 42.1926 - MinusLogProbMetric: 42.1926 - val_loss: 42.4174 - val_MinusLogProbMetric: 42.4174 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 667/1000
2023-10-10 15:10:11.526 
Epoch 667/1000 
	 loss: 42.1915, MinusLogProbMetric: 42.1915, val_loss: 42.4196, val_MinusLogProbMetric: 42.4196

Epoch 667: val_loss did not improve from 42.41557
196/196 - 67s - loss: 42.1915 - MinusLogProbMetric: 42.1915 - val_loss: 42.4196 - val_MinusLogProbMetric: 42.4196 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 668/1000
2023-10-10 15:11:18.966 
Epoch 668/1000 
	 loss: 42.1930, MinusLogProbMetric: 42.1930, val_loss: 42.4223, val_MinusLogProbMetric: 42.4223

Epoch 668: val_loss did not improve from 42.41557
196/196 - 67s - loss: 42.1930 - MinusLogProbMetric: 42.1930 - val_loss: 42.4223 - val_MinusLogProbMetric: 42.4223 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 669/1000
2023-10-10 15:12:25.291 
Epoch 669/1000 
	 loss: 42.1930, MinusLogProbMetric: 42.1930, val_loss: 42.4216, val_MinusLogProbMetric: 42.4216

Epoch 669: val_loss did not improve from 42.41557
196/196 - 66s - loss: 42.1930 - MinusLogProbMetric: 42.1930 - val_loss: 42.4216 - val_MinusLogProbMetric: 42.4216 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 670/1000
2023-10-10 15:13:30.899 
Epoch 670/1000 
	 loss: 42.1925, MinusLogProbMetric: 42.1925, val_loss: 42.4202, val_MinusLogProbMetric: 42.4202

Epoch 670: val_loss did not improve from 42.41557
196/196 - 66s - loss: 42.1925 - MinusLogProbMetric: 42.1925 - val_loss: 42.4202 - val_MinusLogProbMetric: 42.4202 - lr: 1.6935e-08 - 66s/epoch - 335ms/step
Epoch 671/1000
2023-10-10 15:14:38.536 
Epoch 671/1000 
	 loss: 42.1940, MinusLogProbMetric: 42.1940, val_loss: 42.4171, val_MinusLogProbMetric: 42.4171

Epoch 671: val_loss did not improve from 42.41557
196/196 - 68s - loss: 42.1940 - MinusLogProbMetric: 42.1940 - val_loss: 42.4171 - val_MinusLogProbMetric: 42.4171 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 672/1000
2023-10-10 15:15:44.827 
Epoch 672/1000 
	 loss: 42.1932, MinusLogProbMetric: 42.1932, val_loss: 42.4226, val_MinusLogProbMetric: 42.4226

Epoch 672: val_loss did not improve from 42.41557
196/196 - 66s - loss: 42.1932 - MinusLogProbMetric: 42.1932 - val_loss: 42.4226 - val_MinusLogProbMetric: 42.4226 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 673/1000
2023-10-10 15:16:50.755 
Epoch 673/1000 
	 loss: 42.1936, MinusLogProbMetric: 42.1936, val_loss: 42.4230, val_MinusLogProbMetric: 42.4230

Epoch 673: val_loss did not improve from 42.41557
196/196 - 66s - loss: 42.1936 - MinusLogProbMetric: 42.1936 - val_loss: 42.4230 - val_MinusLogProbMetric: 42.4230 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 674/1000
2023-10-10 15:17:57.051 
Epoch 674/1000 
	 loss: 42.1934, MinusLogProbMetric: 42.1934, val_loss: 42.4232, val_MinusLogProbMetric: 42.4232

Epoch 674: val_loss did not improve from 42.41557
196/196 - 66s - loss: 42.1934 - MinusLogProbMetric: 42.1934 - val_loss: 42.4232 - val_MinusLogProbMetric: 42.4232 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 675/1000
2023-10-10 15:19:04.044 
Epoch 675/1000 
	 loss: 42.1917, MinusLogProbMetric: 42.1917, val_loss: 42.4250, val_MinusLogProbMetric: 42.4250

Epoch 675: val_loss did not improve from 42.41557
196/196 - 67s - loss: 42.1917 - MinusLogProbMetric: 42.1917 - val_loss: 42.4250 - val_MinusLogProbMetric: 42.4250 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 676/1000
2023-10-10 15:20:10.267 
Epoch 676/1000 
	 loss: 42.1914, MinusLogProbMetric: 42.1914, val_loss: 42.4207, val_MinusLogProbMetric: 42.4207

Epoch 676: val_loss did not improve from 42.41557
196/196 - 66s - loss: 42.1914 - MinusLogProbMetric: 42.1914 - val_loss: 42.4207 - val_MinusLogProbMetric: 42.4207 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 677/1000
2023-10-10 15:21:17.041 
Epoch 677/1000 
	 loss: 42.1915, MinusLogProbMetric: 42.1915, val_loss: 42.4208, val_MinusLogProbMetric: 42.4208

Epoch 677: val_loss did not improve from 42.41557
196/196 - 67s - loss: 42.1915 - MinusLogProbMetric: 42.1915 - val_loss: 42.4208 - val_MinusLogProbMetric: 42.4208 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 678/1000
2023-10-10 15:22:24.237 
Epoch 678/1000 
	 loss: 42.1914, MinusLogProbMetric: 42.1914, val_loss: 42.4227, val_MinusLogProbMetric: 42.4227

Epoch 678: val_loss did not improve from 42.41557
196/196 - 67s - loss: 42.1914 - MinusLogProbMetric: 42.1914 - val_loss: 42.4227 - val_MinusLogProbMetric: 42.4227 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 679/1000
2023-10-10 15:23:28.830 
Epoch 679/1000 
	 loss: 42.1903, MinusLogProbMetric: 42.1903, val_loss: 42.4202, val_MinusLogProbMetric: 42.4202

Epoch 679: val_loss did not improve from 42.41557
196/196 - 65s - loss: 42.1903 - MinusLogProbMetric: 42.1903 - val_loss: 42.4202 - val_MinusLogProbMetric: 42.4202 - lr: 1.6935e-08 - 65s/epoch - 330ms/step
Epoch 680/1000
2023-10-10 15:24:35.085 
Epoch 680/1000 
	 loss: 42.1899, MinusLogProbMetric: 42.1899, val_loss: 42.4161, val_MinusLogProbMetric: 42.4161

Epoch 680: val_loss did not improve from 42.41557
196/196 - 66s - loss: 42.1899 - MinusLogProbMetric: 42.1899 - val_loss: 42.4161 - val_MinusLogProbMetric: 42.4161 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 681/1000
2023-10-10 15:25:41.147 
Epoch 681/1000 
	 loss: 42.1883, MinusLogProbMetric: 42.1883, val_loss: 42.4184, val_MinusLogProbMetric: 42.4184

Epoch 681: val_loss did not improve from 42.41557
196/196 - 66s - loss: 42.1883 - MinusLogProbMetric: 42.1883 - val_loss: 42.4184 - val_MinusLogProbMetric: 42.4184 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 682/1000
2023-10-10 15:26:49.661 
Epoch 682/1000 
	 loss: 42.1886, MinusLogProbMetric: 42.1886, val_loss: 42.4201, val_MinusLogProbMetric: 42.4201

Epoch 682: val_loss did not improve from 42.41557
196/196 - 69s - loss: 42.1886 - MinusLogProbMetric: 42.1886 - val_loss: 42.4201 - val_MinusLogProbMetric: 42.4201 - lr: 1.6935e-08 - 69s/epoch - 350ms/step
Epoch 683/1000
2023-10-10 15:27:54.980 
Epoch 683/1000 
	 loss: 42.1891, MinusLogProbMetric: 42.1891, val_loss: 42.4223, val_MinusLogProbMetric: 42.4223

Epoch 683: val_loss did not improve from 42.41557
196/196 - 65s - loss: 42.1891 - MinusLogProbMetric: 42.1891 - val_loss: 42.4223 - val_MinusLogProbMetric: 42.4223 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 684/1000
2023-10-10 15:29:01.814 
Epoch 684/1000 
	 loss: 42.1901, MinusLogProbMetric: 42.1901, val_loss: 42.4203, val_MinusLogProbMetric: 42.4203

Epoch 684: val_loss did not improve from 42.41557
196/196 - 67s - loss: 42.1901 - MinusLogProbMetric: 42.1901 - val_loss: 42.4203 - val_MinusLogProbMetric: 42.4203 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 685/1000
2023-10-10 15:30:07.892 
Epoch 685/1000 
	 loss: 42.1903, MinusLogProbMetric: 42.1903, val_loss: 42.4206, val_MinusLogProbMetric: 42.4206

Epoch 685: val_loss did not improve from 42.41557
196/196 - 66s - loss: 42.1903 - MinusLogProbMetric: 42.1903 - val_loss: 42.4206 - val_MinusLogProbMetric: 42.4206 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 686/1000
2023-10-10 15:31:15.205 
Epoch 686/1000 
	 loss: 42.1878, MinusLogProbMetric: 42.1878, val_loss: 42.4206, val_MinusLogProbMetric: 42.4206

Epoch 686: val_loss did not improve from 42.41557
196/196 - 67s - loss: 42.1878 - MinusLogProbMetric: 42.1878 - val_loss: 42.4206 - val_MinusLogProbMetric: 42.4206 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 687/1000
2023-10-10 15:32:21.097 
Epoch 687/1000 
	 loss: 42.1883, MinusLogProbMetric: 42.1883, val_loss: 42.4190, val_MinusLogProbMetric: 42.4190

Epoch 687: val_loss did not improve from 42.41557
196/196 - 66s - loss: 42.1883 - MinusLogProbMetric: 42.1883 - val_loss: 42.4190 - val_MinusLogProbMetric: 42.4190 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 688/1000
2023-10-10 15:33:27.835 
Epoch 688/1000 
	 loss: 42.1895, MinusLogProbMetric: 42.1895, val_loss: 42.4181, val_MinusLogProbMetric: 42.4181

Epoch 688: val_loss did not improve from 42.41557
196/196 - 67s - loss: 42.1895 - MinusLogProbMetric: 42.1895 - val_loss: 42.4181 - val_MinusLogProbMetric: 42.4181 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 689/1000
2023-10-10 15:34:34.898 
Epoch 689/1000 
	 loss: 42.1879, MinusLogProbMetric: 42.1879, val_loss: 42.4234, val_MinusLogProbMetric: 42.4234

Epoch 689: val_loss did not improve from 42.41557
196/196 - 67s - loss: 42.1879 - MinusLogProbMetric: 42.1879 - val_loss: 42.4234 - val_MinusLogProbMetric: 42.4234 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 690/1000
2023-10-10 15:35:42.180 
Epoch 690/1000 
	 loss: 42.1890, MinusLogProbMetric: 42.1890, val_loss: 42.4205, val_MinusLogProbMetric: 42.4205

Epoch 690: val_loss did not improve from 42.41557
196/196 - 67s - loss: 42.1890 - MinusLogProbMetric: 42.1890 - val_loss: 42.4205 - val_MinusLogProbMetric: 42.4205 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 691/1000
2023-10-10 15:36:48.682 
Epoch 691/1000 
	 loss: 42.1858, MinusLogProbMetric: 42.1858, val_loss: 42.4190, val_MinusLogProbMetric: 42.4190

Epoch 691: val_loss did not improve from 42.41557
196/196 - 66s - loss: 42.1858 - MinusLogProbMetric: 42.1858 - val_loss: 42.4190 - val_MinusLogProbMetric: 42.4190 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 692/1000
2023-10-10 15:37:56.600 
Epoch 692/1000 
	 loss: 42.1874, MinusLogProbMetric: 42.1874, val_loss: 42.4189, val_MinusLogProbMetric: 42.4189

Epoch 692: val_loss did not improve from 42.41557
196/196 - 68s - loss: 42.1874 - MinusLogProbMetric: 42.1874 - val_loss: 42.4189 - val_MinusLogProbMetric: 42.4189 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 693/1000
2023-10-10 15:39:03.501 
Epoch 693/1000 
	 loss: 42.1880, MinusLogProbMetric: 42.1880, val_loss: 42.4167, val_MinusLogProbMetric: 42.4167

Epoch 693: val_loss did not improve from 42.41557
196/196 - 67s - loss: 42.1880 - MinusLogProbMetric: 42.1880 - val_loss: 42.4167 - val_MinusLogProbMetric: 42.4167 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 694/1000
2023-10-10 15:40:08.907 
Epoch 694/1000 
	 loss: 42.1870, MinusLogProbMetric: 42.1870, val_loss: 42.4172, val_MinusLogProbMetric: 42.4172

Epoch 694: val_loss did not improve from 42.41557
196/196 - 65s - loss: 42.1870 - MinusLogProbMetric: 42.1870 - val_loss: 42.4172 - val_MinusLogProbMetric: 42.4172 - lr: 1.6935e-08 - 65s/epoch - 334ms/step
Epoch 695/1000
2023-10-10 15:41:15.172 
Epoch 695/1000 
	 loss: 42.1866, MinusLogProbMetric: 42.1866, val_loss: 42.4161, val_MinusLogProbMetric: 42.4161

Epoch 695: val_loss did not improve from 42.41557
196/196 - 66s - loss: 42.1866 - MinusLogProbMetric: 42.1866 - val_loss: 42.4161 - val_MinusLogProbMetric: 42.4161 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 696/1000
2023-10-10 15:42:23.060 
Epoch 696/1000 
	 loss: 42.1851, MinusLogProbMetric: 42.1851, val_loss: 42.4159, val_MinusLogProbMetric: 42.4159

Epoch 696: val_loss did not improve from 42.41557
196/196 - 68s - loss: 42.1851 - MinusLogProbMetric: 42.1851 - val_loss: 42.4159 - val_MinusLogProbMetric: 42.4159 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 697/1000
2023-10-10 15:43:28.602 
Epoch 697/1000 
	 loss: 42.1885, MinusLogProbMetric: 42.1885, val_loss: 42.4149, val_MinusLogProbMetric: 42.4149

Epoch 697: val_loss improved from 42.41557 to 42.41488, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 42.1885 - MinusLogProbMetric: 42.1885 - val_loss: 42.4149 - val_MinusLogProbMetric: 42.4149 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 698/1000
2023-10-10 15:44:34.754 
Epoch 698/1000 
	 loss: 42.1857, MinusLogProbMetric: 42.1857, val_loss: 42.4156, val_MinusLogProbMetric: 42.4156

Epoch 698: val_loss did not improve from 42.41488
196/196 - 65s - loss: 42.1857 - MinusLogProbMetric: 42.1857 - val_loss: 42.4156 - val_MinusLogProbMetric: 42.4156 - lr: 1.6935e-08 - 65s/epoch - 332ms/step
Epoch 699/1000
2023-10-10 15:45:41.138 
Epoch 699/1000 
	 loss: 42.1873, MinusLogProbMetric: 42.1873, val_loss: 42.4171, val_MinusLogProbMetric: 42.4171

Epoch 699: val_loss did not improve from 42.41488
196/196 - 66s - loss: 42.1873 - MinusLogProbMetric: 42.1873 - val_loss: 42.4171 - val_MinusLogProbMetric: 42.4171 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 700/1000
2023-10-10 15:46:44.617 
Epoch 700/1000 
	 loss: 42.1876, MinusLogProbMetric: 42.1876, val_loss: 42.4166, val_MinusLogProbMetric: 42.4166

Epoch 700: val_loss did not improve from 42.41488
196/196 - 63s - loss: 42.1876 - MinusLogProbMetric: 42.1876 - val_loss: 42.4166 - val_MinusLogProbMetric: 42.4166 - lr: 1.6935e-08 - 63s/epoch - 324ms/step
Epoch 701/1000
2023-10-10 15:47:48.592 
Epoch 701/1000 
	 loss: 42.1883, MinusLogProbMetric: 42.1883, val_loss: 42.4181, val_MinusLogProbMetric: 42.4181

Epoch 701: val_loss did not improve from 42.41488
196/196 - 64s - loss: 42.1883 - MinusLogProbMetric: 42.1883 - val_loss: 42.4181 - val_MinusLogProbMetric: 42.4181 - lr: 1.6935e-08 - 64s/epoch - 326ms/step
Epoch 702/1000
2023-10-10 15:48:55.312 
Epoch 702/1000 
	 loss: 42.1872, MinusLogProbMetric: 42.1872, val_loss: 42.4169, val_MinusLogProbMetric: 42.4169

Epoch 702: val_loss did not improve from 42.41488
196/196 - 67s - loss: 42.1872 - MinusLogProbMetric: 42.1872 - val_loss: 42.4169 - val_MinusLogProbMetric: 42.4169 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 703/1000
2023-10-10 15:50:02.863 
Epoch 703/1000 
	 loss: 42.1864, MinusLogProbMetric: 42.1864, val_loss: 42.4158, val_MinusLogProbMetric: 42.4158

Epoch 703: val_loss did not improve from 42.41488
196/196 - 68s - loss: 42.1864 - MinusLogProbMetric: 42.1864 - val_loss: 42.4158 - val_MinusLogProbMetric: 42.4158 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 704/1000
2023-10-10 15:51:10.000 
Epoch 704/1000 
	 loss: 42.1858, MinusLogProbMetric: 42.1858, val_loss: 42.4174, val_MinusLogProbMetric: 42.4174

Epoch 704: val_loss did not improve from 42.41488
196/196 - 67s - loss: 42.1858 - MinusLogProbMetric: 42.1858 - val_loss: 42.4174 - val_MinusLogProbMetric: 42.4174 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 705/1000
2023-10-10 15:52:16.393 
Epoch 705/1000 
	 loss: 42.1870, MinusLogProbMetric: 42.1870, val_loss: 42.4180, val_MinusLogProbMetric: 42.4180

Epoch 705: val_loss did not improve from 42.41488
196/196 - 66s - loss: 42.1870 - MinusLogProbMetric: 42.1870 - val_loss: 42.4180 - val_MinusLogProbMetric: 42.4180 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 706/1000
2023-10-10 15:53:22.502 
Epoch 706/1000 
	 loss: 42.1866, MinusLogProbMetric: 42.1866, val_loss: 42.4171, val_MinusLogProbMetric: 42.4171

Epoch 706: val_loss did not improve from 42.41488
196/196 - 66s - loss: 42.1866 - MinusLogProbMetric: 42.1866 - val_loss: 42.4171 - val_MinusLogProbMetric: 42.4171 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 707/1000
2023-10-10 15:54:29.716 
Epoch 707/1000 
	 loss: 42.1848, MinusLogProbMetric: 42.1848, val_loss: 42.4184, val_MinusLogProbMetric: 42.4184

Epoch 707: val_loss did not improve from 42.41488
196/196 - 67s - loss: 42.1848 - MinusLogProbMetric: 42.1848 - val_loss: 42.4184 - val_MinusLogProbMetric: 42.4184 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 708/1000
2023-10-10 15:55:36.649 
Epoch 708/1000 
	 loss: 42.1876, MinusLogProbMetric: 42.1876, val_loss: 42.4156, val_MinusLogProbMetric: 42.4156

Epoch 708: val_loss did not improve from 42.41488
196/196 - 67s - loss: 42.1876 - MinusLogProbMetric: 42.1876 - val_loss: 42.4156 - val_MinusLogProbMetric: 42.4156 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 709/1000
2023-10-10 15:56:42.766 
Epoch 709/1000 
	 loss: 42.1876, MinusLogProbMetric: 42.1876, val_loss: 42.4187, val_MinusLogProbMetric: 42.4187

Epoch 709: val_loss did not improve from 42.41488
196/196 - 66s - loss: 42.1876 - MinusLogProbMetric: 42.1876 - val_loss: 42.4187 - val_MinusLogProbMetric: 42.4187 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 710/1000
2023-10-10 15:57:50.689 
Epoch 710/1000 
	 loss: 42.1888, MinusLogProbMetric: 42.1888, val_loss: 42.4157, val_MinusLogProbMetric: 42.4157

Epoch 710: val_loss did not improve from 42.41488
196/196 - 68s - loss: 42.1888 - MinusLogProbMetric: 42.1888 - val_loss: 42.4157 - val_MinusLogProbMetric: 42.4157 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 711/1000
2023-10-10 15:58:56.077 
Epoch 711/1000 
	 loss: 42.1860, MinusLogProbMetric: 42.1860, val_loss: 42.4142, val_MinusLogProbMetric: 42.4142

Epoch 711: val_loss improved from 42.41488 to 42.41416, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 66s - loss: 42.1860 - MinusLogProbMetric: 42.1860 - val_loss: 42.4142 - val_MinusLogProbMetric: 42.4142 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 712/1000
2023-10-10 16:00:01.485 
Epoch 712/1000 
	 loss: 42.1845, MinusLogProbMetric: 42.1845, val_loss: 42.4143, val_MinusLogProbMetric: 42.4143

Epoch 712: val_loss did not improve from 42.41416
196/196 - 64s - loss: 42.1845 - MinusLogProbMetric: 42.1845 - val_loss: 42.4143 - val_MinusLogProbMetric: 42.4143 - lr: 1.6935e-08 - 64s/epoch - 329ms/step
Epoch 713/1000
2023-10-10 16:01:04.763 
Epoch 713/1000 
	 loss: 42.1850, MinusLogProbMetric: 42.1850, val_loss: 42.4158, val_MinusLogProbMetric: 42.4158

Epoch 713: val_loss did not improve from 42.41416
196/196 - 63s - loss: 42.1850 - MinusLogProbMetric: 42.1850 - val_loss: 42.4158 - val_MinusLogProbMetric: 42.4158 - lr: 1.6935e-08 - 63s/epoch - 323ms/step
Epoch 714/1000
2023-10-10 16:02:10.378 
Epoch 714/1000 
	 loss: 42.1869, MinusLogProbMetric: 42.1869, val_loss: 42.4156, val_MinusLogProbMetric: 42.4156

Epoch 714: val_loss did not improve from 42.41416
196/196 - 66s - loss: 42.1869 - MinusLogProbMetric: 42.1869 - val_loss: 42.4156 - val_MinusLogProbMetric: 42.4156 - lr: 1.6935e-08 - 66s/epoch - 335ms/step
Epoch 715/1000
2023-10-10 16:03:16.654 
Epoch 715/1000 
	 loss: 42.1854, MinusLogProbMetric: 42.1854, val_loss: 42.4160, val_MinusLogProbMetric: 42.4160

Epoch 715: val_loss did not improve from 42.41416
196/196 - 66s - loss: 42.1854 - MinusLogProbMetric: 42.1854 - val_loss: 42.4160 - val_MinusLogProbMetric: 42.4160 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 716/1000
2023-10-10 16:04:21.971 
Epoch 716/1000 
	 loss: 42.1871, MinusLogProbMetric: 42.1871, val_loss: 42.4178, val_MinusLogProbMetric: 42.4178

Epoch 716: val_loss did not improve from 42.41416
196/196 - 65s - loss: 42.1871 - MinusLogProbMetric: 42.1871 - val_loss: 42.4178 - val_MinusLogProbMetric: 42.4178 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 717/1000
2023-10-10 16:05:27.504 
Epoch 717/1000 
	 loss: 42.1884, MinusLogProbMetric: 42.1884, val_loss: 42.4157, val_MinusLogProbMetric: 42.4157

Epoch 717: val_loss did not improve from 42.41416
196/196 - 66s - loss: 42.1884 - MinusLogProbMetric: 42.1884 - val_loss: 42.4157 - val_MinusLogProbMetric: 42.4157 - lr: 1.6935e-08 - 66s/epoch - 334ms/step
Epoch 718/1000
2023-10-10 16:06:34.275 
Epoch 718/1000 
	 loss: 42.1856, MinusLogProbMetric: 42.1856, val_loss: 42.4145, val_MinusLogProbMetric: 42.4145

Epoch 718: val_loss did not improve from 42.41416
196/196 - 67s - loss: 42.1856 - MinusLogProbMetric: 42.1856 - val_loss: 42.4145 - val_MinusLogProbMetric: 42.4145 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 719/1000
2023-10-10 16:07:40.141 
Epoch 719/1000 
	 loss: 42.1852, MinusLogProbMetric: 42.1852, val_loss: 42.4141, val_MinusLogProbMetric: 42.4141

Epoch 719: val_loss improved from 42.41416 to 42.41409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 42.1852 - MinusLogProbMetric: 42.1852 - val_loss: 42.4141 - val_MinusLogProbMetric: 42.4141 - lr: 1.6935e-08 - 67s/epoch - 340ms/step
Epoch 720/1000
2023-10-10 16:08:45.169 
Epoch 720/1000 
	 loss: 42.1856, MinusLogProbMetric: 42.1856, val_loss: 42.4149, val_MinusLogProbMetric: 42.4149

Epoch 720: val_loss did not improve from 42.41409
196/196 - 64s - loss: 42.1856 - MinusLogProbMetric: 42.1856 - val_loss: 42.4149 - val_MinusLogProbMetric: 42.4149 - lr: 1.6935e-08 - 64s/epoch - 328ms/step
Epoch 721/1000
2023-10-10 16:09:50.623 
Epoch 721/1000 
	 loss: 42.1860, MinusLogProbMetric: 42.1860, val_loss: 42.4168, val_MinusLogProbMetric: 42.4168

Epoch 721: val_loss did not improve from 42.41409
196/196 - 65s - loss: 42.1860 - MinusLogProbMetric: 42.1860 - val_loss: 42.4168 - val_MinusLogProbMetric: 42.4168 - lr: 1.6935e-08 - 65s/epoch - 334ms/step
Epoch 722/1000
2023-10-10 16:10:55.620 
Epoch 722/1000 
	 loss: 42.1861, MinusLogProbMetric: 42.1861, val_loss: 42.4169, val_MinusLogProbMetric: 42.4169

Epoch 722: val_loss did not improve from 42.41409
196/196 - 65s - loss: 42.1861 - MinusLogProbMetric: 42.1861 - val_loss: 42.4169 - val_MinusLogProbMetric: 42.4169 - lr: 1.6935e-08 - 65s/epoch - 332ms/step
Epoch 723/1000
2023-10-10 16:12:01.055 
Epoch 723/1000 
	 loss: 42.1864, MinusLogProbMetric: 42.1864, val_loss: 42.4145, val_MinusLogProbMetric: 42.4145

Epoch 723: val_loss did not improve from 42.41409
196/196 - 65s - loss: 42.1864 - MinusLogProbMetric: 42.1864 - val_loss: 42.4145 - val_MinusLogProbMetric: 42.4145 - lr: 1.6935e-08 - 65s/epoch - 334ms/step
Epoch 724/1000
2023-10-10 16:13:06.832 
Epoch 724/1000 
	 loss: 42.1853, MinusLogProbMetric: 42.1853, val_loss: 42.4153, val_MinusLogProbMetric: 42.4153

Epoch 724: val_loss did not improve from 42.41409
196/196 - 66s - loss: 42.1853 - MinusLogProbMetric: 42.1853 - val_loss: 42.4153 - val_MinusLogProbMetric: 42.4153 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 725/1000
2023-10-10 16:14:13.474 
Epoch 725/1000 
	 loss: 42.1845, MinusLogProbMetric: 42.1845, val_loss: 42.4103, val_MinusLogProbMetric: 42.4103

Epoch 725: val_loss improved from 42.41409 to 42.41031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 68s - loss: 42.1845 - MinusLogProbMetric: 42.1845 - val_loss: 42.4103 - val_MinusLogProbMetric: 42.4103 - lr: 1.6935e-08 - 68s/epoch - 347ms/step
Epoch 726/1000
2023-10-10 16:15:20.046 
Epoch 726/1000 
	 loss: 42.1842, MinusLogProbMetric: 42.1842, val_loss: 42.4103, val_MinusLogProbMetric: 42.4103

Epoch 726: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1842 - MinusLogProbMetric: 42.1842 - val_loss: 42.4103 - val_MinusLogProbMetric: 42.4103 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 727/1000
2023-10-10 16:16:25.152 
Epoch 727/1000 
	 loss: 42.1845, MinusLogProbMetric: 42.1845, val_loss: 42.4213, val_MinusLogProbMetric: 42.4213

Epoch 727: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1845 - MinusLogProbMetric: 42.1845 - val_loss: 42.4213 - val_MinusLogProbMetric: 42.4213 - lr: 1.6935e-08 - 65s/epoch - 332ms/step
Epoch 728/1000
2023-10-10 16:17:31.195 
Epoch 728/1000 
	 loss: 42.1909, MinusLogProbMetric: 42.1909, val_loss: 42.4191, val_MinusLogProbMetric: 42.4191

Epoch 728: val_loss did not improve from 42.41031
196/196 - 66s - loss: 42.1909 - MinusLogProbMetric: 42.1909 - val_loss: 42.4191 - val_MinusLogProbMetric: 42.4191 - lr: 1.6935e-08 - 66s/epoch - 337ms/step
Epoch 729/1000
2023-10-10 16:18:35.366 
Epoch 729/1000 
	 loss: 42.1891, MinusLogProbMetric: 42.1891, val_loss: 42.4145, val_MinusLogProbMetric: 42.4145

Epoch 729: val_loss did not improve from 42.41031
196/196 - 64s - loss: 42.1891 - MinusLogProbMetric: 42.1891 - val_loss: 42.4145 - val_MinusLogProbMetric: 42.4145 - lr: 1.6935e-08 - 64s/epoch - 327ms/step
Epoch 730/1000
2023-10-10 16:19:40.743 
Epoch 730/1000 
	 loss: 42.1863, MinusLogProbMetric: 42.1863, val_loss: 42.4161, val_MinusLogProbMetric: 42.4161

Epoch 730: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1863 - MinusLogProbMetric: 42.1863 - val_loss: 42.4161 - val_MinusLogProbMetric: 42.4161 - lr: 1.6935e-08 - 65s/epoch - 334ms/step
Epoch 731/1000
2023-10-10 16:20:47.721 
Epoch 731/1000 
	 loss: 42.1854, MinusLogProbMetric: 42.1854, val_loss: 42.4141, val_MinusLogProbMetric: 42.4141

Epoch 731: val_loss did not improve from 42.41031
196/196 - 67s - loss: 42.1854 - MinusLogProbMetric: 42.1854 - val_loss: 42.4141 - val_MinusLogProbMetric: 42.4141 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 732/1000
2023-10-10 16:21:52.088 
Epoch 732/1000 
	 loss: 42.1842, MinusLogProbMetric: 42.1842, val_loss: 42.4164, val_MinusLogProbMetric: 42.4164

Epoch 732: val_loss did not improve from 42.41031
196/196 - 64s - loss: 42.1842 - MinusLogProbMetric: 42.1842 - val_loss: 42.4164 - val_MinusLogProbMetric: 42.4164 - lr: 1.6935e-08 - 64s/epoch - 328ms/step
Epoch 733/1000
2023-10-10 16:22:57.479 
Epoch 733/1000 
	 loss: 42.1856, MinusLogProbMetric: 42.1856, val_loss: 42.4168, val_MinusLogProbMetric: 42.4168

Epoch 733: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1856 - MinusLogProbMetric: 42.1856 - val_loss: 42.4168 - val_MinusLogProbMetric: 42.4168 - lr: 1.6935e-08 - 65s/epoch - 334ms/step
Epoch 734/1000
2023-10-10 16:24:02.212 
Epoch 734/1000 
	 loss: 42.1855, MinusLogProbMetric: 42.1855, val_loss: 42.4143, val_MinusLogProbMetric: 42.4143

Epoch 734: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1855 - MinusLogProbMetric: 42.1855 - val_loss: 42.4143 - val_MinusLogProbMetric: 42.4143 - lr: 1.6935e-08 - 65s/epoch - 330ms/step
Epoch 735/1000
2023-10-10 16:25:07.356 
Epoch 735/1000 
	 loss: 42.1850, MinusLogProbMetric: 42.1850, val_loss: 42.4154, val_MinusLogProbMetric: 42.4154

Epoch 735: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1850 - MinusLogProbMetric: 42.1850 - val_loss: 42.4154 - val_MinusLogProbMetric: 42.4154 - lr: 1.6935e-08 - 65s/epoch - 332ms/step
Epoch 736/1000
2023-10-10 16:26:12.686 
Epoch 736/1000 
	 loss: 42.1850, MinusLogProbMetric: 42.1850, val_loss: 42.4122, val_MinusLogProbMetric: 42.4122

Epoch 736: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1850 - MinusLogProbMetric: 42.1850 - val_loss: 42.4122 - val_MinusLogProbMetric: 42.4122 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 737/1000
2023-10-10 16:27:17.261 
Epoch 737/1000 
	 loss: 42.1827, MinusLogProbMetric: 42.1827, val_loss: 42.4137, val_MinusLogProbMetric: 42.4137

Epoch 737: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1827 - MinusLogProbMetric: 42.1827 - val_loss: 42.4137 - val_MinusLogProbMetric: 42.4137 - lr: 1.6935e-08 - 65s/epoch - 329ms/step
Epoch 738/1000
2023-10-10 16:28:21.854 
Epoch 738/1000 
	 loss: 42.1824, MinusLogProbMetric: 42.1824, val_loss: 42.4132, val_MinusLogProbMetric: 42.4132

Epoch 738: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1824 - MinusLogProbMetric: 42.1824 - val_loss: 42.4132 - val_MinusLogProbMetric: 42.4132 - lr: 1.6935e-08 - 65s/epoch - 330ms/step
Epoch 739/1000
2023-10-10 16:29:25.886 
Epoch 739/1000 
	 loss: 42.1822, MinusLogProbMetric: 42.1822, val_loss: 42.4113, val_MinusLogProbMetric: 42.4113

Epoch 739: val_loss did not improve from 42.41031
196/196 - 64s - loss: 42.1822 - MinusLogProbMetric: 42.1822 - val_loss: 42.4113 - val_MinusLogProbMetric: 42.4113 - lr: 1.6935e-08 - 64s/epoch - 327ms/step
Epoch 740/1000
2023-10-10 16:30:32.801 
Epoch 740/1000 
	 loss: 42.1827, MinusLogProbMetric: 42.1827, val_loss: 42.4138, val_MinusLogProbMetric: 42.4138

Epoch 740: val_loss did not improve from 42.41031
196/196 - 67s - loss: 42.1827 - MinusLogProbMetric: 42.1827 - val_loss: 42.4138 - val_MinusLogProbMetric: 42.4138 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 741/1000
2023-10-10 16:31:36.811 
Epoch 741/1000 
	 loss: 42.1837, MinusLogProbMetric: 42.1837, val_loss: 42.4114, val_MinusLogProbMetric: 42.4114

Epoch 741: val_loss did not improve from 42.41031
196/196 - 64s - loss: 42.1837 - MinusLogProbMetric: 42.1837 - val_loss: 42.4114 - val_MinusLogProbMetric: 42.4114 - lr: 1.6935e-08 - 64s/epoch - 327ms/step
Epoch 742/1000
2023-10-10 16:32:42.680 
Epoch 742/1000 
	 loss: 42.1820, MinusLogProbMetric: 42.1820, val_loss: 42.4134, val_MinusLogProbMetric: 42.4134

Epoch 742: val_loss did not improve from 42.41031
196/196 - 66s - loss: 42.1820 - MinusLogProbMetric: 42.1820 - val_loss: 42.4134 - val_MinusLogProbMetric: 42.4134 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 743/1000
2023-10-10 16:33:47.339 
Epoch 743/1000 
	 loss: 42.1830, MinusLogProbMetric: 42.1830, val_loss: 42.4127, val_MinusLogProbMetric: 42.4127

Epoch 743: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1830 - MinusLogProbMetric: 42.1830 - val_loss: 42.4127 - val_MinusLogProbMetric: 42.4127 - lr: 1.6935e-08 - 65s/epoch - 330ms/step
Epoch 744/1000
2023-10-10 16:34:54.155 
Epoch 744/1000 
	 loss: 42.1818, MinusLogProbMetric: 42.1818, val_loss: 42.4107, val_MinusLogProbMetric: 42.4107

Epoch 744: val_loss did not improve from 42.41031
196/196 - 67s - loss: 42.1818 - MinusLogProbMetric: 42.1818 - val_loss: 42.4107 - val_MinusLogProbMetric: 42.4107 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 745/1000
2023-10-10 16:35:59.890 
Epoch 745/1000 
	 loss: 42.1851, MinusLogProbMetric: 42.1851, val_loss: 42.4202, val_MinusLogProbMetric: 42.4202

Epoch 745: val_loss did not improve from 42.41031
196/196 - 66s - loss: 42.1851 - MinusLogProbMetric: 42.1851 - val_loss: 42.4202 - val_MinusLogProbMetric: 42.4202 - lr: 1.6935e-08 - 66s/epoch - 335ms/step
Epoch 746/1000
2023-10-10 16:37:04.892 
Epoch 746/1000 
	 loss: 42.1902, MinusLogProbMetric: 42.1902, val_loss: 42.4175, val_MinusLogProbMetric: 42.4175

Epoch 746: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1902 - MinusLogProbMetric: 42.1902 - val_loss: 42.4175 - val_MinusLogProbMetric: 42.4175 - lr: 1.6935e-08 - 65s/epoch - 332ms/step
Epoch 747/1000
2023-10-10 16:38:10.310 
Epoch 747/1000 
	 loss: 42.1881, MinusLogProbMetric: 42.1881, val_loss: 42.4173, val_MinusLogProbMetric: 42.4173

Epoch 747: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1881 - MinusLogProbMetric: 42.1881 - val_loss: 42.4173 - val_MinusLogProbMetric: 42.4173 - lr: 1.6935e-08 - 65s/epoch - 334ms/step
Epoch 748/1000
2023-10-10 16:39:15.617 
Epoch 748/1000 
	 loss: 42.1871, MinusLogProbMetric: 42.1871, val_loss: 42.4160, val_MinusLogProbMetric: 42.4160

Epoch 748: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1871 - MinusLogProbMetric: 42.1871 - val_loss: 42.4160 - val_MinusLogProbMetric: 42.4160 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 749/1000
2023-10-10 16:40:20.912 
Epoch 749/1000 
	 loss: 42.1895, MinusLogProbMetric: 42.1895, val_loss: 42.4206, val_MinusLogProbMetric: 42.4206

Epoch 749: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1895 - MinusLogProbMetric: 42.1895 - val_loss: 42.4206 - val_MinusLogProbMetric: 42.4206 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 750/1000
2023-10-10 16:41:25.517 
Epoch 750/1000 
	 loss: 42.1919, MinusLogProbMetric: 42.1919, val_loss: 42.4201, val_MinusLogProbMetric: 42.4201

Epoch 750: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1919 - MinusLogProbMetric: 42.1919 - val_loss: 42.4201 - val_MinusLogProbMetric: 42.4201 - lr: 1.6935e-08 - 65s/epoch - 330ms/step
Epoch 751/1000
2023-10-10 16:42:32.999 
Epoch 751/1000 
	 loss: 42.1909, MinusLogProbMetric: 42.1909, val_loss: 42.4170, val_MinusLogProbMetric: 42.4170

Epoch 751: val_loss did not improve from 42.41031
196/196 - 67s - loss: 42.1909 - MinusLogProbMetric: 42.1909 - val_loss: 42.4170 - val_MinusLogProbMetric: 42.4170 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 752/1000
2023-10-10 16:43:38.819 
Epoch 752/1000 
	 loss: 42.1876, MinusLogProbMetric: 42.1876, val_loss: 42.4197, val_MinusLogProbMetric: 42.4197

Epoch 752: val_loss did not improve from 42.41031
196/196 - 66s - loss: 42.1876 - MinusLogProbMetric: 42.1876 - val_loss: 42.4197 - val_MinusLogProbMetric: 42.4197 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 753/1000
2023-10-10 16:44:45.187 
Epoch 753/1000 
	 loss: 42.1892, MinusLogProbMetric: 42.1892, val_loss: 42.4173, val_MinusLogProbMetric: 42.4173

Epoch 753: val_loss did not improve from 42.41031
196/196 - 66s - loss: 42.1892 - MinusLogProbMetric: 42.1892 - val_loss: 42.4173 - val_MinusLogProbMetric: 42.4173 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 754/1000
2023-10-10 16:45:48.042 
Epoch 754/1000 
	 loss: 42.1881, MinusLogProbMetric: 42.1881, val_loss: 42.4162, val_MinusLogProbMetric: 42.4162

Epoch 754: val_loss did not improve from 42.41031
196/196 - 63s - loss: 42.1881 - MinusLogProbMetric: 42.1881 - val_loss: 42.4162 - val_MinusLogProbMetric: 42.4162 - lr: 1.6935e-08 - 63s/epoch - 321ms/step
Epoch 755/1000
2023-10-10 16:46:53.544 
Epoch 755/1000 
	 loss: 42.1873, MinusLogProbMetric: 42.1873, val_loss: 42.4161, val_MinusLogProbMetric: 42.4161

Epoch 755: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1873 - MinusLogProbMetric: 42.1873 - val_loss: 42.4161 - val_MinusLogProbMetric: 42.4161 - lr: 1.6935e-08 - 65s/epoch - 334ms/step
Epoch 756/1000
2023-10-10 16:47:57.543 
Epoch 756/1000 
	 loss: 42.1852, MinusLogProbMetric: 42.1852, val_loss: 42.4182, val_MinusLogProbMetric: 42.4182

Epoch 756: val_loss did not improve from 42.41031
196/196 - 64s - loss: 42.1852 - MinusLogProbMetric: 42.1852 - val_loss: 42.4182 - val_MinusLogProbMetric: 42.4182 - lr: 1.6935e-08 - 64s/epoch - 327ms/step
Epoch 757/1000
2023-10-10 16:49:02.499 
Epoch 757/1000 
	 loss: 42.1862, MinusLogProbMetric: 42.1862, val_loss: 42.4178, val_MinusLogProbMetric: 42.4178

Epoch 757: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1862 - MinusLogProbMetric: 42.1862 - val_loss: 42.4178 - val_MinusLogProbMetric: 42.4178 - lr: 1.6935e-08 - 65s/epoch - 331ms/step
Epoch 758/1000
2023-10-10 16:50:06.207 
Epoch 758/1000 
	 loss: 42.1895, MinusLogProbMetric: 42.1895, val_loss: 42.4187, val_MinusLogProbMetric: 42.4187

Epoch 758: val_loss did not improve from 42.41031
196/196 - 64s - loss: 42.1895 - MinusLogProbMetric: 42.1895 - val_loss: 42.4187 - val_MinusLogProbMetric: 42.4187 - lr: 1.6935e-08 - 64s/epoch - 325ms/step
Epoch 759/1000
2023-10-10 16:51:11.992 
Epoch 759/1000 
	 loss: 42.1906, MinusLogProbMetric: 42.1906, val_loss: 42.4169, val_MinusLogProbMetric: 42.4169

Epoch 759: val_loss did not improve from 42.41031
196/196 - 66s - loss: 42.1906 - MinusLogProbMetric: 42.1906 - val_loss: 42.4169 - val_MinusLogProbMetric: 42.4169 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 760/1000
2023-10-10 16:52:17.007 
Epoch 760/1000 
	 loss: 42.1976, MinusLogProbMetric: 42.1976, val_loss: 42.4286, val_MinusLogProbMetric: 42.4286

Epoch 760: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1976 - MinusLogProbMetric: 42.1976 - val_loss: 42.4286 - val_MinusLogProbMetric: 42.4286 - lr: 1.6935e-08 - 65s/epoch - 332ms/step
Epoch 761/1000
2023-10-10 16:53:22.923 
Epoch 761/1000 
	 loss: 42.2014, MinusLogProbMetric: 42.2014, val_loss: 42.4277, val_MinusLogProbMetric: 42.4277

Epoch 761: val_loss did not improve from 42.41031
196/196 - 66s - loss: 42.2014 - MinusLogProbMetric: 42.2014 - val_loss: 42.4277 - val_MinusLogProbMetric: 42.4277 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 762/1000
2023-10-10 16:54:28.761 
Epoch 762/1000 
	 loss: 42.1974, MinusLogProbMetric: 42.1974, val_loss: 42.4241, val_MinusLogProbMetric: 42.4241

Epoch 762: val_loss did not improve from 42.41031
196/196 - 66s - loss: 42.1974 - MinusLogProbMetric: 42.1974 - val_loss: 42.4241 - val_MinusLogProbMetric: 42.4241 - lr: 1.6935e-08 - 66s/epoch - 336ms/step
Epoch 763/1000
2023-10-10 16:55:34.187 
Epoch 763/1000 
	 loss: 42.1934, MinusLogProbMetric: 42.1934, val_loss: 42.4215, val_MinusLogProbMetric: 42.4215

Epoch 763: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1934 - MinusLogProbMetric: 42.1934 - val_loss: 42.4215 - val_MinusLogProbMetric: 42.4215 - lr: 1.6935e-08 - 65s/epoch - 334ms/step
Epoch 764/1000
2023-10-10 16:56:39.859 
Epoch 764/1000 
	 loss: 42.1937, MinusLogProbMetric: 42.1937, val_loss: 42.4214, val_MinusLogProbMetric: 42.4214

Epoch 764: val_loss did not improve from 42.41031
196/196 - 66s - loss: 42.1937 - MinusLogProbMetric: 42.1937 - val_loss: 42.4214 - val_MinusLogProbMetric: 42.4214 - lr: 1.6935e-08 - 66s/epoch - 335ms/step
Epoch 765/1000
2023-10-10 16:57:46.146 
Epoch 765/1000 
	 loss: 42.1932, MinusLogProbMetric: 42.1932, val_loss: 42.4229, val_MinusLogProbMetric: 42.4229

Epoch 765: val_loss did not improve from 42.41031
196/196 - 66s - loss: 42.1932 - MinusLogProbMetric: 42.1932 - val_loss: 42.4229 - val_MinusLogProbMetric: 42.4229 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 766/1000
2023-10-10 16:58:52.881 
Epoch 766/1000 
	 loss: 42.1932, MinusLogProbMetric: 42.1932, val_loss: 42.4165, val_MinusLogProbMetric: 42.4165

Epoch 766: val_loss did not improve from 42.41031
196/196 - 67s - loss: 42.1932 - MinusLogProbMetric: 42.1932 - val_loss: 42.4165 - val_MinusLogProbMetric: 42.4165 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 767/1000
2023-10-10 16:59:58.234 
Epoch 767/1000 
	 loss: 42.1896, MinusLogProbMetric: 42.1896, val_loss: 42.4161, val_MinusLogProbMetric: 42.4161

Epoch 767: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1896 - MinusLogProbMetric: 42.1896 - val_loss: 42.4161 - val_MinusLogProbMetric: 42.4161 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 768/1000
2023-10-10 17:01:05.115 
Epoch 768/1000 
	 loss: 42.1881, MinusLogProbMetric: 42.1881, val_loss: 42.4193, val_MinusLogProbMetric: 42.4193

Epoch 768: val_loss did not improve from 42.41031
196/196 - 67s - loss: 42.1881 - MinusLogProbMetric: 42.1881 - val_loss: 42.4193 - val_MinusLogProbMetric: 42.4193 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 769/1000
2023-10-10 17:02:11.503 
Epoch 769/1000 
	 loss: 42.1870, MinusLogProbMetric: 42.1870, val_loss: 42.4153, val_MinusLogProbMetric: 42.4153

Epoch 769: val_loss did not improve from 42.41031
196/196 - 66s - loss: 42.1870 - MinusLogProbMetric: 42.1870 - val_loss: 42.4153 - val_MinusLogProbMetric: 42.4153 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 770/1000
2023-10-10 17:03:18.278 
Epoch 770/1000 
	 loss: 42.1890, MinusLogProbMetric: 42.1890, val_loss: 42.4155, val_MinusLogProbMetric: 42.4155

Epoch 770: val_loss did not improve from 42.41031
196/196 - 67s - loss: 42.1890 - MinusLogProbMetric: 42.1890 - val_loss: 42.4155 - val_MinusLogProbMetric: 42.4155 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 771/1000
2023-10-10 17:04:23.851 
Epoch 771/1000 
	 loss: 42.1860, MinusLogProbMetric: 42.1860, val_loss: 42.4155, val_MinusLogProbMetric: 42.4155

Epoch 771: val_loss did not improve from 42.41031
196/196 - 66s - loss: 42.1860 - MinusLogProbMetric: 42.1860 - val_loss: 42.4155 - val_MinusLogProbMetric: 42.4155 - lr: 1.6935e-08 - 66s/epoch - 335ms/step
Epoch 772/1000
2023-10-10 17:05:31.564 
Epoch 772/1000 
	 loss: 42.1859, MinusLogProbMetric: 42.1859, val_loss: 42.4150, val_MinusLogProbMetric: 42.4150

Epoch 772: val_loss did not improve from 42.41031
196/196 - 68s - loss: 42.1859 - MinusLogProbMetric: 42.1859 - val_loss: 42.4150 - val_MinusLogProbMetric: 42.4150 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 773/1000
2023-10-10 17:06:36.540 
Epoch 773/1000 
	 loss: 42.1870, MinusLogProbMetric: 42.1870, val_loss: 42.4137, val_MinusLogProbMetric: 42.4137

Epoch 773: val_loss did not improve from 42.41031
196/196 - 65s - loss: 42.1870 - MinusLogProbMetric: 42.1870 - val_loss: 42.4137 - val_MinusLogProbMetric: 42.4137 - lr: 1.6935e-08 - 65s/epoch - 331ms/step
Epoch 774/1000
2023-10-10 17:07:42.713 
Epoch 774/1000 
	 loss: 42.1846, MinusLogProbMetric: 42.1846, val_loss: 42.4096, val_MinusLogProbMetric: 42.4096

Epoch 774: val_loss improved from 42.41031 to 42.40964, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 42.1846 - MinusLogProbMetric: 42.1846 - val_loss: 42.4096 - val_MinusLogProbMetric: 42.4096 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 775/1000
2023-10-10 17:08:49.980 
Epoch 775/1000 
	 loss: 42.1850, MinusLogProbMetric: 42.1850, val_loss: 42.4116, val_MinusLogProbMetric: 42.4116

Epoch 775: val_loss did not improve from 42.40964
196/196 - 66s - loss: 42.1850 - MinusLogProbMetric: 42.1850 - val_loss: 42.4116 - val_MinusLogProbMetric: 42.4116 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 776/1000
2023-10-10 17:09:55.127 
Epoch 776/1000 
	 loss: 42.1856, MinusLogProbMetric: 42.1856, val_loss: 42.4118, val_MinusLogProbMetric: 42.4118

Epoch 776: val_loss did not improve from 42.40964
196/196 - 65s - loss: 42.1856 - MinusLogProbMetric: 42.1856 - val_loss: 42.4118 - val_MinusLogProbMetric: 42.4118 - lr: 1.6935e-08 - 65s/epoch - 332ms/step
Epoch 777/1000
2023-10-10 17:11:01.668 
Epoch 777/1000 
	 loss: 42.1842, MinusLogProbMetric: 42.1842, val_loss: 42.4120, val_MinusLogProbMetric: 42.4120

Epoch 777: val_loss did not improve from 42.40964
196/196 - 67s - loss: 42.1842 - MinusLogProbMetric: 42.1842 - val_loss: 42.4120 - val_MinusLogProbMetric: 42.4120 - lr: 1.6935e-08 - 67s/epoch - 339ms/step
Epoch 778/1000
2023-10-10 17:12:10.434 
Epoch 778/1000 
	 loss: 42.1844, MinusLogProbMetric: 42.1844, val_loss: 42.4103, val_MinusLogProbMetric: 42.4103

Epoch 778: val_loss did not improve from 42.40964
196/196 - 69s - loss: 42.1844 - MinusLogProbMetric: 42.1844 - val_loss: 42.4103 - val_MinusLogProbMetric: 42.4103 - lr: 1.6935e-08 - 69s/epoch - 351ms/step
Epoch 779/1000
2023-10-10 17:13:16.837 
Epoch 779/1000 
	 loss: 42.1835, MinusLogProbMetric: 42.1835, val_loss: 42.4116, val_MinusLogProbMetric: 42.4116

Epoch 779: val_loss did not improve from 42.40964
196/196 - 66s - loss: 42.1835 - MinusLogProbMetric: 42.1835 - val_loss: 42.4116 - val_MinusLogProbMetric: 42.4116 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 780/1000
2023-10-10 17:14:26.003 
Epoch 780/1000 
	 loss: 42.1830, MinusLogProbMetric: 42.1830, val_loss: 42.4119, val_MinusLogProbMetric: 42.4119

Epoch 780: val_loss did not improve from 42.40964
196/196 - 69s - loss: 42.1830 - MinusLogProbMetric: 42.1830 - val_loss: 42.4119 - val_MinusLogProbMetric: 42.4119 - lr: 1.6935e-08 - 69s/epoch - 353ms/step
Epoch 781/1000
2023-10-10 17:15:38.507 
Epoch 781/1000 
	 loss: 42.1807, MinusLogProbMetric: 42.1807, val_loss: 42.4101, val_MinusLogProbMetric: 42.4101

Epoch 781: val_loss did not improve from 42.40964
196/196 - 73s - loss: 42.1807 - MinusLogProbMetric: 42.1807 - val_loss: 42.4101 - val_MinusLogProbMetric: 42.4101 - lr: 1.6935e-08 - 73s/epoch - 370ms/step
Epoch 782/1000
2023-10-10 17:16:50.970 
Epoch 782/1000 
	 loss: 42.1820, MinusLogProbMetric: 42.1820, val_loss: 42.4090, val_MinusLogProbMetric: 42.4090

Epoch 782: val_loss improved from 42.40964 to 42.40905, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 74s - loss: 42.1820 - MinusLogProbMetric: 42.1820 - val_loss: 42.4090 - val_MinusLogProbMetric: 42.4090 - lr: 1.6935e-08 - 74s/epoch - 375ms/step
Epoch 783/1000
2023-10-10 17:18:00.023 
Epoch 783/1000 
	 loss: 42.1833, MinusLogProbMetric: 42.1833, val_loss: 42.4146, val_MinusLogProbMetric: 42.4146

Epoch 783: val_loss did not improve from 42.40905
196/196 - 68s - loss: 42.1833 - MinusLogProbMetric: 42.1833 - val_loss: 42.4146 - val_MinusLogProbMetric: 42.4146 - lr: 1.6935e-08 - 68s/epoch - 346ms/step
Epoch 784/1000
2023-10-10 17:19:09.684 
Epoch 784/1000 
	 loss: 42.1852, MinusLogProbMetric: 42.1852, val_loss: 42.4165, val_MinusLogProbMetric: 42.4165

Epoch 784: val_loss did not improve from 42.40905
196/196 - 70s - loss: 42.1852 - MinusLogProbMetric: 42.1852 - val_loss: 42.4165 - val_MinusLogProbMetric: 42.4165 - lr: 1.6935e-08 - 70s/epoch - 355ms/step
Epoch 785/1000
2023-10-10 17:20:18.629 
Epoch 785/1000 
	 loss: 42.1844, MinusLogProbMetric: 42.1844, val_loss: 42.4119, val_MinusLogProbMetric: 42.4119

Epoch 785: val_loss did not improve from 42.40905
196/196 - 69s - loss: 42.1844 - MinusLogProbMetric: 42.1844 - val_loss: 42.4119 - val_MinusLogProbMetric: 42.4119 - lr: 1.6935e-08 - 69s/epoch - 352ms/step
Epoch 786/1000
2023-10-10 17:21:23.843 
Epoch 786/1000 
	 loss: 42.1820, MinusLogProbMetric: 42.1820, val_loss: 42.4119, val_MinusLogProbMetric: 42.4119

Epoch 786: val_loss did not improve from 42.40905
196/196 - 65s - loss: 42.1820 - MinusLogProbMetric: 42.1820 - val_loss: 42.4119 - val_MinusLogProbMetric: 42.4119 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 787/1000
2023-10-10 17:22:29.458 
Epoch 787/1000 
	 loss: 42.1830, MinusLogProbMetric: 42.1830, val_loss: 42.4116, val_MinusLogProbMetric: 42.4116

Epoch 787: val_loss did not improve from 42.40905
196/196 - 66s - loss: 42.1830 - MinusLogProbMetric: 42.1830 - val_loss: 42.4116 - val_MinusLogProbMetric: 42.4116 - lr: 1.6935e-08 - 66s/epoch - 335ms/step
Epoch 788/1000
2023-10-10 17:23:33.121 
Epoch 788/1000 
	 loss: 42.1818, MinusLogProbMetric: 42.1818, val_loss: 42.4067, val_MinusLogProbMetric: 42.4067

Epoch 788: val_loss improved from 42.40905 to 42.40672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 65s - loss: 42.1818 - MinusLogProbMetric: 42.1818 - val_loss: 42.4067 - val_MinusLogProbMetric: 42.4067 - lr: 1.6935e-08 - 65s/epoch - 330ms/step
Epoch 789/1000
2023-10-10 17:24:41.445 
Epoch 789/1000 
	 loss: 42.1812, MinusLogProbMetric: 42.1812, val_loss: 42.4116, val_MinusLogProbMetric: 42.4116

Epoch 789: val_loss did not improve from 42.40672
196/196 - 67s - loss: 42.1812 - MinusLogProbMetric: 42.1812 - val_loss: 42.4116 - val_MinusLogProbMetric: 42.4116 - lr: 1.6935e-08 - 67s/epoch - 343ms/step
Epoch 790/1000
2023-10-10 17:25:49.183 
Epoch 790/1000 
	 loss: 42.1821, MinusLogProbMetric: 42.1821, val_loss: 42.4095, val_MinusLogProbMetric: 42.4095

Epoch 790: val_loss did not improve from 42.40672
196/196 - 68s - loss: 42.1821 - MinusLogProbMetric: 42.1821 - val_loss: 42.4095 - val_MinusLogProbMetric: 42.4095 - lr: 1.6935e-08 - 68s/epoch - 345ms/step
Epoch 791/1000
2023-10-10 17:26:54.724 
Epoch 791/1000 
	 loss: 42.1825, MinusLogProbMetric: 42.1825, val_loss: 42.4125, val_MinusLogProbMetric: 42.4125

Epoch 791: val_loss did not improve from 42.40672
196/196 - 66s - loss: 42.1825 - MinusLogProbMetric: 42.1825 - val_loss: 42.4125 - val_MinusLogProbMetric: 42.4125 - lr: 1.6935e-08 - 66s/epoch - 334ms/step
Epoch 792/1000
2023-10-10 17:27:59.790 
Epoch 792/1000 
	 loss: 42.1847, MinusLogProbMetric: 42.1847, val_loss: 42.4159, val_MinusLogProbMetric: 42.4159

Epoch 792: val_loss did not improve from 42.40672
196/196 - 65s - loss: 42.1847 - MinusLogProbMetric: 42.1847 - val_loss: 42.4159 - val_MinusLogProbMetric: 42.4159 - lr: 1.6935e-08 - 65s/epoch - 332ms/step
Epoch 793/1000
2023-10-10 17:29:07.245 
Epoch 793/1000 
	 loss: 42.1840, MinusLogProbMetric: 42.1840, val_loss: 42.4124, val_MinusLogProbMetric: 42.4124

Epoch 793: val_loss did not improve from 42.40672
196/196 - 67s - loss: 42.1840 - MinusLogProbMetric: 42.1840 - val_loss: 42.4124 - val_MinusLogProbMetric: 42.4124 - lr: 1.6935e-08 - 67s/epoch - 344ms/step
Epoch 794/1000
2023-10-10 17:30:12.015 
Epoch 794/1000 
	 loss: 42.1833, MinusLogProbMetric: 42.1833, val_loss: 42.4099, val_MinusLogProbMetric: 42.4099

Epoch 794: val_loss did not improve from 42.40672
196/196 - 65s - loss: 42.1833 - MinusLogProbMetric: 42.1833 - val_loss: 42.4099 - val_MinusLogProbMetric: 42.4099 - lr: 1.6935e-08 - 65s/epoch - 330ms/step
Epoch 795/1000
2023-10-10 17:31:19.017 
Epoch 795/1000 
	 loss: 42.1835, MinusLogProbMetric: 42.1835, val_loss: 42.4133, val_MinusLogProbMetric: 42.4133

Epoch 795: val_loss did not improve from 42.40672
196/196 - 67s - loss: 42.1835 - MinusLogProbMetric: 42.1835 - val_loss: 42.4133 - val_MinusLogProbMetric: 42.4133 - lr: 1.6935e-08 - 67s/epoch - 342ms/step
Epoch 796/1000
2023-10-10 17:32:23.421 
Epoch 796/1000 
	 loss: 42.1800, MinusLogProbMetric: 42.1800, val_loss: 42.4100, val_MinusLogProbMetric: 42.4100

Epoch 796: val_loss did not improve from 42.40672
196/196 - 64s - loss: 42.1800 - MinusLogProbMetric: 42.1800 - val_loss: 42.4100 - val_MinusLogProbMetric: 42.4100 - lr: 1.6935e-08 - 64s/epoch - 329ms/step
Epoch 797/1000
2023-10-10 17:33:29.745 
Epoch 797/1000 
	 loss: 42.1816, MinusLogProbMetric: 42.1816, val_loss: 42.4128, val_MinusLogProbMetric: 42.4128

Epoch 797: val_loss did not improve from 42.40672
196/196 - 66s - loss: 42.1816 - MinusLogProbMetric: 42.1816 - val_loss: 42.4128 - val_MinusLogProbMetric: 42.4128 - lr: 1.6935e-08 - 66s/epoch - 338ms/step
Epoch 798/1000
2023-10-10 17:34:35.027 
Epoch 798/1000 
	 loss: 42.1798, MinusLogProbMetric: 42.1798, val_loss: 42.4073, val_MinusLogProbMetric: 42.4073

Epoch 798: val_loss did not improve from 42.40672
196/196 - 65s - loss: 42.1798 - MinusLogProbMetric: 42.1798 - val_loss: 42.4073 - val_MinusLogProbMetric: 42.4073 - lr: 1.6935e-08 - 65s/epoch - 333ms/step
Epoch 799/1000
2023-10-10 17:35:40.552 
Epoch 799/1000 
	 loss: 42.1804, MinusLogProbMetric: 42.1804, val_loss: 42.4063, val_MinusLogProbMetric: 42.4063

Epoch 799: val_loss improved from 42.40672 to 42.40630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 67s - loss: 42.1804 - MinusLogProbMetric: 42.1804 - val_loss: 42.4063 - val_MinusLogProbMetric: 42.4063 - lr: 1.6935e-08 - 67s/epoch - 341ms/step
Epoch 800/1000
2023-10-10 17:36:48.260 
Epoch 800/1000 
	 loss: 42.1786, MinusLogProbMetric: 42.1786, val_loss: 42.4087, val_MinusLogProbMetric: 42.4087

Epoch 800: val_loss did not improve from 42.40630
196/196 - 66s - loss: 42.1786 - MinusLogProbMetric: 42.1786 - val_loss: 42.4087 - val_MinusLogProbMetric: 42.4087 - lr: 1.6935e-08 - 66s/epoch - 339ms/step
Epoch 801/1000
2023-10-10 17:37:52.761 
Epoch 801/1000 
	 loss: 42.1802, MinusLogProbMetric: 42.1802, val_loss: 42.4071, val_MinusLogProbMetric: 42.4071

Epoch 801: val_loss did not improve from 42.40630
196/196 - 65s - loss: 42.1802 - MinusLogProbMetric: 42.1802 - val_loss: 42.4071 - val_MinusLogProbMetric: 42.4071 - lr: 1.6935e-08 - 65s/epoch - 329ms/step
Epoch 802/1000
2023-10-10 17:39:02.784 
Epoch 802/1000 
	 loss: 42.1797, MinusLogProbMetric: 42.1797, val_loss: 42.4072, val_MinusLogProbMetric: 42.4072

Epoch 802: val_loss did not improve from 42.40630
196/196 - 70s - loss: 42.1797 - MinusLogProbMetric: 42.1797 - val_loss: 42.4072 - val_MinusLogProbMetric: 42.4072 - lr: 1.6935e-08 - 70s/epoch - 357ms/step
Epoch 803/1000
2023-10-10 17:40:37.101 
Epoch 803/1000 
	 loss: 42.1788, MinusLogProbMetric: 42.1788, val_loss: 42.4094, val_MinusLogProbMetric: 42.4094

Epoch 803: val_loss did not improve from 42.40630
196/196 - 94s - loss: 42.1788 - MinusLogProbMetric: 42.1788 - val_loss: 42.4094 - val_MinusLogProbMetric: 42.4094 - lr: 1.6935e-08 - 94s/epoch - 481ms/step
Epoch 804/1000
2023-10-10 17:42:19.611 
Epoch 804/1000 
	 loss: 42.1801, MinusLogProbMetric: 42.1801, val_loss: 42.4064, val_MinusLogProbMetric: 42.4064

Epoch 804: val_loss did not improve from 42.40630
196/196 - 102s - loss: 42.1801 - MinusLogProbMetric: 42.1801 - val_loss: 42.4064 - val_MinusLogProbMetric: 42.4064 - lr: 1.6935e-08 - 102s/epoch - 523ms/step
Epoch 805/1000
2023-10-10 17:44:01.571 
Epoch 805/1000 
	 loss: 42.1797, MinusLogProbMetric: 42.1797, val_loss: 42.4102, val_MinusLogProbMetric: 42.4102

Epoch 805: val_loss did not improve from 42.40630
196/196 - 102s - loss: 42.1797 - MinusLogProbMetric: 42.1797 - val_loss: 42.4102 - val_MinusLogProbMetric: 42.4102 - lr: 1.6935e-08 - 102s/epoch - 520ms/step
Epoch 806/1000
2023-10-10 17:45:35.404 
Epoch 806/1000 
	 loss: 42.1814, MinusLogProbMetric: 42.1814, val_loss: 42.4134, val_MinusLogProbMetric: 42.4134

Epoch 806: val_loss did not improve from 42.40630
196/196 - 94s - loss: 42.1814 - MinusLogProbMetric: 42.1814 - val_loss: 42.4134 - val_MinusLogProbMetric: 42.4134 - lr: 1.6935e-08 - 94s/epoch - 479ms/step
Epoch 807/1000
2023-10-10 17:47:12.668 
Epoch 807/1000 
	 loss: 42.1855, MinusLogProbMetric: 42.1855, val_loss: 42.4131, val_MinusLogProbMetric: 42.4131

Epoch 807: val_loss did not improve from 42.40630
196/196 - 97s - loss: 42.1855 - MinusLogProbMetric: 42.1855 - val_loss: 42.4131 - val_MinusLogProbMetric: 42.4131 - lr: 1.6935e-08 - 97s/epoch - 496ms/step
Epoch 808/1000
2023-10-10 17:48:49.433 
Epoch 808/1000 
	 loss: 42.1837, MinusLogProbMetric: 42.1837, val_loss: 42.4099, val_MinusLogProbMetric: 42.4099

Epoch 808: val_loss did not improve from 42.40630
196/196 - 97s - loss: 42.1837 - MinusLogProbMetric: 42.1837 - val_loss: 42.4099 - val_MinusLogProbMetric: 42.4099 - lr: 1.6935e-08 - 97s/epoch - 494ms/step
Epoch 809/1000
2023-10-10 17:50:26.170 
Epoch 809/1000 
	 loss: 42.1834, MinusLogProbMetric: 42.1834, val_loss: 42.4104, val_MinusLogProbMetric: 42.4104

Epoch 809: val_loss did not improve from 42.40630
196/196 - 97s - loss: 42.1834 - MinusLogProbMetric: 42.1834 - val_loss: 42.4104 - val_MinusLogProbMetric: 42.4104 - lr: 1.6935e-08 - 97s/epoch - 493ms/step
Epoch 810/1000
2023-10-10 17:51:59.416 
Epoch 810/1000 
	 loss: 42.1815, MinusLogProbMetric: 42.1815, val_loss: 42.4093, val_MinusLogProbMetric: 42.4093

Epoch 810: val_loss did not improve from 42.40630
196/196 - 93s - loss: 42.1815 - MinusLogProbMetric: 42.1815 - val_loss: 42.4093 - val_MinusLogProbMetric: 42.4093 - lr: 1.6935e-08 - 93s/epoch - 476ms/step
Epoch 811/1000
2023-10-10 17:53:34.957 
Epoch 811/1000 
	 loss: 42.1799, MinusLogProbMetric: 42.1799, val_loss: 42.4078, val_MinusLogProbMetric: 42.4078

Epoch 811: val_loss did not improve from 42.40630
196/196 - 96s - loss: 42.1799 - MinusLogProbMetric: 42.1799 - val_loss: 42.4078 - val_MinusLogProbMetric: 42.4078 - lr: 1.6935e-08 - 96s/epoch - 487ms/step
Epoch 812/1000
2023-10-10 17:55:07.725 
Epoch 812/1000 
	 loss: 42.1819, MinusLogProbMetric: 42.1819, val_loss: 42.4082, val_MinusLogProbMetric: 42.4082

Epoch 812: val_loss did not improve from 42.40630
196/196 - 93s - loss: 42.1819 - MinusLogProbMetric: 42.1819 - val_loss: 42.4082 - val_MinusLogProbMetric: 42.4082 - lr: 1.6935e-08 - 93s/epoch - 473ms/step
Epoch 813/1000
2023-10-10 17:56:43.539 
Epoch 813/1000 
	 loss: 42.1806, MinusLogProbMetric: 42.1806, val_loss: 42.4040, val_MinusLogProbMetric: 42.4040

Epoch 813: val_loss improved from 42.40630 to 42.40398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 97s - loss: 42.1806 - MinusLogProbMetric: 42.1806 - val_loss: 42.4040 - val_MinusLogProbMetric: 42.4040 - lr: 1.6935e-08 - 97s/epoch - 497ms/step
Epoch 814/1000
2023-10-10 17:58:20.619 
Epoch 814/1000 
	 loss: 42.1786, MinusLogProbMetric: 42.1786, val_loss: 42.4080, val_MinusLogProbMetric: 42.4080

Epoch 814: val_loss did not improve from 42.40398
196/196 - 95s - loss: 42.1786 - MinusLogProbMetric: 42.1786 - val_loss: 42.4080 - val_MinusLogProbMetric: 42.4080 - lr: 1.6935e-08 - 95s/epoch - 487ms/step
Epoch 815/1000
2023-10-10 17:59:52.788 
Epoch 815/1000 
	 loss: 42.1800, MinusLogProbMetric: 42.1800, val_loss: 42.4038, val_MinusLogProbMetric: 42.4038

Epoch 815: val_loss improved from 42.40398 to 42.40377, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 95s - loss: 42.1800 - MinusLogProbMetric: 42.1800 - val_loss: 42.4038 - val_MinusLogProbMetric: 42.4038 - lr: 1.6935e-08 - 95s/epoch - 484ms/step
Epoch 816/1000
2023-10-10 18:01:30.512 
Epoch 816/1000 
	 loss: 42.1792, MinusLogProbMetric: 42.1792, val_loss: 42.4013, val_MinusLogProbMetric: 42.4013

Epoch 816: val_loss improved from 42.40377 to 42.40131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 96s - loss: 42.1792 - MinusLogProbMetric: 42.1792 - val_loss: 42.4013 - val_MinusLogProbMetric: 42.4013 - lr: 1.6935e-08 - 96s/epoch - 489ms/step
Epoch 817/1000
2023-10-10 18:03:08.099 
Epoch 817/1000 
	 loss: 42.1789, MinusLogProbMetric: 42.1789, val_loss: 42.4048, val_MinusLogProbMetric: 42.4048

Epoch 817: val_loss did not improve from 42.40131
196/196 - 97s - loss: 42.1789 - MinusLogProbMetric: 42.1789 - val_loss: 42.4048 - val_MinusLogProbMetric: 42.4048 - lr: 1.6935e-08 - 97s/epoch - 494ms/step
Epoch 818/1000
2023-10-10 18:04:40.267 
Epoch 818/1000 
	 loss: 42.1790, MinusLogProbMetric: 42.1790, val_loss: 42.4075, val_MinusLogProbMetric: 42.4075

Epoch 818: val_loss did not improve from 42.40131
196/196 - 92s - loss: 42.1790 - MinusLogProbMetric: 42.1790 - val_loss: 42.4075 - val_MinusLogProbMetric: 42.4075 - lr: 1.6935e-08 - 92s/epoch - 470ms/step
Epoch 819/1000
2023-10-10 18:06:14.669 
Epoch 819/1000 
	 loss: 42.1782, MinusLogProbMetric: 42.1782, val_loss: 42.4089, val_MinusLogProbMetric: 42.4089

Epoch 819: val_loss did not improve from 42.40131
196/196 - 94s - loss: 42.1782 - MinusLogProbMetric: 42.1782 - val_loss: 42.4089 - val_MinusLogProbMetric: 42.4089 - lr: 1.6935e-08 - 94s/epoch - 482ms/step
Epoch 820/1000
2023-10-10 18:07:50.528 
Epoch 820/1000 
	 loss: 42.1780, MinusLogProbMetric: 42.1780, val_loss: 42.4073, val_MinusLogProbMetric: 42.4073

Epoch 820: val_loss did not improve from 42.40131
196/196 - 96s - loss: 42.1780 - MinusLogProbMetric: 42.1780 - val_loss: 42.4073 - val_MinusLogProbMetric: 42.4073 - lr: 1.6935e-08 - 96s/epoch - 489ms/step
Epoch 821/1000
2023-10-10 18:09:22.069 
Epoch 821/1000 
	 loss: 42.1772, MinusLogProbMetric: 42.1772, val_loss: 42.4045, val_MinusLogProbMetric: 42.4045

Epoch 821: val_loss did not improve from 42.40131
196/196 - 92s - loss: 42.1772 - MinusLogProbMetric: 42.1772 - val_loss: 42.4045 - val_MinusLogProbMetric: 42.4045 - lr: 1.6935e-08 - 92s/epoch - 467ms/step
Epoch 822/1000
2023-10-10 18:10:58.086 
Epoch 822/1000 
	 loss: 42.1797, MinusLogProbMetric: 42.1797, val_loss: 42.4069, val_MinusLogProbMetric: 42.4069

Epoch 822: val_loss did not improve from 42.40131
196/196 - 96s - loss: 42.1797 - MinusLogProbMetric: 42.1797 - val_loss: 42.4069 - val_MinusLogProbMetric: 42.4069 - lr: 1.6935e-08 - 96s/epoch - 490ms/step
Epoch 823/1000
2023-10-10 18:12:37.547 
Epoch 823/1000 
	 loss: 42.1768, MinusLogProbMetric: 42.1768, val_loss: 42.4043, val_MinusLogProbMetric: 42.4043

Epoch 823: val_loss did not improve from 42.40131
196/196 - 99s - loss: 42.1768 - MinusLogProbMetric: 42.1768 - val_loss: 42.4043 - val_MinusLogProbMetric: 42.4043 - lr: 1.6935e-08 - 99s/epoch - 507ms/step
Epoch 824/1000
2023-10-10 18:14:12.132 
Epoch 824/1000 
	 loss: 42.1769, MinusLogProbMetric: 42.1769, val_loss: 42.4053, val_MinusLogProbMetric: 42.4053

Epoch 824: val_loss did not improve from 42.40131
196/196 - 95s - loss: 42.1769 - MinusLogProbMetric: 42.1769 - val_loss: 42.4053 - val_MinusLogProbMetric: 42.4053 - lr: 1.6935e-08 - 95s/epoch - 483ms/step
Epoch 825/1000
2023-10-10 18:15:43.918 
Epoch 825/1000 
	 loss: 42.1766, MinusLogProbMetric: 42.1766, val_loss: 42.4044, val_MinusLogProbMetric: 42.4044

Epoch 825: val_loss did not improve from 42.40131
196/196 - 92s - loss: 42.1766 - MinusLogProbMetric: 42.1766 - val_loss: 42.4044 - val_MinusLogProbMetric: 42.4044 - lr: 1.6935e-08 - 92s/epoch - 468ms/step
Epoch 826/1000
2023-10-10 18:17:17.435 
Epoch 826/1000 
	 loss: 42.1752, MinusLogProbMetric: 42.1752, val_loss: 42.4027, val_MinusLogProbMetric: 42.4027

Epoch 826: val_loss did not improve from 42.40131
196/196 - 94s - loss: 42.1752 - MinusLogProbMetric: 42.1752 - val_loss: 42.4027 - val_MinusLogProbMetric: 42.4027 - lr: 1.6935e-08 - 94s/epoch - 477ms/step
Epoch 827/1000
2023-10-10 18:18:51.193 
Epoch 827/1000 
	 loss: 42.1736, MinusLogProbMetric: 42.1736, val_loss: 42.4018, val_MinusLogProbMetric: 42.4018

Epoch 827: val_loss did not improve from 42.40131
196/196 - 94s - loss: 42.1736 - MinusLogProbMetric: 42.1736 - val_loss: 42.4018 - val_MinusLogProbMetric: 42.4018 - lr: 1.6935e-08 - 94s/epoch - 478ms/step
Epoch 828/1000
2023-10-10 18:20:26.804 
Epoch 828/1000 
	 loss: 42.1736, MinusLogProbMetric: 42.1736, val_loss: 42.4031, val_MinusLogProbMetric: 42.4031

Epoch 828: val_loss did not improve from 42.40131
196/196 - 96s - loss: 42.1736 - MinusLogProbMetric: 42.1736 - val_loss: 42.4031 - val_MinusLogProbMetric: 42.4031 - lr: 1.6935e-08 - 96s/epoch - 488ms/step
Epoch 829/1000
2023-10-10 18:22:00.602 
Epoch 829/1000 
	 loss: 42.1740, MinusLogProbMetric: 42.1740, val_loss: 42.4034, val_MinusLogProbMetric: 42.4034

Epoch 829: val_loss did not improve from 42.40131
196/196 - 94s - loss: 42.1740 - MinusLogProbMetric: 42.1740 - val_loss: 42.4034 - val_MinusLogProbMetric: 42.4034 - lr: 1.6935e-08 - 94s/epoch - 479ms/step
Epoch 830/1000
2023-10-10 18:23:36.709 
Epoch 830/1000 
	 loss: 42.1746, MinusLogProbMetric: 42.1746, val_loss: 42.4044, val_MinusLogProbMetric: 42.4044

Epoch 830: val_loss did not improve from 42.40131
196/196 - 96s - loss: 42.1746 - MinusLogProbMetric: 42.1746 - val_loss: 42.4044 - val_MinusLogProbMetric: 42.4044 - lr: 1.6935e-08 - 96s/epoch - 490ms/step
Epoch 831/1000
2023-10-10 18:25:13.680 
Epoch 831/1000 
	 loss: 42.1737, MinusLogProbMetric: 42.1737, val_loss: 42.4007, val_MinusLogProbMetric: 42.4007

Epoch 831: val_loss improved from 42.40131 to 42.40068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 98s - loss: 42.1737 - MinusLogProbMetric: 42.1737 - val_loss: 42.4007 - val_MinusLogProbMetric: 42.4007 - lr: 1.6935e-08 - 98s/epoch - 499ms/step
Epoch 832/1000
2023-10-10 18:26:51.333 
Epoch 832/1000 
	 loss: 42.1731, MinusLogProbMetric: 42.1731, val_loss: 42.4005, val_MinusLogProbMetric: 42.4005

Epoch 832: val_loss improved from 42.40068 to 42.40047, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 98s - loss: 42.1731 - MinusLogProbMetric: 42.1731 - val_loss: 42.4005 - val_MinusLogProbMetric: 42.4005 - lr: 1.6935e-08 - 98s/epoch - 501ms/step
Epoch 833/1000
2023-10-10 18:28:27.311 
Epoch 833/1000 
	 loss: 42.1744, MinusLogProbMetric: 42.1744, val_loss: 42.4003, val_MinusLogProbMetric: 42.4003

Epoch 833: val_loss improved from 42.40047 to 42.40031, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 97s - loss: 42.1744 - MinusLogProbMetric: 42.1744 - val_loss: 42.4003 - val_MinusLogProbMetric: 42.4003 - lr: 1.6935e-08 - 97s/epoch - 493ms/step
Epoch 834/1000
2023-10-10 18:30:02.749 
Epoch 834/1000 
	 loss: 42.1746, MinusLogProbMetric: 42.1746, val_loss: 42.4018, val_MinusLogProbMetric: 42.4018

Epoch 834: val_loss did not improve from 42.40031
196/196 - 93s - loss: 42.1746 - MinusLogProbMetric: 42.1746 - val_loss: 42.4018 - val_MinusLogProbMetric: 42.4018 - lr: 1.6935e-08 - 93s/epoch - 476ms/step
Epoch 835/1000
2023-10-10 18:31:37.883 
Epoch 835/1000 
	 loss: 42.1744, MinusLogProbMetric: 42.1744, val_loss: 42.4028, val_MinusLogProbMetric: 42.4028

Epoch 835: val_loss did not improve from 42.40031
196/196 - 95s - loss: 42.1744 - MinusLogProbMetric: 42.1744 - val_loss: 42.4028 - val_MinusLogProbMetric: 42.4028 - lr: 1.6935e-08 - 95s/epoch - 485ms/step
Epoch 836/1000
2023-10-10 18:33:12.349 
Epoch 836/1000 
	 loss: 42.1750, MinusLogProbMetric: 42.1750, val_loss: 42.4054, val_MinusLogProbMetric: 42.4054

Epoch 836: val_loss did not improve from 42.40031
196/196 - 95s - loss: 42.1750 - MinusLogProbMetric: 42.1750 - val_loss: 42.4054 - val_MinusLogProbMetric: 42.4054 - lr: 1.6935e-08 - 95s/epoch - 482ms/step
Epoch 837/1000
2023-10-10 18:34:50.187 
Epoch 837/1000 
	 loss: 42.1758, MinusLogProbMetric: 42.1758, val_loss: 42.4045, val_MinusLogProbMetric: 42.4045

Epoch 837: val_loss did not improve from 42.40031
196/196 - 98s - loss: 42.1758 - MinusLogProbMetric: 42.1758 - val_loss: 42.4045 - val_MinusLogProbMetric: 42.4045 - lr: 1.6935e-08 - 98s/epoch - 499ms/step
Epoch 838/1000
2023-10-10 18:36:21.181 
Epoch 838/1000 
	 loss: 42.1738, MinusLogProbMetric: 42.1738, val_loss: 42.4016, val_MinusLogProbMetric: 42.4016

Epoch 838: val_loss did not improve from 42.40031
196/196 - 91s - loss: 42.1738 - MinusLogProbMetric: 42.1738 - val_loss: 42.4016 - val_MinusLogProbMetric: 42.4016 - lr: 1.6935e-08 - 91s/epoch - 464ms/step
Epoch 839/1000
2023-10-10 18:37:56.364 
Epoch 839/1000 
	 loss: 42.1743, MinusLogProbMetric: 42.1743, val_loss: 42.4034, val_MinusLogProbMetric: 42.4034

Epoch 839: val_loss did not improve from 42.40031
196/196 - 95s - loss: 42.1743 - MinusLogProbMetric: 42.1743 - val_loss: 42.4034 - val_MinusLogProbMetric: 42.4034 - lr: 1.6935e-08 - 95s/epoch - 486ms/step
Epoch 840/1000
2023-10-10 18:39:32.251 
Epoch 840/1000 
	 loss: 42.1760, MinusLogProbMetric: 42.1760, val_loss: 42.4088, val_MinusLogProbMetric: 42.4088

Epoch 840: val_loss did not improve from 42.40031
196/196 - 96s - loss: 42.1760 - MinusLogProbMetric: 42.1760 - val_loss: 42.4088 - val_MinusLogProbMetric: 42.4088 - lr: 1.6935e-08 - 96s/epoch - 489ms/step
Epoch 841/1000
2023-10-10 18:41:03.083 
Epoch 841/1000 
	 loss: 42.1778, MinusLogProbMetric: 42.1778, val_loss: 42.4077, val_MinusLogProbMetric: 42.4077

Epoch 841: val_loss did not improve from 42.40031
196/196 - 91s - loss: 42.1778 - MinusLogProbMetric: 42.1778 - val_loss: 42.4077 - val_MinusLogProbMetric: 42.4077 - lr: 1.6935e-08 - 91s/epoch - 463ms/step
Epoch 842/1000
2023-10-10 18:42:41.296 
Epoch 842/1000 
	 loss: 42.1761, MinusLogProbMetric: 42.1761, val_loss: 42.4079, val_MinusLogProbMetric: 42.4079

Epoch 842: val_loss did not improve from 42.40031
196/196 - 98s - loss: 42.1761 - MinusLogProbMetric: 42.1761 - val_loss: 42.4079 - val_MinusLogProbMetric: 42.4079 - lr: 1.6935e-08 - 98s/epoch - 501ms/step
Epoch 843/1000
2023-10-10 18:44:12.704 
Epoch 843/1000 
	 loss: 42.1745, MinusLogProbMetric: 42.1745, val_loss: 42.4074, val_MinusLogProbMetric: 42.4074

Epoch 843: val_loss did not improve from 42.40031
196/196 - 91s - loss: 42.1745 - MinusLogProbMetric: 42.1745 - val_loss: 42.4074 - val_MinusLogProbMetric: 42.4074 - lr: 1.6935e-08 - 91s/epoch - 466ms/step
Epoch 844/1000
2023-10-10 18:45:47.857 
Epoch 844/1000 
	 loss: 42.1741, MinusLogProbMetric: 42.1741, val_loss: 42.4068, val_MinusLogProbMetric: 42.4068

Epoch 844: val_loss did not improve from 42.40031
196/196 - 95s - loss: 42.1741 - MinusLogProbMetric: 42.1741 - val_loss: 42.4068 - val_MinusLogProbMetric: 42.4068 - lr: 1.6935e-08 - 95s/epoch - 485ms/step
Epoch 845/1000
2023-10-10 18:47:20.244 
Epoch 845/1000 
	 loss: 42.1735, MinusLogProbMetric: 42.1735, val_loss: 42.4034, val_MinusLogProbMetric: 42.4034

Epoch 845: val_loss did not improve from 42.40031
196/196 - 92s - loss: 42.1735 - MinusLogProbMetric: 42.1735 - val_loss: 42.4034 - val_MinusLogProbMetric: 42.4034 - lr: 1.6935e-08 - 92s/epoch - 471ms/step
Epoch 846/1000
2023-10-10 18:48:54.712 
Epoch 846/1000 
	 loss: 42.1731, MinusLogProbMetric: 42.1731, val_loss: 42.4035, val_MinusLogProbMetric: 42.4035

Epoch 846: val_loss did not improve from 42.40031
196/196 - 94s - loss: 42.1731 - MinusLogProbMetric: 42.1731 - val_loss: 42.4035 - val_MinusLogProbMetric: 42.4035 - lr: 1.6935e-08 - 94s/epoch - 482ms/step
Epoch 847/1000
2023-10-10 18:50:27.472 
Epoch 847/1000 
	 loss: 42.1711, MinusLogProbMetric: 42.1711, val_loss: 42.4030, val_MinusLogProbMetric: 42.4030

Epoch 847: val_loss did not improve from 42.40031
196/196 - 93s - loss: 42.1711 - MinusLogProbMetric: 42.1711 - val_loss: 42.4030 - val_MinusLogProbMetric: 42.4030 - lr: 1.6935e-08 - 93s/epoch - 473ms/step
Epoch 848/1000
2023-10-10 18:51:58.773 
Epoch 848/1000 
	 loss: 42.1714, MinusLogProbMetric: 42.1714, val_loss: 42.4024, val_MinusLogProbMetric: 42.4024

Epoch 848: val_loss did not improve from 42.40031
196/196 - 91s - loss: 42.1714 - MinusLogProbMetric: 42.1714 - val_loss: 42.4024 - val_MinusLogProbMetric: 42.4024 - lr: 1.6935e-08 - 91s/epoch - 466ms/step
Epoch 849/1000
2023-10-10 18:53:31.268 
Epoch 849/1000 
	 loss: 42.1733, MinusLogProbMetric: 42.1733, val_loss: 42.4068, val_MinusLogProbMetric: 42.4068

Epoch 849: val_loss did not improve from 42.40031
196/196 - 93s - loss: 42.1733 - MinusLogProbMetric: 42.1733 - val_loss: 42.4068 - val_MinusLogProbMetric: 42.4068 - lr: 1.6935e-08 - 93s/epoch - 472ms/step
Epoch 850/1000
2023-10-10 18:55:09.425 
Epoch 850/1000 
	 loss: 42.1709, MinusLogProbMetric: 42.1709, val_loss: 42.4030, val_MinusLogProbMetric: 42.4030

Epoch 850: val_loss did not improve from 42.40031
196/196 - 98s - loss: 42.1709 - MinusLogProbMetric: 42.1709 - val_loss: 42.4030 - val_MinusLogProbMetric: 42.4030 - lr: 1.6935e-08 - 98s/epoch - 501ms/step
Epoch 851/1000
2023-10-10 18:56:42.678 
Epoch 851/1000 
	 loss: 42.1714, MinusLogProbMetric: 42.1714, val_loss: 42.4058, val_MinusLogProbMetric: 42.4058

Epoch 851: val_loss did not improve from 42.40031
196/196 - 93s - loss: 42.1714 - MinusLogProbMetric: 42.1714 - val_loss: 42.4058 - val_MinusLogProbMetric: 42.4058 - lr: 1.6935e-08 - 93s/epoch - 476ms/step
Epoch 852/1000
2023-10-10 18:58:13.763 
Epoch 852/1000 
	 loss: 42.1723, MinusLogProbMetric: 42.1723, val_loss: 42.4081, val_MinusLogProbMetric: 42.4081

Epoch 852: val_loss did not improve from 42.40031
196/196 - 91s - loss: 42.1723 - MinusLogProbMetric: 42.1723 - val_loss: 42.4081 - val_MinusLogProbMetric: 42.4081 - lr: 1.6935e-08 - 91s/epoch - 465ms/step
Epoch 853/1000
2023-10-10 18:59:45.279 
Epoch 853/1000 
	 loss: 42.1724, MinusLogProbMetric: 42.1724, val_loss: 42.4031, val_MinusLogProbMetric: 42.4031

Epoch 853: val_loss did not improve from 42.40031
196/196 - 92s - loss: 42.1724 - MinusLogProbMetric: 42.1724 - val_loss: 42.4031 - val_MinusLogProbMetric: 42.4031 - lr: 1.6935e-08 - 92s/epoch - 467ms/step
Epoch 854/1000
2023-10-10 19:01:21.111 
Epoch 854/1000 
	 loss: 42.1724, MinusLogProbMetric: 42.1724, val_loss: 42.4042, val_MinusLogProbMetric: 42.4042

Epoch 854: val_loss did not improve from 42.40031
196/196 - 96s - loss: 42.1724 - MinusLogProbMetric: 42.1724 - val_loss: 42.4042 - val_MinusLogProbMetric: 42.4042 - lr: 1.6935e-08 - 96s/epoch - 489ms/step
Epoch 855/1000
2023-10-10 19:02:55.843 
Epoch 855/1000 
	 loss: 42.1715, MinusLogProbMetric: 42.1715, val_loss: 42.4066, val_MinusLogProbMetric: 42.4066

Epoch 855: val_loss did not improve from 42.40031
196/196 - 95s - loss: 42.1715 - MinusLogProbMetric: 42.1715 - val_loss: 42.4066 - val_MinusLogProbMetric: 42.4066 - lr: 1.6935e-08 - 95s/epoch - 483ms/step
Epoch 856/1000
2023-10-10 19:04:29.507 
Epoch 856/1000 
	 loss: 42.1712, MinusLogProbMetric: 42.1712, val_loss: 42.4002, val_MinusLogProbMetric: 42.4002

Epoch 856: val_loss improved from 42.40031 to 42.40023, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 96s - loss: 42.1712 - MinusLogProbMetric: 42.1712 - val_loss: 42.4002 - val_MinusLogProbMetric: 42.4002 - lr: 1.6935e-08 - 96s/epoch - 489ms/step
Epoch 857/1000
2023-10-10 19:06:07.149 
Epoch 857/1000 
	 loss: 42.1713, MinusLogProbMetric: 42.1713, val_loss: 42.4012, val_MinusLogProbMetric: 42.4012

Epoch 857: val_loss did not improve from 42.40023
196/196 - 96s - loss: 42.1713 - MinusLogProbMetric: 42.1713 - val_loss: 42.4012 - val_MinusLogProbMetric: 42.4012 - lr: 1.6935e-08 - 96s/epoch - 487ms/step
Epoch 858/1000
2023-10-10 19:07:44.459 
Epoch 858/1000 
	 loss: 42.1717, MinusLogProbMetric: 42.1717, val_loss: 42.3993, val_MinusLogProbMetric: 42.3993

Epoch 858: val_loss improved from 42.40023 to 42.39928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 98s - loss: 42.1717 - MinusLogProbMetric: 42.1717 - val_loss: 42.3993 - val_MinusLogProbMetric: 42.3993 - lr: 1.6935e-08 - 98s/epoch - 500ms/step
Epoch 859/1000
2023-10-10 19:09:16.827 
Epoch 859/1000 
	 loss: 42.1710, MinusLogProbMetric: 42.1710, val_loss: 42.3981, val_MinusLogProbMetric: 42.3981

Epoch 859: val_loss improved from 42.39928 to 42.39806, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 95s - loss: 42.1710 - MinusLogProbMetric: 42.1710 - val_loss: 42.3981 - val_MinusLogProbMetric: 42.3981 - lr: 1.6935e-08 - 95s/epoch - 484ms/step
Epoch 860/1000
2023-10-10 19:10:52.583 
Epoch 860/1000 
	 loss: 42.1712, MinusLogProbMetric: 42.1712, val_loss: 42.4032, val_MinusLogProbMetric: 42.4032

Epoch 860: val_loss did not improve from 42.39806
196/196 - 93s - loss: 42.1712 - MinusLogProbMetric: 42.1712 - val_loss: 42.4032 - val_MinusLogProbMetric: 42.4032 - lr: 1.6935e-08 - 93s/epoch - 472ms/step
Epoch 861/1000
2023-10-10 19:12:27.739 
Epoch 861/1000 
	 loss: 42.1714, MinusLogProbMetric: 42.1714, val_loss: 42.4001, val_MinusLogProbMetric: 42.4001

Epoch 861: val_loss did not improve from 42.39806
196/196 - 95s - loss: 42.1714 - MinusLogProbMetric: 42.1714 - val_loss: 42.4001 - val_MinusLogProbMetric: 42.4001 - lr: 1.6935e-08 - 95s/epoch - 485ms/step
Epoch 862/1000
2023-10-10 19:13:59.585 
Epoch 862/1000 
	 loss: 42.1710, MinusLogProbMetric: 42.1710, val_loss: 42.4023, val_MinusLogProbMetric: 42.4023

Epoch 862: val_loss did not improve from 42.39806
196/196 - 92s - loss: 42.1710 - MinusLogProbMetric: 42.1710 - val_loss: 42.4023 - val_MinusLogProbMetric: 42.4023 - lr: 1.6935e-08 - 92s/epoch - 469ms/step
Epoch 863/1000
2023-10-10 19:15:35.556 
Epoch 863/1000 
	 loss: 42.1704, MinusLogProbMetric: 42.1704, val_loss: 42.4009, val_MinusLogProbMetric: 42.4009

Epoch 863: val_loss did not improve from 42.39806
196/196 - 96s - loss: 42.1704 - MinusLogProbMetric: 42.1704 - val_loss: 42.4009 - val_MinusLogProbMetric: 42.4009 - lr: 1.6935e-08 - 96s/epoch - 490ms/step
Epoch 864/1000
2023-10-10 19:17:08.654 
Epoch 864/1000 
	 loss: 42.1711, MinusLogProbMetric: 42.1711, val_loss: 42.4047, val_MinusLogProbMetric: 42.4047

Epoch 864: val_loss did not improve from 42.39806
196/196 - 93s - loss: 42.1711 - MinusLogProbMetric: 42.1711 - val_loss: 42.4047 - val_MinusLogProbMetric: 42.4047 - lr: 1.6935e-08 - 93s/epoch - 475ms/step
Epoch 865/1000
2023-10-10 19:18:43.310 
Epoch 865/1000 
	 loss: 42.1722, MinusLogProbMetric: 42.1722, val_loss: 42.4007, val_MinusLogProbMetric: 42.4007

Epoch 865: val_loss did not improve from 42.39806
196/196 - 95s - loss: 42.1722 - MinusLogProbMetric: 42.1722 - val_loss: 42.4007 - val_MinusLogProbMetric: 42.4007 - lr: 1.6935e-08 - 95s/epoch - 483ms/step
Epoch 866/1000
2023-10-10 19:20:15.417 
Epoch 866/1000 
	 loss: 42.1725, MinusLogProbMetric: 42.1725, val_loss: 42.3974, val_MinusLogProbMetric: 42.3974

Epoch 866: val_loss improved from 42.39806 to 42.39742, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 93s - loss: 42.1725 - MinusLogProbMetric: 42.1725 - val_loss: 42.3974 - val_MinusLogProbMetric: 42.3974 - lr: 1.6935e-08 - 93s/epoch - 474ms/step
Epoch 867/1000
2023-10-10 19:21:51.647 
Epoch 867/1000 
	 loss: 42.1702, MinusLogProbMetric: 42.1702, val_loss: 42.3967, val_MinusLogProbMetric: 42.3967

Epoch 867: val_loss improved from 42.39742 to 42.39669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 97s - loss: 42.1702 - MinusLogProbMetric: 42.1702 - val_loss: 42.3967 - val_MinusLogProbMetric: 42.3967 - lr: 1.6935e-08 - 97s/epoch - 496ms/step
Epoch 868/1000
2023-10-10 19:23:28.903 
Epoch 868/1000 
	 loss: 42.1681, MinusLogProbMetric: 42.1681, val_loss: 42.3997, val_MinusLogProbMetric: 42.3997

Epoch 868: val_loss did not improve from 42.39669
196/196 - 95s - loss: 42.1681 - MinusLogProbMetric: 42.1681 - val_loss: 42.3997 - val_MinusLogProbMetric: 42.3997 - lr: 1.6935e-08 - 95s/epoch - 486ms/step
Epoch 869/1000
2023-10-10 19:25:01.078 
Epoch 869/1000 
	 loss: 42.1707, MinusLogProbMetric: 42.1707, val_loss: 42.3972, val_MinusLogProbMetric: 42.3972

Epoch 869: val_loss did not improve from 42.39669
196/196 - 92s - loss: 42.1707 - MinusLogProbMetric: 42.1707 - val_loss: 42.3972 - val_MinusLogProbMetric: 42.3972 - lr: 1.6935e-08 - 92s/epoch - 470ms/step
Epoch 870/1000
2023-10-10 19:26:33.834 
Epoch 870/1000 
	 loss: 42.1687, MinusLogProbMetric: 42.1687, val_loss: 42.4004, val_MinusLogProbMetric: 42.4004

Epoch 870: val_loss did not improve from 42.39669
196/196 - 93s - loss: 42.1687 - MinusLogProbMetric: 42.1687 - val_loss: 42.4004 - val_MinusLogProbMetric: 42.4004 - lr: 1.6935e-08 - 93s/epoch - 473ms/step
Epoch 871/1000
2023-10-10 19:28:09.175 
Epoch 871/1000 
	 loss: 42.1710, MinusLogProbMetric: 42.1710, val_loss: 42.3995, val_MinusLogProbMetric: 42.3995

Epoch 871: val_loss did not improve from 42.39669
196/196 - 95s - loss: 42.1710 - MinusLogProbMetric: 42.1710 - val_loss: 42.3995 - val_MinusLogProbMetric: 42.3995 - lr: 1.6935e-08 - 95s/epoch - 486ms/step
Epoch 872/1000
2023-10-10 19:29:41.197 
Epoch 872/1000 
	 loss: 42.1695, MinusLogProbMetric: 42.1695, val_loss: 42.3999, val_MinusLogProbMetric: 42.3999

Epoch 872: val_loss did not improve from 42.39669
196/196 - 92s - loss: 42.1695 - MinusLogProbMetric: 42.1695 - val_loss: 42.3999 - val_MinusLogProbMetric: 42.3999 - lr: 1.6935e-08 - 92s/epoch - 470ms/step
Epoch 873/1000
2023-10-10 19:31:17.063 
Epoch 873/1000 
	 loss: 42.1692, MinusLogProbMetric: 42.1692, val_loss: 42.4010, val_MinusLogProbMetric: 42.4010

Epoch 873: val_loss did not improve from 42.39669
196/196 - 96s - loss: 42.1692 - MinusLogProbMetric: 42.1692 - val_loss: 42.4010 - val_MinusLogProbMetric: 42.4010 - lr: 1.6935e-08 - 96s/epoch - 489ms/step
Epoch 874/1000
2023-10-10 19:32:49.461 
Epoch 874/1000 
	 loss: 42.1709, MinusLogProbMetric: 42.1709, val_loss: 42.3978, val_MinusLogProbMetric: 42.3978

Epoch 874: val_loss did not improve from 42.39669
196/196 - 92s - loss: 42.1709 - MinusLogProbMetric: 42.1709 - val_loss: 42.3978 - val_MinusLogProbMetric: 42.3978 - lr: 1.6935e-08 - 92s/epoch - 471ms/step
Epoch 875/1000
2023-10-10 19:34:26.402 
Epoch 875/1000 
	 loss: 42.1688, MinusLogProbMetric: 42.1688, val_loss: 42.3971, val_MinusLogProbMetric: 42.3971

Epoch 875: val_loss did not improve from 42.39669
196/196 - 97s - loss: 42.1688 - MinusLogProbMetric: 42.1688 - val_loss: 42.3971 - val_MinusLogProbMetric: 42.3971 - lr: 1.6935e-08 - 97s/epoch - 495ms/step
Epoch 876/1000
2023-10-10 19:35:58.959 
Epoch 876/1000 
	 loss: 42.1701, MinusLogProbMetric: 42.1701, val_loss: 42.4010, val_MinusLogProbMetric: 42.4010

Epoch 876: val_loss did not improve from 42.39669
196/196 - 93s - loss: 42.1701 - MinusLogProbMetric: 42.1701 - val_loss: 42.4010 - val_MinusLogProbMetric: 42.4010 - lr: 1.6935e-08 - 93s/epoch - 472ms/step
Epoch 877/1000
2023-10-10 19:37:30.867 
Epoch 877/1000 
	 loss: 42.1692, MinusLogProbMetric: 42.1692, val_loss: 42.3997, val_MinusLogProbMetric: 42.3997

Epoch 877: val_loss did not improve from 42.39669
196/196 - 92s - loss: 42.1692 - MinusLogProbMetric: 42.1692 - val_loss: 42.3997 - val_MinusLogProbMetric: 42.3997 - lr: 1.6935e-08 - 92s/epoch - 469ms/step
Epoch 878/1000
2023-10-10 19:39:04.632 
Epoch 878/1000 
	 loss: 42.1685, MinusLogProbMetric: 42.1685, val_loss: 42.4003, val_MinusLogProbMetric: 42.4003

Epoch 878: val_loss did not improve from 42.39669
196/196 - 94s - loss: 42.1685 - MinusLogProbMetric: 42.1685 - val_loss: 42.4003 - val_MinusLogProbMetric: 42.4003 - lr: 1.6935e-08 - 94s/epoch - 478ms/step
Epoch 879/1000
2023-10-10 19:40:36.606 
Epoch 879/1000 
	 loss: 42.1671, MinusLogProbMetric: 42.1671, val_loss: 42.3954, val_MinusLogProbMetric: 42.3954

Epoch 879: val_loss improved from 42.39669 to 42.39540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 94s - loss: 42.1671 - MinusLogProbMetric: 42.1671 - val_loss: 42.3954 - val_MinusLogProbMetric: 42.3954 - lr: 1.6935e-08 - 94s/epoch - 479ms/step
Epoch 880/1000
2023-10-10 19:42:13.859 
Epoch 880/1000 
	 loss: 42.1687, MinusLogProbMetric: 42.1687, val_loss: 42.4018, val_MinusLogProbMetric: 42.4018

Epoch 880: val_loss did not improve from 42.39540
196/196 - 95s - loss: 42.1687 - MinusLogProbMetric: 42.1687 - val_loss: 42.4018 - val_MinusLogProbMetric: 42.4018 - lr: 1.6935e-08 - 95s/epoch - 487ms/step
Epoch 881/1000
2023-10-10 19:43:46.467 
Epoch 881/1000 
	 loss: 42.1708, MinusLogProbMetric: 42.1708, val_loss: 42.3998, val_MinusLogProbMetric: 42.3998

Epoch 881: val_loss did not improve from 42.39540
196/196 - 93s - loss: 42.1708 - MinusLogProbMetric: 42.1708 - val_loss: 42.3998 - val_MinusLogProbMetric: 42.3998 - lr: 1.6935e-08 - 93s/epoch - 472ms/step
Epoch 882/1000
2023-10-10 19:45:21.497 
Epoch 882/1000 
	 loss: 42.1698, MinusLogProbMetric: 42.1698, val_loss: 42.3965, val_MinusLogProbMetric: 42.3965

Epoch 882: val_loss did not improve from 42.39540
196/196 - 95s - loss: 42.1698 - MinusLogProbMetric: 42.1698 - val_loss: 42.3965 - val_MinusLogProbMetric: 42.3965 - lr: 1.6935e-08 - 95s/epoch - 485ms/step
Epoch 883/1000
2023-10-10 19:46:54.785 
Epoch 883/1000 
	 loss: 42.1699, MinusLogProbMetric: 42.1699, val_loss: 42.3955, val_MinusLogProbMetric: 42.3955

Epoch 883: val_loss did not improve from 42.39540
196/196 - 93s - loss: 42.1699 - MinusLogProbMetric: 42.1699 - val_loss: 42.3955 - val_MinusLogProbMetric: 42.3955 - lr: 1.6935e-08 - 93s/epoch - 476ms/step
Epoch 884/1000
2023-10-10 19:48:32.630 
Epoch 884/1000 
	 loss: 42.1701, MinusLogProbMetric: 42.1701, val_loss: 42.3976, val_MinusLogProbMetric: 42.3976

Epoch 884: val_loss did not improve from 42.39540
196/196 - 98s - loss: 42.1701 - MinusLogProbMetric: 42.1701 - val_loss: 42.3976 - val_MinusLogProbMetric: 42.3976 - lr: 1.6935e-08 - 98s/epoch - 499ms/step
Epoch 885/1000
2023-10-10 19:50:05.857 
Epoch 885/1000 
	 loss: 42.1698, MinusLogProbMetric: 42.1698, val_loss: 42.3972, val_MinusLogProbMetric: 42.3972

Epoch 885: val_loss did not improve from 42.39540
196/196 - 93s - loss: 42.1698 - MinusLogProbMetric: 42.1698 - val_loss: 42.3972 - val_MinusLogProbMetric: 42.3972 - lr: 1.6935e-08 - 93s/epoch - 476ms/step
Epoch 886/1000
2023-10-10 19:51:41.524 
Epoch 886/1000 
	 loss: 42.1682, MinusLogProbMetric: 42.1682, val_loss: 42.3952, val_MinusLogProbMetric: 42.3952

Epoch 886: val_loss improved from 42.39540 to 42.39522, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 96s - loss: 42.1682 - MinusLogProbMetric: 42.1682 - val_loss: 42.3952 - val_MinusLogProbMetric: 42.3952 - lr: 1.6935e-08 - 96s/epoch - 492ms/step
Epoch 887/1000
2023-10-10 19:53:20.676 
Epoch 887/1000 
	 loss: 42.1701, MinusLogProbMetric: 42.1701, val_loss: 42.3997, val_MinusLogProbMetric: 42.3997

Epoch 887: val_loss did not improve from 42.39522
196/196 - 98s - loss: 42.1701 - MinusLogProbMetric: 42.1701 - val_loss: 42.3997 - val_MinusLogProbMetric: 42.3997 - lr: 1.6935e-08 - 98s/epoch - 502ms/step
Epoch 888/1000
2023-10-10 19:54:55.509 
Epoch 888/1000 
	 loss: 42.1693, MinusLogProbMetric: 42.1693, val_loss: 42.3978, val_MinusLogProbMetric: 42.3978

Epoch 888: val_loss did not improve from 42.39522
196/196 - 95s - loss: 42.1693 - MinusLogProbMetric: 42.1693 - val_loss: 42.3978 - val_MinusLogProbMetric: 42.3978 - lr: 1.6935e-08 - 95s/epoch - 484ms/step
Epoch 889/1000
2023-10-10 19:56:30.470 
Epoch 889/1000 
	 loss: 42.1692, MinusLogProbMetric: 42.1692, val_loss: 42.3970, val_MinusLogProbMetric: 42.3970

Epoch 889: val_loss did not improve from 42.39522
196/196 - 95s - loss: 42.1692 - MinusLogProbMetric: 42.1692 - val_loss: 42.3970 - val_MinusLogProbMetric: 42.3970 - lr: 1.6935e-08 - 95s/epoch - 484ms/step
Epoch 890/1000
2023-10-10 19:58:01.090 
Epoch 890/1000 
	 loss: 42.1688, MinusLogProbMetric: 42.1688, val_loss: 42.3969, val_MinusLogProbMetric: 42.3969

Epoch 890: val_loss did not improve from 42.39522
196/196 - 91s - loss: 42.1688 - MinusLogProbMetric: 42.1688 - val_loss: 42.3969 - val_MinusLogProbMetric: 42.3969 - lr: 1.6935e-08 - 91s/epoch - 462ms/step
Epoch 891/1000
2023-10-10 19:59:35.765 
Epoch 891/1000 
	 loss: 42.1680, MinusLogProbMetric: 42.1680, val_loss: 42.3977, val_MinusLogProbMetric: 42.3977

Epoch 891: val_loss did not improve from 42.39522
196/196 - 95s - loss: 42.1680 - MinusLogProbMetric: 42.1680 - val_loss: 42.3977 - val_MinusLogProbMetric: 42.3977 - lr: 1.6935e-08 - 95s/epoch - 483ms/step
Epoch 892/1000
2023-10-10 20:01:08.148 
Epoch 892/1000 
	 loss: 42.1665, MinusLogProbMetric: 42.1665, val_loss: 42.3980, val_MinusLogProbMetric: 42.3980

Epoch 892: val_loss did not improve from 42.39522
196/196 - 92s - loss: 42.1665 - MinusLogProbMetric: 42.1665 - val_loss: 42.3980 - val_MinusLogProbMetric: 42.3980 - lr: 1.6935e-08 - 92s/epoch - 471ms/step
Epoch 893/1000
2023-10-10 20:02:43.110 
Epoch 893/1000 
	 loss: 42.1659, MinusLogProbMetric: 42.1659, val_loss: 42.3977, val_MinusLogProbMetric: 42.3977

Epoch 893: val_loss did not improve from 42.39522
196/196 - 95s - loss: 42.1659 - MinusLogProbMetric: 42.1659 - val_loss: 42.3977 - val_MinusLogProbMetric: 42.3977 - lr: 1.6935e-08 - 95s/epoch - 485ms/step
Epoch 894/1000
2023-10-10 20:04:18.118 
Epoch 894/1000 
	 loss: 42.1669, MinusLogProbMetric: 42.1669, val_loss: 42.3944, val_MinusLogProbMetric: 42.3944

Epoch 894: val_loss improved from 42.39522 to 42.39442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 99s - loss: 42.1669 - MinusLogProbMetric: 42.1669 - val_loss: 42.3944 - val_MinusLogProbMetric: 42.3944 - lr: 1.6935e-08 - 99s/epoch - 503ms/step
Epoch 895/1000
2023-10-10 20:05:53.693 
Epoch 895/1000 
	 loss: 42.1656, MinusLogProbMetric: 42.1656, val_loss: 42.3971, val_MinusLogProbMetric: 42.3971

Epoch 895: val_loss did not improve from 42.39442
196/196 - 92s - loss: 42.1656 - MinusLogProbMetric: 42.1656 - val_loss: 42.3971 - val_MinusLogProbMetric: 42.3971 - lr: 1.6935e-08 - 92s/epoch - 469ms/step
Epoch 896/1000
2023-10-10 20:07:24.613 
Epoch 896/1000 
	 loss: 42.1662, MinusLogProbMetric: 42.1662, val_loss: 42.3945, val_MinusLogProbMetric: 42.3945

Epoch 896: val_loss did not improve from 42.39442
196/196 - 91s - loss: 42.1662 - MinusLogProbMetric: 42.1662 - val_loss: 42.3945 - val_MinusLogProbMetric: 42.3945 - lr: 1.6935e-08 - 91s/epoch - 464ms/step
Epoch 897/1000
2023-10-10 20:08:57.308 
Epoch 897/1000 
	 loss: 42.1657, MinusLogProbMetric: 42.1657, val_loss: 42.3965, val_MinusLogProbMetric: 42.3965

Epoch 897: val_loss did not improve from 42.39442
196/196 - 93s - loss: 42.1657 - MinusLogProbMetric: 42.1657 - val_loss: 42.3965 - val_MinusLogProbMetric: 42.3965 - lr: 1.6935e-08 - 93s/epoch - 473ms/step
Epoch 898/1000
2023-10-10 20:10:29.329 
Epoch 898/1000 
	 loss: 42.1679, MinusLogProbMetric: 42.1679, val_loss: 42.3953, val_MinusLogProbMetric: 42.3953

Epoch 898: val_loss did not improve from 42.39442
196/196 - 92s - loss: 42.1679 - MinusLogProbMetric: 42.1679 - val_loss: 42.3953 - val_MinusLogProbMetric: 42.3953 - lr: 1.6935e-08 - 92s/epoch - 470ms/step
Epoch 899/1000
2023-10-10 20:12:01.723 
Epoch 899/1000 
	 loss: 42.1696, MinusLogProbMetric: 42.1696, val_loss: 42.3971, val_MinusLogProbMetric: 42.3971

Epoch 899: val_loss did not improve from 42.39442
196/196 - 92s - loss: 42.1696 - MinusLogProbMetric: 42.1696 - val_loss: 42.3971 - val_MinusLogProbMetric: 42.3971 - lr: 1.6935e-08 - 92s/epoch - 471ms/step
Epoch 900/1000
2023-10-10 20:13:36.551 
Epoch 900/1000 
	 loss: 42.1692, MinusLogProbMetric: 42.1692, val_loss: 42.3956, val_MinusLogProbMetric: 42.3956

Epoch 900: val_loss did not improve from 42.39442
196/196 - 95s - loss: 42.1692 - MinusLogProbMetric: 42.1692 - val_loss: 42.3956 - val_MinusLogProbMetric: 42.3956 - lr: 1.6935e-08 - 95s/epoch - 484ms/step
Epoch 901/1000
2023-10-10 20:15:06.629 
Epoch 901/1000 
	 loss: 42.1668, MinusLogProbMetric: 42.1668, val_loss: 42.3976, val_MinusLogProbMetric: 42.3976

Epoch 901: val_loss did not improve from 42.39442
196/196 - 90s - loss: 42.1668 - MinusLogProbMetric: 42.1668 - val_loss: 42.3976 - val_MinusLogProbMetric: 42.3976 - lr: 1.6935e-08 - 90s/epoch - 460ms/step
Epoch 902/1000
2023-10-10 20:16:37.272 
Epoch 902/1000 
	 loss: 42.1674, MinusLogProbMetric: 42.1674, val_loss: 42.3970, val_MinusLogProbMetric: 42.3970

Epoch 902: val_loss did not improve from 42.39442
196/196 - 91s - loss: 42.1674 - MinusLogProbMetric: 42.1674 - val_loss: 42.3970 - val_MinusLogProbMetric: 42.3970 - lr: 1.6935e-08 - 91s/epoch - 462ms/step
Epoch 903/1000
2023-10-10 20:18:06.532 
Epoch 903/1000 
	 loss: 42.1694, MinusLogProbMetric: 42.1694, val_loss: 42.3996, val_MinusLogProbMetric: 42.3996

Epoch 903: val_loss did not improve from 42.39442
196/196 - 89s - loss: 42.1694 - MinusLogProbMetric: 42.1694 - val_loss: 42.3996 - val_MinusLogProbMetric: 42.3996 - lr: 1.6935e-08 - 89s/epoch - 455ms/step
Epoch 904/1000
2023-10-10 20:19:39.349 
Epoch 904/1000 
	 loss: 42.1670, MinusLogProbMetric: 42.1670, val_loss: 42.3986, val_MinusLogProbMetric: 42.3986

Epoch 904: val_loss did not improve from 42.39442
196/196 - 93s - loss: 42.1670 - MinusLogProbMetric: 42.1670 - val_loss: 42.3986 - val_MinusLogProbMetric: 42.3986 - lr: 1.6935e-08 - 93s/epoch - 473ms/step
Epoch 905/1000
2023-10-10 20:21:11.236 
Epoch 905/1000 
	 loss: 42.1660, MinusLogProbMetric: 42.1660, val_loss: 42.3969, val_MinusLogProbMetric: 42.3969

Epoch 905: val_loss did not improve from 42.39442
196/196 - 92s - loss: 42.1660 - MinusLogProbMetric: 42.1660 - val_loss: 42.3969 - val_MinusLogProbMetric: 42.3969 - lr: 1.6935e-08 - 92s/epoch - 469ms/step
Epoch 906/1000
2023-10-10 20:22:42.864 
Epoch 906/1000 
	 loss: 42.1667, MinusLogProbMetric: 42.1667, val_loss: 42.3956, val_MinusLogProbMetric: 42.3956

Epoch 906: val_loss did not improve from 42.39442
196/196 - 92s - loss: 42.1667 - MinusLogProbMetric: 42.1667 - val_loss: 42.3956 - val_MinusLogProbMetric: 42.3956 - lr: 1.6935e-08 - 92s/epoch - 467ms/step
Epoch 907/1000
2023-10-10 20:24:17.928 
Epoch 907/1000 
	 loss: 42.1644, MinusLogProbMetric: 42.1644, val_loss: 42.3944, val_MinusLogProbMetric: 42.3944

Epoch 907: val_loss did not improve from 42.39442
196/196 - 95s - loss: 42.1644 - MinusLogProbMetric: 42.1644 - val_loss: 42.3944 - val_MinusLogProbMetric: 42.3944 - lr: 1.6935e-08 - 95s/epoch - 485ms/step
Epoch 908/1000
2023-10-10 20:25:49.121 
Epoch 908/1000 
	 loss: 42.1649, MinusLogProbMetric: 42.1649, val_loss: 42.3948, val_MinusLogProbMetric: 42.3948

Epoch 908: val_loss did not improve from 42.39442
196/196 - 91s - loss: 42.1649 - MinusLogProbMetric: 42.1649 - val_loss: 42.3948 - val_MinusLogProbMetric: 42.3948 - lr: 1.6935e-08 - 91s/epoch - 465ms/step
Epoch 909/1000
2023-10-10 20:27:21.150 
Epoch 909/1000 
	 loss: 42.1661, MinusLogProbMetric: 42.1661, val_loss: 42.3926, val_MinusLogProbMetric: 42.3926

Epoch 909: val_loss improved from 42.39442 to 42.39264, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 93s - loss: 42.1661 - MinusLogProbMetric: 42.1661 - val_loss: 42.3926 - val_MinusLogProbMetric: 42.3926 - lr: 1.6935e-08 - 93s/epoch - 474ms/step
Epoch 910/1000
2023-10-10 20:28:57.516 
Epoch 910/1000 
	 loss: 42.1658, MinusLogProbMetric: 42.1658, val_loss: 42.3930, val_MinusLogProbMetric: 42.3930

Epoch 910: val_loss did not improve from 42.39264
196/196 - 95s - loss: 42.1658 - MinusLogProbMetric: 42.1658 - val_loss: 42.3930 - val_MinusLogProbMetric: 42.3930 - lr: 1.6935e-08 - 95s/epoch - 487ms/step
Epoch 911/1000
2023-10-10 20:30:27.890 
Epoch 911/1000 
	 loss: 42.1658, MinusLogProbMetric: 42.1658, val_loss: 42.3958, val_MinusLogProbMetric: 42.3958

Epoch 911: val_loss did not improve from 42.39264
196/196 - 90s - loss: 42.1658 - MinusLogProbMetric: 42.1658 - val_loss: 42.3958 - val_MinusLogProbMetric: 42.3958 - lr: 1.6935e-08 - 90s/epoch - 461ms/step
Epoch 912/1000
2023-10-10 20:32:00.158 
Epoch 912/1000 
	 loss: 42.1661, MinusLogProbMetric: 42.1661, val_loss: 42.3932, val_MinusLogProbMetric: 42.3932

Epoch 912: val_loss did not improve from 42.39264
196/196 - 92s - loss: 42.1661 - MinusLogProbMetric: 42.1661 - val_loss: 42.3932 - val_MinusLogProbMetric: 42.3932 - lr: 1.6935e-08 - 92s/epoch - 471ms/step
Epoch 913/1000
2023-10-10 20:33:31.335 
Epoch 913/1000 
	 loss: 42.1650, MinusLogProbMetric: 42.1650, val_loss: 42.3975, val_MinusLogProbMetric: 42.3975

Epoch 913: val_loss did not improve from 42.39264
196/196 - 91s - loss: 42.1650 - MinusLogProbMetric: 42.1650 - val_loss: 42.3975 - val_MinusLogProbMetric: 42.3975 - lr: 1.6935e-08 - 91s/epoch - 465ms/step
Epoch 914/1000
2023-10-10 20:35:02.186 
Epoch 914/1000 
	 loss: 42.1679, MinusLogProbMetric: 42.1679, val_loss: 42.3992, val_MinusLogProbMetric: 42.3992

Epoch 914: val_loss did not improve from 42.39264
196/196 - 91s - loss: 42.1679 - MinusLogProbMetric: 42.1679 - val_loss: 42.3992 - val_MinusLogProbMetric: 42.3992 - lr: 1.6935e-08 - 91s/epoch - 464ms/step
Epoch 915/1000
2023-10-10 20:36:32.421 
Epoch 915/1000 
	 loss: 42.1683, MinusLogProbMetric: 42.1683, val_loss: 42.3976, val_MinusLogProbMetric: 42.3976

Epoch 915: val_loss did not improve from 42.39264
196/196 - 90s - loss: 42.1683 - MinusLogProbMetric: 42.1683 - val_loss: 42.3976 - val_MinusLogProbMetric: 42.3976 - lr: 1.6935e-08 - 90s/epoch - 460ms/step
Epoch 916/1000
2023-10-10 20:38:03.354 
Epoch 916/1000 
	 loss: 42.1691, MinusLogProbMetric: 42.1691, val_loss: 42.3973, val_MinusLogProbMetric: 42.3973

Epoch 916: val_loss did not improve from 42.39264
196/196 - 91s - loss: 42.1691 - MinusLogProbMetric: 42.1691 - val_loss: 42.3973 - val_MinusLogProbMetric: 42.3973 - lr: 1.6935e-08 - 91s/epoch - 464ms/step
Epoch 917/1000
2023-10-10 20:39:33.913 
Epoch 917/1000 
	 loss: 42.1670, MinusLogProbMetric: 42.1670, val_loss: 42.3971, val_MinusLogProbMetric: 42.3971

Epoch 917: val_loss did not improve from 42.39264
196/196 - 91s - loss: 42.1670 - MinusLogProbMetric: 42.1670 - val_loss: 42.3971 - val_MinusLogProbMetric: 42.3971 - lr: 1.6935e-08 - 91s/epoch - 462ms/step
Epoch 918/1000
2023-10-10 20:41:06.393 
Epoch 918/1000 
	 loss: 42.1662, MinusLogProbMetric: 42.1662, val_loss: 42.3966, val_MinusLogProbMetric: 42.3966

Epoch 918: val_loss did not improve from 42.39264
196/196 - 92s - loss: 42.1662 - MinusLogProbMetric: 42.1662 - val_loss: 42.3966 - val_MinusLogProbMetric: 42.3966 - lr: 1.6935e-08 - 92s/epoch - 472ms/step
Epoch 919/1000
2023-10-10 20:42:36.094 
Epoch 919/1000 
	 loss: 42.1660, MinusLogProbMetric: 42.1660, val_loss: 42.3955, val_MinusLogProbMetric: 42.3955

Epoch 919: val_loss did not improve from 42.39264
196/196 - 90s - loss: 42.1660 - MinusLogProbMetric: 42.1660 - val_loss: 42.3955 - val_MinusLogProbMetric: 42.3955 - lr: 1.6935e-08 - 90s/epoch - 457ms/step
Epoch 920/1000
2023-10-10 20:44:07.694 
Epoch 920/1000 
	 loss: 42.1660, MinusLogProbMetric: 42.1660, val_loss: 42.3938, val_MinusLogProbMetric: 42.3938

Epoch 920: val_loss did not improve from 42.39264
196/196 - 92s - loss: 42.1660 - MinusLogProbMetric: 42.1660 - val_loss: 42.3938 - val_MinusLogProbMetric: 42.3938 - lr: 1.6935e-08 - 92s/epoch - 467ms/step
Epoch 921/1000
2023-10-10 20:45:44.060 
Epoch 921/1000 
	 loss: 42.1645, MinusLogProbMetric: 42.1645, val_loss: 42.3945, val_MinusLogProbMetric: 42.3945

Epoch 921: val_loss did not improve from 42.39264
196/196 - 96s - loss: 42.1645 - MinusLogProbMetric: 42.1645 - val_loss: 42.3945 - val_MinusLogProbMetric: 42.3945 - lr: 1.6935e-08 - 96s/epoch - 492ms/step
Epoch 922/1000
2023-10-10 20:47:15.057 
Epoch 922/1000 
	 loss: 42.1653, MinusLogProbMetric: 42.1653, val_loss: 42.3948, val_MinusLogProbMetric: 42.3948

Epoch 922: val_loss did not improve from 42.39264
196/196 - 91s - loss: 42.1653 - MinusLogProbMetric: 42.1653 - val_loss: 42.3948 - val_MinusLogProbMetric: 42.3948 - lr: 1.6935e-08 - 91s/epoch - 464ms/step
Epoch 923/1000
2023-10-10 20:48:46.994 
Epoch 923/1000 
	 loss: 42.1649, MinusLogProbMetric: 42.1649, val_loss: 42.3941, val_MinusLogProbMetric: 42.3941

Epoch 923: val_loss did not improve from 42.39264
196/196 - 92s - loss: 42.1649 - MinusLogProbMetric: 42.1649 - val_loss: 42.3941 - val_MinusLogProbMetric: 42.3941 - lr: 1.6935e-08 - 92s/epoch - 469ms/step
Epoch 924/1000
2023-10-10 20:50:18.969 
Epoch 924/1000 
	 loss: 42.1656, MinusLogProbMetric: 42.1656, val_loss: 42.3934, val_MinusLogProbMetric: 42.3934

Epoch 924: val_loss did not improve from 42.39264
196/196 - 92s - loss: 42.1656 - MinusLogProbMetric: 42.1656 - val_loss: 42.3934 - val_MinusLogProbMetric: 42.3934 - lr: 1.6935e-08 - 92s/epoch - 469ms/step
Epoch 925/1000
2023-10-10 20:51:50.725 
Epoch 925/1000 
	 loss: 42.1640, MinusLogProbMetric: 42.1640, val_loss: 42.3931, val_MinusLogProbMetric: 42.3931

Epoch 925: val_loss did not improve from 42.39264
196/196 - 92s - loss: 42.1640 - MinusLogProbMetric: 42.1640 - val_loss: 42.3931 - val_MinusLogProbMetric: 42.3931 - lr: 1.6935e-08 - 92s/epoch - 468ms/step
Epoch 926/1000
2023-10-10 20:53:22.952 
Epoch 926/1000 
	 loss: 42.1654, MinusLogProbMetric: 42.1654, val_loss: 42.3944, val_MinusLogProbMetric: 42.3944

Epoch 926: val_loss did not improve from 42.39264
196/196 - 92s - loss: 42.1654 - MinusLogProbMetric: 42.1654 - val_loss: 42.3944 - val_MinusLogProbMetric: 42.3944 - lr: 1.6935e-08 - 92s/epoch - 471ms/step
Epoch 927/1000
2023-10-10 20:54:53.864 
Epoch 927/1000 
	 loss: 42.1658, MinusLogProbMetric: 42.1658, val_loss: 42.3909, val_MinusLogProbMetric: 42.3909

Epoch 927: val_loss improved from 42.39264 to 42.39086, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 92s - loss: 42.1658 - MinusLogProbMetric: 42.1658 - val_loss: 42.3909 - val_MinusLogProbMetric: 42.3909 - lr: 1.6935e-08 - 92s/epoch - 471ms/step
Epoch 928/1000
2023-10-10 20:56:28.546 
Epoch 928/1000 
	 loss: 42.1645, MinusLogProbMetric: 42.1645, val_loss: 42.3921, val_MinusLogProbMetric: 42.3921

Epoch 928: val_loss did not improve from 42.39086
196/196 - 93s - loss: 42.1645 - MinusLogProbMetric: 42.1645 - val_loss: 42.3921 - val_MinusLogProbMetric: 42.3921 - lr: 1.6935e-08 - 93s/epoch - 476ms/step
Epoch 929/1000
2023-10-10 20:58:02.415 
Epoch 929/1000 
	 loss: 42.1646, MinusLogProbMetric: 42.1646, val_loss: 42.3929, val_MinusLogProbMetric: 42.3929

Epoch 929: val_loss did not improve from 42.39086
196/196 - 94s - loss: 42.1646 - MinusLogProbMetric: 42.1646 - val_loss: 42.3929 - val_MinusLogProbMetric: 42.3929 - lr: 1.6935e-08 - 94s/epoch - 479ms/step
Epoch 930/1000
2023-10-10 20:59:35.552 
Epoch 930/1000 
	 loss: 42.1650, MinusLogProbMetric: 42.1650, val_loss: 42.3911, val_MinusLogProbMetric: 42.3911

Epoch 930: val_loss did not improve from 42.39086
196/196 - 93s - loss: 42.1650 - MinusLogProbMetric: 42.1650 - val_loss: 42.3911 - val_MinusLogProbMetric: 42.3911 - lr: 1.6935e-08 - 93s/epoch - 475ms/step
Epoch 931/1000
2023-10-10 21:01:07.927 
Epoch 931/1000 
	 loss: 42.1636, MinusLogProbMetric: 42.1636, val_loss: 42.3921, val_MinusLogProbMetric: 42.3921

Epoch 931: val_loss did not improve from 42.39086
196/196 - 92s - loss: 42.1636 - MinusLogProbMetric: 42.1636 - val_loss: 42.3921 - val_MinusLogProbMetric: 42.3921 - lr: 1.6935e-08 - 92s/epoch - 471ms/step
Epoch 932/1000
2023-10-10 21:02:40.056 
Epoch 932/1000 
	 loss: 42.1625, MinusLogProbMetric: 42.1625, val_loss: 42.3910, val_MinusLogProbMetric: 42.3910

Epoch 932: val_loss did not improve from 42.39086
196/196 - 92s - loss: 42.1625 - MinusLogProbMetric: 42.1625 - val_loss: 42.3910 - val_MinusLogProbMetric: 42.3910 - lr: 1.6935e-08 - 92s/epoch - 470ms/step
Epoch 933/1000
2023-10-10 21:04:13.245 
Epoch 933/1000 
	 loss: 42.1616, MinusLogProbMetric: 42.1616, val_loss: 42.3905, val_MinusLogProbMetric: 42.3905

Epoch 933: val_loss improved from 42.39086 to 42.39054, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 96s - loss: 42.1616 - MinusLogProbMetric: 42.1616 - val_loss: 42.3905 - val_MinusLogProbMetric: 42.3905 - lr: 1.6935e-08 - 96s/epoch - 492ms/step
Epoch 934/1000
2023-10-10 21:05:49.839 
Epoch 934/1000 
	 loss: 42.1626, MinusLogProbMetric: 42.1626, val_loss: 42.3943, val_MinusLogProbMetric: 42.3943

Epoch 934: val_loss did not improve from 42.39054
196/196 - 93s - loss: 42.1626 - MinusLogProbMetric: 42.1626 - val_loss: 42.3943 - val_MinusLogProbMetric: 42.3943 - lr: 1.6935e-08 - 93s/epoch - 477ms/step
Epoch 935/1000
2023-10-10 21:07:23.209 
Epoch 935/1000 
	 loss: 42.1677, MinusLogProbMetric: 42.1677, val_loss: 42.3914, val_MinusLogProbMetric: 42.3914

Epoch 935: val_loss did not improve from 42.39054
196/196 - 93s - loss: 42.1677 - MinusLogProbMetric: 42.1677 - val_loss: 42.3914 - val_MinusLogProbMetric: 42.3914 - lr: 1.6935e-08 - 93s/epoch - 476ms/step
Epoch 936/1000
2023-10-10 21:08:59.235 
Epoch 936/1000 
	 loss: 42.1631, MinusLogProbMetric: 42.1631, val_loss: 42.3932, val_MinusLogProbMetric: 42.3932

Epoch 936: val_loss did not improve from 42.39054
196/196 - 96s - loss: 42.1631 - MinusLogProbMetric: 42.1631 - val_loss: 42.3932 - val_MinusLogProbMetric: 42.3932 - lr: 1.6935e-08 - 96s/epoch - 490ms/step
Epoch 937/1000
2023-10-10 21:10:31.233 
Epoch 937/1000 
	 loss: 42.1671, MinusLogProbMetric: 42.1671, val_loss: 42.3949, val_MinusLogProbMetric: 42.3949

Epoch 937: val_loss did not improve from 42.39054
196/196 - 92s - loss: 42.1671 - MinusLogProbMetric: 42.1671 - val_loss: 42.3949 - val_MinusLogProbMetric: 42.3949 - lr: 1.6935e-08 - 92s/epoch - 469ms/step
Epoch 938/1000
2023-10-10 21:12:04.909 
Epoch 938/1000 
	 loss: 42.1640, MinusLogProbMetric: 42.1640, val_loss: 42.3933, val_MinusLogProbMetric: 42.3933

Epoch 938: val_loss did not improve from 42.39054
196/196 - 94s - loss: 42.1640 - MinusLogProbMetric: 42.1640 - val_loss: 42.3933 - val_MinusLogProbMetric: 42.3933 - lr: 1.6935e-08 - 94s/epoch - 478ms/step
Epoch 939/1000
2023-10-10 21:13:36.137 
Epoch 939/1000 
	 loss: 42.1634, MinusLogProbMetric: 42.1634, val_loss: 42.3938, val_MinusLogProbMetric: 42.3938

Epoch 939: val_loss did not improve from 42.39054
196/196 - 91s - loss: 42.1634 - MinusLogProbMetric: 42.1634 - val_loss: 42.3938 - val_MinusLogProbMetric: 42.3938 - lr: 1.6935e-08 - 91s/epoch - 465ms/step
Epoch 940/1000
2023-10-10 21:15:07.120 
Epoch 940/1000 
	 loss: 42.1651, MinusLogProbMetric: 42.1651, val_loss: 42.3929, val_MinusLogProbMetric: 42.3929

Epoch 940: val_loss did not improve from 42.39054
196/196 - 91s - loss: 42.1651 - MinusLogProbMetric: 42.1651 - val_loss: 42.3929 - val_MinusLogProbMetric: 42.3929 - lr: 1.6935e-08 - 91s/epoch - 464ms/step
Epoch 941/1000
2023-10-10 21:16:40.644 
Epoch 941/1000 
	 loss: 42.1643, MinusLogProbMetric: 42.1643, val_loss: 42.3944, val_MinusLogProbMetric: 42.3944

Epoch 941: val_loss did not improve from 42.39054
196/196 - 94s - loss: 42.1643 - MinusLogProbMetric: 42.1643 - val_loss: 42.3944 - val_MinusLogProbMetric: 42.3944 - lr: 1.6935e-08 - 94s/epoch - 477ms/step
Epoch 942/1000
2023-10-10 21:18:13.718 
Epoch 942/1000 
	 loss: 42.1615, MinusLogProbMetric: 42.1615, val_loss: 42.3935, val_MinusLogProbMetric: 42.3935

Epoch 942: val_loss did not improve from 42.39054
196/196 - 93s - loss: 42.1615 - MinusLogProbMetric: 42.1615 - val_loss: 42.3935 - val_MinusLogProbMetric: 42.3935 - lr: 1.6935e-08 - 93s/epoch - 475ms/step
Epoch 943/1000
2023-10-10 21:19:48.062 
Epoch 943/1000 
	 loss: 42.1619, MinusLogProbMetric: 42.1619, val_loss: 42.3946, val_MinusLogProbMetric: 42.3946

Epoch 943: val_loss did not improve from 42.39054
196/196 - 94s - loss: 42.1619 - MinusLogProbMetric: 42.1619 - val_loss: 42.3946 - val_MinusLogProbMetric: 42.3946 - lr: 1.6935e-08 - 94s/epoch - 481ms/step
Epoch 944/1000
2023-10-10 21:21:20.392 
Epoch 944/1000 
	 loss: 42.1621, MinusLogProbMetric: 42.1621, val_loss: 42.3934, val_MinusLogProbMetric: 42.3934

Epoch 944: val_loss did not improve from 42.39054
196/196 - 92s - loss: 42.1621 - MinusLogProbMetric: 42.1621 - val_loss: 42.3934 - val_MinusLogProbMetric: 42.3934 - lr: 1.6935e-08 - 92s/epoch - 471ms/step
Epoch 945/1000
2023-10-10 21:22:54.619 
Epoch 945/1000 
	 loss: 42.1617, MinusLogProbMetric: 42.1617, val_loss: 42.3958, val_MinusLogProbMetric: 42.3958

Epoch 945: val_loss did not improve from 42.39054
196/196 - 94s - loss: 42.1617 - MinusLogProbMetric: 42.1617 - val_loss: 42.3958 - val_MinusLogProbMetric: 42.3958 - lr: 1.6935e-08 - 94s/epoch - 481ms/step
Epoch 946/1000
2023-10-10 21:24:26.112 
Epoch 946/1000 
	 loss: 42.1622, MinusLogProbMetric: 42.1622, val_loss: 42.3934, val_MinusLogProbMetric: 42.3934

Epoch 946: val_loss did not improve from 42.39054
196/196 - 92s - loss: 42.1622 - MinusLogProbMetric: 42.1622 - val_loss: 42.3934 - val_MinusLogProbMetric: 42.3934 - lr: 1.6935e-08 - 92s/epoch - 467ms/step
Epoch 947/1000
2023-10-10 21:25:59.993 
Epoch 947/1000 
	 loss: 42.1639, MinusLogProbMetric: 42.1639, val_loss: 42.3950, val_MinusLogProbMetric: 42.3950

Epoch 947: val_loss did not improve from 42.39054
196/196 - 94s - loss: 42.1639 - MinusLogProbMetric: 42.1639 - val_loss: 42.3950 - val_MinusLogProbMetric: 42.3950 - lr: 1.6935e-08 - 94s/epoch - 479ms/step
Epoch 948/1000
2023-10-10 21:27:32.046 
Epoch 948/1000 
	 loss: 42.1614, MinusLogProbMetric: 42.1614, val_loss: 42.3977, val_MinusLogProbMetric: 42.3977

Epoch 948: val_loss did not improve from 42.39054
196/196 - 92s - loss: 42.1614 - MinusLogProbMetric: 42.1614 - val_loss: 42.3977 - val_MinusLogProbMetric: 42.3977 - lr: 1.6935e-08 - 92s/epoch - 470ms/step
Epoch 949/1000
2023-10-10 21:29:04.987 
Epoch 949/1000 
	 loss: 42.1630, MinusLogProbMetric: 42.1630, val_loss: 42.3945, val_MinusLogProbMetric: 42.3945

Epoch 949: val_loss did not improve from 42.39054
196/196 - 93s - loss: 42.1630 - MinusLogProbMetric: 42.1630 - val_loss: 42.3945 - val_MinusLogProbMetric: 42.3945 - lr: 1.6935e-08 - 93s/epoch - 474ms/step
Epoch 950/1000
2023-10-10 21:30:38.115 
Epoch 950/1000 
	 loss: 42.1619, MinusLogProbMetric: 42.1619, val_loss: 42.3928, val_MinusLogProbMetric: 42.3928

Epoch 950: val_loss did not improve from 42.39054
196/196 - 93s - loss: 42.1619 - MinusLogProbMetric: 42.1619 - val_loss: 42.3928 - val_MinusLogProbMetric: 42.3928 - lr: 1.6935e-08 - 93s/epoch - 475ms/step
Epoch 951/1000
2023-10-10 21:32:10.501 
Epoch 951/1000 
	 loss: 42.1613, MinusLogProbMetric: 42.1613, val_loss: 42.3936, val_MinusLogProbMetric: 42.3936

Epoch 951: val_loss did not improve from 42.39054
196/196 - 92s - loss: 42.1613 - MinusLogProbMetric: 42.1613 - val_loss: 42.3936 - val_MinusLogProbMetric: 42.3936 - lr: 1.6935e-08 - 92s/epoch - 471ms/step
Epoch 952/1000
2023-10-10 21:33:41.203 
Epoch 952/1000 
	 loss: 42.1620, MinusLogProbMetric: 42.1620, val_loss: 42.3911, val_MinusLogProbMetric: 42.3911

Epoch 952: val_loss did not improve from 42.39054
196/196 - 91s - loss: 42.1620 - MinusLogProbMetric: 42.1620 - val_loss: 42.3911 - val_MinusLogProbMetric: 42.3911 - lr: 1.6935e-08 - 91s/epoch - 463ms/step
Epoch 953/1000
2023-10-10 21:35:12.593 
Epoch 953/1000 
	 loss: 42.1620, MinusLogProbMetric: 42.1620, val_loss: 42.3937, val_MinusLogProbMetric: 42.3937

Epoch 953: val_loss did not improve from 42.39054
196/196 - 91s - loss: 42.1620 - MinusLogProbMetric: 42.1620 - val_loss: 42.3937 - val_MinusLogProbMetric: 42.3937 - lr: 1.6935e-08 - 91s/epoch - 466ms/step
Epoch 954/1000
2023-10-10 21:36:43.161 
Epoch 954/1000 
	 loss: 42.1626, MinusLogProbMetric: 42.1626, val_loss: 42.3921, val_MinusLogProbMetric: 42.3921

Epoch 954: val_loss did not improve from 42.39054
196/196 - 91s - loss: 42.1626 - MinusLogProbMetric: 42.1626 - val_loss: 42.3921 - val_MinusLogProbMetric: 42.3921 - lr: 1.6935e-08 - 91s/epoch - 462ms/step
Epoch 955/1000
2023-10-10 21:38:12.545 
Epoch 955/1000 
	 loss: 42.1628, MinusLogProbMetric: 42.1628, val_loss: 42.3950, val_MinusLogProbMetric: 42.3950

Epoch 955: val_loss did not improve from 42.39054
196/196 - 89s - loss: 42.1628 - MinusLogProbMetric: 42.1628 - val_loss: 42.3950 - val_MinusLogProbMetric: 42.3950 - lr: 1.6935e-08 - 89s/epoch - 456ms/step
Epoch 956/1000
2023-10-10 21:39:44.098 
Epoch 956/1000 
	 loss: 42.1634, MinusLogProbMetric: 42.1634, val_loss: 42.3987, val_MinusLogProbMetric: 42.3987

Epoch 956: val_loss did not improve from 42.39054
196/196 - 92s - loss: 42.1634 - MinusLogProbMetric: 42.1634 - val_loss: 42.3987 - val_MinusLogProbMetric: 42.3987 - lr: 1.6935e-08 - 92s/epoch - 467ms/step
Epoch 957/1000
2023-10-10 21:41:15.970 
Epoch 957/1000 
	 loss: 42.1629, MinusLogProbMetric: 42.1629, val_loss: 42.3951, val_MinusLogProbMetric: 42.3951

Epoch 957: val_loss did not improve from 42.39054
196/196 - 92s - loss: 42.1629 - MinusLogProbMetric: 42.1629 - val_loss: 42.3951 - val_MinusLogProbMetric: 42.3951 - lr: 1.6935e-08 - 92s/epoch - 469ms/step
Epoch 958/1000
2023-10-10 21:42:47.018 
Epoch 958/1000 
	 loss: 42.1623, MinusLogProbMetric: 42.1623, val_loss: 42.3926, val_MinusLogProbMetric: 42.3926

Epoch 958: val_loss did not improve from 42.39054
196/196 - 91s - loss: 42.1623 - MinusLogProbMetric: 42.1623 - val_loss: 42.3926 - val_MinusLogProbMetric: 42.3926 - lr: 1.6935e-08 - 91s/epoch - 465ms/step
Epoch 959/1000
2023-10-10 21:44:19.505 
Epoch 959/1000 
	 loss: 42.1615, MinusLogProbMetric: 42.1615, val_loss: 42.3932, val_MinusLogProbMetric: 42.3932

Epoch 959: val_loss did not improve from 42.39054
196/196 - 92s - loss: 42.1615 - MinusLogProbMetric: 42.1615 - val_loss: 42.3932 - val_MinusLogProbMetric: 42.3932 - lr: 1.6935e-08 - 92s/epoch - 472ms/step
Epoch 960/1000
2023-10-10 21:45:50.490 
Epoch 960/1000 
	 loss: 42.1615, MinusLogProbMetric: 42.1615, val_loss: 42.3945, val_MinusLogProbMetric: 42.3945

Epoch 960: val_loss did not improve from 42.39054
196/196 - 91s - loss: 42.1615 - MinusLogProbMetric: 42.1615 - val_loss: 42.3945 - val_MinusLogProbMetric: 42.3945 - lr: 1.6935e-08 - 91s/epoch - 464ms/step
Epoch 961/1000
2023-10-10 21:47:21.642 
Epoch 961/1000 
	 loss: 42.1616, MinusLogProbMetric: 42.1616, val_loss: 42.3928, val_MinusLogProbMetric: 42.3928

Epoch 961: val_loss did not improve from 42.39054
196/196 - 91s - loss: 42.1616 - MinusLogProbMetric: 42.1616 - val_loss: 42.3928 - val_MinusLogProbMetric: 42.3928 - lr: 1.6935e-08 - 91s/epoch - 465ms/step
Epoch 962/1000
2023-10-10 21:48:52.542 
Epoch 962/1000 
	 loss: 42.1609, MinusLogProbMetric: 42.1609, val_loss: 42.3942, val_MinusLogProbMetric: 42.3942

Epoch 962: val_loss did not improve from 42.39054
196/196 - 91s - loss: 42.1609 - MinusLogProbMetric: 42.1609 - val_loss: 42.3942 - val_MinusLogProbMetric: 42.3942 - lr: 1.6935e-08 - 91s/epoch - 464ms/step
Epoch 963/1000
2023-10-10 21:50:24.073 
Epoch 963/1000 
	 loss: 42.1604, MinusLogProbMetric: 42.1604, val_loss: 42.3939, val_MinusLogProbMetric: 42.3939

Epoch 963: val_loss did not improve from 42.39054
196/196 - 92s - loss: 42.1604 - MinusLogProbMetric: 42.1604 - val_loss: 42.3939 - val_MinusLogProbMetric: 42.3939 - lr: 1.6935e-08 - 92s/epoch - 467ms/step
Epoch 964/1000
2023-10-10 21:51:56.309 
Epoch 964/1000 
	 loss: 42.1584, MinusLogProbMetric: 42.1584, val_loss: 42.3907, val_MinusLogProbMetric: 42.3907

Epoch 964: val_loss did not improve from 42.39054
196/196 - 92s - loss: 42.1584 - MinusLogProbMetric: 42.1584 - val_loss: 42.3907 - val_MinusLogProbMetric: 42.3907 - lr: 1.6935e-08 - 92s/epoch - 471ms/step
Epoch 965/1000
2023-10-10 21:53:28.564 
Epoch 965/1000 
	 loss: 42.1583, MinusLogProbMetric: 42.1583, val_loss: 42.3910, val_MinusLogProbMetric: 42.3910

Epoch 965: val_loss did not improve from 42.39054
196/196 - 92s - loss: 42.1583 - MinusLogProbMetric: 42.1583 - val_loss: 42.3910 - val_MinusLogProbMetric: 42.3910 - lr: 1.6935e-08 - 92s/epoch - 471ms/step
Epoch 966/1000
2023-10-10 21:54:57.903 
Epoch 966/1000 
	 loss: 42.1576, MinusLogProbMetric: 42.1576, val_loss: 42.3892, val_MinusLogProbMetric: 42.3892

Epoch 966: val_loss improved from 42.39054 to 42.38921, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 93s - loss: 42.1576 - MinusLogProbMetric: 42.1576 - val_loss: 42.3892 - val_MinusLogProbMetric: 42.3892 - lr: 1.6935e-08 - 93s/epoch - 476ms/step
Epoch 967/1000
2023-10-10 21:56:30.893 
Epoch 967/1000 
	 loss: 42.1597, MinusLogProbMetric: 42.1597, val_loss: 42.3889, val_MinusLogProbMetric: 42.3889

Epoch 967: val_loss improved from 42.38921 to 42.38888, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 92s - loss: 42.1597 - MinusLogProbMetric: 42.1597 - val_loss: 42.3889 - val_MinusLogProbMetric: 42.3889 - lr: 1.6935e-08 - 92s/epoch - 469ms/step
Epoch 968/1000
2023-10-10 21:58:07.385 
Epoch 968/1000 
	 loss: 42.1573, MinusLogProbMetric: 42.1573, val_loss: 42.3919, val_MinusLogProbMetric: 42.3919

Epoch 968: val_loss did not improve from 42.38888
196/196 - 94s - loss: 42.1573 - MinusLogProbMetric: 42.1573 - val_loss: 42.3919 - val_MinusLogProbMetric: 42.3919 - lr: 1.6935e-08 - 94s/epoch - 477ms/step
Epoch 969/1000
2023-10-10 21:59:39.811 
Epoch 969/1000 
	 loss: 42.1575, MinusLogProbMetric: 42.1575, val_loss: 42.3903, val_MinusLogProbMetric: 42.3903

Epoch 969: val_loss did not improve from 42.38888
196/196 - 92s - loss: 42.1575 - MinusLogProbMetric: 42.1575 - val_loss: 42.3903 - val_MinusLogProbMetric: 42.3903 - lr: 1.6935e-08 - 92s/epoch - 472ms/step
Epoch 970/1000
2023-10-10 22:01:14.352 
Epoch 970/1000 
	 loss: 42.1584, MinusLogProbMetric: 42.1584, val_loss: 42.3910, val_MinusLogProbMetric: 42.3910

Epoch 970: val_loss did not improve from 42.38888
196/196 - 95s - loss: 42.1584 - MinusLogProbMetric: 42.1584 - val_loss: 42.3910 - val_MinusLogProbMetric: 42.3910 - lr: 1.6935e-08 - 95s/epoch - 482ms/step
Epoch 971/1000
2023-10-10 22:02:46.343 
Epoch 971/1000 
	 loss: 42.1595, MinusLogProbMetric: 42.1595, val_loss: 42.3893, val_MinusLogProbMetric: 42.3893

Epoch 971: val_loss did not improve from 42.38888
196/196 - 92s - loss: 42.1595 - MinusLogProbMetric: 42.1595 - val_loss: 42.3893 - val_MinusLogProbMetric: 42.3893 - lr: 1.6935e-08 - 92s/epoch - 469ms/step
Epoch 972/1000
2023-10-10 22:04:20.833 
Epoch 972/1000 
	 loss: 42.1586, MinusLogProbMetric: 42.1586, val_loss: 42.3924, val_MinusLogProbMetric: 42.3924

Epoch 972: val_loss did not improve from 42.38888
196/196 - 94s - loss: 42.1586 - MinusLogProbMetric: 42.1586 - val_loss: 42.3924 - val_MinusLogProbMetric: 42.3924 - lr: 1.6935e-08 - 94s/epoch - 482ms/step
Epoch 973/1000
2023-10-10 22:05:53.962 
Epoch 973/1000 
	 loss: 42.1575, MinusLogProbMetric: 42.1575, val_loss: 42.3859, val_MinusLogProbMetric: 42.3859

Epoch 973: val_loss improved from 42.38888 to 42.38588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_263/weights/best_weights.h5
196/196 - 97s - loss: 42.1575 - MinusLogProbMetric: 42.1575 - val_loss: 42.3859 - val_MinusLogProbMetric: 42.3859 - lr: 1.6935e-08 - 97s/epoch - 494ms/step
Epoch 974/1000
2023-10-10 22:07:33.431 
Epoch 974/1000 
	 loss: 42.1577, MinusLogProbMetric: 42.1577, val_loss: 42.3923, val_MinusLogProbMetric: 42.3923

Epoch 974: val_loss did not improve from 42.38588
196/196 - 96s - loss: 42.1577 - MinusLogProbMetric: 42.1577 - val_loss: 42.3923 - val_MinusLogProbMetric: 42.3923 - lr: 1.6935e-08 - 96s/epoch - 489ms/step
Epoch 975/1000
2023-10-10 22:09:07.019 
Epoch 975/1000 
	 loss: 42.1589, MinusLogProbMetric: 42.1589, val_loss: 42.3909, val_MinusLogProbMetric: 42.3909

Epoch 975: val_loss did not improve from 42.38588
196/196 - 94s - loss: 42.1589 - MinusLogProbMetric: 42.1589 - val_loss: 42.3909 - val_MinusLogProbMetric: 42.3909 - lr: 1.6935e-08 - 94s/epoch - 477ms/step
Epoch 976/1000
2023-10-10 22:10:41.960 
Epoch 976/1000 
	 loss: 42.1596, MinusLogProbMetric: 42.1596, val_loss: 42.3913, val_MinusLogProbMetric: 42.3913

Epoch 976: val_loss did not improve from 42.38588
196/196 - 95s - loss: 42.1596 - MinusLogProbMetric: 42.1596 - val_loss: 42.3913 - val_MinusLogProbMetric: 42.3913 - lr: 1.6935e-08 - 95s/epoch - 484ms/step
Epoch 977/1000
2023-10-10 22:12:16.524 
Epoch 977/1000 
	 loss: 42.1579, MinusLogProbMetric: 42.1579, val_loss: 42.3887, val_MinusLogProbMetric: 42.3887

Epoch 977: val_loss did not improve from 42.38588
196/196 - 95s - loss: 42.1579 - MinusLogProbMetric: 42.1579 - val_loss: 42.3887 - val_MinusLogProbMetric: 42.3887 - lr: 1.6935e-08 - 95s/epoch - 482ms/step
Epoch 978/1000
2023-10-10 22:13:50.144 
Epoch 978/1000 
	 loss: 42.1566, MinusLogProbMetric: 42.1566, val_loss: 42.3916, val_MinusLogProbMetric: 42.3916

Epoch 978: val_loss did not improve from 42.38588
196/196 - 94s - loss: 42.1566 - MinusLogProbMetric: 42.1566 - val_loss: 42.3916 - val_MinusLogProbMetric: 42.3916 - lr: 1.6935e-08 - 94s/epoch - 478ms/step
Epoch 979/1000
2023-10-10 22:15:23.759 
Epoch 979/1000 
	 loss: 42.1591, MinusLogProbMetric: 42.1591, val_loss: 42.3924, val_MinusLogProbMetric: 42.3924

Epoch 979: val_loss did not improve from 42.38588
196/196 - 94s - loss: 42.1591 - MinusLogProbMetric: 42.1591 - val_loss: 42.3924 - val_MinusLogProbMetric: 42.3924 - lr: 1.6935e-08 - 94s/epoch - 478ms/step
Epoch 980/1000
2023-10-10 22:16:56.049 
Epoch 980/1000 
	 loss: 42.1566, MinusLogProbMetric: 42.1566, val_loss: 42.3894, val_MinusLogProbMetric: 42.3894

Epoch 980: val_loss did not improve from 42.38588
196/196 - 92s - loss: 42.1566 - MinusLogProbMetric: 42.1566 - val_loss: 42.3894 - val_MinusLogProbMetric: 42.3894 - lr: 1.6935e-08 - 92s/epoch - 471ms/step
Epoch 981/1000
2023-10-10 22:18:32.761 
Epoch 981/1000 
	 loss: 42.1583, MinusLogProbMetric: 42.1583, val_loss: 42.3890, val_MinusLogProbMetric: 42.3890

Epoch 981: val_loss did not improve from 42.38588
196/196 - 97s - loss: 42.1583 - MinusLogProbMetric: 42.1583 - val_loss: 42.3890 - val_MinusLogProbMetric: 42.3890 - lr: 1.6935e-08 - 97s/epoch - 493ms/step
Epoch 982/1000
2023-10-10 22:20:06.397 
Epoch 982/1000 
	 loss: 42.1580, MinusLogProbMetric: 42.1580, val_loss: 42.3867, val_MinusLogProbMetric: 42.3867

Epoch 982: val_loss did not improve from 42.38588
196/196 - 94s - loss: 42.1580 - MinusLogProbMetric: 42.1580 - val_loss: 42.3867 - val_MinusLogProbMetric: 42.3867 - lr: 1.6935e-08 - 94s/epoch - 478ms/step
Epoch 983/1000
2023-10-10 22:21:40.369 
Epoch 983/1000 
	 loss: 42.1590, MinusLogProbMetric: 42.1590, val_loss: 42.3886, val_MinusLogProbMetric: 42.3886

Epoch 983: val_loss did not improve from 42.38588
196/196 - 94s - loss: 42.1590 - MinusLogProbMetric: 42.1590 - val_loss: 42.3886 - val_MinusLogProbMetric: 42.3886 - lr: 1.6935e-08 - 94s/epoch - 479ms/step
Epoch 984/1000
2023-10-10 22:23:13.406 
Epoch 984/1000 
	 loss: 42.1567, MinusLogProbMetric: 42.1567, val_loss: 42.3886, val_MinusLogProbMetric: 42.3886

Epoch 984: val_loss did not improve from 42.38588
196/196 - 93s - loss: 42.1567 - MinusLogProbMetric: 42.1567 - val_loss: 42.3886 - val_MinusLogProbMetric: 42.3886 - lr: 1.6935e-08 - 93s/epoch - 475ms/step
Epoch 985/1000
2023-10-10 22:24:51.734 
Epoch 985/1000 
	 loss: 42.1570, MinusLogProbMetric: 42.1570, val_loss: 42.3890, val_MinusLogProbMetric: 42.3890

Epoch 985: val_loss did not improve from 42.38588
196/196 - 98s - loss: 42.1570 - MinusLogProbMetric: 42.1570 - val_loss: 42.3890 - val_MinusLogProbMetric: 42.3890 - lr: 1.6935e-08 - 98s/epoch - 502ms/step
Epoch 986/1000
2023-10-10 22:26:23.439 
Epoch 986/1000 
	 loss: 42.1579, MinusLogProbMetric: 42.1579, val_loss: 42.3956, val_MinusLogProbMetric: 42.3956

Epoch 986: val_loss did not improve from 42.38588
196/196 - 92s - loss: 42.1579 - MinusLogProbMetric: 42.1579 - val_loss: 42.3956 - val_MinusLogProbMetric: 42.3956 - lr: 1.6935e-08 - 92s/epoch - 468ms/step
Epoch 987/1000
2023-10-10 22:27:58.359 
Epoch 987/1000 
	 loss: 42.1602, MinusLogProbMetric: 42.1602, val_loss: 42.3911, val_MinusLogProbMetric: 42.3911

Epoch 987: val_loss did not improve from 42.38588
196/196 - 95s - loss: 42.1602 - MinusLogProbMetric: 42.1602 - val_loss: 42.3911 - val_MinusLogProbMetric: 42.3911 - lr: 1.6935e-08 - 95s/epoch - 484ms/step
Epoch 988/1000
2023-10-10 22:29:34.253 
Epoch 988/1000 
	 loss: 42.1600, MinusLogProbMetric: 42.1600, val_loss: 42.3964, val_MinusLogProbMetric: 42.3964

Epoch 988: val_loss did not improve from 42.38588
196/196 - 96s - loss: 42.1600 - MinusLogProbMetric: 42.1600 - val_loss: 42.3964 - val_MinusLogProbMetric: 42.3964 - lr: 1.6935e-08 - 96s/epoch - 489ms/step
Epoch 989/1000
2023-10-10 22:31:08.151 
Epoch 989/1000 
	 loss: 42.1615, MinusLogProbMetric: 42.1615, val_loss: 42.3943, val_MinusLogProbMetric: 42.3943

Epoch 989: val_loss did not improve from 42.38588
196/196 - 94s - loss: 42.1615 - MinusLogProbMetric: 42.1615 - val_loss: 42.3943 - val_MinusLogProbMetric: 42.3943 - lr: 1.6935e-08 - 94s/epoch - 479ms/step
Epoch 990/1000
2023-10-10 22:32:42.773 
Epoch 990/1000 
	 loss: 42.1643, MinusLogProbMetric: 42.1643, val_loss: 42.3954, val_MinusLogProbMetric: 42.3954

Epoch 990: val_loss did not improve from 42.38588
196/196 - 95s - loss: 42.1643 - MinusLogProbMetric: 42.1643 - val_loss: 42.3954 - val_MinusLogProbMetric: 42.3954 - lr: 1.6935e-08 - 95s/epoch - 483ms/step
Epoch 991/1000
2023-10-10 22:34:18.173 
Epoch 991/1000 
	 loss: 42.1626, MinusLogProbMetric: 42.1626, val_loss: 42.3956, val_MinusLogProbMetric: 42.3956

Epoch 991: val_loss did not improve from 42.38588
196/196 - 95s - loss: 42.1626 - MinusLogProbMetric: 42.1626 - val_loss: 42.3956 - val_MinusLogProbMetric: 42.3956 - lr: 1.6935e-08 - 95s/epoch - 487ms/step
Epoch 992/1000
2023-10-10 22:35:52.894 
Epoch 992/1000 
	 loss: 42.1614, MinusLogProbMetric: 42.1614, val_loss: 42.3930, val_MinusLogProbMetric: 42.3930

Epoch 992: val_loss did not improve from 42.38588
196/196 - 95s - loss: 42.1614 - MinusLogProbMetric: 42.1614 - val_loss: 42.3930 - val_MinusLogProbMetric: 42.3930 - lr: 1.6935e-08 - 95s/epoch - 483ms/step
Epoch 993/1000
2023-10-10 22:37:28.547 
Epoch 993/1000 
	 loss: 42.1604, MinusLogProbMetric: 42.1604, val_loss: 42.3919, val_MinusLogProbMetric: 42.3919

Epoch 993: val_loss did not improve from 42.38588
196/196 - 96s - loss: 42.1604 - MinusLogProbMetric: 42.1604 - val_loss: 42.3919 - val_MinusLogProbMetric: 42.3919 - lr: 1.6935e-08 - 96s/epoch - 488ms/step
Epoch 994/1000
2023-10-10 22:39:07.571 
Epoch 994/1000 
	 loss: 42.1576, MinusLogProbMetric: 42.1576, val_loss: 42.3909, val_MinusLogProbMetric: 42.3909

Epoch 994: val_loss did not improve from 42.38588
196/196 - 99s - loss: 42.1576 - MinusLogProbMetric: 42.1576 - val_loss: 42.3909 - val_MinusLogProbMetric: 42.3909 - lr: 1.6935e-08 - 99s/epoch - 505ms/step
Epoch 995/1000
2023-10-10 22:41:00.812 
Epoch 995/1000 
	 loss: 42.1590, MinusLogProbMetric: 42.1590, val_loss: 42.3910, val_MinusLogProbMetric: 42.3910

Epoch 995: val_loss did not improve from 42.38588
196/196 - 113s - loss: 42.1590 - MinusLogProbMetric: 42.1590 - val_loss: 42.3910 - val_MinusLogProbMetric: 42.3910 - lr: 1.6935e-08 - 113s/epoch - 579ms/step
Epoch 996/1000
2023-10-10 22:42:46.241 
Epoch 996/1000 
	 loss: 42.1588, MinusLogProbMetric: 42.1588, val_loss: 42.3884, val_MinusLogProbMetric: 42.3884

Epoch 996: val_loss did not improve from 42.38588
196/196 - 105s - loss: 42.1588 - MinusLogProbMetric: 42.1588 - val_loss: 42.3884 - val_MinusLogProbMetric: 42.3884 - lr: 1.6935e-08 - 105s/epoch - 537ms/step
Epoch 997/1000
2023-10-10 22:44:31.934 
Epoch 997/1000 
	 loss: 42.1581, MinusLogProbMetric: 42.1581, val_loss: 42.3926, val_MinusLogProbMetric: 42.3926

Epoch 997: val_loss did not improve from 42.38588
196/196 - 106s - loss: 42.1581 - MinusLogProbMetric: 42.1581 - val_loss: 42.3926 - val_MinusLogProbMetric: 42.3926 - lr: 1.6935e-08 - 106s/epoch - 539ms/step
Epoch 998/1000
2023-10-10 22:46:12.556 
Epoch 998/1000 
	 loss: 42.1573, MinusLogProbMetric: 42.1573, val_loss: 42.3895, val_MinusLogProbMetric: 42.3895

Epoch 998: val_loss did not improve from 42.38588
196/196 - 101s - loss: 42.1573 - MinusLogProbMetric: 42.1573 - val_loss: 42.3895 - val_MinusLogProbMetric: 42.3895 - lr: 1.6935e-08 - 101s/epoch - 513ms/step
Epoch 999/1000
2023-10-10 22:47:52.905 
Epoch 999/1000 
	 loss: 42.1571, MinusLogProbMetric: 42.1571, val_loss: 42.3881, val_MinusLogProbMetric: 42.3881

Epoch 999: val_loss did not improve from 42.38588
196/196 - 100s - loss: 42.1571 - MinusLogProbMetric: 42.1571 - val_loss: 42.3881 - val_MinusLogProbMetric: 42.3881 - lr: 1.6935e-08 - 100s/epoch - 512ms/step
Epoch 1000/1000
2023-10-10 22:49:33.858 
Epoch 1000/1000 
	 loss: 42.1575, MinusLogProbMetric: 42.1575, val_loss: 42.3911, val_MinusLogProbMetric: 42.3911

Epoch 1000: val_loss did not improve from 42.38588
196/196 - 101s - loss: 42.1575 - MinusLogProbMetric: 42.1575 - val_loss: 42.3911 - val_MinusLogProbMetric: 42.3911 - lr: 1.6935e-08 - 101s/epoch - 515ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
LR metric calculation completed in 47.81507245008834 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 27.597317456966266 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 20.705339916981757 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
FN metric calculation completed in 23.16612353688106 seconds.
Training succeeded with seed 377.
Model trained in 71284.23 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 122.34 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 123.26 s.
===========
Run 263/720 done in 82093.20 s.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

===========
Generating train data for run 317.
===========
Train data generated in 0.56 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_317/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_317/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_317/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_317
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_164"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_165 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f0ce6a58c70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0ce69f6b00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0ce69f6b00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c58207970>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0ce63e6440>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0ce63e69b0>, <keras.callbacks.ModelCheckpoint object at 0x7f0ce63e6a70>, <keras.callbacks.EarlyStopping object at 0x7f0ce63e6ce0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0ce63e6d10>, <keras.callbacks.TerminateOnNaN object at 0x7f0ce63e6950>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_317/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 317/720 with hyperparameters:
timestamp = 2023-10-10 22:51:50.889066
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 57: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 22:56:57.024 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1630.2334, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 306s - loss: nan - MinusLogProbMetric: 1630.2334 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 306s/epoch - 2s/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0003333333333333333.
===========
Generating train data for run 317.
===========
Train data generated in 0.41 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_317/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_317/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_317/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_317
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_175"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_176 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f0f783f6b90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0f582a69b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0f582a69b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0edc3b3a00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0edc44fc70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0edc368220>, <keras.callbacks.ModelCheckpoint object at 0x7f0edc3682e0>, <keras.callbacks.EarlyStopping object at 0x7f0edc368550>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0edc368580>, <keras.callbacks.TerminateOnNaN object at 0x7f0edc3681c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_317/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 317/720 with hyperparameters:
timestamp = 2023-10-10 22:57:09.416600
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
2023-10-10 23:03:05.815 
Epoch 1/1000 
	 loss: 750.7647, MinusLogProbMetric: 750.7647, val_loss: 295.4149, val_MinusLogProbMetric: 295.4149

Epoch 1: val_loss improved from inf to 295.41489, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 358s - loss: 750.7647 - MinusLogProbMetric: 750.7647 - val_loss: 295.4149 - val_MinusLogProbMetric: 295.4149 - lr: 3.3333e-04 - 358s/epoch - 2s/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 142: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-10 23:04:05.989 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 423.9292, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 295.41489
196/196 - 57s - loss: nan - MinusLogProbMetric: 423.9292 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 57s/epoch - 293ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0001111111111111111.
===========
Generating train data for run 317.
===========
Train data generated in 1.38 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_317/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_317/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_317/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_317
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_186"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_187 (InputLayer)      [(None, 32)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  826720    
 ayer)                                                           
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f0b282d5b10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d30617fa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d30617fa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0f3c4d3370>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b50ca1480>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b50ca19f0>, <keras.callbacks.ModelCheckpoint object at 0x7f0b50ca1ab0>, <keras.callbacks.EarlyStopping object at 0x7f0b50ca1d20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b50ca1d50>, <keras.callbacks.TerminateOnNaN object at 0x7f0b50ca1990>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 317/720 with hyperparameters:
timestamp = 2023-10-10 23:04:22.825275
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
2023-10-10 23:10:16.278 
Epoch 1/1000 
	 loss: 230.6420, MinusLogProbMetric: 230.6420, val_loss: 176.2888, val_MinusLogProbMetric: 176.2888

Epoch 1: val_loss improved from inf to 176.28876, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 355s - loss: 230.6420 - MinusLogProbMetric: 230.6420 - val_loss: 176.2888 - val_MinusLogProbMetric: 176.2888 - lr: 1.1111e-04 - 355s/epoch - 2s/step
Epoch 2/1000
2023-10-10 23:11:37.075 
Epoch 2/1000 
	 loss: 172.4576, MinusLogProbMetric: 172.4576, val_loss: 141.4882, val_MinusLogProbMetric: 141.4882

Epoch 2: val_loss improved from 176.28876 to 141.48816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 80s - loss: 172.4576 - MinusLogProbMetric: 172.4576 - val_loss: 141.4882 - val_MinusLogProbMetric: 141.4882 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 3/1000
2023-10-10 23:12:57.988 
Epoch 3/1000 
	 loss: 253.8270, MinusLogProbMetric: 253.8270, val_loss: 220.2161, val_MinusLogProbMetric: 220.2161

Epoch 3: val_loss did not improve from 141.48816
196/196 - 80s - loss: 253.8270 - MinusLogProbMetric: 253.8270 - val_loss: 220.2161 - val_MinusLogProbMetric: 220.2161 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 4/1000
2023-10-10 23:14:17.261 
Epoch 4/1000 
	 loss: 199.9204, MinusLogProbMetric: 199.9204, val_loss: 159.4967, val_MinusLogProbMetric: 159.4967

Epoch 4: val_loss did not improve from 141.48816
196/196 - 79s - loss: 199.9204 - MinusLogProbMetric: 199.9204 - val_loss: 159.4967 - val_MinusLogProbMetric: 159.4967 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 5/1000
2023-10-10 23:15:35.165 
Epoch 5/1000 
	 loss: 143.8628, MinusLogProbMetric: 143.8628, val_loss: 130.2139, val_MinusLogProbMetric: 130.2139

Epoch 5: val_loss improved from 141.48816 to 130.21391, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 80s - loss: 143.8628 - MinusLogProbMetric: 143.8628 - val_loss: 130.2139 - val_MinusLogProbMetric: 130.2139 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 6/1000
2023-10-10 23:16:53.300 
Epoch 6/1000 
	 loss: 121.7372, MinusLogProbMetric: 121.7372, val_loss: 115.1503, val_MinusLogProbMetric: 115.1503

Epoch 6: val_loss improved from 130.21391 to 115.15028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 78s - loss: 121.7372 - MinusLogProbMetric: 121.7372 - val_loss: 115.1503 - val_MinusLogProbMetric: 115.1503 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 7/1000
2023-10-10 23:18:17.832 
Epoch 7/1000 
	 loss: 110.9011, MinusLogProbMetric: 110.9011, val_loss: 107.0854, val_MinusLogProbMetric: 107.0854

Epoch 7: val_loss improved from 115.15028 to 107.08542, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 84s - loss: 110.9011 - MinusLogProbMetric: 110.9011 - val_loss: 107.0854 - val_MinusLogProbMetric: 107.0854 - lr: 1.1111e-04 - 84s/epoch - 427ms/step
Epoch 8/1000
2023-10-10 23:19:39.562 
Epoch 8/1000 
	 loss: 107.6246, MinusLogProbMetric: 107.6246, val_loss: 107.5333, val_MinusLogProbMetric: 107.5333

Epoch 8: val_loss did not improve from 107.08542
196/196 - 80s - loss: 107.6246 - MinusLogProbMetric: 107.6246 - val_loss: 107.5333 - val_MinusLogProbMetric: 107.5333 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 9/1000
2023-10-10 23:20:55.343 
Epoch 9/1000 
	 loss: 102.6812, MinusLogProbMetric: 102.6812, val_loss: 96.2351, val_MinusLogProbMetric: 96.2351

Epoch 9: val_loss improved from 107.08542 to 96.23507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 78s - loss: 102.6812 - MinusLogProbMetric: 102.6812 - val_loss: 96.2351 - val_MinusLogProbMetric: 96.2351 - lr: 1.1111e-04 - 78s/epoch - 400ms/step
Epoch 10/1000
2023-10-10 23:22:14.694 
Epoch 10/1000 
	 loss: 120.2212, MinusLogProbMetric: 120.2212, val_loss: 110.1473, val_MinusLogProbMetric: 110.1473

Epoch 10: val_loss did not improve from 96.23507
196/196 - 77s - loss: 120.2212 - MinusLogProbMetric: 120.2212 - val_loss: 110.1473 - val_MinusLogProbMetric: 110.1473 - lr: 1.1111e-04 - 77s/epoch - 392ms/step
Epoch 11/1000
2023-10-10 23:23:34.643 
Epoch 11/1000 
	 loss: 102.6873, MinusLogProbMetric: 102.6873, val_loss: 97.3512, val_MinusLogProbMetric: 97.3512

Epoch 11: val_loss did not improve from 96.23507
196/196 - 80s - loss: 102.6873 - MinusLogProbMetric: 102.6873 - val_loss: 97.3512 - val_MinusLogProbMetric: 97.3512 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 12/1000
2023-10-10 23:24:53.643 
Epoch 12/1000 
	 loss: 95.3909, MinusLogProbMetric: 95.3909, val_loss: 93.2521, val_MinusLogProbMetric: 93.2521

Epoch 12: val_loss improved from 96.23507 to 93.25213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 81s - loss: 95.3909 - MinusLogProbMetric: 95.3909 - val_loss: 93.2521 - val_MinusLogProbMetric: 93.2521 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 13/1000
2023-10-10 23:26:13.457 
Epoch 13/1000 
	 loss: 178.1218, MinusLogProbMetric: 178.1218, val_loss: 173.0913, val_MinusLogProbMetric: 173.0913

Epoch 13: val_loss did not improve from 93.25213
196/196 - 78s - loss: 178.1218 - MinusLogProbMetric: 178.1218 - val_loss: 173.0913 - val_MinusLogProbMetric: 173.0913 - lr: 1.1111e-04 - 78s/epoch - 399ms/step
Epoch 14/1000
2023-10-10 23:27:32.732 
Epoch 14/1000 
	 loss: 150.0113, MinusLogProbMetric: 150.0113, val_loss: 130.0101, val_MinusLogProbMetric: 130.0101

Epoch 14: val_loss did not improve from 93.25213
196/196 - 79s - loss: 150.0113 - MinusLogProbMetric: 150.0113 - val_loss: 130.0101 - val_MinusLogProbMetric: 130.0101 - lr: 1.1111e-04 - 79s/epoch - 404ms/step
Epoch 15/1000
2023-10-10 23:28:48.990 
Epoch 15/1000 
	 loss: 114.6503, MinusLogProbMetric: 114.6503, val_loss: 102.2808, val_MinusLogProbMetric: 102.2808

Epoch 15: val_loss did not improve from 93.25213
196/196 - 76s - loss: 114.6503 - MinusLogProbMetric: 114.6503 - val_loss: 102.2808 - val_MinusLogProbMetric: 102.2808 - lr: 1.1111e-04 - 76s/epoch - 389ms/step
Epoch 16/1000
2023-10-10 23:30:08.360 
Epoch 16/1000 
	 loss: 97.6350, MinusLogProbMetric: 97.6350, val_loss: 92.2990, val_MinusLogProbMetric: 92.2990

Epoch 16: val_loss improved from 93.25213 to 92.29900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 80s - loss: 97.6350 - MinusLogProbMetric: 97.6350 - val_loss: 92.2990 - val_MinusLogProbMetric: 92.2990 - lr: 1.1111e-04 - 80s/epoch - 409ms/step
Epoch 17/1000
2023-10-10 23:31:28.877 
Epoch 17/1000 
	 loss: 88.9333, MinusLogProbMetric: 88.9333, val_loss: 85.1481, val_MinusLogProbMetric: 85.1481

Epoch 17: val_loss improved from 92.29900 to 85.14809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 81s - loss: 88.9333 - MinusLogProbMetric: 88.9333 - val_loss: 85.1481 - val_MinusLogProbMetric: 85.1481 - lr: 1.1111e-04 - 81s/epoch - 414ms/step
Epoch 18/1000
2023-10-10 23:32:51.194 
Epoch 18/1000 
	 loss: 84.4295, MinusLogProbMetric: 84.4295, val_loss: 81.6648, val_MinusLogProbMetric: 81.6648

Epoch 18: val_loss improved from 85.14809 to 81.66476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 82s - loss: 84.4295 - MinusLogProbMetric: 84.4295 - val_loss: 81.6648 - val_MinusLogProbMetric: 81.6648 - lr: 1.1111e-04 - 82s/epoch - 416ms/step
Epoch 19/1000
2023-10-10 23:34:11.728 
Epoch 19/1000 
	 loss: 80.7735, MinusLogProbMetric: 80.7735, val_loss: 79.7690, val_MinusLogProbMetric: 79.7690

Epoch 19: val_loss improved from 81.66476 to 79.76897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 81s - loss: 80.7735 - MinusLogProbMetric: 80.7735 - val_loss: 79.7690 - val_MinusLogProbMetric: 79.7690 - lr: 1.1111e-04 - 81s/epoch - 411ms/step
Epoch 20/1000
2023-10-10 23:35:32.407 
Epoch 20/1000 
	 loss: 81.2126, MinusLogProbMetric: 81.2126, val_loss: 81.9394, val_MinusLogProbMetric: 81.9394

Epoch 20: val_loss did not improve from 79.76897
196/196 - 80s - loss: 81.2126 - MinusLogProbMetric: 81.2126 - val_loss: 81.9394 - val_MinusLogProbMetric: 81.9394 - lr: 1.1111e-04 - 80s/epoch - 408ms/step
Epoch 21/1000
2023-10-10 23:36:53.047 
Epoch 21/1000 
	 loss: 87.1172, MinusLogProbMetric: 87.1172, val_loss: 84.9879, val_MinusLogProbMetric: 84.9879

Epoch 21: val_loss did not improve from 79.76897
196/196 - 81s - loss: 87.1172 - MinusLogProbMetric: 87.1172 - val_loss: 84.9879 - val_MinusLogProbMetric: 84.9879 - lr: 1.1111e-04 - 81s/epoch - 411ms/step
Epoch 22/1000
2023-10-10 23:38:14.480 
Epoch 22/1000 
	 loss: 79.1219, MinusLogProbMetric: 79.1219, val_loss: 76.5229, val_MinusLogProbMetric: 76.5229

Epoch 22: val_loss improved from 79.76897 to 76.52294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 82s - loss: 79.1219 - MinusLogProbMetric: 79.1219 - val_loss: 76.5229 - val_MinusLogProbMetric: 76.5229 - lr: 1.1111e-04 - 82s/epoch - 418ms/step
Epoch 23/1000
2023-10-10 23:39:35.407 
Epoch 23/1000 
	 loss: 75.8254, MinusLogProbMetric: 75.8254, val_loss: 75.5414, val_MinusLogProbMetric: 75.5414

Epoch 23: val_loss improved from 76.52294 to 75.54142, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 82s - loss: 75.8254 - MinusLogProbMetric: 75.8254 - val_loss: 75.5414 - val_MinusLogProbMetric: 75.5414 - lr: 1.1111e-04 - 82s/epoch - 416ms/step
Epoch 24/1000
2023-10-10 23:40:55.673 
Epoch 24/1000 
	 loss: 73.9402, MinusLogProbMetric: 73.9402, val_loss: 73.2625, val_MinusLogProbMetric: 73.2625

Epoch 24: val_loss improved from 75.54142 to 73.26245, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 81s - loss: 73.9402 - MinusLogProbMetric: 73.9402 - val_loss: 73.2625 - val_MinusLogProbMetric: 73.2625 - lr: 1.1111e-04 - 81s/epoch - 412ms/step
Epoch 25/1000
2023-10-10 23:42:16.360 
Epoch 25/1000 
	 loss: 72.6177, MinusLogProbMetric: 72.6177, val_loss: 73.1897, val_MinusLogProbMetric: 73.1897

Epoch 25: val_loss improved from 73.26245 to 73.18973, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 80s - loss: 72.6177 - MinusLogProbMetric: 72.6177 - val_loss: 73.1897 - val_MinusLogProbMetric: 73.1897 - lr: 1.1111e-04 - 80s/epoch - 406ms/step
Epoch 26/1000
2023-10-10 23:43:59.259 
Epoch 26/1000 
	 loss: 71.3744, MinusLogProbMetric: 71.3744, val_loss: 70.7942, val_MinusLogProbMetric: 70.7942

Epoch 26: val_loss improved from 73.18973 to 70.79424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 105s - loss: 71.3744 - MinusLogProbMetric: 71.3744 - val_loss: 70.7942 - val_MinusLogProbMetric: 70.7942 - lr: 1.1111e-04 - 105s/epoch - 535ms/step
Epoch 27/1000
2023-10-10 23:46:05.187 
Epoch 27/1000 
	 loss: 215.2802, MinusLogProbMetric: 215.2802, val_loss: 199.3239, val_MinusLogProbMetric: 199.3239

Epoch 27: val_loss did not improve from 70.79424
196/196 - 123s - loss: 215.2802 - MinusLogProbMetric: 215.2802 - val_loss: 199.3239 - val_MinusLogProbMetric: 199.3239 - lr: 1.1111e-04 - 123s/epoch - 630ms/step
Epoch 28/1000
2023-10-10 23:48:02.927 
Epoch 28/1000 
	 loss: 145.7714, MinusLogProbMetric: 145.7714, val_loss: 113.5781, val_MinusLogProbMetric: 113.5781

Epoch 28: val_loss did not improve from 70.79424
196/196 - 118s - loss: 145.7714 - MinusLogProbMetric: 145.7714 - val_loss: 113.5781 - val_MinusLogProbMetric: 113.5781 - lr: 1.1111e-04 - 118s/epoch - 601ms/step
Epoch 29/1000
2023-10-10 23:49:59.635 
Epoch 29/1000 
	 loss: 88.0437, MinusLogProbMetric: 88.0437, val_loss: 74.4233, val_MinusLogProbMetric: 74.4233

Epoch 29: val_loss did not improve from 70.79424
196/196 - 117s - loss: 88.0437 - MinusLogProbMetric: 88.0437 - val_loss: 74.4233 - val_MinusLogProbMetric: 74.4233 - lr: 1.1111e-04 - 117s/epoch - 595ms/step
Epoch 30/1000
2023-10-10 23:51:54.372 
Epoch 30/1000 
	 loss: 67.6747, MinusLogProbMetric: 67.6747, val_loss: 61.1260, val_MinusLogProbMetric: 61.1260

Epoch 30: val_loss improved from 70.79424 to 61.12600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 117s - loss: 67.6747 - MinusLogProbMetric: 67.6747 - val_loss: 61.1260 - val_MinusLogProbMetric: 61.1260 - lr: 1.1111e-04 - 117s/epoch - 599ms/step
Epoch 31/1000
2023-10-10 23:53:58.247 
Epoch 31/1000 
	 loss: 59.7948, MinusLogProbMetric: 59.7948, val_loss: 56.1233, val_MinusLogProbMetric: 56.1233

Epoch 31: val_loss improved from 61.12600 to 56.12330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 124s - loss: 59.7948 - MinusLogProbMetric: 59.7948 - val_loss: 56.1233 - val_MinusLogProbMetric: 56.1233 - lr: 1.1111e-04 - 124s/epoch - 632ms/step
Epoch 32/1000
2023-10-10 23:55:58.479 
Epoch 32/1000 
	 loss: 128.1253, MinusLogProbMetric: 128.1253, val_loss: 111.4658, val_MinusLogProbMetric: 111.4658

Epoch 32: val_loss did not improve from 56.12330
196/196 - 117s - loss: 128.1253 - MinusLogProbMetric: 128.1253 - val_loss: 111.4658 - val_MinusLogProbMetric: 111.4658 - lr: 1.1111e-04 - 117s/epoch - 599ms/step
Epoch 33/1000
2023-10-10 23:57:51.712 
Epoch 33/1000 
	 loss: 132.9030, MinusLogProbMetric: 132.9030, val_loss: 120.3402, val_MinusLogProbMetric: 120.3402

Epoch 33: val_loss did not improve from 56.12330
196/196 - 113s - loss: 132.9030 - MinusLogProbMetric: 132.9030 - val_loss: 120.3402 - val_MinusLogProbMetric: 120.3402 - lr: 1.1111e-04 - 113s/epoch - 578ms/step
Epoch 34/1000
2023-10-10 23:59:44.569 
Epoch 34/1000 
	 loss: 116.1886, MinusLogProbMetric: 116.1886, val_loss: 111.4932, val_MinusLogProbMetric: 111.4932

Epoch 34: val_loss did not improve from 56.12330
196/196 - 113s - loss: 116.1886 - MinusLogProbMetric: 116.1886 - val_loss: 111.4932 - val_MinusLogProbMetric: 111.4932 - lr: 1.1111e-04 - 113s/epoch - 576ms/step
Epoch 35/1000
2023-10-11 00:01:34.998 
Epoch 35/1000 
	 loss: 112.9904, MinusLogProbMetric: 112.9904, val_loss: 117.7083, val_MinusLogProbMetric: 117.7083

Epoch 35: val_loss did not improve from 56.12330
196/196 - 110s - loss: 112.9904 - MinusLogProbMetric: 112.9904 - val_loss: 117.7083 - val_MinusLogProbMetric: 117.7083 - lr: 1.1111e-04 - 110s/epoch - 563ms/step
Epoch 36/1000
2023-10-11 00:03:29.118 
Epoch 36/1000 
	 loss: 186.0478, MinusLogProbMetric: 186.0478, val_loss: 209.5198, val_MinusLogProbMetric: 209.5198

Epoch 36: val_loss did not improve from 56.12330
196/196 - 114s - loss: 186.0478 - MinusLogProbMetric: 186.0478 - val_loss: 209.5198 - val_MinusLogProbMetric: 209.5198 - lr: 1.1111e-04 - 114s/epoch - 582ms/step
Epoch 37/1000
2023-10-11 00:05:23.322 
Epoch 37/1000 
	 loss: 165.8084, MinusLogProbMetric: 165.8084, val_loss: 143.7989, val_MinusLogProbMetric: 143.7989

Epoch 37: val_loss did not improve from 56.12330
196/196 - 114s - loss: 165.8084 - MinusLogProbMetric: 165.8084 - val_loss: 143.7989 - val_MinusLogProbMetric: 143.7989 - lr: 1.1111e-04 - 114s/epoch - 583ms/step
Epoch 38/1000
2023-10-11 00:07:14.739 
Epoch 38/1000 
	 loss: 136.7530, MinusLogProbMetric: 136.7530, val_loss: 130.9393, val_MinusLogProbMetric: 130.9393

Epoch 38: val_loss did not improve from 56.12330
196/196 - 111s - loss: 136.7530 - MinusLogProbMetric: 136.7530 - val_loss: 130.9393 - val_MinusLogProbMetric: 130.9393 - lr: 1.1111e-04 - 111s/epoch - 568ms/step
Epoch 39/1000
2023-10-11 00:09:11.062 
Epoch 39/1000 
	 loss: 125.5380, MinusLogProbMetric: 125.5380, val_loss: 121.6771, val_MinusLogProbMetric: 121.6771

Epoch 39: val_loss did not improve from 56.12330
196/196 - 116s - loss: 125.5380 - MinusLogProbMetric: 125.5380 - val_loss: 121.6771 - val_MinusLogProbMetric: 121.6771 - lr: 1.1111e-04 - 116s/epoch - 593ms/step
Epoch 40/1000
2023-10-11 00:11:04.626 
Epoch 40/1000 
	 loss: 118.8525, MinusLogProbMetric: 118.8525, val_loss: 116.9239, val_MinusLogProbMetric: 116.9239

Epoch 40: val_loss did not improve from 56.12330
196/196 - 114s - loss: 118.8525 - MinusLogProbMetric: 118.8525 - val_loss: 116.9239 - val_MinusLogProbMetric: 116.9239 - lr: 1.1111e-04 - 114s/epoch - 580ms/step
Epoch 41/1000
2023-10-11 00:13:01.400 
Epoch 41/1000 
	 loss: 113.5039, MinusLogProbMetric: 113.5039, val_loss: 110.5847, val_MinusLogProbMetric: 110.5847

Epoch 41: val_loss did not improve from 56.12330
196/196 - 117s - loss: 113.5039 - MinusLogProbMetric: 113.5039 - val_loss: 110.5847 - val_MinusLogProbMetric: 110.5847 - lr: 1.1111e-04 - 117s/epoch - 596ms/step
Epoch 42/1000
2023-10-11 00:14:52.664 
Epoch 42/1000 
	 loss: 104.5666, MinusLogProbMetric: 104.5666, val_loss: 95.0002, val_MinusLogProbMetric: 95.0002

Epoch 42: val_loss did not improve from 56.12330
196/196 - 111s - loss: 104.5666 - MinusLogProbMetric: 104.5666 - val_loss: 95.0002 - val_MinusLogProbMetric: 95.0002 - lr: 1.1111e-04 - 111s/epoch - 568ms/step
Epoch 43/1000
2023-10-11 00:16:38.701 
Epoch 43/1000 
	 loss: 89.4571, MinusLogProbMetric: 89.4571, val_loss: 81.6297, val_MinusLogProbMetric: 81.6297

Epoch 43: val_loss did not improve from 56.12330
196/196 - 106s - loss: 89.4571 - MinusLogProbMetric: 89.4571 - val_loss: 81.6297 - val_MinusLogProbMetric: 81.6297 - lr: 1.1111e-04 - 106s/epoch - 541ms/step
Epoch 44/1000
2023-10-11 00:18:30.554 
Epoch 44/1000 
	 loss: 75.8436, MinusLogProbMetric: 75.8436, val_loss: 71.6494, val_MinusLogProbMetric: 71.6494

Epoch 44: val_loss did not improve from 56.12330
196/196 - 112s - loss: 75.8436 - MinusLogProbMetric: 75.8436 - val_loss: 71.6494 - val_MinusLogProbMetric: 71.6494 - lr: 1.1111e-04 - 112s/epoch - 571ms/step
Epoch 45/1000
2023-10-11 00:20:25.430 
Epoch 45/1000 
	 loss: 70.3130, MinusLogProbMetric: 70.3130, val_loss: 68.9746, val_MinusLogProbMetric: 68.9746

Epoch 45: val_loss did not improve from 56.12330
196/196 - 115s - loss: 70.3130 - MinusLogProbMetric: 70.3130 - val_loss: 68.9746 - val_MinusLogProbMetric: 68.9746 - lr: 1.1111e-04 - 115s/epoch - 586ms/step
Epoch 46/1000
2023-10-11 00:22:22.890 
Epoch 46/1000 
	 loss: 72.6772, MinusLogProbMetric: 72.6772, val_loss: 80.1937, val_MinusLogProbMetric: 80.1937

Epoch 46: val_loss did not improve from 56.12330
196/196 - 117s - loss: 72.6772 - MinusLogProbMetric: 72.6772 - val_loss: 80.1937 - val_MinusLogProbMetric: 80.1937 - lr: 1.1111e-04 - 117s/epoch - 599ms/step
Epoch 47/1000
2023-10-11 00:24:14.468 
Epoch 47/1000 
	 loss: 70.7239, MinusLogProbMetric: 70.7239, val_loss: 67.8627, val_MinusLogProbMetric: 67.8627

Epoch 47: val_loss did not improve from 56.12330
196/196 - 112s - loss: 70.7239 - MinusLogProbMetric: 70.7239 - val_loss: 67.8627 - val_MinusLogProbMetric: 67.8627 - lr: 1.1111e-04 - 112s/epoch - 569ms/step
Epoch 48/1000
2023-10-11 00:26:08.917 
Epoch 48/1000 
	 loss: 87.4607, MinusLogProbMetric: 87.4607, val_loss: 68.7609, val_MinusLogProbMetric: 68.7609

Epoch 48: val_loss did not improve from 56.12330
196/196 - 114s - loss: 87.4607 - MinusLogProbMetric: 87.4607 - val_loss: 68.7609 - val_MinusLogProbMetric: 68.7609 - lr: 1.1111e-04 - 114s/epoch - 584ms/step
Epoch 49/1000
2023-10-11 00:28:04.321 
Epoch 49/1000 
	 loss: 55.0552, MinusLogProbMetric: 55.0552, val_loss: 49.9406, val_MinusLogProbMetric: 49.9406

Epoch 49: val_loss improved from 56.12330 to 49.94059, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 119s - loss: 55.0552 - MinusLogProbMetric: 55.0552 - val_loss: 49.9406 - val_MinusLogProbMetric: 49.9406 - lr: 1.1111e-04 - 119s/epoch - 605ms/step
Epoch 50/1000
2023-10-11 00:29:58.833 
Epoch 50/1000 
	 loss: 47.7851, MinusLogProbMetric: 47.7851, val_loss: 45.7749, val_MinusLogProbMetric: 45.7749

Epoch 50: val_loss improved from 49.94059 to 45.77486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 114s - loss: 47.7851 - MinusLogProbMetric: 47.7851 - val_loss: 45.7749 - val_MinusLogProbMetric: 45.7749 - lr: 1.1111e-04 - 114s/epoch - 582ms/step
Epoch 51/1000
2023-10-11 00:31:50.375 
Epoch 51/1000 
	 loss: 44.8219, MinusLogProbMetric: 44.8219, val_loss: 43.8412, val_MinusLogProbMetric: 43.8412

Epoch 51: val_loss improved from 45.77486 to 43.84117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 110s - loss: 44.8219 - MinusLogProbMetric: 44.8219 - val_loss: 43.8412 - val_MinusLogProbMetric: 43.8412 - lr: 1.1111e-04 - 110s/epoch - 560ms/step
Epoch 52/1000
2023-10-11 00:33:38.750 
Epoch 52/1000 
	 loss: 43.1624, MinusLogProbMetric: 43.1624, val_loss: 42.3064, val_MinusLogProbMetric: 42.3064

Epoch 52: val_loss improved from 43.84117 to 42.30642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 110s - loss: 43.1624 - MinusLogProbMetric: 43.1624 - val_loss: 42.3064 - val_MinusLogProbMetric: 42.3064 - lr: 1.1111e-04 - 110s/epoch - 561ms/step
Epoch 53/1000
2023-10-11 00:35:27.667 
Epoch 53/1000 
	 loss: 41.8644, MinusLogProbMetric: 41.8644, val_loss: 41.2710, val_MinusLogProbMetric: 41.2710

Epoch 53: val_loss improved from 42.30642 to 41.27096, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 108s - loss: 41.8644 - MinusLogProbMetric: 41.8644 - val_loss: 41.2710 - val_MinusLogProbMetric: 41.2710 - lr: 1.1111e-04 - 108s/epoch - 552ms/step
Epoch 54/1000
2023-10-11 00:37:23.070 
Epoch 54/1000 
	 loss: 40.8782, MinusLogProbMetric: 40.8782, val_loss: 40.5536, val_MinusLogProbMetric: 40.5536

Epoch 54: val_loss improved from 41.27096 to 40.55364, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 114s - loss: 40.8782 - MinusLogProbMetric: 40.8782 - val_loss: 40.5536 - val_MinusLogProbMetric: 40.5536 - lr: 1.1111e-04 - 114s/epoch - 584ms/step
Epoch 55/1000
2023-10-11 00:39:18.355 
Epoch 55/1000 
	 loss: 40.2388, MinusLogProbMetric: 40.2388, val_loss: 39.9473, val_MinusLogProbMetric: 39.9473

Epoch 55: val_loss improved from 40.55364 to 39.94731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 116s - loss: 40.2388 - MinusLogProbMetric: 40.2388 - val_loss: 39.9473 - val_MinusLogProbMetric: 39.9473 - lr: 1.1111e-04 - 116s/epoch - 594ms/step
Epoch 56/1000
2023-10-11 00:41:12.941 
Epoch 56/1000 
	 loss: 71.7333, MinusLogProbMetric: 71.7333, val_loss: 82.5548, val_MinusLogProbMetric: 82.5548

Epoch 56: val_loss did not improve from 39.94731
196/196 - 113s - loss: 71.7333 - MinusLogProbMetric: 71.7333 - val_loss: 82.5548 - val_MinusLogProbMetric: 82.5548 - lr: 1.1111e-04 - 113s/epoch - 574ms/step
Epoch 57/1000
2023-10-11 00:43:09.015 
Epoch 57/1000 
	 loss: 75.7648, MinusLogProbMetric: 75.7648, val_loss: 71.1359, val_MinusLogProbMetric: 71.1359

Epoch 57: val_loss did not improve from 39.94731
196/196 - 116s - loss: 75.7648 - MinusLogProbMetric: 75.7648 - val_loss: 71.1359 - val_MinusLogProbMetric: 71.1359 - lr: 1.1111e-04 - 116s/epoch - 592ms/step
Epoch 58/1000
2023-10-11 00:44:59.623 
Epoch 58/1000 
	 loss: 69.5266, MinusLogProbMetric: 69.5266, val_loss: 67.7354, val_MinusLogProbMetric: 67.7354

Epoch 58: val_loss did not improve from 39.94731
196/196 - 111s - loss: 69.5266 - MinusLogProbMetric: 69.5266 - val_loss: 67.7354 - val_MinusLogProbMetric: 67.7354 - lr: 1.1111e-04 - 111s/epoch - 564ms/step
Epoch 59/1000
2023-10-11 00:46:52.254 
Epoch 59/1000 
	 loss: 67.1157, MinusLogProbMetric: 67.1157, val_loss: 72.0856, val_MinusLogProbMetric: 72.0856

Epoch 59: val_loss did not improve from 39.94731
196/196 - 113s - loss: 67.1157 - MinusLogProbMetric: 67.1157 - val_loss: 72.0856 - val_MinusLogProbMetric: 72.0856 - lr: 1.1111e-04 - 113s/epoch - 575ms/step
Epoch 60/1000
2023-10-11 00:48:47.711 
Epoch 60/1000 
	 loss: 60.7704, MinusLogProbMetric: 60.7704, val_loss: 43.6048, val_MinusLogProbMetric: 43.6048

Epoch 60: val_loss did not improve from 39.94731
196/196 - 115s - loss: 60.7704 - MinusLogProbMetric: 60.7704 - val_loss: 43.6048 - val_MinusLogProbMetric: 43.6048 - lr: 1.1111e-04 - 115s/epoch - 589ms/step
Epoch 61/1000
2023-10-11 00:50:37.563 
Epoch 61/1000 
	 loss: 41.7087, MinusLogProbMetric: 41.7087, val_loss: 40.6012, val_MinusLogProbMetric: 40.6012

Epoch 61: val_loss did not improve from 39.94731
196/196 - 110s - loss: 41.7087 - MinusLogProbMetric: 41.7087 - val_loss: 40.6012 - val_MinusLogProbMetric: 40.6012 - lr: 1.1111e-04 - 110s/epoch - 560ms/step
Epoch 62/1000
2023-10-11 00:52:30.010 
Epoch 62/1000 
	 loss: 39.6883, MinusLogProbMetric: 39.6883, val_loss: 38.9223, val_MinusLogProbMetric: 38.9223

Epoch 62: val_loss improved from 39.94731 to 38.92233, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 39.6883 - MinusLogProbMetric: 39.6883 - val_loss: 38.9223 - val_MinusLogProbMetric: 38.9223 - lr: 1.1111e-04 - 115s/epoch - 586ms/step
Epoch 63/1000
2023-10-11 00:54:26.885 
Epoch 63/1000 
	 loss: 38.5105, MinusLogProbMetric: 38.5105, val_loss: 37.9894, val_MinusLogProbMetric: 37.9894

Epoch 63: val_loss improved from 38.92233 to 37.98943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 38.5105 - MinusLogProbMetric: 38.5105 - val_loss: 37.9894 - val_MinusLogProbMetric: 37.9894 - lr: 1.1111e-04 - 115s/epoch - 587ms/step
Epoch 64/1000
2023-10-11 00:56:21.186 
Epoch 64/1000 
	 loss: 37.7362, MinusLogProbMetric: 37.7362, val_loss: 37.3271, val_MinusLogProbMetric: 37.3271

Epoch 64: val_loss improved from 37.98943 to 37.32713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 37.7362 - MinusLogProbMetric: 37.7362 - val_loss: 37.3271 - val_MinusLogProbMetric: 37.3271 - lr: 1.1111e-04 - 115s/epoch - 588ms/step
Epoch 65/1000
2023-10-11 00:58:16.627 
Epoch 65/1000 
	 loss: 37.1066, MinusLogProbMetric: 37.1066, val_loss: 36.8115, val_MinusLogProbMetric: 36.8115

Epoch 65: val_loss improved from 37.32713 to 36.81153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 37.1066 - MinusLogProbMetric: 37.1066 - val_loss: 36.8115 - val_MinusLogProbMetric: 36.8115 - lr: 1.1111e-04 - 115s/epoch - 584ms/step
Epoch 66/1000
2023-10-11 01:00:11.613 
Epoch 66/1000 
	 loss: 36.6161, MinusLogProbMetric: 36.6161, val_loss: 36.4757, val_MinusLogProbMetric: 36.4757

Epoch 66: val_loss improved from 36.81153 to 36.47568, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 116s - loss: 36.6161 - MinusLogProbMetric: 36.6161 - val_loss: 36.4757 - val_MinusLogProbMetric: 36.4757 - lr: 1.1111e-04 - 116s/epoch - 594ms/step
Epoch 67/1000
2023-10-11 01:02:07.341 
Epoch 67/1000 
	 loss: 36.4177, MinusLogProbMetric: 36.4177, val_loss: 41.1592, val_MinusLogProbMetric: 41.1592

Epoch 67: val_loss did not improve from 36.47568
196/196 - 114s - loss: 36.4177 - MinusLogProbMetric: 36.4177 - val_loss: 41.1592 - val_MinusLogProbMetric: 41.1592 - lr: 1.1111e-04 - 114s/epoch - 579ms/step
Epoch 68/1000
2023-10-11 01:04:00.825 
Epoch 68/1000 
	 loss: 36.4352, MinusLogProbMetric: 36.4352, val_loss: 35.7637, val_MinusLogProbMetric: 35.7637

Epoch 68: val_loss improved from 36.47568 to 35.76371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 36.4352 - MinusLogProbMetric: 36.4352 - val_loss: 35.7637 - val_MinusLogProbMetric: 35.7637 - lr: 1.1111e-04 - 115s/epoch - 587ms/step
Epoch 69/1000
2023-10-11 01:05:55.271 
Epoch 69/1000 
	 loss: 35.7577, MinusLogProbMetric: 35.7577, val_loss: 36.7565, val_MinusLogProbMetric: 36.7565

Epoch 69: val_loss did not improve from 35.76371
196/196 - 113s - loss: 35.7577 - MinusLogProbMetric: 35.7577 - val_loss: 36.7565 - val_MinusLogProbMetric: 36.7565 - lr: 1.1111e-04 - 113s/epoch - 576ms/step
Epoch 70/1000
2023-10-11 01:07:49.404 
Epoch 70/1000 
	 loss: 38.8053, MinusLogProbMetric: 38.8053, val_loss: 38.5638, val_MinusLogProbMetric: 38.5638

Epoch 70: val_loss did not improve from 35.76371
196/196 - 114s - loss: 38.8053 - MinusLogProbMetric: 38.8053 - val_loss: 38.5638 - val_MinusLogProbMetric: 38.5638 - lr: 1.1111e-04 - 114s/epoch - 582ms/step
Epoch 71/1000
2023-10-11 01:09:35.878 
Epoch 71/1000 
	 loss: 35.5403, MinusLogProbMetric: 35.5403, val_loss: 35.0622, val_MinusLogProbMetric: 35.0622

Epoch 71: val_loss improved from 35.76371 to 35.06216, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 108s - loss: 35.5403 - MinusLogProbMetric: 35.5403 - val_loss: 35.0622 - val_MinusLogProbMetric: 35.0622 - lr: 1.1111e-04 - 108s/epoch - 550ms/step
Epoch 72/1000
2023-10-11 01:11:30.679 
Epoch 72/1000 
	 loss: 34.8350, MinusLogProbMetric: 34.8350, val_loss: 34.6961, val_MinusLogProbMetric: 34.6961

Epoch 72: val_loss improved from 35.06216 to 34.69612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 34.8350 - MinusLogProbMetric: 34.8350 - val_loss: 34.6961 - val_MinusLogProbMetric: 34.6961 - lr: 1.1111e-04 - 115s/epoch - 587ms/step
Epoch 73/1000
2023-10-11 01:13:27.679 
Epoch 73/1000 
	 loss: 35.9536, MinusLogProbMetric: 35.9536, val_loss: 34.4638, val_MinusLogProbMetric: 34.4638

Epoch 73: val_loss improved from 34.69612 to 34.46376, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 117s - loss: 35.9536 - MinusLogProbMetric: 35.9536 - val_loss: 34.4638 - val_MinusLogProbMetric: 34.4638 - lr: 1.1111e-04 - 117s/epoch - 596ms/step
Epoch 74/1000
2023-10-11 01:15:20.491 
Epoch 74/1000 
	 loss: 34.9436, MinusLogProbMetric: 34.9436, val_loss: 34.2693, val_MinusLogProbMetric: 34.2693

Epoch 74: val_loss improved from 34.46376 to 34.26931, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 114s - loss: 34.9436 - MinusLogProbMetric: 34.9436 - val_loss: 34.2693 - val_MinusLogProbMetric: 34.2693 - lr: 1.1111e-04 - 114s/epoch - 580ms/step
Epoch 75/1000
2023-10-11 01:17:16.562 
Epoch 75/1000 
	 loss: 36.7435, MinusLogProbMetric: 36.7435, val_loss: 39.6792, val_MinusLogProbMetric: 39.6792

Epoch 75: val_loss did not improve from 34.26931
196/196 - 114s - loss: 36.7435 - MinusLogProbMetric: 36.7435 - val_loss: 39.6792 - val_MinusLogProbMetric: 39.6792 - lr: 1.1111e-04 - 114s/epoch - 581ms/step
Epoch 76/1000
2023-10-11 01:19:12.871 
Epoch 76/1000 
	 loss: 34.4197, MinusLogProbMetric: 34.4197, val_loss: 34.1562, val_MinusLogProbMetric: 34.1562

Epoch 76: val_loss improved from 34.26931 to 34.15619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 117s - loss: 34.4197 - MinusLogProbMetric: 34.4197 - val_loss: 34.1562 - val_MinusLogProbMetric: 34.1562 - lr: 1.1111e-04 - 117s/epoch - 599ms/step
Epoch 77/1000
2023-10-11 01:21:07.974 
Epoch 77/1000 
	 loss: 38.8412, MinusLogProbMetric: 38.8412, val_loss: 34.4125, val_MinusLogProbMetric: 34.4125

Epoch 77: val_loss did not improve from 34.15619
196/196 - 114s - loss: 38.8412 - MinusLogProbMetric: 38.8412 - val_loss: 34.4125 - val_MinusLogProbMetric: 34.4125 - lr: 1.1111e-04 - 114s/epoch - 582ms/step
Epoch 78/1000
2023-10-11 01:23:01.624 
Epoch 78/1000 
	 loss: 34.0531, MinusLogProbMetric: 34.0531, val_loss: 33.9813, val_MinusLogProbMetric: 33.9813

Epoch 78: val_loss improved from 34.15619 to 33.98130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 34.0531 - MinusLogProbMetric: 34.0531 - val_loss: 33.9813 - val_MinusLogProbMetric: 33.9813 - lr: 1.1111e-04 - 115s/epoch - 589ms/step
Epoch 79/1000
2023-10-11 01:25:01.109 
Epoch 79/1000 
	 loss: 34.2337, MinusLogProbMetric: 34.2337, val_loss: 34.5412, val_MinusLogProbMetric: 34.5412

Epoch 79: val_loss did not improve from 33.98130
196/196 - 118s - loss: 34.2337 - MinusLogProbMetric: 34.2337 - val_loss: 34.5412 - val_MinusLogProbMetric: 34.5412 - lr: 1.1111e-04 - 118s/epoch - 600ms/step
Epoch 80/1000
2023-10-11 01:26:53.508 
Epoch 80/1000 
	 loss: 33.9267, MinusLogProbMetric: 33.9267, val_loss: 33.7430, val_MinusLogProbMetric: 33.7430

Epoch 80: val_loss improved from 33.98130 to 33.74300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 33.9267 - MinusLogProbMetric: 33.9267 - val_loss: 33.7430 - val_MinusLogProbMetric: 33.7430 - lr: 1.1111e-04 - 115s/epoch - 589ms/step
Epoch 81/1000
2023-10-11 01:28:48.836 
Epoch 81/1000 
	 loss: 33.9571, MinusLogProbMetric: 33.9571, val_loss: 33.6479, val_MinusLogProbMetric: 33.6479

Epoch 81: val_loss improved from 33.74300 to 33.64787, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 114s - loss: 33.9571 - MinusLogProbMetric: 33.9571 - val_loss: 33.6479 - val_MinusLogProbMetric: 33.6479 - lr: 1.1111e-04 - 114s/epoch - 584ms/step
Epoch 82/1000
2023-10-11 01:30:37.929 
Epoch 82/1000 
	 loss: 33.4889, MinusLogProbMetric: 33.4889, val_loss: 33.1248, val_MinusLogProbMetric: 33.1248

Epoch 82: val_loss improved from 33.64787 to 33.12483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 109s - loss: 33.4889 - MinusLogProbMetric: 33.4889 - val_loss: 33.1248 - val_MinusLogProbMetric: 33.1248 - lr: 1.1111e-04 - 109s/epoch - 558ms/step
Epoch 83/1000
2023-10-11 01:32:35.277 
Epoch 83/1000 
	 loss: 33.3820, MinusLogProbMetric: 33.3820, val_loss: 33.7870, val_MinusLogProbMetric: 33.7870

Epoch 83: val_loss did not improve from 33.12483
196/196 - 115s - loss: 33.3820 - MinusLogProbMetric: 33.3820 - val_loss: 33.7870 - val_MinusLogProbMetric: 33.7870 - lr: 1.1111e-04 - 115s/epoch - 586ms/step
Epoch 84/1000
2023-10-11 01:34:28.900 
Epoch 84/1000 
	 loss: 33.4309, MinusLogProbMetric: 33.4309, val_loss: 32.6835, val_MinusLogProbMetric: 32.6835

Epoch 84: val_loss improved from 33.12483 to 32.68354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 117s - loss: 33.4309 - MinusLogProbMetric: 33.4309 - val_loss: 32.6835 - val_MinusLogProbMetric: 32.6835 - lr: 1.1111e-04 - 117s/epoch - 599ms/step
Epoch 85/1000
2023-10-11 01:36:22.853 
Epoch 85/1000 
	 loss: 32.1221, MinusLogProbMetric: 32.1221, val_loss: 30.9697, val_MinusLogProbMetric: 30.9697

Epoch 85: val_loss improved from 32.68354 to 30.96971, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 113s - loss: 32.1221 - MinusLogProbMetric: 32.1221 - val_loss: 30.9697 - val_MinusLogProbMetric: 30.9697 - lr: 1.1111e-04 - 113s/epoch - 576ms/step
Epoch 86/1000
2023-10-11 01:38:16.676 
Epoch 86/1000 
	 loss: 30.1133, MinusLogProbMetric: 30.1133, val_loss: 29.5128, val_MinusLogProbMetric: 29.5128

Epoch 86: val_loss improved from 30.96971 to 29.51277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 114s - loss: 30.1133 - MinusLogProbMetric: 30.1133 - val_loss: 29.5128 - val_MinusLogProbMetric: 29.5128 - lr: 1.1111e-04 - 114s/epoch - 581ms/step
Epoch 87/1000
2023-10-11 01:40:09.911 
Epoch 87/1000 
	 loss: 28.3174, MinusLogProbMetric: 28.3174, val_loss: 27.7043, val_MinusLogProbMetric: 27.7043

Epoch 87: val_loss improved from 29.51277 to 27.70434, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 112s - loss: 28.3174 - MinusLogProbMetric: 28.3174 - val_loss: 27.7043 - val_MinusLogProbMetric: 27.7043 - lr: 1.1111e-04 - 112s/epoch - 572ms/step
Epoch 88/1000
2023-10-11 01:42:00.626 
Epoch 88/1000 
	 loss: 27.2154, MinusLogProbMetric: 27.2154, val_loss: 26.9961, val_MinusLogProbMetric: 26.9961

Epoch 88: val_loss improved from 27.70434 to 26.99614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 112s - loss: 27.2154 - MinusLogProbMetric: 27.2154 - val_loss: 26.9961 - val_MinusLogProbMetric: 26.9961 - lr: 1.1111e-04 - 112s/epoch - 571ms/step
Epoch 89/1000
2023-10-11 01:43:55.048 
Epoch 89/1000 
	 loss: 26.5658, MinusLogProbMetric: 26.5658, val_loss: 26.3436, val_MinusLogProbMetric: 26.3436

Epoch 89: val_loss improved from 26.99614 to 26.34358, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 26.5658 - MinusLogProbMetric: 26.5658 - val_loss: 26.3436 - val_MinusLogProbMetric: 26.3436 - lr: 1.1111e-04 - 115s/epoch - 587ms/step
Epoch 90/1000
2023-10-11 01:45:55.825 
Epoch 90/1000 
	 loss: 26.1360, MinusLogProbMetric: 26.1360, val_loss: 25.9496, val_MinusLogProbMetric: 25.9496

Epoch 90: val_loss improved from 26.34358 to 25.94959, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 120s - loss: 26.1360 - MinusLogProbMetric: 26.1360 - val_loss: 25.9496 - val_MinusLogProbMetric: 25.9496 - lr: 1.1111e-04 - 120s/epoch - 611ms/step
Epoch 91/1000
2023-10-11 01:47:52.885 
Epoch 91/1000 
	 loss: 25.6879, MinusLogProbMetric: 25.6879, val_loss: 25.7180, val_MinusLogProbMetric: 25.7180

Epoch 91: val_loss improved from 25.94959 to 25.71796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 116s - loss: 25.6879 - MinusLogProbMetric: 25.6879 - val_loss: 25.7180 - val_MinusLogProbMetric: 25.7180 - lr: 1.1111e-04 - 116s/epoch - 593ms/step
Epoch 92/1000
2023-10-11 01:49:46.485 
Epoch 92/1000 
	 loss: 25.3544, MinusLogProbMetric: 25.3544, val_loss: 25.1161, val_MinusLogProbMetric: 25.1161

Epoch 92: val_loss improved from 25.71796 to 25.11612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 113s - loss: 25.3544 - MinusLogProbMetric: 25.3544 - val_loss: 25.1161 - val_MinusLogProbMetric: 25.1161 - lr: 1.1111e-04 - 113s/epoch - 579ms/step
Epoch 93/1000
2023-10-11 01:51:40.263 
Epoch 93/1000 
	 loss: 24.9935, MinusLogProbMetric: 24.9935, val_loss: 24.9524, val_MinusLogProbMetric: 24.9524

Epoch 93: val_loss improved from 25.11612 to 24.95244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 24.9935 - MinusLogProbMetric: 24.9935 - val_loss: 24.9524 - val_MinusLogProbMetric: 24.9524 - lr: 1.1111e-04 - 115s/epoch - 585ms/step
Epoch 94/1000
2023-10-11 01:53:29.528 
Epoch 94/1000 
	 loss: 24.7567, MinusLogProbMetric: 24.7567, val_loss: 24.4570, val_MinusLogProbMetric: 24.4570

Epoch 94: val_loss improved from 24.95244 to 24.45697, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 108s - loss: 24.7567 - MinusLogProbMetric: 24.7567 - val_loss: 24.4570 - val_MinusLogProbMetric: 24.4570 - lr: 1.1111e-04 - 108s/epoch - 549ms/step
Epoch 95/1000
2023-10-11 01:55:17.333 
Epoch 95/1000 
	 loss: 24.6134, MinusLogProbMetric: 24.6134, val_loss: 24.3907, val_MinusLogProbMetric: 24.3907

Epoch 95: val_loss improved from 24.45697 to 24.39068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 110s - loss: 24.6134 - MinusLogProbMetric: 24.6134 - val_loss: 24.3907 - val_MinusLogProbMetric: 24.3907 - lr: 1.1111e-04 - 110s/epoch - 561ms/step
Epoch 96/1000
2023-10-11 01:57:05.240 
Epoch 96/1000 
	 loss: 24.2842, MinusLogProbMetric: 24.2842, val_loss: 24.3609, val_MinusLogProbMetric: 24.3609

Epoch 96: val_loss improved from 24.39068 to 24.36089, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 107s - loss: 24.2842 - MinusLogProbMetric: 24.2842 - val_loss: 24.3609 - val_MinusLogProbMetric: 24.3609 - lr: 1.1111e-04 - 107s/epoch - 547ms/step
Epoch 97/1000
2023-10-11 01:58:57.106 
Epoch 97/1000 
	 loss: 24.0581, MinusLogProbMetric: 24.0581, val_loss: 23.8517, val_MinusLogProbMetric: 23.8517

Epoch 97: val_loss improved from 24.36089 to 23.85172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 111s - loss: 24.0581 - MinusLogProbMetric: 24.0581 - val_loss: 23.8517 - val_MinusLogProbMetric: 23.8517 - lr: 1.1111e-04 - 111s/epoch - 568ms/step
Epoch 98/1000
2023-10-11 02:00:49.252 
Epoch 98/1000 
	 loss: 24.3388, MinusLogProbMetric: 24.3388, val_loss: 24.4965, val_MinusLogProbMetric: 24.4965

Epoch 98: val_loss did not improve from 23.85172
196/196 - 111s - loss: 24.3388 - MinusLogProbMetric: 24.3388 - val_loss: 24.4965 - val_MinusLogProbMetric: 24.4965 - lr: 1.1111e-04 - 111s/epoch - 564ms/step
Epoch 99/1000
2023-10-11 02:02:36.401 
Epoch 99/1000 
	 loss: 24.2683, MinusLogProbMetric: 24.2683, val_loss: 26.8378, val_MinusLogProbMetric: 26.8378

Epoch 99: val_loss did not improve from 23.85172
196/196 - 107s - loss: 24.2683 - MinusLogProbMetric: 24.2683 - val_loss: 26.8378 - val_MinusLogProbMetric: 26.8378 - lr: 1.1111e-04 - 107s/epoch - 546ms/step
Epoch 100/1000
2023-10-11 02:04:20.344 
Epoch 100/1000 
	 loss: 23.9870, MinusLogProbMetric: 23.9870, val_loss: 24.9525, val_MinusLogProbMetric: 24.9525

Epoch 100: val_loss did not improve from 23.85172
196/196 - 104s - loss: 23.9870 - MinusLogProbMetric: 23.9870 - val_loss: 24.9525 - val_MinusLogProbMetric: 24.9525 - lr: 1.1111e-04 - 104s/epoch - 530ms/step
Epoch 101/1000
2023-10-11 02:06:04.057 
Epoch 101/1000 
	 loss: 46.2328, MinusLogProbMetric: 46.2328, val_loss: 107.4997, val_MinusLogProbMetric: 107.4997

Epoch 101: val_loss did not improve from 23.85172
196/196 - 104s - loss: 46.2328 - MinusLogProbMetric: 46.2328 - val_loss: 107.4997 - val_MinusLogProbMetric: 107.4997 - lr: 1.1111e-04 - 104s/epoch - 529ms/step
Epoch 102/1000
2023-10-11 02:07:51.116 
Epoch 102/1000 
	 loss: 66.3800, MinusLogProbMetric: 66.3800, val_loss: 51.0280, val_MinusLogProbMetric: 51.0280

Epoch 102: val_loss did not improve from 23.85172
196/196 - 107s - loss: 66.3800 - MinusLogProbMetric: 66.3800 - val_loss: 51.0280 - val_MinusLogProbMetric: 51.0280 - lr: 1.1111e-04 - 107s/epoch - 546ms/step
Epoch 103/1000
2023-10-11 02:09:41.042 
Epoch 103/1000 
	 loss: 45.3464, MinusLogProbMetric: 45.3464, val_loss: 41.5176, val_MinusLogProbMetric: 41.5176

Epoch 103: val_loss did not improve from 23.85172
196/196 - 110s - loss: 45.3464 - MinusLogProbMetric: 45.3464 - val_loss: 41.5176 - val_MinusLogProbMetric: 41.5176 - lr: 1.1111e-04 - 110s/epoch - 561ms/step
Epoch 104/1000
2023-10-11 02:11:29.612 
Epoch 104/1000 
	 loss: 38.5550, MinusLogProbMetric: 38.5550, val_loss: 36.4228, val_MinusLogProbMetric: 36.4228

Epoch 104: val_loss did not improve from 23.85172
196/196 - 109s - loss: 38.5550 - MinusLogProbMetric: 38.5550 - val_loss: 36.4228 - val_MinusLogProbMetric: 36.4228 - lr: 1.1111e-04 - 109s/epoch - 554ms/step
Epoch 105/1000
2023-10-11 02:13:15.387 
Epoch 105/1000 
	 loss: 35.9244, MinusLogProbMetric: 35.9244, val_loss: 34.2320, val_MinusLogProbMetric: 34.2320

Epoch 105: val_loss did not improve from 23.85172
196/196 - 106s - loss: 35.9244 - MinusLogProbMetric: 35.9244 - val_loss: 34.2320 - val_MinusLogProbMetric: 34.2320 - lr: 1.1111e-04 - 106s/epoch - 540ms/step
Epoch 106/1000
2023-10-11 02:15:06.660 
Epoch 106/1000 
	 loss: 46.3993, MinusLogProbMetric: 46.3993, val_loss: 87.2927, val_MinusLogProbMetric: 87.2927

Epoch 106: val_loss did not improve from 23.85172
196/196 - 111s - loss: 46.3993 - MinusLogProbMetric: 46.3993 - val_loss: 87.2927 - val_MinusLogProbMetric: 87.2927 - lr: 1.1111e-04 - 111s/epoch - 568ms/step
Epoch 107/1000
2023-10-11 02:16:52.679 
Epoch 107/1000 
	 loss: 56.5661, MinusLogProbMetric: 56.5661, val_loss: 43.8548, val_MinusLogProbMetric: 43.8548

Epoch 107: val_loss did not improve from 23.85172
196/196 - 106s - loss: 56.5661 - MinusLogProbMetric: 56.5661 - val_loss: 43.8548 - val_MinusLogProbMetric: 43.8548 - lr: 1.1111e-04 - 106s/epoch - 541ms/step
Epoch 108/1000
2023-10-11 02:18:38.204 
Epoch 108/1000 
	 loss: 38.9909, MinusLogProbMetric: 38.9909, val_loss: 35.3878, val_MinusLogProbMetric: 35.3878

Epoch 108: val_loss did not improve from 23.85172
196/196 - 106s - loss: 38.9909 - MinusLogProbMetric: 38.9909 - val_loss: 35.3878 - val_MinusLogProbMetric: 35.3878 - lr: 1.1111e-04 - 106s/epoch - 538ms/step
Epoch 109/1000
2023-10-11 02:20:24.814 
Epoch 109/1000 
	 loss: 33.5540, MinusLogProbMetric: 33.5540, val_loss: 32.2725, val_MinusLogProbMetric: 32.2725

Epoch 109: val_loss did not improve from 23.85172
196/196 - 107s - loss: 33.5540 - MinusLogProbMetric: 33.5540 - val_loss: 32.2725 - val_MinusLogProbMetric: 32.2725 - lr: 1.1111e-04 - 107s/epoch - 544ms/step
Epoch 110/1000
2023-10-11 02:22:18.054 
Epoch 110/1000 
	 loss: 31.1737, MinusLogProbMetric: 31.1737, val_loss: 30.4927, val_MinusLogProbMetric: 30.4927

Epoch 110: val_loss did not improve from 23.85172
196/196 - 113s - loss: 31.1737 - MinusLogProbMetric: 31.1737 - val_loss: 30.4927 - val_MinusLogProbMetric: 30.4927 - lr: 1.1111e-04 - 113s/epoch - 578ms/step
Epoch 111/1000
2023-10-11 02:24:03.684 
Epoch 111/1000 
	 loss: 31.2303, MinusLogProbMetric: 31.2303, val_loss: 29.5115, val_MinusLogProbMetric: 29.5115

Epoch 111: val_loss did not improve from 23.85172
196/196 - 106s - loss: 31.2303 - MinusLogProbMetric: 31.2303 - val_loss: 29.5115 - val_MinusLogProbMetric: 29.5115 - lr: 1.1111e-04 - 106s/epoch - 539ms/step
Epoch 112/1000
2023-10-11 02:25:55.157 
Epoch 112/1000 
	 loss: 29.0174, MinusLogProbMetric: 29.0174, val_loss: 28.7808, val_MinusLogProbMetric: 28.7808

Epoch 112: val_loss did not improve from 23.85172
196/196 - 111s - loss: 29.0174 - MinusLogProbMetric: 29.0174 - val_loss: 28.7808 - val_MinusLogProbMetric: 28.7808 - lr: 1.1111e-04 - 111s/epoch - 569ms/step
Epoch 113/1000
2023-10-11 02:27:41.659 
Epoch 113/1000 
	 loss: 35.0816, MinusLogProbMetric: 35.0816, val_loss: 72.2848, val_MinusLogProbMetric: 72.2848

Epoch 113: val_loss did not improve from 23.85172
196/196 - 107s - loss: 35.0816 - MinusLogProbMetric: 35.0816 - val_loss: 72.2848 - val_MinusLogProbMetric: 72.2848 - lr: 1.1111e-04 - 107s/epoch - 543ms/step
Epoch 114/1000
2023-10-11 02:29:38.052 
Epoch 114/1000 
	 loss: 69.8415, MinusLogProbMetric: 69.8415, val_loss: 46.9544, val_MinusLogProbMetric: 46.9544

Epoch 114: val_loss did not improve from 23.85172
196/196 - 116s - loss: 69.8415 - MinusLogProbMetric: 69.8415 - val_loss: 46.9544 - val_MinusLogProbMetric: 46.9544 - lr: 1.1111e-04 - 116s/epoch - 594ms/step
Epoch 115/1000
2023-10-11 02:31:31.393 
Epoch 115/1000 
	 loss: 40.6404, MinusLogProbMetric: 40.6404, val_loss: 35.4520, val_MinusLogProbMetric: 35.4520

Epoch 115: val_loss did not improve from 23.85172
196/196 - 113s - loss: 40.6404 - MinusLogProbMetric: 40.6404 - val_loss: 35.4520 - val_MinusLogProbMetric: 35.4520 - lr: 1.1111e-04 - 113s/epoch - 578ms/step
Epoch 116/1000
2023-10-11 02:33:20.250 
Epoch 116/1000 
	 loss: 39.7330, MinusLogProbMetric: 39.7330, val_loss: 193.5720, val_MinusLogProbMetric: 193.5720

Epoch 116: val_loss did not improve from 23.85172
196/196 - 109s - loss: 39.7330 - MinusLogProbMetric: 39.7330 - val_loss: 193.5720 - val_MinusLogProbMetric: 193.5720 - lr: 1.1111e-04 - 109s/epoch - 555ms/step
Epoch 117/1000
2023-10-11 02:35:15.121 
Epoch 117/1000 
	 loss: 66.8173, MinusLogProbMetric: 66.8173, val_loss: 46.6787, val_MinusLogProbMetric: 46.6787

Epoch 117: val_loss did not improve from 23.85172
196/196 - 115s - loss: 66.8173 - MinusLogProbMetric: 66.8173 - val_loss: 46.6787 - val_MinusLogProbMetric: 46.6787 - lr: 1.1111e-04 - 115s/epoch - 586ms/step
Epoch 118/1000
2023-10-11 02:37:07.590 
Epoch 118/1000 
	 loss: 41.8224, MinusLogProbMetric: 41.8224, val_loss: 38.0526, val_MinusLogProbMetric: 38.0526

Epoch 118: val_loss did not improve from 23.85172
196/196 - 112s - loss: 41.8224 - MinusLogProbMetric: 41.8224 - val_loss: 38.0526 - val_MinusLogProbMetric: 38.0526 - lr: 1.1111e-04 - 112s/epoch - 574ms/step
Epoch 119/1000
2023-10-11 02:38:55.283 
Epoch 119/1000 
	 loss: 35.1444, MinusLogProbMetric: 35.1444, val_loss: 33.0717, val_MinusLogProbMetric: 33.0717

Epoch 119: val_loss did not improve from 23.85172
196/196 - 108s - loss: 35.1444 - MinusLogProbMetric: 35.1444 - val_loss: 33.0717 - val_MinusLogProbMetric: 33.0717 - lr: 1.1111e-04 - 108s/epoch - 549ms/step
Epoch 120/1000
2023-10-11 02:40:48.253 
Epoch 120/1000 
	 loss: 31.5462, MinusLogProbMetric: 31.5462, val_loss: 30.4617, val_MinusLogProbMetric: 30.4617

Epoch 120: val_loss did not improve from 23.85172
196/196 - 113s - loss: 31.5462 - MinusLogProbMetric: 31.5462 - val_loss: 30.4617 - val_MinusLogProbMetric: 30.4617 - lr: 1.1111e-04 - 113s/epoch - 576ms/step
Epoch 121/1000
2023-10-11 02:42:40.190 
Epoch 121/1000 
	 loss: 29.7528, MinusLogProbMetric: 29.7528, val_loss: 29.2678, val_MinusLogProbMetric: 29.2678

Epoch 121: val_loss did not improve from 23.85172
196/196 - 112s - loss: 29.7528 - MinusLogProbMetric: 29.7528 - val_loss: 29.2678 - val_MinusLogProbMetric: 29.2678 - lr: 1.1111e-04 - 112s/epoch - 571ms/step
Epoch 122/1000
2023-10-11 02:44:29.872 
Epoch 122/1000 
	 loss: 28.5818, MinusLogProbMetric: 28.5818, val_loss: 28.4390, val_MinusLogProbMetric: 28.4390

Epoch 122: val_loss did not improve from 23.85172
196/196 - 110s - loss: 28.5818 - MinusLogProbMetric: 28.5818 - val_loss: 28.4390 - val_MinusLogProbMetric: 28.4390 - lr: 1.1111e-04 - 110s/epoch - 560ms/step
Epoch 123/1000
2023-10-11 02:46:15.861 
Epoch 123/1000 
	 loss: 27.7130, MinusLogProbMetric: 27.7130, val_loss: 27.2943, val_MinusLogProbMetric: 27.2943

Epoch 123: val_loss did not improve from 23.85172
196/196 - 106s - loss: 27.7130 - MinusLogProbMetric: 27.7130 - val_loss: 27.2943 - val_MinusLogProbMetric: 27.2943 - lr: 1.1111e-04 - 106s/epoch - 541ms/step
Epoch 124/1000
2023-10-11 02:48:08.107 
Epoch 124/1000 
	 loss: 26.9905, MinusLogProbMetric: 26.9905, val_loss: 26.7118, val_MinusLogProbMetric: 26.7118

Epoch 124: val_loss did not improve from 23.85172
196/196 - 112s - loss: 26.9905 - MinusLogProbMetric: 26.9905 - val_loss: 26.7118 - val_MinusLogProbMetric: 26.7118 - lr: 1.1111e-04 - 112s/epoch - 573ms/step
Epoch 125/1000
2023-10-11 02:49:55.061 
Epoch 125/1000 
	 loss: 26.6421, MinusLogProbMetric: 26.6421, val_loss: 26.2251, val_MinusLogProbMetric: 26.2251

Epoch 125: val_loss did not improve from 23.85172
196/196 - 107s - loss: 26.6421 - MinusLogProbMetric: 26.6421 - val_loss: 26.2251 - val_MinusLogProbMetric: 26.2251 - lr: 1.1111e-04 - 107s/epoch - 546ms/step
Epoch 126/1000
2023-10-11 02:51:45.828 
Epoch 126/1000 
	 loss: 25.9709, MinusLogProbMetric: 25.9709, val_loss: 25.8632, val_MinusLogProbMetric: 25.8632

Epoch 126: val_loss did not improve from 23.85172
196/196 - 111s - loss: 25.9709 - MinusLogProbMetric: 25.9709 - val_loss: 25.8632 - val_MinusLogProbMetric: 25.8632 - lr: 1.1111e-04 - 111s/epoch - 565ms/step
Epoch 127/1000
2023-10-11 02:53:33.174 
Epoch 127/1000 
	 loss: 25.5535, MinusLogProbMetric: 25.5535, val_loss: 25.5218, val_MinusLogProbMetric: 25.5218

Epoch 127: val_loss did not improve from 23.85172
196/196 - 107s - loss: 25.5535 - MinusLogProbMetric: 25.5535 - val_loss: 25.5218 - val_MinusLogProbMetric: 25.5218 - lr: 1.1111e-04 - 107s/epoch - 548ms/step
Epoch 128/1000
2023-10-11 02:55:22.604 
Epoch 128/1000 
	 loss: 25.1820, MinusLogProbMetric: 25.1820, val_loss: 24.9994, val_MinusLogProbMetric: 24.9994

Epoch 128: val_loss did not improve from 23.85172
196/196 - 109s - loss: 25.1820 - MinusLogProbMetric: 25.1820 - val_loss: 24.9994 - val_MinusLogProbMetric: 24.9994 - lr: 1.1111e-04 - 109s/epoch - 558ms/step
Epoch 129/1000
2023-10-11 02:57:12.212 
Epoch 129/1000 
	 loss: 24.8701, MinusLogProbMetric: 24.8701, val_loss: 24.7791, val_MinusLogProbMetric: 24.7791

Epoch 129: val_loss did not improve from 23.85172
196/196 - 110s - loss: 24.8701 - MinusLogProbMetric: 24.8701 - val_loss: 24.7791 - val_MinusLogProbMetric: 24.7791 - lr: 1.1111e-04 - 110s/epoch - 559ms/step
Epoch 130/1000
2023-10-11 02:58:54.058 
Epoch 130/1000 
	 loss: 24.5533, MinusLogProbMetric: 24.5533, val_loss: 24.4883, val_MinusLogProbMetric: 24.4883

Epoch 130: val_loss did not improve from 23.85172
196/196 - 102s - loss: 24.5533 - MinusLogProbMetric: 24.5533 - val_loss: 24.4883 - val_MinusLogProbMetric: 24.4883 - lr: 1.1111e-04 - 102s/epoch - 520ms/step
Epoch 131/1000
2023-10-11 03:00:44.866 
Epoch 131/1000 
	 loss: 24.3128, MinusLogProbMetric: 24.3128, val_loss: 24.1857, val_MinusLogProbMetric: 24.1857

Epoch 131: val_loss did not improve from 23.85172
196/196 - 111s - loss: 24.3128 - MinusLogProbMetric: 24.3128 - val_loss: 24.1857 - val_MinusLogProbMetric: 24.1857 - lr: 1.1111e-04 - 111s/epoch - 565ms/step
Epoch 132/1000
2023-10-11 03:02:36.787 
Epoch 132/1000 
	 loss: 24.0122, MinusLogProbMetric: 24.0122, val_loss: 23.9576, val_MinusLogProbMetric: 23.9576

Epoch 132: val_loss did not improve from 23.85172
196/196 - 112s - loss: 24.0122 - MinusLogProbMetric: 24.0122 - val_loss: 23.9576 - val_MinusLogProbMetric: 23.9576 - lr: 1.1111e-04 - 112s/epoch - 571ms/step
Epoch 133/1000
2023-10-11 03:04:28.072 
Epoch 133/1000 
	 loss: 23.8032, MinusLogProbMetric: 23.8032, val_loss: 23.6089, val_MinusLogProbMetric: 23.6089

Epoch 133: val_loss improved from 23.85172 to 23.60891, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 23.8032 - MinusLogProbMetric: 23.8032 - val_loss: 23.6089 - val_MinusLogProbMetric: 23.6089 - lr: 1.1111e-04 - 115s/epoch - 588ms/step
Epoch 134/1000
2023-10-11 03:06:23.258 
Epoch 134/1000 
	 loss: 23.5926, MinusLogProbMetric: 23.5926, val_loss: 23.4033, val_MinusLogProbMetric: 23.4033

Epoch 134: val_loss improved from 23.60891 to 23.40332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 112s - loss: 23.5926 - MinusLogProbMetric: 23.5926 - val_loss: 23.4033 - val_MinusLogProbMetric: 23.4033 - lr: 1.1111e-04 - 112s/epoch - 573ms/step
Epoch 135/1000
2023-10-11 03:08:19.653 
Epoch 135/1000 
	 loss: 23.3814, MinusLogProbMetric: 23.3814, val_loss: 23.2577, val_MinusLogProbMetric: 23.2577

Epoch 135: val_loss improved from 23.40332 to 23.25767, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 116s - loss: 23.3814 - MinusLogProbMetric: 23.3814 - val_loss: 23.2577 - val_MinusLogProbMetric: 23.2577 - lr: 1.1111e-04 - 116s/epoch - 591ms/step
Epoch 136/1000
2023-10-11 03:10:15.345 
Epoch 136/1000 
	 loss: 23.2911, MinusLogProbMetric: 23.2911, val_loss: 23.1126, val_MinusLogProbMetric: 23.1126

Epoch 136: val_loss improved from 23.25767 to 23.11257, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 117s - loss: 23.2911 - MinusLogProbMetric: 23.2911 - val_loss: 23.1126 - val_MinusLogProbMetric: 23.1126 - lr: 1.1111e-04 - 117s/epoch - 598ms/step
Epoch 137/1000
2023-10-11 03:12:09.792 
Epoch 137/1000 
	 loss: 23.0061, MinusLogProbMetric: 23.0061, val_loss: 22.9076, val_MinusLogProbMetric: 22.9076

Epoch 137: val_loss improved from 23.11257 to 22.90763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 113s - loss: 23.0061 - MinusLogProbMetric: 23.0061 - val_loss: 22.9076 - val_MinusLogProbMetric: 22.9076 - lr: 1.1111e-04 - 113s/epoch - 577ms/step
Epoch 138/1000
2023-10-11 03:13:58.438 
Epoch 138/1000 
	 loss: 22.8460, MinusLogProbMetric: 22.8460, val_loss: 22.8577, val_MinusLogProbMetric: 22.8577

Epoch 138: val_loss improved from 22.90763 to 22.85774, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 110s - loss: 22.8460 - MinusLogProbMetric: 22.8460 - val_loss: 22.8577 - val_MinusLogProbMetric: 22.8577 - lr: 1.1111e-04 - 110s/epoch - 560ms/step
Epoch 139/1000
2023-10-11 03:15:50.073 
Epoch 139/1000 
	 loss: 22.7261, MinusLogProbMetric: 22.7261, val_loss: 22.5851, val_MinusLogProbMetric: 22.5851

Epoch 139: val_loss improved from 22.85774 to 22.58515, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 111s - loss: 22.7261 - MinusLogProbMetric: 22.7261 - val_loss: 22.5851 - val_MinusLogProbMetric: 22.5851 - lr: 1.1111e-04 - 111s/epoch - 568ms/step
Epoch 140/1000
2023-10-11 03:17:47.462 
Epoch 140/1000 
	 loss: 22.5699, MinusLogProbMetric: 22.5699, val_loss: 22.6560, val_MinusLogProbMetric: 22.6560

Epoch 140: val_loss did not improve from 22.58515
196/196 - 116s - loss: 22.5699 - MinusLogProbMetric: 22.5699 - val_loss: 22.6560 - val_MinusLogProbMetric: 22.6560 - lr: 1.1111e-04 - 116s/epoch - 590ms/step
Epoch 141/1000
2023-10-11 03:19:36.959 
Epoch 141/1000 
	 loss: 22.5229, MinusLogProbMetric: 22.5229, val_loss: 22.3862, val_MinusLogProbMetric: 22.3862

Epoch 141: val_loss improved from 22.58515 to 22.38616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 111s - loss: 22.5229 - MinusLogProbMetric: 22.5229 - val_loss: 22.3862 - val_MinusLogProbMetric: 22.3862 - lr: 1.1111e-04 - 111s/epoch - 566ms/step
Epoch 142/1000
2023-10-11 03:21:31.613 
Epoch 142/1000 
	 loss: 22.2806, MinusLogProbMetric: 22.2806, val_loss: 22.1134, val_MinusLogProbMetric: 22.1134

Epoch 142: val_loss improved from 22.38616 to 22.11336, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 22.2806 - MinusLogProbMetric: 22.2806 - val_loss: 22.1134 - val_MinusLogProbMetric: 22.1134 - lr: 1.1111e-04 - 115s/epoch - 585ms/step
Epoch 143/1000
2023-10-11 03:23:22.063 
Epoch 143/1000 
	 loss: 22.1857, MinusLogProbMetric: 22.1857, val_loss: 22.1891, val_MinusLogProbMetric: 22.1891

Epoch 143: val_loss did not improve from 22.11336
196/196 - 109s - loss: 22.1857 - MinusLogProbMetric: 22.1857 - val_loss: 22.1891 - val_MinusLogProbMetric: 22.1891 - lr: 1.1111e-04 - 109s/epoch - 556ms/step
Epoch 144/1000
2023-10-11 03:25:10.427 
Epoch 144/1000 
	 loss: 22.0458, MinusLogProbMetric: 22.0458, val_loss: 22.1762, val_MinusLogProbMetric: 22.1762

Epoch 144: val_loss did not improve from 22.11336
196/196 - 108s - loss: 22.0458 - MinusLogProbMetric: 22.0458 - val_loss: 22.1762 - val_MinusLogProbMetric: 22.1762 - lr: 1.1111e-04 - 108s/epoch - 553ms/step
Epoch 145/1000
2023-10-11 03:27:01.355 
Epoch 145/1000 
	 loss: 29.5151, MinusLogProbMetric: 29.5151, val_loss: 24.1556, val_MinusLogProbMetric: 24.1556

Epoch 145: val_loss did not improve from 22.11336
196/196 - 111s - loss: 29.5151 - MinusLogProbMetric: 29.5151 - val_loss: 24.1556 - val_MinusLogProbMetric: 24.1556 - lr: 1.1111e-04 - 111s/epoch - 566ms/step
Epoch 146/1000
2023-10-11 03:28:48.933 
Epoch 146/1000 
	 loss: 23.4616, MinusLogProbMetric: 23.4616, val_loss: 22.9600, val_MinusLogProbMetric: 22.9600

Epoch 146: val_loss did not improve from 22.11336
196/196 - 108s - loss: 23.4616 - MinusLogProbMetric: 23.4616 - val_loss: 22.9600 - val_MinusLogProbMetric: 22.9600 - lr: 1.1111e-04 - 108s/epoch - 549ms/step
Epoch 147/1000
2023-10-11 03:30:41.430 
Epoch 147/1000 
	 loss: 22.6613, MinusLogProbMetric: 22.6613, val_loss: 22.6169, val_MinusLogProbMetric: 22.6169

Epoch 147: val_loss did not improve from 22.11336
196/196 - 112s - loss: 22.6613 - MinusLogProbMetric: 22.6613 - val_loss: 22.6169 - val_MinusLogProbMetric: 22.6169 - lr: 1.1111e-04 - 112s/epoch - 574ms/step
Epoch 148/1000
2023-10-11 03:32:30.347 
Epoch 148/1000 
	 loss: 22.3077, MinusLogProbMetric: 22.3077, val_loss: 22.1494, val_MinusLogProbMetric: 22.1494

Epoch 148: val_loss did not improve from 22.11336
196/196 - 109s - loss: 22.3077 - MinusLogProbMetric: 22.3077 - val_loss: 22.1494 - val_MinusLogProbMetric: 22.1494 - lr: 1.1111e-04 - 109s/epoch - 556ms/step
Epoch 149/1000
2023-10-11 03:34:20.434 
Epoch 149/1000 
	 loss: 22.0281, MinusLogProbMetric: 22.0281, val_loss: 21.8460, val_MinusLogProbMetric: 21.8460

Epoch 149: val_loss improved from 22.11336 to 21.84604, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 112s - loss: 22.0281 - MinusLogProbMetric: 22.0281 - val_loss: 21.8460 - val_MinusLogProbMetric: 21.8460 - lr: 1.1111e-04 - 112s/epoch - 570ms/step
Epoch 150/1000
2023-10-11 03:36:19.426 
Epoch 150/1000 
	 loss: 21.7953, MinusLogProbMetric: 21.7953, val_loss: 21.7404, val_MinusLogProbMetric: 21.7404

Epoch 150: val_loss improved from 21.84604 to 21.74043, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 118s - loss: 21.7953 - MinusLogProbMetric: 21.7953 - val_loss: 21.7404 - val_MinusLogProbMetric: 21.7404 - lr: 1.1111e-04 - 118s/epoch - 603ms/step
Epoch 151/1000
2023-10-11 03:38:09.273 
Epoch 151/1000 
	 loss: 21.6244, MinusLogProbMetric: 21.6244, val_loss: 21.4780, val_MinusLogProbMetric: 21.4780

Epoch 151: val_loss improved from 21.74043 to 21.47798, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 111s - loss: 21.6244 - MinusLogProbMetric: 21.6244 - val_loss: 21.4780 - val_MinusLogProbMetric: 21.4780 - lr: 1.1111e-04 - 111s/epoch - 569ms/step
Epoch 152/1000
2023-10-11 03:39:59.253 
Epoch 152/1000 
	 loss: 21.4485, MinusLogProbMetric: 21.4485, val_loss: 21.3969, val_MinusLogProbMetric: 21.3969

Epoch 152: val_loss improved from 21.47798 to 21.39694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 109s - loss: 21.4485 - MinusLogProbMetric: 21.4485 - val_loss: 21.3969 - val_MinusLogProbMetric: 21.3969 - lr: 1.1111e-04 - 109s/epoch - 556ms/step
Epoch 153/1000
2023-10-11 03:41:47.818 
Epoch 153/1000 
	 loss: 21.3456, MinusLogProbMetric: 21.3456, val_loss: 21.2828, val_MinusLogProbMetric: 21.2828

Epoch 153: val_loss improved from 21.39694 to 21.28281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 110s - loss: 21.3456 - MinusLogProbMetric: 21.3456 - val_loss: 21.2828 - val_MinusLogProbMetric: 21.2828 - lr: 1.1111e-04 - 110s/epoch - 559ms/step
Epoch 154/1000
2023-10-11 03:43:31.777 
Epoch 154/1000 
	 loss: 21.2064, MinusLogProbMetric: 21.2064, val_loss: 21.1576, val_MinusLogProbMetric: 21.1576

Epoch 154: val_loss improved from 21.28281 to 21.15759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 105s - loss: 21.2064 - MinusLogProbMetric: 21.2064 - val_loss: 21.1576 - val_MinusLogProbMetric: 21.1576 - lr: 1.1111e-04 - 105s/epoch - 536ms/step
Epoch 155/1000
2023-10-11 03:45:18.924 
Epoch 155/1000 
	 loss: 21.1025, MinusLogProbMetric: 21.1025, val_loss: 21.0654, val_MinusLogProbMetric: 21.0654

Epoch 155: val_loss improved from 21.15759 to 21.06542, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 105s - loss: 21.1025 - MinusLogProbMetric: 21.1025 - val_loss: 21.0654 - val_MinusLogProbMetric: 21.0654 - lr: 1.1111e-04 - 105s/epoch - 533ms/step
Epoch 156/1000
2023-10-11 03:47:07.282 
Epoch 156/1000 
	 loss: 21.0021, MinusLogProbMetric: 21.0021, val_loss: 20.9332, val_MinusLogProbMetric: 20.9332

Epoch 156: val_loss improved from 21.06542 to 20.93322, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 110s - loss: 21.0021 - MinusLogProbMetric: 21.0021 - val_loss: 20.9332 - val_MinusLogProbMetric: 20.9332 - lr: 1.1111e-04 - 110s/epoch - 561ms/step
Epoch 157/1000
2023-10-11 03:48:58.732 
Epoch 157/1000 
	 loss: 20.9518, MinusLogProbMetric: 20.9518, val_loss: 21.8717, val_MinusLogProbMetric: 21.8717

Epoch 157: val_loss did not improve from 20.93322
196/196 - 109s - loss: 20.9518 - MinusLogProbMetric: 20.9518 - val_loss: 21.8717 - val_MinusLogProbMetric: 21.8717 - lr: 1.1111e-04 - 109s/epoch - 555ms/step
Epoch 158/1000
2023-10-11 03:50:45.034 
Epoch 158/1000 
	 loss: 20.8943, MinusLogProbMetric: 20.8943, val_loss: 20.8273, val_MinusLogProbMetric: 20.8273

Epoch 158: val_loss improved from 20.93322 to 20.82729, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 108s - loss: 20.8943 - MinusLogProbMetric: 20.8943 - val_loss: 20.8273 - val_MinusLogProbMetric: 20.8273 - lr: 1.1111e-04 - 108s/epoch - 550ms/step
Epoch 159/1000
2023-10-11 03:52:28.727 
Epoch 159/1000 
	 loss: 20.7322, MinusLogProbMetric: 20.7322, val_loss: 20.7066, val_MinusLogProbMetric: 20.7066

Epoch 159: val_loss improved from 20.82729 to 20.70664, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 104s - loss: 20.7322 - MinusLogProbMetric: 20.7322 - val_loss: 20.7066 - val_MinusLogProbMetric: 20.7066 - lr: 1.1111e-04 - 104s/epoch - 531ms/step
Epoch 160/1000
2023-10-11 03:54:15.829 
Epoch 160/1000 
	 loss: 20.6332, MinusLogProbMetric: 20.6332, val_loss: 20.8079, val_MinusLogProbMetric: 20.8079

Epoch 160: val_loss did not improve from 20.70664
196/196 - 105s - loss: 20.6332 - MinusLogProbMetric: 20.6332 - val_loss: 20.8079 - val_MinusLogProbMetric: 20.8079 - lr: 1.1111e-04 - 105s/epoch - 536ms/step
Epoch 161/1000
2023-10-11 03:56:01.760 
Epoch 161/1000 
	 loss: 20.6063, MinusLogProbMetric: 20.6063, val_loss: 20.5651, val_MinusLogProbMetric: 20.5651

Epoch 161: val_loss improved from 20.70664 to 20.56514, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 107s - loss: 20.6063 - MinusLogProbMetric: 20.6063 - val_loss: 20.5651 - val_MinusLogProbMetric: 20.5651 - lr: 1.1111e-04 - 107s/epoch - 546ms/step
Epoch 162/1000
2023-10-11 03:57:43.338 
Epoch 162/1000 
	 loss: 20.5035, MinusLogProbMetric: 20.5035, val_loss: 20.4427, val_MinusLogProbMetric: 20.4427

Epoch 162: val_loss improved from 20.56514 to 20.44271, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 103s - loss: 20.5035 - MinusLogProbMetric: 20.5035 - val_loss: 20.4427 - val_MinusLogProbMetric: 20.4427 - lr: 1.1111e-04 - 103s/epoch - 528ms/step
Epoch 163/1000
2023-10-11 03:59:29.776 
Epoch 163/1000 
	 loss: 20.4473, MinusLogProbMetric: 20.4473, val_loss: 20.3692, val_MinusLogProbMetric: 20.3692

Epoch 163: val_loss improved from 20.44271 to 20.36919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 104s - loss: 20.4473 - MinusLogProbMetric: 20.4473 - val_loss: 20.3692 - val_MinusLogProbMetric: 20.3692 - lr: 1.1111e-04 - 104s/epoch - 531ms/step
Epoch 164/1000
2023-10-11 04:01:16.130 
Epoch 164/1000 
	 loss: 20.3507, MinusLogProbMetric: 20.3507, val_loss: 20.2470, val_MinusLogProbMetric: 20.2470

Epoch 164: val_loss improved from 20.36919 to 20.24701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 109s - loss: 20.3507 - MinusLogProbMetric: 20.3507 - val_loss: 20.2470 - val_MinusLogProbMetric: 20.2470 - lr: 1.1111e-04 - 109s/epoch - 556ms/step
Epoch 165/1000
2023-10-11 04:03:05.662 
Epoch 165/1000 
	 loss: 20.3118, MinusLogProbMetric: 20.3118, val_loss: 20.4509, val_MinusLogProbMetric: 20.4509

Epoch 165: val_loss did not improve from 20.24701
196/196 - 106s - loss: 20.3118 - MinusLogProbMetric: 20.3118 - val_loss: 20.4509 - val_MinusLogProbMetric: 20.4509 - lr: 1.1111e-04 - 106s/epoch - 542ms/step
Epoch 166/1000
2023-10-11 04:04:49.144 
Epoch 166/1000 
	 loss: 20.2577, MinusLogProbMetric: 20.2577, val_loss: 20.5946, val_MinusLogProbMetric: 20.5946

Epoch 166: val_loss did not improve from 20.24701
196/196 - 103s - loss: 20.2577 - MinusLogProbMetric: 20.2577 - val_loss: 20.5946 - val_MinusLogProbMetric: 20.5946 - lr: 1.1111e-04 - 103s/epoch - 528ms/step
Epoch 167/1000
2023-10-11 04:06:28.131 
Epoch 167/1000 
	 loss: 20.2053, MinusLogProbMetric: 20.2053, val_loss: 20.6340, val_MinusLogProbMetric: 20.6340

Epoch 167: val_loss did not improve from 20.24701
196/196 - 99s - loss: 20.2053 - MinusLogProbMetric: 20.2053 - val_loss: 20.6340 - val_MinusLogProbMetric: 20.6340 - lr: 1.1111e-04 - 99s/epoch - 505ms/step
Epoch 168/1000
2023-10-11 04:08:16.788 
Epoch 168/1000 
	 loss: 20.1243, MinusLogProbMetric: 20.1243, val_loss: 20.1537, val_MinusLogProbMetric: 20.1537

Epoch 168: val_loss improved from 20.24701 to 20.15367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 112s - loss: 20.1243 - MinusLogProbMetric: 20.1243 - val_loss: 20.1537 - val_MinusLogProbMetric: 20.1537 - lr: 1.1111e-04 - 112s/epoch - 571ms/step
Epoch 169/1000
2023-10-11 04:10:05.794 
Epoch 169/1000 
	 loss: 20.0903, MinusLogProbMetric: 20.0903, val_loss: 20.1128, val_MinusLogProbMetric: 20.1128

Epoch 169: val_loss improved from 20.15367 to 20.11280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 106s - loss: 20.0903 - MinusLogProbMetric: 20.0903 - val_loss: 20.1128 - val_MinusLogProbMetric: 20.1128 - lr: 1.1111e-04 - 106s/epoch - 542ms/step
Epoch 170/1000
2023-10-11 04:11:50.739 
Epoch 170/1000 
	 loss: 20.0263, MinusLogProbMetric: 20.0263, val_loss: 19.9330, val_MinusLogProbMetric: 19.9330

Epoch 170: val_loss improved from 20.11280 to 19.93302, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 107s - loss: 20.0263 - MinusLogProbMetric: 20.0263 - val_loss: 19.9330 - val_MinusLogProbMetric: 19.9330 - lr: 1.1111e-04 - 107s/epoch - 543ms/step
Epoch 171/1000
2023-10-11 04:13:41.656 
Epoch 171/1000 
	 loss: 19.9602, MinusLogProbMetric: 19.9602, val_loss: 20.0138, val_MinusLogProbMetric: 20.0138

Epoch 171: val_loss did not improve from 19.93302
196/196 - 109s - loss: 19.9602 - MinusLogProbMetric: 19.9602 - val_loss: 20.0138 - val_MinusLogProbMetric: 20.0138 - lr: 1.1111e-04 - 109s/epoch - 555ms/step
Epoch 172/1000
2023-10-11 04:15:26.892 
Epoch 172/1000 
	 loss: 19.9158, MinusLogProbMetric: 19.9158, val_loss: 19.9517, val_MinusLogProbMetric: 19.9517

Epoch 172: val_loss did not improve from 19.93302
196/196 - 105s - loss: 19.9158 - MinusLogProbMetric: 19.9158 - val_loss: 19.9517 - val_MinusLogProbMetric: 19.9517 - lr: 1.1111e-04 - 105s/epoch - 537ms/step
Epoch 173/1000
2023-10-11 04:17:11.700 
Epoch 173/1000 
	 loss: 19.8820, MinusLogProbMetric: 19.8820, val_loss: 19.8230, val_MinusLogProbMetric: 19.8230

Epoch 173: val_loss improved from 19.93302 to 19.82301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 106s - loss: 19.8820 - MinusLogProbMetric: 19.8820 - val_loss: 19.8230 - val_MinusLogProbMetric: 19.8230 - lr: 1.1111e-04 - 106s/epoch - 540ms/step
Epoch 174/1000
2023-10-11 04:19:00.010 
Epoch 174/1000 
	 loss: 19.8525, MinusLogProbMetric: 19.8525, val_loss: 19.9267, val_MinusLogProbMetric: 19.9267

Epoch 174: val_loss did not improve from 19.82301
196/196 - 107s - loss: 19.8525 - MinusLogProbMetric: 19.8525 - val_loss: 19.9267 - val_MinusLogProbMetric: 19.9267 - lr: 1.1111e-04 - 107s/epoch - 547ms/step
Epoch 175/1000
2023-10-11 04:20:49.586 
Epoch 175/1000 
	 loss: 19.7977, MinusLogProbMetric: 19.7977, val_loss: 19.9749, val_MinusLogProbMetric: 19.9749

Epoch 175: val_loss did not improve from 19.82301
196/196 - 110s - loss: 19.7977 - MinusLogProbMetric: 19.7977 - val_loss: 19.9749 - val_MinusLogProbMetric: 19.9749 - lr: 1.1111e-04 - 110s/epoch - 559ms/step
Epoch 176/1000
2023-10-11 04:22:34.239 
Epoch 176/1000 
	 loss: 19.7738, MinusLogProbMetric: 19.7738, val_loss: 19.6804, val_MinusLogProbMetric: 19.6804

Epoch 176: val_loss improved from 19.82301 to 19.68038, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 108s - loss: 19.7738 - MinusLogProbMetric: 19.7738 - val_loss: 19.6804 - val_MinusLogProbMetric: 19.6804 - lr: 1.1111e-04 - 108s/epoch - 549ms/step
Epoch 177/1000
2023-10-11 04:24:20.445 
Epoch 177/1000 
	 loss: 19.7486, MinusLogProbMetric: 19.7486, val_loss: 19.6954, val_MinusLogProbMetric: 19.6954

Epoch 177: val_loss did not improve from 19.68038
196/196 - 103s - loss: 19.7486 - MinusLogProbMetric: 19.7486 - val_loss: 19.6954 - val_MinusLogProbMetric: 19.6954 - lr: 1.1111e-04 - 103s/epoch - 526ms/step
Epoch 178/1000
2023-10-11 04:26:03.633 
Epoch 178/1000 
	 loss: 19.6969, MinusLogProbMetric: 19.6969, val_loss: 19.7848, val_MinusLogProbMetric: 19.7848

Epoch 178: val_loss did not improve from 19.68038
196/196 - 103s - loss: 19.6969 - MinusLogProbMetric: 19.6969 - val_loss: 19.7848 - val_MinusLogProbMetric: 19.7848 - lr: 1.1111e-04 - 103s/epoch - 526ms/step
Epoch 179/1000
2023-10-11 04:27:49.129 
Epoch 179/1000 
	 loss: 19.6414, MinusLogProbMetric: 19.6414, val_loss: 19.7894, val_MinusLogProbMetric: 19.7894

Epoch 179: val_loss did not improve from 19.68038
196/196 - 106s - loss: 19.6414 - MinusLogProbMetric: 19.6414 - val_loss: 19.7894 - val_MinusLogProbMetric: 19.7894 - lr: 1.1111e-04 - 106s/epoch - 538ms/step
Epoch 180/1000
2023-10-11 04:29:39.848 
Epoch 180/1000 
	 loss: 19.6490, MinusLogProbMetric: 19.6490, val_loss: 19.5968, val_MinusLogProbMetric: 19.5968

Epoch 180: val_loss improved from 19.68038 to 19.59678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 113s - loss: 19.6490 - MinusLogProbMetric: 19.6490 - val_loss: 19.5968 - val_MinusLogProbMetric: 19.5968 - lr: 1.1111e-04 - 113s/epoch - 574ms/step
Epoch 181/1000
2023-10-11 04:31:29.204 
Epoch 181/1000 
	 loss: 19.6029, MinusLogProbMetric: 19.6029, val_loss: 19.5450, val_MinusLogProbMetric: 19.5450

Epoch 181: val_loss improved from 19.59678 to 19.54500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 110s - loss: 19.6029 - MinusLogProbMetric: 19.6029 - val_loss: 19.5450 - val_MinusLogProbMetric: 19.5450 - lr: 1.1111e-04 - 110s/epoch - 562ms/step
Epoch 182/1000
2023-10-11 04:33:19.933 
Epoch 182/1000 
	 loss: 19.5539, MinusLogProbMetric: 19.5539, val_loss: 19.7369, val_MinusLogProbMetric: 19.7369

Epoch 182: val_loss did not improve from 19.54500
196/196 - 108s - loss: 19.5539 - MinusLogProbMetric: 19.5539 - val_loss: 19.7369 - val_MinusLogProbMetric: 19.7369 - lr: 1.1111e-04 - 108s/epoch - 551ms/step
Epoch 183/1000
2023-10-11 04:35:03.334 
Epoch 183/1000 
	 loss: 19.5102, MinusLogProbMetric: 19.5102, val_loss: 19.6848, val_MinusLogProbMetric: 19.6848

Epoch 183: val_loss did not improve from 19.54500
196/196 - 103s - loss: 19.5102 - MinusLogProbMetric: 19.5102 - val_loss: 19.6848 - val_MinusLogProbMetric: 19.6848 - lr: 1.1111e-04 - 103s/epoch - 527ms/step
Epoch 184/1000
2023-10-11 04:36:48.915 
Epoch 184/1000 
	 loss: 19.4845, MinusLogProbMetric: 19.4845, val_loss: 19.4611, val_MinusLogProbMetric: 19.4611

Epoch 184: val_loss improved from 19.54500 to 19.46109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 106s - loss: 19.4845 - MinusLogProbMetric: 19.4845 - val_loss: 19.4611 - val_MinusLogProbMetric: 19.4611 - lr: 1.1111e-04 - 106s/epoch - 543ms/step
Epoch 185/1000
2023-10-11 04:38:33.995 
Epoch 185/1000 
	 loss: 19.4733, MinusLogProbMetric: 19.4733, val_loss: 19.7601, val_MinusLogProbMetric: 19.7601

Epoch 185: val_loss did not improve from 19.46109
196/196 - 104s - loss: 19.4733 - MinusLogProbMetric: 19.4733 - val_loss: 19.7601 - val_MinusLogProbMetric: 19.7601 - lr: 1.1111e-04 - 104s/epoch - 532ms/step
Epoch 186/1000
2023-10-11 04:40:20.892 
Epoch 186/1000 
	 loss: 19.4371, MinusLogProbMetric: 19.4371, val_loss: 19.5398, val_MinusLogProbMetric: 19.5398

Epoch 186: val_loss did not improve from 19.46109
196/196 - 107s - loss: 19.4371 - MinusLogProbMetric: 19.4371 - val_loss: 19.5398 - val_MinusLogProbMetric: 19.5398 - lr: 1.1111e-04 - 107s/epoch - 545ms/step
Epoch 187/1000
2023-10-11 04:42:06.106 
Epoch 187/1000 
	 loss: 19.4879, MinusLogProbMetric: 19.4879, val_loss: 19.7300, val_MinusLogProbMetric: 19.7300

Epoch 187: val_loss did not improve from 19.46109
196/196 - 105s - loss: 19.4879 - MinusLogProbMetric: 19.4879 - val_loss: 19.7300 - val_MinusLogProbMetric: 19.7300 - lr: 1.1111e-04 - 105s/epoch - 537ms/step
Epoch 188/1000
2023-10-11 04:43:51.609 
Epoch 188/1000 
	 loss: 19.4491, MinusLogProbMetric: 19.4491, val_loss: 19.6595, val_MinusLogProbMetric: 19.6595

Epoch 188: val_loss did not improve from 19.46109
196/196 - 106s - loss: 19.4491 - MinusLogProbMetric: 19.4491 - val_loss: 19.6595 - val_MinusLogProbMetric: 19.6595 - lr: 1.1111e-04 - 106s/epoch - 538ms/step
Epoch 189/1000
2023-10-11 04:45:39.448 
Epoch 189/1000 
	 loss: 19.4021, MinusLogProbMetric: 19.4021, val_loss: 19.2898, val_MinusLogProbMetric: 19.2898

Epoch 189: val_loss improved from 19.46109 to 19.28976, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 110s - loss: 19.4021 - MinusLogProbMetric: 19.4021 - val_loss: 19.2898 - val_MinusLogProbMetric: 19.2898 - lr: 1.1111e-04 - 110s/epoch - 562ms/step
Epoch 190/1000
2023-10-11 04:47:27.180 
Epoch 190/1000 
	 loss: 19.3830, MinusLogProbMetric: 19.3830, val_loss: 19.4657, val_MinusLogProbMetric: 19.4657

Epoch 190: val_loss did not improve from 19.28976
196/196 - 105s - loss: 19.3830 - MinusLogProbMetric: 19.3830 - val_loss: 19.4657 - val_MinusLogProbMetric: 19.4657 - lr: 1.1111e-04 - 105s/epoch - 538ms/step
Epoch 191/1000
2023-10-11 04:49:13.351 
Epoch 191/1000 
	 loss: 19.3213, MinusLogProbMetric: 19.3213, val_loss: 19.2829, val_MinusLogProbMetric: 19.2829

Epoch 191: val_loss improved from 19.28976 to 19.28286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 108s - loss: 19.3213 - MinusLogProbMetric: 19.3213 - val_loss: 19.2829 - val_MinusLogProbMetric: 19.2829 - lr: 1.1111e-04 - 108s/epoch - 552ms/step
Epoch 192/1000
2023-10-11 04:51:03.073 
Epoch 192/1000 
	 loss: 19.2726, MinusLogProbMetric: 19.2726, val_loss: 19.2133, val_MinusLogProbMetric: 19.2133

Epoch 192: val_loss improved from 19.28286 to 19.21332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 109s - loss: 19.2726 - MinusLogProbMetric: 19.2726 - val_loss: 19.2133 - val_MinusLogProbMetric: 19.2133 - lr: 1.1111e-04 - 109s/epoch - 557ms/step
Epoch 193/1000
2023-10-11 04:52:50.227 
Epoch 193/1000 
	 loss: 19.2785, MinusLogProbMetric: 19.2785, val_loss: 19.2798, val_MinusLogProbMetric: 19.2798

Epoch 193: val_loss did not improve from 19.21332
196/196 - 106s - loss: 19.2785 - MinusLogProbMetric: 19.2785 - val_loss: 19.2798 - val_MinusLogProbMetric: 19.2798 - lr: 1.1111e-04 - 106s/epoch - 539ms/step
Epoch 194/1000
2023-10-11 04:54:36.044 
Epoch 194/1000 
	 loss: 19.2494, MinusLogProbMetric: 19.2494, val_loss: 19.3886, val_MinusLogProbMetric: 19.3886

Epoch 194: val_loss did not improve from 19.21332
196/196 - 106s - loss: 19.2494 - MinusLogProbMetric: 19.2494 - val_loss: 19.3886 - val_MinusLogProbMetric: 19.3886 - lr: 1.1111e-04 - 106s/epoch - 540ms/step
Epoch 195/1000
2023-10-11 04:56:21.340 
Epoch 195/1000 
	 loss: 19.2750, MinusLogProbMetric: 19.2750, val_loss: 19.3501, val_MinusLogProbMetric: 19.3501

Epoch 195: val_loss did not improve from 19.21332
196/196 - 105s - loss: 19.2750 - MinusLogProbMetric: 19.2750 - val_loss: 19.3501 - val_MinusLogProbMetric: 19.3501 - lr: 1.1111e-04 - 105s/epoch - 537ms/step
Epoch 196/1000
2023-10-11 04:58:08.225 
Epoch 196/1000 
	 loss: 19.2343, MinusLogProbMetric: 19.2343, val_loss: 19.1712, val_MinusLogProbMetric: 19.1712

Epoch 196: val_loss improved from 19.21332 to 19.17116, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 110s - loss: 19.2343 - MinusLogProbMetric: 19.2343 - val_loss: 19.1712 - val_MinusLogProbMetric: 19.1712 - lr: 1.1111e-04 - 110s/epoch - 560ms/step
Epoch 197/1000
2023-10-11 04:59:57.557 
Epoch 197/1000 
	 loss: 19.1680, MinusLogProbMetric: 19.1680, val_loss: 19.3545, val_MinusLogProbMetric: 19.3545

Epoch 197: val_loss did not improve from 19.17116
196/196 - 107s - loss: 19.1680 - MinusLogProbMetric: 19.1680 - val_loss: 19.3545 - val_MinusLogProbMetric: 19.3545 - lr: 1.1111e-04 - 107s/epoch - 543ms/step
Epoch 198/1000
2023-10-11 05:01:45.145 
Epoch 198/1000 
	 loss: 19.1598, MinusLogProbMetric: 19.1598, val_loss: 19.1811, val_MinusLogProbMetric: 19.1811

Epoch 198: val_loss did not improve from 19.17116
196/196 - 108s - loss: 19.1598 - MinusLogProbMetric: 19.1598 - val_loss: 19.1811 - val_MinusLogProbMetric: 19.1811 - lr: 1.1111e-04 - 108s/epoch - 549ms/step
Epoch 199/1000
2023-10-11 05:03:28.935 
Epoch 199/1000 
	 loss: 19.2158, MinusLogProbMetric: 19.2158, val_loss: 19.0712, val_MinusLogProbMetric: 19.0712

Epoch 199: val_loss improved from 19.17116 to 19.07123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 107s - loss: 19.2158 - MinusLogProbMetric: 19.2158 - val_loss: 19.0712 - val_MinusLogProbMetric: 19.0712 - lr: 1.1111e-04 - 107s/epoch - 545ms/step
Epoch 200/1000
2023-10-11 05:05:19.739 
Epoch 200/1000 
	 loss: 19.1340, MinusLogProbMetric: 19.1340, val_loss: 19.1223, val_MinusLogProbMetric: 19.1223

Epoch 200: val_loss did not improve from 19.07123
196/196 - 108s - loss: 19.1340 - MinusLogProbMetric: 19.1340 - val_loss: 19.1223 - val_MinusLogProbMetric: 19.1223 - lr: 1.1111e-04 - 108s/epoch - 550ms/step
Epoch 201/1000
2023-10-11 05:07:03.598 
Epoch 201/1000 
	 loss: 19.0843, MinusLogProbMetric: 19.0843, val_loss: 19.0528, val_MinusLogProbMetric: 19.0528

Epoch 201: val_loss improved from 19.07123 to 19.05283, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 107s - loss: 19.0843 - MinusLogProbMetric: 19.0843 - val_loss: 19.0528 - val_MinusLogProbMetric: 19.0528 - lr: 1.1111e-04 - 107s/epoch - 547ms/step
Epoch 202/1000
2023-10-11 05:08:52.100 
Epoch 202/1000 
	 loss: 19.0954, MinusLogProbMetric: 19.0954, val_loss: 19.2340, val_MinusLogProbMetric: 19.2340

Epoch 202: val_loss did not improve from 19.05283
196/196 - 105s - loss: 19.0954 - MinusLogProbMetric: 19.0954 - val_loss: 19.2340 - val_MinusLogProbMetric: 19.2340 - lr: 1.1111e-04 - 105s/epoch - 536ms/step
Epoch 203/1000
2023-10-11 05:10:42.791 
Epoch 203/1000 
	 loss: 19.0869, MinusLogProbMetric: 19.0869, val_loss: 19.2275, val_MinusLogProbMetric: 19.2275

Epoch 203: val_loss did not improve from 19.05283
196/196 - 111s - loss: 19.0869 - MinusLogProbMetric: 19.0869 - val_loss: 19.2275 - val_MinusLogProbMetric: 19.2275 - lr: 1.1111e-04 - 111s/epoch - 565ms/step
Epoch 204/1000
2023-10-11 05:12:30.173 
Epoch 204/1000 
	 loss: 19.0791, MinusLogProbMetric: 19.0791, val_loss: 19.7605, val_MinusLogProbMetric: 19.7605

Epoch 204: val_loss did not improve from 19.05283
196/196 - 107s - loss: 19.0791 - MinusLogProbMetric: 19.0791 - val_loss: 19.7605 - val_MinusLogProbMetric: 19.7605 - lr: 1.1111e-04 - 107s/epoch - 548ms/step
Epoch 205/1000
2023-10-11 05:14:12.398 
Epoch 205/1000 
	 loss: 19.0685, MinusLogProbMetric: 19.0685, val_loss: 18.9053, val_MinusLogProbMetric: 18.9053

Epoch 205: val_loss improved from 19.05283 to 18.90531, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 103s - loss: 19.0685 - MinusLogProbMetric: 19.0685 - val_loss: 18.9053 - val_MinusLogProbMetric: 18.9053 - lr: 1.1111e-04 - 103s/epoch - 524ms/step
Epoch 206/1000
2023-10-11 05:16:03.428 
Epoch 206/1000 
	 loss: 19.0767, MinusLogProbMetric: 19.0767, val_loss: 19.9013, val_MinusLogProbMetric: 19.9013

Epoch 206: val_loss did not improve from 18.90531
196/196 - 111s - loss: 19.0767 - MinusLogProbMetric: 19.0767 - val_loss: 19.9013 - val_MinusLogProbMetric: 19.9013 - lr: 1.1111e-04 - 111s/epoch - 564ms/step
Epoch 207/1000
2023-10-11 05:17:48.741 
Epoch 207/1000 
	 loss: 18.9788, MinusLogProbMetric: 18.9788, val_loss: 19.1116, val_MinusLogProbMetric: 19.1116

Epoch 207: val_loss did not improve from 18.90531
196/196 - 105s - loss: 18.9788 - MinusLogProbMetric: 18.9788 - val_loss: 19.1116 - val_MinusLogProbMetric: 19.1116 - lr: 1.1111e-04 - 105s/epoch - 537ms/step
Epoch 208/1000
2023-10-11 05:19:35.926 
Epoch 208/1000 
	 loss: 18.9994, MinusLogProbMetric: 18.9994, val_loss: 18.9570, val_MinusLogProbMetric: 18.9570

Epoch 208: val_loss did not improve from 18.90531
196/196 - 107s - loss: 18.9994 - MinusLogProbMetric: 18.9994 - val_loss: 18.9570 - val_MinusLogProbMetric: 18.9570 - lr: 1.1111e-04 - 107s/epoch - 547ms/step
Epoch 209/1000
2023-10-11 05:21:23.891 
Epoch 209/1000 
	 loss: 19.0032, MinusLogProbMetric: 19.0032, val_loss: 18.9897, val_MinusLogProbMetric: 18.9897

Epoch 209: val_loss did not improve from 18.90531
196/196 - 108s - loss: 19.0032 - MinusLogProbMetric: 19.0032 - val_loss: 18.9897 - val_MinusLogProbMetric: 18.9897 - lr: 1.1111e-04 - 108s/epoch - 551ms/step
Epoch 210/1000
2023-10-11 05:23:10.810 
Epoch 210/1000 
	 loss: 18.9554, MinusLogProbMetric: 18.9554, val_loss: 18.9040, val_MinusLogProbMetric: 18.9040

Epoch 210: val_loss improved from 18.90531 to 18.90397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 108s - loss: 18.9554 - MinusLogProbMetric: 18.9554 - val_loss: 18.9040 - val_MinusLogProbMetric: 18.9040 - lr: 1.1111e-04 - 108s/epoch - 553ms/step
Epoch 211/1000
2023-10-11 05:25:03.134 
Epoch 211/1000 
	 loss: 18.9468, MinusLogProbMetric: 18.9468, val_loss: 18.8849, val_MinusLogProbMetric: 18.8849

Epoch 211: val_loss improved from 18.90397 to 18.88489, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 113s - loss: 18.9468 - MinusLogProbMetric: 18.9468 - val_loss: 18.8849 - val_MinusLogProbMetric: 18.8849 - lr: 1.1111e-04 - 113s/epoch - 575ms/step
Epoch 212/1000
2023-10-11 05:26:53.825 
Epoch 212/1000 
	 loss: 18.9059, MinusLogProbMetric: 18.9059, val_loss: 18.8953, val_MinusLogProbMetric: 18.8953

Epoch 212: val_loss did not improve from 18.88489
196/196 - 109s - loss: 18.9059 - MinusLogProbMetric: 18.9059 - val_loss: 18.8953 - val_MinusLogProbMetric: 18.8953 - lr: 1.1111e-04 - 109s/epoch - 556ms/step
Epoch 213/1000
2023-10-11 05:28:46.153 
Epoch 213/1000 
	 loss: 18.9348, MinusLogProbMetric: 18.9348, val_loss: 19.0325, val_MinusLogProbMetric: 19.0325

Epoch 213: val_loss did not improve from 18.88489
196/196 - 112s - loss: 18.9348 - MinusLogProbMetric: 18.9348 - val_loss: 19.0325 - val_MinusLogProbMetric: 19.0325 - lr: 1.1111e-04 - 112s/epoch - 573ms/step
Epoch 214/1000
2023-10-11 05:30:38.262 
Epoch 214/1000 
	 loss: 18.8749, MinusLogProbMetric: 18.8749, val_loss: 18.9142, val_MinusLogProbMetric: 18.9142

Epoch 214: val_loss did not improve from 18.88489
196/196 - 112s - loss: 18.8749 - MinusLogProbMetric: 18.8749 - val_loss: 18.9142 - val_MinusLogProbMetric: 18.9142 - lr: 1.1111e-04 - 112s/epoch - 572ms/step
Epoch 215/1000
2023-10-11 05:32:30.302 
Epoch 215/1000 
	 loss: 18.9083, MinusLogProbMetric: 18.9083, val_loss: 19.0023, val_MinusLogProbMetric: 19.0023

Epoch 215: val_loss did not improve from 18.88489
196/196 - 112s - loss: 18.9083 - MinusLogProbMetric: 18.9083 - val_loss: 19.0023 - val_MinusLogProbMetric: 19.0023 - lr: 1.1111e-04 - 112s/epoch - 571ms/step
Epoch 216/1000
2023-10-11 05:34:25.804 
Epoch 216/1000 
	 loss: 18.9040, MinusLogProbMetric: 18.9040, val_loss: 18.7429, val_MinusLogProbMetric: 18.7429

Epoch 216: val_loss improved from 18.88489 to 18.74287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 116s - loss: 18.9040 - MinusLogProbMetric: 18.9040 - val_loss: 18.7429 - val_MinusLogProbMetric: 18.7429 - lr: 1.1111e-04 - 116s/epoch - 594ms/step
Epoch 217/1000
2023-10-11 05:36:20.777 
Epoch 217/1000 
	 loss: 18.9236, MinusLogProbMetric: 18.9236, val_loss: 18.7996, val_MinusLogProbMetric: 18.7996

Epoch 217: val_loss did not improve from 18.74287
196/196 - 114s - loss: 18.9236 - MinusLogProbMetric: 18.9236 - val_loss: 18.7996 - val_MinusLogProbMetric: 18.7996 - lr: 1.1111e-04 - 114s/epoch - 582ms/step
Epoch 218/1000
2023-10-11 05:38:12.267 
Epoch 218/1000 
	 loss: 18.8216, MinusLogProbMetric: 18.8216, val_loss: 18.8421, val_MinusLogProbMetric: 18.8421

Epoch 218: val_loss did not improve from 18.74287
196/196 - 111s - loss: 18.8216 - MinusLogProbMetric: 18.8216 - val_loss: 18.8421 - val_MinusLogProbMetric: 18.8421 - lr: 1.1111e-04 - 111s/epoch - 569ms/step
Epoch 219/1000
2023-10-11 05:40:09.372 
Epoch 219/1000 
	 loss: 18.8059, MinusLogProbMetric: 18.8059, val_loss: 19.1585, val_MinusLogProbMetric: 19.1585

Epoch 219: val_loss did not improve from 18.74287
196/196 - 117s - loss: 18.8059 - MinusLogProbMetric: 18.8059 - val_loss: 19.1585 - val_MinusLogProbMetric: 19.1585 - lr: 1.1111e-04 - 117s/epoch - 598ms/step
Epoch 220/1000
2023-10-11 05:42:10.253 
Epoch 220/1000 
	 loss: 18.8409, MinusLogProbMetric: 18.8409, val_loss: 18.8154, val_MinusLogProbMetric: 18.8154

Epoch 220: val_loss did not improve from 18.74287
196/196 - 121s - loss: 18.8409 - MinusLogProbMetric: 18.8409 - val_loss: 18.8154 - val_MinusLogProbMetric: 18.8154 - lr: 1.1111e-04 - 121s/epoch - 617ms/step
Epoch 221/1000
2023-10-11 05:44:06.836 
Epoch 221/1000 
	 loss: 18.8121, MinusLogProbMetric: 18.8121, val_loss: 18.8237, val_MinusLogProbMetric: 18.8237

Epoch 221: val_loss did not improve from 18.74287
196/196 - 117s - loss: 18.8121 - MinusLogProbMetric: 18.8121 - val_loss: 18.8237 - val_MinusLogProbMetric: 18.8237 - lr: 1.1111e-04 - 117s/epoch - 595ms/step
Epoch 222/1000
2023-10-11 05:45:58.485 
Epoch 222/1000 
	 loss: 18.8587, MinusLogProbMetric: 18.8587, val_loss: 18.8331, val_MinusLogProbMetric: 18.8331

Epoch 222: val_loss did not improve from 18.74287
196/196 - 112s - loss: 18.8587 - MinusLogProbMetric: 18.8587 - val_loss: 18.8331 - val_MinusLogProbMetric: 18.8331 - lr: 1.1111e-04 - 112s/epoch - 569ms/step
Epoch 223/1000
2023-10-11 05:48:08.050 
Epoch 223/1000 
	 loss: 18.7988, MinusLogProbMetric: 18.7988, val_loss: 19.1283, val_MinusLogProbMetric: 19.1283

Epoch 223: val_loss did not improve from 18.74287
196/196 - 130s - loss: 18.7988 - MinusLogProbMetric: 18.7988 - val_loss: 19.1283 - val_MinusLogProbMetric: 19.1283 - lr: 1.1111e-04 - 130s/epoch - 661ms/step
Epoch 224/1000
2023-10-11 05:50:03.925 
Epoch 224/1000 
	 loss: 18.8391, MinusLogProbMetric: 18.8391, val_loss: 18.7028, val_MinusLogProbMetric: 18.7028

Epoch 224: val_loss improved from 18.74287 to 18.70279, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 119s - loss: 18.8391 - MinusLogProbMetric: 18.8391 - val_loss: 18.7028 - val_MinusLogProbMetric: 18.7028 - lr: 1.1111e-04 - 119s/epoch - 607ms/step
Epoch 225/1000
2023-10-11 05:52:01.220 
Epoch 225/1000 
	 loss: 18.6882, MinusLogProbMetric: 18.6882, val_loss: 19.8324, val_MinusLogProbMetric: 19.8324

Epoch 225: val_loss did not improve from 18.70279
196/196 - 114s - loss: 18.6882 - MinusLogProbMetric: 18.6882 - val_loss: 19.8324 - val_MinusLogProbMetric: 19.8324 - lr: 1.1111e-04 - 114s/epoch - 582ms/step
Epoch 226/1000
2023-10-11 05:53:54.073 
Epoch 226/1000 
	 loss: 18.7935, MinusLogProbMetric: 18.7935, val_loss: 18.6091, val_MinusLogProbMetric: 18.6091

Epoch 226: val_loss improved from 18.70279 to 18.60905, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 18.7935 - MinusLogProbMetric: 18.7935 - val_loss: 18.6091 - val_MinusLogProbMetric: 18.6091 - lr: 1.1111e-04 - 115s/epoch - 586ms/step
Epoch 227/1000
2023-10-11 05:55:49.264 
Epoch 227/1000 
	 loss: 18.7413, MinusLogProbMetric: 18.7413, val_loss: 18.6867, val_MinusLogProbMetric: 18.6867

Epoch 227: val_loss did not improve from 18.60905
196/196 - 113s - loss: 18.7413 - MinusLogProbMetric: 18.7413 - val_loss: 18.6867 - val_MinusLogProbMetric: 18.6867 - lr: 1.1111e-04 - 113s/epoch - 577ms/step
Epoch 228/1000
2023-10-11 05:57:36.663 
Epoch 228/1000 
	 loss: 18.7246, MinusLogProbMetric: 18.7246, val_loss: 18.6318, val_MinusLogProbMetric: 18.6318

Epoch 228: val_loss did not improve from 18.60905
196/196 - 107s - loss: 18.7246 - MinusLogProbMetric: 18.7246 - val_loss: 18.6318 - val_MinusLogProbMetric: 18.6318 - lr: 1.1111e-04 - 107s/epoch - 548ms/step
Epoch 229/1000
2023-10-11 05:59:27.875 
Epoch 229/1000 
	 loss: 18.7167, MinusLogProbMetric: 18.7167, val_loss: 18.7057, val_MinusLogProbMetric: 18.7057

Epoch 229: val_loss did not improve from 18.60905
196/196 - 111s - loss: 18.7167 - MinusLogProbMetric: 18.7167 - val_loss: 18.7057 - val_MinusLogProbMetric: 18.7057 - lr: 1.1111e-04 - 111s/epoch - 568ms/step
Epoch 230/1000
2023-10-11 06:01:14.229 
Epoch 230/1000 
	 loss: 18.7112, MinusLogProbMetric: 18.7112, val_loss: 19.2483, val_MinusLogProbMetric: 19.2483

Epoch 230: val_loss did not improve from 18.60905
196/196 - 106s - loss: 18.7112 - MinusLogProbMetric: 18.7112 - val_loss: 19.2483 - val_MinusLogProbMetric: 19.2483 - lr: 1.1111e-04 - 106s/epoch - 542ms/step
Epoch 231/1000
2023-10-11 06:03:02.443 
Epoch 231/1000 
	 loss: 18.6852, MinusLogProbMetric: 18.6852, val_loss: 18.9499, val_MinusLogProbMetric: 18.9499

Epoch 231: val_loss did not improve from 18.60905
196/196 - 108s - loss: 18.6852 - MinusLogProbMetric: 18.6852 - val_loss: 18.9499 - val_MinusLogProbMetric: 18.9499 - lr: 1.1111e-04 - 108s/epoch - 552ms/step
Epoch 232/1000
2023-10-11 06:04:54.989 
Epoch 232/1000 
	 loss: 18.7310, MinusLogProbMetric: 18.7310, val_loss: 18.8517, val_MinusLogProbMetric: 18.8517

Epoch 232: val_loss did not improve from 18.60905
196/196 - 113s - loss: 18.7310 - MinusLogProbMetric: 18.7310 - val_loss: 18.8517 - val_MinusLogProbMetric: 18.8517 - lr: 1.1111e-04 - 113s/epoch - 574ms/step
Epoch 233/1000
2023-10-11 06:06:41.409 
Epoch 233/1000 
	 loss: 18.6700, MinusLogProbMetric: 18.6700, val_loss: 18.6894, val_MinusLogProbMetric: 18.6894

Epoch 233: val_loss did not improve from 18.60905
196/196 - 106s - loss: 18.6700 - MinusLogProbMetric: 18.6700 - val_loss: 18.6894 - val_MinusLogProbMetric: 18.6894 - lr: 1.1111e-04 - 106s/epoch - 543ms/step
Epoch 234/1000
2023-10-11 06:08:44.107 
Epoch 234/1000 
	 loss: 18.6468, MinusLogProbMetric: 18.6468, val_loss: 18.8022, val_MinusLogProbMetric: 18.8022

Epoch 234: val_loss did not improve from 18.60905
196/196 - 123s - loss: 18.6468 - MinusLogProbMetric: 18.6468 - val_loss: 18.8022 - val_MinusLogProbMetric: 18.8022 - lr: 1.1111e-04 - 123s/epoch - 626ms/step
Epoch 235/1000
2023-10-11 06:10:27.983 
Epoch 235/1000 
	 loss: 18.6166, MinusLogProbMetric: 18.6166, val_loss: 18.7126, val_MinusLogProbMetric: 18.7126

Epoch 235: val_loss did not improve from 18.60905
196/196 - 104s - loss: 18.6166 - MinusLogProbMetric: 18.6166 - val_loss: 18.7126 - val_MinusLogProbMetric: 18.7126 - lr: 1.1111e-04 - 104s/epoch - 530ms/step
Epoch 236/1000
2023-10-11 06:12:12.516 
Epoch 236/1000 
	 loss: 18.6096, MinusLogProbMetric: 18.6096, val_loss: 18.9448, val_MinusLogProbMetric: 18.9448

Epoch 236: val_loss did not improve from 18.60905
196/196 - 105s - loss: 18.6096 - MinusLogProbMetric: 18.6096 - val_loss: 18.9448 - val_MinusLogProbMetric: 18.9448 - lr: 1.1111e-04 - 105s/epoch - 533ms/step
Epoch 237/1000
2023-10-11 06:14:04.136 
Epoch 237/1000 
	 loss: 18.6778, MinusLogProbMetric: 18.6778, val_loss: 19.0091, val_MinusLogProbMetric: 19.0091

Epoch 237: val_loss did not improve from 18.60905
196/196 - 112s - loss: 18.6778 - MinusLogProbMetric: 18.6778 - val_loss: 19.0091 - val_MinusLogProbMetric: 19.0091 - lr: 1.1111e-04 - 112s/epoch - 569ms/step
Epoch 238/1000
2023-10-11 06:15:47.809 
Epoch 238/1000 
	 loss: 18.6119, MinusLogProbMetric: 18.6119, val_loss: 18.5977, val_MinusLogProbMetric: 18.5977

Epoch 238: val_loss improved from 18.60905 to 18.59770, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 107s - loss: 18.6119 - MinusLogProbMetric: 18.6119 - val_loss: 18.5977 - val_MinusLogProbMetric: 18.5977 - lr: 1.1111e-04 - 107s/epoch - 546ms/step
Epoch 239/1000
2023-10-11 06:17:49.850 
Epoch 239/1000 
	 loss: 18.6324, MinusLogProbMetric: 18.6324, val_loss: 18.7417, val_MinusLogProbMetric: 18.7417

Epoch 239: val_loss did not improve from 18.59770
196/196 - 119s - loss: 18.6324 - MinusLogProbMetric: 18.6324 - val_loss: 18.7417 - val_MinusLogProbMetric: 18.7417 - lr: 1.1111e-04 - 119s/epoch - 605ms/step
Epoch 240/1000
2023-10-11 06:19:38.993 
Epoch 240/1000 
	 loss: 18.6014, MinusLogProbMetric: 18.6014, val_loss: 18.7703, val_MinusLogProbMetric: 18.7703

Epoch 240: val_loss did not improve from 18.59770
196/196 - 109s - loss: 18.6014 - MinusLogProbMetric: 18.6014 - val_loss: 18.7703 - val_MinusLogProbMetric: 18.7703 - lr: 1.1111e-04 - 109s/epoch - 557ms/step
Epoch 241/1000
2023-10-11 06:21:30.252 
Epoch 241/1000 
	 loss: 18.6100, MinusLogProbMetric: 18.6100, val_loss: 19.1790, val_MinusLogProbMetric: 19.1790

Epoch 241: val_loss did not improve from 18.59770
196/196 - 111s - loss: 18.6100 - MinusLogProbMetric: 18.6100 - val_loss: 19.1790 - val_MinusLogProbMetric: 19.1790 - lr: 1.1111e-04 - 111s/epoch - 568ms/step
Epoch 242/1000
2023-10-11 06:23:17.889 
Epoch 242/1000 
	 loss: 18.5773, MinusLogProbMetric: 18.5773, val_loss: 18.5476, val_MinusLogProbMetric: 18.5476

Epoch 242: val_loss improved from 18.59770 to 18.54757, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 108s - loss: 18.5773 - MinusLogProbMetric: 18.5773 - val_loss: 18.5476 - val_MinusLogProbMetric: 18.5476 - lr: 1.1111e-04 - 108s/epoch - 552ms/step
Epoch 243/1000
2023-10-11 06:25:09.353 
Epoch 243/1000 
	 loss: 18.6221, MinusLogProbMetric: 18.6221, val_loss: 18.5183, val_MinusLogProbMetric: 18.5183

Epoch 243: val_loss improved from 18.54757 to 18.51833, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 112s - loss: 18.6221 - MinusLogProbMetric: 18.6221 - val_loss: 18.5183 - val_MinusLogProbMetric: 18.5183 - lr: 1.1111e-04 - 112s/epoch - 571ms/step
Epoch 244/1000
2023-10-11 06:26:57.622 
Epoch 244/1000 
	 loss: 18.5162, MinusLogProbMetric: 18.5162, val_loss: 18.7802, val_MinusLogProbMetric: 18.7802

Epoch 244: val_loss did not improve from 18.51833
196/196 - 107s - loss: 18.5162 - MinusLogProbMetric: 18.5162 - val_loss: 18.7802 - val_MinusLogProbMetric: 18.7802 - lr: 1.1111e-04 - 107s/epoch - 547ms/step
Epoch 245/1000
2023-10-11 06:28:47.885 
Epoch 245/1000 
	 loss: 18.5695, MinusLogProbMetric: 18.5695, val_loss: 18.5459, val_MinusLogProbMetric: 18.5459

Epoch 245: val_loss did not improve from 18.51833
196/196 - 110s - loss: 18.5695 - MinusLogProbMetric: 18.5695 - val_loss: 18.5459 - val_MinusLogProbMetric: 18.5459 - lr: 1.1111e-04 - 110s/epoch - 563ms/step
Epoch 246/1000
2023-10-11 06:30:35.179 
Epoch 246/1000 
	 loss: 18.5809, MinusLogProbMetric: 18.5809, val_loss: 18.9089, val_MinusLogProbMetric: 18.9089

Epoch 246: val_loss did not improve from 18.51833
196/196 - 107s - loss: 18.5809 - MinusLogProbMetric: 18.5809 - val_loss: 18.9089 - val_MinusLogProbMetric: 18.9089 - lr: 1.1111e-04 - 107s/epoch - 547ms/step
Epoch 247/1000
2023-10-11 06:32:29.194 
Epoch 247/1000 
	 loss: 18.5242, MinusLogProbMetric: 18.5242, val_loss: 18.6658, val_MinusLogProbMetric: 18.6658

Epoch 247: val_loss did not improve from 18.51833
196/196 - 114s - loss: 18.5242 - MinusLogProbMetric: 18.5242 - val_loss: 18.6658 - val_MinusLogProbMetric: 18.6658 - lr: 1.1111e-04 - 114s/epoch - 582ms/step
Epoch 248/1000
2023-10-11 06:34:17.479 
Epoch 248/1000 
	 loss: 18.5771, MinusLogProbMetric: 18.5771, val_loss: 18.4269, val_MinusLogProbMetric: 18.4269

Epoch 248: val_loss improved from 18.51833 to 18.42691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 111s - loss: 18.5771 - MinusLogProbMetric: 18.5771 - val_loss: 18.4269 - val_MinusLogProbMetric: 18.4269 - lr: 1.1111e-04 - 111s/epoch - 564ms/step
Epoch 249/1000
2023-10-11 06:36:08.634 
Epoch 249/1000 
	 loss: 18.5097, MinusLogProbMetric: 18.5097, val_loss: 18.4212, val_MinusLogProbMetric: 18.4212

Epoch 249: val_loss improved from 18.42691 to 18.42122, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 110s - loss: 18.5097 - MinusLogProbMetric: 18.5097 - val_loss: 18.4212 - val_MinusLogProbMetric: 18.4212 - lr: 1.1111e-04 - 110s/epoch - 561ms/step
Epoch 250/1000
2023-10-11 06:38:01.921 
Epoch 250/1000 
	 loss: 18.4772, MinusLogProbMetric: 18.4772, val_loss: 18.5907, val_MinusLogProbMetric: 18.5907

Epoch 250: val_loss did not improve from 18.42122
196/196 - 112s - loss: 18.4772 - MinusLogProbMetric: 18.4772 - val_loss: 18.5907 - val_MinusLogProbMetric: 18.5907 - lr: 1.1111e-04 - 112s/epoch - 572ms/step
Epoch 251/1000
2023-10-11 06:39:52.047 
Epoch 251/1000 
	 loss: 18.5135, MinusLogProbMetric: 18.5135, val_loss: 18.5541, val_MinusLogProbMetric: 18.5541

Epoch 251: val_loss did not improve from 18.42122
196/196 - 110s - loss: 18.5135 - MinusLogProbMetric: 18.5135 - val_loss: 18.5541 - val_MinusLogProbMetric: 18.5541 - lr: 1.1111e-04 - 110s/epoch - 562ms/step
Epoch 252/1000
2023-10-11 06:41:41.241 
Epoch 252/1000 
	 loss: 18.5074, MinusLogProbMetric: 18.5074, val_loss: 18.4407, val_MinusLogProbMetric: 18.4407

Epoch 252: val_loss did not improve from 18.42122
196/196 - 109s - loss: 18.5074 - MinusLogProbMetric: 18.5074 - val_loss: 18.4407 - val_MinusLogProbMetric: 18.4407 - lr: 1.1111e-04 - 109s/epoch - 557ms/step
Epoch 253/1000
2023-10-11 06:43:29.834 
Epoch 253/1000 
	 loss: 18.4818, MinusLogProbMetric: 18.4818, val_loss: 18.4610, val_MinusLogProbMetric: 18.4610

Epoch 253: val_loss did not improve from 18.42122
196/196 - 109s - loss: 18.4818 - MinusLogProbMetric: 18.4818 - val_loss: 18.4610 - val_MinusLogProbMetric: 18.4610 - lr: 1.1111e-04 - 109s/epoch - 554ms/step
Epoch 254/1000
2023-10-11 06:45:18.748 
Epoch 254/1000 
	 loss: 18.4819, MinusLogProbMetric: 18.4819, val_loss: 18.5424, val_MinusLogProbMetric: 18.5424

Epoch 254: val_loss did not improve from 18.42122
196/196 - 109s - loss: 18.4819 - MinusLogProbMetric: 18.4819 - val_loss: 18.5424 - val_MinusLogProbMetric: 18.5424 - lr: 1.1111e-04 - 109s/epoch - 556ms/step
Epoch 255/1000
2023-10-11 06:47:04.966 
Epoch 255/1000 
	 loss: 18.6541, MinusLogProbMetric: 18.6541, val_loss: 18.5458, val_MinusLogProbMetric: 18.5458

Epoch 255: val_loss did not improve from 18.42122
196/196 - 106s - loss: 18.6541 - MinusLogProbMetric: 18.6541 - val_loss: 18.5458 - val_MinusLogProbMetric: 18.5458 - lr: 1.1111e-04 - 106s/epoch - 542ms/step
Epoch 256/1000
2023-10-11 06:48:55.274 
Epoch 256/1000 
	 loss: 18.4685, MinusLogProbMetric: 18.4685, val_loss: 18.4296, val_MinusLogProbMetric: 18.4296

Epoch 256: val_loss did not improve from 18.42122
196/196 - 110s - loss: 18.4685 - MinusLogProbMetric: 18.4685 - val_loss: 18.4296 - val_MinusLogProbMetric: 18.4296 - lr: 1.1111e-04 - 110s/epoch - 563ms/step
Epoch 257/1000
2023-10-11 06:50:44.604 
Epoch 257/1000 
	 loss: 18.4369, MinusLogProbMetric: 18.4369, val_loss: 18.4060, val_MinusLogProbMetric: 18.4060

Epoch 257: val_loss improved from 18.42122 to 18.40597, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 110s - loss: 18.4369 - MinusLogProbMetric: 18.4369 - val_loss: 18.4060 - val_MinusLogProbMetric: 18.4060 - lr: 1.1111e-04 - 110s/epoch - 563ms/step
Epoch 258/1000
2023-10-11 06:52:34.972 
Epoch 258/1000 
	 loss: 18.4630, MinusLogProbMetric: 18.4630, val_loss: 18.5425, val_MinusLogProbMetric: 18.5425

Epoch 258: val_loss did not improve from 18.40597
196/196 - 109s - loss: 18.4630 - MinusLogProbMetric: 18.4630 - val_loss: 18.5425 - val_MinusLogProbMetric: 18.5425 - lr: 1.1111e-04 - 109s/epoch - 557ms/step
Epoch 259/1000
2023-10-11 06:54:21.737 
Epoch 259/1000 
	 loss: 18.4612, MinusLogProbMetric: 18.4612, val_loss: 18.8525, val_MinusLogProbMetric: 18.8525

Epoch 259: val_loss did not improve from 18.40597
196/196 - 107s - loss: 18.4612 - MinusLogProbMetric: 18.4612 - val_loss: 18.8525 - val_MinusLogProbMetric: 18.8525 - lr: 1.1111e-04 - 107s/epoch - 545ms/step
Epoch 260/1000
2023-10-11 06:56:09.769 
Epoch 260/1000 
	 loss: 18.5150, MinusLogProbMetric: 18.5150, val_loss: 18.4914, val_MinusLogProbMetric: 18.4914

Epoch 260: val_loss did not improve from 18.40597
196/196 - 108s - loss: 18.5150 - MinusLogProbMetric: 18.5150 - val_loss: 18.4914 - val_MinusLogProbMetric: 18.4914 - lr: 1.1111e-04 - 108s/epoch - 551ms/step
Epoch 261/1000
2023-10-11 06:58:01.827 
Epoch 261/1000 
	 loss: 18.4415, MinusLogProbMetric: 18.4415, val_loss: 18.4085, val_MinusLogProbMetric: 18.4085

Epoch 261: val_loss did not improve from 18.40597
196/196 - 112s - loss: 18.4415 - MinusLogProbMetric: 18.4415 - val_loss: 18.4085 - val_MinusLogProbMetric: 18.4085 - lr: 1.1111e-04 - 112s/epoch - 572ms/step
Epoch 262/1000
2023-10-11 06:59:58.554 
Epoch 262/1000 
	 loss: 18.4955, MinusLogProbMetric: 18.4955, val_loss: 18.8373, val_MinusLogProbMetric: 18.8373

Epoch 262: val_loss did not improve from 18.40597
196/196 - 117s - loss: 18.4955 - MinusLogProbMetric: 18.4955 - val_loss: 18.8373 - val_MinusLogProbMetric: 18.8373 - lr: 1.1111e-04 - 117s/epoch - 596ms/step
Epoch 263/1000
2023-10-11 07:01:49.852 
Epoch 263/1000 
	 loss: 18.4197, MinusLogProbMetric: 18.4197, val_loss: 18.4332, val_MinusLogProbMetric: 18.4332

Epoch 263: val_loss did not improve from 18.40597
196/196 - 111s - loss: 18.4197 - MinusLogProbMetric: 18.4197 - val_loss: 18.4332 - val_MinusLogProbMetric: 18.4332 - lr: 1.1111e-04 - 111s/epoch - 568ms/step
Epoch 264/1000
2023-10-11 07:03:36.616 
Epoch 264/1000 
	 loss: 18.3699, MinusLogProbMetric: 18.3699, val_loss: 18.3331, val_MinusLogProbMetric: 18.3331

Epoch 264: val_loss improved from 18.40597 to 18.33311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 109s - loss: 18.3699 - MinusLogProbMetric: 18.3699 - val_loss: 18.3331 - val_MinusLogProbMetric: 18.3331 - lr: 1.1111e-04 - 109s/epoch - 555ms/step
Epoch 265/1000
2023-10-11 07:05:30.892 
Epoch 265/1000 
	 loss: 18.4318, MinusLogProbMetric: 18.4318, val_loss: 18.7893, val_MinusLogProbMetric: 18.7893

Epoch 265: val_loss did not improve from 18.33311
196/196 - 112s - loss: 18.4318 - MinusLogProbMetric: 18.4318 - val_loss: 18.7893 - val_MinusLogProbMetric: 18.7893 - lr: 1.1111e-04 - 112s/epoch - 573ms/step
Epoch 266/1000
2023-10-11 07:07:18.176 
Epoch 266/1000 
	 loss: 18.4148, MinusLogProbMetric: 18.4148, val_loss: 18.3845, val_MinusLogProbMetric: 18.3845

Epoch 266: val_loss did not improve from 18.33311
196/196 - 107s - loss: 18.4148 - MinusLogProbMetric: 18.4148 - val_loss: 18.3845 - val_MinusLogProbMetric: 18.3845 - lr: 1.1111e-04 - 107s/epoch - 547ms/step
Epoch 267/1000
2023-10-11 07:09:06.152 
Epoch 267/1000 
	 loss: 18.4546, MinusLogProbMetric: 18.4546, val_loss: 18.2999, val_MinusLogProbMetric: 18.2999

Epoch 267: val_loss improved from 18.33311 to 18.29986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 111s - loss: 18.4546 - MinusLogProbMetric: 18.4546 - val_loss: 18.2999 - val_MinusLogProbMetric: 18.2999 - lr: 1.1111e-04 - 111s/epoch - 564ms/step
Epoch 268/1000
2023-10-11 07:10:59.773 
Epoch 268/1000 
	 loss: 18.3334, MinusLogProbMetric: 18.3334, val_loss: 18.9422, val_MinusLogProbMetric: 18.9422

Epoch 268: val_loss did not improve from 18.29986
196/196 - 111s - loss: 18.3334 - MinusLogProbMetric: 18.3334 - val_loss: 18.9422 - val_MinusLogProbMetric: 18.9422 - lr: 1.1111e-04 - 111s/epoch - 566ms/step
Epoch 269/1000
2023-10-11 07:12:51.493 
Epoch 269/1000 
	 loss: 18.3814, MinusLogProbMetric: 18.3814, val_loss: 18.3025, val_MinusLogProbMetric: 18.3025

Epoch 269: val_loss did not improve from 18.29986
196/196 - 112s - loss: 18.3814 - MinusLogProbMetric: 18.3814 - val_loss: 18.3025 - val_MinusLogProbMetric: 18.3025 - lr: 1.1111e-04 - 112s/epoch - 570ms/step
Epoch 270/1000
2023-10-11 07:14:46.574 
Epoch 270/1000 
	 loss: 18.3571, MinusLogProbMetric: 18.3571, val_loss: 18.2297, val_MinusLogProbMetric: 18.2297

Epoch 270: val_loss improved from 18.29986 to 18.22968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 117s - loss: 18.3571 - MinusLogProbMetric: 18.3571 - val_loss: 18.2297 - val_MinusLogProbMetric: 18.2297 - lr: 1.1111e-04 - 117s/epoch - 597ms/step
Epoch 271/1000
2023-10-11 07:16:35.836 
Epoch 271/1000 
	 loss: 18.3970, MinusLogProbMetric: 18.3970, val_loss: 18.2813, val_MinusLogProbMetric: 18.2813

Epoch 271: val_loss did not improve from 18.22968
196/196 - 107s - loss: 18.3970 - MinusLogProbMetric: 18.3970 - val_loss: 18.2813 - val_MinusLogProbMetric: 18.2813 - lr: 1.1111e-04 - 107s/epoch - 547ms/step
Epoch 272/1000
2023-10-11 07:18:24.010 
Epoch 272/1000 
	 loss: 18.3461, MinusLogProbMetric: 18.3461, val_loss: 18.2979, val_MinusLogProbMetric: 18.2979

Epoch 272: val_loss did not improve from 18.22968
196/196 - 108s - loss: 18.3461 - MinusLogProbMetric: 18.3461 - val_loss: 18.2979 - val_MinusLogProbMetric: 18.2979 - lr: 1.1111e-04 - 108s/epoch - 552ms/step
Epoch 273/1000
2023-10-11 07:20:13.179 
Epoch 273/1000 
	 loss: 18.4387, MinusLogProbMetric: 18.4387, val_loss: 18.4710, val_MinusLogProbMetric: 18.4710

Epoch 273: val_loss did not improve from 18.22968
196/196 - 109s - loss: 18.4387 - MinusLogProbMetric: 18.4387 - val_loss: 18.4710 - val_MinusLogProbMetric: 18.4710 - lr: 1.1111e-04 - 109s/epoch - 557ms/step
Epoch 274/1000
2023-10-11 07:22:01.176 
Epoch 274/1000 
	 loss: 18.3859, MinusLogProbMetric: 18.3859, val_loss: 19.3713, val_MinusLogProbMetric: 19.3713

Epoch 274: val_loss did not improve from 18.22968
196/196 - 108s - loss: 18.3859 - MinusLogProbMetric: 18.3859 - val_loss: 19.3713 - val_MinusLogProbMetric: 19.3713 - lr: 1.1111e-04 - 108s/epoch - 551ms/step
Epoch 275/1000
2023-10-11 07:23:43.540 
Epoch 275/1000 
	 loss: 18.4440, MinusLogProbMetric: 18.4440, val_loss: 18.6571, val_MinusLogProbMetric: 18.6571

Epoch 275: val_loss did not improve from 18.22968
196/196 - 102s - loss: 18.4440 - MinusLogProbMetric: 18.4440 - val_loss: 18.6571 - val_MinusLogProbMetric: 18.6571 - lr: 1.1111e-04 - 102s/epoch - 522ms/step
Epoch 276/1000
2023-10-11 07:25:30.873 
Epoch 276/1000 
	 loss: 18.3519, MinusLogProbMetric: 18.3519, val_loss: 18.2964, val_MinusLogProbMetric: 18.2964

Epoch 276: val_loss did not improve from 18.22968
196/196 - 107s - loss: 18.3519 - MinusLogProbMetric: 18.3519 - val_loss: 18.2964 - val_MinusLogProbMetric: 18.2964 - lr: 1.1111e-04 - 107s/epoch - 548ms/step
Epoch 277/1000
2023-10-11 07:27:23.248 
Epoch 277/1000 
	 loss: 18.2809, MinusLogProbMetric: 18.2809, val_loss: 19.1845, val_MinusLogProbMetric: 19.1845

Epoch 277: val_loss did not improve from 18.22968
196/196 - 112s - loss: 18.2809 - MinusLogProbMetric: 18.2809 - val_loss: 19.1845 - val_MinusLogProbMetric: 19.1845 - lr: 1.1111e-04 - 112s/epoch - 573ms/step
Epoch 278/1000
2023-10-11 07:29:11.713 
Epoch 278/1000 
	 loss: 18.3659, MinusLogProbMetric: 18.3659, val_loss: 18.4754, val_MinusLogProbMetric: 18.4754

Epoch 278: val_loss did not improve from 18.22968
196/196 - 108s - loss: 18.3659 - MinusLogProbMetric: 18.3659 - val_loss: 18.4754 - val_MinusLogProbMetric: 18.4754 - lr: 1.1111e-04 - 108s/epoch - 554ms/step
Epoch 279/1000
2023-10-11 07:31:01.245 
Epoch 279/1000 
	 loss: 18.3168, MinusLogProbMetric: 18.3168, val_loss: 18.2670, val_MinusLogProbMetric: 18.2670

Epoch 279: val_loss did not improve from 18.22968
196/196 - 109s - loss: 18.3168 - MinusLogProbMetric: 18.3168 - val_loss: 18.2670 - val_MinusLogProbMetric: 18.2670 - lr: 1.1111e-04 - 109s/epoch - 559ms/step
Epoch 280/1000
2023-10-11 07:32:49.117 
Epoch 280/1000 
	 loss: 18.3403, MinusLogProbMetric: 18.3403, val_loss: 18.4779, val_MinusLogProbMetric: 18.4779

Epoch 280: val_loss did not improve from 18.22968
196/196 - 108s - loss: 18.3403 - MinusLogProbMetric: 18.3403 - val_loss: 18.4779 - val_MinusLogProbMetric: 18.4779 - lr: 1.1111e-04 - 108s/epoch - 550ms/step
Epoch 281/1000
2023-10-11 07:34:44.574 
Epoch 281/1000 
	 loss: 18.3514, MinusLogProbMetric: 18.3514, val_loss: 18.3027, val_MinusLogProbMetric: 18.3027

Epoch 281: val_loss did not improve from 18.22968
196/196 - 115s - loss: 18.3514 - MinusLogProbMetric: 18.3514 - val_loss: 18.3027 - val_MinusLogProbMetric: 18.3027 - lr: 1.1111e-04 - 115s/epoch - 589ms/step
Epoch 282/1000
2023-10-11 07:36:36.925 
Epoch 282/1000 
	 loss: 18.2658, MinusLogProbMetric: 18.2658, val_loss: 18.2045, val_MinusLogProbMetric: 18.2045

Epoch 282: val_loss improved from 18.22968 to 18.20451, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 18.2658 - MinusLogProbMetric: 18.2658 - val_loss: 18.2045 - val_MinusLogProbMetric: 18.2045 - lr: 1.1111e-04 - 115s/epoch - 588ms/step
Epoch 283/1000
2023-10-11 07:38:36.013 
Epoch 283/1000 
	 loss: 18.2791, MinusLogProbMetric: 18.2791, val_loss: 18.7572, val_MinusLogProbMetric: 18.7572

Epoch 283: val_loss did not improve from 18.20451
196/196 - 116s - loss: 18.2791 - MinusLogProbMetric: 18.2791 - val_loss: 18.7572 - val_MinusLogProbMetric: 18.7572 - lr: 1.1111e-04 - 116s/epoch - 592ms/step
Epoch 284/1000
2023-10-11 07:40:27.523 
Epoch 284/1000 
	 loss: 18.2616, MinusLogProbMetric: 18.2616, val_loss: 18.3608, val_MinusLogProbMetric: 18.3608

Epoch 284: val_loss did not improve from 18.20451
196/196 - 111s - loss: 18.2616 - MinusLogProbMetric: 18.2616 - val_loss: 18.3608 - val_MinusLogProbMetric: 18.3608 - lr: 1.1111e-04 - 111s/epoch - 569ms/step
Epoch 285/1000
2023-10-11 07:42:19.980 
Epoch 285/1000 
	 loss: 18.2408, MinusLogProbMetric: 18.2408, val_loss: 18.9780, val_MinusLogProbMetric: 18.9780

Epoch 285: val_loss did not improve from 18.20451
196/196 - 112s - loss: 18.2408 - MinusLogProbMetric: 18.2408 - val_loss: 18.9780 - val_MinusLogProbMetric: 18.9780 - lr: 1.1111e-04 - 112s/epoch - 574ms/step
Epoch 286/1000
2023-10-11 07:44:09.273 
Epoch 286/1000 
	 loss: 18.3298, MinusLogProbMetric: 18.3298, val_loss: 18.2613, val_MinusLogProbMetric: 18.2613

Epoch 286: val_loss did not improve from 18.20451
196/196 - 109s - loss: 18.3298 - MinusLogProbMetric: 18.3298 - val_loss: 18.2613 - val_MinusLogProbMetric: 18.2613 - lr: 1.1111e-04 - 109s/epoch - 558ms/step
Epoch 287/1000
2023-10-11 07:46:01.451 
Epoch 287/1000 
	 loss: 18.3226, MinusLogProbMetric: 18.3226, val_loss: 19.4590, val_MinusLogProbMetric: 19.4590

Epoch 287: val_loss did not improve from 18.20451
196/196 - 112s - loss: 18.3226 - MinusLogProbMetric: 18.3226 - val_loss: 19.4590 - val_MinusLogProbMetric: 19.4590 - lr: 1.1111e-04 - 112s/epoch - 572ms/step
Epoch 288/1000
2023-10-11 07:47:57.650 
Epoch 288/1000 
	 loss: 18.3028, MinusLogProbMetric: 18.3028, val_loss: 18.3182, val_MinusLogProbMetric: 18.3182

Epoch 288: val_loss did not improve from 18.20451
196/196 - 116s - loss: 18.3028 - MinusLogProbMetric: 18.3028 - val_loss: 18.3182 - val_MinusLogProbMetric: 18.3182 - lr: 1.1111e-04 - 116s/epoch - 593ms/step
Epoch 289/1000
2023-10-11 07:49:47.448 
Epoch 289/1000 
	 loss: 18.3338, MinusLogProbMetric: 18.3338, val_loss: 18.6145, val_MinusLogProbMetric: 18.6145

Epoch 289: val_loss did not improve from 18.20451
196/196 - 110s - loss: 18.3338 - MinusLogProbMetric: 18.3338 - val_loss: 18.6145 - val_MinusLogProbMetric: 18.6145 - lr: 1.1111e-04 - 110s/epoch - 560ms/step
Epoch 290/1000
2023-10-11 07:51:37.513 
Epoch 290/1000 
	 loss: 18.2699, MinusLogProbMetric: 18.2699, val_loss: 18.1769, val_MinusLogProbMetric: 18.1769

Epoch 290: val_loss improved from 18.20451 to 18.17687, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 112s - loss: 18.2699 - MinusLogProbMetric: 18.2699 - val_loss: 18.1769 - val_MinusLogProbMetric: 18.1769 - lr: 1.1111e-04 - 112s/epoch - 574ms/step
Epoch 291/1000
2023-10-11 07:53:26.765 
Epoch 291/1000 
	 loss: 18.2365, MinusLogProbMetric: 18.2365, val_loss: 18.5487, val_MinusLogProbMetric: 18.5487

Epoch 291: val_loss did not improve from 18.17687
196/196 - 107s - loss: 18.2365 - MinusLogProbMetric: 18.2365 - val_loss: 18.5487 - val_MinusLogProbMetric: 18.5487 - lr: 1.1111e-04 - 107s/epoch - 545ms/step
Epoch 292/1000
2023-10-11 07:55:17.777 
Epoch 292/1000 
	 loss: 18.3026, MinusLogProbMetric: 18.3026, val_loss: 18.3287, val_MinusLogProbMetric: 18.3287

Epoch 292: val_loss did not improve from 18.17687
196/196 - 111s - loss: 18.3026 - MinusLogProbMetric: 18.3026 - val_loss: 18.3287 - val_MinusLogProbMetric: 18.3287 - lr: 1.1111e-04 - 111s/epoch - 567ms/step
Epoch 293/1000
2023-10-11 07:57:12.331 
Epoch 293/1000 
	 loss: 18.2520, MinusLogProbMetric: 18.2520, val_loss: 18.3653, val_MinusLogProbMetric: 18.3653

Epoch 293: val_loss did not improve from 18.17687
196/196 - 114s - loss: 18.2520 - MinusLogProbMetric: 18.2520 - val_loss: 18.3653 - val_MinusLogProbMetric: 18.3653 - lr: 1.1111e-04 - 114s/epoch - 584ms/step
Epoch 294/1000
2023-10-11 07:59:01.347 
Epoch 294/1000 
	 loss: 18.1908, MinusLogProbMetric: 18.1908, val_loss: 18.2310, val_MinusLogProbMetric: 18.2310

Epoch 294: val_loss did not improve from 18.17687
196/196 - 109s - loss: 18.1908 - MinusLogProbMetric: 18.1908 - val_loss: 18.2310 - val_MinusLogProbMetric: 18.2310 - lr: 1.1111e-04 - 109s/epoch - 556ms/step
Epoch 295/1000
2023-10-11 08:00:52.079 
Epoch 295/1000 
	 loss: 18.2664, MinusLogProbMetric: 18.2664, val_loss: 18.2728, val_MinusLogProbMetric: 18.2728

Epoch 295: val_loss did not improve from 18.17687
196/196 - 111s - loss: 18.2664 - MinusLogProbMetric: 18.2664 - val_loss: 18.2728 - val_MinusLogProbMetric: 18.2728 - lr: 1.1111e-04 - 111s/epoch - 565ms/step
Epoch 296/1000
2023-10-11 08:02:44.092 
Epoch 296/1000 
	 loss: 18.2607, MinusLogProbMetric: 18.2607, val_loss: 18.3522, val_MinusLogProbMetric: 18.3522

Epoch 296: val_loss did not improve from 18.17687
196/196 - 112s - loss: 18.2607 - MinusLogProbMetric: 18.2607 - val_loss: 18.3522 - val_MinusLogProbMetric: 18.3522 - lr: 1.1111e-04 - 112s/epoch - 572ms/step
Epoch 297/1000
2023-10-11 08:04:30.050 
Epoch 297/1000 
	 loss: 18.2824, MinusLogProbMetric: 18.2824, val_loss: 18.3303, val_MinusLogProbMetric: 18.3303

Epoch 297: val_loss did not improve from 18.17687
196/196 - 106s - loss: 18.2824 - MinusLogProbMetric: 18.2824 - val_loss: 18.3303 - val_MinusLogProbMetric: 18.3303 - lr: 1.1111e-04 - 106s/epoch - 541ms/step
Epoch 298/1000
2023-10-11 08:06:21.989 
Epoch 298/1000 
	 loss: 18.2052, MinusLogProbMetric: 18.2052, val_loss: 18.3077, val_MinusLogProbMetric: 18.3077

Epoch 298: val_loss did not improve from 18.17687
196/196 - 112s - loss: 18.2052 - MinusLogProbMetric: 18.2052 - val_loss: 18.3077 - val_MinusLogProbMetric: 18.3077 - lr: 1.1111e-04 - 112s/epoch - 571ms/step
Epoch 299/1000
2023-10-11 08:08:10.847 
Epoch 299/1000 
	 loss: 18.2498, MinusLogProbMetric: 18.2498, val_loss: 18.2659, val_MinusLogProbMetric: 18.2659

Epoch 299: val_loss did not improve from 18.17687
196/196 - 109s - loss: 18.2498 - MinusLogProbMetric: 18.2498 - val_loss: 18.2659 - val_MinusLogProbMetric: 18.2659 - lr: 1.1111e-04 - 109s/epoch - 555ms/step
Epoch 300/1000
2023-10-11 08:10:00.943 
Epoch 300/1000 
	 loss: 18.2232, MinusLogProbMetric: 18.2232, val_loss: 18.3798, val_MinusLogProbMetric: 18.3798

Epoch 300: val_loss did not improve from 18.17687
196/196 - 110s - loss: 18.2232 - MinusLogProbMetric: 18.2232 - val_loss: 18.3798 - val_MinusLogProbMetric: 18.3798 - lr: 1.1111e-04 - 110s/epoch - 562ms/step
Epoch 301/1000
2023-10-11 08:11:58.570 
Epoch 301/1000 
	 loss: 18.2643, MinusLogProbMetric: 18.2643, val_loss: 18.1336, val_MinusLogProbMetric: 18.1336

Epoch 301: val_loss improved from 18.17687 to 18.13356, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 119s - loss: 18.2643 - MinusLogProbMetric: 18.2643 - val_loss: 18.1336 - val_MinusLogProbMetric: 18.1336 - lr: 1.1111e-04 - 119s/epoch - 608ms/step
Epoch 302/1000
2023-10-11 08:13:54.277 
Epoch 302/1000 
	 loss: 18.2048, MinusLogProbMetric: 18.2048, val_loss: 19.3619, val_MinusLogProbMetric: 19.3619

Epoch 302: val_loss did not improve from 18.13356
196/196 - 114s - loss: 18.2048 - MinusLogProbMetric: 18.2048 - val_loss: 19.3619 - val_MinusLogProbMetric: 19.3619 - lr: 1.1111e-04 - 114s/epoch - 583ms/step
Epoch 303/1000
2023-10-11 08:15:43.347 
Epoch 303/1000 
	 loss: 18.2050, MinusLogProbMetric: 18.2050, val_loss: 18.2790, val_MinusLogProbMetric: 18.2790

Epoch 303: val_loss did not improve from 18.13356
196/196 - 109s - loss: 18.2050 - MinusLogProbMetric: 18.2050 - val_loss: 18.2790 - val_MinusLogProbMetric: 18.2790 - lr: 1.1111e-04 - 109s/epoch - 556ms/step
Epoch 304/1000
2023-10-11 08:17:36.952 
Epoch 304/1000 
	 loss: 18.2816, MinusLogProbMetric: 18.2816, val_loss: 18.8345, val_MinusLogProbMetric: 18.8345

Epoch 304: val_loss did not improve from 18.13356
196/196 - 114s - loss: 18.2816 - MinusLogProbMetric: 18.2816 - val_loss: 18.8345 - val_MinusLogProbMetric: 18.8345 - lr: 1.1111e-04 - 114s/epoch - 580ms/step
Epoch 305/1000
2023-10-11 08:19:27.212 
Epoch 305/1000 
	 loss: 18.1652, MinusLogProbMetric: 18.1652, val_loss: 18.1951, val_MinusLogProbMetric: 18.1951

Epoch 305: val_loss did not improve from 18.13356
196/196 - 110s - loss: 18.1652 - MinusLogProbMetric: 18.1652 - val_loss: 18.1951 - val_MinusLogProbMetric: 18.1951 - lr: 1.1111e-04 - 110s/epoch - 563ms/step
Epoch 306/1000
2023-10-11 08:21:12.910 
Epoch 306/1000 
	 loss: 18.2105, MinusLogProbMetric: 18.2105, val_loss: 18.4208, val_MinusLogProbMetric: 18.4208

Epoch 306: val_loss did not improve from 18.13356
196/196 - 106s - loss: 18.2105 - MinusLogProbMetric: 18.2105 - val_loss: 18.4208 - val_MinusLogProbMetric: 18.4208 - lr: 1.1111e-04 - 106s/epoch - 539ms/step
Epoch 307/1000
2023-10-11 08:23:03.727 
Epoch 307/1000 
	 loss: 18.1468, MinusLogProbMetric: 18.1468, val_loss: 18.1108, val_MinusLogProbMetric: 18.1108

Epoch 307: val_loss improved from 18.13356 to 18.11076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 114s - loss: 18.1468 - MinusLogProbMetric: 18.1468 - val_loss: 18.1108 - val_MinusLogProbMetric: 18.1108 - lr: 1.1111e-04 - 114s/epoch - 580ms/step
Epoch 308/1000
2023-10-11 08:24:52.778 
Epoch 308/1000 
	 loss: 18.1779, MinusLogProbMetric: 18.1779, val_loss: 18.1508, val_MinusLogProbMetric: 18.1508

Epoch 308: val_loss did not improve from 18.11076
196/196 - 106s - loss: 18.1779 - MinusLogProbMetric: 18.1779 - val_loss: 18.1508 - val_MinusLogProbMetric: 18.1508 - lr: 1.1111e-04 - 106s/epoch - 542ms/step
Epoch 309/1000
2023-10-11 08:26:43.965 
Epoch 309/1000 
	 loss: 18.2406, MinusLogProbMetric: 18.2406, val_loss: 18.1455, val_MinusLogProbMetric: 18.1455

Epoch 309: val_loss did not improve from 18.11076
196/196 - 111s - loss: 18.2406 - MinusLogProbMetric: 18.2406 - val_loss: 18.1455 - val_MinusLogProbMetric: 18.1455 - lr: 1.1111e-04 - 111s/epoch - 567ms/step
Epoch 310/1000
2023-10-11 08:28:40.077 
Epoch 310/1000 
	 loss: 18.1706, MinusLogProbMetric: 18.1706, val_loss: 18.1755, val_MinusLogProbMetric: 18.1755

Epoch 310: val_loss did not improve from 18.11076
196/196 - 116s - loss: 18.1706 - MinusLogProbMetric: 18.1706 - val_loss: 18.1755 - val_MinusLogProbMetric: 18.1755 - lr: 1.1111e-04 - 116s/epoch - 592ms/step
Epoch 311/1000
2023-10-11 08:30:32.877 
Epoch 311/1000 
	 loss: 18.1689, MinusLogProbMetric: 18.1689, val_loss: 18.2767, val_MinusLogProbMetric: 18.2767

Epoch 311: val_loss did not improve from 18.11076
196/196 - 113s - loss: 18.1689 - MinusLogProbMetric: 18.1689 - val_loss: 18.2767 - val_MinusLogProbMetric: 18.2767 - lr: 1.1111e-04 - 113s/epoch - 576ms/step
Epoch 312/1000
2023-10-11 08:32:22.089 
Epoch 312/1000 
	 loss: 18.2107, MinusLogProbMetric: 18.2107, val_loss: 18.1439, val_MinusLogProbMetric: 18.1439

Epoch 312: val_loss did not improve from 18.11076
196/196 - 109s - loss: 18.2107 - MinusLogProbMetric: 18.2107 - val_loss: 18.1439 - val_MinusLogProbMetric: 18.1439 - lr: 1.1111e-04 - 109s/epoch - 557ms/step
Epoch 313/1000
2023-10-11 08:34:13.632 
Epoch 313/1000 
	 loss: 18.1363, MinusLogProbMetric: 18.1363, val_loss: 18.4798, val_MinusLogProbMetric: 18.4798

Epoch 313: val_loss did not improve from 18.11076
196/196 - 112s - loss: 18.1363 - MinusLogProbMetric: 18.1363 - val_loss: 18.4798 - val_MinusLogProbMetric: 18.4798 - lr: 1.1111e-04 - 112s/epoch - 569ms/step
Epoch 314/1000
2023-10-11 08:36:02.004 
Epoch 314/1000 
	 loss: 18.2142, MinusLogProbMetric: 18.2142, val_loss: 18.2143, val_MinusLogProbMetric: 18.2143

Epoch 314: val_loss did not improve from 18.11076
196/196 - 108s - loss: 18.2142 - MinusLogProbMetric: 18.2142 - val_loss: 18.2143 - val_MinusLogProbMetric: 18.2143 - lr: 1.1111e-04 - 108s/epoch - 553ms/step
Epoch 315/1000
2023-10-11 08:37:54.240 
Epoch 315/1000 
	 loss: 18.0952, MinusLogProbMetric: 18.0952, val_loss: 18.1931, val_MinusLogProbMetric: 18.1931

Epoch 315: val_loss did not improve from 18.11076
196/196 - 112s - loss: 18.0952 - MinusLogProbMetric: 18.0952 - val_loss: 18.1931 - val_MinusLogProbMetric: 18.1931 - lr: 1.1111e-04 - 112s/epoch - 573ms/step
Epoch 316/1000
2023-10-11 08:39:50.029 
Epoch 316/1000 
	 loss: 18.2440, MinusLogProbMetric: 18.2440, val_loss: 18.1013, val_MinusLogProbMetric: 18.1013

Epoch 316: val_loss improved from 18.11076 to 18.10133, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 117s - loss: 18.2440 - MinusLogProbMetric: 18.2440 - val_loss: 18.1013 - val_MinusLogProbMetric: 18.1013 - lr: 1.1111e-04 - 117s/epoch - 599ms/step
Epoch 317/1000
2023-10-11 08:41:36.977 
Epoch 317/1000 
	 loss: 18.2075, MinusLogProbMetric: 18.2075, val_loss: 18.4072, val_MinusLogProbMetric: 18.4072

Epoch 317: val_loss did not improve from 18.10133
196/196 - 105s - loss: 18.2075 - MinusLogProbMetric: 18.2075 - val_loss: 18.4072 - val_MinusLogProbMetric: 18.4072 - lr: 1.1111e-04 - 105s/epoch - 537ms/step
Epoch 318/1000
2023-10-11 08:43:31.950 
Epoch 318/1000 
	 loss: 18.1467, MinusLogProbMetric: 18.1467, val_loss: 18.1818, val_MinusLogProbMetric: 18.1818

Epoch 318: val_loss did not improve from 18.10133
196/196 - 115s - loss: 18.1467 - MinusLogProbMetric: 18.1467 - val_loss: 18.1818 - val_MinusLogProbMetric: 18.1818 - lr: 1.1111e-04 - 115s/epoch - 587ms/step
Epoch 319/1000
2023-10-11 08:45:24.192 
Epoch 319/1000 
	 loss: 18.1583, MinusLogProbMetric: 18.1583, val_loss: 18.0458, val_MinusLogProbMetric: 18.0458

Epoch 319: val_loss improved from 18.10133 to 18.04580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 114s - loss: 18.1583 - MinusLogProbMetric: 18.1583 - val_loss: 18.0458 - val_MinusLogProbMetric: 18.0458 - lr: 1.1111e-04 - 114s/epoch - 584ms/step
Epoch 320/1000
2023-10-11 08:47:15.484 
Epoch 320/1000 
	 loss: 18.1538, MinusLogProbMetric: 18.1538, val_loss: 18.1304, val_MinusLogProbMetric: 18.1304

Epoch 320: val_loss did not improve from 18.04580
196/196 - 109s - loss: 18.1538 - MinusLogProbMetric: 18.1538 - val_loss: 18.1304 - val_MinusLogProbMetric: 18.1304 - lr: 1.1111e-04 - 109s/epoch - 556ms/step
Epoch 321/1000
2023-10-11 08:49:06.926 
Epoch 321/1000 
	 loss: 18.1398, MinusLogProbMetric: 18.1398, val_loss: 18.2333, val_MinusLogProbMetric: 18.2333

Epoch 321: val_loss did not improve from 18.04580
196/196 - 111s - loss: 18.1398 - MinusLogProbMetric: 18.1398 - val_loss: 18.2333 - val_MinusLogProbMetric: 18.2333 - lr: 1.1111e-04 - 111s/epoch - 568ms/step
Epoch 322/1000
2023-10-11 08:50:53.943 
Epoch 322/1000 
	 loss: 18.1712, MinusLogProbMetric: 18.1712, val_loss: 18.1632, val_MinusLogProbMetric: 18.1632

Epoch 322: val_loss did not improve from 18.04580
196/196 - 107s - loss: 18.1712 - MinusLogProbMetric: 18.1712 - val_loss: 18.1632 - val_MinusLogProbMetric: 18.1632 - lr: 1.1111e-04 - 107s/epoch - 546ms/step
Epoch 323/1000
2023-10-11 08:52:44.797 
Epoch 323/1000 
	 loss: 18.1070, MinusLogProbMetric: 18.1070, val_loss: 18.2649, val_MinusLogProbMetric: 18.2649

Epoch 323: val_loss did not improve from 18.04580
196/196 - 111s - loss: 18.1070 - MinusLogProbMetric: 18.1070 - val_loss: 18.2649 - val_MinusLogProbMetric: 18.2649 - lr: 1.1111e-04 - 111s/epoch - 566ms/step
Epoch 324/1000
2023-10-11 08:54:40.637 
Epoch 324/1000 
	 loss: 18.1485, MinusLogProbMetric: 18.1485, val_loss: 18.0744, val_MinusLogProbMetric: 18.0744

Epoch 324: val_loss did not improve from 18.04580
196/196 - 116s - loss: 18.1485 - MinusLogProbMetric: 18.1485 - val_loss: 18.0744 - val_MinusLogProbMetric: 18.0744 - lr: 1.1111e-04 - 116s/epoch - 591ms/step
Epoch 325/1000
2023-10-11 08:56:34.881 
Epoch 325/1000 
	 loss: 18.1270, MinusLogProbMetric: 18.1270, val_loss: 17.9768, val_MinusLogProbMetric: 17.9768

Epoch 325: val_loss improved from 18.04580 to 17.97685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 116s - loss: 18.1270 - MinusLogProbMetric: 18.1270 - val_loss: 17.9768 - val_MinusLogProbMetric: 17.9768 - lr: 1.1111e-04 - 116s/epoch - 590ms/step
Epoch 326/1000
2023-10-11 08:58:36.535 
Epoch 326/1000 
	 loss: 18.0986, MinusLogProbMetric: 18.0986, val_loss: 18.0816, val_MinusLogProbMetric: 18.0816

Epoch 326: val_loss did not improve from 17.97685
196/196 - 120s - loss: 18.0986 - MinusLogProbMetric: 18.0986 - val_loss: 18.0816 - val_MinusLogProbMetric: 18.0816 - lr: 1.1111e-04 - 120s/epoch - 614ms/step
Epoch 327/1000
2023-10-11 09:00:31.821 
Epoch 327/1000 
	 loss: 18.1852, MinusLogProbMetric: 18.1852, val_loss: 18.1258, val_MinusLogProbMetric: 18.1258

Epoch 327: val_loss did not improve from 17.97685
196/196 - 115s - loss: 18.1852 - MinusLogProbMetric: 18.1852 - val_loss: 18.1258 - val_MinusLogProbMetric: 18.1258 - lr: 1.1111e-04 - 115s/epoch - 588ms/step
Epoch 328/1000
2023-10-11 09:02:29.260 
Epoch 328/1000 
	 loss: 18.1013, MinusLogProbMetric: 18.1013, val_loss: 19.0478, val_MinusLogProbMetric: 19.0478

Epoch 328: val_loss did not improve from 17.97685
196/196 - 117s - loss: 18.1013 - MinusLogProbMetric: 18.1013 - val_loss: 19.0478 - val_MinusLogProbMetric: 19.0478 - lr: 1.1111e-04 - 117s/epoch - 599ms/step
Epoch 329/1000
2023-10-11 09:04:26.448 
Epoch 329/1000 
	 loss: 18.0988, MinusLogProbMetric: 18.0988, val_loss: 18.0001, val_MinusLogProbMetric: 18.0001

Epoch 329: val_loss did not improve from 17.97685
196/196 - 117s - loss: 18.0988 - MinusLogProbMetric: 18.0988 - val_loss: 18.0001 - val_MinusLogProbMetric: 18.0001 - lr: 1.1111e-04 - 117s/epoch - 598ms/step
Epoch 330/1000
2023-10-11 09:06:26.815 
Epoch 330/1000 
	 loss: 18.0749, MinusLogProbMetric: 18.0749, val_loss: 18.0694, val_MinusLogProbMetric: 18.0694

Epoch 330: val_loss did not improve from 17.97685
196/196 - 120s - loss: 18.0749 - MinusLogProbMetric: 18.0749 - val_loss: 18.0694 - val_MinusLogProbMetric: 18.0694 - lr: 1.1111e-04 - 120s/epoch - 614ms/step
Epoch 331/1000
2023-10-11 09:08:24.530 
Epoch 331/1000 
	 loss: 18.0986, MinusLogProbMetric: 18.0986, val_loss: 18.6522, val_MinusLogProbMetric: 18.6522

Epoch 331: val_loss did not improve from 17.97685
196/196 - 118s - loss: 18.0986 - MinusLogProbMetric: 18.0986 - val_loss: 18.6522 - val_MinusLogProbMetric: 18.6522 - lr: 1.1111e-04 - 118s/epoch - 601ms/step
Epoch 332/1000
2023-10-11 09:10:19.993 
Epoch 332/1000 
	 loss: 18.1200, MinusLogProbMetric: 18.1200, val_loss: 18.0114, val_MinusLogProbMetric: 18.0114

Epoch 332: val_loss did not improve from 17.97685
196/196 - 116s - loss: 18.1200 - MinusLogProbMetric: 18.1200 - val_loss: 18.0114 - val_MinusLogProbMetric: 18.0114 - lr: 1.1111e-04 - 116s/epoch - 589ms/step
Epoch 333/1000
2023-10-11 09:12:19.717 
Epoch 333/1000 
	 loss: 18.0777, MinusLogProbMetric: 18.0777, val_loss: 18.2544, val_MinusLogProbMetric: 18.2544

Epoch 333: val_loss did not improve from 17.97685
196/196 - 120s - loss: 18.0777 - MinusLogProbMetric: 18.0777 - val_loss: 18.2544 - val_MinusLogProbMetric: 18.2544 - lr: 1.1111e-04 - 120s/epoch - 610ms/step
Epoch 334/1000
2023-10-11 09:14:17.198 
Epoch 334/1000 
	 loss: 18.0661, MinusLogProbMetric: 18.0661, val_loss: 18.0159, val_MinusLogProbMetric: 18.0159

Epoch 334: val_loss did not improve from 17.97685
196/196 - 117s - loss: 18.0661 - MinusLogProbMetric: 18.0661 - val_loss: 18.0159 - val_MinusLogProbMetric: 18.0159 - lr: 1.1111e-04 - 117s/epoch - 599ms/step
Epoch 335/1000
2023-10-11 09:16:16.857 
Epoch 335/1000 
	 loss: 18.0483, MinusLogProbMetric: 18.0483, val_loss: 18.1085, val_MinusLogProbMetric: 18.1085

Epoch 335: val_loss did not improve from 17.97685
196/196 - 120s - loss: 18.0483 - MinusLogProbMetric: 18.0483 - val_loss: 18.1085 - val_MinusLogProbMetric: 18.1085 - lr: 1.1111e-04 - 120s/epoch - 611ms/step
Epoch 336/1000
2023-10-11 09:18:15.570 
Epoch 336/1000 
	 loss: 18.1250, MinusLogProbMetric: 18.1250, val_loss: 18.1316, val_MinusLogProbMetric: 18.1316

Epoch 336: val_loss did not improve from 17.97685
196/196 - 119s - loss: 18.1250 - MinusLogProbMetric: 18.1250 - val_loss: 18.1316 - val_MinusLogProbMetric: 18.1316 - lr: 1.1111e-04 - 119s/epoch - 606ms/step
Epoch 337/1000
2023-10-11 09:20:17.432 
Epoch 337/1000 
	 loss: 18.0277, MinusLogProbMetric: 18.0277, val_loss: 18.0710, val_MinusLogProbMetric: 18.0710

Epoch 337: val_loss did not improve from 17.97685
196/196 - 122s - loss: 18.0277 - MinusLogProbMetric: 18.0277 - val_loss: 18.0710 - val_MinusLogProbMetric: 18.0710 - lr: 1.1111e-04 - 122s/epoch - 622ms/step
Epoch 338/1000
2023-10-11 09:22:17.909 
Epoch 338/1000 
	 loss: 18.0796, MinusLogProbMetric: 18.0796, val_loss: 17.9846, val_MinusLogProbMetric: 17.9846

Epoch 338: val_loss did not improve from 17.97685
196/196 - 120s - loss: 18.0796 - MinusLogProbMetric: 18.0796 - val_loss: 17.9846 - val_MinusLogProbMetric: 17.9846 - lr: 1.1111e-04 - 120s/epoch - 615ms/step
Epoch 339/1000
2023-10-11 09:24:22.311 
Epoch 339/1000 
	 loss: 18.1240, MinusLogProbMetric: 18.1240, val_loss: 18.0168, val_MinusLogProbMetric: 18.0168

Epoch 339: val_loss did not improve from 17.97685
196/196 - 124s - loss: 18.1240 - MinusLogProbMetric: 18.1240 - val_loss: 18.0168 - val_MinusLogProbMetric: 18.0168 - lr: 1.1111e-04 - 124s/epoch - 635ms/step
Epoch 340/1000
2023-10-11 09:26:22.202 
Epoch 340/1000 
	 loss: 18.0246, MinusLogProbMetric: 18.0246, val_loss: 18.1246, val_MinusLogProbMetric: 18.1246

Epoch 340: val_loss did not improve from 17.97685
196/196 - 120s - loss: 18.0246 - MinusLogProbMetric: 18.0246 - val_loss: 18.1246 - val_MinusLogProbMetric: 18.1246 - lr: 1.1111e-04 - 120s/epoch - 612ms/step
Epoch 341/1000
2023-10-11 09:28:17.768 
Epoch 341/1000 
	 loss: 18.0201, MinusLogProbMetric: 18.0201, val_loss: 18.1283, val_MinusLogProbMetric: 18.1283

Epoch 341: val_loss did not improve from 17.97685
196/196 - 116s - loss: 18.0201 - MinusLogProbMetric: 18.0201 - val_loss: 18.1283 - val_MinusLogProbMetric: 18.1283 - lr: 1.1111e-04 - 116s/epoch - 590ms/step
Epoch 342/1000
2023-10-11 09:30:21.437 
Epoch 342/1000 
	 loss: 18.0471, MinusLogProbMetric: 18.0471, val_loss: 18.2626, val_MinusLogProbMetric: 18.2626

Epoch 342: val_loss did not improve from 17.97685
196/196 - 124s - loss: 18.0471 - MinusLogProbMetric: 18.0471 - val_loss: 18.2626 - val_MinusLogProbMetric: 18.2626 - lr: 1.1111e-04 - 124s/epoch - 631ms/step
Epoch 343/1000
2023-10-11 09:32:22.366 
Epoch 343/1000 
	 loss: 18.0843, MinusLogProbMetric: 18.0843, val_loss: 17.9529, val_MinusLogProbMetric: 17.9529

Epoch 343: val_loss improved from 17.97685 to 17.95289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 123s - loss: 18.0843 - MinusLogProbMetric: 18.0843 - val_loss: 17.9529 - val_MinusLogProbMetric: 17.9529 - lr: 1.1111e-04 - 123s/epoch - 627ms/step
Epoch 344/1000
2023-10-11 09:34:28.572 
Epoch 344/1000 
	 loss: 18.0387, MinusLogProbMetric: 18.0387, val_loss: 18.1504, val_MinusLogProbMetric: 18.1504

Epoch 344: val_loss did not improve from 17.95289
196/196 - 124s - loss: 18.0387 - MinusLogProbMetric: 18.0387 - val_loss: 18.1504 - val_MinusLogProbMetric: 18.1504 - lr: 1.1111e-04 - 124s/epoch - 634ms/step
Epoch 345/1000
2023-10-11 09:36:29.759 
Epoch 345/1000 
	 loss: 18.0313, MinusLogProbMetric: 18.0313, val_loss: 18.0109, val_MinusLogProbMetric: 18.0109

Epoch 345: val_loss did not improve from 17.95289
196/196 - 121s - loss: 18.0313 - MinusLogProbMetric: 18.0313 - val_loss: 18.0109 - val_MinusLogProbMetric: 18.0109 - lr: 1.1111e-04 - 121s/epoch - 618ms/step
Epoch 346/1000
2023-10-11 09:38:33.746 
Epoch 346/1000 
	 loss: 18.1647, MinusLogProbMetric: 18.1647, val_loss: 18.0712, val_MinusLogProbMetric: 18.0712

Epoch 346: val_loss did not improve from 17.95289
196/196 - 124s - loss: 18.1647 - MinusLogProbMetric: 18.1647 - val_loss: 18.0712 - val_MinusLogProbMetric: 18.0712 - lr: 1.1111e-04 - 124s/epoch - 633ms/step
Epoch 347/1000
2023-10-11 09:40:32.675 
Epoch 347/1000 
	 loss: 18.0403, MinusLogProbMetric: 18.0403, val_loss: 18.5786, val_MinusLogProbMetric: 18.5786

Epoch 347: val_loss did not improve from 17.95289
196/196 - 119s - loss: 18.0403 - MinusLogProbMetric: 18.0403 - val_loss: 18.5786 - val_MinusLogProbMetric: 18.5786 - lr: 1.1111e-04 - 119s/epoch - 607ms/step
Epoch 348/1000
2023-10-11 09:42:39.307 
Epoch 348/1000 
	 loss: 18.0541, MinusLogProbMetric: 18.0541, val_loss: 18.1173, val_MinusLogProbMetric: 18.1173

Epoch 348: val_loss did not improve from 17.95289
196/196 - 127s - loss: 18.0541 - MinusLogProbMetric: 18.0541 - val_loss: 18.1173 - val_MinusLogProbMetric: 18.1173 - lr: 1.1111e-04 - 127s/epoch - 646ms/step
Epoch 349/1000
2023-10-11 09:44:41.020 
Epoch 349/1000 
	 loss: 18.0391, MinusLogProbMetric: 18.0391, val_loss: 18.0449, val_MinusLogProbMetric: 18.0449

Epoch 349: val_loss did not improve from 17.95289
196/196 - 122s - loss: 18.0391 - MinusLogProbMetric: 18.0391 - val_loss: 18.0449 - val_MinusLogProbMetric: 18.0449 - lr: 1.1111e-04 - 122s/epoch - 621ms/step
Epoch 350/1000
2023-10-11 09:46:46.313 
Epoch 350/1000 
	 loss: 18.0385, MinusLogProbMetric: 18.0385, val_loss: 17.9731, val_MinusLogProbMetric: 17.9731

Epoch 350: val_loss did not improve from 17.95289
196/196 - 125s - loss: 18.0385 - MinusLogProbMetric: 18.0385 - val_loss: 17.9731 - val_MinusLogProbMetric: 17.9731 - lr: 1.1111e-04 - 125s/epoch - 639ms/step
Epoch 351/1000
2023-10-11 09:48:45.026 
Epoch 351/1000 
	 loss: 18.0650, MinusLogProbMetric: 18.0650, val_loss: 17.9535, val_MinusLogProbMetric: 17.9535

Epoch 351: val_loss did not improve from 17.95289
196/196 - 119s - loss: 18.0650 - MinusLogProbMetric: 18.0650 - val_loss: 17.9535 - val_MinusLogProbMetric: 17.9535 - lr: 1.1111e-04 - 119s/epoch - 606ms/step
Epoch 352/1000
2023-10-11 09:50:47.987 
Epoch 352/1000 
	 loss: 18.1643, MinusLogProbMetric: 18.1643, val_loss: 18.0494, val_MinusLogProbMetric: 18.0494

Epoch 352: val_loss did not improve from 17.95289
196/196 - 123s - loss: 18.1643 - MinusLogProbMetric: 18.1643 - val_loss: 18.0494 - val_MinusLogProbMetric: 18.0494 - lr: 1.1111e-04 - 123s/epoch - 627ms/step
Epoch 353/1000
2023-10-11 09:52:46.973 
Epoch 353/1000 
	 loss: 18.0461, MinusLogProbMetric: 18.0461, val_loss: 18.1009, val_MinusLogProbMetric: 18.1009

Epoch 353: val_loss did not improve from 17.95289
196/196 - 119s - loss: 18.0461 - MinusLogProbMetric: 18.0461 - val_loss: 18.1009 - val_MinusLogProbMetric: 18.1009 - lr: 1.1111e-04 - 119s/epoch - 607ms/step
Epoch 354/1000
2023-10-11 09:54:52.512 
Epoch 354/1000 
	 loss: 17.9693, MinusLogProbMetric: 17.9693, val_loss: 18.9660, val_MinusLogProbMetric: 18.9660

Epoch 354: val_loss did not improve from 17.95289
196/196 - 126s - loss: 17.9693 - MinusLogProbMetric: 17.9693 - val_loss: 18.9660 - val_MinusLogProbMetric: 18.9660 - lr: 1.1111e-04 - 126s/epoch - 640ms/step
Epoch 355/1000
2023-10-11 09:56:55.570 
Epoch 355/1000 
	 loss: 18.0385, MinusLogProbMetric: 18.0385, val_loss: 17.8828, val_MinusLogProbMetric: 17.8828

Epoch 355: val_loss improved from 17.95289 to 17.88281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 124s - loss: 18.0385 - MinusLogProbMetric: 18.0385 - val_loss: 17.8828 - val_MinusLogProbMetric: 17.8828 - lr: 1.1111e-04 - 124s/epoch - 632ms/step
Epoch 356/1000
2023-10-11 09:58:55.230 
Epoch 356/1000 
	 loss: 18.0267, MinusLogProbMetric: 18.0267, val_loss: 17.9949, val_MinusLogProbMetric: 17.9949

Epoch 356: val_loss did not improve from 17.88281
196/196 - 119s - loss: 18.0267 - MinusLogProbMetric: 18.0267 - val_loss: 17.9949 - val_MinusLogProbMetric: 17.9949 - lr: 1.1111e-04 - 119s/epoch - 606ms/step
Epoch 357/1000
2023-10-11 10:00:47.614 
Epoch 357/1000 
	 loss: 17.9601, MinusLogProbMetric: 17.9601, val_loss: 18.3423, val_MinusLogProbMetric: 18.3423

Epoch 357: val_loss did not improve from 17.88281
196/196 - 112s - loss: 17.9601 - MinusLogProbMetric: 17.9601 - val_loss: 18.3423 - val_MinusLogProbMetric: 18.3423 - lr: 1.1111e-04 - 112s/epoch - 573ms/step
Epoch 358/1000
2023-10-11 10:02:38.962 
Epoch 358/1000 
	 loss: 18.0364, MinusLogProbMetric: 18.0364, val_loss: 18.2145, val_MinusLogProbMetric: 18.2145

Epoch 358: val_loss did not improve from 17.88281
196/196 - 111s - loss: 18.0364 - MinusLogProbMetric: 18.0364 - val_loss: 18.2145 - val_MinusLogProbMetric: 18.2145 - lr: 1.1111e-04 - 111s/epoch - 568ms/step
Epoch 359/1000
2023-10-11 10:04:27.850 
Epoch 359/1000 
	 loss: 18.0929, MinusLogProbMetric: 18.0929, val_loss: 17.9164, val_MinusLogProbMetric: 17.9164

Epoch 359: val_loss did not improve from 17.88281
196/196 - 109s - loss: 18.0929 - MinusLogProbMetric: 18.0929 - val_loss: 17.9164 - val_MinusLogProbMetric: 17.9164 - lr: 1.1111e-04 - 109s/epoch - 555ms/step
Epoch 360/1000
2023-10-11 10:06:15.037 
Epoch 360/1000 
	 loss: 17.9709, MinusLogProbMetric: 17.9709, val_loss: 18.1430, val_MinusLogProbMetric: 18.1430

Epoch 360: val_loss did not improve from 17.88281
196/196 - 107s - loss: 17.9709 - MinusLogProbMetric: 17.9709 - val_loss: 18.1430 - val_MinusLogProbMetric: 18.1430 - lr: 1.1111e-04 - 107s/epoch - 547ms/step
Epoch 361/1000
2023-10-11 10:08:03.653 
Epoch 361/1000 
	 loss: 17.9755, MinusLogProbMetric: 17.9755, val_loss: 17.8915, val_MinusLogProbMetric: 17.8915

Epoch 361: val_loss did not improve from 17.88281
196/196 - 109s - loss: 17.9755 - MinusLogProbMetric: 17.9755 - val_loss: 17.8915 - val_MinusLogProbMetric: 17.8915 - lr: 1.1111e-04 - 109s/epoch - 554ms/step
Epoch 362/1000
2023-10-11 10:09:54.171 
Epoch 362/1000 
	 loss: 18.1128, MinusLogProbMetric: 18.1128, val_loss: 17.8945, val_MinusLogProbMetric: 17.8945

Epoch 362: val_loss did not improve from 17.88281
196/196 - 111s - loss: 18.1128 - MinusLogProbMetric: 18.1128 - val_loss: 17.8945 - val_MinusLogProbMetric: 17.8945 - lr: 1.1111e-04 - 111s/epoch - 564ms/step
Epoch 363/1000
2023-10-11 10:11:39.224 
Epoch 363/1000 
	 loss: 18.0299, MinusLogProbMetric: 18.0299, val_loss: 18.4140, val_MinusLogProbMetric: 18.4140

Epoch 363: val_loss did not improve from 17.88281
196/196 - 105s - loss: 18.0299 - MinusLogProbMetric: 18.0299 - val_loss: 18.4140 - val_MinusLogProbMetric: 18.4140 - lr: 1.1111e-04 - 105s/epoch - 536ms/step
Epoch 364/1000
2023-10-11 10:13:27.774 
Epoch 364/1000 
	 loss: 18.0085, MinusLogProbMetric: 18.0085, val_loss: 17.9202, val_MinusLogProbMetric: 17.9202

Epoch 364: val_loss did not improve from 17.88281
196/196 - 109s - loss: 18.0085 - MinusLogProbMetric: 18.0085 - val_loss: 17.9202 - val_MinusLogProbMetric: 17.9202 - lr: 1.1111e-04 - 109s/epoch - 554ms/step
Epoch 365/1000
2023-10-11 10:15:19.973 
Epoch 365/1000 
	 loss: 17.9764, MinusLogProbMetric: 17.9764, val_loss: 17.9085, val_MinusLogProbMetric: 17.9085

Epoch 365: val_loss did not improve from 17.88281
196/196 - 112s - loss: 17.9764 - MinusLogProbMetric: 17.9764 - val_loss: 17.9085 - val_MinusLogProbMetric: 17.9085 - lr: 1.1111e-04 - 112s/epoch - 573ms/step
Epoch 366/1000
2023-10-11 10:17:08.156 
Epoch 366/1000 
	 loss: 17.9174, MinusLogProbMetric: 17.9174, val_loss: 17.9288, val_MinusLogProbMetric: 17.9288

Epoch 366: val_loss did not improve from 17.88281
196/196 - 108s - loss: 17.9174 - MinusLogProbMetric: 17.9174 - val_loss: 17.9288 - val_MinusLogProbMetric: 17.9288 - lr: 1.1111e-04 - 108s/epoch - 552ms/step
Epoch 367/1000
2023-10-11 10:19:06.368 
Epoch 367/1000 
	 loss: 17.9216, MinusLogProbMetric: 17.9216, val_loss: 18.0853, val_MinusLogProbMetric: 18.0853

Epoch 367: val_loss did not improve from 17.88281
196/196 - 118s - loss: 17.9216 - MinusLogProbMetric: 17.9216 - val_loss: 18.0853 - val_MinusLogProbMetric: 18.0853 - lr: 1.1111e-04 - 118s/epoch - 603ms/step
Epoch 368/1000
2023-10-11 10:21:13.085 
Epoch 368/1000 
	 loss: 17.9831, MinusLogProbMetric: 17.9831, val_loss: 17.8580, val_MinusLogProbMetric: 17.8580

Epoch 368: val_loss improved from 17.88281 to 17.85796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 129s - loss: 17.9831 - MinusLogProbMetric: 17.9831 - val_loss: 17.8580 - val_MinusLogProbMetric: 17.8580 - lr: 1.1111e-04 - 129s/epoch - 657ms/step
Epoch 369/1000
2023-10-11 10:23:15.928 
Epoch 369/1000 
	 loss: 17.9386, MinusLogProbMetric: 17.9386, val_loss: 18.0831, val_MinusLogProbMetric: 18.0831

Epoch 369: val_loss did not improve from 17.85796
196/196 - 121s - loss: 17.9386 - MinusLogProbMetric: 17.9386 - val_loss: 18.0831 - val_MinusLogProbMetric: 18.0831 - lr: 1.1111e-04 - 121s/epoch - 616ms/step
Epoch 370/1000
2023-10-11 10:25:17.192 
Epoch 370/1000 
	 loss: 17.9393, MinusLogProbMetric: 17.9393, val_loss: 18.4104, val_MinusLogProbMetric: 18.4104

Epoch 370: val_loss did not improve from 17.85796
196/196 - 121s - loss: 17.9393 - MinusLogProbMetric: 17.9393 - val_loss: 18.4104 - val_MinusLogProbMetric: 18.4104 - lr: 1.1111e-04 - 121s/epoch - 619ms/step
Epoch 371/1000
2023-10-11 10:27:09.686 
Epoch 371/1000 
	 loss: 18.0140, MinusLogProbMetric: 18.0140, val_loss: 17.9355, val_MinusLogProbMetric: 17.9355

Epoch 371: val_loss did not improve from 17.85796
196/196 - 112s - loss: 18.0140 - MinusLogProbMetric: 18.0140 - val_loss: 17.9355 - val_MinusLogProbMetric: 17.9355 - lr: 1.1111e-04 - 112s/epoch - 574ms/step
Epoch 372/1000
2023-10-11 10:29:04.839 
Epoch 372/1000 
	 loss: 17.9345, MinusLogProbMetric: 17.9345, val_loss: 17.8555, val_MinusLogProbMetric: 17.8555

Epoch 372: val_loss improved from 17.85796 to 17.85546, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 117s - loss: 17.9345 - MinusLogProbMetric: 17.9345 - val_loss: 17.8555 - val_MinusLogProbMetric: 17.8555 - lr: 1.1111e-04 - 117s/epoch - 595ms/step
Epoch 373/1000
2023-10-11 10:31:03.154 
Epoch 373/1000 
	 loss: 17.9737, MinusLogProbMetric: 17.9737, val_loss: 17.9979, val_MinusLogProbMetric: 17.9979

Epoch 373: val_loss did not improve from 17.85546
196/196 - 117s - loss: 17.9737 - MinusLogProbMetric: 17.9737 - val_loss: 17.9979 - val_MinusLogProbMetric: 17.9979 - lr: 1.1111e-04 - 117s/epoch - 596ms/step
Epoch 374/1000
2023-10-11 10:32:52.910 
Epoch 374/1000 
	 loss: 18.0016, MinusLogProbMetric: 18.0016, val_loss: 18.0893, val_MinusLogProbMetric: 18.0893

Epoch 374: val_loss did not improve from 17.85546
196/196 - 110s - loss: 18.0016 - MinusLogProbMetric: 18.0016 - val_loss: 18.0893 - val_MinusLogProbMetric: 18.0893 - lr: 1.1111e-04 - 110s/epoch - 560ms/step
Epoch 375/1000
2023-10-11 10:34:46.475 
Epoch 375/1000 
	 loss: 17.9142, MinusLogProbMetric: 17.9142, val_loss: 17.8340, val_MinusLogProbMetric: 17.8340

Epoch 375: val_loss improved from 17.85546 to 17.83399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 115s - loss: 17.9142 - MinusLogProbMetric: 17.9142 - val_loss: 17.8340 - val_MinusLogProbMetric: 17.8340 - lr: 1.1111e-04 - 115s/epoch - 589ms/step
Epoch 376/1000
2023-10-11 10:36:45.473 
Epoch 376/1000 
	 loss: 18.0287, MinusLogProbMetric: 18.0287, val_loss: 17.8810, val_MinusLogProbMetric: 17.8810

Epoch 376: val_loss did not improve from 17.83399
196/196 - 117s - loss: 18.0287 - MinusLogProbMetric: 18.0287 - val_loss: 17.8810 - val_MinusLogProbMetric: 17.8810 - lr: 1.1111e-04 - 117s/epoch - 597ms/step
Epoch 377/1000
2023-10-11 10:38:43.972 
Epoch 377/1000 
	 loss: 17.9227, MinusLogProbMetric: 17.9227, val_loss: 18.0877, val_MinusLogProbMetric: 18.0877

Epoch 377: val_loss did not improve from 17.83399
196/196 - 119s - loss: 17.9227 - MinusLogProbMetric: 17.9227 - val_loss: 18.0877 - val_MinusLogProbMetric: 18.0877 - lr: 1.1111e-04 - 119s/epoch - 605ms/step
Epoch 378/1000
2023-10-11 10:40:41.077 
Epoch 378/1000 
	 loss: 17.9718, MinusLogProbMetric: 17.9718, val_loss: 18.2175, val_MinusLogProbMetric: 18.2175

Epoch 378: val_loss did not improve from 17.83399
196/196 - 117s - loss: 17.9718 - MinusLogProbMetric: 17.9718 - val_loss: 18.2175 - val_MinusLogProbMetric: 18.2175 - lr: 1.1111e-04 - 117s/epoch - 597ms/step
Epoch 379/1000
2023-10-11 10:42:32.766 
Epoch 379/1000 
	 loss: 17.9283, MinusLogProbMetric: 17.9283, val_loss: 17.9675, val_MinusLogProbMetric: 17.9675

Epoch 379: val_loss did not improve from 17.83399
196/196 - 112s - loss: 17.9283 - MinusLogProbMetric: 17.9283 - val_loss: 17.9675 - val_MinusLogProbMetric: 17.9675 - lr: 1.1111e-04 - 112s/epoch - 570ms/step
Epoch 380/1000
2023-10-11 10:44:25.518 
Epoch 380/1000 
	 loss: 17.9794, MinusLogProbMetric: 17.9794, val_loss: 18.3881, val_MinusLogProbMetric: 18.3881

Epoch 380: val_loss did not improve from 17.83399
196/196 - 113s - loss: 17.9794 - MinusLogProbMetric: 17.9794 - val_loss: 18.3881 - val_MinusLogProbMetric: 18.3881 - lr: 1.1111e-04 - 113s/epoch - 575ms/step
Epoch 381/1000
2023-10-11 10:46:17.152 
Epoch 381/1000 
	 loss: 17.9606, MinusLogProbMetric: 17.9606, val_loss: 17.8813, val_MinusLogProbMetric: 17.8813

Epoch 381: val_loss did not improve from 17.83399
196/196 - 112s - loss: 17.9606 - MinusLogProbMetric: 17.9606 - val_loss: 17.8813 - val_MinusLogProbMetric: 17.8813 - lr: 1.1111e-04 - 112s/epoch - 570ms/step
Epoch 382/1000
2023-10-11 10:48:06.036 
Epoch 382/1000 
	 loss: 18.0025, MinusLogProbMetric: 18.0025, val_loss: 18.0059, val_MinusLogProbMetric: 18.0059

Epoch 382: val_loss did not improve from 17.83399
196/196 - 109s - loss: 18.0025 - MinusLogProbMetric: 18.0025 - val_loss: 18.0059 - val_MinusLogProbMetric: 18.0059 - lr: 1.1111e-04 - 109s/epoch - 555ms/step
Epoch 383/1000
2023-10-11 10:49:56.545 
Epoch 383/1000 
	 loss: 18.0169, MinusLogProbMetric: 18.0169, val_loss: 18.0914, val_MinusLogProbMetric: 18.0914

Epoch 383: val_loss did not improve from 17.83399
196/196 - 111s - loss: 18.0169 - MinusLogProbMetric: 18.0169 - val_loss: 18.0914 - val_MinusLogProbMetric: 18.0914 - lr: 1.1111e-04 - 111s/epoch - 564ms/step
Epoch 384/1000
2023-10-11 10:51:45.428 
Epoch 384/1000 
	 loss: 17.8788, MinusLogProbMetric: 17.8788, val_loss: 17.9443, val_MinusLogProbMetric: 17.9443

Epoch 384: val_loss did not improve from 17.83399
196/196 - 109s - loss: 17.8788 - MinusLogProbMetric: 17.8788 - val_loss: 17.9443 - val_MinusLogProbMetric: 17.9443 - lr: 1.1111e-04 - 109s/epoch - 555ms/step
Epoch 385/1000
2023-10-11 10:53:40.528 
Epoch 385/1000 
	 loss: 17.8732, MinusLogProbMetric: 17.8732, val_loss: 18.0235, val_MinusLogProbMetric: 18.0235

Epoch 385: val_loss did not improve from 17.83399
196/196 - 115s - loss: 17.8732 - MinusLogProbMetric: 17.8732 - val_loss: 18.0235 - val_MinusLogProbMetric: 18.0235 - lr: 1.1111e-04 - 115s/epoch - 587ms/step
Epoch 386/1000
2023-10-11 10:55:32.853 
Epoch 386/1000 
	 loss: 18.0336, MinusLogProbMetric: 18.0336, val_loss: 17.8594, val_MinusLogProbMetric: 17.8594

Epoch 386: val_loss did not improve from 17.83399
196/196 - 112s - loss: 18.0336 - MinusLogProbMetric: 18.0336 - val_loss: 17.8594 - val_MinusLogProbMetric: 17.8594 - lr: 1.1111e-04 - 112s/epoch - 573ms/step
Epoch 387/1000
2023-10-11 10:56:34.521 
Epoch 387/1000 
	 loss: 17.8819, MinusLogProbMetric: 17.8819, val_loss: 18.1880, val_MinusLogProbMetric: 18.1880

Epoch 387: val_loss did not improve from 17.83399
196/196 - 62s - loss: 17.8819 - MinusLogProbMetric: 17.8819 - val_loss: 18.1880 - val_MinusLogProbMetric: 18.1880 - lr: 1.1111e-04 - 62s/epoch - 314ms/step
Epoch 388/1000
2023-10-11 10:57:29.189 
Epoch 388/1000 
	 loss: 17.9434, MinusLogProbMetric: 17.9434, val_loss: 17.9445, val_MinusLogProbMetric: 17.9445

Epoch 388: val_loss did not improve from 17.83399
196/196 - 55s - loss: 17.9434 - MinusLogProbMetric: 17.9434 - val_loss: 17.9445 - val_MinusLogProbMetric: 17.9445 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 389/1000
2023-10-11 10:58:23.124 
Epoch 389/1000 
	 loss: 17.8728, MinusLogProbMetric: 17.8728, val_loss: 17.8892, val_MinusLogProbMetric: 17.8892

Epoch 389: val_loss did not improve from 17.83399
196/196 - 54s - loss: 17.8728 - MinusLogProbMetric: 17.8728 - val_loss: 17.8892 - val_MinusLogProbMetric: 17.8892 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 390/1000
2023-10-11 10:59:15.219 
Epoch 390/1000 
	 loss: 17.9685, MinusLogProbMetric: 17.9685, val_loss: 17.9379, val_MinusLogProbMetric: 17.9379

Epoch 390: val_loss did not improve from 17.83399
196/196 - 52s - loss: 17.9685 - MinusLogProbMetric: 17.9685 - val_loss: 17.9379 - val_MinusLogProbMetric: 17.9379 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 391/1000
2023-10-11 11:00:08.567 
Epoch 391/1000 
	 loss: 17.9122, MinusLogProbMetric: 17.9122, val_loss: 17.7613, val_MinusLogProbMetric: 17.7613

Epoch 391: val_loss improved from 17.83399 to 17.76126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 55s - loss: 17.9122 - MinusLogProbMetric: 17.9122 - val_loss: 17.7613 - val_MinusLogProbMetric: 17.7613 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 392/1000
2023-10-11 11:01:03.144 
Epoch 392/1000 
	 loss: 17.9412, MinusLogProbMetric: 17.9412, val_loss: 17.8687, val_MinusLogProbMetric: 17.8687

Epoch 392: val_loss did not improve from 17.76126
196/196 - 53s - loss: 17.9412 - MinusLogProbMetric: 17.9412 - val_loss: 17.8687 - val_MinusLogProbMetric: 17.8687 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 393/1000
2023-10-11 11:01:57.936 
Epoch 393/1000 
	 loss: 17.8764, MinusLogProbMetric: 17.8764, val_loss: 17.8308, val_MinusLogProbMetric: 17.8308

Epoch 393: val_loss did not improve from 17.76126
196/196 - 55s - loss: 17.8764 - MinusLogProbMetric: 17.8764 - val_loss: 17.8308 - val_MinusLogProbMetric: 17.8308 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 394/1000
2023-10-11 11:02:50.299 
Epoch 394/1000 
	 loss: 17.8596, MinusLogProbMetric: 17.8596, val_loss: 17.9296, val_MinusLogProbMetric: 17.9296

Epoch 394: val_loss did not improve from 17.76126
196/196 - 52s - loss: 17.8596 - MinusLogProbMetric: 17.8596 - val_loss: 17.9296 - val_MinusLogProbMetric: 17.9296 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 395/1000
2023-10-11 11:03:43.315 
Epoch 395/1000 
	 loss: 17.9204, MinusLogProbMetric: 17.9204, val_loss: 17.7571, val_MinusLogProbMetric: 17.7571

Epoch 395: val_loss improved from 17.76126 to 17.75708, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 54s - loss: 17.9204 - MinusLogProbMetric: 17.9204 - val_loss: 17.7571 - val_MinusLogProbMetric: 17.7571 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 396/1000
2023-10-11 11:04:38.122 
Epoch 396/1000 
	 loss: 17.8939, MinusLogProbMetric: 17.8939, val_loss: 17.9830, val_MinusLogProbMetric: 17.9830

Epoch 396: val_loss did not improve from 17.75708
196/196 - 54s - loss: 17.8939 - MinusLogProbMetric: 17.8939 - val_loss: 17.9830 - val_MinusLogProbMetric: 17.9830 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 397/1000
2023-10-11 11:05:32.807 
Epoch 397/1000 
	 loss: 17.9588, MinusLogProbMetric: 17.9588, val_loss: 17.8844, val_MinusLogProbMetric: 17.8844

Epoch 397: val_loss did not improve from 17.75708
196/196 - 55s - loss: 17.9588 - MinusLogProbMetric: 17.9588 - val_loss: 17.8844 - val_MinusLogProbMetric: 17.8844 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 398/1000
2023-10-11 11:06:27.345 
Epoch 398/1000 
	 loss: 17.8870, MinusLogProbMetric: 17.8870, val_loss: 17.8753, val_MinusLogProbMetric: 17.8753

Epoch 398: val_loss did not improve from 17.75708
196/196 - 55s - loss: 17.8870 - MinusLogProbMetric: 17.8870 - val_loss: 17.8753 - val_MinusLogProbMetric: 17.8753 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 399/1000
2023-10-11 11:07:20.707 
Epoch 399/1000 
	 loss: 17.9169, MinusLogProbMetric: 17.9169, val_loss: 17.9493, val_MinusLogProbMetric: 17.9493

Epoch 399: val_loss did not improve from 17.75708
196/196 - 53s - loss: 17.9169 - MinusLogProbMetric: 17.9169 - val_loss: 17.9493 - val_MinusLogProbMetric: 17.9493 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 400/1000
2023-10-11 11:08:12.130 
Epoch 400/1000 
	 loss: 17.8430, MinusLogProbMetric: 17.8430, val_loss: 17.7822, val_MinusLogProbMetric: 17.7822

Epoch 400: val_loss did not improve from 17.75708
196/196 - 51s - loss: 17.8430 - MinusLogProbMetric: 17.8430 - val_loss: 17.7822 - val_MinusLogProbMetric: 17.7822 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 401/1000
2023-10-11 11:09:06.030 
Epoch 401/1000 
	 loss: 17.9960, MinusLogProbMetric: 17.9960, val_loss: 18.0028, val_MinusLogProbMetric: 18.0028

Epoch 401: val_loss did not improve from 17.75708
196/196 - 54s - loss: 17.9960 - MinusLogProbMetric: 17.9960 - val_loss: 18.0028 - val_MinusLogProbMetric: 18.0028 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 402/1000
2023-10-11 11:09:58.963 
Epoch 402/1000 
	 loss: 17.9269, MinusLogProbMetric: 17.9269, val_loss: 17.7156, val_MinusLogProbMetric: 17.7156

Epoch 402: val_loss improved from 17.75708 to 17.71560, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 54s - loss: 17.9269 - MinusLogProbMetric: 17.9269 - val_loss: 17.7156 - val_MinusLogProbMetric: 17.7156 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 403/1000
2023-10-11 11:10:53.276 
Epoch 403/1000 
	 loss: 17.8949, MinusLogProbMetric: 17.8949, val_loss: 17.8628, val_MinusLogProbMetric: 17.8628

Epoch 403: val_loss did not improve from 17.71560
196/196 - 53s - loss: 17.8949 - MinusLogProbMetric: 17.8949 - val_loss: 17.8628 - val_MinusLogProbMetric: 17.8628 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 404/1000
2023-10-11 11:11:45.281 
Epoch 404/1000 
	 loss: 17.9436, MinusLogProbMetric: 17.9436, val_loss: 17.7566, val_MinusLogProbMetric: 17.7566

Epoch 404: val_loss did not improve from 17.71560
196/196 - 52s - loss: 17.9436 - MinusLogProbMetric: 17.9436 - val_loss: 17.7566 - val_MinusLogProbMetric: 17.7566 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 405/1000
2023-10-11 11:12:37.725 
Epoch 405/1000 
	 loss: 17.8383, MinusLogProbMetric: 17.8383, val_loss: 18.5212, val_MinusLogProbMetric: 18.5212

Epoch 405: val_loss did not improve from 17.71560
196/196 - 52s - loss: 17.8383 - MinusLogProbMetric: 17.8383 - val_loss: 18.5212 - val_MinusLogProbMetric: 18.5212 - lr: 1.1111e-04 - 52s/epoch - 268ms/step
Epoch 406/1000
2023-10-11 11:13:30.845 
Epoch 406/1000 
	 loss: 17.8822, MinusLogProbMetric: 17.8822, val_loss: 17.7461, val_MinusLogProbMetric: 17.7461

Epoch 406: val_loss did not improve from 17.71560
196/196 - 53s - loss: 17.8822 - MinusLogProbMetric: 17.8822 - val_loss: 17.7461 - val_MinusLogProbMetric: 17.7461 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 407/1000
2023-10-11 11:14:23.378 
Epoch 407/1000 
	 loss: 17.8961, MinusLogProbMetric: 17.8961, val_loss: 17.7648, val_MinusLogProbMetric: 17.7648

Epoch 407: val_loss did not improve from 17.71560
196/196 - 53s - loss: 17.8961 - MinusLogProbMetric: 17.8961 - val_loss: 17.7648 - val_MinusLogProbMetric: 17.7648 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 408/1000
2023-10-11 11:15:17.311 
Epoch 408/1000 
	 loss: 17.8670, MinusLogProbMetric: 17.8670, val_loss: 18.0637, val_MinusLogProbMetric: 18.0637

Epoch 408: val_loss did not improve from 17.71560
196/196 - 54s - loss: 17.8670 - MinusLogProbMetric: 17.8670 - val_loss: 18.0637 - val_MinusLogProbMetric: 18.0637 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 409/1000
2023-10-11 11:16:10.881 
Epoch 409/1000 
	 loss: 17.8919, MinusLogProbMetric: 17.8919, val_loss: 17.7681, val_MinusLogProbMetric: 17.7681

Epoch 409: val_loss did not improve from 17.71560
196/196 - 54s - loss: 17.8919 - MinusLogProbMetric: 17.8919 - val_loss: 17.7681 - val_MinusLogProbMetric: 17.7681 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 410/1000
2023-10-11 11:17:04.692 
Epoch 410/1000 
	 loss: 17.8713, MinusLogProbMetric: 17.8713, val_loss: 17.9278, val_MinusLogProbMetric: 17.9278

Epoch 410: val_loss did not improve from 17.71560
196/196 - 54s - loss: 17.8713 - MinusLogProbMetric: 17.8713 - val_loss: 17.9278 - val_MinusLogProbMetric: 17.9278 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 411/1000
2023-10-11 11:17:58.679 
Epoch 411/1000 
	 loss: 17.8591, MinusLogProbMetric: 17.8591, val_loss: 17.9676, val_MinusLogProbMetric: 17.9676

Epoch 411: val_loss did not improve from 17.71560
196/196 - 54s - loss: 17.8591 - MinusLogProbMetric: 17.8591 - val_loss: 17.9676 - val_MinusLogProbMetric: 17.9676 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 412/1000
2023-10-11 11:18:50.688 
Epoch 412/1000 
	 loss: 17.8829, MinusLogProbMetric: 17.8829, val_loss: 18.0463, val_MinusLogProbMetric: 18.0463

Epoch 412: val_loss did not improve from 17.71560
196/196 - 52s - loss: 17.8829 - MinusLogProbMetric: 17.8829 - val_loss: 18.0463 - val_MinusLogProbMetric: 18.0463 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 413/1000
2023-10-11 11:19:44.355 
Epoch 413/1000 
	 loss: 17.8552, MinusLogProbMetric: 17.8552, val_loss: 17.8503, val_MinusLogProbMetric: 17.8503

Epoch 413: val_loss did not improve from 17.71560
196/196 - 54s - loss: 17.8552 - MinusLogProbMetric: 17.8552 - val_loss: 17.8503 - val_MinusLogProbMetric: 17.8503 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 414/1000
2023-10-11 11:20:37.562 
Epoch 414/1000 
	 loss: 17.9647, MinusLogProbMetric: 17.9647, val_loss: 17.8511, val_MinusLogProbMetric: 17.8511

Epoch 414: val_loss did not improve from 17.71560
196/196 - 53s - loss: 17.9647 - MinusLogProbMetric: 17.9647 - val_loss: 17.8511 - val_MinusLogProbMetric: 17.8511 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 415/1000
2023-10-11 11:21:30.753 
Epoch 415/1000 
	 loss: 17.8746, MinusLogProbMetric: 17.8746, val_loss: 17.8877, val_MinusLogProbMetric: 17.8877

Epoch 415: val_loss did not improve from 17.71560
196/196 - 53s - loss: 17.8746 - MinusLogProbMetric: 17.8746 - val_loss: 17.8877 - val_MinusLogProbMetric: 17.8877 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 416/1000
2023-10-11 11:22:23.129 
Epoch 416/1000 
	 loss: 17.8803, MinusLogProbMetric: 17.8803, val_loss: 18.2345, val_MinusLogProbMetric: 18.2345

Epoch 416: val_loss did not improve from 17.71560
196/196 - 52s - loss: 17.8803 - MinusLogProbMetric: 17.8803 - val_loss: 18.2345 - val_MinusLogProbMetric: 18.2345 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 417/1000
2023-10-11 11:23:16.069 
Epoch 417/1000 
	 loss: 17.8692, MinusLogProbMetric: 17.8692, val_loss: 18.1544, val_MinusLogProbMetric: 18.1544

Epoch 417: val_loss did not improve from 17.71560
196/196 - 53s - loss: 17.8692 - MinusLogProbMetric: 17.8692 - val_loss: 18.1544 - val_MinusLogProbMetric: 18.1544 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 418/1000
2023-10-11 11:24:09.339 
Epoch 418/1000 
	 loss: 17.8270, MinusLogProbMetric: 17.8270, val_loss: 18.6036, val_MinusLogProbMetric: 18.6036

Epoch 418: val_loss did not improve from 17.71560
196/196 - 53s - loss: 17.8270 - MinusLogProbMetric: 17.8270 - val_loss: 18.6036 - val_MinusLogProbMetric: 18.6036 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 419/1000
2023-10-11 11:25:02.419 
Epoch 419/1000 
	 loss: 17.8594, MinusLogProbMetric: 17.8594, val_loss: 17.9232, val_MinusLogProbMetric: 17.9232

Epoch 419: val_loss did not improve from 17.71560
196/196 - 53s - loss: 17.8594 - MinusLogProbMetric: 17.8594 - val_loss: 17.9232 - val_MinusLogProbMetric: 17.9232 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 420/1000
2023-10-11 11:25:54.137 
Epoch 420/1000 
	 loss: 17.7962, MinusLogProbMetric: 17.7962, val_loss: 18.0637, val_MinusLogProbMetric: 18.0637

Epoch 420: val_loss did not improve from 17.71560
196/196 - 52s - loss: 17.7962 - MinusLogProbMetric: 17.7962 - val_loss: 18.0637 - val_MinusLogProbMetric: 18.0637 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 421/1000
2023-10-11 11:26:47.551 
Epoch 421/1000 
	 loss: 17.8710, MinusLogProbMetric: 17.8710, val_loss: 18.2310, val_MinusLogProbMetric: 18.2310

Epoch 421: val_loss did not improve from 17.71560
196/196 - 53s - loss: 17.8710 - MinusLogProbMetric: 17.8710 - val_loss: 18.2310 - val_MinusLogProbMetric: 18.2310 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 422/1000
2023-10-11 11:27:40.879 
Epoch 422/1000 
	 loss: 17.8360, MinusLogProbMetric: 17.8360, val_loss: 17.7915, val_MinusLogProbMetric: 17.7915

Epoch 422: val_loss did not improve from 17.71560
196/196 - 53s - loss: 17.8360 - MinusLogProbMetric: 17.8360 - val_loss: 17.7915 - val_MinusLogProbMetric: 17.7915 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 423/1000
2023-10-11 11:28:35.111 
Epoch 423/1000 
	 loss: 17.8077, MinusLogProbMetric: 17.8077, val_loss: 17.7529, val_MinusLogProbMetric: 17.7529

Epoch 423: val_loss did not improve from 17.71560
196/196 - 54s - loss: 17.8077 - MinusLogProbMetric: 17.8077 - val_loss: 17.7529 - val_MinusLogProbMetric: 17.7529 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 424/1000
2023-10-11 11:29:28.410 
Epoch 424/1000 
	 loss: 17.9066, MinusLogProbMetric: 17.9066, val_loss: 17.8695, val_MinusLogProbMetric: 17.8695

Epoch 424: val_loss did not improve from 17.71560
196/196 - 53s - loss: 17.9066 - MinusLogProbMetric: 17.9066 - val_loss: 17.8695 - val_MinusLogProbMetric: 17.8695 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 425/1000
2023-10-11 11:30:23.073 
Epoch 425/1000 
	 loss: 17.7876, MinusLogProbMetric: 17.7876, val_loss: 17.7111, val_MinusLogProbMetric: 17.7111

Epoch 425: val_loss improved from 17.71560 to 17.71109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 56s - loss: 17.7876 - MinusLogProbMetric: 17.7876 - val_loss: 17.7111 - val_MinusLogProbMetric: 17.7111 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 426/1000
2023-10-11 11:31:17.934 
Epoch 426/1000 
	 loss: 17.8434, MinusLogProbMetric: 17.8434, val_loss: 17.8205, val_MinusLogProbMetric: 17.8205

Epoch 426: val_loss did not improve from 17.71109
196/196 - 54s - loss: 17.8434 - MinusLogProbMetric: 17.8434 - val_loss: 17.8205 - val_MinusLogProbMetric: 17.8205 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 427/1000
2023-10-11 11:32:12.891 
Epoch 427/1000 
	 loss: 17.7532, MinusLogProbMetric: 17.7532, val_loss: 17.7658, val_MinusLogProbMetric: 17.7658

Epoch 427: val_loss did not improve from 17.71109
196/196 - 55s - loss: 17.7532 - MinusLogProbMetric: 17.7532 - val_loss: 17.7658 - val_MinusLogProbMetric: 17.7658 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 428/1000
2023-10-11 11:33:07.964 
Epoch 428/1000 
	 loss: 17.8475, MinusLogProbMetric: 17.8475, val_loss: 18.1087, val_MinusLogProbMetric: 18.1087

Epoch 428: val_loss did not improve from 17.71109
196/196 - 55s - loss: 17.8475 - MinusLogProbMetric: 17.8475 - val_loss: 18.1087 - val_MinusLogProbMetric: 18.1087 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 429/1000
2023-10-11 11:34:00.472 
Epoch 429/1000 
	 loss: 17.8026, MinusLogProbMetric: 17.8026, val_loss: 18.3092, val_MinusLogProbMetric: 18.3092

Epoch 429: val_loss did not improve from 17.71109
196/196 - 53s - loss: 17.8026 - MinusLogProbMetric: 17.8026 - val_loss: 18.3092 - val_MinusLogProbMetric: 18.3092 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 430/1000
2023-10-11 11:34:53.240 
Epoch 430/1000 
	 loss: 17.8866, MinusLogProbMetric: 17.8866, val_loss: 17.9688, val_MinusLogProbMetric: 17.9688

Epoch 430: val_loss did not improve from 17.71109
196/196 - 53s - loss: 17.8866 - MinusLogProbMetric: 17.8866 - val_loss: 17.9688 - val_MinusLogProbMetric: 17.9688 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 431/1000
2023-10-11 11:35:45.605 
Epoch 431/1000 
	 loss: 17.8042, MinusLogProbMetric: 17.8042, val_loss: 17.7795, val_MinusLogProbMetric: 17.7795

Epoch 431: val_loss did not improve from 17.71109
196/196 - 52s - loss: 17.8042 - MinusLogProbMetric: 17.8042 - val_loss: 17.7795 - val_MinusLogProbMetric: 17.7795 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 432/1000
2023-10-11 11:36:38.652 
Epoch 432/1000 
	 loss: 17.8125, MinusLogProbMetric: 17.8125, val_loss: 17.7461, val_MinusLogProbMetric: 17.7461

Epoch 432: val_loss did not improve from 17.71109
196/196 - 53s - loss: 17.8125 - MinusLogProbMetric: 17.8125 - val_loss: 17.7461 - val_MinusLogProbMetric: 17.7461 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 433/1000
2023-10-11 11:37:31.236 
Epoch 433/1000 
	 loss: 17.8313, MinusLogProbMetric: 17.8313, val_loss: 17.7640, val_MinusLogProbMetric: 17.7640

Epoch 433: val_loss did not improve from 17.71109
196/196 - 53s - loss: 17.8313 - MinusLogProbMetric: 17.8313 - val_loss: 17.7640 - val_MinusLogProbMetric: 17.7640 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 434/1000
2023-10-11 11:38:25.836 
Epoch 434/1000 
	 loss: 17.8108, MinusLogProbMetric: 17.8108, val_loss: 17.8300, val_MinusLogProbMetric: 17.8300

Epoch 434: val_loss did not improve from 17.71109
196/196 - 55s - loss: 17.8108 - MinusLogProbMetric: 17.8108 - val_loss: 17.8300 - val_MinusLogProbMetric: 17.8300 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 435/1000
2023-10-11 11:39:19.465 
Epoch 435/1000 
	 loss: 17.8423, MinusLogProbMetric: 17.8423, val_loss: 17.8909, val_MinusLogProbMetric: 17.8909

Epoch 435: val_loss did not improve from 17.71109
196/196 - 54s - loss: 17.8423 - MinusLogProbMetric: 17.8423 - val_loss: 17.8909 - val_MinusLogProbMetric: 17.8909 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 436/1000
2023-10-11 11:40:13.020 
Epoch 436/1000 
	 loss: 17.8443, MinusLogProbMetric: 17.8443, val_loss: 17.9115, val_MinusLogProbMetric: 17.9115

Epoch 436: val_loss did not improve from 17.71109
196/196 - 54s - loss: 17.8443 - MinusLogProbMetric: 17.8443 - val_loss: 17.9115 - val_MinusLogProbMetric: 17.9115 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 437/1000
2023-10-11 11:41:06.972 
Epoch 437/1000 
	 loss: 17.7682, MinusLogProbMetric: 17.7682, val_loss: 19.5489, val_MinusLogProbMetric: 19.5489

Epoch 437: val_loss did not improve from 17.71109
196/196 - 54s - loss: 17.7682 - MinusLogProbMetric: 17.7682 - val_loss: 19.5489 - val_MinusLogProbMetric: 19.5489 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 438/1000
2023-10-11 11:41:59.615 
Epoch 438/1000 
	 loss: 17.8130, MinusLogProbMetric: 17.8130, val_loss: 17.9206, val_MinusLogProbMetric: 17.9206

Epoch 438: val_loss did not improve from 17.71109
196/196 - 53s - loss: 17.8130 - MinusLogProbMetric: 17.8130 - val_loss: 17.9206 - val_MinusLogProbMetric: 17.9206 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 439/1000
2023-10-11 11:42:53.511 
Epoch 439/1000 
	 loss: 17.8181, MinusLogProbMetric: 17.8181, val_loss: 17.9767, val_MinusLogProbMetric: 17.9767

Epoch 439: val_loss did not improve from 17.71109
196/196 - 54s - loss: 17.8181 - MinusLogProbMetric: 17.8181 - val_loss: 17.9767 - val_MinusLogProbMetric: 17.9767 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 440/1000
2023-10-11 11:43:47.754 
Epoch 440/1000 
	 loss: 17.7582, MinusLogProbMetric: 17.7582, val_loss: 17.8013, val_MinusLogProbMetric: 17.8013

Epoch 440: val_loss did not improve from 17.71109
196/196 - 54s - loss: 17.7582 - MinusLogProbMetric: 17.7582 - val_loss: 17.8013 - val_MinusLogProbMetric: 17.8013 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 441/1000
2023-10-11 11:44:40.618 
Epoch 441/1000 
	 loss: 17.8888, MinusLogProbMetric: 17.8888, val_loss: 17.9416, val_MinusLogProbMetric: 17.9416

Epoch 441: val_loss did not improve from 17.71109
196/196 - 53s - loss: 17.8888 - MinusLogProbMetric: 17.8888 - val_loss: 17.9416 - val_MinusLogProbMetric: 17.9416 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 442/1000
2023-10-11 11:45:34.230 
Epoch 442/1000 
	 loss: 17.7869, MinusLogProbMetric: 17.7869, val_loss: 17.7466, val_MinusLogProbMetric: 17.7466

Epoch 442: val_loss did not improve from 17.71109
196/196 - 54s - loss: 17.7869 - MinusLogProbMetric: 17.7869 - val_loss: 17.7466 - val_MinusLogProbMetric: 17.7466 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 443/1000
2023-10-11 11:46:29.241 
Epoch 443/1000 
	 loss: 17.7660, MinusLogProbMetric: 17.7660, val_loss: 17.8976, val_MinusLogProbMetric: 17.8976

Epoch 443: val_loss did not improve from 17.71109
196/196 - 55s - loss: 17.7660 - MinusLogProbMetric: 17.7660 - val_loss: 17.8976 - val_MinusLogProbMetric: 17.8976 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 444/1000
2023-10-11 11:47:23.578 
Epoch 444/1000 
	 loss: 17.7764, MinusLogProbMetric: 17.7764, val_loss: 17.8617, val_MinusLogProbMetric: 17.8617

Epoch 444: val_loss did not improve from 17.71109
196/196 - 54s - loss: 17.7764 - MinusLogProbMetric: 17.7764 - val_loss: 17.8617 - val_MinusLogProbMetric: 17.8617 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 445/1000
2023-10-11 11:48:16.311 
Epoch 445/1000 
	 loss: 17.8229, MinusLogProbMetric: 17.8229, val_loss: 17.8335, val_MinusLogProbMetric: 17.8335

Epoch 445: val_loss did not improve from 17.71109
196/196 - 53s - loss: 17.8229 - MinusLogProbMetric: 17.8229 - val_loss: 17.8335 - val_MinusLogProbMetric: 17.8335 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 446/1000
2023-10-11 11:49:10.569 
Epoch 446/1000 
	 loss: 17.8927, MinusLogProbMetric: 17.8927, val_loss: 17.7102, val_MinusLogProbMetric: 17.7102

Epoch 446: val_loss improved from 17.71109 to 17.71025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 55s - loss: 17.8927 - MinusLogProbMetric: 17.8927 - val_loss: 17.7102 - val_MinusLogProbMetric: 17.7102 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 447/1000
2023-10-11 11:50:05.988 
Epoch 447/1000 
	 loss: 17.7591, MinusLogProbMetric: 17.7591, val_loss: 17.7617, val_MinusLogProbMetric: 17.7617

Epoch 447: val_loss did not improve from 17.71025
196/196 - 55s - loss: 17.7591 - MinusLogProbMetric: 17.7591 - val_loss: 17.7617 - val_MinusLogProbMetric: 17.7617 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 448/1000
2023-10-11 11:50:58.953 
Epoch 448/1000 
	 loss: 17.8691, MinusLogProbMetric: 17.8691, val_loss: 17.7636, val_MinusLogProbMetric: 17.7636

Epoch 448: val_loss did not improve from 17.71025
196/196 - 53s - loss: 17.8691 - MinusLogProbMetric: 17.8691 - val_loss: 17.7636 - val_MinusLogProbMetric: 17.7636 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 449/1000
2023-10-11 11:51:52.827 
Epoch 449/1000 
	 loss: 17.8053, MinusLogProbMetric: 17.8053, val_loss: 17.8413, val_MinusLogProbMetric: 17.8413

Epoch 449: val_loss did not improve from 17.71025
196/196 - 54s - loss: 17.8053 - MinusLogProbMetric: 17.8053 - val_loss: 17.8413 - val_MinusLogProbMetric: 17.8413 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 450/1000
2023-10-11 11:52:46.216 
Epoch 450/1000 
	 loss: 17.8237, MinusLogProbMetric: 17.8237, val_loss: 17.8508, val_MinusLogProbMetric: 17.8508

Epoch 450: val_loss did not improve from 17.71025
196/196 - 53s - loss: 17.8237 - MinusLogProbMetric: 17.8237 - val_loss: 17.8508 - val_MinusLogProbMetric: 17.8508 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 451/1000
2023-10-11 11:53:39.392 
Epoch 451/1000 
	 loss: 17.8025, MinusLogProbMetric: 17.8025, val_loss: 17.6720, val_MinusLogProbMetric: 17.6720

Epoch 451: val_loss improved from 17.71025 to 17.67199, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 54s - loss: 17.8025 - MinusLogProbMetric: 17.8025 - val_loss: 17.6720 - val_MinusLogProbMetric: 17.6720 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 452/1000
2023-10-11 11:54:34.773 
Epoch 452/1000 
	 loss: 17.7931, MinusLogProbMetric: 17.7931, val_loss: 17.7487, val_MinusLogProbMetric: 17.7487

Epoch 452: val_loss did not improve from 17.67199
196/196 - 54s - loss: 17.7931 - MinusLogProbMetric: 17.7931 - val_loss: 17.7487 - val_MinusLogProbMetric: 17.7487 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 453/1000
2023-10-11 11:55:27.118 
Epoch 453/1000 
	 loss: 17.7773, MinusLogProbMetric: 17.7773, val_loss: 18.6455, val_MinusLogProbMetric: 18.6455

Epoch 453: val_loss did not improve from 17.67199
196/196 - 52s - loss: 17.7773 - MinusLogProbMetric: 17.7773 - val_loss: 18.6455 - val_MinusLogProbMetric: 18.6455 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 454/1000
2023-10-11 11:56:19.870 
Epoch 454/1000 
	 loss: 17.7932, MinusLogProbMetric: 17.7932, val_loss: 17.7683, val_MinusLogProbMetric: 17.7683

Epoch 454: val_loss did not improve from 17.67199
196/196 - 53s - loss: 17.7932 - MinusLogProbMetric: 17.7932 - val_loss: 17.7683 - val_MinusLogProbMetric: 17.7683 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 455/1000
2023-10-11 11:57:12.747 
Epoch 455/1000 
	 loss: 17.8413, MinusLogProbMetric: 17.8413, val_loss: 17.9632, val_MinusLogProbMetric: 17.9632

Epoch 455: val_loss did not improve from 17.67199
196/196 - 53s - loss: 17.8413 - MinusLogProbMetric: 17.8413 - val_loss: 17.9632 - val_MinusLogProbMetric: 17.9632 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 456/1000
2023-10-11 11:58:06.869 
Epoch 456/1000 
	 loss: 17.7564, MinusLogProbMetric: 17.7564, val_loss: 17.7719, val_MinusLogProbMetric: 17.7719

Epoch 456: val_loss did not improve from 17.67199
196/196 - 54s - loss: 17.7564 - MinusLogProbMetric: 17.7564 - val_loss: 17.7719 - val_MinusLogProbMetric: 17.7719 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 457/1000
2023-10-11 11:58:59.884 
Epoch 457/1000 
	 loss: 17.7721, MinusLogProbMetric: 17.7721, val_loss: 18.5096, val_MinusLogProbMetric: 18.5096

Epoch 457: val_loss did not improve from 17.67199
196/196 - 53s - loss: 17.7721 - MinusLogProbMetric: 17.7721 - val_loss: 18.5096 - val_MinusLogProbMetric: 18.5096 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 458/1000
2023-10-11 11:59:54.253 
Epoch 458/1000 
	 loss: 17.7475, MinusLogProbMetric: 17.7475, val_loss: 17.8596, val_MinusLogProbMetric: 17.8596

Epoch 458: val_loss did not improve from 17.67199
196/196 - 54s - loss: 17.7475 - MinusLogProbMetric: 17.7475 - val_loss: 17.8596 - val_MinusLogProbMetric: 17.8596 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 459/1000
2023-10-11 12:00:48.943 
Epoch 459/1000 
	 loss: 17.8167, MinusLogProbMetric: 17.8167, val_loss: 19.4718, val_MinusLogProbMetric: 19.4718

Epoch 459: val_loss did not improve from 17.67199
196/196 - 55s - loss: 17.8167 - MinusLogProbMetric: 17.8167 - val_loss: 19.4718 - val_MinusLogProbMetric: 19.4718 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 460/1000
2023-10-11 12:01:42.536 
Epoch 460/1000 
	 loss: 17.7423, MinusLogProbMetric: 17.7423, val_loss: 17.8489, val_MinusLogProbMetric: 17.8489

Epoch 460: val_loss did not improve from 17.67199
196/196 - 54s - loss: 17.7423 - MinusLogProbMetric: 17.7423 - val_loss: 17.8489 - val_MinusLogProbMetric: 17.8489 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 461/1000
2023-10-11 12:02:36.870 
Epoch 461/1000 
	 loss: 17.7717, MinusLogProbMetric: 17.7717, val_loss: 20.2661, val_MinusLogProbMetric: 20.2661

Epoch 461: val_loss did not improve from 17.67199
196/196 - 54s - loss: 17.7717 - MinusLogProbMetric: 17.7717 - val_loss: 20.2661 - val_MinusLogProbMetric: 20.2661 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 462/1000
2023-10-11 12:03:29.861 
Epoch 462/1000 
	 loss: 17.7944, MinusLogProbMetric: 17.7944, val_loss: 17.8468, val_MinusLogProbMetric: 17.8468

Epoch 462: val_loss did not improve from 17.67199
196/196 - 53s - loss: 17.7944 - MinusLogProbMetric: 17.7944 - val_loss: 17.8468 - val_MinusLogProbMetric: 17.8468 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 463/1000
2023-10-11 12:04:21.340 
Epoch 463/1000 
	 loss: 17.7243, MinusLogProbMetric: 17.7243, val_loss: 17.7899, val_MinusLogProbMetric: 17.7899

Epoch 463: val_loss did not improve from 17.67199
196/196 - 51s - loss: 17.7243 - MinusLogProbMetric: 17.7243 - val_loss: 17.7899 - val_MinusLogProbMetric: 17.7899 - lr: 1.1111e-04 - 51s/epoch - 263ms/step
Epoch 464/1000
2023-10-11 12:05:12.856 
Epoch 464/1000 
	 loss: 17.7701, MinusLogProbMetric: 17.7701, val_loss: 17.7014, val_MinusLogProbMetric: 17.7014

Epoch 464: val_loss did not improve from 17.67199
196/196 - 52s - loss: 17.7701 - MinusLogProbMetric: 17.7701 - val_loss: 17.7014 - val_MinusLogProbMetric: 17.7014 - lr: 1.1111e-04 - 52s/epoch - 263ms/step
Epoch 465/1000
2023-10-11 12:06:07.758 
Epoch 465/1000 
	 loss: 17.7590, MinusLogProbMetric: 17.7590, val_loss: 17.8242, val_MinusLogProbMetric: 17.8242

Epoch 465: val_loss did not improve from 17.67199
196/196 - 55s - loss: 17.7590 - MinusLogProbMetric: 17.7590 - val_loss: 17.8242 - val_MinusLogProbMetric: 17.8242 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 466/1000
2023-10-11 12:07:01.161 
Epoch 466/1000 
	 loss: 17.8633, MinusLogProbMetric: 17.8633, val_loss: 17.7549, val_MinusLogProbMetric: 17.7549

Epoch 466: val_loss did not improve from 17.67199
196/196 - 53s - loss: 17.8633 - MinusLogProbMetric: 17.8633 - val_loss: 17.7549 - val_MinusLogProbMetric: 17.7549 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 467/1000
2023-10-11 12:07:52.024 
Epoch 467/1000 
	 loss: 17.7402, MinusLogProbMetric: 17.7402, val_loss: 17.6988, val_MinusLogProbMetric: 17.6988

Epoch 467: val_loss did not improve from 17.67199
196/196 - 51s - loss: 17.7402 - MinusLogProbMetric: 17.7402 - val_loss: 17.6988 - val_MinusLogProbMetric: 17.6988 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 468/1000
2023-10-11 12:08:43.408 
Epoch 468/1000 
	 loss: 17.7510, MinusLogProbMetric: 17.7510, val_loss: 17.6610, val_MinusLogProbMetric: 17.6610

Epoch 468: val_loss improved from 17.67199 to 17.66104, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 52s - loss: 17.7510 - MinusLogProbMetric: 17.7510 - val_loss: 17.6610 - val_MinusLogProbMetric: 17.6610 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 469/1000
2023-10-11 12:09:35.164 
Epoch 469/1000 
	 loss: 17.7633, MinusLogProbMetric: 17.7633, val_loss: 17.8052, val_MinusLogProbMetric: 17.8052

Epoch 469: val_loss did not improve from 17.66104
196/196 - 51s - loss: 17.7633 - MinusLogProbMetric: 17.7633 - val_loss: 17.8052 - val_MinusLogProbMetric: 17.8052 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 470/1000
2023-10-11 12:10:27.914 
Epoch 470/1000 
	 loss: 17.8383, MinusLogProbMetric: 17.8383, val_loss: 17.7329, val_MinusLogProbMetric: 17.7329

Epoch 470: val_loss did not improve from 17.66104
196/196 - 53s - loss: 17.8383 - MinusLogProbMetric: 17.8383 - val_loss: 17.7329 - val_MinusLogProbMetric: 17.7329 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 471/1000
2023-10-11 12:11:16.937 
Epoch 471/1000 
	 loss: 17.7078, MinusLogProbMetric: 17.7078, val_loss: 17.6988, val_MinusLogProbMetric: 17.6988

Epoch 471: val_loss did not improve from 17.66104
196/196 - 49s - loss: 17.7078 - MinusLogProbMetric: 17.7078 - val_loss: 17.6988 - val_MinusLogProbMetric: 17.6988 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 472/1000
2023-10-11 12:12:08.031 
Epoch 472/1000 
	 loss: 17.7555, MinusLogProbMetric: 17.7555, val_loss: 17.6929, val_MinusLogProbMetric: 17.6929

Epoch 472: val_loss did not improve from 17.66104
196/196 - 51s - loss: 17.7555 - MinusLogProbMetric: 17.7555 - val_loss: 17.6929 - val_MinusLogProbMetric: 17.6929 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 473/1000
2023-10-11 12:12:59.776 
Epoch 473/1000 
	 loss: 17.7566, MinusLogProbMetric: 17.7566, val_loss: 17.8721, val_MinusLogProbMetric: 17.8721

Epoch 473: val_loss did not improve from 17.66104
196/196 - 52s - loss: 17.7566 - MinusLogProbMetric: 17.7566 - val_loss: 17.8721 - val_MinusLogProbMetric: 17.8721 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 474/1000
2023-10-11 12:13:50.888 
Epoch 474/1000 
	 loss: 17.7647, MinusLogProbMetric: 17.7647, val_loss: 17.8043, val_MinusLogProbMetric: 17.8043

Epoch 474: val_loss did not improve from 17.66104
196/196 - 51s - loss: 17.7647 - MinusLogProbMetric: 17.7647 - val_loss: 17.8043 - val_MinusLogProbMetric: 17.8043 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 475/1000
2023-10-11 12:14:43.022 
Epoch 475/1000 
	 loss: 17.7599, MinusLogProbMetric: 17.7599, val_loss: 18.1167, val_MinusLogProbMetric: 18.1167

Epoch 475: val_loss did not improve from 17.66104
196/196 - 52s - loss: 17.7599 - MinusLogProbMetric: 17.7599 - val_loss: 18.1167 - val_MinusLogProbMetric: 18.1167 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 476/1000
2023-10-11 12:15:35.752 
Epoch 476/1000 
	 loss: 17.7291, MinusLogProbMetric: 17.7291, val_loss: 17.8160, val_MinusLogProbMetric: 17.8160

Epoch 476: val_loss did not improve from 17.66104
196/196 - 53s - loss: 17.7291 - MinusLogProbMetric: 17.7291 - val_loss: 17.8160 - val_MinusLogProbMetric: 17.8160 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 477/1000
2023-10-11 12:16:25.411 
Epoch 477/1000 
	 loss: 17.7154, MinusLogProbMetric: 17.7154, val_loss: 17.7668, val_MinusLogProbMetric: 17.7668

Epoch 477: val_loss did not improve from 17.66104
196/196 - 50s - loss: 17.7154 - MinusLogProbMetric: 17.7154 - val_loss: 17.7668 - val_MinusLogProbMetric: 17.7668 - lr: 1.1111e-04 - 50s/epoch - 253ms/step
Epoch 478/1000
2023-10-11 12:17:17.195 
Epoch 478/1000 
	 loss: 17.8085, MinusLogProbMetric: 17.8085, val_loss: 18.1225, val_MinusLogProbMetric: 18.1225

Epoch 478: val_loss did not improve from 17.66104
196/196 - 52s - loss: 17.8085 - MinusLogProbMetric: 17.8085 - val_loss: 18.1225 - val_MinusLogProbMetric: 18.1225 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 479/1000
2023-10-11 12:18:07.632 
Epoch 479/1000 
	 loss: 17.6749, MinusLogProbMetric: 17.6749, val_loss: 17.7172, val_MinusLogProbMetric: 17.7172

Epoch 479: val_loss did not improve from 17.66104
196/196 - 50s - loss: 17.6749 - MinusLogProbMetric: 17.6749 - val_loss: 17.7172 - val_MinusLogProbMetric: 17.7172 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 480/1000
2023-10-11 12:18:57.470 
Epoch 480/1000 
	 loss: 17.7797, MinusLogProbMetric: 17.7797, val_loss: 17.6718, val_MinusLogProbMetric: 17.6718

Epoch 480: val_loss did not improve from 17.66104
196/196 - 50s - loss: 17.7797 - MinusLogProbMetric: 17.7797 - val_loss: 17.6718 - val_MinusLogProbMetric: 17.6718 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 481/1000
2023-10-11 12:19:47.704 
Epoch 481/1000 
	 loss: 17.7771, MinusLogProbMetric: 17.7771, val_loss: 17.6934, val_MinusLogProbMetric: 17.6934

Epoch 481: val_loss did not improve from 17.66104
196/196 - 50s - loss: 17.7771 - MinusLogProbMetric: 17.7771 - val_loss: 17.6934 - val_MinusLogProbMetric: 17.6934 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 482/1000
2023-10-11 12:20:37.751 
Epoch 482/1000 
	 loss: 17.6947, MinusLogProbMetric: 17.6947, val_loss: 18.1422, val_MinusLogProbMetric: 18.1422

Epoch 482: val_loss did not improve from 17.66104
196/196 - 50s - loss: 17.6947 - MinusLogProbMetric: 17.6947 - val_loss: 18.1422 - val_MinusLogProbMetric: 18.1422 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 483/1000
2023-10-11 12:21:28.954 
Epoch 483/1000 
	 loss: 17.7410, MinusLogProbMetric: 17.7410, val_loss: 18.1093, val_MinusLogProbMetric: 18.1093

Epoch 483: val_loss did not improve from 17.66104
196/196 - 51s - loss: 17.7410 - MinusLogProbMetric: 17.7410 - val_loss: 18.1093 - val_MinusLogProbMetric: 18.1093 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 484/1000
2023-10-11 12:22:20.723 
Epoch 484/1000 
	 loss: 17.7562, MinusLogProbMetric: 17.7562, val_loss: 17.7075, val_MinusLogProbMetric: 17.7075

Epoch 484: val_loss did not improve from 17.66104
196/196 - 52s - loss: 17.7562 - MinusLogProbMetric: 17.7562 - val_loss: 17.7075 - val_MinusLogProbMetric: 17.7075 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 485/1000
2023-10-11 12:23:14.283 
Epoch 485/1000 
	 loss: 17.6751, MinusLogProbMetric: 17.6751, val_loss: 17.8232, val_MinusLogProbMetric: 17.8232

Epoch 485: val_loss did not improve from 17.66104
196/196 - 54s - loss: 17.6751 - MinusLogProbMetric: 17.6751 - val_loss: 17.8232 - val_MinusLogProbMetric: 17.8232 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 486/1000
2023-10-11 12:24:06.158 
Epoch 486/1000 
	 loss: 17.7332, MinusLogProbMetric: 17.7332, val_loss: 17.5888, val_MinusLogProbMetric: 17.5888

Epoch 486: val_loss improved from 17.66104 to 17.58879, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 53s - loss: 17.7332 - MinusLogProbMetric: 17.7332 - val_loss: 17.5888 - val_MinusLogProbMetric: 17.5888 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 487/1000
2023-10-11 12:24:57.817 
Epoch 487/1000 
	 loss: 17.8680, MinusLogProbMetric: 17.8680, val_loss: 17.6465, val_MinusLogProbMetric: 17.6465

Epoch 487: val_loss did not improve from 17.58879
196/196 - 51s - loss: 17.8680 - MinusLogProbMetric: 17.8680 - val_loss: 17.6465 - val_MinusLogProbMetric: 17.6465 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 488/1000
2023-10-11 12:25:49.999 
Epoch 488/1000 
	 loss: 17.7159, MinusLogProbMetric: 17.7159, val_loss: 17.7910, val_MinusLogProbMetric: 17.7910

Epoch 488: val_loss did not improve from 17.58879
196/196 - 52s - loss: 17.7159 - MinusLogProbMetric: 17.7159 - val_loss: 17.7910 - val_MinusLogProbMetric: 17.7910 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 489/1000
2023-10-11 12:26:40.772 
Epoch 489/1000 
	 loss: 17.6462, MinusLogProbMetric: 17.6462, val_loss: 17.7600, val_MinusLogProbMetric: 17.7600

Epoch 489: val_loss did not improve from 17.58879
196/196 - 51s - loss: 17.6462 - MinusLogProbMetric: 17.6462 - val_loss: 17.7600 - val_MinusLogProbMetric: 17.7600 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 490/1000
2023-10-11 12:27:32.453 
Epoch 490/1000 
	 loss: 17.7656, MinusLogProbMetric: 17.7656, val_loss: 17.9546, val_MinusLogProbMetric: 17.9546

Epoch 490: val_loss did not improve from 17.58879
196/196 - 52s - loss: 17.7656 - MinusLogProbMetric: 17.7656 - val_loss: 17.9546 - val_MinusLogProbMetric: 17.9546 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 491/1000
2023-10-11 12:28:24.432 
Epoch 491/1000 
	 loss: 17.7536, MinusLogProbMetric: 17.7536, val_loss: 18.1559, val_MinusLogProbMetric: 18.1559

Epoch 491: val_loss did not improve from 17.58879
196/196 - 52s - loss: 17.7536 - MinusLogProbMetric: 17.7536 - val_loss: 18.1559 - val_MinusLogProbMetric: 18.1559 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 492/1000
2023-10-11 12:29:16.122 
Epoch 492/1000 
	 loss: 17.7932, MinusLogProbMetric: 17.7932, val_loss: 18.1122, val_MinusLogProbMetric: 18.1122

Epoch 492: val_loss did not improve from 17.58879
196/196 - 52s - loss: 17.7932 - MinusLogProbMetric: 17.7932 - val_loss: 18.1122 - val_MinusLogProbMetric: 18.1122 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 493/1000
2023-10-11 12:30:07.197 
Epoch 493/1000 
	 loss: 17.6629, MinusLogProbMetric: 17.6629, val_loss: 17.6105, val_MinusLogProbMetric: 17.6105

Epoch 493: val_loss did not improve from 17.58879
196/196 - 51s - loss: 17.6629 - MinusLogProbMetric: 17.6629 - val_loss: 17.6105 - val_MinusLogProbMetric: 17.6105 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 494/1000
2023-10-11 12:30:57.218 
Epoch 494/1000 
	 loss: 17.6747, MinusLogProbMetric: 17.6747, val_loss: 17.7534, val_MinusLogProbMetric: 17.7534

Epoch 494: val_loss did not improve from 17.58879
196/196 - 50s - loss: 17.6747 - MinusLogProbMetric: 17.6747 - val_loss: 17.7534 - val_MinusLogProbMetric: 17.7534 - lr: 1.1111e-04 - 50s/epoch - 255ms/step
Epoch 495/1000
2023-10-11 12:31:48.591 
Epoch 495/1000 
	 loss: 17.7277, MinusLogProbMetric: 17.7277, val_loss: 18.4911, val_MinusLogProbMetric: 18.4911

Epoch 495: val_loss did not improve from 17.58879
196/196 - 51s - loss: 17.7277 - MinusLogProbMetric: 17.7277 - val_loss: 18.4911 - val_MinusLogProbMetric: 18.4911 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 496/1000
2023-10-11 12:32:38.987 
Epoch 496/1000 
	 loss: 17.7732, MinusLogProbMetric: 17.7732, val_loss: 17.7479, val_MinusLogProbMetric: 17.7479

Epoch 496: val_loss did not improve from 17.58879
196/196 - 50s - loss: 17.7732 - MinusLogProbMetric: 17.7732 - val_loss: 17.7479 - val_MinusLogProbMetric: 17.7479 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 497/1000
2023-10-11 12:33:32.517 
Epoch 497/1000 
	 loss: 17.7878, MinusLogProbMetric: 17.7878, val_loss: 17.9897, val_MinusLogProbMetric: 17.9897

Epoch 497: val_loss did not improve from 17.58879
196/196 - 54s - loss: 17.7878 - MinusLogProbMetric: 17.7878 - val_loss: 17.9897 - val_MinusLogProbMetric: 17.9897 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 498/1000
2023-10-11 12:34:24.980 
Epoch 498/1000 
	 loss: 17.7151, MinusLogProbMetric: 17.7151, val_loss: 17.7285, val_MinusLogProbMetric: 17.7285

Epoch 498: val_loss did not improve from 17.58879
196/196 - 52s - loss: 17.7151 - MinusLogProbMetric: 17.7151 - val_loss: 17.7285 - val_MinusLogProbMetric: 17.7285 - lr: 1.1111e-04 - 52s/epoch - 268ms/step
Epoch 499/1000
2023-10-11 12:35:17.019 
Epoch 499/1000 
	 loss: 17.6478, MinusLogProbMetric: 17.6478, val_loss: 17.6869, val_MinusLogProbMetric: 17.6869

Epoch 499: val_loss did not improve from 17.58879
196/196 - 52s - loss: 17.6478 - MinusLogProbMetric: 17.6478 - val_loss: 17.6869 - val_MinusLogProbMetric: 17.6869 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 500/1000
2023-10-11 12:36:10.951 
Epoch 500/1000 
	 loss: 17.7627, MinusLogProbMetric: 17.7627, val_loss: 18.2081, val_MinusLogProbMetric: 18.2081

Epoch 500: val_loss did not improve from 17.58879
196/196 - 54s - loss: 17.7627 - MinusLogProbMetric: 17.7627 - val_loss: 18.2081 - val_MinusLogProbMetric: 18.2081 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 501/1000
2023-10-11 12:37:04.752 
Epoch 501/1000 
	 loss: 17.7861, MinusLogProbMetric: 17.7861, val_loss: 17.9167, val_MinusLogProbMetric: 17.9167

Epoch 501: val_loss did not improve from 17.58879
196/196 - 54s - loss: 17.7861 - MinusLogProbMetric: 17.7861 - val_loss: 17.9167 - val_MinusLogProbMetric: 17.9167 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 502/1000
2023-10-11 12:37:58.991 
Epoch 502/1000 
	 loss: 17.6636, MinusLogProbMetric: 17.6636, val_loss: 17.7107, val_MinusLogProbMetric: 17.7107

Epoch 502: val_loss did not improve from 17.58879
196/196 - 54s - loss: 17.6636 - MinusLogProbMetric: 17.6636 - val_loss: 17.7107 - val_MinusLogProbMetric: 17.7107 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 503/1000
2023-10-11 12:38:52.210 
Epoch 503/1000 
	 loss: 17.6122, MinusLogProbMetric: 17.6122, val_loss: 18.4009, val_MinusLogProbMetric: 18.4009

Epoch 503: val_loss did not improve from 17.58879
196/196 - 53s - loss: 17.6122 - MinusLogProbMetric: 17.6122 - val_loss: 18.4009 - val_MinusLogProbMetric: 18.4009 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 504/1000
2023-10-11 12:39:44.356 
Epoch 504/1000 
	 loss: 17.7789, MinusLogProbMetric: 17.7789, val_loss: 17.6222, val_MinusLogProbMetric: 17.6222

Epoch 504: val_loss did not improve from 17.58879
196/196 - 52s - loss: 17.7789 - MinusLogProbMetric: 17.7789 - val_loss: 17.6222 - val_MinusLogProbMetric: 17.6222 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 505/1000
2023-10-11 12:40:36.513 
Epoch 505/1000 
	 loss: 17.6747, MinusLogProbMetric: 17.6747, val_loss: 18.0364, val_MinusLogProbMetric: 18.0364

Epoch 505: val_loss did not improve from 17.58879
196/196 - 52s - loss: 17.6747 - MinusLogProbMetric: 17.6747 - val_loss: 18.0364 - val_MinusLogProbMetric: 18.0364 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 506/1000
2023-10-11 12:41:27.980 
Epoch 506/1000 
	 loss: 17.6550, MinusLogProbMetric: 17.6550, val_loss: 17.8695, val_MinusLogProbMetric: 17.8695

Epoch 506: val_loss did not improve from 17.58879
196/196 - 51s - loss: 17.6550 - MinusLogProbMetric: 17.6550 - val_loss: 17.8695 - val_MinusLogProbMetric: 17.8695 - lr: 1.1111e-04 - 51s/epoch - 263ms/step
Epoch 507/1000
2023-10-11 12:42:21.327 
Epoch 507/1000 
	 loss: 17.9216, MinusLogProbMetric: 17.9216, val_loss: 17.6944, val_MinusLogProbMetric: 17.6944

Epoch 507: val_loss did not improve from 17.58879
196/196 - 53s - loss: 17.9216 - MinusLogProbMetric: 17.9216 - val_loss: 17.6944 - val_MinusLogProbMetric: 17.6944 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 508/1000
2023-10-11 12:43:11.311 
Epoch 508/1000 
	 loss: 17.7633, MinusLogProbMetric: 17.7633, val_loss: 17.5722, val_MinusLogProbMetric: 17.5722

Epoch 508: val_loss improved from 17.58879 to 17.57218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 51s - loss: 17.7633 - MinusLogProbMetric: 17.7633 - val_loss: 17.5722 - val_MinusLogProbMetric: 17.5722 - lr: 1.1111e-04 - 51s/epoch - 260ms/step
Epoch 509/1000
2023-10-11 12:44:04.579 
Epoch 509/1000 
	 loss: 17.5774, MinusLogProbMetric: 17.5774, val_loss: 17.6496, val_MinusLogProbMetric: 17.6496

Epoch 509: val_loss did not improve from 17.57218
196/196 - 52s - loss: 17.5774 - MinusLogProbMetric: 17.5774 - val_loss: 17.6496 - val_MinusLogProbMetric: 17.6496 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 510/1000
2023-10-11 12:44:56.460 
Epoch 510/1000 
	 loss: 17.7210, MinusLogProbMetric: 17.7210, val_loss: 18.0020, val_MinusLogProbMetric: 18.0020

Epoch 510: val_loss did not improve from 17.57218
196/196 - 52s - loss: 17.7210 - MinusLogProbMetric: 17.7210 - val_loss: 18.0020 - val_MinusLogProbMetric: 18.0020 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 511/1000
2023-10-11 12:45:48.795 
Epoch 511/1000 
	 loss: 17.6780, MinusLogProbMetric: 17.6780, val_loss: 17.5894, val_MinusLogProbMetric: 17.5894

Epoch 511: val_loss did not improve from 17.57218
196/196 - 52s - loss: 17.6780 - MinusLogProbMetric: 17.6780 - val_loss: 17.5894 - val_MinusLogProbMetric: 17.5894 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 512/1000
2023-10-11 12:46:39.583 
Epoch 512/1000 
	 loss: 17.6457, MinusLogProbMetric: 17.6457, val_loss: 17.7165, val_MinusLogProbMetric: 17.7165

Epoch 512: val_loss did not improve from 17.57218
196/196 - 51s - loss: 17.6457 - MinusLogProbMetric: 17.6457 - val_loss: 17.7165 - val_MinusLogProbMetric: 17.7165 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 513/1000
2023-10-11 12:47:30.781 
Epoch 513/1000 
	 loss: 17.8160, MinusLogProbMetric: 17.8160, val_loss: 17.6469, val_MinusLogProbMetric: 17.6469

Epoch 513: val_loss did not improve from 17.57218
196/196 - 51s - loss: 17.8160 - MinusLogProbMetric: 17.8160 - val_loss: 17.6469 - val_MinusLogProbMetric: 17.6469 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 514/1000
2023-10-11 12:48:23.118 
Epoch 514/1000 
	 loss: 17.5906, MinusLogProbMetric: 17.5906, val_loss: 17.6559, val_MinusLogProbMetric: 17.6559

Epoch 514: val_loss did not improve from 17.57218
196/196 - 52s - loss: 17.5906 - MinusLogProbMetric: 17.5906 - val_loss: 17.6559 - val_MinusLogProbMetric: 17.6559 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 515/1000
2023-10-11 12:49:17.173 
Epoch 515/1000 
	 loss: 17.6881, MinusLogProbMetric: 17.6881, val_loss: 17.5438, val_MinusLogProbMetric: 17.5438

Epoch 515: val_loss improved from 17.57218 to 17.54378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 55s - loss: 17.6881 - MinusLogProbMetric: 17.6881 - val_loss: 17.5438 - val_MinusLogProbMetric: 17.5438 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 516/1000
2023-10-11 12:50:12.704 
Epoch 516/1000 
	 loss: 17.6915, MinusLogProbMetric: 17.6915, val_loss: 18.1559, val_MinusLogProbMetric: 18.1559

Epoch 516: val_loss did not improve from 17.54378
196/196 - 55s - loss: 17.6915 - MinusLogProbMetric: 17.6915 - val_loss: 18.1559 - val_MinusLogProbMetric: 18.1559 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 517/1000
2023-10-11 12:51:05.320 
Epoch 517/1000 
	 loss: 17.7152, MinusLogProbMetric: 17.7152, val_loss: 17.5824, val_MinusLogProbMetric: 17.5824

Epoch 517: val_loss did not improve from 17.54378
196/196 - 53s - loss: 17.7152 - MinusLogProbMetric: 17.7152 - val_loss: 17.5824 - val_MinusLogProbMetric: 17.5824 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 518/1000
2023-10-11 12:51:57.497 
Epoch 518/1000 
	 loss: 17.6686, MinusLogProbMetric: 17.6686, val_loss: 17.6288, val_MinusLogProbMetric: 17.6288

Epoch 518: val_loss did not improve from 17.54378
196/196 - 52s - loss: 17.6686 - MinusLogProbMetric: 17.6686 - val_loss: 17.6288 - val_MinusLogProbMetric: 17.6288 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 519/1000
2023-10-11 12:52:48.633 
Epoch 519/1000 
	 loss: 17.6031, MinusLogProbMetric: 17.6031, val_loss: 18.2667, val_MinusLogProbMetric: 18.2667

Epoch 519: val_loss did not improve from 17.54378
196/196 - 51s - loss: 17.6031 - MinusLogProbMetric: 17.6031 - val_loss: 18.2667 - val_MinusLogProbMetric: 18.2667 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 520/1000
2023-10-11 12:53:41.322 
Epoch 520/1000 
	 loss: 17.6719, MinusLogProbMetric: 17.6719, val_loss: 17.8455, val_MinusLogProbMetric: 17.8455

Epoch 520: val_loss did not improve from 17.54378
196/196 - 53s - loss: 17.6719 - MinusLogProbMetric: 17.6719 - val_loss: 17.8455 - val_MinusLogProbMetric: 17.8455 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 521/1000
2023-10-11 12:54:34.813 
Epoch 521/1000 
	 loss: 17.5778, MinusLogProbMetric: 17.5778, val_loss: 17.7247, val_MinusLogProbMetric: 17.7247

Epoch 521: val_loss did not improve from 17.54378
196/196 - 54s - loss: 17.5778 - MinusLogProbMetric: 17.5778 - val_loss: 17.7247 - val_MinusLogProbMetric: 17.7247 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 522/1000
2023-10-11 12:55:28.346 
Epoch 522/1000 
	 loss: 17.6790, MinusLogProbMetric: 17.6790, val_loss: 17.6891, val_MinusLogProbMetric: 17.6891

Epoch 522: val_loss did not improve from 17.54378
196/196 - 54s - loss: 17.6790 - MinusLogProbMetric: 17.6790 - val_loss: 17.6891 - val_MinusLogProbMetric: 17.6891 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 523/1000
2023-10-11 12:56:19.897 
Epoch 523/1000 
	 loss: 17.7096, MinusLogProbMetric: 17.7096, val_loss: 17.7801, val_MinusLogProbMetric: 17.7801

Epoch 523: val_loss did not improve from 17.54378
196/196 - 52s - loss: 17.7096 - MinusLogProbMetric: 17.7096 - val_loss: 17.7801 - val_MinusLogProbMetric: 17.7801 - lr: 1.1111e-04 - 52s/epoch - 263ms/step
Epoch 524/1000
2023-10-11 12:57:11.125 
Epoch 524/1000 
	 loss: 17.6514, MinusLogProbMetric: 17.6514, val_loss: 17.5924, val_MinusLogProbMetric: 17.5924

Epoch 524: val_loss did not improve from 17.54378
196/196 - 51s - loss: 17.6514 - MinusLogProbMetric: 17.6514 - val_loss: 17.5924 - val_MinusLogProbMetric: 17.5924 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 525/1000
2023-10-11 12:58:05.512 
Epoch 525/1000 
	 loss: 17.5982, MinusLogProbMetric: 17.5982, val_loss: 17.6336, val_MinusLogProbMetric: 17.6336

Epoch 525: val_loss did not improve from 17.54378
196/196 - 54s - loss: 17.5982 - MinusLogProbMetric: 17.5982 - val_loss: 17.6336 - val_MinusLogProbMetric: 17.6336 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 526/1000
2023-10-11 12:59:00.164 
Epoch 526/1000 
	 loss: 17.6333, MinusLogProbMetric: 17.6333, val_loss: 17.5807, val_MinusLogProbMetric: 17.5807

Epoch 526: val_loss did not improve from 17.54378
196/196 - 55s - loss: 17.6333 - MinusLogProbMetric: 17.6333 - val_loss: 17.5807 - val_MinusLogProbMetric: 17.5807 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 527/1000
2023-10-11 12:59:53.311 
Epoch 527/1000 
	 loss: 17.7775, MinusLogProbMetric: 17.7775, val_loss: 18.0315, val_MinusLogProbMetric: 18.0315

Epoch 527: val_loss did not improve from 17.54378
196/196 - 53s - loss: 17.7775 - MinusLogProbMetric: 17.7775 - val_loss: 18.0315 - val_MinusLogProbMetric: 18.0315 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 528/1000
2023-10-11 13:00:47.270 
Epoch 528/1000 
	 loss: 17.7344, MinusLogProbMetric: 17.7344, val_loss: 17.6025, val_MinusLogProbMetric: 17.6025

Epoch 528: val_loss did not improve from 17.54378
196/196 - 54s - loss: 17.7344 - MinusLogProbMetric: 17.7344 - val_loss: 17.6025 - val_MinusLogProbMetric: 17.6025 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 529/1000
2023-10-11 13:01:40.276 
Epoch 529/1000 
	 loss: 17.6120, MinusLogProbMetric: 17.6120, val_loss: 17.9860, val_MinusLogProbMetric: 17.9860

Epoch 529: val_loss did not improve from 17.54378
196/196 - 53s - loss: 17.6120 - MinusLogProbMetric: 17.6120 - val_loss: 17.9860 - val_MinusLogProbMetric: 17.9860 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 530/1000
2023-10-11 13:02:33.542 
Epoch 530/1000 
	 loss: 17.7014, MinusLogProbMetric: 17.7014, val_loss: 17.6603, val_MinusLogProbMetric: 17.6603

Epoch 530: val_loss did not improve from 17.54378
196/196 - 53s - loss: 17.7014 - MinusLogProbMetric: 17.7014 - val_loss: 17.6603 - val_MinusLogProbMetric: 17.6603 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 531/1000
2023-10-11 13:03:25.692 
Epoch 531/1000 
	 loss: 17.6611, MinusLogProbMetric: 17.6611, val_loss: 17.7597, val_MinusLogProbMetric: 17.7597

Epoch 531: val_loss did not improve from 17.54378
196/196 - 52s - loss: 17.6611 - MinusLogProbMetric: 17.6611 - val_loss: 17.7597 - val_MinusLogProbMetric: 17.7597 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 532/1000
2023-10-11 13:04:18.010 
Epoch 532/1000 
	 loss: 17.7056, MinusLogProbMetric: 17.7056, val_loss: 17.6174, val_MinusLogProbMetric: 17.6174

Epoch 532: val_loss did not improve from 17.54378
196/196 - 52s - loss: 17.7056 - MinusLogProbMetric: 17.7056 - val_loss: 17.6174 - val_MinusLogProbMetric: 17.6174 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 533/1000
2023-10-11 13:05:10.637 
Epoch 533/1000 
	 loss: 17.6417, MinusLogProbMetric: 17.6417, val_loss: 17.9352, val_MinusLogProbMetric: 17.9352

Epoch 533: val_loss did not improve from 17.54378
196/196 - 53s - loss: 17.6417 - MinusLogProbMetric: 17.6417 - val_loss: 17.9352 - val_MinusLogProbMetric: 17.9352 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 534/1000
2023-10-11 13:06:04.078 
Epoch 534/1000 
	 loss: 17.5874, MinusLogProbMetric: 17.5874, val_loss: 17.8462, val_MinusLogProbMetric: 17.8462

Epoch 534: val_loss did not improve from 17.54378
196/196 - 53s - loss: 17.5874 - MinusLogProbMetric: 17.5874 - val_loss: 17.8462 - val_MinusLogProbMetric: 17.8462 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 535/1000
2023-10-11 13:06:58.061 
Epoch 535/1000 
	 loss: 17.7283, MinusLogProbMetric: 17.7283, val_loss: 17.7501, val_MinusLogProbMetric: 17.7501

Epoch 535: val_loss did not improve from 17.54378
196/196 - 54s - loss: 17.7283 - MinusLogProbMetric: 17.7283 - val_loss: 17.7501 - val_MinusLogProbMetric: 17.7501 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 536/1000
2023-10-11 13:07:50.533 
Epoch 536/1000 
	 loss: 17.6031, MinusLogProbMetric: 17.6031, val_loss: 17.6549, val_MinusLogProbMetric: 17.6549

Epoch 536: val_loss did not improve from 17.54378
196/196 - 52s - loss: 17.6031 - MinusLogProbMetric: 17.6031 - val_loss: 17.6549 - val_MinusLogProbMetric: 17.6549 - lr: 1.1111e-04 - 52s/epoch - 268ms/step
Epoch 537/1000
2023-10-11 13:08:43.859 
Epoch 537/1000 
	 loss: 17.8187, MinusLogProbMetric: 17.8187, val_loss: 17.9510, val_MinusLogProbMetric: 17.9510

Epoch 537: val_loss did not improve from 17.54378
196/196 - 53s - loss: 17.8187 - MinusLogProbMetric: 17.8187 - val_loss: 17.9510 - val_MinusLogProbMetric: 17.9510 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 538/1000
2023-10-11 13:09:36.689 
Epoch 538/1000 
	 loss: 17.6552, MinusLogProbMetric: 17.6552, val_loss: 17.7025, val_MinusLogProbMetric: 17.7025

Epoch 538: val_loss did not improve from 17.54378
196/196 - 53s - loss: 17.6552 - MinusLogProbMetric: 17.6552 - val_loss: 17.7025 - val_MinusLogProbMetric: 17.7025 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 539/1000
2023-10-11 13:10:28.428 
Epoch 539/1000 
	 loss: 17.5939, MinusLogProbMetric: 17.5939, val_loss: 17.8896, val_MinusLogProbMetric: 17.8896

Epoch 539: val_loss did not improve from 17.54378
196/196 - 52s - loss: 17.5939 - MinusLogProbMetric: 17.5939 - val_loss: 17.8896 - val_MinusLogProbMetric: 17.8896 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 540/1000
2023-10-11 13:11:21.553 
Epoch 540/1000 
	 loss: 17.6116, MinusLogProbMetric: 17.6116, val_loss: 17.6296, val_MinusLogProbMetric: 17.6296

Epoch 540: val_loss did not improve from 17.54378
196/196 - 53s - loss: 17.6116 - MinusLogProbMetric: 17.6116 - val_loss: 17.6296 - val_MinusLogProbMetric: 17.6296 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 541/1000
2023-10-11 13:12:13.210 
Epoch 541/1000 
	 loss: 17.6828, MinusLogProbMetric: 17.6828, val_loss: 17.6205, val_MinusLogProbMetric: 17.6205

Epoch 541: val_loss did not improve from 17.54378
196/196 - 52s - loss: 17.6828 - MinusLogProbMetric: 17.6828 - val_loss: 17.6205 - val_MinusLogProbMetric: 17.6205 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 542/1000
2023-10-11 13:13:05.117 
Epoch 542/1000 
	 loss: 17.6342, MinusLogProbMetric: 17.6342, val_loss: 17.5851, val_MinusLogProbMetric: 17.5851

Epoch 542: val_loss did not improve from 17.54378
196/196 - 52s - loss: 17.6342 - MinusLogProbMetric: 17.6342 - val_loss: 17.5851 - val_MinusLogProbMetric: 17.5851 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 543/1000
2023-10-11 13:13:58.158 
Epoch 543/1000 
	 loss: 17.6403, MinusLogProbMetric: 17.6403, val_loss: 17.6425, val_MinusLogProbMetric: 17.6425

Epoch 543: val_loss did not improve from 17.54378
196/196 - 53s - loss: 17.6403 - MinusLogProbMetric: 17.6403 - val_loss: 17.6425 - val_MinusLogProbMetric: 17.6425 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 544/1000
2023-10-11 13:14:51.672 
Epoch 544/1000 
	 loss: 17.6702, MinusLogProbMetric: 17.6702, val_loss: 17.7630, val_MinusLogProbMetric: 17.7630

Epoch 544: val_loss did not improve from 17.54378
196/196 - 54s - loss: 17.6702 - MinusLogProbMetric: 17.6702 - val_loss: 17.7630 - val_MinusLogProbMetric: 17.7630 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 545/1000
2023-10-11 13:15:46.217 
Epoch 545/1000 
	 loss: 17.6181, MinusLogProbMetric: 17.6181, val_loss: 17.8662, val_MinusLogProbMetric: 17.8662

Epoch 545: val_loss did not improve from 17.54378
196/196 - 55s - loss: 17.6181 - MinusLogProbMetric: 17.6181 - val_loss: 17.8662 - val_MinusLogProbMetric: 17.8662 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 546/1000
2023-10-11 13:16:39.989 
Epoch 546/1000 
	 loss: 17.6755, MinusLogProbMetric: 17.6755, val_loss: 17.5421, val_MinusLogProbMetric: 17.5421

Epoch 546: val_loss improved from 17.54378 to 17.54213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 55s - loss: 17.6755 - MinusLogProbMetric: 17.6755 - val_loss: 17.5421 - val_MinusLogProbMetric: 17.5421 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 547/1000
2023-10-11 13:17:31.796 
Epoch 547/1000 
	 loss: 17.5616, MinusLogProbMetric: 17.5616, val_loss: 17.5418, val_MinusLogProbMetric: 17.5418

Epoch 547: val_loss improved from 17.54213 to 17.54179, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 52s - loss: 17.5616 - MinusLogProbMetric: 17.5616 - val_loss: 17.5418 - val_MinusLogProbMetric: 17.5418 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 548/1000
2023-10-11 13:18:22.798 
Epoch 548/1000 
	 loss: 17.5934, MinusLogProbMetric: 17.5934, val_loss: 17.5925, val_MinusLogProbMetric: 17.5925

Epoch 548: val_loss did not improve from 17.54179
196/196 - 50s - loss: 17.5934 - MinusLogProbMetric: 17.5934 - val_loss: 17.5925 - val_MinusLogProbMetric: 17.5925 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 549/1000
2023-10-11 13:19:14.261 
Epoch 549/1000 
	 loss: 17.5982, MinusLogProbMetric: 17.5982, val_loss: 17.6072, val_MinusLogProbMetric: 17.6072

Epoch 549: val_loss did not improve from 17.54179
196/196 - 51s - loss: 17.5982 - MinusLogProbMetric: 17.5982 - val_loss: 17.6072 - val_MinusLogProbMetric: 17.6072 - lr: 1.1111e-04 - 51s/epoch - 263ms/step
Epoch 550/1000
2023-10-11 13:20:07.183 
Epoch 550/1000 
	 loss: 17.5986, MinusLogProbMetric: 17.5986, val_loss: 17.5538, val_MinusLogProbMetric: 17.5538

Epoch 550: val_loss did not improve from 17.54179
196/196 - 53s - loss: 17.5986 - MinusLogProbMetric: 17.5986 - val_loss: 17.5538 - val_MinusLogProbMetric: 17.5538 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 551/1000
2023-10-11 13:21:00.606 
Epoch 551/1000 
	 loss: 17.6934, MinusLogProbMetric: 17.6934, val_loss: 17.5494, val_MinusLogProbMetric: 17.5494

Epoch 551: val_loss did not improve from 17.54179
196/196 - 53s - loss: 17.6934 - MinusLogProbMetric: 17.6934 - val_loss: 17.5494 - val_MinusLogProbMetric: 17.5494 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 552/1000
2023-10-11 13:21:54.220 
Epoch 552/1000 
	 loss: 17.5876, MinusLogProbMetric: 17.5876, val_loss: 17.7204, val_MinusLogProbMetric: 17.7204

Epoch 552: val_loss did not improve from 17.54179
196/196 - 54s - loss: 17.5876 - MinusLogProbMetric: 17.5876 - val_loss: 17.7204 - val_MinusLogProbMetric: 17.7204 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 553/1000
2023-10-11 13:22:47.540 
Epoch 553/1000 
	 loss: 17.6681, MinusLogProbMetric: 17.6681, val_loss: 17.7229, val_MinusLogProbMetric: 17.7229

Epoch 553: val_loss did not improve from 17.54179
196/196 - 53s - loss: 17.6681 - MinusLogProbMetric: 17.6681 - val_loss: 17.7229 - val_MinusLogProbMetric: 17.7229 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 554/1000
2023-10-11 13:23:40.828 
Epoch 554/1000 
	 loss: 17.5840, MinusLogProbMetric: 17.5840, val_loss: 17.8091, val_MinusLogProbMetric: 17.8091

Epoch 554: val_loss did not improve from 17.54179
196/196 - 53s - loss: 17.5840 - MinusLogProbMetric: 17.5840 - val_loss: 17.8091 - val_MinusLogProbMetric: 17.8091 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 555/1000
2023-10-11 13:24:34.942 
Epoch 555/1000 
	 loss: 17.6132, MinusLogProbMetric: 17.6132, val_loss: 17.8877, val_MinusLogProbMetric: 17.8877

Epoch 555: val_loss did not improve from 17.54179
196/196 - 54s - loss: 17.6132 - MinusLogProbMetric: 17.6132 - val_loss: 17.8877 - val_MinusLogProbMetric: 17.8877 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 556/1000
2023-10-11 13:25:28.356 
Epoch 556/1000 
	 loss: 17.5778, MinusLogProbMetric: 17.5778, val_loss: 17.5309, val_MinusLogProbMetric: 17.5309

Epoch 556: val_loss improved from 17.54179 to 17.53090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 54s - loss: 17.5778 - MinusLogProbMetric: 17.5778 - val_loss: 17.5309 - val_MinusLogProbMetric: 17.5309 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 557/1000
2023-10-11 13:26:22.079 
Epoch 557/1000 
	 loss: 17.6409, MinusLogProbMetric: 17.6409, val_loss: 17.5690, val_MinusLogProbMetric: 17.5690

Epoch 557: val_loss did not improve from 17.53090
196/196 - 53s - loss: 17.6409 - MinusLogProbMetric: 17.6409 - val_loss: 17.5690 - val_MinusLogProbMetric: 17.5690 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 558/1000
2023-10-11 13:27:14.523 
Epoch 558/1000 
	 loss: 17.6721, MinusLogProbMetric: 17.6721, val_loss: 17.7653, val_MinusLogProbMetric: 17.7653

Epoch 558: val_loss did not improve from 17.53090
196/196 - 52s - loss: 17.6721 - MinusLogProbMetric: 17.6721 - val_loss: 17.7653 - val_MinusLogProbMetric: 17.7653 - lr: 1.1111e-04 - 52s/epoch - 268ms/step
Epoch 559/1000
2023-10-11 13:28:07.110 
Epoch 559/1000 
	 loss: 17.5919, MinusLogProbMetric: 17.5919, val_loss: 17.4984, val_MinusLogProbMetric: 17.4984

Epoch 559: val_loss improved from 17.53090 to 17.49842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 53s - loss: 17.5919 - MinusLogProbMetric: 17.5919 - val_loss: 17.4984 - val_MinusLogProbMetric: 17.4984 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 560/1000
2023-10-11 13:29:00.793 
Epoch 560/1000 
	 loss: 17.5746, MinusLogProbMetric: 17.5746, val_loss: 17.7097, val_MinusLogProbMetric: 17.7097

Epoch 560: val_loss did not improve from 17.49842
196/196 - 53s - loss: 17.5746 - MinusLogProbMetric: 17.5746 - val_loss: 17.7097 - val_MinusLogProbMetric: 17.7097 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 561/1000
2023-10-11 13:29:53.312 
Epoch 561/1000 
	 loss: 17.5662, MinusLogProbMetric: 17.5662, val_loss: 17.8311, val_MinusLogProbMetric: 17.8311

Epoch 561: val_loss did not improve from 17.49842
196/196 - 53s - loss: 17.5662 - MinusLogProbMetric: 17.5662 - val_loss: 17.8311 - val_MinusLogProbMetric: 17.8311 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 562/1000
2023-10-11 13:30:43.969 
Epoch 562/1000 
	 loss: 17.5532, MinusLogProbMetric: 17.5532, val_loss: 17.6531, val_MinusLogProbMetric: 17.6531

Epoch 562: val_loss did not improve from 17.49842
196/196 - 51s - loss: 17.5532 - MinusLogProbMetric: 17.5532 - val_loss: 17.6531 - val_MinusLogProbMetric: 17.6531 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 563/1000
2023-10-11 13:31:35.383 
Epoch 563/1000 
	 loss: 17.7541, MinusLogProbMetric: 17.7541, val_loss: 17.5604, val_MinusLogProbMetric: 17.5604

Epoch 563: val_loss did not improve from 17.49842
196/196 - 51s - loss: 17.7541 - MinusLogProbMetric: 17.7541 - val_loss: 17.5604 - val_MinusLogProbMetric: 17.5604 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 564/1000
2023-10-11 13:32:28.583 
Epoch 564/1000 
	 loss: 17.6819, MinusLogProbMetric: 17.6819, val_loss: 17.8865, val_MinusLogProbMetric: 17.8865

Epoch 564: val_loss did not improve from 17.49842
196/196 - 53s - loss: 17.6819 - MinusLogProbMetric: 17.6819 - val_loss: 17.8865 - val_MinusLogProbMetric: 17.8865 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 565/1000
2023-10-11 13:33:22.019 
Epoch 565/1000 
	 loss: 17.6076, MinusLogProbMetric: 17.6076, val_loss: 17.9808, val_MinusLogProbMetric: 17.9808

Epoch 565: val_loss did not improve from 17.49842
196/196 - 53s - loss: 17.6076 - MinusLogProbMetric: 17.6076 - val_loss: 17.9808 - val_MinusLogProbMetric: 17.9808 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 566/1000
2023-10-11 13:34:16.574 
Epoch 566/1000 
	 loss: 17.5975, MinusLogProbMetric: 17.5975, val_loss: 18.1163, val_MinusLogProbMetric: 18.1163

Epoch 566: val_loss did not improve from 17.49842
196/196 - 55s - loss: 17.5975 - MinusLogProbMetric: 17.5975 - val_loss: 18.1163 - val_MinusLogProbMetric: 18.1163 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 567/1000
2023-10-11 13:35:08.509 
Epoch 567/1000 
	 loss: 17.6349, MinusLogProbMetric: 17.6349, val_loss: 18.0243, val_MinusLogProbMetric: 18.0243

Epoch 567: val_loss did not improve from 17.49842
196/196 - 52s - loss: 17.6349 - MinusLogProbMetric: 17.6349 - val_loss: 18.0243 - val_MinusLogProbMetric: 18.0243 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 568/1000
2023-10-11 13:36:00.700 
Epoch 568/1000 
	 loss: 17.5963, MinusLogProbMetric: 17.5963, val_loss: 17.8755, val_MinusLogProbMetric: 17.8755

Epoch 568: val_loss did not improve from 17.49842
196/196 - 52s - loss: 17.5963 - MinusLogProbMetric: 17.5963 - val_loss: 17.8755 - val_MinusLogProbMetric: 17.8755 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 569/1000
2023-10-11 13:36:51.950 
Epoch 569/1000 
	 loss: 17.6462, MinusLogProbMetric: 17.6462, val_loss: 17.6175, val_MinusLogProbMetric: 17.6175

Epoch 569: val_loss did not improve from 17.49842
196/196 - 51s - loss: 17.6462 - MinusLogProbMetric: 17.6462 - val_loss: 17.6175 - val_MinusLogProbMetric: 17.6175 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 570/1000
2023-10-11 13:37:43.227 
Epoch 570/1000 
	 loss: 17.5904, MinusLogProbMetric: 17.5904, val_loss: 17.6256, val_MinusLogProbMetric: 17.6256

Epoch 570: val_loss did not improve from 17.49842
196/196 - 51s - loss: 17.5904 - MinusLogProbMetric: 17.5904 - val_loss: 17.6256 - val_MinusLogProbMetric: 17.6256 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 571/1000
2023-10-11 13:38:33.770 
Epoch 571/1000 
	 loss: 17.6068, MinusLogProbMetric: 17.6068, val_loss: 18.2058, val_MinusLogProbMetric: 18.2058

Epoch 571: val_loss did not improve from 17.49842
196/196 - 51s - loss: 17.6068 - MinusLogProbMetric: 17.6068 - val_loss: 18.2058 - val_MinusLogProbMetric: 18.2058 - lr: 1.1111e-04 - 51s/epoch - 258ms/step
Epoch 572/1000
2023-10-11 13:39:28.191 
Epoch 572/1000 
	 loss: 17.6041, MinusLogProbMetric: 17.6041, val_loss: 17.5888, val_MinusLogProbMetric: 17.5888

Epoch 572: val_loss did not improve from 17.49842
196/196 - 54s - loss: 17.6041 - MinusLogProbMetric: 17.6041 - val_loss: 17.5888 - val_MinusLogProbMetric: 17.5888 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 573/1000
2023-10-11 13:40:22.423 
Epoch 573/1000 
	 loss: 17.6238, MinusLogProbMetric: 17.6238, val_loss: 17.6824, val_MinusLogProbMetric: 17.6824

Epoch 573: val_loss did not improve from 17.49842
196/196 - 54s - loss: 17.6238 - MinusLogProbMetric: 17.6238 - val_loss: 17.6824 - val_MinusLogProbMetric: 17.6824 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 574/1000
2023-10-11 13:41:16.877 
Epoch 574/1000 
	 loss: 17.5526, MinusLogProbMetric: 17.5526, val_loss: 17.5945, val_MinusLogProbMetric: 17.5945

Epoch 574: val_loss did not improve from 17.49842
196/196 - 54s - loss: 17.5526 - MinusLogProbMetric: 17.5526 - val_loss: 17.5945 - val_MinusLogProbMetric: 17.5945 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 575/1000
2023-10-11 13:42:10.837 
Epoch 575/1000 
	 loss: 17.5660, MinusLogProbMetric: 17.5660, val_loss: 17.5511, val_MinusLogProbMetric: 17.5511

Epoch 575: val_loss did not improve from 17.49842
196/196 - 54s - loss: 17.5660 - MinusLogProbMetric: 17.5660 - val_loss: 17.5511 - val_MinusLogProbMetric: 17.5511 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 576/1000
2023-10-11 13:43:03.556 
Epoch 576/1000 
	 loss: 17.5707, MinusLogProbMetric: 17.5707, val_loss: 17.5236, val_MinusLogProbMetric: 17.5236

Epoch 576: val_loss did not improve from 17.49842
196/196 - 53s - loss: 17.5707 - MinusLogProbMetric: 17.5707 - val_loss: 17.5236 - val_MinusLogProbMetric: 17.5236 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 577/1000
2023-10-11 13:43:57.639 
Epoch 577/1000 
	 loss: 17.6279, MinusLogProbMetric: 17.6279, val_loss: 17.5142, val_MinusLogProbMetric: 17.5142

Epoch 577: val_loss did not improve from 17.49842
196/196 - 54s - loss: 17.6279 - MinusLogProbMetric: 17.6279 - val_loss: 17.5142 - val_MinusLogProbMetric: 17.5142 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 578/1000
2023-10-11 13:44:51.901 
Epoch 578/1000 
	 loss: 17.6150, MinusLogProbMetric: 17.6150, val_loss: 17.7094, val_MinusLogProbMetric: 17.7094

Epoch 578: val_loss did not improve from 17.49842
196/196 - 54s - loss: 17.6150 - MinusLogProbMetric: 17.6150 - val_loss: 17.7094 - val_MinusLogProbMetric: 17.7094 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 579/1000
2023-10-11 13:45:45.053 
Epoch 579/1000 
	 loss: 17.5942, MinusLogProbMetric: 17.5942, val_loss: 18.1085, val_MinusLogProbMetric: 18.1085

Epoch 579: val_loss did not improve from 17.49842
196/196 - 53s - loss: 17.5942 - MinusLogProbMetric: 17.5942 - val_loss: 18.1085 - val_MinusLogProbMetric: 18.1085 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 580/1000
2023-10-11 13:46:36.499 
Epoch 580/1000 
	 loss: 17.5879, MinusLogProbMetric: 17.5879, val_loss: 17.5817, val_MinusLogProbMetric: 17.5817

Epoch 580: val_loss did not improve from 17.49842
196/196 - 51s - loss: 17.5879 - MinusLogProbMetric: 17.5879 - val_loss: 17.5817 - val_MinusLogProbMetric: 17.5817 - lr: 1.1111e-04 - 51s/epoch - 263ms/step
Epoch 581/1000
2023-10-11 13:47:29.234 
Epoch 581/1000 
	 loss: 17.5588, MinusLogProbMetric: 17.5588, val_loss: 17.6164, val_MinusLogProbMetric: 17.6164

Epoch 581: val_loss did not improve from 17.49842
196/196 - 53s - loss: 17.5588 - MinusLogProbMetric: 17.5588 - val_loss: 17.6164 - val_MinusLogProbMetric: 17.6164 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 582/1000
2023-10-11 13:48:21.418 
Epoch 582/1000 
	 loss: 17.6007, MinusLogProbMetric: 17.6007, val_loss: 17.5568, val_MinusLogProbMetric: 17.5568

Epoch 582: val_loss did not improve from 17.49842
196/196 - 52s - loss: 17.6007 - MinusLogProbMetric: 17.6007 - val_loss: 17.5568 - val_MinusLogProbMetric: 17.5568 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 583/1000
2023-10-11 13:49:13.899 
Epoch 583/1000 
	 loss: 17.5457, MinusLogProbMetric: 17.5457, val_loss: 17.4850, val_MinusLogProbMetric: 17.4850

Epoch 583: val_loss improved from 17.49842 to 17.48505, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 53s - loss: 17.5457 - MinusLogProbMetric: 17.5457 - val_loss: 17.4850 - val_MinusLogProbMetric: 17.4850 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 584/1000
2023-10-11 13:50:06.739 
Epoch 584/1000 
	 loss: 17.6912, MinusLogProbMetric: 17.6912, val_loss: 17.4792, val_MinusLogProbMetric: 17.4792

Epoch 584: val_loss improved from 17.48505 to 17.47925, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 53s - loss: 17.6912 - MinusLogProbMetric: 17.6912 - val_loss: 17.4792 - val_MinusLogProbMetric: 17.4792 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 585/1000
2023-10-11 13:51:00.329 
Epoch 585/1000 
	 loss: 17.5161, MinusLogProbMetric: 17.5161, val_loss: 17.4694, val_MinusLogProbMetric: 17.4694

Epoch 585: val_loss improved from 17.47925 to 17.46942, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 54s - loss: 17.5161 - MinusLogProbMetric: 17.5161 - val_loss: 17.4694 - val_MinusLogProbMetric: 17.4694 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 586/1000
2023-10-11 13:51:53.241 
Epoch 586/1000 
	 loss: 17.6485, MinusLogProbMetric: 17.6485, val_loss: 17.5189, val_MinusLogProbMetric: 17.5189

Epoch 586: val_loss did not improve from 17.46942
196/196 - 52s - loss: 17.6485 - MinusLogProbMetric: 17.6485 - val_loss: 17.5189 - val_MinusLogProbMetric: 17.5189 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 587/1000
2023-10-11 13:52:46.075 
Epoch 587/1000 
	 loss: 17.4991, MinusLogProbMetric: 17.4991, val_loss: 17.7732, val_MinusLogProbMetric: 17.7732

Epoch 587: val_loss did not improve from 17.46942
196/196 - 53s - loss: 17.4991 - MinusLogProbMetric: 17.4991 - val_loss: 17.7732 - val_MinusLogProbMetric: 17.7732 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 588/1000
2023-10-11 13:53:39.000 
Epoch 588/1000 
	 loss: 17.6206, MinusLogProbMetric: 17.6206, val_loss: 17.9305, val_MinusLogProbMetric: 17.9305

Epoch 588: val_loss did not improve from 17.46942
196/196 - 53s - loss: 17.6206 - MinusLogProbMetric: 17.6206 - val_loss: 17.9305 - val_MinusLogProbMetric: 17.9305 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 589/1000
2023-10-11 13:54:30.203 
Epoch 589/1000 
	 loss: 17.5763, MinusLogProbMetric: 17.5763, val_loss: 17.7093, val_MinusLogProbMetric: 17.7093

Epoch 589: val_loss did not improve from 17.46942
196/196 - 51s - loss: 17.5763 - MinusLogProbMetric: 17.5763 - val_loss: 17.7093 - val_MinusLogProbMetric: 17.7093 - lr: 1.1111e-04 - 51s/epoch - 261ms/step
Epoch 590/1000
2023-10-11 13:55:23.346 
Epoch 590/1000 
	 loss: 17.5847, MinusLogProbMetric: 17.5847, val_loss: 17.7329, val_MinusLogProbMetric: 17.7329

Epoch 590: val_loss did not improve from 17.46942
196/196 - 53s - loss: 17.5847 - MinusLogProbMetric: 17.5847 - val_loss: 17.7329 - val_MinusLogProbMetric: 17.7329 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 591/1000
2023-10-11 13:56:15.705 
Epoch 591/1000 
	 loss: 17.6019, MinusLogProbMetric: 17.6019, val_loss: 17.8319, val_MinusLogProbMetric: 17.8319

Epoch 591: val_loss did not improve from 17.46942
196/196 - 52s - loss: 17.6019 - MinusLogProbMetric: 17.6019 - val_loss: 17.8319 - val_MinusLogProbMetric: 17.8319 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 592/1000
2023-10-11 13:57:10.337 
Epoch 592/1000 
	 loss: 17.5504, MinusLogProbMetric: 17.5504, val_loss: 17.5100, val_MinusLogProbMetric: 17.5100

Epoch 592: val_loss did not improve from 17.46942
196/196 - 55s - loss: 17.5504 - MinusLogProbMetric: 17.5504 - val_loss: 17.5100 - val_MinusLogProbMetric: 17.5100 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 593/1000
2023-10-11 13:58:02.508 
Epoch 593/1000 
	 loss: 17.5734, MinusLogProbMetric: 17.5734, val_loss: 17.4826, val_MinusLogProbMetric: 17.4826

Epoch 593: val_loss did not improve from 17.46942
196/196 - 52s - loss: 17.5734 - MinusLogProbMetric: 17.5734 - val_loss: 17.4826 - val_MinusLogProbMetric: 17.4826 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 594/1000
2023-10-11 13:58:58.013 
Epoch 594/1000 
	 loss: 17.5309, MinusLogProbMetric: 17.5309, val_loss: 17.5701, val_MinusLogProbMetric: 17.5701

Epoch 594: val_loss did not improve from 17.46942
196/196 - 56s - loss: 17.5309 - MinusLogProbMetric: 17.5309 - val_loss: 17.5701 - val_MinusLogProbMetric: 17.5701 - lr: 1.1111e-04 - 56s/epoch - 283ms/step
Epoch 595/1000
2023-10-11 13:59:52.522 
Epoch 595/1000 
	 loss: 17.5491, MinusLogProbMetric: 17.5491, val_loss: 17.6411, val_MinusLogProbMetric: 17.6411

Epoch 595: val_loss did not improve from 17.46942
196/196 - 55s - loss: 17.5491 - MinusLogProbMetric: 17.5491 - val_loss: 17.6411 - val_MinusLogProbMetric: 17.6411 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 596/1000
2023-10-11 14:00:48.267 
Epoch 596/1000 
	 loss: 17.5947, MinusLogProbMetric: 17.5947, val_loss: 17.6705, val_MinusLogProbMetric: 17.6705

Epoch 596: val_loss did not improve from 17.46942
196/196 - 56s - loss: 17.5947 - MinusLogProbMetric: 17.5947 - val_loss: 17.6705 - val_MinusLogProbMetric: 17.6705 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 597/1000
2023-10-11 14:01:43.517 
Epoch 597/1000 
	 loss: 17.5456, MinusLogProbMetric: 17.5456, val_loss: 17.5288, val_MinusLogProbMetric: 17.5288

Epoch 597: val_loss did not improve from 17.46942
196/196 - 55s - loss: 17.5456 - MinusLogProbMetric: 17.5456 - val_loss: 17.5288 - val_MinusLogProbMetric: 17.5288 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 598/1000
2023-10-11 14:02:37.303 
Epoch 598/1000 
	 loss: 17.5789, MinusLogProbMetric: 17.5789, val_loss: 17.4537, val_MinusLogProbMetric: 17.4537

Epoch 598: val_loss improved from 17.46942 to 17.45371, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 55s - loss: 17.5789 - MinusLogProbMetric: 17.5789 - val_loss: 17.4537 - val_MinusLogProbMetric: 17.4537 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 599/1000
2023-10-11 14:03:31.514 
Epoch 599/1000 
	 loss: 17.5524, MinusLogProbMetric: 17.5524, val_loss: 17.6713, val_MinusLogProbMetric: 17.6713

Epoch 599: val_loss did not improve from 17.45371
196/196 - 53s - loss: 17.5524 - MinusLogProbMetric: 17.5524 - val_loss: 17.6713 - val_MinusLogProbMetric: 17.6713 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 600/1000
2023-10-11 14:04:26.575 
Epoch 600/1000 
	 loss: 17.6643, MinusLogProbMetric: 17.6643, val_loss: 17.6137, val_MinusLogProbMetric: 17.6137

Epoch 600: val_loss did not improve from 17.45371
196/196 - 55s - loss: 17.6643 - MinusLogProbMetric: 17.6643 - val_loss: 17.6137 - val_MinusLogProbMetric: 17.6137 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 601/1000
2023-10-11 14:05:21.085 
Epoch 601/1000 
	 loss: 17.5946, MinusLogProbMetric: 17.5946, val_loss: 17.5703, val_MinusLogProbMetric: 17.5703

Epoch 601: val_loss did not improve from 17.45371
196/196 - 55s - loss: 17.5946 - MinusLogProbMetric: 17.5946 - val_loss: 17.5703 - val_MinusLogProbMetric: 17.5703 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 602/1000
2023-10-11 14:06:15.566 
Epoch 602/1000 
	 loss: 17.5350, MinusLogProbMetric: 17.5350, val_loss: 17.6104, val_MinusLogProbMetric: 17.6104

Epoch 602: val_loss did not improve from 17.45371
196/196 - 54s - loss: 17.5350 - MinusLogProbMetric: 17.5350 - val_loss: 17.6104 - val_MinusLogProbMetric: 17.6104 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 603/1000
2023-10-11 14:07:09.437 
Epoch 603/1000 
	 loss: 17.5276, MinusLogProbMetric: 17.5276, val_loss: 17.4893, val_MinusLogProbMetric: 17.4893

Epoch 603: val_loss did not improve from 17.45371
196/196 - 54s - loss: 17.5276 - MinusLogProbMetric: 17.5276 - val_loss: 17.4893 - val_MinusLogProbMetric: 17.4893 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 604/1000
2023-10-11 14:08:02.361 
Epoch 604/1000 
	 loss: 17.6313, MinusLogProbMetric: 17.6313, val_loss: 17.5343, val_MinusLogProbMetric: 17.5343

Epoch 604: val_loss did not improve from 17.45371
196/196 - 53s - loss: 17.6313 - MinusLogProbMetric: 17.6313 - val_loss: 17.5343 - val_MinusLogProbMetric: 17.5343 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 605/1000
2023-10-11 14:08:55.569 
Epoch 605/1000 
	 loss: 17.5766, MinusLogProbMetric: 17.5766, val_loss: 18.6659, val_MinusLogProbMetric: 18.6659

Epoch 605: val_loss did not improve from 17.45371
196/196 - 53s - loss: 17.5766 - MinusLogProbMetric: 17.5766 - val_loss: 18.6659 - val_MinusLogProbMetric: 18.6659 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 606/1000
2023-10-11 14:09:49.867 
Epoch 606/1000 
	 loss: 17.6654, MinusLogProbMetric: 17.6654, val_loss: 17.8877, val_MinusLogProbMetric: 17.8877

Epoch 606: val_loss did not improve from 17.45371
196/196 - 54s - loss: 17.6654 - MinusLogProbMetric: 17.6654 - val_loss: 17.8877 - val_MinusLogProbMetric: 17.8877 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 607/1000
2023-10-11 14:10:44.530 
Epoch 607/1000 
	 loss: 17.5418, MinusLogProbMetric: 17.5418, val_loss: 17.4720, val_MinusLogProbMetric: 17.4720

Epoch 607: val_loss did not improve from 17.45371
196/196 - 55s - loss: 17.5418 - MinusLogProbMetric: 17.5418 - val_loss: 17.4720 - val_MinusLogProbMetric: 17.4720 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 608/1000
2023-10-11 14:11:39.437 
Epoch 608/1000 
	 loss: 17.5598, MinusLogProbMetric: 17.5598, val_loss: 17.8996, val_MinusLogProbMetric: 17.8996

Epoch 608: val_loss did not improve from 17.45371
196/196 - 55s - loss: 17.5598 - MinusLogProbMetric: 17.5598 - val_loss: 17.8996 - val_MinusLogProbMetric: 17.8996 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 609/1000
2023-10-11 14:12:32.120 
Epoch 609/1000 
	 loss: 17.6866, MinusLogProbMetric: 17.6866, val_loss: 17.6201, val_MinusLogProbMetric: 17.6201

Epoch 609: val_loss did not improve from 17.45371
196/196 - 53s - loss: 17.6866 - MinusLogProbMetric: 17.6866 - val_loss: 17.6201 - val_MinusLogProbMetric: 17.6201 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 610/1000
2023-10-11 14:13:24.934 
Epoch 610/1000 
	 loss: 17.5511, MinusLogProbMetric: 17.5511, val_loss: 18.0159, val_MinusLogProbMetric: 18.0159

Epoch 610: val_loss did not improve from 17.45371
196/196 - 53s - loss: 17.5511 - MinusLogProbMetric: 17.5511 - val_loss: 18.0159 - val_MinusLogProbMetric: 18.0159 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 611/1000
2023-10-11 14:14:17.811 
Epoch 611/1000 
	 loss: 17.5498, MinusLogProbMetric: 17.5498, val_loss: 17.6055, val_MinusLogProbMetric: 17.6055

Epoch 611: val_loss did not improve from 17.45371
196/196 - 53s - loss: 17.5498 - MinusLogProbMetric: 17.5498 - val_loss: 17.6055 - val_MinusLogProbMetric: 17.6055 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 612/1000
2023-10-11 14:15:13.425 
Epoch 612/1000 
	 loss: 17.5576, MinusLogProbMetric: 17.5576, val_loss: 17.5670, val_MinusLogProbMetric: 17.5670

Epoch 612: val_loss did not improve from 17.45371
196/196 - 56s - loss: 17.5576 - MinusLogProbMetric: 17.5576 - val_loss: 17.5670 - val_MinusLogProbMetric: 17.5670 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 613/1000
2023-10-11 14:16:07.575 
Epoch 613/1000 
	 loss: 17.5094, MinusLogProbMetric: 17.5094, val_loss: 17.5858, val_MinusLogProbMetric: 17.5858

Epoch 613: val_loss did not improve from 17.45371
196/196 - 54s - loss: 17.5094 - MinusLogProbMetric: 17.5094 - val_loss: 17.5858 - val_MinusLogProbMetric: 17.5858 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 614/1000
2023-10-11 14:17:02.019 
Epoch 614/1000 
	 loss: 17.5292, MinusLogProbMetric: 17.5292, val_loss: 17.5181, val_MinusLogProbMetric: 17.5181

Epoch 614: val_loss did not improve from 17.45371
196/196 - 54s - loss: 17.5292 - MinusLogProbMetric: 17.5292 - val_loss: 17.5181 - val_MinusLogProbMetric: 17.5181 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 615/1000
2023-10-11 14:17:55.779 
Epoch 615/1000 
	 loss: 17.5609, MinusLogProbMetric: 17.5609, val_loss: 17.5434, val_MinusLogProbMetric: 17.5434

Epoch 615: val_loss did not improve from 17.45371
196/196 - 54s - loss: 17.5609 - MinusLogProbMetric: 17.5609 - val_loss: 17.5434 - val_MinusLogProbMetric: 17.5434 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 616/1000
2023-10-11 14:18:49.710 
Epoch 616/1000 
	 loss: 17.6517, MinusLogProbMetric: 17.6517, val_loss: 17.7828, val_MinusLogProbMetric: 17.7828

Epoch 616: val_loss did not improve from 17.45371
196/196 - 54s - loss: 17.6517 - MinusLogProbMetric: 17.6517 - val_loss: 17.7828 - val_MinusLogProbMetric: 17.7828 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 617/1000
2023-10-11 14:19:43.974 
Epoch 617/1000 
	 loss: 17.5722, MinusLogProbMetric: 17.5722, val_loss: 17.7225, val_MinusLogProbMetric: 17.7225

Epoch 617: val_loss did not improve from 17.45371
196/196 - 54s - loss: 17.5722 - MinusLogProbMetric: 17.5722 - val_loss: 17.7225 - val_MinusLogProbMetric: 17.7225 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 618/1000
2023-10-11 14:20:37.593 
Epoch 618/1000 
	 loss: 17.4623, MinusLogProbMetric: 17.4623, val_loss: 18.3468, val_MinusLogProbMetric: 18.3468

Epoch 618: val_loss did not improve from 17.45371
196/196 - 54s - loss: 17.4623 - MinusLogProbMetric: 17.4623 - val_loss: 18.3468 - val_MinusLogProbMetric: 18.3468 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 619/1000
2023-10-11 14:21:31.355 
Epoch 619/1000 
	 loss: 17.5692, MinusLogProbMetric: 17.5692, val_loss: 18.2659, val_MinusLogProbMetric: 18.2659

Epoch 619: val_loss did not improve from 17.45371
196/196 - 54s - loss: 17.5692 - MinusLogProbMetric: 17.5692 - val_loss: 18.2659 - val_MinusLogProbMetric: 18.2659 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 620/1000
2023-10-11 14:22:25.946 
Epoch 620/1000 
	 loss: 17.5459, MinusLogProbMetric: 17.5459, val_loss: 17.5634, val_MinusLogProbMetric: 17.5634

Epoch 620: val_loss did not improve from 17.45371
196/196 - 55s - loss: 17.5459 - MinusLogProbMetric: 17.5459 - val_loss: 17.5634 - val_MinusLogProbMetric: 17.5634 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 621/1000
2023-10-11 14:23:19.586 
Epoch 621/1000 
	 loss: 17.5806, MinusLogProbMetric: 17.5806, val_loss: 17.6672, val_MinusLogProbMetric: 17.6672

Epoch 621: val_loss did not improve from 17.45371
196/196 - 54s - loss: 17.5806 - MinusLogProbMetric: 17.5806 - val_loss: 17.6672 - val_MinusLogProbMetric: 17.6672 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 622/1000
2023-10-11 14:24:12.667 
Epoch 622/1000 
	 loss: 17.4873, MinusLogProbMetric: 17.4873, val_loss: 17.6787, val_MinusLogProbMetric: 17.6787

Epoch 622: val_loss did not improve from 17.45371
196/196 - 53s - loss: 17.4873 - MinusLogProbMetric: 17.4873 - val_loss: 17.6787 - val_MinusLogProbMetric: 17.6787 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 623/1000
2023-10-11 14:25:04.597 
Epoch 623/1000 
	 loss: 17.4942, MinusLogProbMetric: 17.4942, val_loss: 17.4514, val_MinusLogProbMetric: 17.4514

Epoch 623: val_loss improved from 17.45371 to 17.45139, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 53s - loss: 17.4942 - MinusLogProbMetric: 17.4942 - val_loss: 17.4514 - val_MinusLogProbMetric: 17.4514 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 624/1000
2023-10-11 14:25:59.361 
Epoch 624/1000 
	 loss: 17.6508, MinusLogProbMetric: 17.6508, val_loss: 18.1244, val_MinusLogProbMetric: 18.1244

Epoch 624: val_loss did not improve from 17.45139
196/196 - 54s - loss: 17.6508 - MinusLogProbMetric: 17.6508 - val_loss: 18.1244 - val_MinusLogProbMetric: 18.1244 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 625/1000
2023-10-11 14:26:53.052 
Epoch 625/1000 
	 loss: 17.4919, MinusLogProbMetric: 17.4919, val_loss: 17.4658, val_MinusLogProbMetric: 17.4658

Epoch 625: val_loss did not improve from 17.45139
196/196 - 54s - loss: 17.4919 - MinusLogProbMetric: 17.4919 - val_loss: 17.4658 - val_MinusLogProbMetric: 17.4658 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 626/1000
2023-10-11 14:27:48.709 
Epoch 626/1000 
	 loss: 17.5346, MinusLogProbMetric: 17.5346, val_loss: 17.5950, val_MinusLogProbMetric: 17.5950

Epoch 626: val_loss did not improve from 17.45139
196/196 - 56s - loss: 17.5346 - MinusLogProbMetric: 17.5346 - val_loss: 17.5950 - val_MinusLogProbMetric: 17.5950 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 627/1000
2023-10-11 14:28:43.105 
Epoch 627/1000 
	 loss: 17.5933, MinusLogProbMetric: 17.5933, val_loss: 17.4857, val_MinusLogProbMetric: 17.4857

Epoch 627: val_loss did not improve from 17.45139
196/196 - 54s - loss: 17.5933 - MinusLogProbMetric: 17.5933 - val_loss: 17.4857 - val_MinusLogProbMetric: 17.4857 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 628/1000
2023-10-11 14:29:37.736 
Epoch 628/1000 
	 loss: 17.4320, MinusLogProbMetric: 17.4320, val_loss: 17.4181, val_MinusLogProbMetric: 17.4181

Epoch 628: val_loss improved from 17.45139 to 17.41812, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 55s - loss: 17.4320 - MinusLogProbMetric: 17.4320 - val_loss: 17.4181 - val_MinusLogProbMetric: 17.4181 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 629/1000
2023-10-11 14:30:33.546 
Epoch 629/1000 
	 loss: 17.5261, MinusLogProbMetric: 17.5261, val_loss: 17.5820, val_MinusLogProbMetric: 17.5820

Epoch 629: val_loss did not improve from 17.41812
196/196 - 55s - loss: 17.5261 - MinusLogProbMetric: 17.5261 - val_loss: 17.5820 - val_MinusLogProbMetric: 17.5820 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 630/1000
2023-10-11 14:31:27.339 
Epoch 630/1000 
	 loss: 17.5086, MinusLogProbMetric: 17.5086, val_loss: 17.4152, val_MinusLogProbMetric: 17.4152

Epoch 630: val_loss improved from 17.41812 to 17.41519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 55s - loss: 17.5086 - MinusLogProbMetric: 17.5086 - val_loss: 17.4152 - val_MinusLogProbMetric: 17.4152 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 631/1000
2023-10-11 14:32:22.019 
Epoch 631/1000 
	 loss: 17.5018, MinusLogProbMetric: 17.5018, val_loss: 17.4999, val_MinusLogProbMetric: 17.4999

Epoch 631: val_loss did not improve from 17.41519
196/196 - 54s - loss: 17.5018 - MinusLogProbMetric: 17.5018 - val_loss: 17.4999 - val_MinusLogProbMetric: 17.4999 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 632/1000
2023-10-11 14:33:16.945 
Epoch 632/1000 
	 loss: 17.5072, MinusLogProbMetric: 17.5072, val_loss: 17.7612, val_MinusLogProbMetric: 17.7612

Epoch 632: val_loss did not improve from 17.41519
196/196 - 55s - loss: 17.5072 - MinusLogProbMetric: 17.5072 - val_loss: 17.7612 - val_MinusLogProbMetric: 17.7612 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 633/1000
2023-10-11 14:34:10.236 
Epoch 633/1000 
	 loss: 17.6492, MinusLogProbMetric: 17.6492, val_loss: 17.9833, val_MinusLogProbMetric: 17.9833

Epoch 633: val_loss did not improve from 17.41519
196/196 - 53s - loss: 17.6492 - MinusLogProbMetric: 17.6492 - val_loss: 17.9833 - val_MinusLogProbMetric: 17.9833 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 634/1000
2023-10-11 14:35:05.054 
Epoch 634/1000 
	 loss: 17.5238, MinusLogProbMetric: 17.5238, val_loss: 17.5332, val_MinusLogProbMetric: 17.5332

Epoch 634: val_loss did not improve from 17.41519
196/196 - 55s - loss: 17.5238 - MinusLogProbMetric: 17.5238 - val_loss: 17.5332 - val_MinusLogProbMetric: 17.5332 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 635/1000
2023-10-11 14:35:59.474 
Epoch 635/1000 
	 loss: 17.5483, MinusLogProbMetric: 17.5483, val_loss: 17.4796, val_MinusLogProbMetric: 17.4796

Epoch 635: val_loss did not improve from 17.41519
196/196 - 54s - loss: 17.5483 - MinusLogProbMetric: 17.5483 - val_loss: 17.4796 - val_MinusLogProbMetric: 17.4796 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 636/1000
2023-10-11 14:36:54.201 
Epoch 636/1000 
	 loss: 17.6275, MinusLogProbMetric: 17.6275, val_loss: 17.4619, val_MinusLogProbMetric: 17.4619

Epoch 636: val_loss did not improve from 17.41519
196/196 - 55s - loss: 17.6275 - MinusLogProbMetric: 17.6275 - val_loss: 17.4619 - val_MinusLogProbMetric: 17.4619 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 637/1000
2023-10-11 14:37:48.537 
Epoch 637/1000 
	 loss: 17.4676, MinusLogProbMetric: 17.4676, val_loss: 17.4537, val_MinusLogProbMetric: 17.4537

Epoch 637: val_loss did not improve from 17.41519
196/196 - 54s - loss: 17.4676 - MinusLogProbMetric: 17.4676 - val_loss: 17.4537 - val_MinusLogProbMetric: 17.4537 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 638/1000
2023-10-11 14:38:42.461 
Epoch 638/1000 
	 loss: 17.5150, MinusLogProbMetric: 17.5150, val_loss: 18.1540, val_MinusLogProbMetric: 18.1540

Epoch 638: val_loss did not improve from 17.41519
196/196 - 54s - loss: 17.5150 - MinusLogProbMetric: 17.5150 - val_loss: 18.1540 - val_MinusLogProbMetric: 18.1540 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 639/1000
2023-10-11 14:39:37.316 
Epoch 639/1000 
	 loss: 17.5172, MinusLogProbMetric: 17.5172, val_loss: 17.8664, val_MinusLogProbMetric: 17.8664

Epoch 639: val_loss did not improve from 17.41519
196/196 - 55s - loss: 17.5172 - MinusLogProbMetric: 17.5172 - val_loss: 17.8664 - val_MinusLogProbMetric: 17.8664 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 640/1000
2023-10-11 14:40:32.159 
Epoch 640/1000 
	 loss: 17.6569, MinusLogProbMetric: 17.6569, val_loss: 17.7477, val_MinusLogProbMetric: 17.7477

Epoch 640: val_loss did not improve from 17.41519
196/196 - 55s - loss: 17.6569 - MinusLogProbMetric: 17.6569 - val_loss: 17.7477 - val_MinusLogProbMetric: 17.7477 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 641/1000
2023-10-11 14:41:25.805 
Epoch 641/1000 
	 loss: 17.4694, MinusLogProbMetric: 17.4694, val_loss: 17.4837, val_MinusLogProbMetric: 17.4837

Epoch 641: val_loss did not improve from 17.41519
196/196 - 54s - loss: 17.4694 - MinusLogProbMetric: 17.4694 - val_loss: 17.4837 - val_MinusLogProbMetric: 17.4837 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 642/1000
2023-10-11 14:42:19.864 
Epoch 642/1000 
	 loss: 17.4937, MinusLogProbMetric: 17.4937, val_loss: 17.4975, val_MinusLogProbMetric: 17.4975

Epoch 642: val_loss did not improve from 17.41519
196/196 - 54s - loss: 17.4937 - MinusLogProbMetric: 17.4937 - val_loss: 17.4975 - val_MinusLogProbMetric: 17.4975 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 643/1000
2023-10-11 14:43:14.937 
Epoch 643/1000 
	 loss: 17.4715, MinusLogProbMetric: 17.4715, val_loss: 17.5701, val_MinusLogProbMetric: 17.5701

Epoch 643: val_loss did not improve from 17.41519
196/196 - 55s - loss: 17.4715 - MinusLogProbMetric: 17.4715 - val_loss: 17.5701 - val_MinusLogProbMetric: 17.5701 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 644/1000
2023-10-11 14:44:08.786 
Epoch 644/1000 
	 loss: 17.5900, MinusLogProbMetric: 17.5900, val_loss: 17.5612, val_MinusLogProbMetric: 17.5612

Epoch 644: val_loss did not improve from 17.41519
196/196 - 54s - loss: 17.5900 - MinusLogProbMetric: 17.5900 - val_loss: 17.5612 - val_MinusLogProbMetric: 17.5612 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 645/1000
2023-10-11 14:45:02.948 
Epoch 645/1000 
	 loss: 17.5617, MinusLogProbMetric: 17.5617, val_loss: 17.5941, val_MinusLogProbMetric: 17.5941

Epoch 645: val_loss did not improve from 17.41519
196/196 - 54s - loss: 17.5617 - MinusLogProbMetric: 17.5617 - val_loss: 17.5941 - val_MinusLogProbMetric: 17.5941 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 646/1000
2023-10-11 14:45:58.567 
Epoch 646/1000 
	 loss: 17.5085, MinusLogProbMetric: 17.5085, val_loss: 17.6820, val_MinusLogProbMetric: 17.6820

Epoch 646: val_loss did not improve from 17.41519
196/196 - 56s - loss: 17.5085 - MinusLogProbMetric: 17.5085 - val_loss: 17.6820 - val_MinusLogProbMetric: 17.6820 - lr: 1.1111e-04 - 56s/epoch - 284ms/step
Epoch 647/1000
2023-10-11 14:46:52.645 
Epoch 647/1000 
	 loss: 17.5550, MinusLogProbMetric: 17.5550, val_loss: 17.7258, val_MinusLogProbMetric: 17.7258

Epoch 647: val_loss did not improve from 17.41519
196/196 - 54s - loss: 17.5550 - MinusLogProbMetric: 17.5550 - val_loss: 17.7258 - val_MinusLogProbMetric: 17.7258 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 648/1000
2023-10-11 14:47:46.657 
Epoch 648/1000 
	 loss: 17.5032, MinusLogProbMetric: 17.5032, val_loss: 17.4405, val_MinusLogProbMetric: 17.4405

Epoch 648: val_loss did not improve from 17.41519
196/196 - 54s - loss: 17.5032 - MinusLogProbMetric: 17.5032 - val_loss: 17.4405 - val_MinusLogProbMetric: 17.4405 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 649/1000
2023-10-11 14:48:39.623 
Epoch 649/1000 
	 loss: 17.5283, MinusLogProbMetric: 17.5283, val_loss: 18.2333, val_MinusLogProbMetric: 18.2333

Epoch 649: val_loss did not improve from 17.41519
196/196 - 53s - loss: 17.5283 - MinusLogProbMetric: 17.5283 - val_loss: 18.2333 - val_MinusLogProbMetric: 18.2333 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 650/1000
2023-10-11 14:49:33.953 
Epoch 650/1000 
	 loss: 17.4545, MinusLogProbMetric: 17.4545, val_loss: 17.5276, val_MinusLogProbMetric: 17.5276

Epoch 650: val_loss did not improve from 17.41519
196/196 - 54s - loss: 17.4545 - MinusLogProbMetric: 17.4545 - val_loss: 17.5276 - val_MinusLogProbMetric: 17.5276 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 651/1000
2023-10-11 14:50:28.525 
Epoch 651/1000 
	 loss: 17.5239, MinusLogProbMetric: 17.5239, val_loss: 17.6278, val_MinusLogProbMetric: 17.6278

Epoch 651: val_loss did not improve from 17.41519
196/196 - 55s - loss: 17.5239 - MinusLogProbMetric: 17.5239 - val_loss: 17.6278 - val_MinusLogProbMetric: 17.6278 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 652/1000
2023-10-11 14:51:23.745 
Epoch 652/1000 
	 loss: 17.5944, MinusLogProbMetric: 17.5944, val_loss: 17.5643, val_MinusLogProbMetric: 17.5643

Epoch 652: val_loss did not improve from 17.41519
196/196 - 55s - loss: 17.5944 - MinusLogProbMetric: 17.5944 - val_loss: 17.5643 - val_MinusLogProbMetric: 17.5643 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 653/1000
2023-10-11 14:52:18.072 
Epoch 653/1000 
	 loss: 17.4600, MinusLogProbMetric: 17.4600, val_loss: 17.6130, val_MinusLogProbMetric: 17.6130

Epoch 653: val_loss did not improve from 17.41519
196/196 - 54s - loss: 17.4600 - MinusLogProbMetric: 17.4600 - val_loss: 17.6130 - val_MinusLogProbMetric: 17.6130 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 654/1000
2023-10-11 14:53:12.870 
Epoch 654/1000 
	 loss: 17.5409, MinusLogProbMetric: 17.5409, val_loss: 17.4963, val_MinusLogProbMetric: 17.4963

Epoch 654: val_loss did not improve from 17.41519
196/196 - 55s - loss: 17.5409 - MinusLogProbMetric: 17.5409 - val_loss: 17.4963 - val_MinusLogProbMetric: 17.4963 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 655/1000
2023-10-11 14:54:07.773 
Epoch 655/1000 
	 loss: 17.5325, MinusLogProbMetric: 17.5325, val_loss: 17.5966, val_MinusLogProbMetric: 17.5966

Epoch 655: val_loss did not improve from 17.41519
196/196 - 55s - loss: 17.5325 - MinusLogProbMetric: 17.5325 - val_loss: 17.5966 - val_MinusLogProbMetric: 17.5966 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 656/1000
2023-10-11 14:55:03.808 
Epoch 656/1000 
	 loss: 17.5260, MinusLogProbMetric: 17.5260, val_loss: 17.7635, val_MinusLogProbMetric: 17.7635

Epoch 656: val_loss did not improve from 17.41519
196/196 - 56s - loss: 17.5260 - MinusLogProbMetric: 17.5260 - val_loss: 17.7635 - val_MinusLogProbMetric: 17.7635 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 657/1000
2023-10-11 14:55:59.800 
Epoch 657/1000 
	 loss: 17.5149, MinusLogProbMetric: 17.5149, val_loss: 17.6002, val_MinusLogProbMetric: 17.6002

Epoch 657: val_loss did not improve from 17.41519
196/196 - 56s - loss: 17.5149 - MinusLogProbMetric: 17.5149 - val_loss: 17.6002 - val_MinusLogProbMetric: 17.6002 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 658/1000
2023-10-11 14:56:55.835 
Epoch 658/1000 
	 loss: 17.5412, MinusLogProbMetric: 17.5412, val_loss: 17.5633, val_MinusLogProbMetric: 17.5633

Epoch 658: val_loss did not improve from 17.41519
196/196 - 56s - loss: 17.5412 - MinusLogProbMetric: 17.5412 - val_loss: 17.5633 - val_MinusLogProbMetric: 17.5633 - lr: 1.1111e-04 - 56s/epoch - 286ms/step
Epoch 659/1000
2023-10-11 14:57:51.688 
Epoch 659/1000 
	 loss: 17.5004, MinusLogProbMetric: 17.5004, val_loss: 17.4586, val_MinusLogProbMetric: 17.4586

Epoch 659: val_loss did not improve from 17.41519
196/196 - 56s - loss: 17.5004 - MinusLogProbMetric: 17.5004 - val_loss: 17.4586 - val_MinusLogProbMetric: 17.4586 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 660/1000
2023-10-11 14:58:47.047 
Epoch 660/1000 
	 loss: 17.4965, MinusLogProbMetric: 17.4965, val_loss: 17.5811, val_MinusLogProbMetric: 17.5811

Epoch 660: val_loss did not improve from 17.41519
196/196 - 55s - loss: 17.4965 - MinusLogProbMetric: 17.4965 - val_loss: 17.5811 - val_MinusLogProbMetric: 17.5811 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 661/1000
2023-10-11 14:59:40.657 
Epoch 661/1000 
	 loss: 17.4390, MinusLogProbMetric: 17.4390, val_loss: 17.4008, val_MinusLogProbMetric: 17.4008

Epoch 661: val_loss improved from 17.41519 to 17.40077, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 54s - loss: 17.4390 - MinusLogProbMetric: 17.4390 - val_loss: 17.4008 - val_MinusLogProbMetric: 17.4008 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 662/1000
2023-10-11 15:00:33.534 
Epoch 662/1000 
	 loss: 17.4800, MinusLogProbMetric: 17.4800, val_loss: 17.8183, val_MinusLogProbMetric: 17.8183

Epoch 662: val_loss did not improve from 17.40077
196/196 - 52s - loss: 17.4800 - MinusLogProbMetric: 17.4800 - val_loss: 17.8183 - val_MinusLogProbMetric: 17.8183 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 663/1000
2023-10-11 15:01:27.342 
Epoch 663/1000 
	 loss: 17.5061, MinusLogProbMetric: 17.5061, val_loss: 17.5489, val_MinusLogProbMetric: 17.5489

Epoch 663: val_loss did not improve from 17.40077
196/196 - 54s - loss: 17.5061 - MinusLogProbMetric: 17.5061 - val_loss: 17.5489 - val_MinusLogProbMetric: 17.5489 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 664/1000
2023-10-11 15:02:21.095 
Epoch 664/1000 
	 loss: 17.4945, MinusLogProbMetric: 17.4945, val_loss: 17.6459, val_MinusLogProbMetric: 17.6459

Epoch 664: val_loss did not improve from 17.40077
196/196 - 54s - loss: 17.4945 - MinusLogProbMetric: 17.4945 - val_loss: 17.6459 - val_MinusLogProbMetric: 17.6459 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 665/1000
2023-10-11 15:03:15.484 
Epoch 665/1000 
	 loss: 17.5097, MinusLogProbMetric: 17.5097, val_loss: 17.4244, val_MinusLogProbMetric: 17.4244

Epoch 665: val_loss did not improve from 17.40077
196/196 - 54s - loss: 17.5097 - MinusLogProbMetric: 17.5097 - val_loss: 17.4244 - val_MinusLogProbMetric: 17.4244 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 666/1000
2023-10-11 15:04:09.870 
Epoch 666/1000 
	 loss: 17.4808, MinusLogProbMetric: 17.4808, val_loss: 17.4041, val_MinusLogProbMetric: 17.4041

Epoch 666: val_loss did not improve from 17.40077
196/196 - 54s - loss: 17.4808 - MinusLogProbMetric: 17.4808 - val_loss: 17.4041 - val_MinusLogProbMetric: 17.4041 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 667/1000
2023-10-11 15:05:02.580 
Epoch 667/1000 
	 loss: 17.4655, MinusLogProbMetric: 17.4655, val_loss: 17.6348, val_MinusLogProbMetric: 17.6348

Epoch 667: val_loss did not improve from 17.40077
196/196 - 53s - loss: 17.4655 - MinusLogProbMetric: 17.4655 - val_loss: 17.6348 - val_MinusLogProbMetric: 17.6348 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 668/1000
2023-10-11 15:05:56.611 
Epoch 668/1000 
	 loss: 17.4324, MinusLogProbMetric: 17.4324, val_loss: 18.1621, val_MinusLogProbMetric: 18.1621

Epoch 668: val_loss did not improve from 17.40077
196/196 - 54s - loss: 17.4324 - MinusLogProbMetric: 17.4324 - val_loss: 18.1621 - val_MinusLogProbMetric: 18.1621 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 669/1000
2023-10-11 15:06:51.497 
Epoch 669/1000 
	 loss: 17.5224, MinusLogProbMetric: 17.5224, val_loss: 17.6119, val_MinusLogProbMetric: 17.6119

Epoch 669: val_loss did not improve from 17.40077
196/196 - 55s - loss: 17.5224 - MinusLogProbMetric: 17.5224 - val_loss: 17.6119 - val_MinusLogProbMetric: 17.6119 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 670/1000
2023-10-11 15:07:45.126 
Epoch 670/1000 
	 loss: 17.4519, MinusLogProbMetric: 17.4519, val_loss: 17.6142, val_MinusLogProbMetric: 17.6142

Epoch 670: val_loss did not improve from 17.40077
196/196 - 54s - loss: 17.4519 - MinusLogProbMetric: 17.4519 - val_loss: 17.6142 - val_MinusLogProbMetric: 17.6142 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 671/1000
2023-10-11 15:08:38.289 
Epoch 671/1000 
	 loss: 17.4839, MinusLogProbMetric: 17.4839, val_loss: 18.0452, val_MinusLogProbMetric: 18.0452

Epoch 671: val_loss did not improve from 17.40077
196/196 - 53s - loss: 17.4839 - MinusLogProbMetric: 17.4839 - val_loss: 18.0452 - val_MinusLogProbMetric: 18.0452 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 672/1000
2023-10-11 15:09:32.181 
Epoch 672/1000 
	 loss: 17.4741, MinusLogProbMetric: 17.4741, val_loss: 17.6158, val_MinusLogProbMetric: 17.6158

Epoch 672: val_loss did not improve from 17.40077
196/196 - 54s - loss: 17.4741 - MinusLogProbMetric: 17.4741 - val_loss: 17.6158 - val_MinusLogProbMetric: 17.6158 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 673/1000
2023-10-11 15:10:26.873 
Epoch 673/1000 
	 loss: 17.4553, MinusLogProbMetric: 17.4553, val_loss: 17.4399, val_MinusLogProbMetric: 17.4399

Epoch 673: val_loss did not improve from 17.40077
196/196 - 55s - loss: 17.4553 - MinusLogProbMetric: 17.4553 - val_loss: 17.4399 - val_MinusLogProbMetric: 17.4399 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 674/1000
2023-10-11 15:11:19.707 
Epoch 674/1000 
	 loss: 17.5773, MinusLogProbMetric: 17.5773, val_loss: 17.9490, val_MinusLogProbMetric: 17.9490

Epoch 674: val_loss did not improve from 17.40077
196/196 - 53s - loss: 17.5773 - MinusLogProbMetric: 17.5773 - val_loss: 17.9490 - val_MinusLogProbMetric: 17.9490 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 675/1000
2023-10-11 15:12:13.781 
Epoch 675/1000 
	 loss: 17.4701, MinusLogProbMetric: 17.4701, val_loss: 17.5238, val_MinusLogProbMetric: 17.5238

Epoch 675: val_loss did not improve from 17.40077
196/196 - 54s - loss: 17.4701 - MinusLogProbMetric: 17.4701 - val_loss: 17.5238 - val_MinusLogProbMetric: 17.5238 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 676/1000
2023-10-11 15:13:08.336 
Epoch 676/1000 
	 loss: 17.5947, MinusLogProbMetric: 17.5947, val_loss: 17.7497, val_MinusLogProbMetric: 17.7497

Epoch 676: val_loss did not improve from 17.40077
196/196 - 55s - loss: 17.5947 - MinusLogProbMetric: 17.5947 - val_loss: 17.7497 - val_MinusLogProbMetric: 17.7497 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 677/1000
2023-10-11 15:14:02.499 
Epoch 677/1000 
	 loss: 17.4189, MinusLogProbMetric: 17.4189, val_loss: 17.6444, val_MinusLogProbMetric: 17.6444

Epoch 677: val_loss did not improve from 17.40077
196/196 - 54s - loss: 17.4189 - MinusLogProbMetric: 17.4189 - val_loss: 17.6444 - val_MinusLogProbMetric: 17.6444 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 678/1000
2023-10-11 15:14:55.639 
Epoch 678/1000 
	 loss: 17.5724, MinusLogProbMetric: 17.5724, val_loss: 17.5510, val_MinusLogProbMetric: 17.5510

Epoch 678: val_loss did not improve from 17.40077
196/196 - 53s - loss: 17.5724 - MinusLogProbMetric: 17.5724 - val_loss: 17.5510 - val_MinusLogProbMetric: 17.5510 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 679/1000
2023-10-11 15:15:50.296 
Epoch 679/1000 
	 loss: 17.5128, MinusLogProbMetric: 17.5128, val_loss: 17.5027, val_MinusLogProbMetric: 17.5027

Epoch 679: val_loss did not improve from 17.40077
196/196 - 55s - loss: 17.5128 - MinusLogProbMetric: 17.5128 - val_loss: 17.5027 - val_MinusLogProbMetric: 17.5027 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 680/1000
2023-10-11 15:16:42.962 
Epoch 680/1000 
	 loss: 17.4532, MinusLogProbMetric: 17.4532, val_loss: 17.7639, val_MinusLogProbMetric: 17.7639

Epoch 680: val_loss did not improve from 17.40077
196/196 - 53s - loss: 17.4532 - MinusLogProbMetric: 17.4532 - val_loss: 17.7639 - val_MinusLogProbMetric: 17.7639 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 681/1000
2023-10-11 15:17:36.082 
Epoch 681/1000 
	 loss: 17.5732, MinusLogProbMetric: 17.5732, val_loss: 17.5889, val_MinusLogProbMetric: 17.5889

Epoch 681: val_loss did not improve from 17.40077
196/196 - 53s - loss: 17.5732 - MinusLogProbMetric: 17.5732 - val_loss: 17.5889 - val_MinusLogProbMetric: 17.5889 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 682/1000
2023-10-11 15:18:29.235 
Epoch 682/1000 
	 loss: 17.4364, MinusLogProbMetric: 17.4364, val_loss: 17.6122, val_MinusLogProbMetric: 17.6122

Epoch 682: val_loss did not improve from 17.40077
196/196 - 53s - loss: 17.4364 - MinusLogProbMetric: 17.4364 - val_loss: 17.6122 - val_MinusLogProbMetric: 17.6122 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 683/1000
2023-10-11 15:19:23.440 
Epoch 683/1000 
	 loss: 17.5072, MinusLogProbMetric: 17.5072, val_loss: 17.5377, val_MinusLogProbMetric: 17.5377

Epoch 683: val_loss did not improve from 17.40077
196/196 - 54s - loss: 17.5072 - MinusLogProbMetric: 17.5072 - val_loss: 17.5377 - val_MinusLogProbMetric: 17.5377 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 684/1000
2023-10-11 15:20:17.656 
Epoch 684/1000 
	 loss: 17.4589, MinusLogProbMetric: 17.4589, val_loss: 17.5636, val_MinusLogProbMetric: 17.5636

Epoch 684: val_loss did not improve from 17.40077
196/196 - 54s - loss: 17.4589 - MinusLogProbMetric: 17.4589 - val_loss: 17.5636 - val_MinusLogProbMetric: 17.5636 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 685/1000
2023-10-11 15:21:11.648 
Epoch 685/1000 
	 loss: 17.5422, MinusLogProbMetric: 17.5422, val_loss: 18.0771, val_MinusLogProbMetric: 18.0771

Epoch 685: val_loss did not improve from 17.40077
196/196 - 54s - loss: 17.5422 - MinusLogProbMetric: 17.5422 - val_loss: 18.0771 - val_MinusLogProbMetric: 18.0771 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 686/1000
2023-10-11 15:22:06.124 
Epoch 686/1000 
	 loss: 17.4510, MinusLogProbMetric: 17.4510, val_loss: 17.3698, val_MinusLogProbMetric: 17.3698

Epoch 686: val_loss improved from 17.40077 to 17.36978, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 55s - loss: 17.4510 - MinusLogProbMetric: 17.4510 - val_loss: 17.3698 - val_MinusLogProbMetric: 17.3698 - lr: 1.1111e-04 - 55s/epoch - 283ms/step
Epoch 687/1000
2023-10-11 15:23:00.731 
Epoch 687/1000 
	 loss: 17.4410, MinusLogProbMetric: 17.4410, val_loss: 17.4377, val_MinusLogProbMetric: 17.4377

Epoch 687: val_loss did not improve from 17.36978
196/196 - 54s - loss: 17.4410 - MinusLogProbMetric: 17.4410 - val_loss: 17.4377 - val_MinusLogProbMetric: 17.4377 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 688/1000
2023-10-11 15:23:54.740 
Epoch 688/1000 
	 loss: 17.4706, MinusLogProbMetric: 17.4706, val_loss: 17.4909, val_MinusLogProbMetric: 17.4909

Epoch 688: val_loss did not improve from 17.36978
196/196 - 54s - loss: 17.4706 - MinusLogProbMetric: 17.4706 - val_loss: 17.4909 - val_MinusLogProbMetric: 17.4909 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 689/1000
2023-10-11 15:24:49.741 
Epoch 689/1000 
	 loss: 17.4758, MinusLogProbMetric: 17.4758, val_loss: 17.4695, val_MinusLogProbMetric: 17.4695

Epoch 689: val_loss did not improve from 17.36978
196/196 - 55s - loss: 17.4758 - MinusLogProbMetric: 17.4758 - val_loss: 17.4695 - val_MinusLogProbMetric: 17.4695 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 690/1000
2023-10-11 15:25:44.591 
Epoch 690/1000 
	 loss: 17.4717, MinusLogProbMetric: 17.4717, val_loss: 17.6718, val_MinusLogProbMetric: 17.6718

Epoch 690: val_loss did not improve from 17.36978
196/196 - 55s - loss: 17.4717 - MinusLogProbMetric: 17.4717 - val_loss: 17.6718 - val_MinusLogProbMetric: 17.6718 - lr: 1.1111e-04 - 55s/epoch - 280ms/step
Epoch 691/1000
2023-10-11 15:26:39.116 
Epoch 691/1000 
	 loss: 17.5080, MinusLogProbMetric: 17.5080, val_loss: 17.4435, val_MinusLogProbMetric: 17.4435

Epoch 691: val_loss did not improve from 17.36978
196/196 - 55s - loss: 17.5080 - MinusLogProbMetric: 17.5080 - val_loss: 17.4435 - val_MinusLogProbMetric: 17.4435 - lr: 1.1111e-04 - 55s/epoch - 278ms/step
Epoch 692/1000
2023-10-11 15:27:32.271 
Epoch 692/1000 
	 loss: 17.4180, MinusLogProbMetric: 17.4180, val_loss: 17.8353, val_MinusLogProbMetric: 17.8353

Epoch 692: val_loss did not improve from 17.36978
196/196 - 53s - loss: 17.4180 - MinusLogProbMetric: 17.4180 - val_loss: 17.8353 - val_MinusLogProbMetric: 17.8353 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 693/1000
2023-10-11 15:28:25.041 
Epoch 693/1000 
	 loss: 17.4425, MinusLogProbMetric: 17.4425, val_loss: 17.6382, val_MinusLogProbMetric: 17.6382

Epoch 693: val_loss did not improve from 17.36978
196/196 - 53s - loss: 17.4425 - MinusLogProbMetric: 17.4425 - val_loss: 17.6382 - val_MinusLogProbMetric: 17.6382 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 694/1000
2023-10-11 15:29:18.785 
Epoch 694/1000 
	 loss: 17.5142, MinusLogProbMetric: 17.5142, val_loss: 17.6336, val_MinusLogProbMetric: 17.6336

Epoch 694: val_loss did not improve from 17.36978
196/196 - 54s - loss: 17.5142 - MinusLogProbMetric: 17.5142 - val_loss: 17.6336 - val_MinusLogProbMetric: 17.6336 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 695/1000
2023-10-11 15:30:12.526 
Epoch 695/1000 
	 loss: 17.5104, MinusLogProbMetric: 17.5104, val_loss: 17.5700, val_MinusLogProbMetric: 17.5700

Epoch 695: val_loss did not improve from 17.36978
196/196 - 54s - loss: 17.5104 - MinusLogProbMetric: 17.5104 - val_loss: 17.5700 - val_MinusLogProbMetric: 17.5700 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 696/1000
2023-10-11 15:31:06.356 
Epoch 696/1000 
	 loss: 17.4967, MinusLogProbMetric: 17.4967, val_loss: 17.7285, val_MinusLogProbMetric: 17.7285

Epoch 696: val_loss did not improve from 17.36978
196/196 - 54s - loss: 17.4967 - MinusLogProbMetric: 17.4967 - val_loss: 17.7285 - val_MinusLogProbMetric: 17.7285 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 697/1000
2023-10-11 15:32:02.143 
Epoch 697/1000 
	 loss: 17.5334, MinusLogProbMetric: 17.5334, val_loss: 17.6711, val_MinusLogProbMetric: 17.6711

Epoch 697: val_loss did not improve from 17.36978
196/196 - 56s - loss: 17.5334 - MinusLogProbMetric: 17.5334 - val_loss: 17.6711 - val_MinusLogProbMetric: 17.6711 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 698/1000
2023-10-11 15:32:55.173 
Epoch 698/1000 
	 loss: 17.4397, MinusLogProbMetric: 17.4397, val_loss: 17.5656, val_MinusLogProbMetric: 17.5656

Epoch 698: val_loss did not improve from 17.36978
196/196 - 53s - loss: 17.4397 - MinusLogProbMetric: 17.4397 - val_loss: 17.5656 - val_MinusLogProbMetric: 17.5656 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 699/1000
2023-10-11 15:33:49.552 
Epoch 699/1000 
	 loss: 17.4553, MinusLogProbMetric: 17.4553, val_loss: 17.3709, val_MinusLogProbMetric: 17.3709

Epoch 699: val_loss did not improve from 17.36978
196/196 - 54s - loss: 17.4553 - MinusLogProbMetric: 17.4553 - val_loss: 17.3709 - val_MinusLogProbMetric: 17.3709 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 700/1000
2023-10-11 15:34:43.247 
Epoch 700/1000 
	 loss: 17.4192, MinusLogProbMetric: 17.4192, val_loss: 17.4454, val_MinusLogProbMetric: 17.4454

Epoch 700: val_loss did not improve from 17.36978
196/196 - 54s - loss: 17.4192 - MinusLogProbMetric: 17.4192 - val_loss: 17.4454 - val_MinusLogProbMetric: 17.4454 - lr: 1.1111e-04 - 54s/epoch - 274ms/step
Epoch 701/1000
2023-10-11 15:35:35.716 
Epoch 701/1000 
	 loss: 17.5358, MinusLogProbMetric: 17.5358, val_loss: 18.6516, val_MinusLogProbMetric: 18.6516

Epoch 701: val_loss did not improve from 17.36978
196/196 - 52s - loss: 17.5358 - MinusLogProbMetric: 17.5358 - val_loss: 18.6516 - val_MinusLogProbMetric: 18.6516 - lr: 1.1111e-04 - 52s/epoch - 268ms/step
Epoch 702/1000
2023-10-11 15:36:27.914 
Epoch 702/1000 
	 loss: 17.5197, MinusLogProbMetric: 17.5197, val_loss: 17.4677, val_MinusLogProbMetric: 17.4677

Epoch 702: val_loss did not improve from 17.36978
196/196 - 52s - loss: 17.5197 - MinusLogProbMetric: 17.5197 - val_loss: 17.4677 - val_MinusLogProbMetric: 17.4677 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 703/1000
2023-10-11 15:37:23.134 
Epoch 703/1000 
	 loss: 17.5207, MinusLogProbMetric: 17.5207, val_loss: 18.4480, val_MinusLogProbMetric: 18.4480

Epoch 703: val_loss did not improve from 17.36978
196/196 - 55s - loss: 17.5207 - MinusLogProbMetric: 17.5207 - val_loss: 18.4480 - val_MinusLogProbMetric: 18.4480 - lr: 1.1111e-04 - 55s/epoch - 282ms/step
Epoch 704/1000
2023-10-11 15:38:17.569 
Epoch 704/1000 
	 loss: 17.4803, MinusLogProbMetric: 17.4803, val_loss: 17.4607, val_MinusLogProbMetric: 17.4607

Epoch 704: val_loss did not improve from 17.36978
196/196 - 54s - loss: 17.4803 - MinusLogProbMetric: 17.4803 - val_loss: 17.4607 - val_MinusLogProbMetric: 17.4607 - lr: 1.1111e-04 - 54s/epoch - 278ms/step
Epoch 705/1000
2023-10-11 15:39:10.648 
Epoch 705/1000 
	 loss: 17.4017, MinusLogProbMetric: 17.4017, val_loss: 17.5906, val_MinusLogProbMetric: 17.5906

Epoch 705: val_loss did not improve from 17.36978
196/196 - 53s - loss: 17.4017 - MinusLogProbMetric: 17.4017 - val_loss: 17.5906 - val_MinusLogProbMetric: 17.5906 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 706/1000
2023-10-11 15:40:04.205 
Epoch 706/1000 
	 loss: 17.5832, MinusLogProbMetric: 17.5832, val_loss: 17.4579, val_MinusLogProbMetric: 17.4579

Epoch 706: val_loss did not improve from 17.36978
196/196 - 54s - loss: 17.5832 - MinusLogProbMetric: 17.5832 - val_loss: 17.4579 - val_MinusLogProbMetric: 17.4579 - lr: 1.1111e-04 - 54s/epoch - 273ms/step
Epoch 707/1000
2023-10-11 15:40:58.156 
Epoch 707/1000 
	 loss: 17.4797, MinusLogProbMetric: 17.4797, val_loss: 17.4478, val_MinusLogProbMetric: 17.4478

Epoch 707: val_loss did not improve from 17.36978
196/196 - 54s - loss: 17.4797 - MinusLogProbMetric: 17.4797 - val_loss: 17.4478 - val_MinusLogProbMetric: 17.4478 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 708/1000
2023-10-11 15:41:52.292 
Epoch 708/1000 
	 loss: 17.5186, MinusLogProbMetric: 17.5186, val_loss: 17.6371, val_MinusLogProbMetric: 17.6371

Epoch 708: val_loss did not improve from 17.36978
196/196 - 54s - loss: 17.5186 - MinusLogProbMetric: 17.5186 - val_loss: 17.6371 - val_MinusLogProbMetric: 17.6371 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 709/1000
2023-10-11 15:42:46.916 
Epoch 709/1000 
	 loss: 17.4919, MinusLogProbMetric: 17.4919, val_loss: 17.3848, val_MinusLogProbMetric: 17.3848

Epoch 709: val_loss did not improve from 17.36978
196/196 - 55s - loss: 17.4919 - MinusLogProbMetric: 17.4919 - val_loss: 17.3848 - val_MinusLogProbMetric: 17.3848 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 710/1000
2023-10-11 15:43:40.205 
Epoch 710/1000 
	 loss: 17.4857, MinusLogProbMetric: 17.4857, val_loss: 17.5504, val_MinusLogProbMetric: 17.5504

Epoch 710: val_loss did not improve from 17.36978
196/196 - 53s - loss: 17.4857 - MinusLogProbMetric: 17.4857 - val_loss: 17.5504 - val_MinusLogProbMetric: 17.5504 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 711/1000
2023-10-11 15:44:34.239 
Epoch 711/1000 
	 loss: 17.3782, MinusLogProbMetric: 17.3782, val_loss: 18.3506, val_MinusLogProbMetric: 18.3506

Epoch 711: val_loss did not improve from 17.36978
196/196 - 54s - loss: 17.3782 - MinusLogProbMetric: 17.3782 - val_loss: 18.3506 - val_MinusLogProbMetric: 18.3506 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 712/1000
2023-10-11 15:45:27.804 
Epoch 712/1000 
	 loss: 17.5759, MinusLogProbMetric: 17.5759, val_loss: 17.3165, val_MinusLogProbMetric: 17.3165

Epoch 712: val_loss improved from 17.36978 to 17.31649, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 54s - loss: 17.5759 - MinusLogProbMetric: 17.5759 - val_loss: 17.3165 - val_MinusLogProbMetric: 17.3165 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 713/1000
2023-10-11 15:46:21.603 
Epoch 713/1000 
	 loss: 17.4154, MinusLogProbMetric: 17.4154, val_loss: 17.3566, val_MinusLogProbMetric: 17.3566

Epoch 713: val_loss did not improve from 17.31649
196/196 - 53s - loss: 17.4154 - MinusLogProbMetric: 17.4154 - val_loss: 17.3566 - val_MinusLogProbMetric: 17.3566 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 714/1000
2023-10-11 15:47:15.011 
Epoch 714/1000 
	 loss: 17.4333, MinusLogProbMetric: 17.4333, val_loss: 18.3357, val_MinusLogProbMetric: 18.3357

Epoch 714: val_loss did not improve from 17.31649
196/196 - 53s - loss: 17.4333 - MinusLogProbMetric: 17.4333 - val_loss: 18.3357 - val_MinusLogProbMetric: 18.3357 - lr: 1.1111e-04 - 53s/epoch - 273ms/step
Epoch 715/1000
2023-10-11 15:48:07.943 
Epoch 715/1000 
	 loss: 17.4598, MinusLogProbMetric: 17.4598, val_loss: 17.6500, val_MinusLogProbMetric: 17.6500

Epoch 715: val_loss did not improve from 17.31649
196/196 - 53s - loss: 17.4598 - MinusLogProbMetric: 17.4598 - val_loss: 17.6500 - val_MinusLogProbMetric: 17.6500 - lr: 1.1111e-04 - 53s/epoch - 270ms/step
Epoch 716/1000
2023-10-11 15:49:01.076 
Epoch 716/1000 
	 loss: 17.5997, MinusLogProbMetric: 17.5997, val_loss: 17.4029, val_MinusLogProbMetric: 17.4029

Epoch 716: val_loss did not improve from 17.31649
196/196 - 53s - loss: 17.5997 - MinusLogProbMetric: 17.5997 - val_loss: 17.4029 - val_MinusLogProbMetric: 17.4029 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 717/1000
2023-10-11 15:49:53.750 
Epoch 717/1000 
	 loss: 17.3889, MinusLogProbMetric: 17.3889, val_loss: 17.8496, val_MinusLogProbMetric: 17.8496

Epoch 717: val_loss did not improve from 17.31649
196/196 - 53s - loss: 17.3889 - MinusLogProbMetric: 17.3889 - val_loss: 17.8496 - val_MinusLogProbMetric: 17.8496 - lr: 1.1111e-04 - 53s/epoch - 269ms/step
Epoch 718/1000
2023-10-11 15:50:48.132 
Epoch 718/1000 
	 loss: 17.4546, MinusLogProbMetric: 17.4546, val_loss: 17.5441, val_MinusLogProbMetric: 17.5441

Epoch 718: val_loss did not improve from 17.31649
196/196 - 54s - loss: 17.4546 - MinusLogProbMetric: 17.4546 - val_loss: 17.5441 - val_MinusLogProbMetric: 17.5441 - lr: 1.1111e-04 - 54s/epoch - 277ms/step
Epoch 719/1000
2023-10-11 15:51:42.814 
Epoch 719/1000 
	 loss: 17.4547, MinusLogProbMetric: 17.4547, val_loss: 17.5210, val_MinusLogProbMetric: 17.5210

Epoch 719: val_loss did not improve from 17.31649
196/196 - 55s - loss: 17.4547 - MinusLogProbMetric: 17.4547 - val_loss: 17.5210 - val_MinusLogProbMetric: 17.5210 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 720/1000
2023-10-11 15:52:36.177 
Epoch 720/1000 
	 loss: 17.4897, MinusLogProbMetric: 17.4897, val_loss: 17.3676, val_MinusLogProbMetric: 17.3676

Epoch 720: val_loss did not improve from 17.31649
196/196 - 53s - loss: 17.4897 - MinusLogProbMetric: 17.4897 - val_loss: 17.3676 - val_MinusLogProbMetric: 17.3676 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 721/1000
2023-10-11 15:53:32.055 
Epoch 721/1000 
	 loss: 17.5019, MinusLogProbMetric: 17.5019, val_loss: 17.5963, val_MinusLogProbMetric: 17.5963

Epoch 721: val_loss did not improve from 17.31649
196/196 - 56s - loss: 17.5019 - MinusLogProbMetric: 17.5019 - val_loss: 17.5963 - val_MinusLogProbMetric: 17.5963 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 722/1000
2023-10-11 15:54:22.434 
Epoch 722/1000 
	 loss: 17.3850, MinusLogProbMetric: 17.3850, val_loss: 17.4517, val_MinusLogProbMetric: 17.4517

Epoch 722: val_loss did not improve from 17.31649
196/196 - 50s - loss: 17.3850 - MinusLogProbMetric: 17.3850 - val_loss: 17.4517 - val_MinusLogProbMetric: 17.4517 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 723/1000
2023-10-11 15:55:14.857 
Epoch 723/1000 
	 loss: 17.4012, MinusLogProbMetric: 17.4012, val_loss: 17.3308, val_MinusLogProbMetric: 17.3308

Epoch 723: val_loss did not improve from 17.31649
196/196 - 52s - loss: 17.4012 - MinusLogProbMetric: 17.4012 - val_loss: 17.3308 - val_MinusLogProbMetric: 17.3308 - lr: 1.1111e-04 - 52s/epoch - 267ms/step
Epoch 724/1000
2023-10-11 15:56:05.651 
Epoch 724/1000 
	 loss: 17.3904, MinusLogProbMetric: 17.3904, val_loss: 17.3922, val_MinusLogProbMetric: 17.3922

Epoch 724: val_loss did not improve from 17.31649
196/196 - 51s - loss: 17.3904 - MinusLogProbMetric: 17.3904 - val_loss: 17.3922 - val_MinusLogProbMetric: 17.3922 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 725/1000
2023-10-11 15:56:57.625 
Epoch 725/1000 
	 loss: 17.4282, MinusLogProbMetric: 17.4282, val_loss: 17.4237, val_MinusLogProbMetric: 17.4237

Epoch 725: val_loss did not improve from 17.31649
196/196 - 52s - loss: 17.4282 - MinusLogProbMetric: 17.4282 - val_loss: 17.4237 - val_MinusLogProbMetric: 17.4237 - lr: 1.1111e-04 - 52s/epoch - 265ms/step
Epoch 726/1000
2023-10-11 15:57:49.390 
Epoch 726/1000 
	 loss: 17.4618, MinusLogProbMetric: 17.4618, val_loss: 17.3941, val_MinusLogProbMetric: 17.3941

Epoch 726: val_loss did not improve from 17.31649
196/196 - 52s - loss: 17.4618 - MinusLogProbMetric: 17.4618 - val_loss: 17.3941 - val_MinusLogProbMetric: 17.3941 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 727/1000
2023-10-11 15:58:39.209 
Epoch 727/1000 
	 loss: 17.3804, MinusLogProbMetric: 17.3804, val_loss: 17.3957, val_MinusLogProbMetric: 17.3957

Epoch 727: val_loss did not improve from 17.31649
196/196 - 50s - loss: 17.3804 - MinusLogProbMetric: 17.3804 - val_loss: 17.3957 - val_MinusLogProbMetric: 17.3957 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 728/1000
2023-10-11 15:59:29.508 
Epoch 728/1000 
	 loss: 17.4719, MinusLogProbMetric: 17.4719, val_loss: 17.4380, val_MinusLogProbMetric: 17.4380

Epoch 728: val_loss did not improve from 17.31649
196/196 - 50s - loss: 17.4719 - MinusLogProbMetric: 17.4719 - val_loss: 17.4380 - val_MinusLogProbMetric: 17.4380 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 729/1000
2023-10-11 16:00:18.181 
Epoch 729/1000 
	 loss: 17.5245, MinusLogProbMetric: 17.5245, val_loss: 17.5178, val_MinusLogProbMetric: 17.5178

Epoch 729: val_loss did not improve from 17.31649
196/196 - 49s - loss: 17.5245 - MinusLogProbMetric: 17.5245 - val_loss: 17.5178 - val_MinusLogProbMetric: 17.5178 - lr: 1.1111e-04 - 49s/epoch - 248ms/step
Epoch 730/1000
2023-10-11 16:01:06.847 
Epoch 730/1000 
	 loss: 17.3799, MinusLogProbMetric: 17.3799, val_loss: 17.4425, val_MinusLogProbMetric: 17.4425

Epoch 730: val_loss did not improve from 17.31649
196/196 - 49s - loss: 17.3799 - MinusLogProbMetric: 17.3799 - val_loss: 17.4425 - val_MinusLogProbMetric: 17.4425 - lr: 1.1111e-04 - 49s/epoch - 248ms/step
Epoch 731/1000
2023-10-11 16:01:56.543 
Epoch 731/1000 
	 loss: 17.4996, MinusLogProbMetric: 17.4996, val_loss: 17.6922, val_MinusLogProbMetric: 17.6922

Epoch 731: val_loss did not improve from 17.31649
196/196 - 50s - loss: 17.4996 - MinusLogProbMetric: 17.4996 - val_loss: 17.6922 - val_MinusLogProbMetric: 17.6922 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 732/1000
2023-10-11 16:02:45.993 
Epoch 732/1000 
	 loss: 17.4230, MinusLogProbMetric: 17.4230, val_loss: 17.4166, val_MinusLogProbMetric: 17.4166

Epoch 732: val_loss did not improve from 17.31649
196/196 - 49s - loss: 17.4230 - MinusLogProbMetric: 17.4230 - val_loss: 17.4166 - val_MinusLogProbMetric: 17.4166 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 733/1000
2023-10-11 16:03:36.158 
Epoch 733/1000 
	 loss: 17.3876, MinusLogProbMetric: 17.3876, val_loss: 18.0072, val_MinusLogProbMetric: 18.0072

Epoch 733: val_loss did not improve from 17.31649
196/196 - 50s - loss: 17.3876 - MinusLogProbMetric: 17.3876 - val_loss: 18.0072 - val_MinusLogProbMetric: 18.0072 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 734/1000
2023-10-11 16:04:30.098 
Epoch 734/1000 
	 loss: 17.4719, MinusLogProbMetric: 17.4719, val_loss: 17.8342, val_MinusLogProbMetric: 17.8342

Epoch 734: val_loss did not improve from 17.31649
196/196 - 54s - loss: 17.4719 - MinusLogProbMetric: 17.4719 - val_loss: 17.8342 - val_MinusLogProbMetric: 17.8342 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 735/1000
2023-10-11 16:05:19.122 
Epoch 735/1000 
	 loss: 17.3819, MinusLogProbMetric: 17.3819, val_loss: 17.4158, val_MinusLogProbMetric: 17.4158

Epoch 735: val_loss did not improve from 17.31649
196/196 - 49s - loss: 17.3819 - MinusLogProbMetric: 17.3819 - val_loss: 17.4158 - val_MinusLogProbMetric: 17.4158 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 736/1000
2023-10-11 16:06:07.334 
Epoch 736/1000 
	 loss: 17.4229, MinusLogProbMetric: 17.4229, val_loss: 17.3337, val_MinusLogProbMetric: 17.3337

Epoch 736: val_loss did not improve from 17.31649
196/196 - 48s - loss: 17.4229 - MinusLogProbMetric: 17.4229 - val_loss: 17.3337 - val_MinusLogProbMetric: 17.3337 - lr: 1.1111e-04 - 48s/epoch - 246ms/step
Epoch 737/1000
2023-10-11 16:06:58.679 
Epoch 737/1000 
	 loss: 17.4786, MinusLogProbMetric: 17.4786, val_loss: 17.4622, val_MinusLogProbMetric: 17.4622

Epoch 737: val_loss did not improve from 17.31649
196/196 - 51s - loss: 17.4786 - MinusLogProbMetric: 17.4786 - val_loss: 17.4622 - val_MinusLogProbMetric: 17.4622 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 738/1000
2023-10-11 16:07:51.862 
Epoch 738/1000 
	 loss: 17.3729, MinusLogProbMetric: 17.3729, val_loss: 17.4606, val_MinusLogProbMetric: 17.4606

Epoch 738: val_loss did not improve from 17.31649
196/196 - 53s - loss: 17.3729 - MinusLogProbMetric: 17.3729 - val_loss: 17.4606 - val_MinusLogProbMetric: 17.4606 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 739/1000
2023-10-11 16:08:42.633 
Epoch 739/1000 
	 loss: 17.5306, MinusLogProbMetric: 17.5306, val_loss: 17.4605, val_MinusLogProbMetric: 17.4605

Epoch 739: val_loss did not improve from 17.31649
196/196 - 51s - loss: 17.5306 - MinusLogProbMetric: 17.5306 - val_loss: 17.4605 - val_MinusLogProbMetric: 17.4605 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 740/1000
2023-10-11 16:09:34.769 
Epoch 740/1000 
	 loss: 17.3656, MinusLogProbMetric: 17.3656, val_loss: 17.8789, val_MinusLogProbMetric: 17.8789

Epoch 740: val_loss did not improve from 17.31649
196/196 - 52s - loss: 17.3656 - MinusLogProbMetric: 17.3656 - val_loss: 17.8789 - val_MinusLogProbMetric: 17.8789 - lr: 1.1111e-04 - 52s/epoch - 266ms/step
Epoch 741/1000
2023-10-11 16:10:24.034 
Epoch 741/1000 
	 loss: 17.4918, MinusLogProbMetric: 17.4918, val_loss: 17.6751, val_MinusLogProbMetric: 17.6751

Epoch 741: val_loss did not improve from 17.31649
196/196 - 49s - loss: 17.4918 - MinusLogProbMetric: 17.4918 - val_loss: 17.6751 - val_MinusLogProbMetric: 17.6751 - lr: 1.1111e-04 - 49s/epoch - 251ms/step
Epoch 742/1000
2023-10-11 16:11:13.437 
Epoch 742/1000 
	 loss: 17.4200, MinusLogProbMetric: 17.4200, val_loss: 17.4730, val_MinusLogProbMetric: 17.4730

Epoch 742: val_loss did not improve from 17.31649
196/196 - 49s - loss: 17.4200 - MinusLogProbMetric: 17.4200 - val_loss: 17.4730 - val_MinusLogProbMetric: 17.4730 - lr: 1.1111e-04 - 49s/epoch - 252ms/step
Epoch 743/1000
2023-10-11 16:12:04.709 
Epoch 743/1000 
	 loss: 17.4884, MinusLogProbMetric: 17.4884, val_loss: 17.5479, val_MinusLogProbMetric: 17.5479

Epoch 743: val_loss did not improve from 17.31649
196/196 - 51s - loss: 17.4884 - MinusLogProbMetric: 17.4884 - val_loss: 17.5479 - val_MinusLogProbMetric: 17.5479 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 744/1000
2023-10-11 16:12:57.255 
Epoch 744/1000 
	 loss: 17.3900, MinusLogProbMetric: 17.3900, val_loss: 17.9648, val_MinusLogProbMetric: 17.9648

Epoch 744: val_loss did not improve from 17.31649
196/196 - 53s - loss: 17.3900 - MinusLogProbMetric: 17.3900 - val_loss: 17.9648 - val_MinusLogProbMetric: 17.9648 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 745/1000
2023-10-11 16:13:47.958 
Epoch 745/1000 
	 loss: 17.5343, MinusLogProbMetric: 17.5343, val_loss: 18.2454, val_MinusLogProbMetric: 18.2454

Epoch 745: val_loss did not improve from 17.31649
196/196 - 51s - loss: 17.5343 - MinusLogProbMetric: 17.5343 - val_loss: 18.2454 - val_MinusLogProbMetric: 18.2454 - lr: 1.1111e-04 - 51s/epoch - 259ms/step
Epoch 746/1000
2023-10-11 16:14:41.286 
Epoch 746/1000 
	 loss: 17.3658, MinusLogProbMetric: 17.3658, val_loss: 17.8889, val_MinusLogProbMetric: 17.8889

Epoch 746: val_loss did not improve from 17.31649
196/196 - 53s - loss: 17.3658 - MinusLogProbMetric: 17.3658 - val_loss: 17.8889 - val_MinusLogProbMetric: 17.8889 - lr: 1.1111e-04 - 53s/epoch - 272ms/step
Epoch 747/1000
2023-10-11 16:15:33.796 
Epoch 747/1000 
	 loss: 17.4431, MinusLogProbMetric: 17.4431, val_loss: 17.3394, val_MinusLogProbMetric: 17.3394

Epoch 747: val_loss did not improve from 17.31649
196/196 - 53s - loss: 17.4431 - MinusLogProbMetric: 17.4431 - val_loss: 17.3394 - val_MinusLogProbMetric: 17.3394 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 748/1000
2023-10-11 16:16:23.496 
Epoch 748/1000 
	 loss: 17.4195, MinusLogProbMetric: 17.4195, val_loss: 17.4403, val_MinusLogProbMetric: 17.4403

Epoch 748: val_loss did not improve from 17.31649
196/196 - 50s - loss: 17.4195 - MinusLogProbMetric: 17.4195 - val_loss: 17.4403 - val_MinusLogProbMetric: 17.4403 - lr: 1.1111e-04 - 50s/epoch - 254ms/step
Epoch 749/1000
2023-10-11 16:17:17.377 
Epoch 749/1000 
	 loss: 17.4812, MinusLogProbMetric: 17.4812, val_loss: 17.4438, val_MinusLogProbMetric: 17.4438

Epoch 749: val_loss did not improve from 17.31649
196/196 - 54s - loss: 17.4812 - MinusLogProbMetric: 17.4812 - val_loss: 17.4438 - val_MinusLogProbMetric: 17.4438 - lr: 1.1111e-04 - 54s/epoch - 275ms/step
Epoch 750/1000
2023-10-11 16:18:12.479 
Epoch 750/1000 
	 loss: 17.4460, MinusLogProbMetric: 17.4460, val_loss: 17.4421, val_MinusLogProbMetric: 17.4421

Epoch 750: val_loss did not improve from 17.31649
196/196 - 55s - loss: 17.4460 - MinusLogProbMetric: 17.4460 - val_loss: 17.4421 - val_MinusLogProbMetric: 17.4421 - lr: 1.1111e-04 - 55s/epoch - 281ms/step
Epoch 751/1000
2023-10-11 16:19:08.021 
Epoch 751/1000 
	 loss: 17.3708, MinusLogProbMetric: 17.3708, val_loss: 17.6005, val_MinusLogProbMetric: 17.6005

Epoch 751: val_loss did not improve from 17.31649
196/196 - 56s - loss: 17.3708 - MinusLogProbMetric: 17.3708 - val_loss: 17.6005 - val_MinusLogProbMetric: 17.6005 - lr: 1.1111e-04 - 56s/epoch - 283ms/step
Epoch 752/1000
2023-10-11 16:20:03.920 
Epoch 752/1000 
	 loss: 17.3784, MinusLogProbMetric: 17.3784, val_loss: 17.3853, val_MinusLogProbMetric: 17.3853

Epoch 752: val_loss did not improve from 17.31649
196/196 - 56s - loss: 17.3784 - MinusLogProbMetric: 17.3784 - val_loss: 17.3853 - val_MinusLogProbMetric: 17.3853 - lr: 1.1111e-04 - 56s/epoch - 285ms/step
Epoch 753/1000
2023-10-11 16:20:58.597 
Epoch 753/1000 
	 loss: 17.4553, MinusLogProbMetric: 17.4553, val_loss: 17.3763, val_MinusLogProbMetric: 17.3763

Epoch 753: val_loss did not improve from 17.31649
196/196 - 55s - loss: 17.4553 - MinusLogProbMetric: 17.4553 - val_loss: 17.3763 - val_MinusLogProbMetric: 17.3763 - lr: 1.1111e-04 - 55s/epoch - 279ms/step
Epoch 754/1000
2023-10-11 16:21:50.091 
Epoch 754/1000 
	 loss: 17.4064, MinusLogProbMetric: 17.4064, val_loss: 17.7070, val_MinusLogProbMetric: 17.7070

Epoch 754: val_loss did not improve from 17.31649
196/196 - 51s - loss: 17.4064 - MinusLogProbMetric: 17.4064 - val_loss: 17.7070 - val_MinusLogProbMetric: 17.7070 - lr: 1.1111e-04 - 51s/epoch - 263ms/step
Epoch 755/1000
2023-10-11 16:22:40.233 
Epoch 755/1000 
	 loss: 17.5472, MinusLogProbMetric: 17.5472, val_loss: 17.5033, val_MinusLogProbMetric: 17.5033

Epoch 755: val_loss did not improve from 17.31649
196/196 - 50s - loss: 17.5472 - MinusLogProbMetric: 17.5472 - val_loss: 17.5033 - val_MinusLogProbMetric: 17.5033 - lr: 1.1111e-04 - 50s/epoch - 256ms/step
Epoch 756/1000
2023-10-11 16:23:30.579 
Epoch 756/1000 
	 loss: 17.3755, MinusLogProbMetric: 17.3755, val_loss: 17.7441, val_MinusLogProbMetric: 17.7441

Epoch 756: val_loss did not improve from 17.31649
196/196 - 50s - loss: 17.3755 - MinusLogProbMetric: 17.3755 - val_loss: 17.7441 - val_MinusLogProbMetric: 17.7441 - lr: 1.1111e-04 - 50s/epoch - 257ms/step
Epoch 757/1000
2023-10-11 16:24:23.076 
Epoch 757/1000 
	 loss: 17.4216, MinusLogProbMetric: 17.4216, val_loss: 17.4128, val_MinusLogProbMetric: 17.4128

Epoch 757: val_loss did not improve from 17.31649
196/196 - 52s - loss: 17.4216 - MinusLogProbMetric: 17.4216 - val_loss: 17.4128 - val_MinusLogProbMetric: 17.4128 - lr: 1.1111e-04 - 52s/epoch - 268ms/step
Epoch 758/1000
2023-10-11 16:25:12.143 
Epoch 758/1000 
	 loss: 17.3985, MinusLogProbMetric: 17.3985, val_loss: 17.6009, val_MinusLogProbMetric: 17.6009

Epoch 758: val_loss did not improve from 17.31649
196/196 - 49s - loss: 17.3985 - MinusLogProbMetric: 17.3985 - val_loss: 17.6009 - val_MinusLogProbMetric: 17.6009 - lr: 1.1111e-04 - 49s/epoch - 250ms/step
Epoch 759/1000
2023-10-11 16:26:03.493 
Epoch 759/1000 
	 loss: 17.4367, MinusLogProbMetric: 17.4367, val_loss: 17.6197, val_MinusLogProbMetric: 17.6197

Epoch 759: val_loss did not improve from 17.31649
196/196 - 51s - loss: 17.4367 - MinusLogProbMetric: 17.4367 - val_loss: 17.6197 - val_MinusLogProbMetric: 17.6197 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 760/1000
2023-10-11 16:26:54.828 
Epoch 760/1000 
	 loss: 17.4272, MinusLogProbMetric: 17.4272, val_loss: 17.5484, val_MinusLogProbMetric: 17.5484

Epoch 760: val_loss did not improve from 17.31649
196/196 - 51s - loss: 17.4272 - MinusLogProbMetric: 17.4272 - val_loss: 17.5484 - val_MinusLogProbMetric: 17.5484 - lr: 1.1111e-04 - 51s/epoch - 262ms/step
Epoch 761/1000
2023-10-11 16:27:46.664 
Epoch 761/1000 
	 loss: 17.4022, MinusLogProbMetric: 17.4022, val_loss: 17.6552, val_MinusLogProbMetric: 17.6552

Epoch 761: val_loss did not improve from 17.31649
196/196 - 52s - loss: 17.4022 - MinusLogProbMetric: 17.4022 - val_loss: 17.6552 - val_MinusLogProbMetric: 17.6552 - lr: 1.1111e-04 - 52s/epoch - 264ms/step
Epoch 762/1000
2023-10-11 16:28:34.645 
Epoch 762/1000 
	 loss: 17.3628, MinusLogProbMetric: 17.3628, val_loss: 17.4529, val_MinusLogProbMetric: 17.4529

Epoch 762: val_loss did not improve from 17.31649
196/196 - 48s - loss: 17.3628 - MinusLogProbMetric: 17.3628 - val_loss: 17.4529 - val_MinusLogProbMetric: 17.4529 - lr: 1.1111e-04 - 48s/epoch - 245ms/step
Epoch 763/1000
2023-10-11 16:29:24.904 
Epoch 763/1000 
	 loss: 17.1629, MinusLogProbMetric: 17.1629, val_loss: 17.2730, val_MinusLogProbMetric: 17.2730

Epoch 763: val_loss improved from 17.31649 to 17.27299, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 51s - loss: 17.1629 - MinusLogProbMetric: 17.1629 - val_loss: 17.2730 - val_MinusLogProbMetric: 17.2730 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 764/1000
2023-10-11 16:30:14.683 
Epoch 764/1000 
	 loss: 17.1558, MinusLogProbMetric: 17.1558, val_loss: 17.2484, val_MinusLogProbMetric: 17.2484

Epoch 764: val_loss improved from 17.27299 to 17.24838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 50s - loss: 17.1558 - MinusLogProbMetric: 17.1558 - val_loss: 17.2484 - val_MinusLogProbMetric: 17.2484 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 765/1000
2023-10-11 16:31:03.868 
Epoch 765/1000 
	 loss: 17.1670, MinusLogProbMetric: 17.1670, val_loss: 17.3780, val_MinusLogProbMetric: 17.3780

Epoch 765: val_loss did not improve from 17.24838
196/196 - 48s - loss: 17.1670 - MinusLogProbMetric: 17.1670 - val_loss: 17.3780 - val_MinusLogProbMetric: 17.3780 - lr: 5.5556e-05 - 48s/epoch - 247ms/step
Epoch 766/1000
2023-10-11 16:31:51.785 
Epoch 766/1000 
	 loss: 17.1757, MinusLogProbMetric: 17.1757, val_loss: 17.2906, val_MinusLogProbMetric: 17.2906

Epoch 766: val_loss did not improve from 17.24838
196/196 - 48s - loss: 17.1757 - MinusLogProbMetric: 17.1757 - val_loss: 17.2906 - val_MinusLogProbMetric: 17.2906 - lr: 5.5556e-05 - 48s/epoch - 244ms/step
Epoch 767/1000
2023-10-11 16:32:40.230 
Epoch 767/1000 
	 loss: 17.1645, MinusLogProbMetric: 17.1645, val_loss: 17.2702, val_MinusLogProbMetric: 17.2702

Epoch 767: val_loss did not improve from 17.24838
196/196 - 48s - loss: 17.1645 - MinusLogProbMetric: 17.1645 - val_loss: 17.2702 - val_MinusLogProbMetric: 17.2702 - lr: 5.5556e-05 - 48s/epoch - 247ms/step
Epoch 768/1000
2023-10-11 16:33:29.416 
Epoch 768/1000 
	 loss: 17.1399, MinusLogProbMetric: 17.1399, val_loss: 17.2513, val_MinusLogProbMetric: 17.2513

Epoch 768: val_loss did not improve from 17.24838
196/196 - 49s - loss: 17.1399 - MinusLogProbMetric: 17.1399 - val_loss: 17.2513 - val_MinusLogProbMetric: 17.2513 - lr: 5.5556e-05 - 49s/epoch - 251ms/step
Epoch 769/1000
2023-10-11 16:34:20.161 
Epoch 769/1000 
	 loss: 17.1747, MinusLogProbMetric: 17.1747, val_loss: 17.2730, val_MinusLogProbMetric: 17.2730

Epoch 769: val_loss did not improve from 17.24838
196/196 - 51s - loss: 17.1747 - MinusLogProbMetric: 17.1747 - val_loss: 17.2730 - val_MinusLogProbMetric: 17.2730 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 770/1000
2023-10-11 16:35:13.125 
Epoch 770/1000 
	 loss: 17.1793, MinusLogProbMetric: 17.1793, val_loss: 17.3225, val_MinusLogProbMetric: 17.3225

Epoch 770: val_loss did not improve from 17.24838
196/196 - 53s - loss: 17.1793 - MinusLogProbMetric: 17.1793 - val_loss: 17.3225 - val_MinusLogProbMetric: 17.3225 - lr: 5.5556e-05 - 53s/epoch - 270ms/step
Epoch 771/1000
2023-10-11 16:36:03.754 
Epoch 771/1000 
	 loss: 17.1586, MinusLogProbMetric: 17.1586, val_loss: 17.2031, val_MinusLogProbMetric: 17.2031

Epoch 771: val_loss improved from 17.24838 to 17.20313, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 51s - loss: 17.1586 - MinusLogProbMetric: 17.1586 - val_loss: 17.2031 - val_MinusLogProbMetric: 17.2031 - lr: 5.5556e-05 - 51s/epoch - 262ms/step
Epoch 772/1000
2023-10-11 16:36:55.529 
Epoch 772/1000 
	 loss: 17.1540, MinusLogProbMetric: 17.1540, val_loss: 17.5639, val_MinusLogProbMetric: 17.5639

Epoch 772: val_loss did not improve from 17.20313
196/196 - 51s - loss: 17.1540 - MinusLogProbMetric: 17.1540 - val_loss: 17.5639 - val_MinusLogProbMetric: 17.5639 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 773/1000
2023-10-11 16:37:47.333 
Epoch 773/1000 
	 loss: 17.1644, MinusLogProbMetric: 17.1644, val_loss: 17.2311, val_MinusLogProbMetric: 17.2311

Epoch 773: val_loss did not improve from 17.20313
196/196 - 52s - loss: 17.1644 - MinusLogProbMetric: 17.1644 - val_loss: 17.2311 - val_MinusLogProbMetric: 17.2311 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 774/1000
2023-10-11 16:38:36.753 
Epoch 774/1000 
	 loss: 17.1542, MinusLogProbMetric: 17.1542, val_loss: 17.3392, val_MinusLogProbMetric: 17.3392

Epoch 774: val_loss did not improve from 17.20313
196/196 - 49s - loss: 17.1542 - MinusLogProbMetric: 17.1542 - val_loss: 17.3392 - val_MinusLogProbMetric: 17.3392 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 775/1000
2023-10-11 16:39:26.852 
Epoch 775/1000 
	 loss: 17.1455, MinusLogProbMetric: 17.1455, val_loss: 17.2217, val_MinusLogProbMetric: 17.2217

Epoch 775: val_loss did not improve from 17.20313
196/196 - 50s - loss: 17.1455 - MinusLogProbMetric: 17.1455 - val_loss: 17.2217 - val_MinusLogProbMetric: 17.2217 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 776/1000
2023-10-11 16:40:17.158 
Epoch 776/1000 
	 loss: 17.1683, MinusLogProbMetric: 17.1683, val_loss: 17.2184, val_MinusLogProbMetric: 17.2184

Epoch 776: val_loss did not improve from 17.20313
196/196 - 50s - loss: 17.1683 - MinusLogProbMetric: 17.1683 - val_loss: 17.2184 - val_MinusLogProbMetric: 17.2184 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 777/1000
2023-10-11 16:41:06.729 
Epoch 777/1000 
	 loss: 17.1731, MinusLogProbMetric: 17.1731, val_loss: 17.2404, val_MinusLogProbMetric: 17.2404

Epoch 777: val_loss did not improve from 17.20313
196/196 - 50s - loss: 17.1731 - MinusLogProbMetric: 17.1731 - val_loss: 17.2404 - val_MinusLogProbMetric: 17.2404 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 778/1000
2023-10-11 16:41:55.284 
Epoch 778/1000 
	 loss: 17.1369, MinusLogProbMetric: 17.1369, val_loss: 17.2661, val_MinusLogProbMetric: 17.2661

Epoch 778: val_loss did not improve from 17.20313
196/196 - 49s - loss: 17.1369 - MinusLogProbMetric: 17.1369 - val_loss: 17.2661 - val_MinusLogProbMetric: 17.2661 - lr: 5.5556e-05 - 49s/epoch - 248ms/step
Epoch 779/1000
2023-10-11 16:42:44.682 
Epoch 779/1000 
	 loss: 17.1644, MinusLogProbMetric: 17.1644, val_loss: 17.5021, val_MinusLogProbMetric: 17.5021

Epoch 779: val_loss did not improve from 17.20313
196/196 - 49s - loss: 17.1644 - MinusLogProbMetric: 17.1644 - val_loss: 17.5021 - val_MinusLogProbMetric: 17.5021 - lr: 5.5556e-05 - 49s/epoch - 252ms/step
Epoch 780/1000
2023-10-11 16:43:38.149 
Epoch 780/1000 
	 loss: 17.1678, MinusLogProbMetric: 17.1678, val_loss: 17.2146, val_MinusLogProbMetric: 17.2146

Epoch 780: val_loss did not improve from 17.20313
196/196 - 53s - loss: 17.1678 - MinusLogProbMetric: 17.1678 - val_loss: 17.2146 - val_MinusLogProbMetric: 17.2146 - lr: 5.5556e-05 - 53s/epoch - 273ms/step
Epoch 781/1000
2023-10-11 16:44:33.750 
Epoch 781/1000 
	 loss: 17.1583, MinusLogProbMetric: 17.1583, val_loss: 17.2431, val_MinusLogProbMetric: 17.2431

Epoch 781: val_loss did not improve from 17.20313
196/196 - 56s - loss: 17.1583 - MinusLogProbMetric: 17.1583 - val_loss: 17.2431 - val_MinusLogProbMetric: 17.2431 - lr: 5.5556e-05 - 56s/epoch - 284ms/step
Epoch 782/1000
2023-10-11 16:45:28.651 
Epoch 782/1000 
	 loss: 17.1605, MinusLogProbMetric: 17.1605, val_loss: 17.2362, val_MinusLogProbMetric: 17.2362

Epoch 782: val_loss did not improve from 17.20313
196/196 - 55s - loss: 17.1605 - MinusLogProbMetric: 17.1605 - val_loss: 17.2362 - val_MinusLogProbMetric: 17.2362 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 783/1000
2023-10-11 16:46:24.055 
Epoch 783/1000 
	 loss: 17.1541, MinusLogProbMetric: 17.1541, val_loss: 17.3482, val_MinusLogProbMetric: 17.3482

Epoch 783: val_loss did not improve from 17.20313
196/196 - 55s - loss: 17.1541 - MinusLogProbMetric: 17.1541 - val_loss: 17.3482 - val_MinusLogProbMetric: 17.3482 - lr: 5.5556e-05 - 55s/epoch - 283ms/step
Epoch 784/1000
2023-10-11 16:47:19.385 
Epoch 784/1000 
	 loss: 17.1563, MinusLogProbMetric: 17.1563, val_loss: 17.1888, val_MinusLogProbMetric: 17.1888

Epoch 784: val_loss improved from 17.20313 to 17.18878, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 56s - loss: 17.1563 - MinusLogProbMetric: 17.1563 - val_loss: 17.1888 - val_MinusLogProbMetric: 17.1888 - lr: 5.5556e-05 - 56s/epoch - 286ms/step
Epoch 785/1000
2023-10-11 16:48:14.492 
Epoch 785/1000 
	 loss: 17.1667, MinusLogProbMetric: 17.1667, val_loss: 17.1896, val_MinusLogProbMetric: 17.1896

Epoch 785: val_loss did not improve from 17.18878
196/196 - 54s - loss: 17.1667 - MinusLogProbMetric: 17.1667 - val_loss: 17.1896 - val_MinusLogProbMetric: 17.1896 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 786/1000
2023-10-11 16:49:07.445 
Epoch 786/1000 
	 loss: 17.1485, MinusLogProbMetric: 17.1485, val_loss: 17.2705, val_MinusLogProbMetric: 17.2705

Epoch 786: val_loss did not improve from 17.18878
196/196 - 53s - loss: 17.1485 - MinusLogProbMetric: 17.1485 - val_loss: 17.2705 - val_MinusLogProbMetric: 17.2705 - lr: 5.5556e-05 - 53s/epoch - 270ms/step
Epoch 787/1000
2023-10-11 16:49:56.279 
Epoch 787/1000 
	 loss: 17.1573, MinusLogProbMetric: 17.1573, val_loss: 17.2001, val_MinusLogProbMetric: 17.2001

Epoch 787: val_loss did not improve from 17.18878
196/196 - 49s - loss: 17.1573 - MinusLogProbMetric: 17.1573 - val_loss: 17.2001 - val_MinusLogProbMetric: 17.2001 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 788/1000
2023-10-11 16:50:46.020 
Epoch 788/1000 
	 loss: 17.1384, MinusLogProbMetric: 17.1384, val_loss: 17.2211, val_MinusLogProbMetric: 17.2211

Epoch 788: val_loss did not improve from 17.18878
196/196 - 50s - loss: 17.1384 - MinusLogProbMetric: 17.1384 - val_loss: 17.2211 - val_MinusLogProbMetric: 17.2211 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 789/1000
2023-10-11 16:51:36.116 
Epoch 789/1000 
	 loss: 17.1657, MinusLogProbMetric: 17.1657, val_loss: 17.2702, val_MinusLogProbMetric: 17.2702

Epoch 789: val_loss did not improve from 17.18878
196/196 - 50s - loss: 17.1657 - MinusLogProbMetric: 17.1657 - val_loss: 17.2702 - val_MinusLogProbMetric: 17.2702 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 790/1000
2023-10-11 16:52:25.631 
Epoch 790/1000 
	 loss: 17.1810, MinusLogProbMetric: 17.1810, val_loss: 17.2362, val_MinusLogProbMetric: 17.2362

Epoch 790: val_loss did not improve from 17.18878
196/196 - 50s - loss: 17.1810 - MinusLogProbMetric: 17.1810 - val_loss: 17.2362 - val_MinusLogProbMetric: 17.2362 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 791/1000
2023-10-11 16:53:13.513 
Epoch 791/1000 
	 loss: 17.1343, MinusLogProbMetric: 17.1343, val_loss: 17.3804, val_MinusLogProbMetric: 17.3804

Epoch 791: val_loss did not improve from 17.18878
196/196 - 48s - loss: 17.1343 - MinusLogProbMetric: 17.1343 - val_loss: 17.3804 - val_MinusLogProbMetric: 17.3804 - lr: 5.5556e-05 - 48s/epoch - 244ms/step
Epoch 792/1000
2023-10-11 16:54:03.063 
Epoch 792/1000 
	 loss: 17.1810, MinusLogProbMetric: 17.1810, val_loss: 17.3100, val_MinusLogProbMetric: 17.3100

Epoch 792: val_loss did not improve from 17.18878
196/196 - 50s - loss: 17.1810 - MinusLogProbMetric: 17.1810 - val_loss: 17.3100 - val_MinusLogProbMetric: 17.3100 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 793/1000
2023-10-11 16:54:51.365 
Epoch 793/1000 
	 loss: 17.1596, MinusLogProbMetric: 17.1596, val_loss: 17.3329, val_MinusLogProbMetric: 17.3329

Epoch 793: val_loss did not improve from 17.18878
196/196 - 48s - loss: 17.1596 - MinusLogProbMetric: 17.1596 - val_loss: 17.3329 - val_MinusLogProbMetric: 17.3329 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 794/1000
2023-10-11 16:55:40.980 
Epoch 794/1000 
	 loss: 17.1517, MinusLogProbMetric: 17.1517, val_loss: 17.2026, val_MinusLogProbMetric: 17.2026

Epoch 794: val_loss did not improve from 17.18878
196/196 - 50s - loss: 17.1517 - MinusLogProbMetric: 17.1517 - val_loss: 17.2026 - val_MinusLogProbMetric: 17.2026 - lr: 5.5556e-05 - 50s/epoch - 253ms/step
Epoch 795/1000
2023-10-11 16:56:31.777 
Epoch 795/1000 
	 loss: 17.1685, MinusLogProbMetric: 17.1685, val_loss: 17.3114, val_MinusLogProbMetric: 17.3114

Epoch 795: val_loss did not improve from 17.18878
196/196 - 51s - loss: 17.1685 - MinusLogProbMetric: 17.1685 - val_loss: 17.3114 - val_MinusLogProbMetric: 17.3114 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 796/1000
2023-10-11 16:57:21.895 
Epoch 796/1000 
	 loss: 17.2069, MinusLogProbMetric: 17.2069, val_loss: 17.2939, val_MinusLogProbMetric: 17.2939

Epoch 796: val_loss did not improve from 17.18878
196/196 - 50s - loss: 17.2069 - MinusLogProbMetric: 17.2069 - val_loss: 17.2939 - val_MinusLogProbMetric: 17.2939 - lr: 5.5556e-05 - 50s/epoch - 256ms/step
Epoch 797/1000
2023-10-11 16:58:11.185 
Epoch 797/1000 
	 loss: 17.1551, MinusLogProbMetric: 17.1551, val_loss: 17.1832, val_MinusLogProbMetric: 17.1832

Epoch 797: val_loss improved from 17.18878 to 17.18321, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 50s - loss: 17.1551 - MinusLogProbMetric: 17.1551 - val_loss: 17.1832 - val_MinusLogProbMetric: 17.1832 - lr: 5.5556e-05 - 50s/epoch - 255ms/step
Epoch 798/1000
2023-10-11 16:59:00.771 
Epoch 798/1000 
	 loss: 17.1427, MinusLogProbMetric: 17.1427, val_loss: 17.2538, val_MinusLogProbMetric: 17.2538

Epoch 798: val_loss did not improve from 17.18321
196/196 - 49s - loss: 17.1427 - MinusLogProbMetric: 17.1427 - val_loss: 17.2538 - val_MinusLogProbMetric: 17.2538 - lr: 5.5556e-05 - 49s/epoch - 249ms/step
Epoch 799/1000
2023-10-11 16:59:48.928 
Epoch 799/1000 
	 loss: 17.1773, MinusLogProbMetric: 17.1773, val_loss: 17.3440, val_MinusLogProbMetric: 17.3440

Epoch 799: val_loss did not improve from 17.18321
196/196 - 48s - loss: 17.1773 - MinusLogProbMetric: 17.1773 - val_loss: 17.3440 - val_MinusLogProbMetric: 17.3440 - lr: 5.5556e-05 - 48s/epoch - 246ms/step
Epoch 800/1000
2023-10-11 17:00:41.679 
Epoch 800/1000 
	 loss: 17.1675, MinusLogProbMetric: 17.1675, val_loss: 17.2027, val_MinusLogProbMetric: 17.2027

Epoch 800: val_loss did not improve from 17.18321
196/196 - 53s - loss: 17.1675 - MinusLogProbMetric: 17.1675 - val_loss: 17.2027 - val_MinusLogProbMetric: 17.2027 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 801/1000
2023-10-11 17:01:32.564 
Epoch 801/1000 
	 loss: 17.1502, MinusLogProbMetric: 17.1502, val_loss: 17.2883, val_MinusLogProbMetric: 17.2883

Epoch 801: val_loss did not improve from 17.18321
196/196 - 51s - loss: 17.1502 - MinusLogProbMetric: 17.1502 - val_loss: 17.2883 - val_MinusLogProbMetric: 17.2883 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 802/1000
2023-10-11 17:02:22.436 
Epoch 802/1000 
	 loss: 17.1389, MinusLogProbMetric: 17.1389, val_loss: 17.2028, val_MinusLogProbMetric: 17.2028

Epoch 802: val_loss did not improve from 17.18321
196/196 - 50s - loss: 17.1389 - MinusLogProbMetric: 17.1389 - val_loss: 17.2028 - val_MinusLogProbMetric: 17.2028 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 803/1000
2023-10-11 17:03:14.493 
Epoch 803/1000 
	 loss: 17.1401, MinusLogProbMetric: 17.1401, val_loss: 17.2930, val_MinusLogProbMetric: 17.2930

Epoch 803: val_loss did not improve from 17.18321
196/196 - 52s - loss: 17.1401 - MinusLogProbMetric: 17.1401 - val_loss: 17.2930 - val_MinusLogProbMetric: 17.2930 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 804/1000
2023-10-11 17:04:05.494 
Epoch 804/1000 
	 loss: 17.1628, MinusLogProbMetric: 17.1628, val_loss: 17.3876, val_MinusLogProbMetric: 17.3876

Epoch 804: val_loss did not improve from 17.18321
196/196 - 51s - loss: 17.1628 - MinusLogProbMetric: 17.1628 - val_loss: 17.3876 - val_MinusLogProbMetric: 17.3876 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 805/1000
2023-10-11 17:04:56.109 
Epoch 805/1000 
	 loss: 17.1282, MinusLogProbMetric: 17.1282, val_loss: 17.2909, val_MinusLogProbMetric: 17.2909

Epoch 805: val_loss did not improve from 17.18321
196/196 - 51s - loss: 17.1282 - MinusLogProbMetric: 17.1282 - val_loss: 17.2909 - val_MinusLogProbMetric: 17.2909 - lr: 5.5556e-05 - 51s/epoch - 258ms/step
Epoch 806/1000
2023-10-11 17:05:48.700 
Epoch 806/1000 
	 loss: 17.2435, MinusLogProbMetric: 17.2435, val_loss: 17.9130, val_MinusLogProbMetric: 17.9130

Epoch 806: val_loss did not improve from 17.18321
196/196 - 53s - loss: 17.2435 - MinusLogProbMetric: 17.2435 - val_loss: 17.9130 - val_MinusLogProbMetric: 17.9130 - lr: 5.5556e-05 - 53s/epoch - 268ms/step
Epoch 807/1000
2023-10-11 17:06:37.754 
Epoch 807/1000 
	 loss: 17.1663, MinusLogProbMetric: 17.1663, val_loss: 17.2223, val_MinusLogProbMetric: 17.2223

Epoch 807: val_loss did not improve from 17.18321
196/196 - 49s - loss: 17.1663 - MinusLogProbMetric: 17.1663 - val_loss: 17.2223 - val_MinusLogProbMetric: 17.2223 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 808/1000
2023-10-11 17:07:26.838 
Epoch 808/1000 
	 loss: 17.1395, MinusLogProbMetric: 17.1395, val_loss: 17.2222, val_MinusLogProbMetric: 17.2222

Epoch 808: val_loss did not improve from 17.18321
196/196 - 49s - loss: 17.1395 - MinusLogProbMetric: 17.1395 - val_loss: 17.2222 - val_MinusLogProbMetric: 17.2222 - lr: 5.5556e-05 - 49s/epoch - 250ms/step
Epoch 809/1000
2023-10-11 17:08:17.298 
Epoch 809/1000 
	 loss: 17.1753, MinusLogProbMetric: 17.1753, val_loss: 17.2339, val_MinusLogProbMetric: 17.2339

Epoch 809: val_loss did not improve from 17.18321
196/196 - 50s - loss: 17.1753 - MinusLogProbMetric: 17.1753 - val_loss: 17.2339 - val_MinusLogProbMetric: 17.2339 - lr: 5.5556e-05 - 50s/epoch - 257ms/step
Epoch 810/1000
2023-10-11 17:09:08.308 
Epoch 810/1000 
	 loss: 17.1525, MinusLogProbMetric: 17.1525, val_loss: 17.2613, val_MinusLogProbMetric: 17.2613

Epoch 810: val_loss did not improve from 17.18321
196/196 - 51s - loss: 17.1525 - MinusLogProbMetric: 17.1525 - val_loss: 17.2613 - val_MinusLogProbMetric: 17.2613 - lr: 5.5556e-05 - 51s/epoch - 260ms/step
Epoch 811/1000
2023-10-11 17:10:02.860 
Epoch 811/1000 
	 loss: 17.2030, MinusLogProbMetric: 17.2030, val_loss: 17.1961, val_MinusLogProbMetric: 17.1961

Epoch 811: val_loss did not improve from 17.18321
196/196 - 55s - loss: 17.2030 - MinusLogProbMetric: 17.2030 - val_loss: 17.1961 - val_MinusLogProbMetric: 17.1961 - lr: 5.5556e-05 - 55s/epoch - 278ms/step
Epoch 812/1000
2023-10-11 17:10:56.536 
Epoch 812/1000 
	 loss: 17.1638, MinusLogProbMetric: 17.1638, val_loss: 17.2174, val_MinusLogProbMetric: 17.2174

Epoch 812: val_loss did not improve from 17.18321
196/196 - 54s - loss: 17.1638 - MinusLogProbMetric: 17.1638 - val_loss: 17.2174 - val_MinusLogProbMetric: 17.2174 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 813/1000
2023-10-11 17:11:51.050 
Epoch 813/1000 
	 loss: 17.1817, MinusLogProbMetric: 17.1817, val_loss: 17.2231, val_MinusLogProbMetric: 17.2231

Epoch 813: val_loss did not improve from 17.18321
196/196 - 55s - loss: 17.1817 - MinusLogProbMetric: 17.1817 - val_loss: 17.2231 - val_MinusLogProbMetric: 17.2231 - lr: 5.5556e-05 - 55s/epoch - 278ms/step
Epoch 814/1000
2023-10-11 17:12:45.642 
Epoch 814/1000 
	 loss: 17.1716, MinusLogProbMetric: 17.1716, val_loss: 17.2738, val_MinusLogProbMetric: 17.2738

Epoch 814: val_loss did not improve from 17.18321
196/196 - 55s - loss: 17.1716 - MinusLogProbMetric: 17.1716 - val_loss: 17.2738 - val_MinusLogProbMetric: 17.2738 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 815/1000
2023-10-11 17:13:39.935 
Epoch 815/1000 
	 loss: 17.1693, MinusLogProbMetric: 17.1693, val_loss: 17.2736, val_MinusLogProbMetric: 17.2736

Epoch 815: val_loss did not improve from 17.18321
196/196 - 54s - loss: 17.1693 - MinusLogProbMetric: 17.1693 - val_loss: 17.2736 - val_MinusLogProbMetric: 17.2736 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 816/1000
2023-10-11 17:14:34.754 
Epoch 816/1000 
	 loss: 17.1681, MinusLogProbMetric: 17.1681, val_loss: 17.1778, val_MinusLogProbMetric: 17.1778

Epoch 816: val_loss improved from 17.18321 to 17.17777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 56s - loss: 17.1681 - MinusLogProbMetric: 17.1681 - val_loss: 17.1778 - val_MinusLogProbMetric: 17.1778 - lr: 5.5556e-05 - 56s/epoch - 284ms/step
Epoch 817/1000
2023-10-11 17:15:30.221 
Epoch 817/1000 
	 loss: 17.1681, MinusLogProbMetric: 17.1681, val_loss: 17.2744, val_MinusLogProbMetric: 17.2744

Epoch 817: val_loss did not improve from 17.17777
196/196 - 55s - loss: 17.1681 - MinusLogProbMetric: 17.1681 - val_loss: 17.2744 - val_MinusLogProbMetric: 17.2744 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 818/1000
2023-10-11 17:16:25.655 
Epoch 818/1000 
	 loss: 17.1508, MinusLogProbMetric: 17.1508, val_loss: 17.2209, val_MinusLogProbMetric: 17.2209

Epoch 818: val_loss did not improve from 17.17777
196/196 - 55s - loss: 17.1508 - MinusLogProbMetric: 17.1508 - val_loss: 17.2209 - val_MinusLogProbMetric: 17.2209 - lr: 5.5556e-05 - 55s/epoch - 283ms/step
Epoch 819/1000
2023-10-11 17:17:19.229 
Epoch 819/1000 
	 loss: 17.1299, MinusLogProbMetric: 17.1299, val_loss: 17.1741, val_MinusLogProbMetric: 17.1741

Epoch 819: val_loss improved from 17.17777 to 17.17411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 54s - loss: 17.1299 - MinusLogProbMetric: 17.1299 - val_loss: 17.1741 - val_MinusLogProbMetric: 17.1741 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 820/1000
2023-10-11 17:18:14.779 
Epoch 820/1000 
	 loss: 17.1490, MinusLogProbMetric: 17.1490, val_loss: 17.1700, val_MinusLogProbMetric: 17.1700

Epoch 820: val_loss improved from 17.17411 to 17.17004, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 56s - loss: 17.1490 - MinusLogProbMetric: 17.1490 - val_loss: 17.1700 - val_MinusLogProbMetric: 17.1700 - lr: 5.5556e-05 - 56s/epoch - 283ms/step
Epoch 821/1000
2023-10-11 17:19:09.454 
Epoch 821/1000 
	 loss: 17.1358, MinusLogProbMetric: 17.1358, val_loss: 17.2030, val_MinusLogProbMetric: 17.2030

Epoch 821: val_loss did not improve from 17.17004
196/196 - 54s - loss: 17.1358 - MinusLogProbMetric: 17.1358 - val_loss: 17.2030 - val_MinusLogProbMetric: 17.2030 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 822/1000
2023-10-11 17:20:03.911 
Epoch 822/1000 
	 loss: 17.1503, MinusLogProbMetric: 17.1503, val_loss: 17.2235, val_MinusLogProbMetric: 17.2235

Epoch 822: val_loss did not improve from 17.17004
196/196 - 54s - loss: 17.1503 - MinusLogProbMetric: 17.1503 - val_loss: 17.2235 - val_MinusLogProbMetric: 17.2235 - lr: 5.5556e-05 - 54s/epoch - 278ms/step
Epoch 823/1000
2023-10-11 17:20:57.527 
Epoch 823/1000 
	 loss: 17.1525, MinusLogProbMetric: 17.1525, val_loss: 17.2811, val_MinusLogProbMetric: 17.2811

Epoch 823: val_loss did not improve from 17.17004
196/196 - 54s - loss: 17.1525 - MinusLogProbMetric: 17.1525 - val_loss: 17.2811 - val_MinusLogProbMetric: 17.2811 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 824/1000
2023-10-11 17:21:50.055 
Epoch 824/1000 
	 loss: 17.1494, MinusLogProbMetric: 17.1494, val_loss: 17.1967, val_MinusLogProbMetric: 17.1967

Epoch 824: val_loss did not improve from 17.17004
196/196 - 53s - loss: 17.1494 - MinusLogProbMetric: 17.1494 - val_loss: 17.1967 - val_MinusLogProbMetric: 17.1967 - lr: 5.5556e-05 - 53s/epoch - 268ms/step
Epoch 825/1000
2023-10-11 17:22:39.911 
Epoch 825/1000 
	 loss: 17.1872, MinusLogProbMetric: 17.1872, val_loss: 17.1821, val_MinusLogProbMetric: 17.1821

Epoch 825: val_loss did not improve from 17.17004
196/196 - 50s - loss: 17.1872 - MinusLogProbMetric: 17.1872 - val_loss: 17.1821 - val_MinusLogProbMetric: 17.1821 - lr: 5.5556e-05 - 50s/epoch - 254ms/step
Epoch 826/1000
2023-10-11 17:23:35.152 
Epoch 826/1000 
	 loss: 17.1483, MinusLogProbMetric: 17.1483, val_loss: 17.2348, val_MinusLogProbMetric: 17.2348

Epoch 826: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1483 - MinusLogProbMetric: 17.1483 - val_loss: 17.2348 - val_MinusLogProbMetric: 17.2348 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 827/1000
2023-10-11 17:24:29.739 
Epoch 827/1000 
	 loss: 17.1672, MinusLogProbMetric: 17.1672, val_loss: 17.3481, val_MinusLogProbMetric: 17.3481

Epoch 827: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1672 - MinusLogProbMetric: 17.1672 - val_loss: 17.3481 - val_MinusLogProbMetric: 17.3481 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 828/1000
2023-10-11 17:25:23.393 
Epoch 828/1000 
	 loss: 17.1993, MinusLogProbMetric: 17.1993, val_loss: 17.2318, val_MinusLogProbMetric: 17.2318

Epoch 828: val_loss did not improve from 17.17004
196/196 - 54s - loss: 17.1993 - MinusLogProbMetric: 17.1993 - val_loss: 17.2318 - val_MinusLogProbMetric: 17.2318 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 829/1000
2023-10-11 17:26:17.010 
Epoch 829/1000 
	 loss: 17.1360, MinusLogProbMetric: 17.1360, val_loss: 17.3206, val_MinusLogProbMetric: 17.3206

Epoch 829: val_loss did not improve from 17.17004
196/196 - 54s - loss: 17.1360 - MinusLogProbMetric: 17.1360 - val_loss: 17.3206 - val_MinusLogProbMetric: 17.3206 - lr: 5.5556e-05 - 54s/epoch - 273ms/step
Epoch 830/1000
2023-10-11 17:27:11.535 
Epoch 830/1000 
	 loss: 17.1585, MinusLogProbMetric: 17.1585, val_loss: 17.2281, val_MinusLogProbMetric: 17.2281

Epoch 830: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1585 - MinusLogProbMetric: 17.1585 - val_loss: 17.2281 - val_MinusLogProbMetric: 17.2281 - lr: 5.5556e-05 - 55s/epoch - 278ms/step
Epoch 831/1000
2023-10-11 17:28:07.066 
Epoch 831/1000 
	 loss: 17.1430, MinusLogProbMetric: 17.1430, val_loss: 17.1983, val_MinusLogProbMetric: 17.1983

Epoch 831: val_loss did not improve from 17.17004
196/196 - 56s - loss: 17.1430 - MinusLogProbMetric: 17.1430 - val_loss: 17.1983 - val_MinusLogProbMetric: 17.1983 - lr: 5.5556e-05 - 56s/epoch - 283ms/step
Epoch 832/1000
2023-10-11 17:29:03.663 
Epoch 832/1000 
	 loss: 17.1438, MinusLogProbMetric: 17.1438, val_loss: 17.3125, val_MinusLogProbMetric: 17.3125

Epoch 832: val_loss did not improve from 17.17004
196/196 - 57s - loss: 17.1438 - MinusLogProbMetric: 17.1438 - val_loss: 17.3125 - val_MinusLogProbMetric: 17.3125 - lr: 5.5556e-05 - 57s/epoch - 289ms/step
Epoch 833/1000
2023-10-11 17:29:58.719 
Epoch 833/1000 
	 loss: 17.1906, MinusLogProbMetric: 17.1906, val_loss: 17.3435, val_MinusLogProbMetric: 17.3435

Epoch 833: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1906 - MinusLogProbMetric: 17.1906 - val_loss: 17.3435 - val_MinusLogProbMetric: 17.3435 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 834/1000
2023-10-11 17:30:53.557 
Epoch 834/1000 
	 loss: 17.1613, MinusLogProbMetric: 17.1613, val_loss: 17.2596, val_MinusLogProbMetric: 17.2596

Epoch 834: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1613 - MinusLogProbMetric: 17.1613 - val_loss: 17.2596 - val_MinusLogProbMetric: 17.2596 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 835/1000
2023-10-11 17:31:47.720 
Epoch 835/1000 
	 loss: 17.1806, MinusLogProbMetric: 17.1806, val_loss: 17.2996, val_MinusLogProbMetric: 17.2996

Epoch 835: val_loss did not improve from 17.17004
196/196 - 54s - loss: 17.1806 - MinusLogProbMetric: 17.1806 - val_loss: 17.2996 - val_MinusLogProbMetric: 17.2996 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 836/1000
2023-10-11 17:32:41.836 
Epoch 836/1000 
	 loss: 17.1387, MinusLogProbMetric: 17.1387, val_loss: 17.2952, val_MinusLogProbMetric: 17.2952

Epoch 836: val_loss did not improve from 17.17004
196/196 - 54s - loss: 17.1387 - MinusLogProbMetric: 17.1387 - val_loss: 17.2952 - val_MinusLogProbMetric: 17.2952 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 837/1000
2023-10-11 17:33:36.321 
Epoch 837/1000 
	 loss: 17.1692, MinusLogProbMetric: 17.1692, val_loss: 17.2615, val_MinusLogProbMetric: 17.2615

Epoch 837: val_loss did not improve from 17.17004
196/196 - 54s - loss: 17.1692 - MinusLogProbMetric: 17.1692 - val_loss: 17.2615 - val_MinusLogProbMetric: 17.2615 - lr: 5.5556e-05 - 54s/epoch - 278ms/step
Epoch 838/1000
2023-10-11 17:34:32.026 
Epoch 838/1000 
	 loss: 17.1547, MinusLogProbMetric: 17.1547, val_loss: 17.2265, val_MinusLogProbMetric: 17.2265

Epoch 838: val_loss did not improve from 17.17004
196/196 - 56s - loss: 17.1547 - MinusLogProbMetric: 17.1547 - val_loss: 17.2265 - val_MinusLogProbMetric: 17.2265 - lr: 5.5556e-05 - 56s/epoch - 284ms/step
Epoch 839/1000
2023-10-11 17:35:27.116 
Epoch 839/1000 
	 loss: 17.1143, MinusLogProbMetric: 17.1143, val_loss: 17.1732, val_MinusLogProbMetric: 17.1732

Epoch 839: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1143 - MinusLogProbMetric: 17.1143 - val_loss: 17.1732 - val_MinusLogProbMetric: 17.1732 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 840/1000
2023-10-11 17:36:22.605 
Epoch 840/1000 
	 loss: 17.1465, MinusLogProbMetric: 17.1465, val_loss: 17.2598, val_MinusLogProbMetric: 17.2598

Epoch 840: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1465 - MinusLogProbMetric: 17.1465 - val_loss: 17.2598 - val_MinusLogProbMetric: 17.2598 - lr: 5.5556e-05 - 55s/epoch - 283ms/step
Epoch 841/1000
2023-10-11 17:37:17.223 
Epoch 841/1000 
	 loss: 17.2019, MinusLogProbMetric: 17.2019, val_loss: 17.2027, val_MinusLogProbMetric: 17.2027

Epoch 841: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.2019 - MinusLogProbMetric: 17.2019 - val_loss: 17.2027 - val_MinusLogProbMetric: 17.2027 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 842/1000
2023-10-11 17:38:12.056 
Epoch 842/1000 
	 loss: 17.1422, MinusLogProbMetric: 17.1422, val_loss: 17.5323, val_MinusLogProbMetric: 17.5323

Epoch 842: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1422 - MinusLogProbMetric: 17.1422 - val_loss: 17.5323 - val_MinusLogProbMetric: 17.5323 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 843/1000
2023-10-11 17:39:06.953 
Epoch 843/1000 
	 loss: 17.1564, MinusLogProbMetric: 17.1564, val_loss: 17.4230, val_MinusLogProbMetric: 17.4230

Epoch 843: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1564 - MinusLogProbMetric: 17.1564 - val_loss: 17.4230 - val_MinusLogProbMetric: 17.4230 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 844/1000
2023-10-11 17:40:01.991 
Epoch 844/1000 
	 loss: 17.1427, MinusLogProbMetric: 17.1427, val_loss: 17.2953, val_MinusLogProbMetric: 17.2953

Epoch 844: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1427 - MinusLogProbMetric: 17.1427 - val_loss: 17.2953 - val_MinusLogProbMetric: 17.2953 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 845/1000
2023-10-11 17:40:56.225 
Epoch 845/1000 
	 loss: 17.1445, MinusLogProbMetric: 17.1445, val_loss: 17.2162, val_MinusLogProbMetric: 17.2162

Epoch 845: val_loss did not improve from 17.17004
196/196 - 54s - loss: 17.1445 - MinusLogProbMetric: 17.1445 - val_loss: 17.2162 - val_MinusLogProbMetric: 17.2162 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 846/1000
2023-10-11 17:41:51.158 
Epoch 846/1000 
	 loss: 17.1470, MinusLogProbMetric: 17.1470, val_loss: 17.7374, val_MinusLogProbMetric: 17.7374

Epoch 846: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1470 - MinusLogProbMetric: 17.1470 - val_loss: 17.7374 - val_MinusLogProbMetric: 17.7374 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 847/1000
2023-10-11 17:42:45.776 
Epoch 847/1000 
	 loss: 17.1465, MinusLogProbMetric: 17.1465, val_loss: 17.1958, val_MinusLogProbMetric: 17.1958

Epoch 847: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1465 - MinusLogProbMetric: 17.1465 - val_loss: 17.1958 - val_MinusLogProbMetric: 17.1958 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 848/1000
2023-10-11 17:43:41.194 
Epoch 848/1000 
	 loss: 17.1453, MinusLogProbMetric: 17.1453, val_loss: 17.2276, val_MinusLogProbMetric: 17.2276

Epoch 848: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1453 - MinusLogProbMetric: 17.1453 - val_loss: 17.2276 - val_MinusLogProbMetric: 17.2276 - lr: 5.5556e-05 - 55s/epoch - 283ms/step
Epoch 849/1000
2023-10-11 17:44:36.714 
Epoch 849/1000 
	 loss: 17.1656, MinusLogProbMetric: 17.1656, val_loss: 17.3374, val_MinusLogProbMetric: 17.3374

Epoch 849: val_loss did not improve from 17.17004
196/196 - 56s - loss: 17.1656 - MinusLogProbMetric: 17.1656 - val_loss: 17.3374 - val_MinusLogProbMetric: 17.3374 - lr: 5.5556e-05 - 56s/epoch - 283ms/step
Epoch 850/1000
2023-10-11 17:45:31.792 
Epoch 850/1000 
	 loss: 17.1159, MinusLogProbMetric: 17.1159, val_loss: 17.2614, val_MinusLogProbMetric: 17.2614

Epoch 850: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1159 - MinusLogProbMetric: 17.1159 - val_loss: 17.2614 - val_MinusLogProbMetric: 17.2614 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 851/1000
2023-10-11 17:46:27.002 
Epoch 851/1000 
	 loss: 17.1469, MinusLogProbMetric: 17.1469, val_loss: 17.2376, val_MinusLogProbMetric: 17.2376

Epoch 851: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1469 - MinusLogProbMetric: 17.1469 - val_loss: 17.2376 - val_MinusLogProbMetric: 17.2376 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 852/1000
2023-10-11 17:47:22.207 
Epoch 852/1000 
	 loss: 17.1446, MinusLogProbMetric: 17.1446, val_loss: 17.2460, val_MinusLogProbMetric: 17.2460

Epoch 852: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1446 - MinusLogProbMetric: 17.1446 - val_loss: 17.2460 - val_MinusLogProbMetric: 17.2460 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 853/1000
2023-10-11 17:48:18.085 
Epoch 853/1000 
	 loss: 17.1296, MinusLogProbMetric: 17.1296, val_loss: 17.5772, val_MinusLogProbMetric: 17.5772

Epoch 853: val_loss did not improve from 17.17004
196/196 - 56s - loss: 17.1296 - MinusLogProbMetric: 17.1296 - val_loss: 17.5772 - val_MinusLogProbMetric: 17.5772 - lr: 5.5556e-05 - 56s/epoch - 285ms/step
Epoch 854/1000
2023-10-11 17:49:13.619 
Epoch 854/1000 
	 loss: 17.1769, MinusLogProbMetric: 17.1769, val_loss: 17.2214, val_MinusLogProbMetric: 17.2214

Epoch 854: val_loss did not improve from 17.17004
196/196 - 56s - loss: 17.1769 - MinusLogProbMetric: 17.1769 - val_loss: 17.2214 - val_MinusLogProbMetric: 17.2214 - lr: 5.5556e-05 - 56s/epoch - 283ms/step
Epoch 855/1000
2023-10-11 17:50:09.421 
Epoch 855/1000 
	 loss: 17.1474, MinusLogProbMetric: 17.1474, val_loss: 17.4264, val_MinusLogProbMetric: 17.4264

Epoch 855: val_loss did not improve from 17.17004
196/196 - 56s - loss: 17.1474 - MinusLogProbMetric: 17.1474 - val_loss: 17.4264 - val_MinusLogProbMetric: 17.4264 - lr: 5.5556e-05 - 56s/epoch - 285ms/step
Epoch 856/1000
2023-10-11 17:51:05.876 
Epoch 856/1000 
	 loss: 17.1923, MinusLogProbMetric: 17.1923, val_loss: 17.5543, val_MinusLogProbMetric: 17.5543

Epoch 856: val_loss did not improve from 17.17004
196/196 - 56s - loss: 17.1923 - MinusLogProbMetric: 17.1923 - val_loss: 17.5543 - val_MinusLogProbMetric: 17.5543 - lr: 5.5556e-05 - 56s/epoch - 288ms/step
Epoch 857/1000
2023-10-11 17:52:02.803 
Epoch 857/1000 
	 loss: 17.1745, MinusLogProbMetric: 17.1745, val_loss: 17.3302, val_MinusLogProbMetric: 17.3302

Epoch 857: val_loss did not improve from 17.17004
196/196 - 57s - loss: 17.1745 - MinusLogProbMetric: 17.1745 - val_loss: 17.3302 - val_MinusLogProbMetric: 17.3302 - lr: 5.5556e-05 - 57s/epoch - 290ms/step
Epoch 858/1000
2023-10-11 17:52:58.891 
Epoch 858/1000 
	 loss: 17.1286, MinusLogProbMetric: 17.1286, val_loss: 17.2542, val_MinusLogProbMetric: 17.2542

Epoch 858: val_loss did not improve from 17.17004
196/196 - 56s - loss: 17.1286 - MinusLogProbMetric: 17.1286 - val_loss: 17.2542 - val_MinusLogProbMetric: 17.2542 - lr: 5.5556e-05 - 56s/epoch - 286ms/step
Epoch 859/1000
2023-10-11 17:53:54.977 
Epoch 859/1000 
	 loss: 17.1466, MinusLogProbMetric: 17.1466, val_loss: 17.7387, val_MinusLogProbMetric: 17.7387

Epoch 859: val_loss did not improve from 17.17004
196/196 - 56s - loss: 17.1466 - MinusLogProbMetric: 17.1466 - val_loss: 17.7387 - val_MinusLogProbMetric: 17.7387 - lr: 5.5556e-05 - 56s/epoch - 286ms/step
Epoch 860/1000
2023-10-11 17:54:50.776 
Epoch 860/1000 
	 loss: 17.1341, MinusLogProbMetric: 17.1341, val_loss: 17.4075, val_MinusLogProbMetric: 17.4075

Epoch 860: val_loss did not improve from 17.17004
196/196 - 56s - loss: 17.1341 - MinusLogProbMetric: 17.1341 - val_loss: 17.4075 - val_MinusLogProbMetric: 17.4075 - lr: 5.5556e-05 - 56s/epoch - 284ms/step
Epoch 861/1000
2023-10-11 17:55:46.077 
Epoch 861/1000 
	 loss: 17.1472, MinusLogProbMetric: 17.1472, val_loss: 17.1897, val_MinusLogProbMetric: 17.1897

Epoch 861: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1472 - MinusLogProbMetric: 17.1472 - val_loss: 17.1897 - val_MinusLogProbMetric: 17.1897 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 862/1000
2023-10-11 17:56:41.662 
Epoch 862/1000 
	 loss: 17.1413, MinusLogProbMetric: 17.1413, val_loss: 17.2739, val_MinusLogProbMetric: 17.2739

Epoch 862: val_loss did not improve from 17.17004
196/196 - 56s - loss: 17.1413 - MinusLogProbMetric: 17.1413 - val_loss: 17.2739 - val_MinusLogProbMetric: 17.2739 - lr: 5.5556e-05 - 56s/epoch - 284ms/step
Epoch 863/1000
2023-10-11 17:57:36.618 
Epoch 863/1000 
	 loss: 17.1595, MinusLogProbMetric: 17.1595, val_loss: 17.2135, val_MinusLogProbMetric: 17.2135

Epoch 863: val_loss did not improve from 17.17004
196/196 - 55s - loss: 17.1595 - MinusLogProbMetric: 17.1595 - val_loss: 17.2135 - val_MinusLogProbMetric: 17.2135 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 864/1000
2023-10-11 17:58:31.840 
Epoch 864/1000 
	 loss: 17.1305, MinusLogProbMetric: 17.1305, val_loss: 17.1600, val_MinusLogProbMetric: 17.1600

Epoch 864: val_loss improved from 17.17004 to 17.16001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 56s - loss: 17.1305 - MinusLogProbMetric: 17.1305 - val_loss: 17.1600 - val_MinusLogProbMetric: 17.1600 - lr: 5.5556e-05 - 56s/epoch - 286ms/step
Epoch 865/1000
2023-10-11 17:59:28.083 
Epoch 865/1000 
	 loss: 17.1164, MinusLogProbMetric: 17.1164, val_loss: 17.4175, val_MinusLogProbMetric: 17.4175

Epoch 865: val_loss did not improve from 17.16001
196/196 - 55s - loss: 17.1164 - MinusLogProbMetric: 17.1164 - val_loss: 17.4175 - val_MinusLogProbMetric: 17.4175 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 866/1000
2023-10-11 18:00:23.377 
Epoch 866/1000 
	 loss: 17.1473, MinusLogProbMetric: 17.1473, val_loss: 17.4795, val_MinusLogProbMetric: 17.4795

Epoch 866: val_loss did not improve from 17.16001
196/196 - 55s - loss: 17.1473 - MinusLogProbMetric: 17.1473 - val_loss: 17.4795 - val_MinusLogProbMetric: 17.4795 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 867/1000
2023-10-11 18:01:18.268 
Epoch 867/1000 
	 loss: 17.1187, MinusLogProbMetric: 17.1187, val_loss: 17.1717, val_MinusLogProbMetric: 17.1717

Epoch 867: val_loss did not improve from 17.16001
196/196 - 55s - loss: 17.1187 - MinusLogProbMetric: 17.1187 - val_loss: 17.1717 - val_MinusLogProbMetric: 17.1717 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 868/1000
2023-10-11 18:02:11.913 
Epoch 868/1000 
	 loss: 17.1463, MinusLogProbMetric: 17.1463, val_loss: 17.2324, val_MinusLogProbMetric: 17.2324

Epoch 868: val_loss did not improve from 17.16001
196/196 - 54s - loss: 17.1463 - MinusLogProbMetric: 17.1463 - val_loss: 17.2324 - val_MinusLogProbMetric: 17.2324 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 869/1000
2023-10-11 18:03:06.173 
Epoch 869/1000 
	 loss: 17.1665, MinusLogProbMetric: 17.1665, val_loss: 17.2042, val_MinusLogProbMetric: 17.2042

Epoch 869: val_loss did not improve from 17.16001
196/196 - 54s - loss: 17.1665 - MinusLogProbMetric: 17.1665 - val_loss: 17.2042 - val_MinusLogProbMetric: 17.2042 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 870/1000
2023-10-11 18:04:00.804 
Epoch 870/1000 
	 loss: 17.1654, MinusLogProbMetric: 17.1654, val_loss: 17.2110, val_MinusLogProbMetric: 17.2110

Epoch 870: val_loss did not improve from 17.16001
196/196 - 55s - loss: 17.1654 - MinusLogProbMetric: 17.1654 - val_loss: 17.2110 - val_MinusLogProbMetric: 17.2110 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 871/1000
2023-10-11 18:04:56.269 
Epoch 871/1000 
	 loss: 17.1389, MinusLogProbMetric: 17.1389, val_loss: 17.1790, val_MinusLogProbMetric: 17.1790

Epoch 871: val_loss did not improve from 17.16001
196/196 - 55s - loss: 17.1389 - MinusLogProbMetric: 17.1389 - val_loss: 17.1790 - val_MinusLogProbMetric: 17.1790 - lr: 5.5556e-05 - 55s/epoch - 283ms/step
Epoch 872/1000
2023-10-11 18:05:50.355 
Epoch 872/1000 
	 loss: 17.1522, MinusLogProbMetric: 17.1522, val_loss: 17.4241, val_MinusLogProbMetric: 17.4241

Epoch 872: val_loss did not improve from 17.16001
196/196 - 54s - loss: 17.1522 - MinusLogProbMetric: 17.1522 - val_loss: 17.4241 - val_MinusLogProbMetric: 17.4241 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 873/1000
2023-10-11 18:06:44.690 
Epoch 873/1000 
	 loss: 17.1388, MinusLogProbMetric: 17.1388, val_loss: 17.2689, val_MinusLogProbMetric: 17.2689

Epoch 873: val_loss did not improve from 17.16001
196/196 - 54s - loss: 17.1388 - MinusLogProbMetric: 17.1388 - val_loss: 17.2689 - val_MinusLogProbMetric: 17.2689 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 874/1000
2023-10-11 18:07:38.078 
Epoch 874/1000 
	 loss: 17.1255, MinusLogProbMetric: 17.1255, val_loss: 17.2958, val_MinusLogProbMetric: 17.2958

Epoch 874: val_loss did not improve from 17.16001
196/196 - 53s - loss: 17.1255 - MinusLogProbMetric: 17.1255 - val_loss: 17.2958 - val_MinusLogProbMetric: 17.2958 - lr: 5.5556e-05 - 53s/epoch - 272ms/step
Epoch 875/1000
2023-10-11 18:08:33.109 
Epoch 875/1000 
	 loss: 17.1369, MinusLogProbMetric: 17.1369, val_loss: 17.1801, val_MinusLogProbMetric: 17.1801

Epoch 875: val_loss did not improve from 17.16001
196/196 - 55s - loss: 17.1369 - MinusLogProbMetric: 17.1369 - val_loss: 17.1801 - val_MinusLogProbMetric: 17.1801 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 876/1000
2023-10-11 18:09:27.888 
Epoch 876/1000 
	 loss: 17.1617, MinusLogProbMetric: 17.1617, val_loss: 17.2478, val_MinusLogProbMetric: 17.2478

Epoch 876: val_loss did not improve from 17.16001
196/196 - 55s - loss: 17.1617 - MinusLogProbMetric: 17.1617 - val_loss: 17.2478 - val_MinusLogProbMetric: 17.2478 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 877/1000
2023-10-11 18:10:22.256 
Epoch 877/1000 
	 loss: 17.1708, MinusLogProbMetric: 17.1708, val_loss: 17.1946, val_MinusLogProbMetric: 17.1946

Epoch 877: val_loss did not improve from 17.16001
196/196 - 54s - loss: 17.1708 - MinusLogProbMetric: 17.1708 - val_loss: 17.1946 - val_MinusLogProbMetric: 17.1946 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 878/1000
2023-10-11 18:11:15.055 
Epoch 878/1000 
	 loss: 17.1553, MinusLogProbMetric: 17.1553, val_loss: 17.2995, val_MinusLogProbMetric: 17.2995

Epoch 878: val_loss did not improve from 17.16001
196/196 - 53s - loss: 17.1553 - MinusLogProbMetric: 17.1553 - val_loss: 17.2995 - val_MinusLogProbMetric: 17.2995 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 879/1000
2023-10-11 18:12:08.650 
Epoch 879/1000 
	 loss: 17.1541, MinusLogProbMetric: 17.1541, val_loss: 17.1691, val_MinusLogProbMetric: 17.1691

Epoch 879: val_loss did not improve from 17.16001
196/196 - 54s - loss: 17.1541 - MinusLogProbMetric: 17.1541 - val_loss: 17.1691 - val_MinusLogProbMetric: 17.1691 - lr: 5.5556e-05 - 54s/epoch - 273ms/step
Epoch 880/1000
2023-10-11 18:13:03.045 
Epoch 880/1000 
	 loss: 17.1643, MinusLogProbMetric: 17.1643, val_loss: 17.1942, val_MinusLogProbMetric: 17.1942

Epoch 880: val_loss did not improve from 17.16001
196/196 - 54s - loss: 17.1643 - MinusLogProbMetric: 17.1643 - val_loss: 17.1942 - val_MinusLogProbMetric: 17.1942 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 881/1000
2023-10-11 18:13:58.220 
Epoch 881/1000 
	 loss: 17.1663, MinusLogProbMetric: 17.1663, val_loss: 17.2444, val_MinusLogProbMetric: 17.2444

Epoch 881: val_loss did not improve from 17.16001
196/196 - 55s - loss: 17.1663 - MinusLogProbMetric: 17.1663 - val_loss: 17.2444 - val_MinusLogProbMetric: 17.2444 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 882/1000
2023-10-11 18:14:51.618 
Epoch 882/1000 
	 loss: 17.1198, MinusLogProbMetric: 17.1198, val_loss: 17.2673, val_MinusLogProbMetric: 17.2673

Epoch 882: val_loss did not improve from 17.16001
196/196 - 53s - loss: 17.1198 - MinusLogProbMetric: 17.1198 - val_loss: 17.2673 - val_MinusLogProbMetric: 17.2673 - lr: 5.5556e-05 - 53s/epoch - 272ms/step
Epoch 883/1000
2023-10-11 18:15:44.993 
Epoch 883/1000 
	 loss: 17.1590, MinusLogProbMetric: 17.1590, val_loss: 17.1528, val_MinusLogProbMetric: 17.1528

Epoch 883: val_loss improved from 17.16001 to 17.15284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 54s - loss: 17.1590 - MinusLogProbMetric: 17.1590 - val_loss: 17.1528 - val_MinusLogProbMetric: 17.1528 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 884/1000
2023-10-11 18:16:39.581 
Epoch 884/1000 
	 loss: 17.1327, MinusLogProbMetric: 17.1327, val_loss: 17.3556, val_MinusLogProbMetric: 17.3556

Epoch 884: val_loss did not improve from 17.15284
196/196 - 54s - loss: 17.1327 - MinusLogProbMetric: 17.1327 - val_loss: 17.3556 - val_MinusLogProbMetric: 17.3556 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 885/1000
2023-10-11 18:17:31.589 
Epoch 885/1000 
	 loss: 17.1248, MinusLogProbMetric: 17.1248, val_loss: 17.3626, val_MinusLogProbMetric: 17.3626

Epoch 885: val_loss did not improve from 17.15284
196/196 - 52s - loss: 17.1248 - MinusLogProbMetric: 17.1248 - val_loss: 17.3626 - val_MinusLogProbMetric: 17.3626 - lr: 5.5556e-05 - 52s/epoch - 265ms/step
Epoch 886/1000
2023-10-11 18:18:23.318 
Epoch 886/1000 
	 loss: 17.1308, MinusLogProbMetric: 17.1308, val_loss: 17.5878, val_MinusLogProbMetric: 17.5878

Epoch 886: val_loss did not improve from 17.15284
196/196 - 52s - loss: 17.1308 - MinusLogProbMetric: 17.1308 - val_loss: 17.5878 - val_MinusLogProbMetric: 17.5878 - lr: 5.5556e-05 - 52s/epoch - 264ms/step
Epoch 887/1000
2023-10-11 18:19:16.577 
Epoch 887/1000 
	 loss: 17.1404, MinusLogProbMetric: 17.1404, val_loss: 17.2617, val_MinusLogProbMetric: 17.2617

Epoch 887: val_loss did not improve from 17.15284
196/196 - 53s - loss: 17.1404 - MinusLogProbMetric: 17.1404 - val_loss: 17.2617 - val_MinusLogProbMetric: 17.2617 - lr: 5.5556e-05 - 53s/epoch - 272ms/step
Epoch 888/1000
2023-10-11 18:20:11.760 
Epoch 888/1000 
	 loss: 17.1482, MinusLogProbMetric: 17.1482, val_loss: 17.1656, val_MinusLogProbMetric: 17.1656

Epoch 888: val_loss did not improve from 17.15284
196/196 - 55s - loss: 17.1482 - MinusLogProbMetric: 17.1482 - val_loss: 17.1656 - val_MinusLogProbMetric: 17.1656 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 889/1000
2023-10-11 18:21:07.390 
Epoch 889/1000 
	 loss: 17.1381, MinusLogProbMetric: 17.1381, val_loss: 17.5711, val_MinusLogProbMetric: 17.5711

Epoch 889: val_loss did not improve from 17.15284
196/196 - 56s - loss: 17.1381 - MinusLogProbMetric: 17.1381 - val_loss: 17.5711 - val_MinusLogProbMetric: 17.5711 - lr: 5.5556e-05 - 56s/epoch - 284ms/step
Epoch 890/1000
2023-10-11 18:22:01.624 
Epoch 890/1000 
	 loss: 17.1572, MinusLogProbMetric: 17.1572, val_loss: 17.1487, val_MinusLogProbMetric: 17.1487

Epoch 890: val_loss improved from 17.15284 to 17.14868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 56s - loss: 17.1572 - MinusLogProbMetric: 17.1572 - val_loss: 17.1487 - val_MinusLogProbMetric: 17.1487 - lr: 5.5556e-05 - 56s/epoch - 283ms/step
Epoch 891/1000
2023-10-11 18:22:57.472 
Epoch 891/1000 
	 loss: 17.1214, MinusLogProbMetric: 17.1214, val_loss: 17.1645, val_MinusLogProbMetric: 17.1645

Epoch 891: val_loss did not improve from 17.14868
196/196 - 55s - loss: 17.1214 - MinusLogProbMetric: 17.1214 - val_loss: 17.1645 - val_MinusLogProbMetric: 17.1645 - lr: 5.5556e-05 - 55s/epoch - 278ms/step
Epoch 892/1000
2023-10-11 18:23:51.858 
Epoch 892/1000 
	 loss: 17.1162, MinusLogProbMetric: 17.1162, val_loss: 17.1461, val_MinusLogProbMetric: 17.1461

Epoch 892: val_loss improved from 17.14868 to 17.14613, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 55s - loss: 17.1162 - MinusLogProbMetric: 17.1162 - val_loss: 17.1461 - val_MinusLogProbMetric: 17.1461 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 893/1000
2023-10-11 18:24:47.034 
Epoch 893/1000 
	 loss: 17.1469, MinusLogProbMetric: 17.1469, val_loss: 17.4184, val_MinusLogProbMetric: 17.4184

Epoch 893: val_loss did not improve from 17.14613
196/196 - 54s - loss: 17.1469 - MinusLogProbMetric: 17.1469 - val_loss: 17.4184 - val_MinusLogProbMetric: 17.4184 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 894/1000
2023-10-11 18:25:40.946 
Epoch 894/1000 
	 loss: 17.1476, MinusLogProbMetric: 17.1476, val_loss: 17.1787, val_MinusLogProbMetric: 17.1787

Epoch 894: val_loss did not improve from 17.14613
196/196 - 54s - loss: 17.1476 - MinusLogProbMetric: 17.1476 - val_loss: 17.1787 - val_MinusLogProbMetric: 17.1787 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 895/1000
2023-10-11 18:26:36.291 
Epoch 895/1000 
	 loss: 17.1246, MinusLogProbMetric: 17.1246, val_loss: 17.1544, val_MinusLogProbMetric: 17.1544

Epoch 895: val_loss did not improve from 17.14613
196/196 - 55s - loss: 17.1246 - MinusLogProbMetric: 17.1246 - val_loss: 17.1544 - val_MinusLogProbMetric: 17.1544 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 896/1000
2023-10-11 18:27:31.321 
Epoch 896/1000 
	 loss: 17.1600, MinusLogProbMetric: 17.1600, val_loss: 17.2973, val_MinusLogProbMetric: 17.2973

Epoch 896: val_loss did not improve from 17.14613
196/196 - 55s - loss: 17.1600 - MinusLogProbMetric: 17.1600 - val_loss: 17.2973 - val_MinusLogProbMetric: 17.2973 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 897/1000
2023-10-11 18:28:25.930 
Epoch 897/1000 
	 loss: 17.1099, MinusLogProbMetric: 17.1099, val_loss: 17.1430, val_MinusLogProbMetric: 17.1430

Epoch 897: val_loss improved from 17.14613 to 17.14300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 56s - loss: 17.1099 - MinusLogProbMetric: 17.1099 - val_loss: 17.1430 - val_MinusLogProbMetric: 17.1430 - lr: 5.5556e-05 - 56s/epoch - 284ms/step
Epoch 898/1000
2023-10-11 18:29:21.857 
Epoch 898/1000 
	 loss: 17.1131, MinusLogProbMetric: 17.1131, val_loss: 17.2737, val_MinusLogProbMetric: 17.2737

Epoch 898: val_loss did not improve from 17.14300
196/196 - 55s - loss: 17.1131 - MinusLogProbMetric: 17.1131 - val_loss: 17.2737 - val_MinusLogProbMetric: 17.2737 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 899/1000
2023-10-11 18:30:16.747 
Epoch 899/1000 
	 loss: 17.1350, MinusLogProbMetric: 17.1350, val_loss: 17.1410, val_MinusLogProbMetric: 17.1410

Epoch 899: val_loss improved from 17.14300 to 17.14101, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 56s - loss: 17.1350 - MinusLogProbMetric: 17.1350 - val_loss: 17.1410 - val_MinusLogProbMetric: 17.1410 - lr: 5.5556e-05 - 56s/epoch - 284ms/step
Epoch 900/1000
2023-10-11 18:31:11.230 
Epoch 900/1000 
	 loss: 17.1584, MinusLogProbMetric: 17.1584, val_loss: 17.5553, val_MinusLogProbMetric: 17.5553

Epoch 900: val_loss did not improve from 17.14101
196/196 - 54s - loss: 17.1584 - MinusLogProbMetric: 17.1584 - val_loss: 17.5553 - val_MinusLogProbMetric: 17.5553 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 901/1000
2023-10-11 18:32:04.863 
Epoch 901/1000 
	 loss: 17.1611, MinusLogProbMetric: 17.1611, val_loss: 17.2141, val_MinusLogProbMetric: 17.2141

Epoch 901: val_loss did not improve from 17.14101
196/196 - 54s - loss: 17.1611 - MinusLogProbMetric: 17.1611 - val_loss: 17.2141 - val_MinusLogProbMetric: 17.2141 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 902/1000
2023-10-11 18:32:56.907 
Epoch 902/1000 
	 loss: 17.1377, MinusLogProbMetric: 17.1377, val_loss: 17.2162, val_MinusLogProbMetric: 17.2162

Epoch 902: val_loss did not improve from 17.14101
196/196 - 52s - loss: 17.1377 - MinusLogProbMetric: 17.1377 - val_loss: 17.2162 - val_MinusLogProbMetric: 17.2162 - lr: 5.5556e-05 - 52s/epoch - 266ms/step
Epoch 903/1000
2023-10-11 18:33:53.443 
Epoch 903/1000 
	 loss: 17.1541, MinusLogProbMetric: 17.1541, val_loss: 17.1987, val_MinusLogProbMetric: 17.1987

Epoch 903: val_loss did not improve from 17.14101
196/196 - 57s - loss: 17.1541 - MinusLogProbMetric: 17.1541 - val_loss: 17.1987 - val_MinusLogProbMetric: 17.1987 - lr: 5.5556e-05 - 57s/epoch - 288ms/step
Epoch 904/1000
2023-10-11 18:34:49.242 
Epoch 904/1000 
	 loss: 17.1488, MinusLogProbMetric: 17.1488, val_loss: 17.1905, val_MinusLogProbMetric: 17.1905

Epoch 904: val_loss did not improve from 17.14101
196/196 - 56s - loss: 17.1488 - MinusLogProbMetric: 17.1488 - val_loss: 17.1905 - val_MinusLogProbMetric: 17.1905 - lr: 5.5556e-05 - 56s/epoch - 285ms/step
Epoch 905/1000
2023-10-11 18:35:44.822 
Epoch 905/1000 
	 loss: 17.1399, MinusLogProbMetric: 17.1399, val_loss: 17.6907, val_MinusLogProbMetric: 17.6907

Epoch 905: val_loss did not improve from 17.14101
196/196 - 56s - loss: 17.1399 - MinusLogProbMetric: 17.1399 - val_loss: 17.6907 - val_MinusLogProbMetric: 17.6907 - lr: 5.5556e-05 - 56s/epoch - 284ms/step
Epoch 906/1000
2023-10-11 18:36:40.236 
Epoch 906/1000 
	 loss: 17.1779, MinusLogProbMetric: 17.1779, val_loss: 17.1765, val_MinusLogProbMetric: 17.1765

Epoch 906: val_loss did not improve from 17.14101
196/196 - 55s - loss: 17.1779 - MinusLogProbMetric: 17.1779 - val_loss: 17.1765 - val_MinusLogProbMetric: 17.1765 - lr: 5.5556e-05 - 55s/epoch - 283ms/step
Epoch 907/1000
2023-10-11 18:37:34.759 
Epoch 907/1000 
	 loss: 17.1307, MinusLogProbMetric: 17.1307, val_loss: 17.2213, val_MinusLogProbMetric: 17.2213

Epoch 907: val_loss did not improve from 17.14101
196/196 - 55s - loss: 17.1307 - MinusLogProbMetric: 17.1307 - val_loss: 17.2213 - val_MinusLogProbMetric: 17.2213 - lr: 5.5556e-05 - 55s/epoch - 278ms/step
Epoch 908/1000
2023-10-11 18:38:30.286 
Epoch 908/1000 
	 loss: 17.1755, MinusLogProbMetric: 17.1755, val_loss: 17.2478, val_MinusLogProbMetric: 17.2478

Epoch 908: val_loss did not improve from 17.14101
196/196 - 56s - loss: 17.1755 - MinusLogProbMetric: 17.1755 - val_loss: 17.2478 - val_MinusLogProbMetric: 17.2478 - lr: 5.5556e-05 - 56s/epoch - 283ms/step
Epoch 909/1000
2023-10-11 18:39:25.061 
Epoch 909/1000 
	 loss: 17.1262, MinusLogProbMetric: 17.1262, val_loss: 17.2433, val_MinusLogProbMetric: 17.2433

Epoch 909: val_loss did not improve from 17.14101
196/196 - 55s - loss: 17.1262 - MinusLogProbMetric: 17.1262 - val_loss: 17.2433 - val_MinusLogProbMetric: 17.2433 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 910/1000
2023-10-11 18:40:20.506 
Epoch 910/1000 
	 loss: 17.1281, MinusLogProbMetric: 17.1281, val_loss: 17.2505, val_MinusLogProbMetric: 17.2505

Epoch 910: val_loss did not improve from 17.14101
196/196 - 55s - loss: 17.1281 - MinusLogProbMetric: 17.1281 - val_loss: 17.2505 - val_MinusLogProbMetric: 17.2505 - lr: 5.5556e-05 - 55s/epoch - 283ms/step
Epoch 911/1000
2023-10-11 18:41:15.712 
Epoch 911/1000 
	 loss: 17.1499, MinusLogProbMetric: 17.1499, val_loss: 17.1896, val_MinusLogProbMetric: 17.1896

Epoch 911: val_loss did not improve from 17.14101
196/196 - 55s - loss: 17.1499 - MinusLogProbMetric: 17.1499 - val_loss: 17.1896 - val_MinusLogProbMetric: 17.1896 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 912/1000
2023-10-11 18:42:11.069 
Epoch 912/1000 
	 loss: 17.1345, MinusLogProbMetric: 17.1345, val_loss: 17.2605, val_MinusLogProbMetric: 17.2605

Epoch 912: val_loss did not improve from 17.14101
196/196 - 55s - loss: 17.1345 - MinusLogProbMetric: 17.1345 - val_loss: 17.2605 - val_MinusLogProbMetric: 17.2605 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 913/1000
2023-10-11 18:43:06.522 
Epoch 913/1000 
	 loss: 17.1938, MinusLogProbMetric: 17.1938, val_loss: 17.5246, val_MinusLogProbMetric: 17.5246

Epoch 913: val_loss did not improve from 17.14101
196/196 - 55s - loss: 17.1938 - MinusLogProbMetric: 17.1938 - val_loss: 17.5246 - val_MinusLogProbMetric: 17.5246 - lr: 5.5556e-05 - 55s/epoch - 283ms/step
Epoch 914/1000
2023-10-11 18:44:00.804 
Epoch 914/1000 
	 loss: 17.1251, MinusLogProbMetric: 17.1251, val_loss: 17.1908, val_MinusLogProbMetric: 17.1908

Epoch 914: val_loss did not improve from 17.14101
196/196 - 54s - loss: 17.1251 - MinusLogProbMetric: 17.1251 - val_loss: 17.1908 - val_MinusLogProbMetric: 17.1908 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 915/1000
2023-10-11 18:44:55.698 
Epoch 915/1000 
	 loss: 17.1187, MinusLogProbMetric: 17.1187, val_loss: 17.2092, val_MinusLogProbMetric: 17.2092

Epoch 915: val_loss did not improve from 17.14101
196/196 - 55s - loss: 17.1187 - MinusLogProbMetric: 17.1187 - val_loss: 17.2092 - val_MinusLogProbMetric: 17.2092 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 916/1000
2023-10-11 18:45:50.308 
Epoch 916/1000 
	 loss: 17.1250, MinusLogProbMetric: 17.1250, val_loss: 17.1380, val_MinusLogProbMetric: 17.1380

Epoch 916: val_loss improved from 17.14101 to 17.13803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 55s - loss: 17.1250 - MinusLogProbMetric: 17.1250 - val_loss: 17.1380 - val_MinusLogProbMetric: 17.1380 - lr: 5.5556e-05 - 55s/epoch - 283ms/step
Epoch 917/1000
2023-10-11 18:46:46.433 
Epoch 917/1000 
	 loss: 17.1575, MinusLogProbMetric: 17.1575, val_loss: 17.2852, val_MinusLogProbMetric: 17.2852

Epoch 917: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1575 - MinusLogProbMetric: 17.1575 - val_loss: 17.2852 - val_MinusLogProbMetric: 17.2852 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 918/1000
2023-10-11 18:47:41.001 
Epoch 918/1000 
	 loss: 17.1669, MinusLogProbMetric: 17.1669, val_loss: 17.7304, val_MinusLogProbMetric: 17.7304

Epoch 918: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1669 - MinusLogProbMetric: 17.1669 - val_loss: 17.7304 - val_MinusLogProbMetric: 17.7304 - lr: 5.5556e-05 - 55s/epoch - 278ms/step
Epoch 919/1000
2023-10-11 18:48:36.094 
Epoch 919/1000 
	 loss: 17.1487, MinusLogProbMetric: 17.1487, val_loss: 17.2208, val_MinusLogProbMetric: 17.2208

Epoch 919: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1487 - MinusLogProbMetric: 17.1487 - val_loss: 17.2208 - val_MinusLogProbMetric: 17.2208 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 920/1000
2023-10-11 18:49:30.887 
Epoch 920/1000 
	 loss: 17.1425, MinusLogProbMetric: 17.1425, val_loss: 17.2463, val_MinusLogProbMetric: 17.2463

Epoch 920: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1425 - MinusLogProbMetric: 17.1425 - val_loss: 17.2463 - val_MinusLogProbMetric: 17.2463 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 921/1000
2023-10-11 18:50:24.908 
Epoch 921/1000 
	 loss: 17.1009, MinusLogProbMetric: 17.1009, val_loss: 17.3328, val_MinusLogProbMetric: 17.3328

Epoch 921: val_loss did not improve from 17.13803
196/196 - 54s - loss: 17.1009 - MinusLogProbMetric: 17.1009 - val_loss: 17.3328 - val_MinusLogProbMetric: 17.3328 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 922/1000
2023-10-11 18:51:18.323 
Epoch 922/1000 
	 loss: 17.1451, MinusLogProbMetric: 17.1451, val_loss: 17.1423, val_MinusLogProbMetric: 17.1423

Epoch 922: val_loss did not improve from 17.13803
196/196 - 53s - loss: 17.1451 - MinusLogProbMetric: 17.1451 - val_loss: 17.1423 - val_MinusLogProbMetric: 17.1423 - lr: 5.5556e-05 - 53s/epoch - 273ms/step
Epoch 923/1000
2023-10-11 18:52:09.425 
Epoch 923/1000 
	 loss: 17.1141, MinusLogProbMetric: 17.1141, val_loss: 17.2443, val_MinusLogProbMetric: 17.2443

Epoch 923: val_loss did not improve from 17.13803
196/196 - 51s - loss: 17.1141 - MinusLogProbMetric: 17.1141 - val_loss: 17.2443 - val_MinusLogProbMetric: 17.2443 - lr: 5.5556e-05 - 51s/epoch - 261ms/step
Epoch 924/1000
2023-10-11 18:53:02.907 
Epoch 924/1000 
	 loss: 17.1572, MinusLogProbMetric: 17.1572, val_loss: 17.1738, val_MinusLogProbMetric: 17.1738

Epoch 924: val_loss did not improve from 17.13803
196/196 - 53s - loss: 17.1572 - MinusLogProbMetric: 17.1572 - val_loss: 17.1738 - val_MinusLogProbMetric: 17.1738 - lr: 5.5556e-05 - 53s/epoch - 273ms/step
Epoch 925/1000
2023-10-11 18:53:57.785 
Epoch 925/1000 
	 loss: 17.1096, MinusLogProbMetric: 17.1096, val_loss: 17.1478, val_MinusLogProbMetric: 17.1478

Epoch 925: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1096 - MinusLogProbMetric: 17.1096 - val_loss: 17.1478 - val_MinusLogProbMetric: 17.1478 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 926/1000
2023-10-11 18:54:52.645 
Epoch 926/1000 
	 loss: 17.1262, MinusLogProbMetric: 17.1262, val_loss: 17.1686, val_MinusLogProbMetric: 17.1686

Epoch 926: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1262 - MinusLogProbMetric: 17.1262 - val_loss: 17.1686 - val_MinusLogProbMetric: 17.1686 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 927/1000
2023-10-11 18:55:47.113 
Epoch 927/1000 
	 loss: 17.1300, MinusLogProbMetric: 17.1300, val_loss: 17.3357, val_MinusLogProbMetric: 17.3357

Epoch 927: val_loss did not improve from 17.13803
196/196 - 54s - loss: 17.1300 - MinusLogProbMetric: 17.1300 - val_loss: 17.3357 - val_MinusLogProbMetric: 17.3357 - lr: 5.5556e-05 - 54s/epoch - 278ms/step
Epoch 928/1000
2023-10-11 18:56:41.887 
Epoch 928/1000 
	 loss: 17.1089, MinusLogProbMetric: 17.1089, val_loss: 17.2965, val_MinusLogProbMetric: 17.2965

Epoch 928: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1089 - MinusLogProbMetric: 17.1089 - val_loss: 17.2965 - val_MinusLogProbMetric: 17.2965 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 929/1000
2023-10-11 18:57:36.647 
Epoch 929/1000 
	 loss: 17.1298, MinusLogProbMetric: 17.1298, val_loss: 17.1676, val_MinusLogProbMetric: 17.1676

Epoch 929: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1298 - MinusLogProbMetric: 17.1298 - val_loss: 17.1676 - val_MinusLogProbMetric: 17.1676 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 930/1000
2023-10-11 18:58:31.242 
Epoch 930/1000 
	 loss: 17.1195, MinusLogProbMetric: 17.1195, val_loss: 17.3256, val_MinusLogProbMetric: 17.3256

Epoch 930: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1195 - MinusLogProbMetric: 17.1195 - val_loss: 17.3256 - val_MinusLogProbMetric: 17.3256 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 931/1000
2023-10-11 18:59:26.493 
Epoch 931/1000 
	 loss: 17.1014, MinusLogProbMetric: 17.1014, val_loss: 17.1581, val_MinusLogProbMetric: 17.1581

Epoch 931: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1014 - MinusLogProbMetric: 17.1014 - val_loss: 17.1581 - val_MinusLogProbMetric: 17.1581 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 932/1000
2023-10-11 19:00:20.529 
Epoch 932/1000 
	 loss: 17.1493, MinusLogProbMetric: 17.1493, val_loss: 17.1689, val_MinusLogProbMetric: 17.1689

Epoch 932: val_loss did not improve from 17.13803
196/196 - 54s - loss: 17.1493 - MinusLogProbMetric: 17.1493 - val_loss: 17.1689 - val_MinusLogProbMetric: 17.1689 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 933/1000
2023-10-11 19:01:15.241 
Epoch 933/1000 
	 loss: 17.1077, MinusLogProbMetric: 17.1077, val_loss: 17.2246, val_MinusLogProbMetric: 17.2246

Epoch 933: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1077 - MinusLogProbMetric: 17.1077 - val_loss: 17.2246 - val_MinusLogProbMetric: 17.2246 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 934/1000
2023-10-11 19:02:10.304 
Epoch 934/1000 
	 loss: 17.1247, MinusLogProbMetric: 17.1247, val_loss: 17.1898, val_MinusLogProbMetric: 17.1898

Epoch 934: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1247 - MinusLogProbMetric: 17.1247 - val_loss: 17.1898 - val_MinusLogProbMetric: 17.1898 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 935/1000
2023-10-11 19:03:05.821 
Epoch 935/1000 
	 loss: 17.1245, MinusLogProbMetric: 17.1245, val_loss: 17.3107, val_MinusLogProbMetric: 17.3107

Epoch 935: val_loss did not improve from 17.13803
196/196 - 56s - loss: 17.1245 - MinusLogProbMetric: 17.1245 - val_loss: 17.3107 - val_MinusLogProbMetric: 17.3107 - lr: 5.5556e-05 - 56s/epoch - 283ms/step
Epoch 936/1000
2023-10-11 19:04:00.617 
Epoch 936/1000 
	 loss: 17.1240, MinusLogProbMetric: 17.1240, val_loss: 17.1725, val_MinusLogProbMetric: 17.1725

Epoch 936: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1240 - MinusLogProbMetric: 17.1240 - val_loss: 17.1725 - val_MinusLogProbMetric: 17.1725 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 937/1000
2023-10-11 19:04:56.415 
Epoch 937/1000 
	 loss: 17.1558, MinusLogProbMetric: 17.1558, val_loss: 17.2193, val_MinusLogProbMetric: 17.2193

Epoch 937: val_loss did not improve from 17.13803
196/196 - 56s - loss: 17.1558 - MinusLogProbMetric: 17.1558 - val_loss: 17.2193 - val_MinusLogProbMetric: 17.2193 - lr: 5.5556e-05 - 56s/epoch - 285ms/step
Epoch 938/1000
2023-10-11 19:05:51.188 
Epoch 938/1000 
	 loss: 17.0948, MinusLogProbMetric: 17.0948, val_loss: 17.1398, val_MinusLogProbMetric: 17.1398

Epoch 938: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.0948 - MinusLogProbMetric: 17.0948 - val_loss: 17.1398 - val_MinusLogProbMetric: 17.1398 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 939/1000
2023-10-11 19:06:46.292 
Epoch 939/1000 
	 loss: 17.1531, MinusLogProbMetric: 17.1531, val_loss: 17.1866, val_MinusLogProbMetric: 17.1866

Epoch 939: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1531 - MinusLogProbMetric: 17.1531 - val_loss: 17.1866 - val_MinusLogProbMetric: 17.1866 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 940/1000
2023-10-11 19:07:41.526 
Epoch 940/1000 
	 loss: 17.1353, MinusLogProbMetric: 17.1353, val_loss: 17.3914, val_MinusLogProbMetric: 17.3914

Epoch 940: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1353 - MinusLogProbMetric: 17.1353 - val_loss: 17.3914 - val_MinusLogProbMetric: 17.3914 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 941/1000
2023-10-11 19:08:36.602 
Epoch 941/1000 
	 loss: 17.1135, MinusLogProbMetric: 17.1135, val_loss: 17.1944, val_MinusLogProbMetric: 17.1944

Epoch 941: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1135 - MinusLogProbMetric: 17.1135 - val_loss: 17.1944 - val_MinusLogProbMetric: 17.1944 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 942/1000
2023-10-11 19:09:31.201 
Epoch 942/1000 
	 loss: 17.1323, MinusLogProbMetric: 17.1323, val_loss: 17.1855, val_MinusLogProbMetric: 17.1855

Epoch 942: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1323 - MinusLogProbMetric: 17.1323 - val_loss: 17.1855 - val_MinusLogProbMetric: 17.1855 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 943/1000
2023-10-11 19:10:25.796 
Epoch 943/1000 
	 loss: 17.1399, MinusLogProbMetric: 17.1399, val_loss: 17.1682, val_MinusLogProbMetric: 17.1682

Epoch 943: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1399 - MinusLogProbMetric: 17.1399 - val_loss: 17.1682 - val_MinusLogProbMetric: 17.1682 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 944/1000
2023-10-11 19:11:20.436 
Epoch 944/1000 
	 loss: 17.1809, MinusLogProbMetric: 17.1809, val_loss: 17.2803, val_MinusLogProbMetric: 17.2803

Epoch 944: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1809 - MinusLogProbMetric: 17.1809 - val_loss: 17.2803 - val_MinusLogProbMetric: 17.2803 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 945/1000
2023-10-11 19:12:15.927 
Epoch 945/1000 
	 loss: 17.1206, MinusLogProbMetric: 17.1206, val_loss: 17.1815, val_MinusLogProbMetric: 17.1815

Epoch 945: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1206 - MinusLogProbMetric: 17.1206 - val_loss: 17.1815 - val_MinusLogProbMetric: 17.1815 - lr: 5.5556e-05 - 55s/epoch - 283ms/step
Epoch 946/1000
2023-10-11 19:13:11.242 
Epoch 946/1000 
	 loss: 17.1099, MinusLogProbMetric: 17.1099, val_loss: 17.3839, val_MinusLogProbMetric: 17.3839

Epoch 946: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1099 - MinusLogProbMetric: 17.1099 - val_loss: 17.3839 - val_MinusLogProbMetric: 17.3839 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 947/1000
2023-10-11 19:14:06.302 
Epoch 947/1000 
	 loss: 17.1204, MinusLogProbMetric: 17.1204, val_loss: 17.1464, val_MinusLogProbMetric: 17.1464

Epoch 947: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.1204 - MinusLogProbMetric: 17.1204 - val_loss: 17.1464 - val_MinusLogProbMetric: 17.1464 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 948/1000
2023-10-11 19:15:01.106 
Epoch 948/1000 
	 loss: 17.0990, MinusLogProbMetric: 17.0990, val_loss: 17.1816, val_MinusLogProbMetric: 17.1816

Epoch 948: val_loss did not improve from 17.13803
196/196 - 55s - loss: 17.0990 - MinusLogProbMetric: 17.0990 - val_loss: 17.1816 - val_MinusLogProbMetric: 17.1816 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 949/1000
2023-10-11 19:15:55.738 
Epoch 949/1000 
	 loss: 17.0954, MinusLogProbMetric: 17.0954, val_loss: 17.1359, val_MinusLogProbMetric: 17.1359

Epoch 949: val_loss improved from 17.13803 to 17.13587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 55s - loss: 17.0954 - MinusLogProbMetric: 17.0954 - val_loss: 17.1359 - val_MinusLogProbMetric: 17.1359 - lr: 5.5556e-05 - 55s/epoch - 283ms/step
Epoch 950/1000
2023-10-11 19:16:51.574 
Epoch 950/1000 
	 loss: 17.1194, MinusLogProbMetric: 17.1194, val_loss: 17.2502, val_MinusLogProbMetric: 17.2502

Epoch 950: val_loss did not improve from 17.13587
196/196 - 55s - loss: 17.1194 - MinusLogProbMetric: 17.1194 - val_loss: 17.2502 - val_MinusLogProbMetric: 17.2502 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 951/1000
2023-10-11 19:17:46.547 
Epoch 951/1000 
	 loss: 17.0898, MinusLogProbMetric: 17.0898, val_loss: 17.2442, val_MinusLogProbMetric: 17.2442

Epoch 951: val_loss did not improve from 17.13587
196/196 - 55s - loss: 17.0898 - MinusLogProbMetric: 17.0898 - val_loss: 17.2442 - val_MinusLogProbMetric: 17.2442 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 952/1000
2023-10-11 19:18:41.905 
Epoch 952/1000 
	 loss: 17.1231, MinusLogProbMetric: 17.1231, val_loss: 17.1398, val_MinusLogProbMetric: 17.1398

Epoch 952: val_loss did not improve from 17.13587
196/196 - 55s - loss: 17.1231 - MinusLogProbMetric: 17.1231 - val_loss: 17.1398 - val_MinusLogProbMetric: 17.1398 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 953/1000
2023-10-11 19:19:35.091 
Epoch 953/1000 
	 loss: 17.0983, MinusLogProbMetric: 17.0983, val_loss: 17.2986, val_MinusLogProbMetric: 17.2986

Epoch 953: val_loss did not improve from 17.13587
196/196 - 53s - loss: 17.0983 - MinusLogProbMetric: 17.0983 - val_loss: 17.2986 - val_MinusLogProbMetric: 17.2986 - lr: 5.5556e-05 - 53s/epoch - 271ms/step
Epoch 954/1000
2023-10-11 19:20:30.606 
Epoch 954/1000 
	 loss: 17.1150, MinusLogProbMetric: 17.1150, val_loss: 17.3296, val_MinusLogProbMetric: 17.3296

Epoch 954: val_loss did not improve from 17.13587
196/196 - 56s - loss: 17.1150 - MinusLogProbMetric: 17.1150 - val_loss: 17.3296 - val_MinusLogProbMetric: 17.3296 - lr: 5.5556e-05 - 56s/epoch - 283ms/step
Epoch 955/1000
2023-10-11 19:21:25.042 
Epoch 955/1000 
	 loss: 17.1470, MinusLogProbMetric: 17.1470, val_loss: 17.1798, val_MinusLogProbMetric: 17.1798

Epoch 955: val_loss did not improve from 17.13587
196/196 - 54s - loss: 17.1470 - MinusLogProbMetric: 17.1470 - val_loss: 17.1798 - val_MinusLogProbMetric: 17.1798 - lr: 5.5556e-05 - 54s/epoch - 278ms/step
Epoch 956/1000
2023-10-11 19:22:20.148 
Epoch 956/1000 
	 loss: 17.1152, MinusLogProbMetric: 17.1152, val_loss: 17.3475, val_MinusLogProbMetric: 17.3475

Epoch 956: val_loss did not improve from 17.13587
196/196 - 55s - loss: 17.1152 - MinusLogProbMetric: 17.1152 - val_loss: 17.3475 - val_MinusLogProbMetric: 17.3475 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 957/1000
2023-10-11 19:23:15.208 
Epoch 957/1000 
	 loss: 17.1640, MinusLogProbMetric: 17.1640, val_loss: 17.1109, val_MinusLogProbMetric: 17.1109

Epoch 957: val_loss improved from 17.13587 to 17.11090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 56s - loss: 17.1640 - MinusLogProbMetric: 17.1640 - val_loss: 17.1109 - val_MinusLogProbMetric: 17.1109 - lr: 5.5556e-05 - 56s/epoch - 285ms/step
Epoch 958/1000
2023-10-11 19:24:09.984 
Epoch 958/1000 
	 loss: 17.1075, MinusLogProbMetric: 17.1075, val_loss: 17.2250, val_MinusLogProbMetric: 17.2250

Epoch 958: val_loss did not improve from 17.11090
196/196 - 54s - loss: 17.1075 - MinusLogProbMetric: 17.1075 - val_loss: 17.2250 - val_MinusLogProbMetric: 17.2250 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 959/1000
2023-10-11 19:25:04.348 
Epoch 959/1000 
	 loss: 17.0972, MinusLogProbMetric: 17.0972, val_loss: 17.2917, val_MinusLogProbMetric: 17.2917

Epoch 959: val_loss did not improve from 17.11090
196/196 - 54s - loss: 17.0972 - MinusLogProbMetric: 17.0972 - val_loss: 17.2917 - val_MinusLogProbMetric: 17.2917 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 960/1000
2023-10-11 19:25:59.063 
Epoch 960/1000 
	 loss: 17.1036, MinusLogProbMetric: 17.1036, val_loss: 17.1285, val_MinusLogProbMetric: 17.1285

Epoch 960: val_loss did not improve from 17.11090
196/196 - 55s - loss: 17.1036 - MinusLogProbMetric: 17.1036 - val_loss: 17.1285 - val_MinusLogProbMetric: 17.1285 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 961/1000
2023-10-11 19:26:54.291 
Epoch 961/1000 
	 loss: 17.1310, MinusLogProbMetric: 17.1310, val_loss: 17.1165, val_MinusLogProbMetric: 17.1165

Epoch 961: val_loss did not improve from 17.11090
196/196 - 55s - loss: 17.1310 - MinusLogProbMetric: 17.1310 - val_loss: 17.1165 - val_MinusLogProbMetric: 17.1165 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 962/1000
2023-10-11 19:27:48.884 
Epoch 962/1000 
	 loss: 17.0823, MinusLogProbMetric: 17.0823, val_loss: 17.4208, val_MinusLogProbMetric: 17.4208

Epoch 962: val_loss did not improve from 17.11090
196/196 - 55s - loss: 17.0823 - MinusLogProbMetric: 17.0823 - val_loss: 17.4208 - val_MinusLogProbMetric: 17.4208 - lr: 5.5556e-05 - 55s/epoch - 278ms/step
Epoch 963/1000
2023-10-11 19:28:43.357 
Epoch 963/1000 
	 loss: 17.1306, MinusLogProbMetric: 17.1306, val_loss: 17.3162, val_MinusLogProbMetric: 17.3162

Epoch 963: val_loss did not improve from 17.11090
196/196 - 54s - loss: 17.1306 - MinusLogProbMetric: 17.1306 - val_loss: 17.3162 - val_MinusLogProbMetric: 17.3162 - lr: 5.5556e-05 - 54s/epoch - 278ms/step
Epoch 964/1000
2023-10-11 19:29:39.153 
Epoch 964/1000 
	 loss: 17.1440, MinusLogProbMetric: 17.1440, val_loss: 17.1717, val_MinusLogProbMetric: 17.1717

Epoch 964: val_loss did not improve from 17.11090
196/196 - 56s - loss: 17.1440 - MinusLogProbMetric: 17.1440 - val_loss: 17.1717 - val_MinusLogProbMetric: 17.1717 - lr: 5.5556e-05 - 56s/epoch - 285ms/step
Epoch 965/1000
2023-10-11 19:30:33.820 
Epoch 965/1000 
	 loss: 17.1640, MinusLogProbMetric: 17.1640, val_loss: 17.2300, val_MinusLogProbMetric: 17.2300

Epoch 965: val_loss did not improve from 17.11090
196/196 - 55s - loss: 17.1640 - MinusLogProbMetric: 17.1640 - val_loss: 17.2300 - val_MinusLogProbMetric: 17.2300 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 966/1000
2023-10-11 19:31:27.959 
Epoch 966/1000 
	 loss: 17.1050, MinusLogProbMetric: 17.1050, val_loss: 17.2818, val_MinusLogProbMetric: 17.2818

Epoch 966: val_loss did not improve from 17.11090
196/196 - 54s - loss: 17.1050 - MinusLogProbMetric: 17.1050 - val_loss: 17.2818 - val_MinusLogProbMetric: 17.2818 - lr: 5.5556e-05 - 54s/epoch - 276ms/step
Epoch 967/1000
2023-10-11 19:32:20.418 
Epoch 967/1000 
	 loss: 17.1477, MinusLogProbMetric: 17.1477, val_loss: 17.1493, val_MinusLogProbMetric: 17.1493

Epoch 967: val_loss did not improve from 17.11090
196/196 - 52s - loss: 17.1477 - MinusLogProbMetric: 17.1477 - val_loss: 17.1493 - val_MinusLogProbMetric: 17.1493 - lr: 5.5556e-05 - 52s/epoch - 268ms/step
Epoch 968/1000
2023-10-11 19:33:15.594 
Epoch 968/1000 
	 loss: 17.1464, MinusLogProbMetric: 17.1464, val_loss: 17.1557, val_MinusLogProbMetric: 17.1557

Epoch 968: val_loss did not improve from 17.11090
196/196 - 55s - loss: 17.1464 - MinusLogProbMetric: 17.1464 - val_loss: 17.1557 - val_MinusLogProbMetric: 17.1557 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 969/1000
2023-10-11 19:34:10.383 
Epoch 969/1000 
	 loss: 17.1612, MinusLogProbMetric: 17.1612, val_loss: 17.2729, val_MinusLogProbMetric: 17.2729

Epoch 969: val_loss did not improve from 17.11090
196/196 - 55s - loss: 17.1612 - MinusLogProbMetric: 17.1612 - val_loss: 17.2729 - val_MinusLogProbMetric: 17.2729 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 970/1000
2023-10-11 19:35:04.887 
Epoch 970/1000 
	 loss: 17.1237, MinusLogProbMetric: 17.1237, val_loss: 17.1077, val_MinusLogProbMetric: 17.1077

Epoch 970: val_loss improved from 17.11090 to 17.10773, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_317/weights/best_weights.h5
196/196 - 55s - loss: 17.1237 - MinusLogProbMetric: 17.1237 - val_loss: 17.1077 - val_MinusLogProbMetric: 17.1077 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 971/1000
2023-10-11 19:36:01.231 
Epoch 971/1000 
	 loss: 17.1184, MinusLogProbMetric: 17.1184, val_loss: 17.2209, val_MinusLogProbMetric: 17.2209

Epoch 971: val_loss did not improve from 17.10773
196/196 - 56s - loss: 17.1184 - MinusLogProbMetric: 17.1184 - val_loss: 17.2209 - val_MinusLogProbMetric: 17.2209 - lr: 5.5556e-05 - 56s/epoch - 283ms/step
Epoch 972/1000
2023-10-11 19:36:57.082 
Epoch 972/1000 
	 loss: 17.1116, MinusLogProbMetric: 17.1116, val_loss: 17.2457, val_MinusLogProbMetric: 17.2457

Epoch 972: val_loss did not improve from 17.10773
196/196 - 56s - loss: 17.1116 - MinusLogProbMetric: 17.1116 - val_loss: 17.2457 - val_MinusLogProbMetric: 17.2457 - lr: 5.5556e-05 - 56s/epoch - 285ms/step
Epoch 973/1000
2023-10-11 19:37:51.782 
Epoch 973/1000 
	 loss: 17.1141, MinusLogProbMetric: 17.1141, val_loss: 17.3133, val_MinusLogProbMetric: 17.3133

Epoch 973: val_loss did not improve from 17.10773
196/196 - 55s - loss: 17.1141 - MinusLogProbMetric: 17.1141 - val_loss: 17.3133 - val_MinusLogProbMetric: 17.3133 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 974/1000
2023-10-11 19:38:47.267 
Epoch 974/1000 
	 loss: 17.0964, MinusLogProbMetric: 17.0964, val_loss: 17.2384, val_MinusLogProbMetric: 17.2384

Epoch 974: val_loss did not improve from 17.10773
196/196 - 55s - loss: 17.0964 - MinusLogProbMetric: 17.0964 - val_loss: 17.2384 - val_MinusLogProbMetric: 17.2384 - lr: 5.5556e-05 - 55s/epoch - 283ms/step
Epoch 975/1000
2023-10-11 19:39:42.476 
Epoch 975/1000 
	 loss: 17.1233, MinusLogProbMetric: 17.1233, val_loss: 17.2191, val_MinusLogProbMetric: 17.2191

Epoch 975: val_loss did not improve from 17.10773
196/196 - 55s - loss: 17.1233 - MinusLogProbMetric: 17.1233 - val_loss: 17.2191 - val_MinusLogProbMetric: 17.2191 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 976/1000
2023-10-11 19:40:37.592 
Epoch 976/1000 
	 loss: 17.1399, MinusLogProbMetric: 17.1399, val_loss: 17.3026, val_MinusLogProbMetric: 17.3026

Epoch 976: val_loss did not improve from 17.10773
196/196 - 55s - loss: 17.1399 - MinusLogProbMetric: 17.1399 - val_loss: 17.3026 - val_MinusLogProbMetric: 17.3026 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 977/1000
2023-10-11 19:41:32.596 
Epoch 977/1000 
	 loss: 17.0995, MinusLogProbMetric: 17.0995, val_loss: 17.3609, val_MinusLogProbMetric: 17.3609

Epoch 977: val_loss did not improve from 17.10773
196/196 - 55s - loss: 17.0995 - MinusLogProbMetric: 17.0995 - val_loss: 17.3609 - val_MinusLogProbMetric: 17.3609 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Epoch 978/1000
2023-10-11 19:42:26.905 
Epoch 978/1000 
	 loss: 17.0857, MinusLogProbMetric: 17.0857, val_loss: 17.2637, val_MinusLogProbMetric: 17.2637

Epoch 978: val_loss did not improve from 17.10773
196/196 - 54s - loss: 17.0857 - MinusLogProbMetric: 17.0857 - val_loss: 17.2637 - val_MinusLogProbMetric: 17.2637 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 979/1000
2023-10-11 19:43:21.579 
Epoch 979/1000 
	 loss: 17.1019, MinusLogProbMetric: 17.1019, val_loss: 17.1544, val_MinusLogProbMetric: 17.1544

Epoch 979: val_loss did not improve from 17.10773
196/196 - 55s - loss: 17.1019 - MinusLogProbMetric: 17.1019 - val_loss: 17.1544 - val_MinusLogProbMetric: 17.1544 - lr: 5.5556e-05 - 55s/epoch - 279ms/step
Epoch 980/1000
2023-10-11 19:44:15.942 
Epoch 980/1000 
	 loss: 17.1237, MinusLogProbMetric: 17.1237, val_loss: 17.1373, val_MinusLogProbMetric: 17.1373

Epoch 980: val_loss did not improve from 17.10773
196/196 - 54s - loss: 17.1237 - MinusLogProbMetric: 17.1237 - val_loss: 17.1373 - val_MinusLogProbMetric: 17.1373 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 981/1000
2023-10-11 19:45:10.827 
Epoch 981/1000 
	 loss: 17.1170, MinusLogProbMetric: 17.1170, val_loss: 17.1754, val_MinusLogProbMetric: 17.1754

Epoch 981: val_loss did not improve from 17.10773
196/196 - 55s - loss: 17.1170 - MinusLogProbMetric: 17.1170 - val_loss: 17.1754 - val_MinusLogProbMetric: 17.1754 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 982/1000
2023-10-11 19:46:05.228 
Epoch 982/1000 
	 loss: 17.1452, MinusLogProbMetric: 17.1452, val_loss: 17.1865, val_MinusLogProbMetric: 17.1865

Epoch 982: val_loss did not improve from 17.10773
196/196 - 54s - loss: 17.1452 - MinusLogProbMetric: 17.1452 - val_loss: 17.1865 - val_MinusLogProbMetric: 17.1865 - lr: 5.5556e-05 - 54s/epoch - 278ms/step
Epoch 983/1000
2023-10-11 19:47:00.094 
Epoch 983/1000 
	 loss: 17.1242, MinusLogProbMetric: 17.1242, val_loss: 17.2071, val_MinusLogProbMetric: 17.2071

Epoch 983: val_loss did not improve from 17.10773
196/196 - 55s - loss: 17.1242 - MinusLogProbMetric: 17.1242 - val_loss: 17.2071 - val_MinusLogProbMetric: 17.2071 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 984/1000
2023-10-11 19:47:54.304 
Epoch 984/1000 
	 loss: 17.0991, MinusLogProbMetric: 17.0991, val_loss: 17.2332, val_MinusLogProbMetric: 17.2332

Epoch 984: val_loss did not improve from 17.10773
196/196 - 54s - loss: 17.0991 - MinusLogProbMetric: 17.0991 - val_loss: 17.2332 - val_MinusLogProbMetric: 17.2332 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 985/1000
2023-10-11 19:48:45.948 
Epoch 985/1000 
	 loss: 17.1182, MinusLogProbMetric: 17.1182, val_loss: 17.3652, val_MinusLogProbMetric: 17.3652

Epoch 985: val_loss did not improve from 17.10773
196/196 - 52s - loss: 17.1182 - MinusLogProbMetric: 17.1182 - val_loss: 17.3652 - val_MinusLogProbMetric: 17.3652 - lr: 5.5556e-05 - 52s/epoch - 263ms/step
Epoch 986/1000
2023-10-11 19:49:40.481 
Epoch 986/1000 
	 loss: 17.1099, MinusLogProbMetric: 17.1099, val_loss: 17.2208, val_MinusLogProbMetric: 17.2208

Epoch 986: val_loss did not improve from 17.10773
196/196 - 55s - loss: 17.1099 - MinusLogProbMetric: 17.1099 - val_loss: 17.2208 - val_MinusLogProbMetric: 17.2208 - lr: 5.5556e-05 - 55s/epoch - 278ms/step
Epoch 987/1000
2023-10-11 19:50:34.252 
Epoch 987/1000 
	 loss: 17.1328, MinusLogProbMetric: 17.1328, val_loss: 17.2005, val_MinusLogProbMetric: 17.2005

Epoch 987: val_loss did not improve from 17.10773
196/196 - 54s - loss: 17.1328 - MinusLogProbMetric: 17.1328 - val_loss: 17.2005 - val_MinusLogProbMetric: 17.2005 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 988/1000
2023-10-11 19:51:29.546 
Epoch 988/1000 
	 loss: 17.1241, MinusLogProbMetric: 17.1241, val_loss: 17.2195, val_MinusLogProbMetric: 17.2195

Epoch 988: val_loss did not improve from 17.10773
196/196 - 55s - loss: 17.1241 - MinusLogProbMetric: 17.1241 - val_loss: 17.2195 - val_MinusLogProbMetric: 17.2195 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 989/1000
2023-10-11 19:52:22.293 
Epoch 989/1000 
	 loss: 17.1301, MinusLogProbMetric: 17.1301, val_loss: 17.3373, val_MinusLogProbMetric: 17.3373

Epoch 989: val_loss did not improve from 17.10773
196/196 - 53s - loss: 17.1301 - MinusLogProbMetric: 17.1301 - val_loss: 17.3373 - val_MinusLogProbMetric: 17.3373 - lr: 5.5556e-05 - 53s/epoch - 269ms/step
Epoch 990/1000
2023-10-11 19:53:17.872 
Epoch 990/1000 
	 loss: 17.1320, MinusLogProbMetric: 17.1320, val_loss: 17.1715, val_MinusLogProbMetric: 17.1715

Epoch 990: val_loss did not improve from 17.10773
196/196 - 56s - loss: 17.1320 - MinusLogProbMetric: 17.1320 - val_loss: 17.1715 - val_MinusLogProbMetric: 17.1715 - lr: 5.5556e-05 - 56s/epoch - 284ms/step
Epoch 991/1000
2023-10-11 19:54:12.083 
Epoch 991/1000 
	 loss: 17.1409, MinusLogProbMetric: 17.1409, val_loss: 17.1704, val_MinusLogProbMetric: 17.1704

Epoch 991: val_loss did not improve from 17.10773
196/196 - 54s - loss: 17.1409 - MinusLogProbMetric: 17.1409 - val_loss: 17.1704 - val_MinusLogProbMetric: 17.1704 - lr: 5.5556e-05 - 54s/epoch - 277ms/step
Epoch 992/1000
2023-10-11 19:55:05.954 
Epoch 992/1000 
	 loss: 17.1371, MinusLogProbMetric: 17.1371, val_loss: 17.2435, val_MinusLogProbMetric: 17.2435

Epoch 992: val_loss did not improve from 17.10773
196/196 - 54s - loss: 17.1371 - MinusLogProbMetric: 17.1371 - val_loss: 17.2435 - val_MinusLogProbMetric: 17.2435 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 993/1000
2023-10-11 19:55:59.419 
Epoch 993/1000 
	 loss: 17.0745, MinusLogProbMetric: 17.0745, val_loss: 17.1857, val_MinusLogProbMetric: 17.1857

Epoch 993: val_loss did not improve from 17.10773
196/196 - 53s - loss: 17.0745 - MinusLogProbMetric: 17.0745 - val_loss: 17.1857 - val_MinusLogProbMetric: 17.1857 - lr: 5.5556e-05 - 53s/epoch - 273ms/step
Epoch 994/1000
2023-10-11 19:56:54.271 
Epoch 994/1000 
	 loss: 17.1036, MinusLogProbMetric: 17.1036, val_loss: 17.2252, val_MinusLogProbMetric: 17.2252

Epoch 994: val_loss did not improve from 17.10773
196/196 - 55s - loss: 17.1036 - MinusLogProbMetric: 17.1036 - val_loss: 17.2252 - val_MinusLogProbMetric: 17.2252 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 995/1000
2023-10-11 19:57:49.084 
Epoch 995/1000 
	 loss: 17.0811, MinusLogProbMetric: 17.0811, val_loss: 17.1463, val_MinusLogProbMetric: 17.1463

Epoch 995: val_loss did not improve from 17.10773
196/196 - 55s - loss: 17.0811 - MinusLogProbMetric: 17.0811 - val_loss: 17.1463 - val_MinusLogProbMetric: 17.1463 - lr: 5.5556e-05 - 55s/epoch - 280ms/step
Epoch 996/1000
2023-10-11 19:58:42.234 
Epoch 996/1000 
	 loss: 17.0837, MinusLogProbMetric: 17.0837, val_loss: 17.1207, val_MinusLogProbMetric: 17.1207

Epoch 996: val_loss did not improve from 17.10773
196/196 - 53s - loss: 17.0837 - MinusLogProbMetric: 17.0837 - val_loss: 17.1207 - val_MinusLogProbMetric: 17.1207 - lr: 5.5556e-05 - 53s/epoch - 271ms/step
Epoch 997/1000
2023-10-11 19:59:35.532 
Epoch 997/1000 
	 loss: 17.0977, MinusLogProbMetric: 17.0977, val_loss: 17.1609, val_MinusLogProbMetric: 17.1609

Epoch 997: val_loss did not improve from 17.10773
196/196 - 53s - loss: 17.0977 - MinusLogProbMetric: 17.0977 - val_loss: 17.1609 - val_MinusLogProbMetric: 17.1609 - lr: 5.5556e-05 - 53s/epoch - 272ms/step
Epoch 998/1000
2023-10-11 20:00:26.337 
Epoch 998/1000 
	 loss: 17.0926, MinusLogProbMetric: 17.0926, val_loss: 17.5266, val_MinusLogProbMetric: 17.5266

Epoch 998: val_loss did not improve from 17.10773
196/196 - 51s - loss: 17.0926 - MinusLogProbMetric: 17.0926 - val_loss: 17.5266 - val_MinusLogProbMetric: 17.5266 - lr: 5.5556e-05 - 51s/epoch - 259ms/step
Epoch 999/1000
2023-10-11 20:01:19.939 
Epoch 999/1000 
	 loss: 17.1271, MinusLogProbMetric: 17.1271, val_loss: 17.3211, val_MinusLogProbMetric: 17.3211

Epoch 999: val_loss did not improve from 17.10773
196/196 - 54s - loss: 17.1271 - MinusLogProbMetric: 17.1271 - val_loss: 17.3211 - val_MinusLogProbMetric: 17.3211 - lr: 5.5556e-05 - 54s/epoch - 273ms/step
Epoch 1000/1000
2023-10-11 20:02:15.088 
Epoch 1000/1000 
	 loss: 17.1095, MinusLogProbMetric: 17.1095, val_loss: 17.3751, val_MinusLogProbMetric: 17.3751

Epoch 1000: val_loss did not improve from 17.10773
196/196 - 55s - loss: 17.1095 - MinusLogProbMetric: 17.1095 - val_loss: 17.3751 - val_MinusLogProbMetric: 17.3751 - lr: 5.5556e-05 - 55s/epoch - 281ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 20.47436866001226 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
KS tests calculation completed in 10.779172617010772 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 8.489388218149543 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
nchunks = 10
Iterating from 0 to 1 out of 10 .
Iterating from 1 to 2 out of 10 .
Iterating from 2 to 3 out of 10 .
Iterating from 3 to 4 out of 10 .
Iterating from 4 to 5 out of 10 .
Iterating from 5 to 6 out of 10 .
Iterating from 6 to 7 out of 10 .
Iterating from 7 to 8 out of 10 .
Iterating from 8 to 9 out of 10 .
Iterating from 9 to 10 out of 10 .
FN metric calculation completed in 8.052304344018921 seconds.
Training succeeded with seed 933.
Model trained in 75472.39 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 49.18 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 49.32 s.
===========
Run 317/720 done in 76286.44 s.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

===========
Generating train data for run 325.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_197"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_198 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f0e90544100>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0ee0e31ae0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0ee0e31ae0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0e480fd5a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0e48129870>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0e48129de0>, <keras.callbacks.ModelCheckpoint object at 0x7f0e48129ea0>, <keras.callbacks.EarlyStopping object at 0x7f0e4812a110>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0e4812a140>, <keras.callbacks.TerminateOnNaN object at 0x7f0e48129d80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-10-11 20:03:10.778719
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:05:09.330 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 119s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 119s/epoch - 605ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 325.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_208"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_209 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f0b01edf400>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0e505cd750>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0e505cd750>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d70f33fa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0cac347d90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0cac3474f0>, <keras.callbacks.ModelCheckpoint object at 0x7f0cac3449d0>, <keras.callbacks.EarlyStopping object at 0x7f0cac345b10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0cac346a40>, <keras.callbacks.TerminateOnNaN object at 0x7f0cac344ca0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-10-11 20:05:16.557457
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:07:00.819 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 104s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 104s/epoch - 532ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 325.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_219"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_220 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f0b60891db0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0bc02bf850>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0bc02bf850>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f140e2df1f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b69a47400>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b69a47970>, <keras.callbacks.ModelCheckpoint object at 0x7f0b69a47a30>, <keras.callbacks.EarlyStopping object at 0x7f0b69a47ca0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b69a47cd0>, <keras.callbacks.TerminateOnNaN object at 0x7f0b69a47910>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-10-11 20:07:07.595940
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:09:00.579 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 113s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 113s/epoch - 576ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 3.703703703703703e-05.
===========
Generating train data for run 325.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_230"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_231 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f0a74a0fdf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0fcc2a5ba0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0fcc2a5ba0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b6835be50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0a74ad75e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0a74ad7b50>, <keras.callbacks.ModelCheckpoint object at 0x7f0a74ad7c10>, <keras.callbacks.EarlyStopping object at 0x7f0a74ad7e80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0a74ad7eb0>, <keras.callbacks.TerminateOnNaN object at 0x7f0a74ad7af0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-10-11 20:09:07.586895
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:5 out of the last 196004 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0bd0ee23b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:11:02.805 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 115s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 115s/epoch - 588ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.2345679012345677e-05.
===========
Generating train data for run 325.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_241"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_242 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f0b0843d9f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0a5fc59420>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0a5fc59420>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0eb4525930>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b983e5cc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b983e6f20>, <keras.callbacks.ModelCheckpoint object at 0x7f0b983e4820>, <keras.callbacks.EarlyStopping object at 0x7f0b983e40d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b983e4f40>, <keras.callbacks.TerminateOnNaN object at 0x7f0b983e6470>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-10-11 20:11:09.073111
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:6 out of the last 196005 calls to <function Model.make_train_function.<locals>.train_function at 0x7f0b9820b640> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:13:01.483 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 112s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 112s/epoch - 573ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.115226337448558e-06.
===========
Generating train data for run 325.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_252"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_253 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f0f782881c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0e88132740>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0e88132740>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0de865a980>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0ce7f71cc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0ce7f70a30>, <keras.callbacks.ModelCheckpoint object at 0x7f0a74694610>, <keras.callbacks.EarlyStopping object at 0x7f0a746943a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0a74694250>, <keras.callbacks.TerminateOnNaN object at 0x7f0a74694460>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-10-11 20:13:08.113640
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:14:56.528 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 108s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 108s/epoch - 552ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.3717421124828526e-06.
===========
Generating train data for run 325.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_263"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_264 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f138e959f90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c9721be80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c9721be80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0ce5f5a110>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1386173bb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f13861c0160>, <keras.callbacks.ModelCheckpoint object at 0x7f13861c0220>, <keras.callbacks.EarlyStopping object at 0x7f13861c0490>, <keras.callbacks.ReduceLROnPlateau object at 0x7f13861c04c0>, <keras.callbacks.TerminateOnNaN object at 0x7f13861c0100>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-10-11 20:15:03.428814
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:17:02.214 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 119s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 119s/epoch - 606ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.572473708276175e-07.
===========
Generating train data for run 325.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_274"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_275 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f0b8416c3a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c2876e8c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c2876e8c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c9456d030>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b69d09930>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b69d08d90>, <keras.callbacks.ModelCheckpoint object at 0x7f0b69d092a0>, <keras.callbacks.EarlyStopping object at 0x7f0b69d08fa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b69d08a30>, <keras.callbacks.TerminateOnNaN object at 0x7f0b69d09330>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-10-11 20:17:09.337180
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:19:03.563 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 114s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 114s/epoch - 582ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.524157902758725e-07.
===========
Generating train data for run 325.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_285"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_286 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f13c1c7bdf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0caf173b20>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0caf173b20>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d1d3c9750>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b09e63a90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b09e63a60>, <keras.callbacks.ModelCheckpoint object at 0x7f0b09e61300>, <keras.callbacks.EarlyStopping object at 0x7f0b09e62c20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b09e62830>, <keras.callbacks.TerminateOnNaN object at 0x7f0b09e61a80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-10-11 20:19:11.083789
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:20:56.040 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 105s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 105s/epoch - 534ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.0805263425290834e-08.
===========
Generating train data for run 325.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_296"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_297 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f1385e83ee0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b981a58a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b981a58a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c9513ef20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f1385aafe20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f1385ae83d0>, <keras.callbacks.ModelCheckpoint object at 0x7f1385ae8490>, <keras.callbacks.EarlyStopping object at 0x7f1385ae8700>, <keras.callbacks.ReduceLROnPlateau object at 0x7f1385ae8730>, <keras.callbacks.TerminateOnNaN object at 0x7f1385ae8370>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-10-11 20:21:02.821934
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:22:59.655 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 117s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 117s/epoch - 596ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.6935087808430278e-08.
===========
Generating train data for run 325.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_325/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_325
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_307"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_308 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f0e50262680>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b08274af0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b08274af0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c978c7520>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0e10632f80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_325/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0e106334f0>, <keras.callbacks.ModelCheckpoint object at 0x7f0e106335b0>, <keras.callbacks.EarlyStopping object at 0x7f0e10633820>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0e10633850>, <keras.callbacks.TerminateOnNaN object at 0x7f0e10633490>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_325/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 325/720 with hyperparameters:
timestamp = 2023-10-11 20:23:06.661983
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:25:04.381 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7476.8237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 118s - loss: nan - MinusLogProbMetric: 7476.8237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 118s/epoch - 600ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.645029269476759e-09.
===========
Run 325/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

===========
Generating train data for run 327.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_318"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_319 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f0b084089d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c282b3d60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c282b3d60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0a5f463c40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0a045b9b10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f138ea71d50>, <keras.callbacks.ModelCheckpoint object at 0x7f138ea721d0>, <keras.callbacks.EarlyStopping object at 0x7f138ea71180>, <keras.callbacks.ReduceLROnPlateau object at 0x7f138ea73070>, <keras.callbacks.TerminateOnNaN object at 0x7f138ea727d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-11 20:25:11.356740
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:27:32.051 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 141s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 141s/epoch - 717ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 327.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_329"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_330 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f136c6d65c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0bd44067d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0bd44067d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c5bf40ca0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f136c700d00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f136c701270>, <keras.callbacks.ModelCheckpoint object at 0x7f136c701330>, <keras.callbacks.EarlyStopping object at 0x7f136c7015a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f136c7015d0>, <keras.callbacks.TerminateOnNaN object at 0x7f136c701210>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-11 20:27:40.475925
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:29:57.331 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 137s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 137s/epoch - 697ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 327.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_340"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_341 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f135bc0da50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f135b802830>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f135b802830>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0bd4c51150>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f135b707b20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f135b707fd0>, <keras.callbacks.ModelCheckpoint object at 0x7f13640f8190>, <keras.callbacks.EarlyStopping object at 0x7f13640f8400>, <keras.callbacks.ReduceLROnPlateau object at 0x7f13640f8430>, <keras.callbacks.TerminateOnNaN object at 0x7f13640f8070>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-11 20:30:05.499737
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:32:28.062 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 142s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 142s/epoch - 727ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 3.703703703703703e-05.
===========
Generating train data for run 327.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_351"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_352 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f0bebf8f160>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c589c4430>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c589c4430>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0f586f15a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0a16b25d50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0a16b262c0>, <keras.callbacks.ModelCheckpoint object at 0x7f0a16b26380>, <keras.callbacks.EarlyStopping object at 0x7f0a16b265f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0a16b26620>, <keras.callbacks.TerminateOnNaN object at 0x7f0a16b26260>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-11 20:32:36.865877
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:35:00.762 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 144s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 144s/epoch - 734ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.2345679012345677e-05.
===========
Generating train data for run 327.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_362"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_363 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f0e50373a60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b00f17d90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b00f17d90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d7077b070>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0db043f040>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0db043e1a0>, <keras.callbacks.ModelCheckpoint object at 0x7f0db043dbd0>, <keras.callbacks.EarlyStopping object at 0x7f0db043d6c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0db043dba0>, <keras.callbacks.TerminateOnNaN object at 0x7f0db043df00>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-11 20:35:09.103927
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:37:23.289 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 134s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 134s/epoch - 683ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.115226337448558e-06.
===========
Generating train data for run 327.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_373"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_374 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f1352692fe0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0bd1d46c50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0bd1d46c50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c21febdc0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f13526c30d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f13526c3640>, <keras.callbacks.ModelCheckpoint object at 0x7f13526c3700>, <keras.callbacks.EarlyStopping object at 0x7f13526c3970>, <keras.callbacks.ReduceLROnPlateau object at 0x7f13526c39a0>, <keras.callbacks.TerminateOnNaN object at 0x7f13526c35e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-11 20:37:32.643465
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:39:59.247 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 147s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 147s/epoch - 748ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.3717421124828526e-06.
===========
Generating train data for run 327.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_384"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_385 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f0bd415f7f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0a05a640a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0a05a640a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0caea87be0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0bd4146dd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0bd4147340>, <keras.callbacks.ModelCheckpoint object at 0x7f0bd4147400>, <keras.callbacks.EarlyStopping object at 0x7f0bd4147670>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0bd41476a0>, <keras.callbacks.TerminateOnNaN object at 0x7f0bd41472e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-11 20:40:08.216342
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:42:31.440 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 143s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 143s/epoch - 731ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.572473708276175e-07.
===========
Generating train data for run 327.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_395"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_396 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f0c2042f3a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c96d95990>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c96d95990>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c59fe0760>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0bc01f5780>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0bc01f48e0>, <keras.callbacks.ModelCheckpoint object at 0x7f0bc01f7c40>, <keras.callbacks.EarlyStopping object at 0x7f0bc01f5a20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0bc01f43d0>, <keras.callbacks.TerminateOnNaN object at 0x7f0bc01f7bb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-11 20:42:40.127045
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:44:52.839 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 133s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 133s/epoch - 676ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.524157902758725e-07.
===========
Generating train data for run 327.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_406"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_407 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f1341b12170>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f1341824520>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f1341824520>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f134166c3a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f134159bcd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f13415cc280>, <keras.callbacks.ModelCheckpoint object at 0x7f13415cc340>, <keras.callbacks.EarlyStopping object at 0x7f13415cc5b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f13415cc5e0>, <keras.callbacks.TerminateOnNaN object at 0x7f13415cc220>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-11 20:45:01.684815
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:47:32.532 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 151s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 151s/epoch - 769ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.0805263425290834e-08.
===========
Generating train data for run 327.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_417"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_418 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f0c28f53400>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c28f3c9d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c28f3c9d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c290adf30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0c28f06440>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0c28f069b0>, <keras.callbacks.ModelCheckpoint object at 0x7f0c28f06a70>, <keras.callbacks.EarlyStopping object at 0x7f0c28f06ce0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0c28f06d10>, <keras.callbacks.TerminateOnNaN object at 0x7f0c28f06950>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-11 20:47:41.298749
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:50:00.984 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 140s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 140s/epoch - 712ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.6935087808430278e-08.
===========
Generating train data for run 327.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_428"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_429 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f0beadbaec0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0bd4341210>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0bd4341210>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c95925f60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0bead09600>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0bead09b70>, <keras.callbacks.ModelCheckpoint object at 0x7f0bead09c30>, <keras.callbacks.EarlyStopping object at 0x7f0bead09ea0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0bead09ed0>, <keras.callbacks.TerminateOnNaN object at 0x7f0bead09b10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-10-11 20:50:08.665271
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:52:40.393 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 152s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 152s/epoch - 774ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.645029269476759e-09.
===========
Run 327/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

===========
Generating train data for run 333.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_439"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_440 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f0beb23a050>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0beb292d10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0beb292d10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0bd5079bd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0a5f2c7f70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0c5a520520>, <keras.callbacks.ModelCheckpoint object at 0x7f0c5a5205e0>, <keras.callbacks.EarlyStopping object at 0x7f0c5a520850>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0c5a520880>, <keras.callbacks.TerminateOnNaN object at 0x7f0c5a5204c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-11 20:52:48.156928
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:54:34.305 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 106s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 106s/epoch - 541ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 333.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_450"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_451 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f09bd02a320>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b91d6fd60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b91d6fd60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f09bd0438b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0c8a0cbd60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0c5a22c310>, <keras.callbacks.ModelCheckpoint object at 0x7f0c5a22c3d0>, <keras.callbacks.EarlyStopping object at 0x7f0c5a22c640>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0c5a22c670>, <keras.callbacks.TerminateOnNaN object at 0x7f0c5a22c2b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-11 20:54:41.536662
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:56:47.966 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 126s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 126s/epoch - 644ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 333.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_461"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_462 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f0d943b52a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0cad6328c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0cad6328c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d70aa7640>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b70829390>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b7082bdf0>, <keras.callbacks.ModelCheckpoint object at 0x7f0b7082a770>, <keras.callbacks.EarlyStopping object at 0x7f0b708294e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b708290f0>, <keras.callbacks.TerminateOnNaN object at 0x7f0b7082a9b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-11 20:56:54.489547
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 20:58:40.551 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 106s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 106s/epoch - 540ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 333.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_472"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_473 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7f0bd4843fa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b7086c340>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b7086c340>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0bd4842dd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b29f8c310>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b29f8c880>, <keras.callbacks.ModelCheckpoint object at 0x7f0b29f8c940>, <keras.callbacks.EarlyStopping object at 0x7f0b29f8cbb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b29f8cbe0>, <keras.callbacks.TerminateOnNaN object at 0x7f0b29f8c820>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-11 20:58:46.973991
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:00:51.114 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 124s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 124s/epoch - 633ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 333.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_483"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_484 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7f1384da5ae0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d70873790>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d70873790>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0beab8cfa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0b08e2e890>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0b08e2df00>, <keras.callbacks.ModelCheckpoint object at 0x7f0b08e2db40>, <keras.callbacks.EarlyStopping object at 0x7f0b08e2d150>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0b08e2ccd0>, <keras.callbacks.TerminateOnNaN object at 0x7f0b08e2cfd0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-11 21:00:58.109448
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:02:44.660 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 106s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 106s/epoch - 543ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 333.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_494"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_495 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7f0df0145cf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b08b28070>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b08b28070>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c230a1750>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0cafef6440>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0cafef69b0>, <keras.callbacks.ModelCheckpoint object at 0x7f0cafef6a70>, <keras.callbacks.EarlyStopping object at 0x7f0cafef6ce0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0cafef6d10>, <keras.callbacks.TerminateOnNaN object at 0x7f0cafef6950>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-11 21:02:52.280727
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:04:54.438 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 122s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 122s/epoch - 623ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.3717421124828526e-06.
===========
Generating train data for run 333.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_505"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_506 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7f0e106b0be0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c956ec610>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c956ec610>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d70a424a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0c23295f00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0c23297700>, <keras.callbacks.ModelCheckpoint object at 0x7f0c23297310>, <keras.callbacks.EarlyStopping object at 0x7f0c23296fb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0c23297760>, <keras.callbacks.TerminateOnNaN object at 0x7f0c232959c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-11 21:05:01.445540
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:06:52.224 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 111s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 111s/epoch - 565ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.572473708276175e-07.
===========
Generating train data for run 333.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_516"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_517 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7f0c95929420>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0d304b2a40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0d304b2a40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d1c1c98a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0a141020b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0a14102620>, <keras.callbacks.ModelCheckpoint object at 0x7f0a141026e0>, <keras.callbacks.EarlyStopping object at 0x7f0a14102950>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0a14102980>, <keras.callbacks.TerminateOnNaN object at 0x7f0a141025c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-11 21:06:59.304648
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:08:57.369 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 118s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 118s/epoch - 602ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.524157902758725e-07.
===========
Generating train data for run 333.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_527"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_528 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7f0c95c1bcd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0db07be110>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0db07be110>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c2b9175e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0ce45c1720>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0ce45c04f0>, <keras.callbacks.ModelCheckpoint object at 0x7f0ce45c3100>, <keras.callbacks.EarlyStopping object at 0x7f0ce45c25c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0ce45c2ce0>, <keras.callbacks.TerminateOnNaN object at 0x7f0ce45c3940>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-11 21:09:04.638894
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:10:54.415 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 110s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 110s/epoch - 560ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.0805263425290834e-08.
===========
Generating train data for run 333.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_538"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_539 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7f0b51286470>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0c23bb5ff0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0c23bb5ff0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b7d4b3fd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0a04490790>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0a04490d00>, <keras.callbacks.ModelCheckpoint object at 0x7f0a04490dc0>, <keras.callbacks.EarlyStopping object at 0x7f0a04491030>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0a04491060>, <keras.callbacks.TerminateOnNaN object at 0x7f0a04490ca0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-11 21:11:00.814729
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:12:48.178 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 107s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 107s/epoch - 547ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.6935087808430278e-08.
===========
Generating train data for run 333.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_333/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_333
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_549"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_550 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7f1338dd7c40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0e10124070>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0e10124070>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f093c31a200>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f13289c3370>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_333/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f13289c38e0>, <keras.callbacks.ModelCheckpoint object at 0x7f13289c39a0>, <keras.callbacks.EarlyStopping object at 0x7f13289c3c10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f13289c3c40>, <keras.callbacks.TerminateOnNaN object at 0x7f13289c3880>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_333/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 333/720 with hyperparameters:
timestamp = 2023-10-11 21:12:55.365498
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:15:02.055 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7292.4321, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 7292.4321 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 127s/epoch - 646ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 5.645029269476759e-09.
===========
Run 333/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

===========
Generating train data for run 335.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_560"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_561 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7f0d68303fa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0a14174cd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0a14174cd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b51ac7be0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0c223db9a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0c223dbf10>, <keras.callbacks.ModelCheckpoint object at 0x7f0c223dbfd0>, <keras.callbacks.EarlyStopping object at 0x7f0c223dbee0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0c223dbeb0>, <keras.callbacks.TerminateOnNaN object at 0x7f0c223e4280>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-10-11 21:15:11.161372
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:17:25.976 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 135s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 135s/epoch - 687ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 335.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_571"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_572 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7f09bce41ea0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0967ad2170>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0967ad2170>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0b08d89270>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0954552b00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0954553070>, <keras.callbacks.ModelCheckpoint object at 0x7f0954553130>, <keras.callbacks.EarlyStopping object at 0x7f09545533a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f09545533d0>, <keras.callbacks.TerminateOnNaN object at 0x7f0954553010>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-10-11 21:17:33.811939
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:20:04.386 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 151s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 151s/epoch - 768ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0001111111111111111.
===========
Generating train data for run 335.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_582"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_583 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_52 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_52/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_52'")
self.model: <keras.engine.functional.Functional object at 0x7f0a1739c4c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b9126ea70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b9126ea70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0a170a52a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f0a1732eb60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f0a1732f0d0>, <keras.callbacks.ModelCheckpoint object at 0x7f0a1732f190>, <keras.callbacks.EarlyStopping object at 0x7f0a1732f400>, <keras.callbacks.ReduceLROnPlateau object at 0x7f0a1732f430>, <keras.callbacks.TerminateOnNaN object at 0x7f0a1732f070>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-10-11 21:20:13.449111
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:22:29.315 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 136s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 136s/epoch - 693ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 3.703703703703703e-05.
===========
Generating train data for run 335.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_593"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_594 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_53 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_53/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_53'")
self.model: <keras.engine.functional.Functional object at 0x7f096768fc10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f095c3ce260>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f095c3ce260>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0d7092ac80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f09676c6da0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f09676c7310>, <keras.callbacks.ModelCheckpoint object at 0x7f09676c73d0>, <keras.callbacks.EarlyStopping object at 0x7f09676c7640>, <keras.callbacks.ReduceLROnPlateau object at 0x7f09676c7670>, <keras.callbacks.TerminateOnNaN object at 0x7f09676c72b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-10-11 21:22:37.864607
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:25:14.067 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 156s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 156s/epoch - 797ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 1.2345679012345677e-05.
===========
Generating train data for run 335.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_604"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_605 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_54 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_54/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_54'")
self.model: <keras.engine.functional.Functional object at 0x7f09bc98a740>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f0b909c3610>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f0b909c3610>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0c23360b50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f09bc939630>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f09bc939ba0>, <keras.callbacks.ModelCheckpoint object at 0x7f09bc939c60>, <keras.callbacks.EarlyStopping object at 0x7f09bc939ed0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f09bc939f00>, <keras.callbacks.TerminateOnNaN object at 0x7f09bc939b40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-10-11 21:25:22.947251
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-11 21:27:39.578 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.4800, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 137s - loss: nan - MinusLogProbMetric: 7498.4800 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 137s/epoch - 697ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 4.115226337448558e-06.
===========
Generating train data for run 335.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_335/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_335
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_615"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_616 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_55 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_55/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_55'")
self.model: <keras.engine.functional.Functional object at 0x7f0966bd3070>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f09bd6680a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f09bd6680a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f0955100ee0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f09570e5900>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_335/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f09570e5e70>, <keras.callbacks.ModelCheckpoint object at 0x7f09570e5f30>, <keras.callbacks.EarlyStopping object at 0x7f09570e61a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f09570e61d0>, <keras.callbacks.TerminateOnNaN object at 0x7f09570e5e10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_335/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 335/720 with hyperparameters:
timestamp = 2023-10-11 21:27:48.213085
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
JIT session error: Cannot allocate memory
LLVM ERROR: Unable to allocate section memory!
