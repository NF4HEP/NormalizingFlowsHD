2023-10-25 16:06:04.534032: Importing os...
2023-10-25 16:06:04.534102: Importing sys...
2023-10-25 16:06:04.534119: Importing and initializing argparse...
Visible devices: [0]
2023-10-25 16:06:04.553523: Importing timer from timeit...
2023-10-25 16:06:04.554136: Setting env variables for tf import (only device [0] will be available)...
2023-10-25 16:06:04.554196: Importing numpy...
2023-10-25 16:06:04.687601: Importing pandas...
2023-10-25 16:06:04.880431: Importing shutil...
2023-10-25 16:06:04.880460: Importing subprocess...
2023-10-25 16:06:04.880467: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-25 16:06:07.239001: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-25 16:06:07.607484: Importing textwrap...
2023-10-25 16:06:07.607513: Importing timeit...
2023-10-25 16:06:07.607523: Importing traceback...
2023-10-25 16:06:07.607529: Importing typing...
2023-10-25 16:06:07.607539: Setting tf configs...
2023-10-25 16:06:07.959417: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-25 16:06:09.407281: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_349/ already exists.
Skipping it.
===========
Run 349/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_353/ already exists.
Skipping it.
===========
Run 353/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_354/ already exists.
Skipping it.
===========
Run 354/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_355/ already exists.
Skipping it.
===========
Run 355/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_356/ already exists.
Skipping it.
===========
Run 356/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_357/ already exists.
Skipping it.
===========
Run 357/720 already exists. Skipping it.
===========

===========
Generating train data for run 358.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_358/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_358/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_358/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_358
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  3291840   
 r)                                                              
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f308459ed40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f308c1c30d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f308c1c30d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f30306bb850>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f30306e8a30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f30304285b0>, <keras.callbacks.ModelCheckpoint object at 0x7f3030428700>, <keras.callbacks.EarlyStopping object at 0x7f3030428910>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3030428940>, <keras.callbacks.TerminateOnNaN object at 0x7f3030428670>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_358/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 358/720 with hyperparameters:
timestamp = 2023-10-25 16:06:18.001556
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 7: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:08:28.311 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6606.3320, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 130s - loss: nan - MinusLogProbMetric: 6606.3320 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 130s/epoch - 663ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 358.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_358/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_358/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_358/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_358
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f34287ba0b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f34288e6f80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f34288e6f80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2d80119840>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f3427f66950>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f3427f66ec0>, <keras.callbacks.ModelCheckpoint object at 0x7f3427f66f80>, <keras.callbacks.EarlyStopping object at 0x7f3427f671f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f3427f67220>, <keras.callbacks.TerminateOnNaN object at 0x7f3427f66e60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_358/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 358/720 with hyperparameters:
timestamp = 2023-10-25 16:08:34.916240
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 31: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:10:45.620 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5266.1255, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 131s - loss: nan - MinusLogProbMetric: 5266.1255 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 131s/epoch - 666ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 358.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_358/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_358/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_358/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_358
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f2ddc7b7b50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2e78419b40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2e78419b40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2fb83ecc10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f340f41d240>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f340f41d7b0>, <keras.callbacks.ModelCheckpoint object at 0x7f340f41d870>, <keras.callbacks.EarlyStopping object at 0x7f340f41dae0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f340f41db10>, <keras.callbacks.TerminateOnNaN object at 0x7f340f41d750>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_358/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 358/720 with hyperparameters:
timestamp = 2023-10-25 16:10:53.356122
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
2023-10-25 16:13:51.470 
Epoch 1/1000 
	 loss: 3913.0566, MinusLogProbMetric: 3913.0566, val_loss: 2200.2202, val_MinusLogProbMetric: 2200.2202

Epoch 1: val_loss improved from inf to 2200.22021, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 179s - loss: 3913.0566 - MinusLogProbMetric: 3913.0566 - val_loss: 2200.2202 - val_MinusLogProbMetric: 2200.2202 - lr: 1.1111e-04 - 179s/epoch - 912ms/step
Epoch 2/1000
2023-10-25 16:14:50.305 
Epoch 2/1000 
	 loss: 1344.8713, MinusLogProbMetric: 1344.8713, val_loss: 952.1148, val_MinusLogProbMetric: 952.1148

Epoch 2: val_loss improved from 2200.22021 to 952.11481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 58s - loss: 1344.8713 - MinusLogProbMetric: 1344.8713 - val_loss: 952.1148 - val_MinusLogProbMetric: 952.1148 - lr: 1.1111e-04 - 58s/epoch - 298ms/step
Epoch 3/1000
2023-10-25 16:15:42.779 
Epoch 3/1000 
	 loss: 784.4073, MinusLogProbMetric: 784.4073, val_loss: 692.5276, val_MinusLogProbMetric: 692.5276

Epoch 3: val_loss improved from 952.11481 to 692.52759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 53s - loss: 784.4073 - MinusLogProbMetric: 784.4073 - val_loss: 692.5276 - val_MinusLogProbMetric: 692.5276 - lr: 1.1111e-04 - 53s/epoch - 268ms/step
Epoch 4/1000
2023-10-25 16:16:35.796 
Epoch 4/1000 
	 loss: 634.1771, MinusLogProbMetric: 634.1771, val_loss: 584.7958, val_MinusLogProbMetric: 584.7958

Epoch 4: val_loss improved from 692.52759 to 584.79578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 53s - loss: 634.1771 - MinusLogProbMetric: 634.1771 - val_loss: 584.7958 - val_MinusLogProbMetric: 584.7958 - lr: 1.1111e-04 - 53s/epoch - 271ms/step
Epoch 5/1000
2023-10-25 16:17:29.719 
Epoch 5/1000 
	 loss: 565.5741, MinusLogProbMetric: 565.5741, val_loss: 536.2155, val_MinusLogProbMetric: 536.2155

Epoch 5: val_loss improved from 584.79578 to 536.21545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 54s - loss: 565.5741 - MinusLogProbMetric: 565.5741 - val_loss: 536.2155 - val_MinusLogProbMetric: 536.2155 - lr: 1.1111e-04 - 54s/epoch - 276ms/step
Epoch 6/1000
2023-10-25 16:18:26.245 
Epoch 6/1000 
	 loss: 601.4327, MinusLogProbMetric: 601.4327, val_loss: 1195.6736, val_MinusLogProbMetric: 1195.6736

Epoch 6: val_loss did not improve from 536.21545
196/196 - 56s - loss: 601.4327 - MinusLogProbMetric: 601.4327 - val_loss: 1195.6736 - val_MinusLogProbMetric: 1195.6736 - lr: 1.1111e-04 - 56s/epoch - 283ms/step
Epoch 7/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 49: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:18:45.736 
Epoch 7/1000 
	 loss: nan, MinusLogProbMetric: 1357.6257, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 7: val_loss did not improve from 536.21545
196/196 - 19s - loss: nan - MinusLogProbMetric: 1357.6257 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 19s/epoch - 99ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 358.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_358/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_358/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_358/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_358
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_43"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_44 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f2e7824cfd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f340f27fa60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f340f27fa60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2dbc531c00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f340ee49390>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f340ee49900>, <keras.callbacks.ModelCheckpoint object at 0x7f340ee499c0>, <keras.callbacks.EarlyStopping object at 0x7f340ee49c30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f340ee49c60>, <keras.callbacks.TerminateOnNaN object at 0x7f340ee498a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 358/720 with hyperparameters:
timestamp = 2023-10-25 16:18:55.106954
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
2023-10-25 16:21:57.070 
Epoch 1/1000 
	 loss: 801.5051, MinusLogProbMetric: 801.5051, val_loss: 942.9592, val_MinusLogProbMetric: 942.9592

Epoch 1: val_loss improved from inf to 942.95923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 183s - loss: 801.5051 - MinusLogProbMetric: 801.5051 - val_loss: 942.9592 - val_MinusLogProbMetric: 942.9592 - lr: 3.7037e-05 - 183s/epoch - 931ms/step
Epoch 2/1000
2023-10-25 16:22:52.916 
Epoch 2/1000 
	 loss: 661.1555, MinusLogProbMetric: 661.1555, val_loss: 588.1917, val_MinusLogProbMetric: 588.1917

Epoch 2: val_loss improved from 942.95923 to 588.19165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 55s - loss: 661.1555 - MinusLogProbMetric: 661.1555 - val_loss: 588.1917 - val_MinusLogProbMetric: 588.1917 - lr: 3.7037e-05 - 55s/epoch - 283ms/step
Epoch 3/1000
2023-10-25 16:23:46.059 
Epoch 3/1000 
	 loss: 491.3457, MinusLogProbMetric: 491.3457, val_loss: 431.9729, val_MinusLogProbMetric: 431.9729

Epoch 3: val_loss improved from 588.19165 to 431.97290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 53s - loss: 491.3457 - MinusLogProbMetric: 491.3457 - val_loss: 431.9729 - val_MinusLogProbMetric: 431.9729 - lr: 3.7037e-05 - 53s/epoch - 271ms/step
Epoch 4/1000
2023-10-25 16:24:40.477 
Epoch 4/1000 
	 loss: 790.0974, MinusLogProbMetric: 790.0974, val_loss: 708.9011, val_MinusLogProbMetric: 708.9011

Epoch 4: val_loss did not improve from 431.97290
196/196 - 54s - loss: 790.0974 - MinusLogProbMetric: 790.0974 - val_loss: 708.9011 - val_MinusLogProbMetric: 708.9011 - lr: 3.7037e-05 - 54s/epoch - 274ms/step
Epoch 5/1000
2023-10-25 16:25:40.569 
Epoch 5/1000 
	 loss: 597.5561, MinusLogProbMetric: 597.5561, val_loss: 550.7303, val_MinusLogProbMetric: 550.7303

Epoch 5: val_loss did not improve from 431.97290
196/196 - 60s - loss: 597.5561 - MinusLogProbMetric: 597.5561 - val_loss: 550.7303 - val_MinusLogProbMetric: 550.7303 - lr: 3.7037e-05 - 60s/epoch - 307ms/step
Epoch 6/1000
2023-10-25 16:26:39.625 
Epoch 6/1000 
	 loss: 486.2242, MinusLogProbMetric: 486.2242, val_loss: 418.2090, val_MinusLogProbMetric: 418.2090

Epoch 6: val_loss improved from 431.97290 to 418.20901, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 60s - loss: 486.2242 - MinusLogProbMetric: 486.2242 - val_loss: 418.2090 - val_MinusLogProbMetric: 418.2090 - lr: 3.7037e-05 - 60s/epoch - 305ms/step
Epoch 7/1000
2023-10-25 16:27:37.490 
Epoch 7/1000 
	 loss: 389.7273, MinusLogProbMetric: 389.7273, val_loss: 364.7763, val_MinusLogProbMetric: 364.7763

Epoch 7: val_loss improved from 418.20901 to 364.77628, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 58s - loss: 389.7273 - MinusLogProbMetric: 389.7273 - val_loss: 364.7763 - val_MinusLogProbMetric: 364.7763 - lr: 3.7037e-05 - 58s/epoch - 295ms/step
Epoch 8/1000
2023-10-25 16:28:39.524 
Epoch 8/1000 
	 loss: 351.0726, MinusLogProbMetric: 351.0726, val_loss: 333.0081, val_MinusLogProbMetric: 333.0081

Epoch 8: val_loss improved from 364.77628 to 333.00806, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 62s - loss: 351.0726 - MinusLogProbMetric: 351.0726 - val_loss: 333.0081 - val_MinusLogProbMetric: 333.0081 - lr: 3.7037e-05 - 62s/epoch - 317ms/step
Epoch 9/1000
2023-10-25 16:29:36.536 
Epoch 9/1000 
	 loss: 319.2753, MinusLogProbMetric: 319.2753, val_loss: 304.5092, val_MinusLogProbMetric: 304.5092

Epoch 9: val_loss improved from 333.00806 to 304.50916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 57s - loss: 319.2753 - MinusLogProbMetric: 319.2753 - val_loss: 304.5092 - val_MinusLogProbMetric: 304.5092 - lr: 3.7037e-05 - 57s/epoch - 291ms/step
Epoch 10/1000
2023-10-25 16:30:32.160 
Epoch 10/1000 
	 loss: 302.1442, MinusLogProbMetric: 302.1442, val_loss: 309.4264, val_MinusLogProbMetric: 309.4264

Epoch 10: val_loss did not improve from 304.50916
196/196 - 55s - loss: 302.1442 - MinusLogProbMetric: 302.1442 - val_loss: 309.4264 - val_MinusLogProbMetric: 309.4264 - lr: 3.7037e-05 - 55s/epoch - 280ms/step
Epoch 11/1000
2023-10-25 16:31:24.353 
Epoch 11/1000 
	 loss: 297.0548, MinusLogProbMetric: 297.0548, val_loss: 277.8868, val_MinusLogProbMetric: 277.8868

Epoch 11: val_loss improved from 304.50916 to 277.88684, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 53s - loss: 297.0548 - MinusLogProbMetric: 297.0548 - val_loss: 277.8868 - val_MinusLogProbMetric: 277.8868 - lr: 3.7037e-05 - 53s/epoch - 270ms/step
Epoch 12/1000
2023-10-25 16:32:25.077 
Epoch 12/1000 
	 loss: 271.0983, MinusLogProbMetric: 271.0983, val_loss: 259.8855, val_MinusLogProbMetric: 259.8855

Epoch 12: val_loss improved from 277.88684 to 259.88550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 61s - loss: 271.0983 - MinusLogProbMetric: 271.0983 - val_loss: 259.8855 - val_MinusLogProbMetric: 259.8855 - lr: 3.7037e-05 - 61s/epoch - 312ms/step
Epoch 13/1000
2023-10-25 16:33:26.013 
Epoch 13/1000 
	 loss: 306.6640, MinusLogProbMetric: 306.6640, val_loss: 265.7835, val_MinusLogProbMetric: 265.7835

Epoch 13: val_loss did not improve from 259.88550
196/196 - 60s - loss: 306.6640 - MinusLogProbMetric: 306.6640 - val_loss: 265.7835 - val_MinusLogProbMetric: 265.7835 - lr: 3.7037e-05 - 60s/epoch - 305ms/step
Epoch 14/1000
2023-10-25 16:34:18.650 
Epoch 14/1000 
	 loss: 254.8286, MinusLogProbMetric: 254.8286, val_loss: 249.1754, val_MinusLogProbMetric: 249.1754

Epoch 14: val_loss improved from 259.88550 to 249.17538, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 53s - loss: 254.8286 - MinusLogProbMetric: 254.8286 - val_loss: 249.1754 - val_MinusLogProbMetric: 249.1754 - lr: 3.7037e-05 - 53s/epoch - 273ms/step
Epoch 15/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 181: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-25 16:35:08.763 
Epoch 15/1000 
	 loss: nan, MinusLogProbMetric: 419.1678, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 15: val_loss did not improve from 249.17538
196/196 - 49s - loss: nan - MinusLogProbMetric: 419.1678 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 49s/epoch - 251ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 358.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_358/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_358/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_358/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_358
self.data_kwargs: {'seed': 520}
self.x_data: [[2.1731846  3.8937397  6.9821544  ... 5.669328   0.68163395 3.5624375 ]
 [6.7535415  2.6749182  6.1859965  ... 3.3760262  4.638645   2.336814  ]
 [6.1686287  2.7568936  6.2012696  ... 2.5904071  3.339636   2.9827425 ]
 ...
 [6.6194353  2.9398146  6.1845684  ... 3.0365293  3.5444417  1.3393478 ]
 [5.613061   6.348512   5.164087   ... 1.829057   7.7712007  1.4071141 ]
 [6.893006   3.0565336  6.057088   ... 2.613742   4.4370584  2.0332472 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_54"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_55 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  3291840   
 yer)                                                            
                                                                 
=================================================================
Total params: 3,291,840
Trainable params: 3,291,840
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f2d80f53760>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f340eb7a2f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f340eb7a2f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2f04c71a20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f340e6b7a30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f340e6b7fa0>, <keras.callbacks.ModelCheckpoint object at 0x7f340e6b7f40>, <keras.callbacks.EarlyStopping object at 0x7f340e6b7f70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f340e708130>, <keras.callbacks.TerminateOnNaN object at 0x7f340e708310>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.4659383 ,  4.1334686 ,  8.6866865 , ...,  6.8463674 ,
         0.36138976,  1.4300231 ],
       [ 6.484008  ,  3.007975  ,  6.2337604 , ...,  3.739787  ,
         1.4793756 ,  3.0040233 ],
       [ 2.264184  ,  3.3112721 ,  8.797337  , ...,  5.9933467 ,
         0.6142512 ,  3.1057003 ],
       ...,
       [ 5.4343762 ,  7.4912486 ,  6.4172325 , ...,  0.04709697,
         4.887264  ,  1.4711498 ],
       [ 1.6543716 ,  3.8658652 ,  8.345405  , ...,  6.200018  ,
        -1.0324802 ,  3.065315  ],
       [ 6.127924  ,  2.7694616 ,  6.280568  , ...,  3.5089302 ,
         5.2330275 ,  2.417325  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 358/720 with hyperparameters:
timestamp = 2023-10-25 16:35:15.725864
ndims = 64
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 3291840
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.1731846   3.8937397   6.9821544   1.4459838   8.670284    0.996012
  9.747296    5.3488703  10.829596    5.798153    7.4385934  -0.39963475
  2.887594    1.6816981   1.9483483   2.4950166   3.377345    6.4428034
  0.7434967  10.024268    5.9064217   3.9731474   5.977063    2.0677786
  5.421294    7.7411385   2.3877835   6.667169    1.0165741   6.879494
  2.333092    1.6771841   5.2052946  -0.01726444  8.224516   -0.06788193
  7.1896877   2.6926644   7.7422037   9.618431    2.508284    5.3540187
  8.24927     4.96435     2.6443048   8.933116    4.0272393   8.583376
  6.9591885   2.9305875   8.131322    3.9143033   8.096406    5.8134246
  9.931866    7.011071    7.2540216   5.1431737  10.5702715   6.340821
  4.2660613   5.669328    0.68163395  3.5624375 ]
Epoch 1/1000
2023-10-25 16:38:11.175 
Epoch 1/1000 
	 loss: 258.9452, MinusLogProbMetric: 258.9452, val_loss: 270.9261, val_MinusLogProbMetric: 270.9261

Epoch 1: val_loss improved from inf to 270.92606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 176s - loss: 258.9452 - MinusLogProbMetric: 258.9452 - val_loss: 270.9261 - val_MinusLogProbMetric: 270.9261 - lr: 1.2346e-05 - 176s/epoch - 898ms/step
Epoch 2/1000
2023-10-25 16:39:04.873 
Epoch 2/1000 
	 loss: 238.1282, MinusLogProbMetric: 238.1282, val_loss: 221.5573, val_MinusLogProbMetric: 221.5573

Epoch 2: val_loss improved from 270.92606 to 221.55728, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 53s - loss: 238.1282 - MinusLogProbMetric: 238.1282 - val_loss: 221.5573 - val_MinusLogProbMetric: 221.5573 - lr: 1.2346e-05 - 53s/epoch - 273ms/step
Epoch 3/1000
2023-10-25 16:39:59.292 
Epoch 3/1000 
	 loss: 211.9479, MinusLogProbMetric: 211.9479, val_loss: 205.3610, val_MinusLogProbMetric: 205.3610

Epoch 3: val_loss improved from 221.55728 to 205.36096, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 54s - loss: 211.9479 - MinusLogProbMetric: 211.9479 - val_loss: 205.3610 - val_MinusLogProbMetric: 205.3610 - lr: 1.2346e-05 - 54s/epoch - 278ms/step
Epoch 4/1000
2023-10-25 16:41:01.727 
Epoch 4/1000 
	 loss: 202.3267, MinusLogProbMetric: 202.3267, val_loss: 261.0407, val_MinusLogProbMetric: 261.0407

Epoch 4: val_loss did not improve from 205.36096
196/196 - 62s - loss: 202.3267 - MinusLogProbMetric: 202.3267 - val_loss: 261.0407 - val_MinusLogProbMetric: 261.0407 - lr: 1.2346e-05 - 62s/epoch - 315ms/step
Epoch 5/1000
2023-10-25 16:41:57.375 
Epoch 5/1000 
	 loss: 281.0439, MinusLogProbMetric: 281.0439, val_loss: 225.0759, val_MinusLogProbMetric: 225.0759

Epoch 5: val_loss did not improve from 205.36096
196/196 - 56s - loss: 281.0439 - MinusLogProbMetric: 281.0439 - val_loss: 225.0759 - val_MinusLogProbMetric: 225.0759 - lr: 1.2346e-05 - 56s/epoch - 284ms/step
Epoch 6/1000
2023-10-25 16:42:50.410 
Epoch 6/1000 
	 loss: 209.9469, MinusLogProbMetric: 209.9469, val_loss: 198.8038, val_MinusLogProbMetric: 198.8038

Epoch 6: val_loss improved from 205.36096 to 198.80380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 54s - loss: 209.9469 - MinusLogProbMetric: 209.9469 - val_loss: 198.8038 - val_MinusLogProbMetric: 198.8038 - lr: 1.2346e-05 - 54s/epoch - 275ms/step
Epoch 7/1000
2023-10-25 16:43:51.175 
Epoch 7/1000 
	 loss: 189.2242, MinusLogProbMetric: 189.2242, val_loss: 181.7356, val_MinusLogProbMetric: 181.7356

Epoch 7: val_loss improved from 198.80380 to 181.73560, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 61s - loss: 189.2242 - MinusLogProbMetric: 189.2242 - val_loss: 181.7356 - val_MinusLogProbMetric: 181.7356 - lr: 1.2346e-05 - 61s/epoch - 311ms/step
Epoch 8/1000
2023-10-25 16:44:58.427 
Epoch 8/1000 
	 loss: 188.2905, MinusLogProbMetric: 188.2905, val_loss: 183.1317, val_MinusLogProbMetric: 183.1317

Epoch 8: val_loss did not improve from 181.73560
196/196 - 66s - loss: 188.2905 - MinusLogProbMetric: 188.2905 - val_loss: 183.1317 - val_MinusLogProbMetric: 183.1317 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 9/1000
2023-10-25 16:46:04.945 
Epoch 9/1000 
	 loss: 172.1654, MinusLogProbMetric: 172.1654, val_loss: 167.1747, val_MinusLogProbMetric: 167.1747

Epoch 9: val_loss improved from 181.73560 to 167.17465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 172.1654 - MinusLogProbMetric: 172.1654 - val_loss: 167.1747 - val_MinusLogProbMetric: 167.1747 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 10/1000
2023-10-25 16:47:13.919 
Epoch 10/1000 
	 loss: 162.3374, MinusLogProbMetric: 162.3374, val_loss: 160.0709, val_MinusLogProbMetric: 160.0709

Epoch 10: val_loss improved from 167.17465 to 160.07094, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 162.3374 - MinusLogProbMetric: 162.3374 - val_loss: 160.0709 - val_MinusLogProbMetric: 160.0709 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 11/1000
2023-10-25 16:48:23.289 
Epoch 11/1000 
	 loss: 157.2284, MinusLogProbMetric: 157.2284, val_loss: 159.8675, val_MinusLogProbMetric: 159.8675

Epoch 11: val_loss improved from 160.07094 to 159.86749, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 157.2284 - MinusLogProbMetric: 157.2284 - val_loss: 159.8675 - val_MinusLogProbMetric: 159.8675 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 12/1000
2023-10-25 16:49:32.745 
Epoch 12/1000 
	 loss: 152.7942, MinusLogProbMetric: 152.7942, val_loss: 153.0931, val_MinusLogProbMetric: 153.0931

Epoch 12: val_loss improved from 159.86749 to 153.09314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 70s - loss: 152.7942 - MinusLogProbMetric: 152.7942 - val_loss: 153.0931 - val_MinusLogProbMetric: 153.0931 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 13/1000
2023-10-25 16:50:38.379 
Epoch 13/1000 
	 loss: 171.4384, MinusLogProbMetric: 171.4384, val_loss: 202.3465, val_MinusLogProbMetric: 202.3465

Epoch 13: val_loss did not improve from 153.09314
196/196 - 65s - loss: 171.4384 - MinusLogProbMetric: 171.4384 - val_loss: 202.3465 - val_MinusLogProbMetric: 202.3465 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 14/1000
2023-10-25 16:51:39.758 
Epoch 14/1000 
	 loss: 159.0878, MinusLogProbMetric: 159.0878, val_loss: 146.9684, val_MinusLogProbMetric: 146.9684

Epoch 14: val_loss improved from 153.09314 to 146.96838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 62s - loss: 159.0878 - MinusLogProbMetric: 159.0878 - val_loss: 146.9684 - val_MinusLogProbMetric: 146.9684 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 15/1000
2023-10-25 16:52:38.866 
Epoch 15/1000 
	 loss: 143.7973, MinusLogProbMetric: 143.7973, val_loss: 141.3359, val_MinusLogProbMetric: 141.3359

Epoch 15: val_loss improved from 146.96838 to 141.33588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 59s - loss: 143.7973 - MinusLogProbMetric: 143.7973 - val_loss: 141.3359 - val_MinusLogProbMetric: 141.3359 - lr: 1.2346e-05 - 59s/epoch - 303ms/step
Epoch 16/1000
2023-10-25 16:53:47.019 
Epoch 16/1000 
	 loss: 139.5002, MinusLogProbMetric: 139.5002, val_loss: 137.0721, val_MinusLogProbMetric: 137.0721

Epoch 16: val_loss improved from 141.33588 to 137.07213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 139.5002 - MinusLogProbMetric: 139.5002 - val_loss: 137.0721 - val_MinusLogProbMetric: 137.0721 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 17/1000
2023-10-25 16:54:55.420 
Epoch 17/1000 
	 loss: 134.8878, MinusLogProbMetric: 134.8878, val_loss: 133.1096, val_MinusLogProbMetric: 133.1096

Epoch 17: val_loss improved from 137.07213 to 133.10962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 134.8878 - MinusLogProbMetric: 134.8878 - val_loss: 133.1096 - val_MinusLogProbMetric: 133.1096 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 18/1000
2023-10-25 16:56:00.644 
Epoch 18/1000 
	 loss: 131.9524, MinusLogProbMetric: 131.9524, val_loss: 130.5414, val_MinusLogProbMetric: 130.5414

Epoch 18: val_loss improved from 133.10962 to 130.54138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 65s - loss: 131.9524 - MinusLogProbMetric: 131.9524 - val_loss: 130.5414 - val_MinusLogProbMetric: 130.5414 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 19/1000
2023-10-25 16:57:03.068 
Epoch 19/1000 
	 loss: 130.0696, MinusLogProbMetric: 130.0696, val_loss: 128.7518, val_MinusLogProbMetric: 128.7518

Epoch 19: val_loss improved from 130.54138 to 128.75179, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 62s - loss: 130.0696 - MinusLogProbMetric: 130.0696 - val_loss: 128.7518 - val_MinusLogProbMetric: 128.7518 - lr: 1.2346e-05 - 62s/epoch - 319ms/step
Epoch 20/1000
2023-10-25 16:58:07.511 
Epoch 20/1000 
	 loss: 125.1522, MinusLogProbMetric: 125.1522, val_loss: 122.8115, val_MinusLogProbMetric: 122.8115

Epoch 20: val_loss improved from 128.75179 to 122.81153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 65s - loss: 125.1522 - MinusLogProbMetric: 125.1522 - val_loss: 122.8115 - val_MinusLogProbMetric: 122.8115 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 21/1000
2023-10-25 16:59:16.557 
Epoch 21/1000 
	 loss: 121.8342, MinusLogProbMetric: 121.8342, val_loss: 122.1617, val_MinusLogProbMetric: 122.1617

Epoch 21: val_loss improved from 122.81153 to 122.16169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 121.8342 - MinusLogProbMetric: 121.8342 - val_loss: 122.1617 - val_MinusLogProbMetric: 122.1617 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 22/1000
2023-10-25 17:00:25.589 
Epoch 22/1000 
	 loss: 119.7123, MinusLogProbMetric: 119.7123, val_loss: 119.8705, val_MinusLogProbMetric: 119.8705

Epoch 22: val_loss improved from 122.16169 to 119.87052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 119.7123 - MinusLogProbMetric: 119.7123 - val_loss: 119.8705 - val_MinusLogProbMetric: 119.8705 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 23/1000
2023-10-25 17:01:34.407 
Epoch 23/1000 
	 loss: 116.5663, MinusLogProbMetric: 116.5663, val_loss: 115.3803, val_MinusLogProbMetric: 115.3803

Epoch 23: val_loss improved from 119.87052 to 115.38034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 116.5663 - MinusLogProbMetric: 116.5663 - val_loss: 115.3803 - val_MinusLogProbMetric: 115.3803 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 24/1000
2023-10-25 17:02:42.285 
Epoch 24/1000 
	 loss: 114.2914, MinusLogProbMetric: 114.2914, val_loss: 114.7346, val_MinusLogProbMetric: 114.7346

Epoch 24: val_loss improved from 115.38034 to 114.73457, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 114.2914 - MinusLogProbMetric: 114.2914 - val_loss: 114.7346 - val_MinusLogProbMetric: 114.7346 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 25/1000
2023-10-25 17:03:47.753 
Epoch 25/1000 
	 loss: 112.5186, MinusLogProbMetric: 112.5186, val_loss: 112.3356, val_MinusLogProbMetric: 112.3356

Epoch 25: val_loss improved from 114.73457 to 112.33562, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 66s - loss: 112.5186 - MinusLogProbMetric: 112.5186 - val_loss: 112.3356 - val_MinusLogProbMetric: 112.3356 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 26/1000
2023-10-25 17:04:52.673 
Epoch 26/1000 
	 loss: 113.5868, MinusLogProbMetric: 113.5868, val_loss: 111.5563, val_MinusLogProbMetric: 111.5563

Epoch 26: val_loss improved from 112.33562 to 111.55634, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 65s - loss: 113.5868 - MinusLogProbMetric: 113.5868 - val_loss: 111.5563 - val_MinusLogProbMetric: 111.5563 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 27/1000
2023-10-25 17:06:00.739 
Epoch 27/1000 
	 loss: 109.8827, MinusLogProbMetric: 109.8827, val_loss: 110.3244, val_MinusLogProbMetric: 110.3244

Epoch 27: val_loss improved from 111.55634 to 110.32442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 109.8827 - MinusLogProbMetric: 109.8827 - val_loss: 110.3244 - val_MinusLogProbMetric: 110.3244 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 28/1000
2023-10-25 17:07:09.538 
Epoch 28/1000 
	 loss: 108.5156, MinusLogProbMetric: 108.5156, val_loss: 106.6723, val_MinusLogProbMetric: 106.6723

Epoch 28: val_loss improved from 110.32442 to 106.67235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 108.5156 - MinusLogProbMetric: 108.5156 - val_loss: 106.6723 - val_MinusLogProbMetric: 106.6723 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 29/1000
2023-10-25 17:08:17.639 
Epoch 29/1000 
	 loss: 110.6725, MinusLogProbMetric: 110.6725, val_loss: 110.4119, val_MinusLogProbMetric: 110.4119

Epoch 29: val_loss did not improve from 106.67235
196/196 - 67s - loss: 110.6725 - MinusLogProbMetric: 110.6725 - val_loss: 110.4119 - val_MinusLogProbMetric: 110.4119 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 30/1000
2023-10-25 17:09:21.808 
Epoch 30/1000 
	 loss: 105.0011, MinusLogProbMetric: 105.0011, val_loss: 103.1068, val_MinusLogProbMetric: 103.1068

Epoch 30: val_loss improved from 106.67235 to 103.10684, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 65s - loss: 105.0011 - MinusLogProbMetric: 105.0011 - val_loss: 103.1068 - val_MinusLogProbMetric: 103.1068 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 31/1000
2023-10-25 17:10:28.306 
Epoch 31/1000 
	 loss: 106.5127, MinusLogProbMetric: 106.5127, val_loss: 104.5484, val_MinusLogProbMetric: 104.5484

Epoch 31: val_loss did not improve from 103.10684
196/196 - 66s - loss: 106.5127 - MinusLogProbMetric: 106.5127 - val_loss: 104.5484 - val_MinusLogProbMetric: 104.5484 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 32/1000
2023-10-25 17:11:27.305 
Epoch 32/1000 
	 loss: 102.3149, MinusLogProbMetric: 102.3149, val_loss: 102.2854, val_MinusLogProbMetric: 102.2854

Epoch 32: val_loss improved from 103.10684 to 102.28535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 60s - loss: 102.3149 - MinusLogProbMetric: 102.3149 - val_loss: 102.2854 - val_MinusLogProbMetric: 102.2854 - lr: 1.2346e-05 - 60s/epoch - 307ms/step
Epoch 33/1000
2023-10-25 17:12:37.015 
Epoch 33/1000 
	 loss: 271.2042, MinusLogProbMetric: 271.2042, val_loss: 224.3826, val_MinusLogProbMetric: 224.3826

Epoch 33: val_loss did not improve from 102.28535
196/196 - 69s - loss: 271.2042 - MinusLogProbMetric: 271.2042 - val_loss: 224.3826 - val_MinusLogProbMetric: 224.3826 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 34/1000
2023-10-25 17:13:45.859 
Epoch 34/1000 
	 loss: 278.1825, MinusLogProbMetric: 278.1825, val_loss: 219.1318, val_MinusLogProbMetric: 219.1318

Epoch 34: val_loss did not improve from 102.28535
196/196 - 69s - loss: 278.1825 - MinusLogProbMetric: 278.1825 - val_loss: 219.1318 - val_MinusLogProbMetric: 219.1318 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 35/1000
2023-10-25 17:14:54.162 
Epoch 35/1000 
	 loss: 199.8105, MinusLogProbMetric: 199.8105, val_loss: 184.5734, val_MinusLogProbMetric: 184.5734

Epoch 35: val_loss did not improve from 102.28535
196/196 - 68s - loss: 199.8105 - MinusLogProbMetric: 199.8105 - val_loss: 184.5734 - val_MinusLogProbMetric: 184.5734 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 36/1000
2023-10-25 17:16:02.878 
Epoch 36/1000 
	 loss: 176.0828, MinusLogProbMetric: 176.0828, val_loss: 169.8452, val_MinusLogProbMetric: 169.8452

Epoch 36: val_loss did not improve from 102.28535
196/196 - 69s - loss: 176.0828 - MinusLogProbMetric: 176.0828 - val_loss: 169.8452 - val_MinusLogProbMetric: 169.8452 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 37/1000
2023-10-25 17:17:11.184 
Epoch 37/1000 
	 loss: 162.8519, MinusLogProbMetric: 162.8519, val_loss: 157.3281, val_MinusLogProbMetric: 157.3281

Epoch 37: val_loss did not improve from 102.28535
196/196 - 68s - loss: 162.8519 - MinusLogProbMetric: 162.8519 - val_loss: 157.3281 - val_MinusLogProbMetric: 157.3281 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 38/1000
2023-10-25 17:18:19.720 
Epoch 38/1000 
	 loss: 172.2913, MinusLogProbMetric: 172.2913, val_loss: 157.2980, val_MinusLogProbMetric: 157.2980

Epoch 38: val_loss did not improve from 102.28535
196/196 - 69s - loss: 172.2913 - MinusLogProbMetric: 172.2913 - val_loss: 157.2980 - val_MinusLogProbMetric: 157.2980 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 39/1000
2023-10-25 17:19:28.209 
Epoch 39/1000 
	 loss: 234.3955, MinusLogProbMetric: 234.3955, val_loss: 385.3317, val_MinusLogProbMetric: 385.3317

Epoch 39: val_loss did not improve from 102.28535
196/196 - 68s - loss: 234.3955 - MinusLogProbMetric: 234.3955 - val_loss: 385.3317 - val_MinusLogProbMetric: 385.3317 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 40/1000
2023-10-25 17:20:36.790 
Epoch 40/1000 
	 loss: 236.3854, MinusLogProbMetric: 236.3854, val_loss: 191.4258, val_MinusLogProbMetric: 191.4258

Epoch 40: val_loss did not improve from 102.28535
196/196 - 69s - loss: 236.3854 - MinusLogProbMetric: 236.3854 - val_loss: 191.4258 - val_MinusLogProbMetric: 191.4258 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 41/1000
2023-10-25 17:21:45.606 
Epoch 41/1000 
	 loss: 179.3206, MinusLogProbMetric: 179.3206, val_loss: 169.6084, val_MinusLogProbMetric: 169.6084

Epoch 41: val_loss did not improve from 102.28535
196/196 - 69s - loss: 179.3206 - MinusLogProbMetric: 179.3206 - val_loss: 169.6084 - val_MinusLogProbMetric: 169.6084 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 42/1000
2023-10-25 17:22:54.617 
Epoch 42/1000 
	 loss: 163.8013, MinusLogProbMetric: 163.8013, val_loss: 158.9969, val_MinusLogProbMetric: 158.9969

Epoch 42: val_loss did not improve from 102.28535
196/196 - 69s - loss: 163.8013 - MinusLogProbMetric: 163.8013 - val_loss: 158.9969 - val_MinusLogProbMetric: 158.9969 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 43/1000
2023-10-25 17:24:03.146 
Epoch 43/1000 
	 loss: 154.3036, MinusLogProbMetric: 154.3036, val_loss: 149.9461, val_MinusLogProbMetric: 149.9461

Epoch 43: val_loss did not improve from 102.28535
196/196 - 69s - loss: 154.3036 - MinusLogProbMetric: 154.3036 - val_loss: 149.9461 - val_MinusLogProbMetric: 149.9461 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 44/1000
2023-10-25 17:25:11.338 
Epoch 44/1000 
	 loss: 153.2153, MinusLogProbMetric: 153.2153, val_loss: 151.5853, val_MinusLogProbMetric: 151.5853

Epoch 44: val_loss did not improve from 102.28535
196/196 - 68s - loss: 153.2153 - MinusLogProbMetric: 153.2153 - val_loss: 151.5853 - val_MinusLogProbMetric: 151.5853 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 45/1000
2023-10-25 17:26:19.722 
Epoch 45/1000 
	 loss: 145.0374, MinusLogProbMetric: 145.0374, val_loss: 141.2347, val_MinusLogProbMetric: 141.2347

Epoch 45: val_loss did not improve from 102.28535
196/196 - 68s - loss: 145.0374 - MinusLogProbMetric: 145.0374 - val_loss: 141.2347 - val_MinusLogProbMetric: 141.2347 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 46/1000
2023-10-25 17:27:28.799 
Epoch 46/1000 
	 loss: 139.0123, MinusLogProbMetric: 139.0123, val_loss: 136.7226, val_MinusLogProbMetric: 136.7226

Epoch 46: val_loss did not improve from 102.28535
196/196 - 69s - loss: 139.0123 - MinusLogProbMetric: 139.0123 - val_loss: 136.7226 - val_MinusLogProbMetric: 136.7226 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 47/1000
2023-10-25 17:28:37.447 
Epoch 47/1000 
	 loss: 134.5837, MinusLogProbMetric: 134.5837, val_loss: 132.4798, val_MinusLogProbMetric: 132.4798

Epoch 47: val_loss did not improve from 102.28535
196/196 - 69s - loss: 134.5837 - MinusLogProbMetric: 134.5837 - val_loss: 132.4798 - val_MinusLogProbMetric: 132.4798 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 48/1000
2023-10-25 17:29:46.455 
Epoch 48/1000 
	 loss: 130.7973, MinusLogProbMetric: 130.7973, val_loss: 129.0549, val_MinusLogProbMetric: 129.0549

Epoch 48: val_loss did not improve from 102.28535
196/196 - 69s - loss: 130.7973 - MinusLogProbMetric: 130.7973 - val_loss: 129.0549 - val_MinusLogProbMetric: 129.0549 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 49/1000
2023-10-25 17:30:55.928 
Epoch 49/1000 
	 loss: 127.6973, MinusLogProbMetric: 127.6973, val_loss: 126.3887, val_MinusLogProbMetric: 126.3887

Epoch 49: val_loss did not improve from 102.28535
196/196 - 69s - loss: 127.6973 - MinusLogProbMetric: 127.6973 - val_loss: 126.3887 - val_MinusLogProbMetric: 126.3887 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 50/1000
2023-10-25 17:32:05.426 
Epoch 50/1000 
	 loss: 124.9464, MinusLogProbMetric: 124.9464, val_loss: 123.5709, val_MinusLogProbMetric: 123.5709

Epoch 50: val_loss did not improve from 102.28535
196/196 - 69s - loss: 124.9464 - MinusLogProbMetric: 124.9464 - val_loss: 123.5709 - val_MinusLogProbMetric: 123.5709 - lr: 1.2346e-05 - 69s/epoch - 355ms/step
Epoch 51/1000
2023-10-25 17:33:14.511 
Epoch 51/1000 
	 loss: 122.6593, MinusLogProbMetric: 122.6593, val_loss: 122.2727, val_MinusLogProbMetric: 122.2727

Epoch 51: val_loss did not improve from 102.28535
196/196 - 69s - loss: 122.6593 - MinusLogProbMetric: 122.6593 - val_loss: 122.2727 - val_MinusLogProbMetric: 122.2727 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 52/1000
2023-10-25 17:34:23.848 
Epoch 52/1000 
	 loss: 161.4345, MinusLogProbMetric: 161.4345, val_loss: 165.2760, val_MinusLogProbMetric: 165.2760

Epoch 52: val_loss did not improve from 102.28535
196/196 - 69s - loss: 161.4345 - MinusLogProbMetric: 161.4345 - val_loss: 165.2760 - val_MinusLogProbMetric: 165.2760 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 53/1000
2023-10-25 17:35:32.799 
Epoch 53/1000 
	 loss: 145.2868, MinusLogProbMetric: 145.2868, val_loss: 135.3764, val_MinusLogProbMetric: 135.3764

Epoch 53: val_loss did not improve from 102.28535
196/196 - 69s - loss: 145.2868 - MinusLogProbMetric: 145.2868 - val_loss: 135.3764 - val_MinusLogProbMetric: 135.3764 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 54/1000
2023-10-25 17:36:41.384 
Epoch 54/1000 
	 loss: 131.3981, MinusLogProbMetric: 131.3981, val_loss: 126.9261, val_MinusLogProbMetric: 126.9261

Epoch 54: val_loss did not improve from 102.28535
196/196 - 69s - loss: 131.3981 - MinusLogProbMetric: 131.3981 - val_loss: 126.9261 - val_MinusLogProbMetric: 126.9261 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 55/1000
2023-10-25 17:37:49.918 
Epoch 55/1000 
	 loss: 124.1295, MinusLogProbMetric: 124.1295, val_loss: 121.5567, val_MinusLogProbMetric: 121.5567

Epoch 55: val_loss did not improve from 102.28535
196/196 - 69s - loss: 124.1295 - MinusLogProbMetric: 124.1295 - val_loss: 121.5567 - val_MinusLogProbMetric: 121.5567 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 56/1000
2023-10-25 17:38:59.751 
Epoch 56/1000 
	 loss: 119.8300, MinusLogProbMetric: 119.8300, val_loss: 118.0558, val_MinusLogProbMetric: 118.0558

Epoch 56: val_loss did not improve from 102.28535
196/196 - 70s - loss: 119.8300 - MinusLogProbMetric: 119.8300 - val_loss: 118.0558 - val_MinusLogProbMetric: 118.0558 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 57/1000
2023-10-25 17:40:08.240 
Epoch 57/1000 
	 loss: 116.7252, MinusLogProbMetric: 116.7252, val_loss: 115.6672, val_MinusLogProbMetric: 115.6672

Epoch 57: val_loss did not improve from 102.28535
196/196 - 68s - loss: 116.7252 - MinusLogProbMetric: 116.7252 - val_loss: 115.6672 - val_MinusLogProbMetric: 115.6672 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 58/1000
2023-10-25 17:41:17.366 
Epoch 58/1000 
	 loss: 114.7047, MinusLogProbMetric: 114.7047, val_loss: 113.8408, val_MinusLogProbMetric: 113.8408

Epoch 58: val_loss did not improve from 102.28535
196/196 - 69s - loss: 114.7047 - MinusLogProbMetric: 114.7047 - val_loss: 113.8408 - val_MinusLogProbMetric: 113.8408 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 59/1000
2023-10-25 17:42:26.699 
Epoch 59/1000 
	 loss: 112.6660, MinusLogProbMetric: 112.6660, val_loss: 111.7764, val_MinusLogProbMetric: 111.7764

Epoch 59: val_loss did not improve from 102.28535
196/196 - 69s - loss: 112.6660 - MinusLogProbMetric: 112.6660 - val_loss: 111.7764 - val_MinusLogProbMetric: 111.7764 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 60/1000
2023-10-25 17:43:35.827 
Epoch 60/1000 
	 loss: 110.8594, MinusLogProbMetric: 110.8594, val_loss: 110.0675, val_MinusLogProbMetric: 110.0675

Epoch 60: val_loss did not improve from 102.28535
196/196 - 69s - loss: 110.8594 - MinusLogProbMetric: 110.8594 - val_loss: 110.0675 - val_MinusLogProbMetric: 110.0675 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 61/1000
2023-10-25 17:44:44.896 
Epoch 61/1000 
	 loss: 133.0548, MinusLogProbMetric: 133.0548, val_loss: 133.4188, val_MinusLogProbMetric: 133.4188

Epoch 61: val_loss did not improve from 102.28535
196/196 - 69s - loss: 133.0548 - MinusLogProbMetric: 133.0548 - val_loss: 133.4188 - val_MinusLogProbMetric: 133.4188 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 62/1000
2023-10-25 17:45:54.134 
Epoch 62/1000 
	 loss: 118.4825, MinusLogProbMetric: 118.4825, val_loss: 112.2486, val_MinusLogProbMetric: 112.2486

Epoch 62: val_loss did not improve from 102.28535
196/196 - 69s - loss: 118.4825 - MinusLogProbMetric: 118.4825 - val_loss: 112.2486 - val_MinusLogProbMetric: 112.2486 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 63/1000
2023-10-25 17:47:02.681 
Epoch 63/1000 
	 loss: 110.7685, MinusLogProbMetric: 110.7685, val_loss: 109.5192, val_MinusLogProbMetric: 109.5192

Epoch 63: val_loss did not improve from 102.28535
196/196 - 69s - loss: 110.7685 - MinusLogProbMetric: 110.7685 - val_loss: 109.5192 - val_MinusLogProbMetric: 109.5192 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 64/1000
2023-10-25 17:48:11.461 
Epoch 64/1000 
	 loss: 107.5725, MinusLogProbMetric: 107.5725, val_loss: 106.6342, val_MinusLogProbMetric: 106.6342

Epoch 64: val_loss did not improve from 102.28535
196/196 - 69s - loss: 107.5725 - MinusLogProbMetric: 107.5725 - val_loss: 106.6342 - val_MinusLogProbMetric: 106.6342 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 65/1000
2023-10-25 17:49:20.319 
Epoch 65/1000 
	 loss: 107.9155, MinusLogProbMetric: 107.9155, val_loss: 107.0735, val_MinusLogProbMetric: 107.0735

Epoch 65: val_loss did not improve from 102.28535
196/196 - 69s - loss: 107.9155 - MinusLogProbMetric: 107.9155 - val_loss: 107.0735 - val_MinusLogProbMetric: 107.0735 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 66/1000
2023-10-25 17:50:29.129 
Epoch 66/1000 
	 loss: 105.9999, MinusLogProbMetric: 105.9999, val_loss: 104.5921, val_MinusLogProbMetric: 104.5921

Epoch 66: val_loss did not improve from 102.28535
196/196 - 69s - loss: 105.9999 - MinusLogProbMetric: 105.9999 - val_loss: 104.5921 - val_MinusLogProbMetric: 104.5921 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 67/1000
2023-10-25 17:51:37.889 
Epoch 67/1000 
	 loss: 104.0024, MinusLogProbMetric: 104.0024, val_loss: 104.2339, val_MinusLogProbMetric: 104.2339

Epoch 67: val_loss did not improve from 102.28535
196/196 - 69s - loss: 104.0024 - MinusLogProbMetric: 104.0024 - val_loss: 104.2339 - val_MinusLogProbMetric: 104.2339 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 68/1000
2023-10-25 17:52:47.003 
Epoch 68/1000 
	 loss: 103.0623, MinusLogProbMetric: 103.0623, val_loss: 101.6800, val_MinusLogProbMetric: 101.6800

Epoch 68: val_loss improved from 102.28535 to 101.67995, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 70s - loss: 103.0623 - MinusLogProbMetric: 103.0623 - val_loss: 101.6800 - val_MinusLogProbMetric: 101.6800 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 69/1000
2023-10-25 17:53:56.899 
Epoch 69/1000 
	 loss: 101.3351, MinusLogProbMetric: 101.3351, val_loss: 101.8005, val_MinusLogProbMetric: 101.8005

Epoch 69: val_loss did not improve from 101.67995
196/196 - 69s - loss: 101.3351 - MinusLogProbMetric: 101.3351 - val_loss: 101.8005 - val_MinusLogProbMetric: 101.8005 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 70/1000
2023-10-25 17:55:05.820 
Epoch 70/1000 
	 loss: 100.1215, MinusLogProbMetric: 100.1215, val_loss: 99.3979, val_MinusLogProbMetric: 99.3979

Epoch 70: val_loss improved from 101.67995 to 99.39793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 70s - loss: 100.1215 - MinusLogProbMetric: 100.1215 - val_loss: 99.3979 - val_MinusLogProbMetric: 99.3979 - lr: 1.2346e-05 - 70s/epoch - 358ms/step
Epoch 71/1000
2023-10-25 17:56:03.551 
Epoch 71/1000 
	 loss: 98.8126, MinusLogProbMetric: 98.8126, val_loss: 98.4715, val_MinusLogProbMetric: 98.4715

Epoch 71: val_loss improved from 99.39793 to 98.47150, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 57s - loss: 98.8126 - MinusLogProbMetric: 98.8126 - val_loss: 98.4715 - val_MinusLogProbMetric: 98.4715 - lr: 1.2346e-05 - 57s/epoch - 292ms/step
Epoch 72/1000
2023-10-25 17:57:06.340 
Epoch 72/1000 
	 loss: 97.9658, MinusLogProbMetric: 97.9658, val_loss: 97.4643, val_MinusLogProbMetric: 97.4643

Epoch 72: val_loss improved from 98.47150 to 97.46431, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 63s - loss: 97.9658 - MinusLogProbMetric: 97.9658 - val_loss: 97.4643 - val_MinusLogProbMetric: 97.4643 - lr: 1.2346e-05 - 63s/epoch - 321ms/step
Epoch 73/1000
2023-10-25 17:58:13.109 
Epoch 73/1000 
	 loss: 97.0575, MinusLogProbMetric: 97.0575, val_loss: 96.7720, val_MinusLogProbMetric: 96.7720

Epoch 73: val_loss improved from 97.46431 to 96.77202, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 67s - loss: 97.0575 - MinusLogProbMetric: 97.0575 - val_loss: 96.7720 - val_MinusLogProbMetric: 96.7720 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 74/1000
2023-10-25 17:59:17.943 
Epoch 74/1000 
	 loss: 110.2120, MinusLogProbMetric: 110.2120, val_loss: 330.7751, val_MinusLogProbMetric: 330.7751

Epoch 74: val_loss did not improve from 96.77202
196/196 - 64s - loss: 110.2120 - MinusLogProbMetric: 110.2120 - val_loss: 330.7751 - val_MinusLogProbMetric: 330.7751 - lr: 1.2346e-05 - 64s/epoch - 327ms/step
Epoch 75/1000
2023-10-25 18:00:21.883 
Epoch 75/1000 
	 loss: 254.4673, MinusLogProbMetric: 254.4673, val_loss: 159.2381, val_MinusLogProbMetric: 159.2381

Epoch 75: val_loss did not improve from 96.77202
196/196 - 64s - loss: 254.4673 - MinusLogProbMetric: 254.4673 - val_loss: 159.2381 - val_MinusLogProbMetric: 159.2381 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 76/1000
2023-10-25 18:01:25.557 
Epoch 76/1000 
	 loss: 147.0338, MinusLogProbMetric: 147.0338, val_loss: 136.9994, val_MinusLogProbMetric: 136.9994

Epoch 76: val_loss did not improve from 96.77202
196/196 - 64s - loss: 147.0338 - MinusLogProbMetric: 147.0338 - val_loss: 136.9994 - val_MinusLogProbMetric: 136.9994 - lr: 1.2346e-05 - 64s/epoch - 325ms/step
Epoch 77/1000
2023-10-25 18:02:23.121 
Epoch 77/1000 
	 loss: 135.2597, MinusLogProbMetric: 135.2597, val_loss: 128.8537, val_MinusLogProbMetric: 128.8537

Epoch 77: val_loss did not improve from 96.77202
196/196 - 58s - loss: 135.2597 - MinusLogProbMetric: 135.2597 - val_loss: 128.8537 - val_MinusLogProbMetric: 128.8537 - lr: 1.2346e-05 - 58s/epoch - 294ms/step
Epoch 78/1000
2023-10-25 18:03:29.742 
Epoch 78/1000 
	 loss: 125.0808, MinusLogProbMetric: 125.0808, val_loss: 122.0376, val_MinusLogProbMetric: 122.0376

Epoch 78: val_loss did not improve from 96.77202
196/196 - 67s - loss: 125.0808 - MinusLogProbMetric: 125.0808 - val_loss: 122.0376 - val_MinusLogProbMetric: 122.0376 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 79/1000
2023-10-25 18:04:28.558 
Epoch 79/1000 
	 loss: 119.7962, MinusLogProbMetric: 119.7962, val_loss: 117.5499, val_MinusLogProbMetric: 117.5499

Epoch 79: val_loss did not improve from 96.77202
196/196 - 59s - loss: 119.7962 - MinusLogProbMetric: 119.7962 - val_loss: 117.5499 - val_MinusLogProbMetric: 117.5499 - lr: 1.2346e-05 - 59s/epoch - 300ms/step
Epoch 80/1000
2023-10-25 18:05:31.395 
Epoch 80/1000 
	 loss: 161.9821, MinusLogProbMetric: 161.9821, val_loss: 136.4460, val_MinusLogProbMetric: 136.4460

Epoch 80: val_loss did not improve from 96.77202
196/196 - 63s - loss: 161.9821 - MinusLogProbMetric: 161.9821 - val_loss: 136.4460 - val_MinusLogProbMetric: 136.4460 - lr: 1.2346e-05 - 63s/epoch - 321ms/step
Epoch 81/1000
2023-10-25 18:06:38.265 
Epoch 81/1000 
	 loss: 130.7128, MinusLogProbMetric: 130.7128, val_loss: 126.6459, val_MinusLogProbMetric: 126.6459

Epoch 81: val_loss did not improve from 96.77202
196/196 - 67s - loss: 130.7128 - MinusLogProbMetric: 130.7128 - val_loss: 126.6459 - val_MinusLogProbMetric: 126.6459 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 82/1000
2023-10-25 18:07:43.486 
Epoch 82/1000 
	 loss: 124.6565, MinusLogProbMetric: 124.6565, val_loss: 122.4889, val_MinusLogProbMetric: 122.4889

Epoch 82: val_loss did not improve from 96.77202
196/196 - 65s - loss: 124.6565 - MinusLogProbMetric: 124.6565 - val_loss: 122.4889 - val_MinusLogProbMetric: 122.4889 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 83/1000
2023-10-25 18:08:46.937 
Epoch 83/1000 
	 loss: 120.8092, MinusLogProbMetric: 120.8092, val_loss: 118.1854, val_MinusLogProbMetric: 118.1854

Epoch 83: val_loss did not improve from 96.77202
196/196 - 63s - loss: 120.8092 - MinusLogProbMetric: 120.8092 - val_loss: 118.1854 - val_MinusLogProbMetric: 118.1854 - lr: 1.2346e-05 - 63s/epoch - 324ms/step
Epoch 84/1000
2023-10-25 18:09:52.654 
Epoch 84/1000 
	 loss: 116.3841, MinusLogProbMetric: 116.3841, val_loss: 114.4908, val_MinusLogProbMetric: 114.4908

Epoch 84: val_loss did not improve from 96.77202
196/196 - 66s - loss: 116.3841 - MinusLogProbMetric: 116.3841 - val_loss: 114.4908 - val_MinusLogProbMetric: 114.4908 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 85/1000
2023-10-25 18:10:53.979 
Epoch 85/1000 
	 loss: 113.2977, MinusLogProbMetric: 113.2977, val_loss: 111.9621, val_MinusLogProbMetric: 111.9621

Epoch 85: val_loss did not improve from 96.77202
196/196 - 61s - loss: 113.2977 - MinusLogProbMetric: 113.2977 - val_loss: 111.9621 - val_MinusLogProbMetric: 111.9621 - lr: 1.2346e-05 - 61s/epoch - 313ms/step
Epoch 86/1000
2023-10-25 18:11:56.051 
Epoch 86/1000 
	 loss: 110.7278, MinusLogProbMetric: 110.7278, val_loss: 109.7900, val_MinusLogProbMetric: 109.7900

Epoch 86: val_loss did not improve from 96.77202
196/196 - 62s - loss: 110.7278 - MinusLogProbMetric: 110.7278 - val_loss: 109.7900 - val_MinusLogProbMetric: 109.7900 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 87/1000
2023-10-25 18:12:57.034 
Epoch 87/1000 
	 loss: 139.0744, MinusLogProbMetric: 139.0744, val_loss: 113.8295, val_MinusLogProbMetric: 113.8295

Epoch 87: val_loss did not improve from 96.77202
196/196 - 61s - loss: 139.0744 - MinusLogProbMetric: 139.0744 - val_loss: 113.8295 - val_MinusLogProbMetric: 113.8295 - lr: 1.2346e-05 - 61s/epoch - 311ms/step
Epoch 88/1000
2023-10-25 18:13:55.913 
Epoch 88/1000 
	 loss: 109.0855, MinusLogProbMetric: 109.0855, val_loss: 106.3975, val_MinusLogProbMetric: 106.3975

Epoch 88: val_loss did not improve from 96.77202
196/196 - 59s - loss: 109.0855 - MinusLogProbMetric: 109.0855 - val_loss: 106.3975 - val_MinusLogProbMetric: 106.3975 - lr: 1.2346e-05 - 59s/epoch - 300ms/step
Epoch 89/1000
2023-10-25 18:15:02.666 
Epoch 89/1000 
	 loss: 104.6991, MinusLogProbMetric: 104.6991, val_loss: 103.6685, val_MinusLogProbMetric: 103.6685

Epoch 89: val_loss did not improve from 96.77202
196/196 - 67s - loss: 104.6991 - MinusLogProbMetric: 104.6991 - val_loss: 103.6685 - val_MinusLogProbMetric: 103.6685 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 90/1000
2023-10-25 18:16:05.125 
Epoch 90/1000 
	 loss: 102.2800, MinusLogProbMetric: 102.2800, val_loss: 101.2814, val_MinusLogProbMetric: 101.2814

Epoch 90: val_loss did not improve from 96.77202
196/196 - 62s - loss: 102.2800 - MinusLogProbMetric: 102.2800 - val_loss: 101.2814 - val_MinusLogProbMetric: 101.2814 - lr: 1.2346e-05 - 62s/epoch - 319ms/step
Epoch 91/1000
2023-10-25 18:17:05.390 
Epoch 91/1000 
	 loss: 100.4642, MinusLogProbMetric: 100.4642, val_loss: 99.6823, val_MinusLogProbMetric: 99.6823

Epoch 91: val_loss did not improve from 96.77202
196/196 - 60s - loss: 100.4642 - MinusLogProbMetric: 100.4642 - val_loss: 99.6823 - val_MinusLogProbMetric: 99.6823 - lr: 1.2346e-05 - 60s/epoch - 307ms/step
Epoch 92/1000
2023-10-25 18:18:12.960 
Epoch 92/1000 
	 loss: 99.2801, MinusLogProbMetric: 99.2801, val_loss: 98.9755, val_MinusLogProbMetric: 98.9755

Epoch 92: val_loss did not improve from 96.77202
196/196 - 68s - loss: 99.2801 - MinusLogProbMetric: 99.2801 - val_loss: 98.9755 - val_MinusLogProbMetric: 98.9755 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 93/1000
2023-10-25 18:19:15.420 
Epoch 93/1000 
	 loss: 98.0072, MinusLogProbMetric: 98.0072, val_loss: 97.0425, val_MinusLogProbMetric: 97.0425

Epoch 93: val_loss did not improve from 96.77202
196/196 - 62s - loss: 98.0072 - MinusLogProbMetric: 98.0072 - val_loss: 97.0425 - val_MinusLogProbMetric: 97.0425 - lr: 1.2346e-05 - 62s/epoch - 319ms/step
Epoch 94/1000
2023-10-25 18:20:19.706 
Epoch 94/1000 
	 loss: 96.2768, MinusLogProbMetric: 96.2768, val_loss: 95.7834, val_MinusLogProbMetric: 95.7834

Epoch 94: val_loss improved from 96.77202 to 95.78345, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 65s - loss: 96.2768 - MinusLogProbMetric: 96.2768 - val_loss: 95.7834 - val_MinusLogProbMetric: 95.7834 - lr: 1.2346e-05 - 65s/epoch - 333ms/step
Epoch 95/1000
2023-10-25 18:21:26.855 
Epoch 95/1000 
	 loss: 95.1518, MinusLogProbMetric: 95.1518, val_loss: 94.7251, val_MinusLogProbMetric: 94.7251

Epoch 95: val_loss improved from 95.78345 to 94.72506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 67s - loss: 95.1518 - MinusLogProbMetric: 95.1518 - val_loss: 94.7251 - val_MinusLogProbMetric: 94.7251 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 96/1000
2023-10-25 18:22:30.791 
Epoch 96/1000 
	 loss: 94.0069, MinusLogProbMetric: 94.0069, val_loss: 93.9771, val_MinusLogProbMetric: 93.9771

Epoch 96: val_loss improved from 94.72506 to 93.97713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 64s - loss: 94.0069 - MinusLogProbMetric: 94.0069 - val_loss: 93.9771 - val_MinusLogProbMetric: 93.9771 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 97/1000
2023-10-25 18:23:32.459 
Epoch 97/1000 
	 loss: 93.1224, MinusLogProbMetric: 93.1224, val_loss: 92.8419, val_MinusLogProbMetric: 92.8419

Epoch 97: val_loss improved from 93.97713 to 92.84193, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 62s - loss: 93.1224 - MinusLogProbMetric: 93.1224 - val_loss: 92.8419 - val_MinusLogProbMetric: 92.8419 - lr: 1.2346e-05 - 62s/epoch - 316ms/step
Epoch 98/1000
2023-10-25 18:24:40.205 
Epoch 98/1000 
	 loss: 92.2196, MinusLogProbMetric: 92.2196, val_loss: 92.0859, val_MinusLogProbMetric: 92.0859

Epoch 98: val_loss improved from 92.84193 to 92.08587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 92.2196 - MinusLogProbMetric: 92.2196 - val_loss: 92.0859 - val_MinusLogProbMetric: 92.0859 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 99/1000
2023-10-25 18:25:40.698 
Epoch 99/1000 
	 loss: 91.6766, MinusLogProbMetric: 91.6766, val_loss: 91.2246, val_MinusLogProbMetric: 91.2246

Epoch 99: val_loss improved from 92.08587 to 91.22456, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 60s - loss: 91.6766 - MinusLogProbMetric: 91.6766 - val_loss: 91.2246 - val_MinusLogProbMetric: 91.2246 - lr: 1.2346e-05 - 60s/epoch - 308ms/step
Epoch 100/1000
2023-10-25 18:26:47.147 
Epoch 100/1000 
	 loss: 91.5451, MinusLogProbMetric: 91.5451, val_loss: 91.5306, val_MinusLogProbMetric: 91.5306

Epoch 100: val_loss did not improve from 91.22456
196/196 - 66s - loss: 91.5451 - MinusLogProbMetric: 91.5451 - val_loss: 91.5306 - val_MinusLogProbMetric: 91.5306 - lr: 1.2346e-05 - 66s/epoch - 335ms/step
Epoch 101/1000
2023-10-25 18:27:47.776 
Epoch 101/1000 
	 loss: 90.9209, MinusLogProbMetric: 90.9209, val_loss: 90.8142, val_MinusLogProbMetric: 90.8142

Epoch 101: val_loss improved from 91.22456 to 90.81416, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 61s - loss: 90.9209 - MinusLogProbMetric: 90.9209 - val_loss: 90.8142 - val_MinusLogProbMetric: 90.8142 - lr: 1.2346e-05 - 61s/epoch - 314ms/step
Epoch 102/1000
2023-10-25 18:28:50.822 
Epoch 102/1000 
	 loss: 90.0371, MinusLogProbMetric: 90.0371, val_loss: 90.1050, val_MinusLogProbMetric: 90.1050

Epoch 102: val_loss improved from 90.81416 to 90.10503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 63s - loss: 90.0371 - MinusLogProbMetric: 90.0371 - val_loss: 90.1050 - val_MinusLogProbMetric: 90.1050 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 103/1000
2023-10-25 18:29:57.591 
Epoch 103/1000 
	 loss: 89.4098, MinusLogProbMetric: 89.4098, val_loss: 89.2123, val_MinusLogProbMetric: 89.2123

Epoch 103: val_loss improved from 90.10503 to 89.21232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 66s - loss: 89.4098 - MinusLogProbMetric: 89.4098 - val_loss: 89.2123 - val_MinusLogProbMetric: 89.2123 - lr: 1.2346e-05 - 66s/epoch - 339ms/step
Epoch 104/1000
2023-10-25 18:30:59.211 
Epoch 104/1000 
	 loss: 89.1085, MinusLogProbMetric: 89.1085, val_loss: 88.7224, val_MinusLogProbMetric: 88.7224

Epoch 104: val_loss improved from 89.21232 to 88.72241, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 62s - loss: 89.1085 - MinusLogProbMetric: 89.1085 - val_loss: 88.7224 - val_MinusLogProbMetric: 88.7224 - lr: 1.2346e-05 - 62s/epoch - 315ms/step
Epoch 105/1000
2023-10-25 18:32:00.668 
Epoch 105/1000 
	 loss: 87.7945, MinusLogProbMetric: 87.7945, val_loss: 87.4876, val_MinusLogProbMetric: 87.4876

Epoch 105: val_loss improved from 88.72241 to 87.48764, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 61s - loss: 87.7945 - MinusLogProbMetric: 87.7945 - val_loss: 87.4876 - val_MinusLogProbMetric: 87.4876 - lr: 1.2346e-05 - 61s/epoch - 313ms/step
Epoch 106/1000
2023-10-25 18:33:09.103 
Epoch 106/1000 
	 loss: 87.2600, MinusLogProbMetric: 87.2600, val_loss: 86.8322, val_MinusLogProbMetric: 86.8322

Epoch 106: val_loss improved from 87.48764 to 86.83218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 87.2600 - MinusLogProbMetric: 87.2600 - val_loss: 86.8322 - val_MinusLogProbMetric: 86.8322 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 107/1000
2023-10-25 18:34:15.169 
Epoch 107/1000 
	 loss: 86.4731, MinusLogProbMetric: 86.4731, val_loss: 86.3498, val_MinusLogProbMetric: 86.3498

Epoch 107: val_loss improved from 86.83218 to 86.34981, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 66s - loss: 86.4731 - MinusLogProbMetric: 86.4731 - val_loss: 86.3498 - val_MinusLogProbMetric: 86.3498 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 108/1000
2023-10-25 18:35:19.637 
Epoch 108/1000 
	 loss: 85.9059, MinusLogProbMetric: 85.9059, val_loss: 85.8084, val_MinusLogProbMetric: 85.8084

Epoch 108: val_loss improved from 86.34981 to 85.80844, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 65s - loss: 85.9059 - MinusLogProbMetric: 85.9059 - val_loss: 85.8084 - val_MinusLogProbMetric: 85.8084 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 109/1000
2023-10-25 18:36:27.872 
Epoch 109/1000 
	 loss: 86.8932, MinusLogProbMetric: 86.8932, val_loss: 87.6433, val_MinusLogProbMetric: 87.6433

Epoch 109: val_loss did not improve from 85.80844
196/196 - 67s - loss: 86.8932 - MinusLogProbMetric: 86.8932 - val_loss: 87.6433 - val_MinusLogProbMetric: 87.6433 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 110/1000
2023-10-25 18:37:26.773 
Epoch 110/1000 
	 loss: 87.9897, MinusLogProbMetric: 87.9897, val_loss: 86.6671, val_MinusLogProbMetric: 86.6671

Epoch 110: val_loss did not improve from 85.80844
196/196 - 59s - loss: 87.9897 - MinusLogProbMetric: 87.9897 - val_loss: 86.6671 - val_MinusLogProbMetric: 86.6671 - lr: 1.2346e-05 - 59s/epoch - 300ms/step
Epoch 111/1000
2023-10-25 18:38:32.163 
Epoch 111/1000 
	 loss: 86.6166, MinusLogProbMetric: 86.6166, val_loss: 86.0068, val_MinusLogProbMetric: 86.0068

Epoch 111: val_loss did not improve from 85.80844
196/196 - 65s - loss: 86.6166 - MinusLogProbMetric: 86.6166 - val_loss: 86.0068 - val_MinusLogProbMetric: 86.0068 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 112/1000
2023-10-25 18:39:37.142 
Epoch 112/1000 
	 loss: 85.1343, MinusLogProbMetric: 85.1343, val_loss: 84.7764, val_MinusLogProbMetric: 84.7764

Epoch 112: val_loss improved from 85.80844 to 84.77635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 66s - loss: 85.1343 - MinusLogProbMetric: 85.1343 - val_loss: 84.7764 - val_MinusLogProbMetric: 84.7764 - lr: 1.2346e-05 - 66s/epoch - 336ms/step
Epoch 113/1000
2023-10-25 18:40:42.229 
Epoch 113/1000 
	 loss: 84.4516, MinusLogProbMetric: 84.4516, val_loss: 84.6775, val_MinusLogProbMetric: 84.6775

Epoch 113: val_loss improved from 84.77635 to 84.67754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 65s - loss: 84.4516 - MinusLogProbMetric: 84.4516 - val_loss: 84.6775 - val_MinusLogProbMetric: 84.6775 - lr: 1.2346e-05 - 65s/epoch - 332ms/step
Epoch 114/1000
2023-10-25 18:41:49.766 
Epoch 114/1000 
	 loss: 83.9059, MinusLogProbMetric: 83.9059, val_loss: 83.6209, val_MinusLogProbMetric: 83.6209

Epoch 114: val_loss improved from 84.67754 to 83.62093, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 67s - loss: 83.9059 - MinusLogProbMetric: 83.9059 - val_loss: 83.6209 - val_MinusLogProbMetric: 83.6209 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 115/1000
2023-10-25 18:42:50.711 
Epoch 115/1000 
	 loss: 83.3115, MinusLogProbMetric: 83.3115, val_loss: 83.2243, val_MinusLogProbMetric: 83.2243

Epoch 115: val_loss improved from 83.62093 to 83.22433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 61s - loss: 83.3115 - MinusLogProbMetric: 83.3115 - val_loss: 83.2243 - val_MinusLogProbMetric: 83.2243 - lr: 1.2346e-05 - 61s/epoch - 312ms/step
Epoch 116/1000
2023-10-25 18:43:53.403 
Epoch 116/1000 
	 loss: 82.8687, MinusLogProbMetric: 82.8687, val_loss: 82.9593, val_MinusLogProbMetric: 82.9593

Epoch 116: val_loss improved from 83.22433 to 82.95930, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 62s - loss: 82.8687 - MinusLogProbMetric: 82.8687 - val_loss: 82.9593 - val_MinusLogProbMetric: 82.9593 - lr: 1.2346e-05 - 62s/epoch - 318ms/step
Epoch 117/1000
2023-10-25 18:45:00.813 
Epoch 117/1000 
	 loss: 82.3501, MinusLogProbMetric: 82.3501, val_loss: 82.2029, val_MinusLogProbMetric: 82.2029

Epoch 117: val_loss improved from 82.95930 to 82.20287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 82.3501 - MinusLogProbMetric: 82.3501 - val_loss: 82.2029 - val_MinusLogProbMetric: 82.2029 - lr: 1.2346e-05 - 68s/epoch - 344ms/step
Epoch 118/1000
2023-10-25 18:46:07.817 
Epoch 118/1000 
	 loss: 81.8693, MinusLogProbMetric: 81.8693, val_loss: 81.6985, val_MinusLogProbMetric: 81.6985

Epoch 118: val_loss improved from 82.20287 to 81.69849, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 67s - loss: 81.8693 - MinusLogProbMetric: 81.8693 - val_loss: 81.6985 - val_MinusLogProbMetric: 81.6985 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 119/1000
2023-10-25 18:47:07.905 
Epoch 119/1000 
	 loss: 81.4269, MinusLogProbMetric: 81.4269, val_loss: 81.3645, val_MinusLogProbMetric: 81.3645

Epoch 119: val_loss improved from 81.69849 to 81.36455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 60s - loss: 81.4269 - MinusLogProbMetric: 81.4269 - val_loss: 81.3645 - val_MinusLogProbMetric: 81.3645 - lr: 1.2346e-05 - 60s/epoch - 305ms/step
Epoch 120/1000
2023-10-25 18:48:05.082 
Epoch 120/1000 
	 loss: 81.2122, MinusLogProbMetric: 81.2122, val_loss: 81.0246, val_MinusLogProbMetric: 81.0246

Epoch 120: val_loss improved from 81.36455 to 81.02460, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 57s - loss: 81.2122 - MinusLogProbMetric: 81.2122 - val_loss: 81.0246 - val_MinusLogProbMetric: 81.0246 - lr: 1.2346e-05 - 57s/epoch - 292ms/step
Epoch 121/1000
2023-10-25 18:49:11.024 
Epoch 121/1000 
	 loss: 80.6473, MinusLogProbMetric: 80.6473, val_loss: 80.5565, val_MinusLogProbMetric: 80.5565

Epoch 121: val_loss improved from 81.02460 to 80.55654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 66s - loss: 80.6473 - MinusLogProbMetric: 80.6473 - val_loss: 80.5565 - val_MinusLogProbMetric: 80.5565 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 122/1000
2023-10-25 18:50:13.865 
Epoch 122/1000 
	 loss: 80.1837, MinusLogProbMetric: 80.1837, val_loss: 80.3219, val_MinusLogProbMetric: 80.3219

Epoch 122: val_loss improved from 80.55654 to 80.32188, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 63s - loss: 80.1837 - MinusLogProbMetric: 80.1837 - val_loss: 80.3219 - val_MinusLogProbMetric: 80.3219 - lr: 1.2346e-05 - 63s/epoch - 319ms/step
Epoch 123/1000
2023-10-25 18:51:18.714 
Epoch 123/1000 
	 loss: 79.8099, MinusLogProbMetric: 79.8099, val_loss: 79.8436, val_MinusLogProbMetric: 79.8436

Epoch 123: val_loss improved from 80.32188 to 79.84357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 65s - loss: 79.8099 - MinusLogProbMetric: 79.8099 - val_loss: 79.8436 - val_MinusLogProbMetric: 79.8436 - lr: 1.2346e-05 - 65s/epoch - 330ms/step
Epoch 124/1000
2023-10-25 18:52:23.080 
Epoch 124/1000 
	 loss: 79.3993, MinusLogProbMetric: 79.3993, val_loss: 79.3725, val_MinusLogProbMetric: 79.3725

Epoch 124: val_loss improved from 79.84357 to 79.37254, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 65s - loss: 79.3993 - MinusLogProbMetric: 79.3993 - val_loss: 79.3725 - val_MinusLogProbMetric: 79.3725 - lr: 1.2346e-05 - 65s/epoch - 329ms/step
Epoch 125/1000
2023-10-25 18:53:29.966 
Epoch 125/1000 
	 loss: 79.0002, MinusLogProbMetric: 79.0002, val_loss: 78.9364, val_MinusLogProbMetric: 78.9364

Epoch 125: val_loss improved from 79.37254 to 78.93636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 67s - loss: 79.0002 - MinusLogProbMetric: 79.0002 - val_loss: 78.9364 - val_MinusLogProbMetric: 78.9364 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 126/1000
2023-10-25 18:54:29.601 
Epoch 126/1000 
	 loss: 79.2662, MinusLogProbMetric: 79.2662, val_loss: 79.0544, val_MinusLogProbMetric: 79.0544

Epoch 126: val_loss did not improve from 78.93636
196/196 - 59s - loss: 79.2662 - MinusLogProbMetric: 79.2662 - val_loss: 79.0544 - val_MinusLogProbMetric: 79.0544 - lr: 1.2346e-05 - 59s/epoch - 300ms/step
Epoch 127/1000
2023-10-25 18:55:28.468 
Epoch 127/1000 
	 loss: 79.1124, MinusLogProbMetric: 79.1124, val_loss: 79.4386, val_MinusLogProbMetric: 79.4386

Epoch 127: val_loss did not improve from 78.93636
196/196 - 59s - loss: 79.1124 - MinusLogProbMetric: 79.1124 - val_loss: 79.4386 - val_MinusLogProbMetric: 79.4386 - lr: 1.2346e-05 - 59s/epoch - 300ms/step
Epoch 128/1000
2023-10-25 18:56:31.386 
Epoch 128/1000 
	 loss: 78.3840, MinusLogProbMetric: 78.3840, val_loss: 78.2467, val_MinusLogProbMetric: 78.2467

Epoch 128: val_loss improved from 78.93636 to 78.24673, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 64s - loss: 78.3840 - MinusLogProbMetric: 78.3840 - val_loss: 78.2467 - val_MinusLogProbMetric: 78.2467 - lr: 1.2346e-05 - 64s/epoch - 326ms/step
Epoch 129/1000
2023-10-25 18:57:37.296 
Epoch 129/1000 
	 loss: 77.8799, MinusLogProbMetric: 77.8799, val_loss: 77.9865, val_MinusLogProbMetric: 77.9865

Epoch 129: val_loss improved from 78.24673 to 77.98655, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 66s - loss: 77.8799 - MinusLogProbMetric: 77.8799 - val_loss: 77.9865 - val_MinusLogProbMetric: 77.9865 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 130/1000
2023-10-25 18:58:34.934 
Epoch 130/1000 
	 loss: 77.5793, MinusLogProbMetric: 77.5793, val_loss: 77.4753, val_MinusLogProbMetric: 77.4753

Epoch 130: val_loss improved from 77.98655 to 77.47529, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 57s - loss: 77.5793 - MinusLogProbMetric: 77.5793 - val_loss: 77.4753 - val_MinusLogProbMetric: 77.4753 - lr: 1.2346e-05 - 57s/epoch - 292ms/step
Epoch 131/1000
2023-10-25 18:59:33.817 
Epoch 131/1000 
	 loss: 78.3104, MinusLogProbMetric: 78.3104, val_loss: 77.3623, val_MinusLogProbMetric: 77.3623

Epoch 131: val_loss improved from 77.47529 to 77.36232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 59s - loss: 78.3104 - MinusLogProbMetric: 78.3104 - val_loss: 77.3623 - val_MinusLogProbMetric: 77.3623 - lr: 1.2346e-05 - 59s/epoch - 301ms/step
Epoch 132/1000
2023-10-25 19:00:40.834 
Epoch 132/1000 
	 loss: 77.0275, MinusLogProbMetric: 77.0275, val_loss: 76.9422, val_MinusLogProbMetric: 76.9422

Epoch 132: val_loss improved from 77.36232 to 76.94220, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 67s - loss: 77.0275 - MinusLogProbMetric: 77.0275 - val_loss: 76.9422 - val_MinusLogProbMetric: 76.9422 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 133/1000
2023-10-25 19:01:41.564 
Epoch 133/1000 
	 loss: 76.6415, MinusLogProbMetric: 76.6415, val_loss: 76.4384, val_MinusLogProbMetric: 76.4384

Epoch 133: val_loss improved from 76.94220 to 76.43838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 61s - loss: 76.6415 - MinusLogProbMetric: 76.6415 - val_loss: 76.4384 - val_MinusLogProbMetric: 76.4384 - lr: 1.2346e-05 - 61s/epoch - 310ms/step
Epoch 134/1000
2023-10-25 19:02:41.673 
Epoch 134/1000 
	 loss: 76.1634, MinusLogProbMetric: 76.1634, val_loss: 76.2469, val_MinusLogProbMetric: 76.2469

Epoch 134: val_loss improved from 76.43838 to 76.24693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 60s - loss: 76.1634 - MinusLogProbMetric: 76.1634 - val_loss: 76.2469 - val_MinusLogProbMetric: 76.2469 - lr: 1.2346e-05 - 60s/epoch - 308ms/step
Epoch 135/1000
2023-10-25 19:03:49.061 
Epoch 135/1000 
	 loss: 75.8730, MinusLogProbMetric: 75.8730, val_loss: 75.9819, val_MinusLogProbMetric: 75.9819

Epoch 135: val_loss improved from 76.24693 to 75.98186, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 67s - loss: 75.8730 - MinusLogProbMetric: 75.8730 - val_loss: 75.9819 - val_MinusLogProbMetric: 75.9819 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 136/1000
2023-10-25 19:04:50.310 
Epoch 136/1000 
	 loss: 75.5165, MinusLogProbMetric: 75.5165, val_loss: 75.6545, val_MinusLogProbMetric: 75.6545

Epoch 136: val_loss improved from 75.98186 to 75.65450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 61s - loss: 75.5165 - MinusLogProbMetric: 75.5165 - val_loss: 75.6545 - val_MinusLogProbMetric: 75.6545 - lr: 1.2346e-05 - 61s/epoch - 312ms/step
Epoch 137/1000
2023-10-25 19:05:52.312 
Epoch 137/1000 
	 loss: 75.3535, MinusLogProbMetric: 75.3535, val_loss: 75.3517, val_MinusLogProbMetric: 75.3517

Epoch 137: val_loss improved from 75.65450 to 75.35171, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 62s - loss: 75.3535 - MinusLogProbMetric: 75.3535 - val_loss: 75.3517 - val_MinusLogProbMetric: 75.3517 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 138/1000
2023-10-25 19:06:58.271 
Epoch 138/1000 
	 loss: 208.5774, MinusLogProbMetric: 208.5774, val_loss: 151.3999, val_MinusLogProbMetric: 151.3999

Epoch 138: val_loss did not improve from 75.35171
196/196 - 65s - loss: 208.5774 - MinusLogProbMetric: 208.5774 - val_loss: 151.3999 - val_MinusLogProbMetric: 151.3999 - lr: 1.2346e-05 - 65s/epoch - 331ms/step
Epoch 139/1000
2023-10-25 19:07:56.722 
Epoch 139/1000 
	 loss: 133.4925, MinusLogProbMetric: 133.4925, val_loss: 121.2422, val_MinusLogProbMetric: 121.2422

Epoch 139: val_loss did not improve from 75.35171
196/196 - 58s - loss: 133.4925 - MinusLogProbMetric: 133.4925 - val_loss: 121.2422 - val_MinusLogProbMetric: 121.2422 - lr: 1.2346e-05 - 58s/epoch - 298ms/step
Epoch 140/1000
2023-10-25 19:09:01.161 
Epoch 140/1000 
	 loss: 115.5114, MinusLogProbMetric: 115.5114, val_loss: 111.0529, val_MinusLogProbMetric: 111.0529

Epoch 140: val_loss did not improve from 75.35171
196/196 - 64s - loss: 115.5114 - MinusLogProbMetric: 115.5114 - val_loss: 111.0529 - val_MinusLogProbMetric: 111.0529 - lr: 1.2346e-05 - 64s/epoch - 329ms/step
Epoch 141/1000
2023-10-25 19:10:01.319 
Epoch 141/1000 
	 loss: 107.2089, MinusLogProbMetric: 107.2089, val_loss: 104.6033, val_MinusLogProbMetric: 104.6033

Epoch 141: val_loss did not improve from 75.35171
196/196 - 60s - loss: 107.2089 - MinusLogProbMetric: 107.2089 - val_loss: 104.6033 - val_MinusLogProbMetric: 104.6033 - lr: 1.2346e-05 - 60s/epoch - 307ms/step
Epoch 142/1000
2023-10-25 19:11:03.441 
Epoch 142/1000 
	 loss: 102.3468, MinusLogProbMetric: 102.3468, val_loss: 100.8898, val_MinusLogProbMetric: 100.8898

Epoch 142: val_loss did not improve from 75.35171
196/196 - 62s - loss: 102.3468 - MinusLogProbMetric: 102.3468 - val_loss: 100.8898 - val_MinusLogProbMetric: 100.8898 - lr: 1.2346e-05 - 62s/epoch - 317ms/step
Epoch 143/1000
2023-10-25 19:12:06.320 
Epoch 143/1000 
	 loss: 98.5335, MinusLogProbMetric: 98.5335, val_loss: 96.7683, val_MinusLogProbMetric: 96.7683

Epoch 143: val_loss did not improve from 75.35171
196/196 - 63s - loss: 98.5335 - MinusLogProbMetric: 98.5335 - val_loss: 96.7683 - val_MinusLogProbMetric: 96.7683 - lr: 1.2346e-05 - 63s/epoch - 321ms/step
Epoch 144/1000
2023-10-25 19:13:05.981 
Epoch 144/1000 
	 loss: 95.0321, MinusLogProbMetric: 95.0321, val_loss: 93.5097, val_MinusLogProbMetric: 93.5097

Epoch 144: val_loss did not improve from 75.35171
196/196 - 60s - loss: 95.0321 - MinusLogProbMetric: 95.0321 - val_loss: 93.5097 - val_MinusLogProbMetric: 93.5097 - lr: 1.2346e-05 - 60s/epoch - 304ms/step
Epoch 145/1000
2023-10-25 19:14:12.113 
Epoch 145/1000 
	 loss: 92.6872, MinusLogProbMetric: 92.6872, val_loss: 92.0085, val_MinusLogProbMetric: 92.0085

Epoch 145: val_loss did not improve from 75.35171
196/196 - 66s - loss: 92.6872 - MinusLogProbMetric: 92.6872 - val_loss: 92.0085 - val_MinusLogProbMetric: 92.0085 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 146/1000
2023-10-25 19:15:18.812 
Epoch 146/1000 
	 loss: 91.4423, MinusLogProbMetric: 91.4423, val_loss: 91.6373, val_MinusLogProbMetric: 91.6373

Epoch 146: val_loss did not improve from 75.35171
196/196 - 67s - loss: 91.4423 - MinusLogProbMetric: 91.4423 - val_loss: 91.6373 - val_MinusLogProbMetric: 91.6373 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 147/1000
2023-10-25 19:16:24.194 
Epoch 147/1000 
	 loss: 89.9544, MinusLogProbMetric: 89.9544, val_loss: 98.8910, val_MinusLogProbMetric: 98.8910

Epoch 147: val_loss did not improve from 75.35171
196/196 - 65s - loss: 89.9544 - MinusLogProbMetric: 89.9544 - val_loss: 98.8910 - val_MinusLogProbMetric: 98.8910 - lr: 1.2346e-05 - 65s/epoch - 334ms/step
Epoch 148/1000
2023-10-25 19:17:31.365 
Epoch 148/1000 
	 loss: 90.8438, MinusLogProbMetric: 90.8438, val_loss: 87.1866, val_MinusLogProbMetric: 87.1866

Epoch 148: val_loss did not improve from 75.35171
196/196 - 67s - loss: 90.8438 - MinusLogProbMetric: 90.8438 - val_loss: 87.1866 - val_MinusLogProbMetric: 87.1866 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 149/1000
2023-10-25 19:18:39.909 
Epoch 149/1000 
	 loss: 96.2491, MinusLogProbMetric: 96.2491, val_loss: 107.2431, val_MinusLogProbMetric: 107.2431

Epoch 149: val_loss did not improve from 75.35171
196/196 - 69s - loss: 96.2491 - MinusLogProbMetric: 96.2491 - val_loss: 107.2431 - val_MinusLogProbMetric: 107.2431 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 150/1000
2023-10-25 19:19:47.522 
Epoch 150/1000 
	 loss: 98.5036, MinusLogProbMetric: 98.5036, val_loss: 96.6575, val_MinusLogProbMetric: 96.6575

Epoch 150: val_loss did not improve from 75.35171
196/196 - 68s - loss: 98.5036 - MinusLogProbMetric: 98.5036 - val_loss: 96.6575 - val_MinusLogProbMetric: 96.6575 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 151/1000
2023-10-25 19:20:55.328 
Epoch 151/1000 
	 loss: 184.5451, MinusLogProbMetric: 184.5451, val_loss: 152.1829, val_MinusLogProbMetric: 152.1829

Epoch 151: val_loss did not improve from 75.35171
196/196 - 68s - loss: 184.5451 - MinusLogProbMetric: 184.5451 - val_loss: 152.1829 - val_MinusLogProbMetric: 152.1829 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 152/1000
2023-10-25 19:22:03.121 
Epoch 152/1000 
	 loss: 137.7642, MinusLogProbMetric: 137.7642, val_loss: 128.4384, val_MinusLogProbMetric: 128.4384

Epoch 152: val_loss did not improve from 75.35171
196/196 - 68s - loss: 137.7642 - MinusLogProbMetric: 137.7642 - val_loss: 128.4384 - val_MinusLogProbMetric: 128.4384 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 153/1000
2023-10-25 19:23:10.946 
Epoch 153/1000 
	 loss: 124.3832, MinusLogProbMetric: 124.3832, val_loss: 117.8172, val_MinusLogProbMetric: 117.8172

Epoch 153: val_loss did not improve from 75.35171
196/196 - 68s - loss: 124.3832 - MinusLogProbMetric: 124.3832 - val_loss: 117.8172 - val_MinusLogProbMetric: 117.8172 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 154/1000
2023-10-25 19:24:18.471 
Epoch 154/1000 
	 loss: 114.8999, MinusLogProbMetric: 114.8999, val_loss: 112.1972, val_MinusLogProbMetric: 112.1972

Epoch 154: val_loss did not improve from 75.35171
196/196 - 68s - loss: 114.8999 - MinusLogProbMetric: 114.8999 - val_loss: 112.1972 - val_MinusLogProbMetric: 112.1972 - lr: 1.2346e-05 - 68s/epoch - 344ms/step
Epoch 155/1000
2023-10-25 19:25:26.058 
Epoch 155/1000 
	 loss: 110.6923, MinusLogProbMetric: 110.6923, val_loss: 108.4329, val_MinusLogProbMetric: 108.4329

Epoch 155: val_loss did not improve from 75.35171
196/196 - 68s - loss: 110.6923 - MinusLogProbMetric: 110.6923 - val_loss: 108.4329 - val_MinusLogProbMetric: 108.4329 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 156/1000
2023-10-25 19:26:33.915 
Epoch 156/1000 
	 loss: 107.7062, MinusLogProbMetric: 107.7062, val_loss: 106.3203, val_MinusLogProbMetric: 106.3203

Epoch 156: val_loss did not improve from 75.35171
196/196 - 68s - loss: 107.7062 - MinusLogProbMetric: 107.7062 - val_loss: 106.3203 - val_MinusLogProbMetric: 106.3203 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 157/1000
2023-10-25 19:27:40.770 
Epoch 157/1000 
	 loss: 108.8743, MinusLogProbMetric: 108.8743, val_loss: 100.1152, val_MinusLogProbMetric: 100.1152

Epoch 157: val_loss did not improve from 75.35171
196/196 - 67s - loss: 108.8743 - MinusLogProbMetric: 108.8743 - val_loss: 100.1152 - val_MinusLogProbMetric: 100.1152 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 158/1000
2023-10-25 19:28:48.072 
Epoch 158/1000 
	 loss: 97.1910, MinusLogProbMetric: 97.1910, val_loss: 95.3311, val_MinusLogProbMetric: 95.3311

Epoch 158: val_loss did not improve from 75.35171
196/196 - 67s - loss: 97.1910 - MinusLogProbMetric: 97.1910 - val_loss: 95.3311 - val_MinusLogProbMetric: 95.3311 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 159/1000
2023-10-25 19:29:55.028 
Epoch 159/1000 
	 loss: 94.1707, MinusLogProbMetric: 94.1707, val_loss: 93.3005, val_MinusLogProbMetric: 93.3005

Epoch 159: val_loss did not improve from 75.35171
196/196 - 67s - loss: 94.1707 - MinusLogProbMetric: 94.1707 - val_loss: 93.3005 - val_MinusLogProbMetric: 93.3005 - lr: 1.2346e-05 - 67s/epoch - 342ms/step
Epoch 160/1000
2023-10-25 19:31:03.157 
Epoch 160/1000 
	 loss: 92.1717, MinusLogProbMetric: 92.1717, val_loss: 91.4629, val_MinusLogProbMetric: 91.4629

Epoch 160: val_loss did not improve from 75.35171
196/196 - 68s - loss: 92.1717 - MinusLogProbMetric: 92.1717 - val_loss: 91.4629 - val_MinusLogProbMetric: 91.4629 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 161/1000
2023-10-25 19:32:10.691 
Epoch 161/1000 
	 loss: 90.7940, MinusLogProbMetric: 90.7940, val_loss: 90.2770, val_MinusLogProbMetric: 90.2770

Epoch 161: val_loss did not improve from 75.35171
196/196 - 68s - loss: 90.7940 - MinusLogProbMetric: 90.7940 - val_loss: 90.2770 - val_MinusLogProbMetric: 90.2770 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 162/1000
2023-10-25 19:33:18.750 
Epoch 162/1000 
	 loss: 89.7022, MinusLogProbMetric: 89.7022, val_loss: 89.4490, val_MinusLogProbMetric: 89.4490

Epoch 162: val_loss did not improve from 75.35171
196/196 - 68s - loss: 89.7022 - MinusLogProbMetric: 89.7022 - val_loss: 89.4490 - val_MinusLogProbMetric: 89.4490 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 163/1000
2023-10-25 19:34:26.424 
Epoch 163/1000 
	 loss: 88.8192, MinusLogProbMetric: 88.8192, val_loss: 88.0574, val_MinusLogProbMetric: 88.0574

Epoch 163: val_loss did not improve from 75.35171
196/196 - 68s - loss: 88.8192 - MinusLogProbMetric: 88.8192 - val_loss: 88.0574 - val_MinusLogProbMetric: 88.0574 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 164/1000
2023-10-25 19:35:33.743 
Epoch 164/1000 
	 loss: 87.2948, MinusLogProbMetric: 87.2948, val_loss: 86.4975, val_MinusLogProbMetric: 86.4975

Epoch 164: val_loss did not improve from 75.35171
196/196 - 67s - loss: 87.2948 - MinusLogProbMetric: 87.2948 - val_loss: 86.4975 - val_MinusLogProbMetric: 86.4975 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 165/1000
2023-10-25 19:36:41.834 
Epoch 165/1000 
	 loss: 85.8982, MinusLogProbMetric: 85.8982, val_loss: 85.5110, val_MinusLogProbMetric: 85.5110

Epoch 165: val_loss did not improve from 75.35171
196/196 - 68s - loss: 85.8982 - MinusLogProbMetric: 85.8982 - val_loss: 85.5110 - val_MinusLogProbMetric: 85.5110 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 166/1000
2023-10-25 19:37:49.319 
Epoch 166/1000 
	 loss: 84.8973, MinusLogProbMetric: 84.8973, val_loss: 84.5717, val_MinusLogProbMetric: 84.5717

Epoch 166: val_loss did not improve from 75.35171
196/196 - 67s - loss: 84.8973 - MinusLogProbMetric: 84.8973 - val_loss: 84.5717 - val_MinusLogProbMetric: 84.5717 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 167/1000
2023-10-25 19:38:56.728 
Epoch 167/1000 
	 loss: 84.0684, MinusLogProbMetric: 84.0684, val_loss: 83.8812, val_MinusLogProbMetric: 83.8812

Epoch 167: val_loss did not improve from 75.35171
196/196 - 67s - loss: 84.0684 - MinusLogProbMetric: 84.0684 - val_loss: 83.8812 - val_MinusLogProbMetric: 83.8812 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 168/1000
2023-10-25 19:40:04.071 
Epoch 168/1000 
	 loss: 85.0980, MinusLogProbMetric: 85.0980, val_loss: 83.0397, val_MinusLogProbMetric: 83.0397

Epoch 168: val_loss did not improve from 75.35171
196/196 - 67s - loss: 85.0980 - MinusLogProbMetric: 85.0980 - val_loss: 83.0397 - val_MinusLogProbMetric: 83.0397 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 169/1000
2023-10-25 19:41:08.358 
Epoch 169/1000 
	 loss: 82.1634, MinusLogProbMetric: 82.1634, val_loss: 81.7783, val_MinusLogProbMetric: 81.7783

Epoch 169: val_loss did not improve from 75.35171
196/196 - 64s - loss: 82.1634 - MinusLogProbMetric: 82.1634 - val_loss: 81.7783 - val_MinusLogProbMetric: 81.7783 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 170/1000
2023-10-25 19:42:15.124 
Epoch 170/1000 
	 loss: 81.3138, MinusLogProbMetric: 81.3138, val_loss: 81.0582, val_MinusLogProbMetric: 81.0582

Epoch 170: val_loss did not improve from 75.35171
196/196 - 67s - loss: 81.3138 - MinusLogProbMetric: 81.3138 - val_loss: 81.0582 - val_MinusLogProbMetric: 81.0582 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 171/1000
2023-10-25 19:43:21.240 
Epoch 171/1000 
	 loss: 80.6778, MinusLogProbMetric: 80.6778, val_loss: 80.3827, val_MinusLogProbMetric: 80.3827

Epoch 171: val_loss did not improve from 75.35171
196/196 - 66s - loss: 80.6778 - MinusLogProbMetric: 80.6778 - val_loss: 80.3827 - val_MinusLogProbMetric: 80.3827 - lr: 1.2346e-05 - 66s/epoch - 337ms/step
Epoch 172/1000
2023-10-25 19:44:29.037 
Epoch 172/1000 
	 loss: 80.3503, MinusLogProbMetric: 80.3503, val_loss: 80.3190, val_MinusLogProbMetric: 80.3190

Epoch 172: val_loss did not improve from 75.35171
196/196 - 68s - loss: 80.3503 - MinusLogProbMetric: 80.3503 - val_loss: 80.3190 - val_MinusLogProbMetric: 80.3190 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 173/1000
2023-10-25 19:45:35.820 
Epoch 173/1000 
	 loss: 79.5024, MinusLogProbMetric: 79.5024, val_loss: 79.3538, val_MinusLogProbMetric: 79.3538

Epoch 173: val_loss did not improve from 75.35171
196/196 - 67s - loss: 79.5024 - MinusLogProbMetric: 79.5024 - val_loss: 79.3538 - val_MinusLogProbMetric: 79.3538 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 174/1000
2023-10-25 19:46:43.371 
Epoch 174/1000 
	 loss: 81.7952, MinusLogProbMetric: 81.7952, val_loss: 85.1896, val_MinusLogProbMetric: 85.1896

Epoch 174: val_loss did not improve from 75.35171
196/196 - 68s - loss: 81.7952 - MinusLogProbMetric: 81.7952 - val_loss: 85.1896 - val_MinusLogProbMetric: 85.1896 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 175/1000
2023-10-25 19:47:51.346 
Epoch 175/1000 
	 loss: 79.6730, MinusLogProbMetric: 79.6730, val_loss: 79.4739, val_MinusLogProbMetric: 79.4739

Epoch 175: val_loss did not improve from 75.35171
196/196 - 68s - loss: 79.6730 - MinusLogProbMetric: 79.6730 - val_loss: 79.4739 - val_MinusLogProbMetric: 79.4739 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 176/1000
2023-10-25 19:48:59.673 
Epoch 176/1000 
	 loss: 78.2135, MinusLogProbMetric: 78.2135, val_loss: 77.9509, val_MinusLogProbMetric: 77.9509

Epoch 176: val_loss did not improve from 75.35171
196/196 - 68s - loss: 78.2135 - MinusLogProbMetric: 78.2135 - val_loss: 77.9509 - val_MinusLogProbMetric: 77.9509 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 177/1000
2023-10-25 19:50:07.719 
Epoch 177/1000 
	 loss: 77.8435, MinusLogProbMetric: 77.8435, val_loss: 78.7407, val_MinusLogProbMetric: 78.7407

Epoch 177: val_loss did not improve from 75.35171
196/196 - 68s - loss: 77.8435 - MinusLogProbMetric: 77.8435 - val_loss: 78.7407 - val_MinusLogProbMetric: 78.7407 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 178/1000
2023-10-25 19:51:15.529 
Epoch 178/1000 
	 loss: 77.3120, MinusLogProbMetric: 77.3120, val_loss: 76.9625, val_MinusLogProbMetric: 76.9625

Epoch 178: val_loss did not improve from 75.35171
196/196 - 68s - loss: 77.3120 - MinusLogProbMetric: 77.3120 - val_loss: 76.9625 - val_MinusLogProbMetric: 76.9625 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 179/1000
2023-10-25 19:52:23.065 
Epoch 179/1000 
	 loss: 76.7246, MinusLogProbMetric: 76.7246, val_loss: 76.5974, val_MinusLogProbMetric: 76.5974

Epoch 179: val_loss did not improve from 75.35171
196/196 - 68s - loss: 76.7246 - MinusLogProbMetric: 76.7246 - val_loss: 76.5974 - val_MinusLogProbMetric: 76.5974 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 180/1000
2023-10-25 19:53:30.378 
Epoch 180/1000 
	 loss: 77.5660, MinusLogProbMetric: 77.5660, val_loss: 76.7921, val_MinusLogProbMetric: 76.7921

Epoch 180: val_loss did not improve from 75.35171
196/196 - 67s - loss: 77.5660 - MinusLogProbMetric: 77.5660 - val_loss: 76.7921 - val_MinusLogProbMetric: 76.7921 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 181/1000
2023-10-25 19:54:37.925 
Epoch 181/1000 
	 loss: 78.4894, MinusLogProbMetric: 78.4894, val_loss: 76.1105, val_MinusLogProbMetric: 76.1105

Epoch 181: val_loss did not improve from 75.35171
196/196 - 68s - loss: 78.4894 - MinusLogProbMetric: 78.4894 - val_loss: 76.1105 - val_MinusLogProbMetric: 76.1105 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 182/1000
2023-10-25 19:55:46.225 
Epoch 182/1000 
	 loss: 75.5088, MinusLogProbMetric: 75.5088, val_loss: 75.3112, val_MinusLogProbMetric: 75.3112

Epoch 182: val_loss improved from 75.35171 to 75.31116, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 75.5088 - MinusLogProbMetric: 75.5088 - val_loss: 75.3112 - val_MinusLogProbMetric: 75.3112 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 183/1000
2023-10-25 19:56:55.724 
Epoch 183/1000 
	 loss: 77.5308, MinusLogProbMetric: 77.5308, val_loss: 75.5881, val_MinusLogProbMetric: 75.5881

Epoch 183: val_loss did not improve from 75.31116
196/196 - 68s - loss: 77.5308 - MinusLogProbMetric: 77.5308 - val_loss: 75.5881 - val_MinusLogProbMetric: 75.5881 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 184/1000
2023-10-25 19:58:02.971 
Epoch 184/1000 
	 loss: 74.7270, MinusLogProbMetric: 74.7270, val_loss: 74.2463, val_MinusLogProbMetric: 74.2463

Epoch 184: val_loss improved from 75.31116 to 74.24628, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 74.7270 - MinusLogProbMetric: 74.7270 - val_loss: 74.2463 - val_MinusLogProbMetric: 74.2463 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 185/1000
2023-10-25 19:59:11.794 
Epoch 185/1000 
	 loss: 73.5389, MinusLogProbMetric: 73.5389, val_loss: 73.1054, val_MinusLogProbMetric: 73.1054

Epoch 185: val_loss improved from 74.24628 to 73.10540, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 73.5389 - MinusLogProbMetric: 73.5389 - val_loss: 73.1054 - val_MinusLogProbMetric: 73.1054 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 186/1000
2023-10-25 20:00:20.055 
Epoch 186/1000 
	 loss: 72.9495, MinusLogProbMetric: 72.9495, val_loss: 72.8780, val_MinusLogProbMetric: 72.8780

Epoch 186: val_loss improved from 73.10540 to 72.87796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 72.9495 - MinusLogProbMetric: 72.9495 - val_loss: 72.8780 - val_MinusLogProbMetric: 72.8780 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 187/1000
2023-10-25 20:01:29.004 
Epoch 187/1000 
	 loss: 73.1111, MinusLogProbMetric: 73.1111, val_loss: 72.4759, val_MinusLogProbMetric: 72.4759

Epoch 187: val_loss improved from 72.87796 to 72.47588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 73.1111 - MinusLogProbMetric: 73.1111 - val_loss: 72.4759 - val_MinusLogProbMetric: 72.4759 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 188/1000
2023-10-25 20:02:38.094 
Epoch 188/1000 
	 loss: 71.9854, MinusLogProbMetric: 71.9854, val_loss: 71.9814, val_MinusLogProbMetric: 71.9814

Epoch 188: val_loss improved from 72.47588 to 71.98145, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 71.9854 - MinusLogProbMetric: 71.9854 - val_loss: 71.9814 - val_MinusLogProbMetric: 71.9814 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 189/1000
2023-10-25 20:03:46.431 
Epoch 189/1000 
	 loss: 71.6309, MinusLogProbMetric: 71.6309, val_loss: 71.6809, val_MinusLogProbMetric: 71.6809

Epoch 189: val_loss improved from 71.98145 to 71.68090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 71.6309 - MinusLogProbMetric: 71.6309 - val_loss: 71.6809 - val_MinusLogProbMetric: 71.6809 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 190/1000
2023-10-25 20:04:55.085 
Epoch 190/1000 
	 loss: 71.2234, MinusLogProbMetric: 71.2234, val_loss: 71.1942, val_MinusLogProbMetric: 71.1942

Epoch 190: val_loss improved from 71.68090 to 71.19417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 71.2234 - MinusLogProbMetric: 71.2234 - val_loss: 71.1942 - val_MinusLogProbMetric: 71.1942 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 191/1000
2023-10-25 20:06:03.883 
Epoch 191/1000 
	 loss: 70.8571, MinusLogProbMetric: 70.8571, val_loss: 70.9897, val_MinusLogProbMetric: 70.9897

Epoch 191: val_loss improved from 71.19417 to 70.98971, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 70.8571 - MinusLogProbMetric: 70.8571 - val_loss: 70.9897 - val_MinusLogProbMetric: 70.9897 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 192/1000
2023-10-25 20:07:12.537 
Epoch 192/1000 
	 loss: 70.5255, MinusLogProbMetric: 70.5255, val_loss: 70.3608, val_MinusLogProbMetric: 70.3608

Epoch 192: val_loss improved from 70.98971 to 70.36083, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 70.5255 - MinusLogProbMetric: 70.5255 - val_loss: 70.3608 - val_MinusLogProbMetric: 70.3608 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 193/1000
2023-10-25 20:08:21.353 
Epoch 193/1000 
	 loss: 70.0831, MinusLogProbMetric: 70.0831, val_loss: 70.0285, val_MinusLogProbMetric: 70.0285

Epoch 193: val_loss improved from 70.36083 to 70.02845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 70.0831 - MinusLogProbMetric: 70.0831 - val_loss: 70.0285 - val_MinusLogProbMetric: 70.0285 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 194/1000
2023-10-25 20:09:29.585 
Epoch 194/1000 
	 loss: 70.7903, MinusLogProbMetric: 70.7903, val_loss: 69.8771, val_MinusLogProbMetric: 69.8771

Epoch 194: val_loss improved from 70.02845 to 69.87709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 70.7903 - MinusLogProbMetric: 70.7903 - val_loss: 69.8771 - val_MinusLogProbMetric: 69.8771 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 195/1000
2023-10-25 20:10:37.994 
Epoch 195/1000 
	 loss: 69.5155, MinusLogProbMetric: 69.5155, val_loss: 69.5913, val_MinusLogProbMetric: 69.5913

Epoch 195: val_loss improved from 69.87709 to 69.59126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 69.5155 - MinusLogProbMetric: 69.5155 - val_loss: 69.5913 - val_MinusLogProbMetric: 69.5913 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 196/1000
2023-10-25 20:11:46.457 
Epoch 196/1000 
	 loss: 69.5837, MinusLogProbMetric: 69.5837, val_loss: 69.4859, val_MinusLogProbMetric: 69.4859

Epoch 196: val_loss improved from 69.59126 to 69.48586, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 69.5837 - MinusLogProbMetric: 69.5837 - val_loss: 69.4859 - val_MinusLogProbMetric: 69.4859 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 197/1000
2023-10-25 20:12:54.782 
Epoch 197/1000 
	 loss: 68.9555, MinusLogProbMetric: 68.9555, val_loss: 68.9361, val_MinusLogProbMetric: 68.9361

Epoch 197: val_loss improved from 69.48586 to 68.93607, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 68.9555 - MinusLogProbMetric: 68.9555 - val_loss: 68.9361 - val_MinusLogProbMetric: 68.9361 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 198/1000
2023-10-25 20:14:01.023 
Epoch 198/1000 
	 loss: 69.0254, MinusLogProbMetric: 69.0254, val_loss: 68.8131, val_MinusLogProbMetric: 68.8131

Epoch 198: val_loss improved from 68.93607 to 68.81306, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 66s - loss: 69.0254 - MinusLogProbMetric: 69.0254 - val_loss: 68.8131 - val_MinusLogProbMetric: 68.8131 - lr: 1.2346e-05 - 66s/epoch - 338ms/step
Epoch 199/1000
2023-10-25 20:15:09.121 
Epoch 199/1000 
	 loss: 68.5906, MinusLogProbMetric: 68.5906, val_loss: 68.6464, val_MinusLogProbMetric: 68.6464

Epoch 199: val_loss improved from 68.81306 to 68.64635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 68.5906 - MinusLogProbMetric: 68.5906 - val_loss: 68.6464 - val_MinusLogProbMetric: 68.6464 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 200/1000
2023-10-25 20:16:17.526 
Epoch 200/1000 
	 loss: 68.4156, MinusLogProbMetric: 68.4156, val_loss: 68.3242, val_MinusLogProbMetric: 68.3242

Epoch 200: val_loss improved from 68.64635 to 68.32423, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 68.4156 - MinusLogProbMetric: 68.4156 - val_loss: 68.3242 - val_MinusLogProbMetric: 68.3242 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 201/1000
2023-10-25 20:17:26.745 
Epoch 201/1000 
	 loss: 68.0990, MinusLogProbMetric: 68.0990, val_loss: 68.3842, val_MinusLogProbMetric: 68.3842

Epoch 201: val_loss did not improve from 68.32423
196/196 - 68s - loss: 68.0990 - MinusLogProbMetric: 68.0990 - val_loss: 68.3842 - val_MinusLogProbMetric: 68.3842 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 202/1000
2023-10-25 20:18:34.196 
Epoch 202/1000 
	 loss: 67.8071, MinusLogProbMetric: 67.8071, val_loss: 67.9089, val_MinusLogProbMetric: 67.9089

Epoch 202: val_loss improved from 68.32423 to 67.90886, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 67.8071 - MinusLogProbMetric: 67.8071 - val_loss: 67.9089 - val_MinusLogProbMetric: 67.9089 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 203/1000
2023-10-25 20:19:43.484 
Epoch 203/1000 
	 loss: 67.5794, MinusLogProbMetric: 67.5794, val_loss: 67.9863, val_MinusLogProbMetric: 67.9863

Epoch 203: val_loss did not improve from 67.90886
196/196 - 68s - loss: 67.5794 - MinusLogProbMetric: 67.5794 - val_loss: 67.9863 - val_MinusLogProbMetric: 67.9863 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 204/1000
2023-10-25 20:20:51.954 
Epoch 204/1000 
	 loss: 67.4522, MinusLogProbMetric: 67.4522, val_loss: 67.4986, val_MinusLogProbMetric: 67.4986

Epoch 204: val_loss improved from 67.90886 to 67.49865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 67.4522 - MinusLogProbMetric: 67.4522 - val_loss: 67.4986 - val_MinusLogProbMetric: 67.4986 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 205/1000
2023-10-25 20:22:01.642 
Epoch 205/1000 
	 loss: 67.1301, MinusLogProbMetric: 67.1301, val_loss: 67.2464, val_MinusLogProbMetric: 67.2464

Epoch 205: val_loss improved from 67.49865 to 67.24639, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 70s - loss: 67.1301 - MinusLogProbMetric: 67.1301 - val_loss: 67.2464 - val_MinusLogProbMetric: 67.2464 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 206/1000
2023-10-25 20:23:11.243 
Epoch 206/1000 
	 loss: 66.9168, MinusLogProbMetric: 66.9168, val_loss: 66.9462, val_MinusLogProbMetric: 66.9462

Epoch 206: val_loss improved from 67.24639 to 66.94623, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 70s - loss: 66.9168 - MinusLogProbMetric: 66.9168 - val_loss: 66.9462 - val_MinusLogProbMetric: 66.9462 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 207/1000
2023-10-25 20:24:19.910 
Epoch 207/1000 
	 loss: 66.7310, MinusLogProbMetric: 66.7310, val_loss: 66.6973, val_MinusLogProbMetric: 66.6973

Epoch 207: val_loss improved from 66.94623 to 66.69733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 66.7310 - MinusLogProbMetric: 66.7310 - val_loss: 66.6973 - val_MinusLogProbMetric: 66.6973 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 208/1000
2023-10-25 20:25:29.328 
Epoch 208/1000 
	 loss: 69.2025, MinusLogProbMetric: 69.2025, val_loss: 68.1780, val_MinusLogProbMetric: 68.1780

Epoch 208: val_loss did not improve from 66.69733
196/196 - 68s - loss: 69.2025 - MinusLogProbMetric: 69.2025 - val_loss: 68.1780 - val_MinusLogProbMetric: 68.1780 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 209/1000
2023-10-25 20:26:38.139 
Epoch 209/1000 
	 loss: 71.1853, MinusLogProbMetric: 71.1853, val_loss: 67.8989, val_MinusLogProbMetric: 67.8989

Epoch 209: val_loss did not improve from 66.69733
196/196 - 69s - loss: 71.1853 - MinusLogProbMetric: 71.1853 - val_loss: 67.8989 - val_MinusLogProbMetric: 67.8989 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 210/1000
2023-10-25 20:27:46.211 
Epoch 210/1000 
	 loss: 67.9862, MinusLogProbMetric: 67.9862, val_loss: 67.7646, val_MinusLogProbMetric: 67.7646

Epoch 210: val_loss did not improve from 66.69733
196/196 - 68s - loss: 67.9862 - MinusLogProbMetric: 67.9862 - val_loss: 67.7646 - val_MinusLogProbMetric: 67.7646 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 211/1000
2023-10-25 20:28:53.680 
Epoch 211/1000 
	 loss: 66.8423, MinusLogProbMetric: 66.8423, val_loss: 66.7457, val_MinusLogProbMetric: 66.7457

Epoch 211: val_loss did not improve from 66.69733
196/196 - 67s - loss: 66.8423 - MinusLogProbMetric: 66.8423 - val_loss: 66.7457 - val_MinusLogProbMetric: 66.7457 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 212/1000
2023-10-25 20:30:02.001 
Epoch 212/1000 
	 loss: 66.1617, MinusLogProbMetric: 66.1617, val_loss: 66.0322, val_MinusLogProbMetric: 66.0322

Epoch 212: val_loss improved from 66.69733 to 66.03219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 66.1617 - MinusLogProbMetric: 66.1617 - val_loss: 66.0322 - val_MinusLogProbMetric: 66.0322 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 213/1000
2023-10-25 20:31:10.501 
Epoch 213/1000 
	 loss: 65.9392, MinusLogProbMetric: 65.9392, val_loss: 65.7341, val_MinusLogProbMetric: 65.7341

Epoch 213: val_loss improved from 66.03219 to 65.73413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 65.9392 - MinusLogProbMetric: 65.9392 - val_loss: 65.7341 - val_MinusLogProbMetric: 65.7341 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 214/1000
2023-10-25 20:32:19.514 
Epoch 214/1000 
	 loss: 65.5418, MinusLogProbMetric: 65.5418, val_loss: 65.6000, val_MinusLogProbMetric: 65.6000

Epoch 214: val_loss improved from 65.73413 to 65.59999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 65.5418 - MinusLogProbMetric: 65.5418 - val_loss: 65.6000 - val_MinusLogProbMetric: 65.6000 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 215/1000
2023-10-25 20:33:28.570 
Epoch 215/1000 
	 loss: 65.3031, MinusLogProbMetric: 65.3031, val_loss: 65.3231, val_MinusLogProbMetric: 65.3231

Epoch 215: val_loss improved from 65.59999 to 65.32310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 65.3031 - MinusLogProbMetric: 65.3031 - val_loss: 65.3231 - val_MinusLogProbMetric: 65.3231 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 216/1000
2023-10-25 20:34:36.965 
Epoch 216/1000 
	 loss: 65.0975, MinusLogProbMetric: 65.0975, val_loss: 65.1484, val_MinusLogProbMetric: 65.1484

Epoch 216: val_loss improved from 65.32310 to 65.14836, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 65.0975 - MinusLogProbMetric: 65.0975 - val_loss: 65.1484 - val_MinusLogProbMetric: 65.1484 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 217/1000
2023-10-25 20:35:45.797 
Epoch 217/1000 
	 loss: 64.9067, MinusLogProbMetric: 64.9067, val_loss: 64.9711, val_MinusLogProbMetric: 64.9711

Epoch 217: val_loss improved from 65.14836 to 64.97109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 64.9067 - MinusLogProbMetric: 64.9067 - val_loss: 64.9711 - val_MinusLogProbMetric: 64.9711 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 218/1000
2023-10-25 20:36:54.370 
Epoch 218/1000 
	 loss: 64.7005, MinusLogProbMetric: 64.7005, val_loss: 64.8239, val_MinusLogProbMetric: 64.8239

Epoch 218: val_loss improved from 64.97109 to 64.82393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 64.7005 - MinusLogProbMetric: 64.7005 - val_loss: 64.8239 - val_MinusLogProbMetric: 64.8239 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 219/1000
2023-10-25 20:38:03.436 
Epoch 219/1000 
	 loss: 64.4896, MinusLogProbMetric: 64.4896, val_loss: 64.5470, val_MinusLogProbMetric: 64.5470

Epoch 219: val_loss improved from 64.82393 to 64.54697, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 64.4896 - MinusLogProbMetric: 64.4896 - val_loss: 64.5470 - val_MinusLogProbMetric: 64.5470 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 220/1000
2023-10-25 20:39:12.601 
Epoch 220/1000 
	 loss: 64.4851, MinusLogProbMetric: 64.4851, val_loss: 64.3873, val_MinusLogProbMetric: 64.3873

Epoch 220: val_loss improved from 64.54697 to 64.38731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 64.4851 - MinusLogProbMetric: 64.4851 - val_loss: 64.3873 - val_MinusLogProbMetric: 64.3873 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 221/1000
2023-10-25 20:40:21.039 
Epoch 221/1000 
	 loss: 64.1019, MinusLogProbMetric: 64.1019, val_loss: 64.2288, val_MinusLogProbMetric: 64.2288

Epoch 221: val_loss improved from 64.38731 to 64.22877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 64.1019 - MinusLogProbMetric: 64.1019 - val_loss: 64.2288 - val_MinusLogProbMetric: 64.2288 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 222/1000
2023-10-25 20:41:29.587 
Epoch 222/1000 
	 loss: 64.9442, MinusLogProbMetric: 64.9442, val_loss: 64.2209, val_MinusLogProbMetric: 64.2209

Epoch 222: val_loss improved from 64.22877 to 64.22090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 64.9442 - MinusLogProbMetric: 64.9442 - val_loss: 64.2209 - val_MinusLogProbMetric: 64.2209 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 223/1000
2023-10-25 20:42:37.687 
Epoch 223/1000 
	 loss: 63.9272, MinusLogProbMetric: 63.9272, val_loss: 63.9370, val_MinusLogProbMetric: 63.9370

Epoch 223: val_loss improved from 64.22090 to 63.93702, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 63.9272 - MinusLogProbMetric: 63.9272 - val_loss: 63.9370 - val_MinusLogProbMetric: 63.9370 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 224/1000
2023-10-25 20:43:41.829 
Epoch 224/1000 
	 loss: 63.8538, MinusLogProbMetric: 63.8538, val_loss: 63.8091, val_MinusLogProbMetric: 63.8091

Epoch 224: val_loss improved from 63.93702 to 63.80912, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 64s - loss: 63.8538 - MinusLogProbMetric: 63.8538 - val_loss: 63.8091 - val_MinusLogProbMetric: 63.8091 - lr: 1.2346e-05 - 64s/epoch - 327ms/step
Epoch 225/1000
2023-10-25 20:44:46.859 
Epoch 225/1000 
	 loss: 63.9066, MinusLogProbMetric: 63.9066, val_loss: 63.9885, val_MinusLogProbMetric: 63.9885

Epoch 225: val_loss did not improve from 63.80912
196/196 - 64s - loss: 63.9066 - MinusLogProbMetric: 63.9066 - val_loss: 63.9885 - val_MinusLogProbMetric: 63.9885 - lr: 1.2346e-05 - 64s/epoch - 328ms/step
Epoch 226/1000
2023-10-25 20:45:55.599 
Epoch 226/1000 
	 loss: 63.3145, MinusLogProbMetric: 63.3145, val_loss: 63.5677, val_MinusLogProbMetric: 63.5677

Epoch 226: val_loss improved from 63.80912 to 63.56767, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 70s - loss: 63.3145 - MinusLogProbMetric: 63.3145 - val_loss: 63.5677 - val_MinusLogProbMetric: 63.5677 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 227/1000
2023-10-25 20:47:04.239 
Epoch 227/1000 
	 loss: 63.2242, MinusLogProbMetric: 63.2242, val_loss: 63.1582, val_MinusLogProbMetric: 63.1582

Epoch 227: val_loss improved from 63.56767 to 63.15824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 63.2242 - MinusLogProbMetric: 63.2242 - val_loss: 63.1582 - val_MinusLogProbMetric: 63.1582 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 228/1000
2023-10-25 20:48:13.847 
Epoch 228/1000 
	 loss: 62.9978, MinusLogProbMetric: 62.9978, val_loss: 63.2085, val_MinusLogProbMetric: 63.2085

Epoch 228: val_loss did not improve from 63.15824
196/196 - 68s - loss: 62.9978 - MinusLogProbMetric: 62.9978 - val_loss: 63.2085 - val_MinusLogProbMetric: 63.2085 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 229/1000
2023-10-25 20:49:21.279 
Epoch 229/1000 
	 loss: 62.8577, MinusLogProbMetric: 62.8577, val_loss: 62.9417, val_MinusLogProbMetric: 62.9417

Epoch 229: val_loss improved from 63.15824 to 62.94172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 62.8577 - MinusLogProbMetric: 62.8577 - val_loss: 62.9417 - val_MinusLogProbMetric: 62.9417 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 230/1000
2023-10-25 20:50:30.471 
Epoch 230/1000 
	 loss: 62.7270, MinusLogProbMetric: 62.7270, val_loss: 62.8060, val_MinusLogProbMetric: 62.8060

Epoch 230: val_loss improved from 62.94172 to 62.80598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 62.7270 - MinusLogProbMetric: 62.7270 - val_loss: 62.8060 - val_MinusLogProbMetric: 62.8060 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 231/1000
2023-10-25 20:51:39.287 
Epoch 231/1000 
	 loss: 62.4939, MinusLogProbMetric: 62.4939, val_loss: 62.7269, val_MinusLogProbMetric: 62.7269

Epoch 231: val_loss improved from 62.80598 to 62.72691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 62.4939 - MinusLogProbMetric: 62.4939 - val_loss: 62.7269 - val_MinusLogProbMetric: 62.7269 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 232/1000
2023-10-25 20:52:48.099 
Epoch 232/1000 
	 loss: 62.3492, MinusLogProbMetric: 62.3492, val_loss: 62.4744, val_MinusLogProbMetric: 62.4744

Epoch 232: val_loss improved from 62.72691 to 62.47441, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 62.3492 - MinusLogProbMetric: 62.3492 - val_loss: 62.4744 - val_MinusLogProbMetric: 62.4744 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 233/1000
2023-10-25 20:53:57.429 
Epoch 233/1000 
	 loss: 62.3645, MinusLogProbMetric: 62.3645, val_loss: 62.3028, val_MinusLogProbMetric: 62.3028

Epoch 233: val_loss improved from 62.47441 to 62.30283, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 62.3645 - MinusLogProbMetric: 62.3645 - val_loss: 62.3028 - val_MinusLogProbMetric: 62.3028 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 234/1000
2023-10-25 20:55:06.604 
Epoch 234/1000 
	 loss: 62.0263, MinusLogProbMetric: 62.0263, val_loss: 62.2516, val_MinusLogProbMetric: 62.2516

Epoch 234: val_loss improved from 62.30283 to 62.25161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 62.0263 - MinusLogProbMetric: 62.0263 - val_loss: 62.2516 - val_MinusLogProbMetric: 62.2516 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 235/1000
2023-10-25 20:56:15.028 
Epoch 235/1000 
	 loss: 61.9046, MinusLogProbMetric: 61.9046, val_loss: 61.9277, val_MinusLogProbMetric: 61.9277

Epoch 235: val_loss improved from 62.25161 to 61.92767, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 61.9046 - MinusLogProbMetric: 61.9046 - val_loss: 61.9277 - val_MinusLogProbMetric: 61.9277 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 236/1000
2023-10-25 20:57:23.209 
Epoch 236/1000 
	 loss: 61.7667, MinusLogProbMetric: 61.7667, val_loss: 61.9015, val_MinusLogProbMetric: 61.9015

Epoch 236: val_loss improved from 61.92767 to 61.90146, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 61.7667 - MinusLogProbMetric: 61.7667 - val_loss: 61.9015 - val_MinusLogProbMetric: 61.9015 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 237/1000
2023-10-25 20:58:31.873 
Epoch 237/1000 
	 loss: 61.5719, MinusLogProbMetric: 61.5719, val_loss: 61.8221, val_MinusLogProbMetric: 61.8221

Epoch 237: val_loss improved from 61.90146 to 61.82212, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 61.5719 - MinusLogProbMetric: 61.5719 - val_loss: 61.8221 - val_MinusLogProbMetric: 61.8221 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 238/1000
2023-10-25 20:59:39.029 
Epoch 238/1000 
	 loss: 61.4369, MinusLogProbMetric: 61.4369, val_loss: 61.6701, val_MinusLogProbMetric: 61.6701

Epoch 238: val_loss improved from 61.82212 to 61.67012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 61.4369 - MinusLogProbMetric: 61.4369 - val_loss: 61.6701 - val_MinusLogProbMetric: 61.6701 - lr: 1.2346e-05 - 68s/epoch - 344ms/step
Epoch 239/1000
2023-10-25 21:00:47.657 
Epoch 239/1000 
	 loss: 61.2971, MinusLogProbMetric: 61.2971, val_loss: 61.4706, val_MinusLogProbMetric: 61.4706

Epoch 239: val_loss improved from 61.67012 to 61.47056, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 61.2971 - MinusLogProbMetric: 61.2971 - val_loss: 61.4706 - val_MinusLogProbMetric: 61.4706 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 240/1000
2023-10-25 21:01:56.635 
Epoch 240/1000 
	 loss: 61.2678, MinusLogProbMetric: 61.2678, val_loss: 61.2943, val_MinusLogProbMetric: 61.2943

Epoch 240: val_loss improved from 61.47056 to 61.29427, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 61.2678 - MinusLogProbMetric: 61.2678 - val_loss: 61.2943 - val_MinusLogProbMetric: 61.2943 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 241/1000
2023-10-25 21:03:04.816 
Epoch 241/1000 
	 loss: 61.0980, MinusLogProbMetric: 61.0980, val_loss: 61.2338, val_MinusLogProbMetric: 61.2338

Epoch 241: val_loss improved from 61.29427 to 61.23378, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 61.0980 - MinusLogProbMetric: 61.0980 - val_loss: 61.2338 - val_MinusLogProbMetric: 61.2338 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 242/1000
2023-10-25 21:04:12.249 
Epoch 242/1000 
	 loss: 61.0419, MinusLogProbMetric: 61.0419, val_loss: 61.2058, val_MinusLogProbMetric: 61.2058

Epoch 242: val_loss improved from 61.23378 to 61.20578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 67s - loss: 61.0419 - MinusLogProbMetric: 61.0419 - val_loss: 61.2058 - val_MinusLogProbMetric: 61.2058 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 243/1000
2023-10-25 21:05:19.108 
Epoch 243/1000 
	 loss: 60.7818, MinusLogProbMetric: 60.7818, val_loss: 60.8983, val_MinusLogProbMetric: 60.8983

Epoch 243: val_loss improved from 61.20578 to 60.89826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 67s - loss: 60.7818 - MinusLogProbMetric: 60.7818 - val_loss: 60.8983 - val_MinusLogProbMetric: 60.8983 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 244/1000
2023-10-25 21:06:28.986 
Epoch 244/1000 
	 loss: 60.7264, MinusLogProbMetric: 60.7264, val_loss: 60.9186, val_MinusLogProbMetric: 60.9186

Epoch 244: val_loss did not improve from 60.89826
196/196 - 69s - loss: 60.7264 - MinusLogProbMetric: 60.7264 - val_loss: 60.9186 - val_MinusLogProbMetric: 60.9186 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 245/1000
2023-10-25 21:07:36.134 
Epoch 245/1000 
	 loss: 60.8561, MinusLogProbMetric: 60.8561, val_loss: 60.6856, val_MinusLogProbMetric: 60.6856

Epoch 245: val_loss improved from 60.89826 to 60.68564, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 60.8561 - MinusLogProbMetric: 60.8561 - val_loss: 60.6856 - val_MinusLogProbMetric: 60.6856 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 246/1000
2023-10-25 21:08:45.255 
Epoch 246/1000 
	 loss: 60.3948, MinusLogProbMetric: 60.3948, val_loss: 60.6868, val_MinusLogProbMetric: 60.6868

Epoch 246: val_loss did not improve from 60.68564
196/196 - 68s - loss: 60.3948 - MinusLogProbMetric: 60.3948 - val_loss: 60.6868 - val_MinusLogProbMetric: 60.6868 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 247/1000
2023-10-25 21:09:52.997 
Epoch 247/1000 
	 loss: 60.4130, MinusLogProbMetric: 60.4130, val_loss: 60.6116, val_MinusLogProbMetric: 60.6116

Epoch 247: val_loss improved from 60.68564 to 60.61162, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 60.4130 - MinusLogProbMetric: 60.4130 - val_loss: 60.6116 - val_MinusLogProbMetric: 60.6116 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 248/1000
2023-10-25 21:11:01.914 
Epoch 248/1000 
	 loss: 60.6612, MinusLogProbMetric: 60.6612, val_loss: 60.3765, val_MinusLogProbMetric: 60.3765

Epoch 248: val_loss improved from 60.61162 to 60.37654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 60.6612 - MinusLogProbMetric: 60.6612 - val_loss: 60.3765 - val_MinusLogProbMetric: 60.3765 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 249/1000
2023-10-25 21:12:11.242 
Epoch 249/1000 
	 loss: 63.1164, MinusLogProbMetric: 63.1164, val_loss: 60.9483, val_MinusLogProbMetric: 60.9483

Epoch 249: val_loss did not improve from 60.37654
196/196 - 68s - loss: 63.1164 - MinusLogProbMetric: 63.1164 - val_loss: 60.9483 - val_MinusLogProbMetric: 60.9483 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 250/1000
2023-10-25 21:13:19.118 
Epoch 250/1000 
	 loss: 60.5752, MinusLogProbMetric: 60.5752, val_loss: 60.1801, val_MinusLogProbMetric: 60.1801

Epoch 250: val_loss improved from 60.37654 to 60.18014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 60.5752 - MinusLogProbMetric: 60.5752 - val_loss: 60.1801 - val_MinusLogProbMetric: 60.1801 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 251/1000
2023-10-25 21:14:27.991 
Epoch 251/1000 
	 loss: 59.9440, MinusLogProbMetric: 59.9440, val_loss: 60.0091, val_MinusLogProbMetric: 60.0091

Epoch 251: val_loss improved from 60.18014 to 60.00906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 59.9440 - MinusLogProbMetric: 59.9440 - val_loss: 60.0091 - val_MinusLogProbMetric: 60.0091 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 252/1000
2023-10-25 21:15:36.892 
Epoch 252/1000 
	 loss: 59.8191, MinusLogProbMetric: 59.8191, val_loss: 60.0938, val_MinusLogProbMetric: 60.0938

Epoch 252: val_loss did not improve from 60.00906
196/196 - 68s - loss: 59.8191 - MinusLogProbMetric: 59.8191 - val_loss: 60.0938 - val_MinusLogProbMetric: 60.0938 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 253/1000
2023-10-25 21:16:45.175 
Epoch 253/1000 
	 loss: 59.7632, MinusLogProbMetric: 59.7632, val_loss: 59.9162, val_MinusLogProbMetric: 59.9162

Epoch 253: val_loss improved from 60.00906 to 59.91621, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 59.7632 - MinusLogProbMetric: 59.7632 - val_loss: 59.9162 - val_MinusLogProbMetric: 59.9162 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 254/1000
2023-10-25 21:17:54.718 
Epoch 254/1000 
	 loss: 59.7061, MinusLogProbMetric: 59.7061, val_loss: 59.9006, val_MinusLogProbMetric: 59.9006

Epoch 254: val_loss improved from 59.91621 to 59.90055, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 59.7061 - MinusLogProbMetric: 59.7061 - val_loss: 59.9006 - val_MinusLogProbMetric: 59.9006 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 255/1000
2023-10-25 21:19:04.214 
Epoch 255/1000 
	 loss: 61.7510, MinusLogProbMetric: 61.7510, val_loss: 65.9284, val_MinusLogProbMetric: 65.9284

Epoch 255: val_loss did not improve from 59.90055
196/196 - 69s - loss: 61.7510 - MinusLogProbMetric: 61.7510 - val_loss: 65.9284 - val_MinusLogProbMetric: 65.9284 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 256/1000
2023-10-25 21:20:12.843 
Epoch 256/1000 
	 loss: 69.5075, MinusLogProbMetric: 69.5075, val_loss: 82.0466, val_MinusLogProbMetric: 82.0466

Epoch 256: val_loss did not improve from 59.90055
196/196 - 69s - loss: 69.5075 - MinusLogProbMetric: 69.5075 - val_loss: 82.0466 - val_MinusLogProbMetric: 82.0466 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 257/1000
2023-10-25 21:21:21.338 
Epoch 257/1000 
	 loss: 65.5546, MinusLogProbMetric: 65.5546, val_loss: 62.0150, val_MinusLogProbMetric: 62.0150

Epoch 257: val_loss did not improve from 59.90055
196/196 - 68s - loss: 65.5546 - MinusLogProbMetric: 65.5546 - val_loss: 62.0150 - val_MinusLogProbMetric: 62.0150 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 258/1000
2023-10-25 21:22:29.594 
Epoch 258/1000 
	 loss: 61.2298, MinusLogProbMetric: 61.2298, val_loss: 61.1866, val_MinusLogProbMetric: 61.1866

Epoch 258: val_loss did not improve from 59.90055
196/196 - 68s - loss: 61.2298 - MinusLogProbMetric: 61.2298 - val_loss: 61.1866 - val_MinusLogProbMetric: 61.1866 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 259/1000
2023-10-25 21:23:39.079 
Epoch 259/1000 
	 loss: 60.5452, MinusLogProbMetric: 60.5452, val_loss: 71.8302, val_MinusLogProbMetric: 71.8302

Epoch 259: val_loss did not improve from 59.90055
196/196 - 69s - loss: 60.5452 - MinusLogProbMetric: 60.5452 - val_loss: 71.8302 - val_MinusLogProbMetric: 71.8302 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 260/1000
2023-10-25 21:24:47.284 
Epoch 260/1000 
	 loss: 60.4663, MinusLogProbMetric: 60.4663, val_loss: 60.3873, val_MinusLogProbMetric: 60.3873

Epoch 260: val_loss did not improve from 59.90055
196/196 - 68s - loss: 60.4663 - MinusLogProbMetric: 60.4663 - val_loss: 60.3873 - val_MinusLogProbMetric: 60.3873 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 261/1000
2023-10-25 21:25:55.818 
Epoch 261/1000 
	 loss: 60.0048, MinusLogProbMetric: 60.0048, val_loss: 60.0395, val_MinusLogProbMetric: 60.0395

Epoch 261: val_loss did not improve from 59.90055
196/196 - 69s - loss: 60.0048 - MinusLogProbMetric: 60.0048 - val_loss: 60.0395 - val_MinusLogProbMetric: 60.0395 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 262/1000
2023-10-25 21:27:03.573 
Epoch 262/1000 
	 loss: 59.6701, MinusLogProbMetric: 59.6701, val_loss: 60.0207, val_MinusLogProbMetric: 60.0207

Epoch 262: val_loss did not improve from 59.90055
196/196 - 68s - loss: 59.6701 - MinusLogProbMetric: 59.6701 - val_loss: 60.0207 - val_MinusLogProbMetric: 60.0207 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 263/1000
2023-10-25 21:28:12.020 
Epoch 263/1000 
	 loss: 59.4551, MinusLogProbMetric: 59.4551, val_loss: 59.5103, val_MinusLogProbMetric: 59.5103

Epoch 263: val_loss improved from 59.90055 to 59.51034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 59.4551 - MinusLogProbMetric: 59.4551 - val_loss: 59.5103 - val_MinusLogProbMetric: 59.5103 - lr: 1.2346e-05 - 69s/epoch - 355ms/step
Epoch 264/1000
2023-10-25 21:29:21.442 
Epoch 264/1000 
	 loss: 59.2773, MinusLogProbMetric: 59.2773, val_loss: 59.2631, val_MinusLogProbMetric: 59.2631

Epoch 264: val_loss improved from 59.51034 to 59.26310, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 59.2773 - MinusLogProbMetric: 59.2773 - val_loss: 59.2631 - val_MinusLogProbMetric: 59.2631 - lr: 1.2346e-05 - 69s/epoch - 355ms/step
Epoch 265/1000
2023-10-25 21:30:31.153 
Epoch 265/1000 
	 loss: 58.9423, MinusLogProbMetric: 58.9423, val_loss: 59.0328, val_MinusLogProbMetric: 59.0328

Epoch 265: val_loss improved from 59.26310 to 59.03278, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 70s - loss: 58.9423 - MinusLogProbMetric: 58.9423 - val_loss: 59.0328 - val_MinusLogProbMetric: 59.0328 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 266/1000
2023-10-25 21:31:40.100 
Epoch 266/1000 
	 loss: 58.6260, MinusLogProbMetric: 58.6260, val_loss: 58.6852, val_MinusLogProbMetric: 58.6852

Epoch 266: val_loss improved from 59.03278 to 58.68525, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 58.6260 - MinusLogProbMetric: 58.6260 - val_loss: 58.6852 - val_MinusLogProbMetric: 58.6852 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 267/1000
2023-10-25 21:32:50.016 
Epoch 267/1000 
	 loss: 60.3953, MinusLogProbMetric: 60.3953, val_loss: 58.5484, val_MinusLogProbMetric: 58.5484

Epoch 267: val_loss improved from 58.68525 to 58.54840, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 70s - loss: 60.3953 - MinusLogProbMetric: 60.3953 - val_loss: 58.5484 - val_MinusLogProbMetric: 58.5484 - lr: 1.2346e-05 - 70s/epoch - 357ms/step
Epoch 268/1000
2023-10-25 21:33:58.792 
Epoch 268/1000 
	 loss: 58.1910, MinusLogProbMetric: 58.1910, val_loss: 58.4547, val_MinusLogProbMetric: 58.4547

Epoch 268: val_loss improved from 58.54840 to 58.45473, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 58.1910 - MinusLogProbMetric: 58.1910 - val_loss: 58.4547 - val_MinusLogProbMetric: 58.4547 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 269/1000
2023-10-25 21:35:07.979 
Epoch 269/1000 
	 loss: 58.4902, MinusLogProbMetric: 58.4902, val_loss: 58.1629, val_MinusLogProbMetric: 58.1629

Epoch 269: val_loss improved from 58.45473 to 58.16293, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 58.4902 - MinusLogProbMetric: 58.4902 - val_loss: 58.1629 - val_MinusLogProbMetric: 58.1629 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 270/1000
2023-10-25 21:36:17.207 
Epoch 270/1000 
	 loss: 57.8861, MinusLogProbMetric: 57.8861, val_loss: 58.0666, val_MinusLogProbMetric: 58.0666

Epoch 270: val_loss improved from 58.16293 to 58.06656, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 57.8861 - MinusLogProbMetric: 57.8861 - val_loss: 58.0666 - val_MinusLogProbMetric: 58.0666 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 271/1000
2023-10-25 21:37:26.290 
Epoch 271/1000 
	 loss: 57.7261, MinusLogProbMetric: 57.7261, val_loss: 57.9608, val_MinusLogProbMetric: 57.9608

Epoch 271: val_loss improved from 58.06656 to 57.96080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 57.7261 - MinusLogProbMetric: 57.7261 - val_loss: 57.9608 - val_MinusLogProbMetric: 57.9608 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 272/1000
2023-10-25 21:38:34.859 
Epoch 272/1000 
	 loss: 57.6740, MinusLogProbMetric: 57.6740, val_loss: 57.8350, val_MinusLogProbMetric: 57.8350

Epoch 272: val_loss improved from 57.96080 to 57.83500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 57.6740 - MinusLogProbMetric: 57.6740 - val_loss: 57.8350 - val_MinusLogProbMetric: 57.8350 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 273/1000
2023-10-25 21:39:43.883 
Epoch 273/1000 
	 loss: 57.5110, MinusLogProbMetric: 57.5110, val_loss: 57.5991, val_MinusLogProbMetric: 57.5991

Epoch 273: val_loss improved from 57.83500 to 57.59912, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 57.5110 - MinusLogProbMetric: 57.5110 - val_loss: 57.5991 - val_MinusLogProbMetric: 57.5991 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 274/1000
2023-10-25 21:40:52.734 
Epoch 274/1000 
	 loss: 57.5116, MinusLogProbMetric: 57.5116, val_loss: 57.5196, val_MinusLogProbMetric: 57.5196

Epoch 274: val_loss improved from 57.59912 to 57.51960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 57.5116 - MinusLogProbMetric: 57.5116 - val_loss: 57.5196 - val_MinusLogProbMetric: 57.5196 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 275/1000
2023-10-25 21:42:01.702 
Epoch 275/1000 
	 loss: 57.2538, MinusLogProbMetric: 57.2538, val_loss: 57.4297, val_MinusLogProbMetric: 57.4297

Epoch 275: val_loss improved from 57.51960 to 57.42968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 57.2538 - MinusLogProbMetric: 57.2538 - val_loss: 57.4297 - val_MinusLogProbMetric: 57.4297 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 276/1000
2023-10-25 21:43:11.038 
Epoch 276/1000 
	 loss: 57.2093, MinusLogProbMetric: 57.2093, val_loss: 57.3704, val_MinusLogProbMetric: 57.3704

Epoch 276: val_loss improved from 57.42968 to 57.37042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 70s - loss: 57.2093 - MinusLogProbMetric: 57.2093 - val_loss: 57.3704 - val_MinusLogProbMetric: 57.3704 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 277/1000
2023-10-25 21:44:19.828 
Epoch 277/1000 
	 loss: 57.0940, MinusLogProbMetric: 57.0940, val_loss: 57.3179, val_MinusLogProbMetric: 57.3179

Epoch 277: val_loss improved from 57.37042 to 57.31789, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 57.0940 - MinusLogProbMetric: 57.0940 - val_loss: 57.3179 - val_MinusLogProbMetric: 57.3179 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 278/1000
2023-10-25 21:45:29.326 
Epoch 278/1000 
	 loss: 57.1524, MinusLogProbMetric: 57.1524, val_loss: 57.2175, val_MinusLogProbMetric: 57.2175

Epoch 278: val_loss improved from 57.31789 to 57.21748, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 57.1524 - MinusLogProbMetric: 57.1524 - val_loss: 57.2175 - val_MinusLogProbMetric: 57.2175 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 279/1000
2023-10-25 21:46:38.494 
Epoch 279/1000 
	 loss: 56.9265, MinusLogProbMetric: 56.9265, val_loss: 57.1674, val_MinusLogProbMetric: 57.1674

Epoch 279: val_loss improved from 57.21748 to 57.16743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 56.9265 - MinusLogProbMetric: 56.9265 - val_loss: 57.1674 - val_MinusLogProbMetric: 57.1674 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 280/1000
2023-10-25 21:47:46.964 
Epoch 280/1000 
	 loss: 56.8681, MinusLogProbMetric: 56.8681, val_loss: 57.0605, val_MinusLogProbMetric: 57.0605

Epoch 280: val_loss improved from 57.16743 to 57.06045, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 56.8681 - MinusLogProbMetric: 56.8681 - val_loss: 57.0605 - val_MinusLogProbMetric: 57.0605 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 281/1000
2023-10-25 21:48:55.137 
Epoch 281/1000 
	 loss: 56.6640, MinusLogProbMetric: 56.6640, val_loss: 56.8743, val_MinusLogProbMetric: 56.8743

Epoch 281: val_loss improved from 57.06045 to 56.87435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 56.6640 - MinusLogProbMetric: 56.6640 - val_loss: 56.8743 - val_MinusLogProbMetric: 56.8743 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 282/1000
2023-10-25 21:50:03.923 
Epoch 282/1000 
	 loss: 56.6290, MinusLogProbMetric: 56.6290, val_loss: 56.7777, val_MinusLogProbMetric: 56.7777

Epoch 282: val_loss improved from 56.87435 to 56.77768, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 56.6290 - MinusLogProbMetric: 56.6290 - val_loss: 56.7777 - val_MinusLogProbMetric: 56.7777 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 283/1000
2023-10-25 21:51:12.860 
Epoch 283/1000 
	 loss: 56.5157, MinusLogProbMetric: 56.5157, val_loss: 56.6838, val_MinusLogProbMetric: 56.6838

Epoch 283: val_loss improved from 56.77768 to 56.68376, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 56.5157 - MinusLogProbMetric: 56.5157 - val_loss: 56.6838 - val_MinusLogProbMetric: 56.6838 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 284/1000
2023-10-25 21:52:21.625 
Epoch 284/1000 
	 loss: 56.4787, MinusLogProbMetric: 56.4787, val_loss: 57.0120, val_MinusLogProbMetric: 57.0120

Epoch 284: val_loss did not improve from 56.68376
196/196 - 68s - loss: 56.4787 - MinusLogProbMetric: 56.4787 - val_loss: 57.0120 - val_MinusLogProbMetric: 57.0120 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 285/1000
2023-10-25 21:53:29.785 
Epoch 285/1000 
	 loss: 56.5544, MinusLogProbMetric: 56.5544, val_loss: 57.0338, val_MinusLogProbMetric: 57.0338

Epoch 285: val_loss did not improve from 56.68376
196/196 - 68s - loss: 56.5544 - MinusLogProbMetric: 56.5544 - val_loss: 57.0338 - val_MinusLogProbMetric: 57.0338 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 286/1000
2023-10-25 21:54:37.047 
Epoch 286/1000 
	 loss: 56.3835, MinusLogProbMetric: 56.3835, val_loss: 56.5235, val_MinusLogProbMetric: 56.5235

Epoch 286: val_loss improved from 56.68376 to 56.52355, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 56.3835 - MinusLogProbMetric: 56.3835 - val_loss: 56.5235 - val_MinusLogProbMetric: 56.5235 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 287/1000
2023-10-25 21:55:45.710 
Epoch 287/1000 
	 loss: 56.3086, MinusLogProbMetric: 56.3086, val_loss: 56.6237, val_MinusLogProbMetric: 56.6237

Epoch 287: val_loss did not improve from 56.52355
196/196 - 68s - loss: 56.3086 - MinusLogProbMetric: 56.3086 - val_loss: 56.6237 - val_MinusLogProbMetric: 56.6237 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 288/1000
2023-10-25 21:56:53.457 
Epoch 288/1000 
	 loss: 56.2299, MinusLogProbMetric: 56.2299, val_loss: 56.4192, val_MinusLogProbMetric: 56.4192

Epoch 288: val_loss improved from 56.52355 to 56.41920, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 56.2299 - MinusLogProbMetric: 56.2299 - val_loss: 56.4192 - val_MinusLogProbMetric: 56.4192 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 289/1000
2023-10-25 21:58:02.773 
Epoch 289/1000 
	 loss: 56.8105, MinusLogProbMetric: 56.8105, val_loss: 56.4523, val_MinusLogProbMetric: 56.4523

Epoch 289: val_loss did not improve from 56.41920
196/196 - 68s - loss: 56.8105 - MinusLogProbMetric: 56.8105 - val_loss: 56.4523 - val_MinusLogProbMetric: 56.4523 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 290/1000
2023-10-25 21:59:11.143 
Epoch 290/1000 
	 loss: 57.6853, MinusLogProbMetric: 57.6853, val_loss: 57.2291, val_MinusLogProbMetric: 57.2291

Epoch 290: val_loss did not improve from 56.41920
196/196 - 68s - loss: 57.6853 - MinusLogProbMetric: 57.6853 - val_loss: 57.2291 - val_MinusLogProbMetric: 57.2291 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 291/1000
2023-10-25 22:00:18.915 
Epoch 291/1000 
	 loss: 56.4206, MinusLogProbMetric: 56.4206, val_loss: 56.2455, val_MinusLogProbMetric: 56.2455

Epoch 291: val_loss improved from 56.41920 to 56.24548, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 56.4206 - MinusLogProbMetric: 56.4206 - val_loss: 56.2455 - val_MinusLogProbMetric: 56.2455 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 292/1000
2023-10-25 22:01:26.435 
Epoch 292/1000 
	 loss: 69.6318, MinusLogProbMetric: 69.6318, val_loss: 60.1005, val_MinusLogProbMetric: 60.1005

Epoch 292: val_loss did not improve from 56.24548
196/196 - 67s - loss: 69.6318 - MinusLogProbMetric: 69.6318 - val_loss: 60.1005 - val_MinusLogProbMetric: 60.1005 - lr: 1.2346e-05 - 67s/epoch - 340ms/step
Epoch 293/1000
2023-10-25 22:02:33.999 
Epoch 293/1000 
	 loss: 59.0408, MinusLogProbMetric: 59.0408, val_loss: 61.2403, val_MinusLogProbMetric: 61.2403

Epoch 293: val_loss did not improve from 56.24548
196/196 - 68s - loss: 59.0408 - MinusLogProbMetric: 59.0408 - val_loss: 61.2403 - val_MinusLogProbMetric: 61.2403 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 294/1000
2023-10-25 22:03:41.292 
Epoch 294/1000 
	 loss: 57.3825, MinusLogProbMetric: 57.3825, val_loss: 57.1361, val_MinusLogProbMetric: 57.1361

Epoch 294: val_loss did not improve from 56.24548
196/196 - 67s - loss: 57.3825 - MinusLogProbMetric: 57.3825 - val_loss: 57.1361 - val_MinusLogProbMetric: 57.1361 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 295/1000
2023-10-25 22:04:48.881 
Epoch 295/1000 
	 loss: 56.6960, MinusLogProbMetric: 56.6960, val_loss: 56.7634, val_MinusLogProbMetric: 56.7634

Epoch 295: val_loss did not improve from 56.24548
196/196 - 68s - loss: 56.6960 - MinusLogProbMetric: 56.6960 - val_loss: 56.7634 - val_MinusLogProbMetric: 56.7634 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 296/1000
2023-10-25 22:05:56.434 
Epoch 296/1000 
	 loss: 56.3794, MinusLogProbMetric: 56.3794, val_loss: 56.6095, val_MinusLogProbMetric: 56.6095

Epoch 296: val_loss did not improve from 56.24548
196/196 - 68s - loss: 56.3794 - MinusLogProbMetric: 56.3794 - val_loss: 56.6095 - val_MinusLogProbMetric: 56.6095 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 297/1000
2023-10-25 22:07:03.271 
Epoch 297/1000 
	 loss: 56.1846, MinusLogProbMetric: 56.1846, val_loss: 56.8060, val_MinusLogProbMetric: 56.8060

Epoch 297: val_loss did not improve from 56.24548
196/196 - 67s - loss: 56.1846 - MinusLogProbMetric: 56.1846 - val_loss: 56.8060 - val_MinusLogProbMetric: 56.8060 - lr: 1.2346e-05 - 67s/epoch - 341ms/step
Epoch 298/1000
2023-10-25 22:08:10.733 
Epoch 298/1000 
	 loss: 56.0230, MinusLogProbMetric: 56.0230, val_loss: 56.1728, val_MinusLogProbMetric: 56.1728

Epoch 298: val_loss improved from 56.24548 to 56.17282, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 56.0230 - MinusLogProbMetric: 56.0230 - val_loss: 56.1728 - val_MinusLogProbMetric: 56.1728 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 299/1000
2023-10-25 22:09:18.614 
Epoch 299/1000 
	 loss: 55.8516, MinusLogProbMetric: 55.8516, val_loss: 56.0685, val_MinusLogProbMetric: 56.0685

Epoch 299: val_loss improved from 56.17282 to 56.06851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 55.8516 - MinusLogProbMetric: 55.8516 - val_loss: 56.0685 - val_MinusLogProbMetric: 56.0685 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 300/1000
2023-10-25 22:10:26.793 
Epoch 300/1000 
	 loss: 55.6969, MinusLogProbMetric: 55.6969, val_loss: 55.9682, val_MinusLogProbMetric: 55.9682

Epoch 300: val_loss improved from 56.06851 to 55.96820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 55.6969 - MinusLogProbMetric: 55.6969 - val_loss: 55.9682 - val_MinusLogProbMetric: 55.9682 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 301/1000
2023-10-25 22:11:34.518 
Epoch 301/1000 
	 loss: 55.6839, MinusLogProbMetric: 55.6839, val_loss: 55.9148, val_MinusLogProbMetric: 55.9148

Epoch 301: val_loss improved from 55.96820 to 55.91481, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 55.6839 - MinusLogProbMetric: 55.6839 - val_loss: 55.9148 - val_MinusLogProbMetric: 55.9148 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 302/1000
2023-10-25 22:12:42.510 
Epoch 302/1000 
	 loss: 55.5104, MinusLogProbMetric: 55.5104, val_loss: 55.8110, val_MinusLogProbMetric: 55.8110

Epoch 302: val_loss improved from 55.91481 to 55.81100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 55.5104 - MinusLogProbMetric: 55.5104 - val_loss: 55.8110 - val_MinusLogProbMetric: 55.8110 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 303/1000
2023-10-25 22:13:50.940 
Epoch 303/1000 
	 loss: 56.0349, MinusLogProbMetric: 56.0349, val_loss: 55.9294, val_MinusLogProbMetric: 55.9294

Epoch 303: val_loss did not improve from 55.81100
196/196 - 68s - loss: 56.0349 - MinusLogProbMetric: 56.0349 - val_loss: 55.9294 - val_MinusLogProbMetric: 55.9294 - lr: 1.2346e-05 - 68s/epoch - 344ms/step
Epoch 304/1000
2023-10-25 22:14:58.360 
Epoch 304/1000 
	 loss: 55.5245, MinusLogProbMetric: 55.5245, val_loss: 55.6290, val_MinusLogProbMetric: 55.6290

Epoch 304: val_loss improved from 55.81100 to 55.62896, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 55.5245 - MinusLogProbMetric: 55.5245 - val_loss: 55.6290 - val_MinusLogProbMetric: 55.6290 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 305/1000
2023-10-25 22:16:06.523 
Epoch 305/1000 
	 loss: 57.3720, MinusLogProbMetric: 57.3720, val_loss: 55.5855, val_MinusLogProbMetric: 55.5855

Epoch 305: val_loss improved from 55.62896 to 55.58550, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 57.3720 - MinusLogProbMetric: 57.3720 - val_loss: 55.5855 - val_MinusLogProbMetric: 55.5855 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 306/1000
2023-10-25 22:17:14.921 
Epoch 306/1000 
	 loss: 55.3361, MinusLogProbMetric: 55.3361, val_loss: 55.3001, val_MinusLogProbMetric: 55.3001

Epoch 306: val_loss improved from 55.58550 to 55.30007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 55.3361 - MinusLogProbMetric: 55.3361 - val_loss: 55.3001 - val_MinusLogProbMetric: 55.3001 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 307/1000
2023-10-25 22:18:23.639 
Epoch 307/1000 
	 loss: 54.9292, MinusLogProbMetric: 54.9292, val_loss: 55.4032, val_MinusLogProbMetric: 55.4032

Epoch 307: val_loss did not improve from 55.30007
196/196 - 68s - loss: 54.9292 - MinusLogProbMetric: 54.9292 - val_loss: 55.4032 - val_MinusLogProbMetric: 55.4032 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 308/1000
2023-10-25 22:19:31.107 
Epoch 308/1000 
	 loss: 54.7147, MinusLogProbMetric: 54.7147, val_loss: 54.7588, val_MinusLogProbMetric: 54.7588

Epoch 308: val_loss improved from 55.30007 to 54.75877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 54.7147 - MinusLogProbMetric: 54.7147 - val_loss: 54.7588 - val_MinusLogProbMetric: 54.7588 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 309/1000
2023-10-25 22:20:39.655 
Epoch 309/1000 
	 loss: 54.5369, MinusLogProbMetric: 54.5369, val_loss: 54.8115, val_MinusLogProbMetric: 54.8115

Epoch 309: val_loss did not improve from 54.75877
196/196 - 68s - loss: 54.5369 - MinusLogProbMetric: 54.5369 - val_loss: 54.8115 - val_MinusLogProbMetric: 54.8115 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 310/1000
2023-10-25 22:21:47.529 
Epoch 310/1000 
	 loss: 54.3075, MinusLogProbMetric: 54.3075, val_loss: 54.5182, val_MinusLogProbMetric: 54.5182

Epoch 310: val_loss improved from 54.75877 to 54.51815, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 54.3075 - MinusLogProbMetric: 54.3075 - val_loss: 54.5182 - val_MinusLogProbMetric: 54.5182 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 311/1000
2023-10-25 22:22:55.455 
Epoch 311/1000 
	 loss: 54.2821, MinusLogProbMetric: 54.2821, val_loss: 54.4718, val_MinusLogProbMetric: 54.4718

Epoch 311: val_loss improved from 54.51815 to 54.47176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 54.2821 - MinusLogProbMetric: 54.2821 - val_loss: 54.4718 - val_MinusLogProbMetric: 54.4718 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 312/1000
2023-10-25 22:24:03.945 
Epoch 312/1000 
	 loss: 54.1362, MinusLogProbMetric: 54.1362, val_loss: 54.3057, val_MinusLogProbMetric: 54.3057

Epoch 312: val_loss improved from 54.47176 to 54.30569, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 54.1362 - MinusLogProbMetric: 54.1362 - val_loss: 54.3057 - val_MinusLogProbMetric: 54.3057 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 313/1000
2023-10-25 22:25:12.217 
Epoch 313/1000 
	 loss: 56.0398, MinusLogProbMetric: 56.0398, val_loss: 63.8650, val_MinusLogProbMetric: 63.8650

Epoch 313: val_loss did not improve from 54.30569
196/196 - 67s - loss: 56.0398 - MinusLogProbMetric: 56.0398 - val_loss: 63.8650 - val_MinusLogProbMetric: 63.8650 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 314/1000
2023-10-25 22:26:20.408 
Epoch 314/1000 
	 loss: 55.3017, MinusLogProbMetric: 55.3017, val_loss: 54.2480, val_MinusLogProbMetric: 54.2480

Epoch 314: val_loss improved from 54.30569 to 54.24797, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 55.3017 - MinusLogProbMetric: 55.3017 - val_loss: 54.2480 - val_MinusLogProbMetric: 54.2480 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 315/1000
2023-10-25 22:27:28.579 
Epoch 315/1000 
	 loss: 53.9466, MinusLogProbMetric: 53.9466, val_loss: 54.1474, val_MinusLogProbMetric: 54.1474

Epoch 315: val_loss improved from 54.24797 to 54.14738, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 53.9466 - MinusLogProbMetric: 53.9466 - val_loss: 54.1474 - val_MinusLogProbMetric: 54.1474 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 316/1000
2023-10-25 22:28:37.788 
Epoch 316/1000 
	 loss: 53.8703, MinusLogProbMetric: 53.8703, val_loss: 54.1439, val_MinusLogProbMetric: 54.1439

Epoch 316: val_loss improved from 54.14738 to 54.14386, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 53.8703 - MinusLogProbMetric: 53.8703 - val_loss: 54.1439 - val_MinusLogProbMetric: 54.1439 - lr: 1.2346e-05 - 69s/epoch - 353ms/step
Epoch 317/1000
2023-10-25 22:29:41.340 
Epoch 317/1000 
	 loss: 53.8226, MinusLogProbMetric: 53.8226, val_loss: 53.9769, val_MinusLogProbMetric: 53.9769

Epoch 317: val_loss improved from 54.14386 to 53.97685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 63s - loss: 53.8226 - MinusLogProbMetric: 53.8226 - val_loss: 53.9769 - val_MinusLogProbMetric: 53.9769 - lr: 1.2346e-05 - 63s/epoch - 323ms/step
Epoch 318/1000
2023-10-25 22:30:49.112 
Epoch 318/1000 
	 loss: 53.7919, MinusLogProbMetric: 53.7919, val_loss: 53.9683, val_MinusLogProbMetric: 53.9683

Epoch 318: val_loss improved from 53.97685 to 53.96833, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 68s - loss: 53.7919 - MinusLogProbMetric: 53.7919 - val_loss: 53.9683 - val_MinusLogProbMetric: 53.9683 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 319/1000
2023-10-25 22:31:58.206 
Epoch 319/1000 
	 loss: 53.8072, MinusLogProbMetric: 53.8072, val_loss: 55.2388, val_MinusLogProbMetric: 55.2388

Epoch 319: val_loss did not improve from 53.96833
196/196 - 68s - loss: 53.8072 - MinusLogProbMetric: 53.8072 - val_loss: 55.2388 - val_MinusLogProbMetric: 55.2388 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 320/1000
2023-10-25 22:33:06.038 
Epoch 320/1000 
	 loss: 53.7878, MinusLogProbMetric: 53.7878, val_loss: 53.9246, val_MinusLogProbMetric: 53.9246

Epoch 320: val_loss improved from 53.96833 to 53.92463, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 53.7878 - MinusLogProbMetric: 53.7878 - val_loss: 53.9246 - val_MinusLogProbMetric: 53.9246 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 321/1000
2023-10-25 22:34:15.505 
Epoch 321/1000 
	 loss: 53.7381, MinusLogProbMetric: 53.7381, val_loss: 53.8085, val_MinusLogProbMetric: 53.8085

Epoch 321: val_loss improved from 53.92463 to 53.80853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 53.7381 - MinusLogProbMetric: 53.7381 - val_loss: 53.8085 - val_MinusLogProbMetric: 53.8085 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 322/1000
2023-10-25 22:35:24.935 
Epoch 322/1000 
	 loss: 53.7195, MinusLogProbMetric: 53.7195, val_loss: 53.7820, val_MinusLogProbMetric: 53.7820

Epoch 322: val_loss improved from 53.80853 to 53.78203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 70s - loss: 53.7195 - MinusLogProbMetric: 53.7195 - val_loss: 53.7820 - val_MinusLogProbMetric: 53.7820 - lr: 1.2346e-05 - 70s/epoch - 356ms/step
Epoch 323/1000
2023-10-25 22:36:34.101 
Epoch 323/1000 
	 loss: 53.5462, MinusLogProbMetric: 53.5462, val_loss: 53.4528, val_MinusLogProbMetric: 53.4528

Epoch 323: val_loss improved from 53.78203 to 53.45282, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 53.5462 - MinusLogProbMetric: 53.5462 - val_loss: 53.4528 - val_MinusLogProbMetric: 53.4528 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 324/1000
2023-10-25 22:37:44.198 
Epoch 324/1000 
	 loss: 53.2797, MinusLogProbMetric: 53.2797, val_loss: 53.5437, val_MinusLogProbMetric: 53.5437

Epoch 324: val_loss did not improve from 53.45282
196/196 - 69s - loss: 53.2797 - MinusLogProbMetric: 53.2797 - val_loss: 53.5437 - val_MinusLogProbMetric: 53.5437 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 325/1000
2023-10-25 22:38:52.522 
Epoch 325/1000 
	 loss: 53.8890, MinusLogProbMetric: 53.8890, val_loss: 53.6993, val_MinusLogProbMetric: 53.6993

Epoch 325: val_loss did not improve from 53.45282
196/196 - 68s - loss: 53.8890 - MinusLogProbMetric: 53.8890 - val_loss: 53.6993 - val_MinusLogProbMetric: 53.6993 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 326/1000
2023-10-25 22:40:00.947 
Epoch 326/1000 
	 loss: 53.7661, MinusLogProbMetric: 53.7661, val_loss: 54.8119, val_MinusLogProbMetric: 54.8119

Epoch 326: val_loss did not improve from 53.45282
196/196 - 68s - loss: 53.7661 - MinusLogProbMetric: 53.7661 - val_loss: 54.8119 - val_MinusLogProbMetric: 54.8119 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 327/1000
2023-10-25 22:41:09.079 
Epoch 327/1000 
	 loss: 53.6721, MinusLogProbMetric: 53.6721, val_loss: 53.5849, val_MinusLogProbMetric: 53.5849

Epoch 327: val_loss did not improve from 53.45282
196/196 - 68s - loss: 53.6721 - MinusLogProbMetric: 53.6721 - val_loss: 53.5849 - val_MinusLogProbMetric: 53.5849 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 328/1000
2023-10-25 22:42:16.832 
Epoch 328/1000 
	 loss: 53.2698, MinusLogProbMetric: 53.2698, val_loss: 53.4503, val_MinusLogProbMetric: 53.4503

Epoch 328: val_loss improved from 53.45282 to 53.45028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 53.2698 - MinusLogProbMetric: 53.2698 - val_loss: 53.4503 - val_MinusLogProbMetric: 53.4503 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 329/1000
2023-10-25 22:43:26.585 
Epoch 329/1000 
	 loss: 56.1764, MinusLogProbMetric: 56.1764, val_loss: 54.3200, val_MinusLogProbMetric: 54.3200

Epoch 329: val_loss did not improve from 53.45028
196/196 - 68s - loss: 56.1764 - MinusLogProbMetric: 56.1764 - val_loss: 54.3200 - val_MinusLogProbMetric: 54.3200 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 330/1000
2023-10-25 22:44:35.223 
Epoch 330/1000 
	 loss: 53.5710, MinusLogProbMetric: 53.5710, val_loss: 53.7487, val_MinusLogProbMetric: 53.7487

Epoch 330: val_loss did not improve from 53.45028
196/196 - 69s - loss: 53.5710 - MinusLogProbMetric: 53.5710 - val_loss: 53.7487 - val_MinusLogProbMetric: 53.7487 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 331/1000
2023-10-25 22:45:43.840 
Epoch 331/1000 
	 loss: 53.2689, MinusLogProbMetric: 53.2689, val_loss: 53.3837, val_MinusLogProbMetric: 53.3837

Epoch 331: val_loss improved from 53.45028 to 53.38368, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 70s - loss: 53.2689 - MinusLogProbMetric: 53.2689 - val_loss: 53.3837 - val_MinusLogProbMetric: 53.3837 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 332/1000
2023-10-25 22:46:53.580 
Epoch 332/1000 
	 loss: 53.2047, MinusLogProbMetric: 53.2047, val_loss: 53.3699, val_MinusLogProbMetric: 53.3699

Epoch 332: val_loss improved from 53.38368 to 53.36988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 70s - loss: 53.2047 - MinusLogProbMetric: 53.2047 - val_loss: 53.3699 - val_MinusLogProbMetric: 53.3699 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 333/1000
2023-10-25 22:48:02.139 
Epoch 333/1000 
	 loss: 53.0341, MinusLogProbMetric: 53.0341, val_loss: 53.2682, val_MinusLogProbMetric: 53.2682

Epoch 333: val_loss improved from 53.36988 to 53.26821, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 53.0341 - MinusLogProbMetric: 53.0341 - val_loss: 53.2682 - val_MinusLogProbMetric: 53.2682 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 334/1000
2023-10-25 22:49:11.429 
Epoch 334/1000 
	 loss: 54.9548, MinusLogProbMetric: 54.9548, val_loss: 61.8882, val_MinusLogProbMetric: 61.8882

Epoch 334: val_loss did not improve from 53.26821
196/196 - 68s - loss: 54.9548 - MinusLogProbMetric: 54.9548 - val_loss: 61.8882 - val_MinusLogProbMetric: 61.8882 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 335/1000
2023-10-25 22:50:19.682 
Epoch 335/1000 
	 loss: 63.4883, MinusLogProbMetric: 63.4883, val_loss: 55.7034, val_MinusLogProbMetric: 55.7034

Epoch 335: val_loss did not improve from 53.26821
196/196 - 68s - loss: 63.4883 - MinusLogProbMetric: 63.4883 - val_loss: 55.7034 - val_MinusLogProbMetric: 55.7034 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 336/1000
2023-10-25 22:51:28.043 
Epoch 336/1000 
	 loss: 53.5150, MinusLogProbMetric: 53.5150, val_loss: 53.1604, val_MinusLogProbMetric: 53.1604

Epoch 336: val_loss improved from 53.26821 to 53.16036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 53.5150 - MinusLogProbMetric: 53.5150 - val_loss: 53.1604 - val_MinusLogProbMetric: 53.1604 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 337/1000
2023-10-25 22:52:37.157 
Epoch 337/1000 
	 loss: 52.7259, MinusLogProbMetric: 52.7259, val_loss: 52.9167, val_MinusLogProbMetric: 52.9167

Epoch 337: val_loss improved from 53.16036 to 52.91674, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 52.7259 - MinusLogProbMetric: 52.7259 - val_loss: 52.9167 - val_MinusLogProbMetric: 52.9167 - lr: 1.2346e-05 - 69s/epoch - 352ms/step
Epoch 338/1000
2023-10-25 22:53:46.610 
Epoch 338/1000 
	 loss: 52.6065, MinusLogProbMetric: 52.6065, val_loss: 52.8734, val_MinusLogProbMetric: 52.8734

Epoch 338: val_loss improved from 52.91674 to 52.87337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 52.6065 - MinusLogProbMetric: 52.6065 - val_loss: 52.8734 - val_MinusLogProbMetric: 52.8734 - lr: 1.2346e-05 - 69s/epoch - 354ms/step
Epoch 339/1000
2023-10-25 22:54:55.517 
Epoch 339/1000 
	 loss: 52.5153, MinusLogProbMetric: 52.5153, val_loss: 52.8964, val_MinusLogProbMetric: 52.8964

Epoch 339: val_loss did not improve from 52.87337
196/196 - 68s - loss: 52.5153 - MinusLogProbMetric: 52.5153 - val_loss: 52.8964 - val_MinusLogProbMetric: 52.8964 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 340/1000
2023-10-25 22:56:03.278 
Epoch 340/1000 
	 loss: 52.4487, MinusLogProbMetric: 52.4487, val_loss: 52.7087, val_MinusLogProbMetric: 52.7087

Epoch 340: val_loss improved from 52.87337 to 52.70873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 69s - loss: 52.4487 - MinusLogProbMetric: 52.4487 - val_loss: 52.7087 - val_MinusLogProbMetric: 52.7087 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 341/1000
2023-10-25 22:57:11.730 
Epoch 341/1000 
	 loss: 52.4146, MinusLogProbMetric: 52.4146, val_loss: 52.7225, val_MinusLogProbMetric: 52.7225

Epoch 341: val_loss did not improve from 52.70873
196/196 - 67s - loss: 52.4146 - MinusLogProbMetric: 52.4146 - val_loss: 52.7225 - val_MinusLogProbMetric: 52.7225 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 342/1000
2023-10-25 22:58:20.271 
Epoch 342/1000 
	 loss: 53.2264, MinusLogProbMetric: 53.2264, val_loss: 52.4911, val_MinusLogProbMetric: 52.4911

Epoch 342: val_loss improved from 52.70873 to 52.49111, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_358/weights/best_weights.h5
196/196 - 70s - loss: 53.2264 - MinusLogProbMetric: 53.2264 - val_loss: 52.4911 - val_MinusLogProbMetric: 52.4911 - lr: 1.2346e-05 - 70s/epoch - 355ms/step
Epoch 343/1000
2023-10-25 22:59:29.648 
Epoch 343/1000 
	 loss: 105.4220, MinusLogProbMetric: 105.4220, val_loss: 74.2920, val_MinusLogProbMetric: 74.2920

Epoch 343: val_loss did not improve from 52.49111
196/196 - 68s - loss: 105.4220 - MinusLogProbMetric: 105.4220 - val_loss: 74.2920 - val_MinusLogProbMetric: 74.2920 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 344/1000
2023-10-25 23:00:37.417 
Epoch 344/1000 
	 loss: 68.3698, MinusLogProbMetric: 68.3698, val_loss: 64.8503, val_MinusLogProbMetric: 64.8503

Epoch 344: val_loss did not improve from 52.49111
196/196 - 68s - loss: 68.3698 - MinusLogProbMetric: 68.3698 - val_loss: 64.8503 - val_MinusLogProbMetric: 64.8503 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 345/1000
2023-10-25 23:01:45.268 
Epoch 345/1000 
	 loss: 99.8093, MinusLogProbMetric: 99.8093, val_loss: 73.2005, val_MinusLogProbMetric: 73.2005

Epoch 345: val_loss did not improve from 52.49111
196/196 - 68s - loss: 99.8093 - MinusLogProbMetric: 99.8093 - val_loss: 73.2005 - val_MinusLogProbMetric: 73.2005 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 346/1000
2023-10-25 23:02:52.765 
Epoch 346/1000 
	 loss: 71.1753, MinusLogProbMetric: 71.1753, val_loss: 65.2966, val_MinusLogProbMetric: 65.2966

Epoch 346: val_loss did not improve from 52.49111
196/196 - 67s - loss: 71.1753 - MinusLogProbMetric: 71.1753 - val_loss: 65.2966 - val_MinusLogProbMetric: 65.2966 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 347/1000
2023-10-25 23:04:00.343 
Epoch 347/1000 
	 loss: 64.3951, MinusLogProbMetric: 64.3951, val_loss: 65.4708, val_MinusLogProbMetric: 65.4708

Epoch 347: val_loss did not improve from 52.49111
196/196 - 68s - loss: 64.3951 - MinusLogProbMetric: 64.3951 - val_loss: 65.4708 - val_MinusLogProbMetric: 65.4708 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 348/1000
2023-10-25 23:05:08.503 
Epoch 348/1000 
	 loss: 165.9902, MinusLogProbMetric: 165.9902, val_loss: 179.6131, val_MinusLogProbMetric: 179.6131

Epoch 348: val_loss did not improve from 52.49111
196/196 - 68s - loss: 165.9902 - MinusLogProbMetric: 165.9902 - val_loss: 179.6131 - val_MinusLogProbMetric: 179.6131 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 349/1000
2023-10-25 23:06:16.859 
Epoch 349/1000 
	 loss: 148.0385, MinusLogProbMetric: 148.0385, val_loss: 122.7449, val_MinusLogProbMetric: 122.7449

Epoch 349: val_loss did not improve from 52.49111
196/196 - 68s - loss: 148.0385 - MinusLogProbMetric: 148.0385 - val_loss: 122.7449 - val_MinusLogProbMetric: 122.7449 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 350/1000
2023-10-25 23:07:25.577 
Epoch 350/1000 
	 loss: 109.7050, MinusLogProbMetric: 109.7050, val_loss: 101.0458, val_MinusLogProbMetric: 101.0458

Epoch 350: val_loss did not improve from 52.49111
196/196 - 69s - loss: 109.7050 - MinusLogProbMetric: 109.7050 - val_loss: 101.0458 - val_MinusLogProbMetric: 101.0458 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 351/1000
2023-10-25 23:08:33.388 
Epoch 351/1000 
	 loss: 95.1666, MinusLogProbMetric: 95.1666, val_loss: 89.9204, val_MinusLogProbMetric: 89.9204

Epoch 351: val_loss did not improve from 52.49111
196/196 - 68s - loss: 95.1666 - MinusLogProbMetric: 95.1666 - val_loss: 89.9204 - val_MinusLogProbMetric: 89.9204 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 352/1000
2023-10-25 23:09:41.259 
Epoch 352/1000 
	 loss: 87.9065, MinusLogProbMetric: 87.9065, val_loss: 89.9951, val_MinusLogProbMetric: 89.9951

Epoch 352: val_loss did not improve from 52.49111
196/196 - 68s - loss: 87.9065 - MinusLogProbMetric: 87.9065 - val_loss: 89.9951 - val_MinusLogProbMetric: 89.9951 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 353/1000
2023-10-25 23:10:49.098 
Epoch 353/1000 
	 loss: 80.9083, MinusLogProbMetric: 80.9083, val_loss: 77.7591, val_MinusLogProbMetric: 77.7591

Epoch 353: val_loss did not improve from 52.49111
196/196 - 68s - loss: 80.9083 - MinusLogProbMetric: 80.9083 - val_loss: 77.7591 - val_MinusLogProbMetric: 77.7591 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 354/1000
2023-10-25 23:11:57.245 
Epoch 354/1000 
	 loss: 75.8509, MinusLogProbMetric: 75.8509, val_loss: 74.4465, val_MinusLogProbMetric: 74.4465

Epoch 354: val_loss did not improve from 52.49111
196/196 - 68s - loss: 75.8509 - MinusLogProbMetric: 75.8509 - val_loss: 74.4465 - val_MinusLogProbMetric: 74.4465 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 355/1000
2023-10-25 23:13:04.560 
Epoch 355/1000 
	 loss: 73.0712, MinusLogProbMetric: 73.0712, val_loss: 72.0673, val_MinusLogProbMetric: 72.0673

Epoch 355: val_loss did not improve from 52.49111
196/196 - 67s - loss: 73.0712 - MinusLogProbMetric: 73.0712 - val_loss: 72.0673 - val_MinusLogProbMetric: 72.0673 - lr: 1.2346e-05 - 67s/epoch - 343ms/step
Epoch 356/1000
2023-10-25 23:14:12.959 
Epoch 356/1000 
	 loss: 72.0200, MinusLogProbMetric: 72.0200, val_loss: 70.5413, val_MinusLogProbMetric: 70.5413

Epoch 356: val_loss did not improve from 52.49111
196/196 - 68s - loss: 72.0200 - MinusLogProbMetric: 72.0200 - val_loss: 70.5413 - val_MinusLogProbMetric: 70.5413 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 357/1000
2023-10-25 23:15:20.600 
Epoch 357/1000 
	 loss: 69.1807, MinusLogProbMetric: 69.1807, val_loss: 68.5931, val_MinusLogProbMetric: 68.5931

Epoch 357: val_loss did not improve from 52.49111
196/196 - 68s - loss: 69.1807 - MinusLogProbMetric: 69.1807 - val_loss: 68.5931 - val_MinusLogProbMetric: 68.5931 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 358/1000
2023-10-25 23:16:28.617 
Epoch 358/1000 
	 loss: 67.5064, MinusLogProbMetric: 67.5064, val_loss: 66.9699, val_MinusLogProbMetric: 66.9699

Epoch 358: val_loss did not improve from 52.49111
196/196 - 68s - loss: 67.5064 - MinusLogProbMetric: 67.5064 - val_loss: 66.9699 - val_MinusLogProbMetric: 66.9699 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 359/1000
2023-10-25 23:17:36.990 
Epoch 359/1000 
	 loss: 66.0036, MinusLogProbMetric: 66.0036, val_loss: 65.5454, val_MinusLogProbMetric: 65.5454

Epoch 359: val_loss did not improve from 52.49111
196/196 - 68s - loss: 66.0036 - MinusLogProbMetric: 66.0036 - val_loss: 65.5454 - val_MinusLogProbMetric: 65.5454 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 360/1000
2023-10-25 23:18:44.643 
Epoch 360/1000 
	 loss: 64.3721, MinusLogProbMetric: 64.3721, val_loss: 63.4603, val_MinusLogProbMetric: 63.4603

Epoch 360: val_loss did not improve from 52.49111
196/196 - 68s - loss: 64.3721 - MinusLogProbMetric: 64.3721 - val_loss: 63.4603 - val_MinusLogProbMetric: 63.4603 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 361/1000
2023-10-25 23:19:52.702 
Epoch 361/1000 
	 loss: 62.8772, MinusLogProbMetric: 62.8772, val_loss: 62.6322, val_MinusLogProbMetric: 62.6322

Epoch 361: val_loss did not improve from 52.49111
196/196 - 68s - loss: 62.8772 - MinusLogProbMetric: 62.8772 - val_loss: 62.6322 - val_MinusLogProbMetric: 62.6322 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 362/1000
2023-10-25 23:21:00.646 
Epoch 362/1000 
	 loss: 62.0364, MinusLogProbMetric: 62.0364, val_loss: 61.9122, val_MinusLogProbMetric: 61.9122

Epoch 362: val_loss did not improve from 52.49111
196/196 - 68s - loss: 62.0364 - MinusLogProbMetric: 62.0364 - val_loss: 61.9122 - val_MinusLogProbMetric: 61.9122 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 363/1000
2023-10-25 23:22:08.081 
Epoch 363/1000 
	 loss: 61.2992, MinusLogProbMetric: 61.2992, val_loss: 61.0106, val_MinusLogProbMetric: 61.0106

Epoch 363: val_loss did not improve from 52.49111
196/196 - 67s - loss: 61.2992 - MinusLogProbMetric: 61.2992 - val_loss: 61.0106 - val_MinusLogProbMetric: 61.0106 - lr: 1.2346e-05 - 67s/epoch - 344ms/step
Epoch 364/1000
2023-10-25 23:23:16.179 
Epoch 364/1000 
	 loss: 60.6471, MinusLogProbMetric: 60.6471, val_loss: 60.5200, val_MinusLogProbMetric: 60.5200

Epoch 364: val_loss did not improve from 52.49111
196/196 - 68s - loss: 60.6471 - MinusLogProbMetric: 60.6471 - val_loss: 60.5200 - val_MinusLogProbMetric: 60.5200 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 365/1000
2023-10-25 23:24:23.981 
Epoch 365/1000 
	 loss: 60.2618, MinusLogProbMetric: 60.2618, val_loss: 60.1810, val_MinusLogProbMetric: 60.1810

Epoch 365: val_loss did not improve from 52.49111
196/196 - 68s - loss: 60.2618 - MinusLogProbMetric: 60.2618 - val_loss: 60.1810 - val_MinusLogProbMetric: 60.1810 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 366/1000
2023-10-25 23:25:32.516 
Epoch 366/1000 
	 loss: 108.0181, MinusLogProbMetric: 108.0181, val_loss: 196.8136, val_MinusLogProbMetric: 196.8136

Epoch 366: val_loss did not improve from 52.49111
196/196 - 69s - loss: 108.0181 - MinusLogProbMetric: 108.0181 - val_loss: 196.8136 - val_MinusLogProbMetric: 196.8136 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 367/1000
2023-10-25 23:26:40.275 
Epoch 367/1000 
	 loss: 117.5344, MinusLogProbMetric: 117.5344, val_loss: 94.9113, val_MinusLogProbMetric: 94.9113

Epoch 367: val_loss did not improve from 52.49111
196/196 - 68s - loss: 117.5344 - MinusLogProbMetric: 117.5344 - val_loss: 94.9113 - val_MinusLogProbMetric: 94.9113 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 368/1000
2023-10-25 23:27:48.968 
Epoch 368/1000 
	 loss: 86.0984, MinusLogProbMetric: 86.0984, val_loss: 80.1551, val_MinusLogProbMetric: 80.1551

Epoch 368: val_loss did not improve from 52.49111
196/196 - 69s - loss: 86.0984 - MinusLogProbMetric: 86.0984 - val_loss: 80.1551 - val_MinusLogProbMetric: 80.1551 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 369/1000
2023-10-25 23:28:57.714 
Epoch 369/1000 
	 loss: 76.8648, MinusLogProbMetric: 76.8648, val_loss: 74.5122, val_MinusLogProbMetric: 74.5122

Epoch 369: val_loss did not improve from 52.49111
196/196 - 69s - loss: 76.8648 - MinusLogProbMetric: 76.8648 - val_loss: 74.5122 - val_MinusLogProbMetric: 74.5122 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 370/1000
2023-10-25 23:30:06.059 
Epoch 370/1000 
	 loss: 73.2883, MinusLogProbMetric: 73.2883, val_loss: 71.7600, val_MinusLogProbMetric: 71.7600

Epoch 370: val_loss did not improve from 52.49111
196/196 - 68s - loss: 73.2883 - MinusLogProbMetric: 73.2883 - val_loss: 71.7600 - val_MinusLogProbMetric: 71.7600 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 371/1000
2023-10-25 23:31:13.950 
Epoch 371/1000 
	 loss: 70.0296, MinusLogProbMetric: 70.0296, val_loss: 69.1545, val_MinusLogProbMetric: 69.1545

Epoch 371: val_loss did not improve from 52.49111
196/196 - 68s - loss: 70.0296 - MinusLogProbMetric: 70.0296 - val_loss: 69.1545 - val_MinusLogProbMetric: 69.1545 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 372/1000
2023-10-25 23:32:21.980 
Epoch 372/1000 
	 loss: 69.7791, MinusLogProbMetric: 69.7791, val_loss: 67.4041, val_MinusLogProbMetric: 67.4041

Epoch 372: val_loss did not improve from 52.49111
196/196 - 68s - loss: 69.7791 - MinusLogProbMetric: 69.7791 - val_loss: 67.4041 - val_MinusLogProbMetric: 67.4041 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 373/1000
2023-10-25 23:33:30.515 
Epoch 373/1000 
	 loss: 115.4977, MinusLogProbMetric: 115.4977, val_loss: 246.0547, val_MinusLogProbMetric: 246.0547

Epoch 373: val_loss did not improve from 52.49111
196/196 - 69s - loss: 115.4977 - MinusLogProbMetric: 115.4977 - val_loss: 246.0547 - val_MinusLogProbMetric: 246.0547 - lr: 1.2346e-05 - 69s/epoch - 350ms/step
Epoch 374/1000
2023-10-25 23:34:38.705 
Epoch 374/1000 
	 loss: 151.6765, MinusLogProbMetric: 151.6765, val_loss: 105.8617, val_MinusLogProbMetric: 105.8617

Epoch 374: val_loss did not improve from 52.49111
196/196 - 68s - loss: 151.6765 - MinusLogProbMetric: 151.6765 - val_loss: 105.8617 - val_MinusLogProbMetric: 105.8617 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 375/1000
2023-10-25 23:35:46.522 
Epoch 375/1000 
	 loss: 97.2206, MinusLogProbMetric: 97.2206, val_loss: 91.6998, val_MinusLogProbMetric: 91.6998

Epoch 375: val_loss did not improve from 52.49111
196/196 - 68s - loss: 97.2206 - MinusLogProbMetric: 97.2206 - val_loss: 91.6998 - val_MinusLogProbMetric: 91.6998 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 376/1000
2023-10-25 23:36:54.199 
Epoch 376/1000 
	 loss: 88.1719, MinusLogProbMetric: 88.1719, val_loss: 85.5370, val_MinusLogProbMetric: 85.5370

Epoch 376: val_loss did not improve from 52.49111
196/196 - 68s - loss: 88.1719 - MinusLogProbMetric: 88.1719 - val_loss: 85.5370 - val_MinusLogProbMetric: 85.5370 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 377/1000
2023-10-25 23:38:01.900 
Epoch 377/1000 
	 loss: 82.8196, MinusLogProbMetric: 82.8196, val_loss: 81.1441, val_MinusLogProbMetric: 81.1441

Epoch 377: val_loss did not improve from 52.49111
196/196 - 68s - loss: 82.8196 - MinusLogProbMetric: 82.8196 - val_loss: 81.1441 - val_MinusLogProbMetric: 81.1441 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 378/1000
2023-10-25 23:39:10.013 
Epoch 378/1000 
	 loss: 79.2306, MinusLogProbMetric: 79.2306, val_loss: 77.9587, val_MinusLogProbMetric: 77.9587

Epoch 378: val_loss did not improve from 52.49111
196/196 - 68s - loss: 79.2306 - MinusLogProbMetric: 79.2306 - val_loss: 77.9587 - val_MinusLogProbMetric: 77.9587 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 379/1000
2023-10-25 23:40:17.784 
Epoch 379/1000 
	 loss: 77.7735, MinusLogProbMetric: 77.7735, val_loss: 76.7885, val_MinusLogProbMetric: 76.7885

Epoch 379: val_loss did not improve from 52.49111
196/196 - 68s - loss: 77.7735 - MinusLogProbMetric: 77.7735 - val_loss: 76.7885 - val_MinusLogProbMetric: 76.7885 - lr: 1.2346e-05 - 68s/epoch - 346ms/step
Epoch 380/1000
2023-10-25 23:41:26.186 
Epoch 380/1000 
	 loss: 75.1737, MinusLogProbMetric: 75.1737, val_loss: 74.0999, val_MinusLogProbMetric: 74.0999

Epoch 380: val_loss did not improve from 52.49111
196/196 - 68s - loss: 75.1737 - MinusLogProbMetric: 75.1737 - val_loss: 74.0999 - val_MinusLogProbMetric: 74.0999 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 381/1000
2023-10-25 23:42:34.226 
Epoch 381/1000 
	 loss: 73.2654, MinusLogProbMetric: 73.2654, val_loss: 72.6663, val_MinusLogProbMetric: 72.6663

Epoch 381: val_loss did not improve from 52.49111
196/196 - 68s - loss: 73.2654 - MinusLogProbMetric: 73.2654 - val_loss: 72.6663 - val_MinusLogProbMetric: 72.6663 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 382/1000
2023-10-25 23:43:42.519 
Epoch 382/1000 
	 loss: 71.9881, MinusLogProbMetric: 71.9881, val_loss: 71.6456, val_MinusLogProbMetric: 71.6456

Epoch 382: val_loss did not improve from 52.49111
196/196 - 68s - loss: 71.9881 - MinusLogProbMetric: 71.9881 - val_loss: 71.6456 - val_MinusLogProbMetric: 71.6456 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 383/1000
2023-10-25 23:44:51.362 
Epoch 383/1000 
	 loss: 70.9392, MinusLogProbMetric: 70.9392, val_loss: 70.4377, val_MinusLogProbMetric: 70.4377

Epoch 383: val_loss did not improve from 52.49111
196/196 - 69s - loss: 70.9392 - MinusLogProbMetric: 70.9392 - val_loss: 70.4377 - val_MinusLogProbMetric: 70.4377 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 384/1000
2023-10-25 23:45:59.668 
Epoch 384/1000 
	 loss: 69.9176, MinusLogProbMetric: 69.9176, val_loss: 69.6725, val_MinusLogProbMetric: 69.6725

Epoch 384: val_loss did not improve from 52.49111
196/196 - 68s - loss: 69.9176 - MinusLogProbMetric: 69.9176 - val_loss: 69.6725 - val_MinusLogProbMetric: 69.6725 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 385/1000
2023-10-25 23:47:07.987 
Epoch 385/1000 
	 loss: 69.2179, MinusLogProbMetric: 69.2179, val_loss: 69.2471, val_MinusLogProbMetric: 69.2471

Epoch 385: val_loss did not improve from 52.49111
196/196 - 68s - loss: 69.2179 - MinusLogProbMetric: 69.2179 - val_loss: 69.2471 - val_MinusLogProbMetric: 69.2471 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 386/1000
2023-10-25 23:48:16.312 
Epoch 386/1000 
	 loss: 68.3286, MinusLogProbMetric: 68.3286, val_loss: 68.1075, val_MinusLogProbMetric: 68.1075

Epoch 386: val_loss did not improve from 52.49111
196/196 - 68s - loss: 68.3286 - MinusLogProbMetric: 68.3286 - val_loss: 68.1075 - val_MinusLogProbMetric: 68.1075 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 387/1000
2023-10-25 23:49:23.874 
Epoch 387/1000 
	 loss: 68.8077, MinusLogProbMetric: 68.8077, val_loss: 76.9593, val_MinusLogProbMetric: 76.9593

Epoch 387: val_loss did not improve from 52.49111
196/196 - 68s - loss: 68.8077 - MinusLogProbMetric: 68.8077 - val_loss: 76.9593 - val_MinusLogProbMetric: 76.9593 - lr: 1.2346e-05 - 68s/epoch - 345ms/step
Epoch 388/1000
2023-10-25 23:50:31.927 
Epoch 388/1000 
	 loss: 68.2402, MinusLogProbMetric: 68.2402, val_loss: 67.1159, val_MinusLogProbMetric: 67.1159

Epoch 388: val_loss did not improve from 52.49111
196/196 - 68s - loss: 68.2402 - MinusLogProbMetric: 68.2402 - val_loss: 67.1159 - val_MinusLogProbMetric: 67.1159 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 389/1000
2023-10-25 23:51:39.972 
Epoch 389/1000 
	 loss: 66.5339, MinusLogProbMetric: 66.5339, val_loss: 66.3566, val_MinusLogProbMetric: 66.3566

Epoch 389: val_loss did not improve from 52.49111
196/196 - 68s - loss: 66.5339 - MinusLogProbMetric: 66.5339 - val_loss: 66.3566 - val_MinusLogProbMetric: 66.3566 - lr: 1.2346e-05 - 68s/epoch - 347ms/step
Epoch 390/1000
2023-10-25 23:52:48.742 
Epoch 390/1000 
	 loss: 66.0722, MinusLogProbMetric: 66.0722, val_loss: 69.6347, val_MinusLogProbMetric: 69.6347

Epoch 390: val_loss did not improve from 52.49111
196/196 - 69s - loss: 66.0722 - MinusLogProbMetric: 66.0722 - val_loss: 69.6347 - val_MinusLogProbMetric: 69.6347 - lr: 1.2346e-05 - 69s/epoch - 351ms/step
Epoch 391/1000
2023-10-25 23:53:57.122 
Epoch 391/1000 
	 loss: 65.7304, MinusLogProbMetric: 65.7304, val_loss: 65.3772, val_MinusLogProbMetric: 65.3772

Epoch 391: val_loss did not improve from 52.49111
196/196 - 68s - loss: 65.7304 - MinusLogProbMetric: 65.7304 - val_loss: 65.3772 - val_MinusLogProbMetric: 65.3772 - lr: 1.2346e-05 - 68s/epoch - 349ms/step
Epoch 392/1000
2023-10-25 23:55:05.302 
Epoch 392/1000 
	 loss: 65.1016, MinusLogProbMetric: 65.1016, val_loss: 65.0139, val_MinusLogProbMetric: 65.0139

Epoch 392: val_loss did not improve from 52.49111
196/196 - 68s - loss: 65.1016 - MinusLogProbMetric: 65.1016 - val_loss: 65.0139 - val_MinusLogProbMetric: 65.0139 - lr: 1.2346e-05 - 68s/epoch - 348ms/step
Epoch 393/1000
2023-10-25 23:56:13.003 
Epoch 393/1000 
	 loss: 64.8530, MinusLogProbMetric: 64.8530, val_loss: 64.9509, val_MinusLogProbMetric: 64.9509

Epoch 393: val_loss did not improve from 52.49111
196/196 - 68s - loss: 64.8530 - MinusLogProbMetric: 64.8530 - val_loss: 64.9509 - val_MinusLogProbMetric: 64.9509 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 394/1000
2023-10-25 23:57:21.300 
Epoch 394/1000 
	 loss: 64.5137, MinusLogProbMetric: 64.5137, val_loss: 64.5726, val_MinusLogProbMetric: 64.5726

Epoch 394: val_loss did not improve from 52.49111
196/196 - 68s - loss: 64.5137 - MinusLogProbMetric: 64.5137 - val_loss: 64.5726 - val_MinusLogProbMetric: 64.5726 - lr: 6.1728e-06 - 68s/epoch - 348ms/step
Epoch 395/1000
2023-10-25 23:58:29.539 
Epoch 395/1000 
	 loss: 64.3250, MinusLogProbMetric: 64.3250, val_loss: 64.3324, val_MinusLogProbMetric: 64.3324

Epoch 395: val_loss did not improve from 52.49111
196/196 - 68s - loss: 64.3250 - MinusLogProbMetric: 64.3250 - val_loss: 64.3324 - val_MinusLogProbMetric: 64.3324 - lr: 6.1728e-06 - 68s/epoch - 348ms/step
Epoch 396/1000
2023-10-25 23:59:37.968 
Epoch 396/1000 
	 loss: 64.8471, MinusLogProbMetric: 64.8471, val_loss: 71.7933, val_MinusLogProbMetric: 71.7933

Epoch 396: val_loss did not improve from 52.49111
196/196 - 68s - loss: 64.8471 - MinusLogProbMetric: 64.8471 - val_loss: 71.7933 - val_MinusLogProbMetric: 71.7933 - lr: 6.1728e-06 - 68s/epoch - 349ms/step
Epoch 397/1000
2023-10-26 00:00:46.168 
Epoch 397/1000 
	 loss: 65.4947, MinusLogProbMetric: 65.4947, val_loss: 64.0432, val_MinusLogProbMetric: 64.0432

Epoch 397: val_loss did not improve from 52.49111
196/196 - 68s - loss: 65.4947 - MinusLogProbMetric: 65.4947 - val_loss: 64.0432 - val_MinusLogProbMetric: 64.0432 - lr: 6.1728e-06 - 68s/epoch - 348ms/step
Epoch 398/1000
2023-10-26 00:01:48.131 
Epoch 398/1000 
	 loss: 63.8908, MinusLogProbMetric: 63.8908, val_loss: 63.9022, val_MinusLogProbMetric: 63.9022

Epoch 398: val_loss did not improve from 52.49111
196/196 - 62s - loss: 63.8908 - MinusLogProbMetric: 63.8908 - val_loss: 63.9022 - val_MinusLogProbMetric: 63.9022 - lr: 6.1728e-06 - 62s/epoch - 316ms/step
Epoch 399/1000
2023-10-26 00:02:56.890 
Epoch 399/1000 
	 loss: 63.7906, MinusLogProbMetric: 63.7906, val_loss: 63.6120, val_MinusLogProbMetric: 63.6120

Epoch 399: val_loss did not improve from 52.49111
196/196 - 69s - loss: 63.7906 - MinusLogProbMetric: 63.7906 - val_loss: 63.6120 - val_MinusLogProbMetric: 63.6120 - lr: 6.1728e-06 - 69s/epoch - 351ms/step
Epoch 400/1000
2023-10-26 00:04:04.974 
Epoch 400/1000 
	 loss: 64.5342, MinusLogProbMetric: 64.5342, val_loss: 64.1212, val_MinusLogProbMetric: 64.1212

Epoch 400: val_loss did not improve from 52.49111
196/196 - 68s - loss: 64.5342 - MinusLogProbMetric: 64.5342 - val_loss: 64.1212 - val_MinusLogProbMetric: 64.1212 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 401/1000
2023-10-26 00:05:07.240 
Epoch 401/1000 
	 loss: 63.3995, MinusLogProbMetric: 63.3995, val_loss: 63.2691, val_MinusLogProbMetric: 63.2691

Epoch 401: val_loss did not improve from 52.49111
196/196 - 62s - loss: 63.3995 - MinusLogProbMetric: 63.3995 - val_loss: 63.2691 - val_MinusLogProbMetric: 63.2691 - lr: 6.1728e-06 - 62s/epoch - 318ms/step
Epoch 402/1000
2023-10-26 00:06:14.657 
Epoch 402/1000 
	 loss: 62.9638, MinusLogProbMetric: 62.9638, val_loss: 62.9874, val_MinusLogProbMetric: 62.9874

Epoch 402: val_loss did not improve from 52.49111
196/196 - 67s - loss: 62.9638 - MinusLogProbMetric: 62.9638 - val_loss: 62.9874 - val_MinusLogProbMetric: 62.9874 - lr: 6.1728e-06 - 67s/epoch - 344ms/step
Epoch 403/1000
2023-10-26 00:07:22.819 
Epoch 403/1000 
	 loss: 62.7049, MinusLogProbMetric: 62.7049, val_loss: 62.8275, val_MinusLogProbMetric: 62.8275

Epoch 403: val_loss did not improve from 52.49111
196/196 - 68s - loss: 62.7049 - MinusLogProbMetric: 62.7049 - val_loss: 62.8275 - val_MinusLogProbMetric: 62.8275 - lr: 6.1728e-06 - 68s/epoch - 348ms/step
Epoch 404/1000
2023-10-26 00:08:30.727 
Epoch 404/1000 
	 loss: 62.5496, MinusLogProbMetric: 62.5496, val_loss: 62.7455, val_MinusLogProbMetric: 62.7455

Epoch 404: val_loss did not improve from 52.49111
196/196 - 68s - loss: 62.5496 - MinusLogProbMetric: 62.5496 - val_loss: 62.7455 - val_MinusLogProbMetric: 62.7455 - lr: 6.1728e-06 - 68s/epoch - 346ms/step
Epoch 405/1000
2023-10-26 00:09:38.879 
Epoch 405/1000 
	 loss: 62.3601, MinusLogProbMetric: 62.3601, val_loss: 62.4061, val_MinusLogProbMetric: 62.4061

Epoch 405: val_loss did not improve from 52.49111
196/196 - 68s - loss: 62.3601 - MinusLogProbMetric: 62.3601 - val_loss: 62.4061 - val_MinusLogProbMetric: 62.4061 - lr: 6.1728e-06 - 68s/epoch - 348ms/step
Epoch 406/1000
2023-10-26 00:10:47.700 
Epoch 406/1000 
	 loss: 62.2295, MinusLogProbMetric: 62.2295, val_loss: 62.3018, val_MinusLogProbMetric: 62.3018

Epoch 406: val_loss did not improve from 52.49111
196/196 - 69s - loss: 62.2295 - MinusLogProbMetric: 62.2295 - val_loss: 62.3018 - val_MinusLogProbMetric: 62.3018 - lr: 6.1728e-06 - 69s/epoch - 351ms/step
Epoch 407/1000
2023-10-26 00:11:55.419 
Epoch 407/1000 
	 loss: 62.0501, MinusLogProbMetric: 62.0501, val_loss: 62.0987, val_MinusLogProbMetric: 62.0987

Epoch 407: val_loss did not improve from 52.49111
196/196 - 68s - loss: 62.0501 - MinusLogProbMetric: 62.0501 - val_loss: 62.0987 - val_MinusLogProbMetric: 62.0987 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 408/1000
2023-10-26 00:13:03.181 
Epoch 408/1000 
	 loss: 61.9126, MinusLogProbMetric: 61.9126, val_loss: 61.9871, val_MinusLogProbMetric: 61.9871

Epoch 408: val_loss did not improve from 52.49111
196/196 - 68s - loss: 61.9126 - MinusLogProbMetric: 61.9126 - val_loss: 61.9871 - val_MinusLogProbMetric: 61.9871 - lr: 6.1728e-06 - 68s/epoch - 346ms/step
Epoch 409/1000
2023-10-26 00:14:11.730 
Epoch 409/1000 
	 loss: 61.9323, MinusLogProbMetric: 61.9323, val_loss: 63.5554, val_MinusLogProbMetric: 63.5554

Epoch 409: val_loss did not improve from 52.49111
196/196 - 69s - loss: 61.9323 - MinusLogProbMetric: 61.9323 - val_loss: 63.5554 - val_MinusLogProbMetric: 63.5554 - lr: 6.1728e-06 - 69s/epoch - 350ms/step
Epoch 410/1000
2023-10-26 00:15:20.189 
Epoch 410/1000 
	 loss: 61.7822, MinusLogProbMetric: 61.7822, val_loss: 61.7323, val_MinusLogProbMetric: 61.7323

Epoch 410: val_loss did not improve from 52.49111
196/196 - 68s - loss: 61.7822 - MinusLogProbMetric: 61.7822 - val_loss: 61.7323 - val_MinusLogProbMetric: 61.7323 - lr: 6.1728e-06 - 68s/epoch - 349ms/step
Epoch 411/1000
2023-10-26 00:16:27.778 
Epoch 411/1000 
	 loss: 62.2734, MinusLogProbMetric: 62.2734, val_loss: 61.6343, val_MinusLogProbMetric: 61.6343

Epoch 411: val_loss did not improve from 52.49111
196/196 - 68s - loss: 62.2734 - MinusLogProbMetric: 62.2734 - val_loss: 61.6343 - val_MinusLogProbMetric: 61.6343 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 412/1000
2023-10-26 00:17:35.677 
Epoch 412/1000 
	 loss: 61.3566, MinusLogProbMetric: 61.3566, val_loss: 61.4619, val_MinusLogProbMetric: 61.4619

Epoch 412: val_loss did not improve from 52.49111
196/196 - 68s - loss: 61.3566 - MinusLogProbMetric: 61.3566 - val_loss: 61.4619 - val_MinusLogProbMetric: 61.4619 - lr: 6.1728e-06 - 68s/epoch - 346ms/step
Epoch 413/1000
2023-10-26 00:18:43.981 
Epoch 413/1000 
	 loss: 61.2571, MinusLogProbMetric: 61.2571, val_loss: 61.3588, val_MinusLogProbMetric: 61.3588

Epoch 413: val_loss did not improve from 52.49111
196/196 - 68s - loss: 61.2571 - MinusLogProbMetric: 61.2571 - val_loss: 61.3588 - val_MinusLogProbMetric: 61.3588 - lr: 6.1728e-06 - 68s/epoch - 348ms/step
Epoch 414/1000
2023-10-26 00:19:51.440 
Epoch 414/1000 
	 loss: 61.1437, MinusLogProbMetric: 61.1437, val_loss: 61.3264, val_MinusLogProbMetric: 61.3264

Epoch 414: val_loss did not improve from 52.49111
196/196 - 67s - loss: 61.1437 - MinusLogProbMetric: 61.1437 - val_loss: 61.3264 - val_MinusLogProbMetric: 61.3264 - lr: 6.1728e-06 - 67s/epoch - 344ms/step
Epoch 415/1000
2023-10-26 00:21:00.129 
Epoch 415/1000 
	 loss: 61.0947, MinusLogProbMetric: 61.0947, val_loss: 61.1958, val_MinusLogProbMetric: 61.1958

Epoch 415: val_loss did not improve from 52.49111
196/196 - 69s - loss: 61.0947 - MinusLogProbMetric: 61.0947 - val_loss: 61.1958 - val_MinusLogProbMetric: 61.1958 - lr: 6.1728e-06 - 69s/epoch - 350ms/step
Epoch 416/1000
2023-10-26 00:22:08.338 
Epoch 416/1000 
	 loss: 61.0174, MinusLogProbMetric: 61.0174, val_loss: 61.1809, val_MinusLogProbMetric: 61.1809

Epoch 416: val_loss did not improve from 52.49111
196/196 - 68s - loss: 61.0174 - MinusLogProbMetric: 61.0174 - val_loss: 61.1809 - val_MinusLogProbMetric: 61.1809 - lr: 6.1728e-06 - 68s/epoch - 348ms/step
Epoch 417/1000
2023-10-26 00:23:16.889 
Epoch 417/1000 
	 loss: 60.9600, MinusLogProbMetric: 60.9600, val_loss: 61.0026, val_MinusLogProbMetric: 61.0026

Epoch 417: val_loss did not improve from 52.49111
196/196 - 69s - loss: 60.9600 - MinusLogProbMetric: 60.9600 - val_loss: 61.0026 - val_MinusLogProbMetric: 61.0026 - lr: 6.1728e-06 - 69s/epoch - 350ms/step
Epoch 418/1000
2023-10-26 00:24:24.710 
Epoch 418/1000 
	 loss: 60.7964, MinusLogProbMetric: 60.7964, val_loss: 60.8861, val_MinusLogProbMetric: 60.8861

Epoch 418: val_loss did not improve from 52.49111
196/196 - 68s - loss: 60.7964 - MinusLogProbMetric: 60.7964 - val_loss: 60.8861 - val_MinusLogProbMetric: 60.8861 - lr: 6.1728e-06 - 68s/epoch - 346ms/step
Epoch 419/1000
2023-10-26 00:25:32.949 
Epoch 419/1000 
	 loss: 60.7066, MinusLogProbMetric: 60.7066, val_loss: 60.8079, val_MinusLogProbMetric: 60.8079

Epoch 419: val_loss did not improve from 52.49111
196/196 - 68s - loss: 60.7066 - MinusLogProbMetric: 60.7066 - val_loss: 60.8079 - val_MinusLogProbMetric: 60.8079 - lr: 6.1728e-06 - 68s/epoch - 348ms/step
Epoch 420/1000
2023-10-26 00:26:41.142 
Epoch 420/1000 
	 loss: 60.6020, MinusLogProbMetric: 60.6020, val_loss: 60.7144, val_MinusLogProbMetric: 60.7144

Epoch 420: val_loss did not improve from 52.49111
196/196 - 68s - loss: 60.6020 - MinusLogProbMetric: 60.6020 - val_loss: 60.7144 - val_MinusLogProbMetric: 60.7144 - lr: 6.1728e-06 - 68s/epoch - 348ms/step
Epoch 421/1000
2023-10-26 00:27:48.774 
Epoch 421/1000 
	 loss: 60.5242, MinusLogProbMetric: 60.5242, val_loss: 60.6203, val_MinusLogProbMetric: 60.6203

Epoch 421: val_loss did not improve from 52.49111
196/196 - 68s - loss: 60.5242 - MinusLogProbMetric: 60.5242 - val_loss: 60.6203 - val_MinusLogProbMetric: 60.6203 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 422/1000
2023-10-26 00:28:56.795 
Epoch 422/1000 
	 loss: 60.5086, MinusLogProbMetric: 60.5086, val_loss: 60.5428, val_MinusLogProbMetric: 60.5428

Epoch 422: val_loss did not improve from 52.49111
196/196 - 68s - loss: 60.5086 - MinusLogProbMetric: 60.5086 - val_loss: 60.5428 - val_MinusLogProbMetric: 60.5428 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 423/1000
2023-10-26 00:30:04.721 
Epoch 423/1000 
	 loss: 60.3741, MinusLogProbMetric: 60.3741, val_loss: 60.5339, val_MinusLogProbMetric: 60.5339

Epoch 423: val_loss did not improve from 52.49111
196/196 - 68s - loss: 60.3741 - MinusLogProbMetric: 60.3741 - val_loss: 60.5339 - val_MinusLogProbMetric: 60.5339 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 424/1000
2023-10-26 00:31:13.182 
Epoch 424/1000 
	 loss: 60.2880, MinusLogProbMetric: 60.2880, val_loss: 60.3657, val_MinusLogProbMetric: 60.3657

Epoch 424: val_loss did not improve from 52.49111
196/196 - 68s - loss: 60.2880 - MinusLogProbMetric: 60.2880 - val_loss: 60.3657 - val_MinusLogProbMetric: 60.3657 - lr: 6.1728e-06 - 68s/epoch - 349ms/step
Epoch 425/1000
2023-10-26 00:32:21.729 
Epoch 425/1000 
	 loss: 60.2674, MinusLogProbMetric: 60.2674, val_loss: 60.2667, val_MinusLogProbMetric: 60.2667

Epoch 425: val_loss did not improve from 52.49111
196/196 - 69s - loss: 60.2674 - MinusLogProbMetric: 60.2674 - val_loss: 60.2667 - val_MinusLogProbMetric: 60.2667 - lr: 6.1728e-06 - 69s/epoch - 350ms/step
Epoch 426/1000
2023-10-26 00:33:30.305 
Epoch 426/1000 
	 loss: 60.0883, MinusLogProbMetric: 60.0883, val_loss: 60.2075, val_MinusLogProbMetric: 60.2075

Epoch 426: val_loss did not improve from 52.49111
196/196 - 69s - loss: 60.0883 - MinusLogProbMetric: 60.0883 - val_loss: 60.2075 - val_MinusLogProbMetric: 60.2075 - lr: 6.1728e-06 - 69s/epoch - 350ms/step
Epoch 427/1000
2023-10-26 00:34:37.791 
Epoch 427/1000 
	 loss: 60.0344, MinusLogProbMetric: 60.0344, val_loss: 60.2099, val_MinusLogProbMetric: 60.2099

Epoch 427: val_loss did not improve from 52.49111
196/196 - 67s - loss: 60.0344 - MinusLogProbMetric: 60.0344 - val_loss: 60.2099 - val_MinusLogProbMetric: 60.2099 - lr: 6.1728e-06 - 67s/epoch - 344ms/step
Epoch 428/1000
2023-10-26 00:35:45.481 
Epoch 428/1000 
	 loss: 59.9956, MinusLogProbMetric: 59.9956, val_loss: 60.0475, val_MinusLogProbMetric: 60.0475

Epoch 428: val_loss did not improve from 52.49111
196/196 - 68s - loss: 59.9956 - MinusLogProbMetric: 59.9956 - val_loss: 60.0475 - val_MinusLogProbMetric: 60.0475 - lr: 6.1728e-06 - 68s/epoch - 345ms/step
Epoch 429/1000
2023-10-26 00:36:53.519 
Epoch 429/1000 
	 loss: 65.4591, MinusLogProbMetric: 65.4591, val_loss: 61.9498, val_MinusLogProbMetric: 61.9498

Epoch 429: val_loss did not improve from 52.49111
196/196 - 68s - loss: 65.4591 - MinusLogProbMetric: 65.4591 - val_loss: 61.9498 - val_MinusLogProbMetric: 61.9498 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 430/1000
2023-10-26 00:38:01.542 
Epoch 430/1000 
	 loss: 60.8398, MinusLogProbMetric: 60.8398, val_loss: 60.4544, val_MinusLogProbMetric: 60.4544

Epoch 430: val_loss did not improve from 52.49111
196/196 - 68s - loss: 60.8398 - MinusLogProbMetric: 60.8398 - val_loss: 60.4544 - val_MinusLogProbMetric: 60.4544 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 431/1000
2023-10-26 00:39:10.164 
Epoch 431/1000 
	 loss: 60.1653, MinusLogProbMetric: 60.1653, val_loss: 60.1076, val_MinusLogProbMetric: 60.1076

Epoch 431: val_loss did not improve from 52.49111
196/196 - 69s - loss: 60.1653 - MinusLogProbMetric: 60.1653 - val_loss: 60.1076 - val_MinusLogProbMetric: 60.1076 - lr: 6.1728e-06 - 69s/epoch - 350ms/step
Epoch 432/1000
2023-10-26 00:40:18.116 
Epoch 432/1000 
	 loss: 59.9299, MinusLogProbMetric: 59.9299, val_loss: 59.9590, val_MinusLogProbMetric: 59.9590

Epoch 432: val_loss did not improve from 52.49111
196/196 - 68s - loss: 59.9299 - MinusLogProbMetric: 59.9299 - val_loss: 59.9590 - val_MinusLogProbMetric: 59.9590 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 433/1000
2023-10-26 00:41:26.965 
Epoch 433/1000 
	 loss: 59.7406, MinusLogProbMetric: 59.7406, val_loss: 59.8421, val_MinusLogProbMetric: 59.8421

Epoch 433: val_loss did not improve from 52.49111
196/196 - 69s - loss: 59.7406 - MinusLogProbMetric: 59.7406 - val_loss: 59.8421 - val_MinusLogProbMetric: 59.8421 - lr: 6.1728e-06 - 69s/epoch - 351ms/step
Epoch 434/1000
2023-10-26 00:42:35.467 
Epoch 434/1000 
	 loss: 59.6197, MinusLogProbMetric: 59.6197, val_loss: 59.7023, val_MinusLogProbMetric: 59.7023

Epoch 434: val_loss did not improve from 52.49111
196/196 - 68s - loss: 59.6197 - MinusLogProbMetric: 59.6197 - val_loss: 59.7023 - val_MinusLogProbMetric: 59.7023 - lr: 6.1728e-06 - 68s/epoch - 349ms/step
Epoch 435/1000
2023-10-26 00:43:42.572 
Epoch 435/1000 
	 loss: 75.6600, MinusLogProbMetric: 75.6600, val_loss: 81.3139, val_MinusLogProbMetric: 81.3139

Epoch 435: val_loss did not improve from 52.49111
196/196 - 67s - loss: 75.6600 - MinusLogProbMetric: 75.6600 - val_loss: 81.3139 - val_MinusLogProbMetric: 81.3139 - lr: 6.1728e-06 - 67s/epoch - 342ms/step
Epoch 436/1000
2023-10-26 00:44:50.512 
Epoch 436/1000 
	 loss: 72.1744, MinusLogProbMetric: 72.1744, val_loss: 64.1641, val_MinusLogProbMetric: 64.1641

Epoch 436: val_loss did not improve from 52.49111
196/196 - 68s - loss: 72.1744 - MinusLogProbMetric: 72.1744 - val_loss: 64.1641 - val_MinusLogProbMetric: 64.1641 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 437/1000
2023-10-26 00:45:58.368 
Epoch 437/1000 
	 loss: 62.4690, MinusLogProbMetric: 62.4690, val_loss: 61.8796, val_MinusLogProbMetric: 61.8796

Epoch 437: val_loss did not improve from 52.49111
196/196 - 68s - loss: 62.4690 - MinusLogProbMetric: 62.4690 - val_loss: 61.8796 - val_MinusLogProbMetric: 61.8796 - lr: 6.1728e-06 - 68s/epoch - 346ms/step
Epoch 438/1000
2023-10-26 00:47:05.667 
Epoch 438/1000 
	 loss: 61.3395, MinusLogProbMetric: 61.3395, val_loss: 61.1939, val_MinusLogProbMetric: 61.1939

Epoch 438: val_loss did not improve from 52.49111
196/196 - 67s - loss: 61.3395 - MinusLogProbMetric: 61.3395 - val_loss: 61.1939 - val_MinusLogProbMetric: 61.1939 - lr: 6.1728e-06 - 67s/epoch - 343ms/step
Epoch 439/1000
2023-10-26 00:48:14.130 
Epoch 439/1000 
	 loss: 60.7747, MinusLogProbMetric: 60.7747, val_loss: 60.7802, val_MinusLogProbMetric: 60.7802

Epoch 439: val_loss did not improve from 52.49111
196/196 - 68s - loss: 60.7747 - MinusLogProbMetric: 60.7747 - val_loss: 60.7802 - val_MinusLogProbMetric: 60.7802 - lr: 6.1728e-06 - 68s/epoch - 349ms/step
Epoch 440/1000
2023-10-26 00:49:22.243 
Epoch 440/1000 
	 loss: 60.9400, MinusLogProbMetric: 60.9400, val_loss: 63.6980, val_MinusLogProbMetric: 63.6980

Epoch 440: val_loss did not improve from 52.49111
196/196 - 68s - loss: 60.9400 - MinusLogProbMetric: 60.9400 - val_loss: 63.6980 - val_MinusLogProbMetric: 63.6980 - lr: 6.1728e-06 - 68s/epoch - 347ms/step
Epoch 441/1000
2023-10-26 00:50:30.571 
Epoch 441/1000 
	 loss: 60.4508, MinusLogProbMetric: 60.4508, val_loss: 60.2669, val_MinusLogProbMetric: 60.2669

Epoch 441: val_loss did not improve from 52.49111
196/196 - 68s - loss: 60.4508 - MinusLogProbMetric: 60.4508 - val_loss: 60.2669 - val_MinusLogProbMetric: 60.2669 - lr: 6.1728e-06 - 68s/epoch - 349ms/step
Epoch 442/1000
2023-10-26 00:51:38.376 
Epoch 442/1000 
	 loss: 59.9451, MinusLogProbMetric: 59.9451, val_loss: 60.0930, val_MinusLogProbMetric: 60.0930

Epoch 442: val_loss did not improve from 52.49111
Restoring model weights from the end of the best epoch: 342.
196/196 - 68s - loss: 59.9451 - MinusLogProbMetric: 59.9451 - val_loss: 60.0930 - val_MinusLogProbMetric: 60.0930 - lr: 6.1728e-06 - 68s/epoch - 349ms/step
Epoch 442: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 520.
Model trained in 29783.46 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 1.69 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 2.16 s.
===========
Run 358/720 done in 31530.28 s.
===========

Directory ../../results/CsplineN_new/run_359/ already exists.
Skipping it.
===========
Run 359/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_360/ already exists.
Skipping it.
===========
Run 360/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_361/ already exists.
Skipping it.
===========
Run 361/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_362/ already exists.
Skipping it.
===========
Run 362/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_363/ already exists.
Skipping it.
===========
Run 363/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_364/ already exists.
Skipping it.
===========
Run 364/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_365/ already exists.
Skipping it.
===========
Run 365/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_366/ already exists.
Skipping it.
===========
Run 366/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_367/ already exists.
Skipping it.
===========
Run 367/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_368/ already exists.
Skipping it.
===========
Run 368/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_369/ already exists.
Skipping it.
===========
Run 369/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_370/ already exists.
Skipping it.
===========
Run 370/720 already exists. Skipping it.
===========

===========
Generating train data for run 371.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_371/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 721}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_371/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_371/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_371
self.data_kwargs: {'seed': 721}
self.x_data: [[ 5.475364    7.6366067   6.593081   ...  0.8530688   8.875271
   1.2600539 ]
 [ 6.624233    2.860935    6.2971916  ...  2.665269    4.2119365
   3.0900779 ]
 [ 6.4123483   2.9544969   6.2372165  ...  3.7006004   4.897939
   1.9067069 ]
 ...
 [ 5.291359    8.643322    5.9392395  ...  0.5393052   7.1813807
   1.3439621 ]
 [ 5.5789056   6.2932816   6.272862   ... -0.29475307  6.0157323
   1.3976022 ]
 [ 1.6532166   2.3152595  10.214214   ...  6.271919   -0.05406091
   3.9072165 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_60"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_61 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  908640    
 yer)                                                            
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f2ffc2c7460>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2ffc357c70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2ffc357c70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2e4015fa30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2dbc7f13c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2dbc7f1930>, <keras.callbacks.ModelCheckpoint object at 0x7f2dbc7f19f0>, <keras.callbacks.EarlyStopping object at 0x7f2dbc7f1c60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2dbc7f1c90>, <keras.callbacks.TerminateOnNaN object at 0x7f2dbc7f18d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 0.958336 ,  3.6590893,  8.676455 , ...,  5.9946833,  2.2094154,
         2.624469 ],
       [ 1.2393765,  3.8911932,  8.905665 , ...,  5.783149 ,  1.3067368,
         2.3810356],
       [ 6.815516 ,  2.883391 ,  6.09811  , ...,  2.679101 ,  5.190114 ,
         1.7287396],
       ...,
       [ 5.1098504,  8.570955 ,  5.486371 , ...,  2.6942174,  5.2584124,
         1.4402856],
       [ 1.5225409,  2.876459 ,  6.062782 , ...,  5.341303 , -0.5483551,
         3.446848 ],
       [ 5.6395645,  6.8262954,  6.6988196, ...,  1.2809412,  5.949959 ,
         1.4368113]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_371/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 371/720 with hyperparameters:
timestamp = 2023-10-26 00:51:47.676410
ndims = 64
seed_train = 721
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.475364    7.6366067   6.593081    5.2213635   4.3851104   6.547967
  4.392531    8.637874    9.379391    3.0568433   8.712279    4.9198484
  5.701744    9.309082    0.31514573  1.1504446  -0.4705431   8.813661
  9.537466    8.269614    8.780608    7.8593726   4.6449614   7.4048796
  1.683444    6.839795    1.5740938   9.839       4.622554    3.4715672
  2.0572462   7.526533    4.5307302   5.4113736   0.1867744   5.891015
  4.975408    5.49995     9.17347     6.8054004   3.9538507   4.336993
  6.759812    0.17014661  6.225843    6.527422    2.054394    1.277253
  3.0545924   3.5868065   4.708149    4.201944    9.617644    1.0881705
  1.922195    1.5136921   6.466659    1.6895405   4.5659165   2.2596717
  1.6085154   0.8530688   8.875271    1.2600539 ]
Epoch 1/1000
2023-10-26 00:53:45.105 
Epoch 1/1000 
	 loss: 829.1823, MinusLogProbMetric: 829.1823, val_loss: 217.8396, val_MinusLogProbMetric: 217.8396

Epoch 1: val_loss improved from inf to 217.83958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 118s - loss: 829.1823 - MinusLogProbMetric: 829.1823 - val_loss: 217.8396 - val_MinusLogProbMetric: 217.8396 - lr: 0.0010 - 118s/epoch - 602ms/step
Epoch 2/1000
2023-10-26 00:54:29.093 
Epoch 2/1000 
	 loss: 160.3174, MinusLogProbMetric: 160.3174, val_loss: 122.9808, val_MinusLogProbMetric: 122.9808

Epoch 2: val_loss improved from 217.83958 to 122.98076, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 160.3174 - MinusLogProbMetric: 160.3174 - val_loss: 122.9808 - val_MinusLogProbMetric: 122.9808 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 3/1000
2023-10-26 00:55:13.036 
Epoch 3/1000 
	 loss: 106.9171, MinusLogProbMetric: 106.9171, val_loss: 93.4421, val_MinusLogProbMetric: 93.4421

Epoch 3: val_loss improved from 122.98076 to 93.44207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 106.9171 - MinusLogProbMetric: 106.9171 - val_loss: 93.4421 - val_MinusLogProbMetric: 93.4421 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 4/1000
2023-10-26 00:55:57.147 
Epoch 4/1000 
	 loss: 84.2219, MinusLogProbMetric: 84.2219, val_loss: 74.5078, val_MinusLogProbMetric: 74.5078

Epoch 4: val_loss improved from 93.44207 to 74.50783, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 84.2219 - MinusLogProbMetric: 84.2219 - val_loss: 74.5078 - val_MinusLogProbMetric: 74.5078 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 5/1000
2023-10-26 00:56:41.111 
Epoch 5/1000 
	 loss: 69.3322, MinusLogProbMetric: 69.3322, val_loss: 66.7243, val_MinusLogProbMetric: 66.7243

Epoch 5: val_loss improved from 74.50783 to 66.72433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 69.3322 - MinusLogProbMetric: 69.3322 - val_loss: 66.7243 - val_MinusLogProbMetric: 66.7243 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 6/1000
2023-10-26 00:57:25.246 
Epoch 6/1000 
	 loss: 60.5329, MinusLogProbMetric: 60.5329, val_loss: 57.1255, val_MinusLogProbMetric: 57.1255

Epoch 6: val_loss improved from 66.72433 to 57.12553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 60.5329 - MinusLogProbMetric: 60.5329 - val_loss: 57.1255 - val_MinusLogProbMetric: 57.1255 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 7/1000
2023-10-26 00:58:08.817 
Epoch 7/1000 
	 loss: 54.8702, MinusLogProbMetric: 54.8702, val_loss: 54.5479, val_MinusLogProbMetric: 54.5479

Epoch 7: val_loss improved from 57.12553 to 54.54792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 54.8702 - MinusLogProbMetric: 54.8702 - val_loss: 54.5479 - val_MinusLogProbMetric: 54.5479 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 8/1000
2023-10-26 00:58:52.611 
Epoch 8/1000 
	 loss: 51.1113, MinusLogProbMetric: 51.1113, val_loss: 49.8272, val_MinusLogProbMetric: 49.8272

Epoch 8: val_loss improved from 54.54792 to 49.82724, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 51.1113 - MinusLogProbMetric: 51.1113 - val_loss: 49.8272 - val_MinusLogProbMetric: 49.8272 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 9/1000
2023-10-26 00:59:36.081 
Epoch 9/1000 
	 loss: 48.8566, MinusLogProbMetric: 48.8566, val_loss: 47.6739, val_MinusLogProbMetric: 47.6739

Epoch 9: val_loss improved from 49.82724 to 47.67388, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 48.8566 - MinusLogProbMetric: 48.8566 - val_loss: 47.6739 - val_MinusLogProbMetric: 47.6739 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 10/1000
2023-10-26 01:00:20.022 
Epoch 10/1000 
	 loss: 47.1888, MinusLogProbMetric: 47.1888, val_loss: 46.4551, val_MinusLogProbMetric: 46.4551

Epoch 10: val_loss improved from 47.67388 to 46.45510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 47.1888 - MinusLogProbMetric: 47.1888 - val_loss: 46.4551 - val_MinusLogProbMetric: 46.4551 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 11/1000
2023-10-26 01:01:03.794 
Epoch 11/1000 
	 loss: 44.8837, MinusLogProbMetric: 44.8837, val_loss: 44.0168, val_MinusLogProbMetric: 44.0168

Epoch 11: val_loss improved from 46.45510 to 44.01679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 44.8837 - MinusLogProbMetric: 44.8837 - val_loss: 44.0168 - val_MinusLogProbMetric: 44.0168 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 12/1000
2023-10-26 01:01:47.817 
Epoch 12/1000 
	 loss: 43.4840, MinusLogProbMetric: 43.4840, val_loss: 43.9407, val_MinusLogProbMetric: 43.9407

Epoch 12: val_loss improved from 44.01679 to 43.94069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 43.4840 - MinusLogProbMetric: 43.4840 - val_loss: 43.9407 - val_MinusLogProbMetric: 43.9407 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 13/1000
2023-10-26 01:02:31.738 
Epoch 13/1000 
	 loss: 42.5835, MinusLogProbMetric: 42.5835, val_loss: 43.0676, val_MinusLogProbMetric: 43.0676

Epoch 13: val_loss improved from 43.94069 to 43.06762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 42.5835 - MinusLogProbMetric: 42.5835 - val_loss: 43.0676 - val_MinusLogProbMetric: 43.0676 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 14/1000
2023-10-26 01:03:15.881 
Epoch 14/1000 
	 loss: 41.7519, MinusLogProbMetric: 41.7519, val_loss: 41.3839, val_MinusLogProbMetric: 41.3839

Epoch 14: val_loss improved from 43.06762 to 41.38390, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 41.7519 - MinusLogProbMetric: 41.7519 - val_loss: 41.3839 - val_MinusLogProbMetric: 41.3839 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 15/1000
2023-10-26 01:03:59.722 
Epoch 15/1000 
	 loss: 40.7633, MinusLogProbMetric: 40.7633, val_loss: 41.8071, val_MinusLogProbMetric: 41.8071

Epoch 15: val_loss did not improve from 41.38390
196/196 - 43s - loss: 40.7633 - MinusLogProbMetric: 40.7633 - val_loss: 41.8071 - val_MinusLogProbMetric: 41.8071 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 16/1000
2023-10-26 01:04:42.596 
Epoch 16/1000 
	 loss: 41.0725, MinusLogProbMetric: 41.0725, val_loss: 40.8429, val_MinusLogProbMetric: 40.8429

Epoch 16: val_loss improved from 41.38390 to 40.84293, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 41.0725 - MinusLogProbMetric: 41.0725 - val_loss: 40.8429 - val_MinusLogProbMetric: 40.8429 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 17/1000
2023-10-26 01:05:26.316 
Epoch 17/1000 
	 loss: 39.3913, MinusLogProbMetric: 39.3913, val_loss: 39.3763, val_MinusLogProbMetric: 39.3763

Epoch 17: val_loss improved from 40.84293 to 39.37628, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 39.3913 - MinusLogProbMetric: 39.3913 - val_loss: 39.3763 - val_MinusLogProbMetric: 39.3763 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 18/1000
2023-10-26 01:06:09.998 
Epoch 18/1000 
	 loss: 39.5789, MinusLogProbMetric: 39.5789, val_loss: 38.8133, val_MinusLogProbMetric: 38.8133

Epoch 18: val_loss improved from 39.37628 to 38.81326, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 39.5789 - MinusLogProbMetric: 39.5789 - val_loss: 38.8133 - val_MinusLogProbMetric: 38.8133 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 19/1000
2023-10-26 01:06:53.756 
Epoch 19/1000 
	 loss: 38.8738, MinusLogProbMetric: 38.8738, val_loss: 39.3303, val_MinusLogProbMetric: 39.3303

Epoch 19: val_loss did not improve from 38.81326
196/196 - 43s - loss: 38.8738 - MinusLogProbMetric: 38.8738 - val_loss: 39.3303 - val_MinusLogProbMetric: 39.3303 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 20/1000
2023-10-26 01:07:36.450 
Epoch 20/1000 
	 loss: 39.3639, MinusLogProbMetric: 39.3639, val_loss: 43.0882, val_MinusLogProbMetric: 43.0882

Epoch 20: val_loss did not improve from 38.81326
196/196 - 43s - loss: 39.3639 - MinusLogProbMetric: 39.3639 - val_loss: 43.0882 - val_MinusLogProbMetric: 43.0882 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 21/1000
2023-10-26 01:08:19.931 
Epoch 21/1000 
	 loss: 37.9196, MinusLogProbMetric: 37.9196, val_loss: 38.3727, val_MinusLogProbMetric: 38.3727

Epoch 21: val_loss improved from 38.81326 to 38.37268, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 37.9196 - MinusLogProbMetric: 37.9196 - val_loss: 38.3727 - val_MinusLogProbMetric: 38.3727 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 22/1000
2023-10-26 01:09:03.299 
Epoch 22/1000 
	 loss: 37.4221, MinusLogProbMetric: 37.4221, val_loss: 39.1508, val_MinusLogProbMetric: 39.1508

Epoch 22: val_loss did not improve from 38.37268
196/196 - 43s - loss: 37.4221 - MinusLogProbMetric: 37.4221 - val_loss: 39.1508 - val_MinusLogProbMetric: 39.1508 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 23/1000
2023-10-26 01:09:46.289 
Epoch 23/1000 
	 loss: 37.6806, MinusLogProbMetric: 37.6806, val_loss: 38.3898, val_MinusLogProbMetric: 38.3898

Epoch 23: val_loss did not improve from 38.37268
196/196 - 43s - loss: 37.6806 - MinusLogProbMetric: 37.6806 - val_loss: 38.3898 - val_MinusLogProbMetric: 38.3898 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 24/1000
2023-10-26 01:10:29.220 
Epoch 24/1000 
	 loss: 37.1979, MinusLogProbMetric: 37.1979, val_loss: 36.6496, val_MinusLogProbMetric: 36.6496

Epoch 24: val_loss improved from 38.37268 to 36.64965, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 37.1979 - MinusLogProbMetric: 37.1979 - val_loss: 36.6496 - val_MinusLogProbMetric: 36.6496 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 25/1000
2023-10-26 01:11:12.655 
Epoch 25/1000 
	 loss: 36.6684, MinusLogProbMetric: 36.6684, val_loss: 38.0680, val_MinusLogProbMetric: 38.0680

Epoch 25: val_loss did not improve from 36.64965
196/196 - 43s - loss: 36.6684 - MinusLogProbMetric: 36.6684 - val_loss: 38.0680 - val_MinusLogProbMetric: 38.0680 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 26/1000
2023-10-26 01:11:55.830 
Epoch 26/1000 
	 loss: 36.8804, MinusLogProbMetric: 36.8804, val_loss: 42.5164, val_MinusLogProbMetric: 42.5164

Epoch 26: val_loss did not improve from 36.64965
196/196 - 43s - loss: 36.8804 - MinusLogProbMetric: 36.8804 - val_loss: 42.5164 - val_MinusLogProbMetric: 42.5164 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 27/1000
2023-10-26 01:12:38.873 
Epoch 27/1000 
	 loss: 36.3704, MinusLogProbMetric: 36.3704, val_loss: 36.5634, val_MinusLogProbMetric: 36.5634

Epoch 27: val_loss improved from 36.64965 to 36.56340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 36.3704 - MinusLogProbMetric: 36.3704 - val_loss: 36.5634 - val_MinusLogProbMetric: 36.5634 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 28/1000
2023-10-26 01:13:22.449 
Epoch 28/1000 
	 loss: 36.0417, MinusLogProbMetric: 36.0417, val_loss: 35.9531, val_MinusLogProbMetric: 35.9531

Epoch 28: val_loss improved from 36.56340 to 35.95308, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 36.0417 - MinusLogProbMetric: 36.0417 - val_loss: 35.9531 - val_MinusLogProbMetric: 35.9531 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 29/1000
2023-10-26 01:14:06.202 
Epoch 29/1000 
	 loss: 35.8757, MinusLogProbMetric: 35.8757, val_loss: 35.3010, val_MinusLogProbMetric: 35.3010

Epoch 29: val_loss improved from 35.95308 to 35.30104, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 35.8757 - MinusLogProbMetric: 35.8757 - val_loss: 35.3010 - val_MinusLogProbMetric: 35.3010 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 30/1000
2023-10-26 01:14:50.067 
Epoch 30/1000 
	 loss: 35.8102, MinusLogProbMetric: 35.8102, val_loss: 35.1778, val_MinusLogProbMetric: 35.1778

Epoch 30: val_loss improved from 35.30104 to 35.17777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 35.8102 - MinusLogProbMetric: 35.8102 - val_loss: 35.1778 - val_MinusLogProbMetric: 35.1778 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 31/1000
2023-10-26 01:15:34.018 
Epoch 31/1000 
	 loss: 35.7184, MinusLogProbMetric: 35.7184, val_loss: 42.5250, val_MinusLogProbMetric: 42.5250

Epoch 31: val_loss did not improve from 35.17777
196/196 - 43s - loss: 35.7184 - MinusLogProbMetric: 35.7184 - val_loss: 42.5250 - val_MinusLogProbMetric: 42.5250 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 32/1000
2023-10-26 01:16:16.760 
Epoch 32/1000 
	 loss: 36.1161, MinusLogProbMetric: 36.1161, val_loss: 37.6432, val_MinusLogProbMetric: 37.6432

Epoch 32: val_loss did not improve from 35.17777
196/196 - 43s - loss: 36.1161 - MinusLogProbMetric: 36.1161 - val_loss: 37.6432 - val_MinusLogProbMetric: 37.6432 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 33/1000
2023-10-26 01:16:59.615 
Epoch 33/1000 
	 loss: 35.6284, MinusLogProbMetric: 35.6284, val_loss: 34.9908, val_MinusLogProbMetric: 34.9908

Epoch 33: val_loss improved from 35.17777 to 34.99084, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 35.6284 - MinusLogProbMetric: 35.6284 - val_loss: 34.9908 - val_MinusLogProbMetric: 34.9908 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 34/1000
2023-10-26 01:17:43.041 
Epoch 34/1000 
	 loss: 34.7582, MinusLogProbMetric: 34.7582, val_loss: 34.5301, val_MinusLogProbMetric: 34.5301

Epoch 34: val_loss improved from 34.99084 to 34.53012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 34.7582 - MinusLogProbMetric: 34.7582 - val_loss: 34.5301 - val_MinusLogProbMetric: 34.5301 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 35/1000
2023-10-26 01:18:26.498 
Epoch 35/1000 
	 loss: 34.9234, MinusLogProbMetric: 34.9234, val_loss: 36.2474, val_MinusLogProbMetric: 36.2474

Epoch 35: val_loss did not improve from 34.53012
196/196 - 43s - loss: 34.9234 - MinusLogProbMetric: 34.9234 - val_loss: 36.2474 - val_MinusLogProbMetric: 36.2474 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 36/1000
2023-10-26 01:19:09.355 
Epoch 36/1000 
	 loss: 34.5313, MinusLogProbMetric: 34.5313, val_loss: 35.4479, val_MinusLogProbMetric: 35.4479

Epoch 36: val_loss did not improve from 34.53012
196/196 - 43s - loss: 34.5313 - MinusLogProbMetric: 34.5313 - val_loss: 35.4479 - val_MinusLogProbMetric: 35.4479 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 37/1000
2023-10-26 01:19:52.299 
Epoch 37/1000 
	 loss: 34.7932, MinusLogProbMetric: 34.7932, val_loss: 35.3677, val_MinusLogProbMetric: 35.3677

Epoch 37: val_loss did not improve from 34.53012
196/196 - 43s - loss: 34.7932 - MinusLogProbMetric: 34.7932 - val_loss: 35.3677 - val_MinusLogProbMetric: 35.3677 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 38/1000
2023-10-26 01:20:35.691 
Epoch 38/1000 
	 loss: 34.3368, MinusLogProbMetric: 34.3368, val_loss: 33.7625, val_MinusLogProbMetric: 33.7625

Epoch 38: val_loss improved from 34.53012 to 33.76254, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 34.3368 - MinusLogProbMetric: 34.3368 - val_loss: 33.7625 - val_MinusLogProbMetric: 33.7625 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 39/1000
2023-10-26 01:21:19.505 
Epoch 39/1000 
	 loss: 34.4608, MinusLogProbMetric: 34.4608, val_loss: 34.6308, val_MinusLogProbMetric: 34.6308

Epoch 39: val_loss did not improve from 33.76254
196/196 - 43s - loss: 34.4608 - MinusLogProbMetric: 34.4608 - val_loss: 34.6308 - val_MinusLogProbMetric: 34.6308 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 40/1000
2023-10-26 01:22:02.135 
Epoch 40/1000 
	 loss: 33.9777, MinusLogProbMetric: 33.9777, val_loss: 36.6644, val_MinusLogProbMetric: 36.6644

Epoch 40: val_loss did not improve from 33.76254
196/196 - 43s - loss: 33.9777 - MinusLogProbMetric: 33.9777 - val_loss: 36.6644 - val_MinusLogProbMetric: 36.6644 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 41/1000
2023-10-26 01:22:45.019 
Epoch 41/1000 
	 loss: 34.1541, MinusLogProbMetric: 34.1541, val_loss: 34.6891, val_MinusLogProbMetric: 34.6891

Epoch 41: val_loss did not improve from 33.76254
196/196 - 43s - loss: 34.1541 - MinusLogProbMetric: 34.1541 - val_loss: 34.6891 - val_MinusLogProbMetric: 34.6891 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 42/1000
2023-10-26 01:23:27.657 
Epoch 42/1000 
	 loss: 33.8520, MinusLogProbMetric: 33.8520, val_loss: 34.9662, val_MinusLogProbMetric: 34.9662

Epoch 42: val_loss did not improve from 33.76254
196/196 - 43s - loss: 33.8520 - MinusLogProbMetric: 33.8520 - val_loss: 34.9662 - val_MinusLogProbMetric: 34.9662 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 43/1000
2023-10-26 01:24:10.540 
Epoch 43/1000 
	 loss: 34.2250, MinusLogProbMetric: 34.2250, val_loss: 35.1888, val_MinusLogProbMetric: 35.1888

Epoch 43: val_loss did not improve from 33.76254
196/196 - 43s - loss: 34.2250 - MinusLogProbMetric: 34.2250 - val_loss: 35.1888 - val_MinusLogProbMetric: 35.1888 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 44/1000
2023-10-26 01:24:53.602 
Epoch 44/1000 
	 loss: 33.7218, MinusLogProbMetric: 33.7218, val_loss: 34.6288, val_MinusLogProbMetric: 34.6288

Epoch 44: val_loss did not improve from 33.76254
196/196 - 43s - loss: 33.7218 - MinusLogProbMetric: 33.7218 - val_loss: 34.6288 - val_MinusLogProbMetric: 34.6288 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 45/1000
2023-10-26 01:25:36.409 
Epoch 45/1000 
	 loss: 33.7736, MinusLogProbMetric: 33.7736, val_loss: 33.8048, val_MinusLogProbMetric: 33.8048

Epoch 45: val_loss did not improve from 33.76254
196/196 - 43s - loss: 33.7736 - MinusLogProbMetric: 33.7736 - val_loss: 33.8048 - val_MinusLogProbMetric: 33.8048 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 46/1000
2023-10-26 01:26:19.031 
Epoch 46/1000 
	 loss: 33.5208, MinusLogProbMetric: 33.5208, val_loss: 34.3005, val_MinusLogProbMetric: 34.3005

Epoch 46: val_loss did not improve from 33.76254
196/196 - 43s - loss: 33.5208 - MinusLogProbMetric: 33.5208 - val_loss: 34.3005 - val_MinusLogProbMetric: 34.3005 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 47/1000
2023-10-26 01:27:02.249 
Epoch 47/1000 
	 loss: 33.6518, MinusLogProbMetric: 33.6518, val_loss: 35.3712, val_MinusLogProbMetric: 35.3712

Epoch 47: val_loss did not improve from 33.76254
196/196 - 43s - loss: 33.6518 - MinusLogProbMetric: 33.6518 - val_loss: 35.3712 - val_MinusLogProbMetric: 35.3712 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 48/1000
2023-10-26 01:27:45.057 
Epoch 48/1000 
	 loss: 33.4440, MinusLogProbMetric: 33.4440, val_loss: 33.2824, val_MinusLogProbMetric: 33.2824

Epoch 48: val_loss improved from 33.76254 to 33.28240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 33.4440 - MinusLogProbMetric: 33.4440 - val_loss: 33.2824 - val_MinusLogProbMetric: 33.2824 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 49/1000
2023-10-26 01:28:28.174 
Epoch 49/1000 
	 loss: 33.4312, MinusLogProbMetric: 33.4312, val_loss: 33.7947, val_MinusLogProbMetric: 33.7947

Epoch 49: val_loss did not improve from 33.28240
196/196 - 42s - loss: 33.4312 - MinusLogProbMetric: 33.4312 - val_loss: 33.7947 - val_MinusLogProbMetric: 33.7947 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 50/1000
2023-10-26 01:29:10.709 
Epoch 50/1000 
	 loss: 33.8876, MinusLogProbMetric: 33.8876, val_loss: 33.4052, val_MinusLogProbMetric: 33.4052

Epoch 50: val_loss did not improve from 33.28240
196/196 - 43s - loss: 33.8876 - MinusLogProbMetric: 33.8876 - val_loss: 33.4052 - val_MinusLogProbMetric: 33.4052 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 51/1000
2023-10-26 01:29:53.620 
Epoch 51/1000 
	 loss: 33.2736, MinusLogProbMetric: 33.2736, val_loss: 33.0564, val_MinusLogProbMetric: 33.0564

Epoch 51: val_loss improved from 33.28240 to 33.05635, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 33.2736 - MinusLogProbMetric: 33.2736 - val_loss: 33.0564 - val_MinusLogProbMetric: 33.0564 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 52/1000
2023-10-26 01:30:37.593 
Epoch 52/1000 
	 loss: 33.4695, MinusLogProbMetric: 33.4695, val_loss: 32.8442, val_MinusLogProbMetric: 32.8442

Epoch 52: val_loss improved from 33.05635 to 32.84415, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 33.4695 - MinusLogProbMetric: 33.4695 - val_loss: 32.8442 - val_MinusLogProbMetric: 32.8442 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 53/1000
2023-10-26 01:31:21.279 
Epoch 53/1000 
	 loss: 32.9115, MinusLogProbMetric: 32.9115, val_loss: 34.5740, val_MinusLogProbMetric: 34.5740

Epoch 53: val_loss did not improve from 32.84415
196/196 - 43s - loss: 32.9115 - MinusLogProbMetric: 32.9115 - val_loss: 34.5740 - val_MinusLogProbMetric: 34.5740 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 54/1000
2023-10-26 01:32:03.861 
Epoch 54/1000 
	 loss: 34.0174, MinusLogProbMetric: 34.0174, val_loss: 32.9985, val_MinusLogProbMetric: 32.9985

Epoch 54: val_loss did not improve from 32.84415
196/196 - 43s - loss: 34.0174 - MinusLogProbMetric: 34.0174 - val_loss: 32.9985 - val_MinusLogProbMetric: 32.9985 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 55/1000
2023-10-26 01:32:46.347 
Epoch 55/1000 
	 loss: 32.8107, MinusLogProbMetric: 32.8107, val_loss: 33.1725, val_MinusLogProbMetric: 33.1725

Epoch 55: val_loss did not improve from 32.84415
196/196 - 42s - loss: 32.8107 - MinusLogProbMetric: 32.8107 - val_loss: 33.1725 - val_MinusLogProbMetric: 33.1725 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 56/1000
2023-10-26 01:33:29.179 
Epoch 56/1000 
	 loss: 32.8714, MinusLogProbMetric: 32.8714, val_loss: 32.6903, val_MinusLogProbMetric: 32.6903

Epoch 56: val_loss improved from 32.84415 to 32.69035, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 32.8714 - MinusLogProbMetric: 32.8714 - val_loss: 32.6903 - val_MinusLogProbMetric: 32.6903 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 57/1000
2023-10-26 01:34:12.748 
Epoch 57/1000 
	 loss: 32.6505, MinusLogProbMetric: 32.6505, val_loss: 33.3187, val_MinusLogProbMetric: 33.3187

Epoch 57: val_loss did not improve from 32.69035
196/196 - 43s - loss: 32.6505 - MinusLogProbMetric: 32.6505 - val_loss: 33.3187 - val_MinusLogProbMetric: 33.3187 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 58/1000
2023-10-26 01:34:55.399 
Epoch 58/1000 
	 loss: 32.6664, MinusLogProbMetric: 32.6664, val_loss: 33.6648, val_MinusLogProbMetric: 33.6648

Epoch 58: val_loss did not improve from 32.69035
196/196 - 43s - loss: 32.6664 - MinusLogProbMetric: 32.6664 - val_loss: 33.6648 - val_MinusLogProbMetric: 33.6648 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 59/1000
2023-10-26 01:35:38.053 
Epoch 59/1000 
	 loss: 32.6516, MinusLogProbMetric: 32.6516, val_loss: 33.3589, val_MinusLogProbMetric: 33.3589

Epoch 59: val_loss did not improve from 32.69035
196/196 - 43s - loss: 32.6516 - MinusLogProbMetric: 32.6516 - val_loss: 33.3589 - val_MinusLogProbMetric: 33.3589 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 60/1000
2023-10-26 01:36:20.251 
Epoch 60/1000 
	 loss: 32.4357, MinusLogProbMetric: 32.4357, val_loss: 33.2026, val_MinusLogProbMetric: 33.2026

Epoch 60: val_loss did not improve from 32.69035
196/196 - 42s - loss: 32.4357 - MinusLogProbMetric: 32.4357 - val_loss: 33.2026 - val_MinusLogProbMetric: 33.2026 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 61/1000
2023-10-26 01:37:02.630 
Epoch 61/1000 
	 loss: 32.6023, MinusLogProbMetric: 32.6023, val_loss: 33.0690, val_MinusLogProbMetric: 33.0690

Epoch 61: val_loss did not improve from 32.69035
196/196 - 42s - loss: 32.6023 - MinusLogProbMetric: 32.6023 - val_loss: 33.0690 - val_MinusLogProbMetric: 33.0690 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 62/1000
2023-10-26 01:37:43.216 
Epoch 62/1000 
	 loss: 32.5267, MinusLogProbMetric: 32.5267, val_loss: 32.1766, val_MinusLogProbMetric: 32.1766

Epoch 62: val_loss improved from 32.69035 to 32.17661, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 41s - loss: 32.5267 - MinusLogProbMetric: 32.5267 - val_loss: 32.1766 - val_MinusLogProbMetric: 32.1766 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 63/1000
2023-10-26 01:38:20.804 
Epoch 63/1000 
	 loss: 32.5753, MinusLogProbMetric: 32.5753, val_loss: 32.6270, val_MinusLogProbMetric: 32.6270

Epoch 63: val_loss did not improve from 32.17661
196/196 - 37s - loss: 32.5753 - MinusLogProbMetric: 32.5753 - val_loss: 32.6270 - val_MinusLogProbMetric: 32.6270 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 64/1000
2023-10-26 01:39:02.580 
Epoch 64/1000 
	 loss: 32.5068, MinusLogProbMetric: 32.5068, val_loss: 33.4409, val_MinusLogProbMetric: 33.4409

Epoch 64: val_loss did not improve from 32.17661
196/196 - 42s - loss: 32.5068 - MinusLogProbMetric: 32.5068 - val_loss: 33.4409 - val_MinusLogProbMetric: 33.4409 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 65/1000
2023-10-26 01:39:40.270 
Epoch 65/1000 
	 loss: 32.3443, MinusLogProbMetric: 32.3443, val_loss: 33.5326, val_MinusLogProbMetric: 33.5326

Epoch 65: val_loss did not improve from 32.17661
196/196 - 38s - loss: 32.3443 - MinusLogProbMetric: 32.3443 - val_loss: 33.5326 - val_MinusLogProbMetric: 33.5326 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 66/1000
2023-10-26 01:40:21.649 
Epoch 66/1000 
	 loss: 32.1859, MinusLogProbMetric: 32.1859, val_loss: 32.2733, val_MinusLogProbMetric: 32.2733

Epoch 66: val_loss did not improve from 32.17661
196/196 - 41s - loss: 32.1859 - MinusLogProbMetric: 32.1859 - val_loss: 32.2733 - val_MinusLogProbMetric: 32.2733 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 67/1000
2023-10-26 01:41:03.989 
Epoch 67/1000 
	 loss: 32.1085, MinusLogProbMetric: 32.1085, val_loss: 32.0437, val_MinusLogProbMetric: 32.0437

Epoch 67: val_loss improved from 32.17661 to 32.04368, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 32.1085 - MinusLogProbMetric: 32.1085 - val_loss: 32.0437 - val_MinusLogProbMetric: 32.0437 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 68/1000
2023-10-26 01:41:47.178 
Epoch 68/1000 
	 loss: 32.3288, MinusLogProbMetric: 32.3288, val_loss: 32.8457, val_MinusLogProbMetric: 32.8457

Epoch 68: val_loss did not improve from 32.04368
196/196 - 42s - loss: 32.3288 - MinusLogProbMetric: 32.3288 - val_loss: 32.8457 - val_MinusLogProbMetric: 32.8457 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 69/1000
2023-10-26 01:42:29.826 
Epoch 69/1000 
	 loss: 32.1622, MinusLogProbMetric: 32.1622, val_loss: 33.8155, val_MinusLogProbMetric: 33.8155

Epoch 69: val_loss did not improve from 32.04368
196/196 - 43s - loss: 32.1622 - MinusLogProbMetric: 32.1622 - val_loss: 33.8155 - val_MinusLogProbMetric: 33.8155 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 70/1000
2023-10-26 01:43:12.519 
Epoch 70/1000 
	 loss: 32.0250, MinusLogProbMetric: 32.0250, val_loss: 32.7012, val_MinusLogProbMetric: 32.7012

Epoch 70: val_loss did not improve from 32.04368
196/196 - 43s - loss: 32.0250 - MinusLogProbMetric: 32.0250 - val_loss: 32.7012 - val_MinusLogProbMetric: 32.7012 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 71/1000
2023-10-26 01:43:55.367 
Epoch 71/1000 
	 loss: 32.0279, MinusLogProbMetric: 32.0279, val_loss: 32.6432, val_MinusLogProbMetric: 32.6432

Epoch 71: val_loss did not improve from 32.04368
196/196 - 43s - loss: 32.0279 - MinusLogProbMetric: 32.0279 - val_loss: 32.6432 - val_MinusLogProbMetric: 32.6432 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 72/1000
2023-10-26 01:44:37.846 
Epoch 72/1000 
	 loss: 31.9673, MinusLogProbMetric: 31.9673, val_loss: 32.1558, val_MinusLogProbMetric: 32.1558

Epoch 72: val_loss did not improve from 32.04368
196/196 - 42s - loss: 31.9673 - MinusLogProbMetric: 31.9673 - val_loss: 32.1558 - val_MinusLogProbMetric: 32.1558 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 73/1000
2023-10-26 01:45:20.828 
Epoch 73/1000 
	 loss: 31.9203, MinusLogProbMetric: 31.9203, val_loss: 32.1198, val_MinusLogProbMetric: 32.1198

Epoch 73: val_loss did not improve from 32.04368
196/196 - 43s - loss: 31.9203 - MinusLogProbMetric: 31.9203 - val_loss: 32.1198 - val_MinusLogProbMetric: 32.1198 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 74/1000
2023-10-26 01:46:03.499 
Epoch 74/1000 
	 loss: 31.9098, MinusLogProbMetric: 31.9098, val_loss: 32.7117, val_MinusLogProbMetric: 32.7117

Epoch 74: val_loss did not improve from 32.04368
196/196 - 43s - loss: 31.9098 - MinusLogProbMetric: 31.9098 - val_loss: 32.7117 - val_MinusLogProbMetric: 32.7117 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 75/1000
2023-10-26 01:46:46.122 
Epoch 75/1000 
	 loss: 32.0569, MinusLogProbMetric: 32.0569, val_loss: 32.5970, val_MinusLogProbMetric: 32.5970

Epoch 75: val_loss did not improve from 32.04368
196/196 - 43s - loss: 32.0569 - MinusLogProbMetric: 32.0569 - val_loss: 32.5970 - val_MinusLogProbMetric: 32.5970 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 76/1000
2023-10-26 01:47:28.809 
Epoch 76/1000 
	 loss: 31.7956, MinusLogProbMetric: 31.7956, val_loss: 31.3511, val_MinusLogProbMetric: 31.3511

Epoch 76: val_loss improved from 32.04368 to 31.35113, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 31.7956 - MinusLogProbMetric: 31.7956 - val_loss: 31.3511 - val_MinusLogProbMetric: 31.3511 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 77/1000
2023-10-26 01:48:12.633 
Epoch 77/1000 
	 loss: 32.0759, MinusLogProbMetric: 32.0759, val_loss: 32.0275, val_MinusLogProbMetric: 32.0275

Epoch 77: val_loss did not improve from 31.35113
196/196 - 43s - loss: 32.0759 - MinusLogProbMetric: 32.0759 - val_loss: 32.0275 - val_MinusLogProbMetric: 32.0275 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 78/1000
2023-10-26 01:48:55.465 
Epoch 78/1000 
	 loss: 31.8510, MinusLogProbMetric: 31.8510, val_loss: 31.3707, val_MinusLogProbMetric: 31.3707

Epoch 78: val_loss did not improve from 31.35113
196/196 - 43s - loss: 31.8510 - MinusLogProbMetric: 31.8510 - val_loss: 31.3707 - val_MinusLogProbMetric: 31.3707 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 79/1000
2023-10-26 01:49:38.016 
Epoch 79/1000 
	 loss: 31.7678, MinusLogProbMetric: 31.7678, val_loss: 31.8870, val_MinusLogProbMetric: 31.8870

Epoch 79: val_loss did not improve from 31.35113
196/196 - 43s - loss: 31.7678 - MinusLogProbMetric: 31.7678 - val_loss: 31.8870 - val_MinusLogProbMetric: 31.8870 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 80/1000
2023-10-26 01:50:21.056 
Epoch 80/1000 
	 loss: 31.7943, MinusLogProbMetric: 31.7943, val_loss: 32.0792, val_MinusLogProbMetric: 32.0792

Epoch 80: val_loss did not improve from 31.35113
196/196 - 43s - loss: 31.7943 - MinusLogProbMetric: 31.7943 - val_loss: 32.0792 - val_MinusLogProbMetric: 32.0792 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 81/1000
2023-10-26 01:51:03.853 
Epoch 81/1000 
	 loss: 31.6395, MinusLogProbMetric: 31.6395, val_loss: 32.7459, val_MinusLogProbMetric: 32.7459

Epoch 81: val_loss did not improve from 31.35113
196/196 - 43s - loss: 31.6395 - MinusLogProbMetric: 31.6395 - val_loss: 32.7459 - val_MinusLogProbMetric: 32.7459 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 82/1000
2023-10-26 01:51:46.493 
Epoch 82/1000 
	 loss: 31.7038, MinusLogProbMetric: 31.7038, val_loss: 31.4075, val_MinusLogProbMetric: 31.4075

Epoch 82: val_loss did not improve from 31.35113
196/196 - 43s - loss: 31.7038 - MinusLogProbMetric: 31.7038 - val_loss: 31.4075 - val_MinusLogProbMetric: 31.4075 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 83/1000
2023-10-26 01:52:28.956 
Epoch 83/1000 
	 loss: 31.5322, MinusLogProbMetric: 31.5322, val_loss: 32.3392, val_MinusLogProbMetric: 32.3392

Epoch 83: val_loss did not improve from 31.35113
196/196 - 42s - loss: 31.5322 - MinusLogProbMetric: 31.5322 - val_loss: 32.3392 - val_MinusLogProbMetric: 32.3392 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 84/1000
2023-10-26 01:53:11.517 
Epoch 84/1000 
	 loss: 31.6606, MinusLogProbMetric: 31.6606, val_loss: 32.9188, val_MinusLogProbMetric: 32.9188

Epoch 84: val_loss did not improve from 31.35113
196/196 - 43s - loss: 31.6606 - MinusLogProbMetric: 31.6606 - val_loss: 32.9188 - val_MinusLogProbMetric: 32.9188 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 85/1000
2023-10-26 01:53:54.172 
Epoch 85/1000 
	 loss: 31.8500, MinusLogProbMetric: 31.8500, val_loss: 32.2768, val_MinusLogProbMetric: 32.2768

Epoch 85: val_loss did not improve from 31.35113
196/196 - 43s - loss: 31.8500 - MinusLogProbMetric: 31.8500 - val_loss: 32.2768 - val_MinusLogProbMetric: 32.2768 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 86/1000
2023-10-26 01:54:36.607 
Epoch 86/1000 
	 loss: 31.5512, MinusLogProbMetric: 31.5512, val_loss: 31.7588, val_MinusLogProbMetric: 31.7588

Epoch 86: val_loss did not improve from 31.35113
196/196 - 42s - loss: 31.5512 - MinusLogProbMetric: 31.5512 - val_loss: 31.7588 - val_MinusLogProbMetric: 31.7588 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 87/1000
2023-10-26 01:55:19.292 
Epoch 87/1000 
	 loss: 31.3989, MinusLogProbMetric: 31.3989, val_loss: 31.7572, val_MinusLogProbMetric: 31.7572

Epoch 87: val_loss did not improve from 31.35113
196/196 - 43s - loss: 31.3989 - MinusLogProbMetric: 31.3989 - val_loss: 31.7572 - val_MinusLogProbMetric: 31.7572 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 88/1000
2023-10-26 01:56:01.720 
Epoch 88/1000 
	 loss: 31.4702, MinusLogProbMetric: 31.4702, val_loss: 31.7611, val_MinusLogProbMetric: 31.7611

Epoch 88: val_loss did not improve from 31.35113
196/196 - 42s - loss: 31.4702 - MinusLogProbMetric: 31.4702 - val_loss: 31.7611 - val_MinusLogProbMetric: 31.7611 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 89/1000
2023-10-26 01:56:44.053 
Epoch 89/1000 
	 loss: 31.6777, MinusLogProbMetric: 31.6777, val_loss: 31.5010, val_MinusLogProbMetric: 31.5010

Epoch 89: val_loss did not improve from 31.35113
196/196 - 42s - loss: 31.6777 - MinusLogProbMetric: 31.6777 - val_loss: 31.5010 - val_MinusLogProbMetric: 31.5010 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 90/1000
2023-10-26 01:57:26.781 
Epoch 90/1000 
	 loss: 31.2999, MinusLogProbMetric: 31.2999, val_loss: 31.4849, val_MinusLogProbMetric: 31.4849

Epoch 90: val_loss did not improve from 31.35113
196/196 - 43s - loss: 31.2999 - MinusLogProbMetric: 31.2999 - val_loss: 31.4849 - val_MinusLogProbMetric: 31.4849 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 91/1000
2023-10-26 01:58:09.338 
Epoch 91/1000 
	 loss: 31.2744, MinusLogProbMetric: 31.2744, val_loss: 31.7283, val_MinusLogProbMetric: 31.7283

Epoch 91: val_loss did not improve from 31.35113
196/196 - 43s - loss: 31.2744 - MinusLogProbMetric: 31.2744 - val_loss: 31.7283 - val_MinusLogProbMetric: 31.7283 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 92/1000
2023-10-26 01:58:52.312 
Epoch 92/1000 
	 loss: 31.3931, MinusLogProbMetric: 31.3931, val_loss: 31.7708, val_MinusLogProbMetric: 31.7708

Epoch 92: val_loss did not improve from 31.35113
196/196 - 43s - loss: 31.3931 - MinusLogProbMetric: 31.3931 - val_loss: 31.7708 - val_MinusLogProbMetric: 31.7708 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 93/1000
2023-10-26 01:59:35.014 
Epoch 93/1000 
	 loss: 31.2766, MinusLogProbMetric: 31.2766, val_loss: 32.4335, val_MinusLogProbMetric: 32.4335

Epoch 93: val_loss did not improve from 31.35113
196/196 - 43s - loss: 31.2766 - MinusLogProbMetric: 31.2766 - val_loss: 32.4335 - val_MinusLogProbMetric: 32.4335 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 94/1000
2023-10-26 02:00:17.771 
Epoch 94/1000 
	 loss: 31.3263, MinusLogProbMetric: 31.3263, val_loss: 31.3260, val_MinusLogProbMetric: 31.3260

Epoch 94: val_loss improved from 31.35113 to 31.32597, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 31.3263 - MinusLogProbMetric: 31.3263 - val_loss: 31.3260 - val_MinusLogProbMetric: 31.3260 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 95/1000
2023-10-26 02:01:01.088 
Epoch 95/1000 
	 loss: 31.2239, MinusLogProbMetric: 31.2239, val_loss: 31.7723, val_MinusLogProbMetric: 31.7723

Epoch 95: val_loss did not improve from 31.32597
196/196 - 43s - loss: 31.2239 - MinusLogProbMetric: 31.2239 - val_loss: 31.7723 - val_MinusLogProbMetric: 31.7723 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 96/1000
2023-10-26 02:01:43.633 
Epoch 96/1000 
	 loss: 31.3134, MinusLogProbMetric: 31.3134, val_loss: 31.2968, val_MinusLogProbMetric: 31.2968

Epoch 96: val_loss improved from 31.32597 to 31.29678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 31.3134 - MinusLogProbMetric: 31.3134 - val_loss: 31.2968 - val_MinusLogProbMetric: 31.2968 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 97/1000
2023-10-26 02:02:26.605 
Epoch 97/1000 
	 loss: 31.1102, MinusLogProbMetric: 31.1102, val_loss: 31.3869, val_MinusLogProbMetric: 31.3869

Epoch 97: val_loss did not improve from 31.29678
196/196 - 42s - loss: 31.1102 - MinusLogProbMetric: 31.1102 - val_loss: 31.3869 - val_MinusLogProbMetric: 31.3869 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 98/1000
2023-10-26 02:03:09.753 
Epoch 98/1000 
	 loss: 31.2593, MinusLogProbMetric: 31.2593, val_loss: 31.0737, val_MinusLogProbMetric: 31.0737

Epoch 98: val_loss improved from 31.29678 to 31.07374, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 31.2593 - MinusLogProbMetric: 31.2593 - val_loss: 31.0737 - val_MinusLogProbMetric: 31.0737 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 99/1000
2023-10-26 02:03:52.967 
Epoch 99/1000 
	 loss: 31.1038, MinusLogProbMetric: 31.1038, val_loss: 31.1360, val_MinusLogProbMetric: 31.1360

Epoch 99: val_loss did not improve from 31.07374
196/196 - 43s - loss: 31.1038 - MinusLogProbMetric: 31.1038 - val_loss: 31.1360 - val_MinusLogProbMetric: 31.1360 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 100/1000
2023-10-26 02:04:35.857 
Epoch 100/1000 
	 loss: 31.1546, MinusLogProbMetric: 31.1546, val_loss: 32.0387, val_MinusLogProbMetric: 32.0387

Epoch 100: val_loss did not improve from 31.07374
196/196 - 43s - loss: 31.1546 - MinusLogProbMetric: 31.1546 - val_loss: 32.0387 - val_MinusLogProbMetric: 32.0387 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 101/1000
2023-10-26 02:05:18.616 
Epoch 101/1000 
	 loss: 31.2463, MinusLogProbMetric: 31.2463, val_loss: 33.0476, val_MinusLogProbMetric: 33.0476

Epoch 101: val_loss did not improve from 31.07374
196/196 - 43s - loss: 31.2463 - MinusLogProbMetric: 31.2463 - val_loss: 33.0476 - val_MinusLogProbMetric: 33.0476 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 102/1000
2023-10-26 02:06:01.291 
Epoch 102/1000 
	 loss: 31.0726, MinusLogProbMetric: 31.0726, val_loss: 31.1497, val_MinusLogProbMetric: 31.1497

Epoch 102: val_loss did not improve from 31.07374
196/196 - 43s - loss: 31.0726 - MinusLogProbMetric: 31.0726 - val_loss: 31.1497 - val_MinusLogProbMetric: 31.1497 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 103/1000
2023-10-26 02:06:43.902 
Epoch 103/1000 
	 loss: 31.0438, MinusLogProbMetric: 31.0438, val_loss: 31.0579, val_MinusLogProbMetric: 31.0579

Epoch 103: val_loss improved from 31.07374 to 31.05792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 31.0438 - MinusLogProbMetric: 31.0438 - val_loss: 31.0579 - val_MinusLogProbMetric: 31.0579 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 104/1000
2023-10-26 02:07:27.613 
Epoch 104/1000 
	 loss: 31.1267, MinusLogProbMetric: 31.1267, val_loss: 30.6549, val_MinusLogProbMetric: 30.6549

Epoch 104: val_loss improved from 31.05792 to 30.65489, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 31.1267 - MinusLogProbMetric: 31.1267 - val_loss: 30.6549 - val_MinusLogProbMetric: 30.6549 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 105/1000
2023-10-26 02:08:11.029 
Epoch 105/1000 
	 loss: 31.0100, MinusLogProbMetric: 31.0100, val_loss: 31.1109, val_MinusLogProbMetric: 31.1109

Epoch 105: val_loss did not improve from 30.65489
196/196 - 43s - loss: 31.0100 - MinusLogProbMetric: 31.0100 - val_loss: 31.1109 - val_MinusLogProbMetric: 31.1109 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 106/1000
2023-10-26 02:08:54.138 
Epoch 106/1000 
	 loss: 31.0292, MinusLogProbMetric: 31.0292, val_loss: 30.9366, val_MinusLogProbMetric: 30.9366

Epoch 106: val_loss did not improve from 30.65489
196/196 - 43s - loss: 31.0292 - MinusLogProbMetric: 31.0292 - val_loss: 30.9366 - val_MinusLogProbMetric: 30.9366 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 107/1000
2023-10-26 02:09:36.885 
Epoch 107/1000 
	 loss: 30.8275, MinusLogProbMetric: 30.8275, val_loss: 31.2929, val_MinusLogProbMetric: 31.2929

Epoch 107: val_loss did not improve from 30.65489
196/196 - 43s - loss: 30.8275 - MinusLogProbMetric: 30.8275 - val_loss: 31.2929 - val_MinusLogProbMetric: 31.2929 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 108/1000
2023-10-26 02:10:19.394 
Epoch 108/1000 
	 loss: 30.9998, MinusLogProbMetric: 30.9998, val_loss: 31.1512, val_MinusLogProbMetric: 31.1512

Epoch 108: val_loss did not improve from 30.65489
196/196 - 43s - loss: 30.9998 - MinusLogProbMetric: 30.9998 - val_loss: 31.1512 - val_MinusLogProbMetric: 31.1512 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 109/1000
2023-10-26 02:11:01.679 
Epoch 109/1000 
	 loss: 31.0768, MinusLogProbMetric: 31.0768, val_loss: 31.5578, val_MinusLogProbMetric: 31.5578

Epoch 109: val_loss did not improve from 30.65489
196/196 - 42s - loss: 31.0768 - MinusLogProbMetric: 31.0768 - val_loss: 31.5578 - val_MinusLogProbMetric: 31.5578 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 110/1000
2023-10-26 02:11:44.629 
Epoch 110/1000 
	 loss: 30.8776, MinusLogProbMetric: 30.8776, val_loss: 31.9883, val_MinusLogProbMetric: 31.9883

Epoch 110: val_loss did not improve from 30.65489
196/196 - 43s - loss: 30.8776 - MinusLogProbMetric: 30.8776 - val_loss: 31.9883 - val_MinusLogProbMetric: 31.9883 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 111/1000
2023-10-26 02:12:26.979 
Epoch 111/1000 
	 loss: 30.9924, MinusLogProbMetric: 30.9924, val_loss: 31.4019, val_MinusLogProbMetric: 31.4019

Epoch 111: val_loss did not improve from 30.65489
196/196 - 42s - loss: 30.9924 - MinusLogProbMetric: 30.9924 - val_loss: 31.4019 - val_MinusLogProbMetric: 31.4019 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 112/1000
2023-10-26 02:13:09.948 
Epoch 112/1000 
	 loss: 30.9272, MinusLogProbMetric: 30.9272, val_loss: 31.5474, val_MinusLogProbMetric: 31.5474

Epoch 112: val_loss did not improve from 30.65489
196/196 - 43s - loss: 30.9272 - MinusLogProbMetric: 30.9272 - val_loss: 31.5474 - val_MinusLogProbMetric: 31.5474 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 113/1000
2023-10-26 02:13:52.945 
Epoch 113/1000 
	 loss: 30.9609, MinusLogProbMetric: 30.9609, val_loss: 31.0474, val_MinusLogProbMetric: 31.0474

Epoch 113: val_loss did not improve from 30.65489
196/196 - 43s - loss: 30.9609 - MinusLogProbMetric: 30.9609 - val_loss: 31.0474 - val_MinusLogProbMetric: 31.0474 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 114/1000
2023-10-26 02:14:35.393 
Epoch 114/1000 
	 loss: 30.7629, MinusLogProbMetric: 30.7629, val_loss: 31.1289, val_MinusLogProbMetric: 31.1289

Epoch 114: val_loss did not improve from 30.65489
196/196 - 42s - loss: 30.7629 - MinusLogProbMetric: 30.7629 - val_loss: 31.1289 - val_MinusLogProbMetric: 31.1289 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 115/1000
2023-10-26 02:15:18.524 
Epoch 115/1000 
	 loss: 30.8397, MinusLogProbMetric: 30.8397, val_loss: 31.1484, val_MinusLogProbMetric: 31.1484

Epoch 115: val_loss did not improve from 30.65489
196/196 - 43s - loss: 30.8397 - MinusLogProbMetric: 30.8397 - val_loss: 31.1484 - val_MinusLogProbMetric: 31.1484 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 116/1000
2023-10-26 02:16:01.515 
Epoch 116/1000 
	 loss: 30.8316, MinusLogProbMetric: 30.8316, val_loss: 31.0631, val_MinusLogProbMetric: 31.0631

Epoch 116: val_loss did not improve from 30.65489
196/196 - 43s - loss: 30.8316 - MinusLogProbMetric: 30.8316 - val_loss: 31.0631 - val_MinusLogProbMetric: 31.0631 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 117/1000
2023-10-26 02:16:44.697 
Epoch 117/1000 
	 loss: 30.7876, MinusLogProbMetric: 30.7876, val_loss: 32.5508, val_MinusLogProbMetric: 32.5508

Epoch 117: val_loss did not improve from 30.65489
196/196 - 43s - loss: 30.7876 - MinusLogProbMetric: 30.7876 - val_loss: 32.5508 - val_MinusLogProbMetric: 32.5508 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 118/1000
2023-10-26 02:17:27.339 
Epoch 118/1000 
	 loss: 30.7790, MinusLogProbMetric: 30.7790, val_loss: 31.7873, val_MinusLogProbMetric: 31.7873

Epoch 118: val_loss did not improve from 30.65489
196/196 - 43s - loss: 30.7790 - MinusLogProbMetric: 30.7790 - val_loss: 31.7873 - val_MinusLogProbMetric: 31.7873 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 119/1000
2023-10-26 02:18:09.636 
Epoch 119/1000 
	 loss: 30.9041, MinusLogProbMetric: 30.9041, val_loss: 30.4079, val_MinusLogProbMetric: 30.4079

Epoch 119: val_loss improved from 30.65489 to 30.40792, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 30.9041 - MinusLogProbMetric: 30.9041 - val_loss: 30.4079 - val_MinusLogProbMetric: 30.4079 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 120/1000
2023-10-26 02:18:53.437 
Epoch 120/1000 
	 loss: 30.7383, MinusLogProbMetric: 30.7383, val_loss: 30.6392, val_MinusLogProbMetric: 30.6392

Epoch 120: val_loss did not improve from 30.40792
196/196 - 43s - loss: 30.7383 - MinusLogProbMetric: 30.7383 - val_loss: 30.6392 - val_MinusLogProbMetric: 30.6392 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 121/1000
2023-10-26 02:19:36.277 
Epoch 121/1000 
	 loss: 30.7032, MinusLogProbMetric: 30.7032, val_loss: 30.5732, val_MinusLogProbMetric: 30.5732

Epoch 121: val_loss did not improve from 30.40792
196/196 - 43s - loss: 30.7032 - MinusLogProbMetric: 30.7032 - val_loss: 30.5732 - val_MinusLogProbMetric: 30.5732 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 122/1000
2023-10-26 02:20:19.203 
Epoch 122/1000 
	 loss: 30.6857, MinusLogProbMetric: 30.6857, val_loss: 32.0544, val_MinusLogProbMetric: 32.0544

Epoch 122: val_loss did not improve from 30.40792
196/196 - 43s - loss: 30.6857 - MinusLogProbMetric: 30.6857 - val_loss: 32.0544 - val_MinusLogProbMetric: 32.0544 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 123/1000
2023-10-26 02:21:02.037 
Epoch 123/1000 
	 loss: 30.8137, MinusLogProbMetric: 30.8137, val_loss: 31.0740, val_MinusLogProbMetric: 31.0740

Epoch 123: val_loss did not improve from 30.40792
196/196 - 43s - loss: 30.8137 - MinusLogProbMetric: 30.8137 - val_loss: 31.0740 - val_MinusLogProbMetric: 31.0740 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 124/1000
2023-10-26 02:21:44.869 
Epoch 124/1000 
	 loss: 30.7212, MinusLogProbMetric: 30.7212, val_loss: 30.8076, val_MinusLogProbMetric: 30.8076

Epoch 124: val_loss did not improve from 30.40792
196/196 - 43s - loss: 30.7212 - MinusLogProbMetric: 30.7212 - val_loss: 30.8076 - val_MinusLogProbMetric: 30.8076 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 125/1000
2023-10-26 02:22:27.473 
Epoch 125/1000 
	 loss: 30.7247, MinusLogProbMetric: 30.7247, val_loss: 31.5652, val_MinusLogProbMetric: 31.5652

Epoch 125: val_loss did not improve from 30.40792
196/196 - 43s - loss: 30.7247 - MinusLogProbMetric: 30.7247 - val_loss: 31.5652 - val_MinusLogProbMetric: 31.5652 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 126/1000
2023-10-26 02:23:10.194 
Epoch 126/1000 
	 loss: 30.6011, MinusLogProbMetric: 30.6011, val_loss: 32.5665, val_MinusLogProbMetric: 32.5665

Epoch 126: val_loss did not improve from 30.40792
196/196 - 43s - loss: 30.6011 - MinusLogProbMetric: 30.6011 - val_loss: 32.5665 - val_MinusLogProbMetric: 32.5665 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 127/1000
2023-10-26 02:23:53.009 
Epoch 127/1000 
	 loss: 30.7263, MinusLogProbMetric: 30.7263, val_loss: 30.8446, val_MinusLogProbMetric: 30.8446

Epoch 127: val_loss did not improve from 30.40792
196/196 - 43s - loss: 30.7263 - MinusLogProbMetric: 30.7263 - val_loss: 30.8446 - val_MinusLogProbMetric: 30.8446 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 128/1000
2023-10-26 02:24:35.584 
Epoch 128/1000 
	 loss: 30.5051, MinusLogProbMetric: 30.5051, val_loss: 30.2248, val_MinusLogProbMetric: 30.2248

Epoch 128: val_loss improved from 30.40792 to 30.22476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 30.5051 - MinusLogProbMetric: 30.5051 - val_loss: 30.2248 - val_MinusLogProbMetric: 30.2248 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 129/1000
2023-10-26 02:25:19.124 
Epoch 129/1000 
	 loss: 30.6125, MinusLogProbMetric: 30.6125, val_loss: 30.6824, val_MinusLogProbMetric: 30.6824

Epoch 129: val_loss did not improve from 30.22476
196/196 - 43s - loss: 30.6125 - MinusLogProbMetric: 30.6125 - val_loss: 30.6824 - val_MinusLogProbMetric: 30.6824 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 130/1000
2023-10-26 02:26:01.870 
Epoch 130/1000 
	 loss: 30.4298, MinusLogProbMetric: 30.4298, val_loss: 30.7325, val_MinusLogProbMetric: 30.7325

Epoch 130: val_loss did not improve from 30.22476
196/196 - 43s - loss: 30.4298 - MinusLogProbMetric: 30.4298 - val_loss: 30.7325 - val_MinusLogProbMetric: 30.7325 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 131/1000
2023-10-26 02:26:44.710 
Epoch 131/1000 
	 loss: 30.5238, MinusLogProbMetric: 30.5238, val_loss: 31.2493, val_MinusLogProbMetric: 31.2493

Epoch 131: val_loss did not improve from 30.22476
196/196 - 43s - loss: 30.5238 - MinusLogProbMetric: 30.5238 - val_loss: 31.2493 - val_MinusLogProbMetric: 31.2493 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 132/1000
2023-10-26 02:27:27.357 
Epoch 132/1000 
	 loss: 30.6017, MinusLogProbMetric: 30.6017, val_loss: 31.1480, val_MinusLogProbMetric: 31.1480

Epoch 132: val_loss did not improve from 30.22476
196/196 - 43s - loss: 30.6017 - MinusLogProbMetric: 30.6017 - val_loss: 31.1480 - val_MinusLogProbMetric: 31.1480 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 133/1000
2023-10-26 02:28:09.565 
Epoch 133/1000 
	 loss: 30.4227, MinusLogProbMetric: 30.4227, val_loss: 31.1599, val_MinusLogProbMetric: 31.1599

Epoch 133: val_loss did not improve from 30.22476
196/196 - 42s - loss: 30.4227 - MinusLogProbMetric: 30.4227 - val_loss: 31.1599 - val_MinusLogProbMetric: 31.1599 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 134/1000
2023-10-26 02:28:52.272 
Epoch 134/1000 
	 loss: 30.4783, MinusLogProbMetric: 30.4783, val_loss: 31.3175, val_MinusLogProbMetric: 31.3175

Epoch 134: val_loss did not improve from 30.22476
196/196 - 43s - loss: 30.4783 - MinusLogProbMetric: 30.4783 - val_loss: 31.3175 - val_MinusLogProbMetric: 31.3175 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 135/1000
2023-10-26 02:29:35.061 
Epoch 135/1000 
	 loss: 30.6806, MinusLogProbMetric: 30.6806, val_loss: 30.9464, val_MinusLogProbMetric: 30.9464

Epoch 135: val_loss did not improve from 30.22476
196/196 - 43s - loss: 30.6806 - MinusLogProbMetric: 30.6806 - val_loss: 30.9464 - val_MinusLogProbMetric: 30.9464 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 136/1000
2023-10-26 02:30:17.972 
Epoch 136/1000 
	 loss: 30.3294, MinusLogProbMetric: 30.3294, val_loss: 30.8703, val_MinusLogProbMetric: 30.8703

Epoch 136: val_loss did not improve from 30.22476
196/196 - 43s - loss: 30.3294 - MinusLogProbMetric: 30.3294 - val_loss: 30.8703 - val_MinusLogProbMetric: 30.8703 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 137/1000
2023-10-26 02:31:00.852 
Epoch 137/1000 
	 loss: 30.5298, MinusLogProbMetric: 30.5298, val_loss: 30.4584, val_MinusLogProbMetric: 30.4584

Epoch 137: val_loss did not improve from 30.22476
196/196 - 43s - loss: 30.5298 - MinusLogProbMetric: 30.5298 - val_loss: 30.4584 - val_MinusLogProbMetric: 30.4584 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 138/1000
2023-10-26 02:31:43.517 
Epoch 138/1000 
	 loss: 30.5738, MinusLogProbMetric: 30.5738, val_loss: 30.5277, val_MinusLogProbMetric: 30.5277

Epoch 138: val_loss did not improve from 30.22476
196/196 - 43s - loss: 30.5738 - MinusLogProbMetric: 30.5738 - val_loss: 30.5277 - val_MinusLogProbMetric: 30.5277 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 139/1000
2023-10-26 02:32:25.963 
Epoch 139/1000 
	 loss: 30.4012, MinusLogProbMetric: 30.4012, val_loss: 30.6575, val_MinusLogProbMetric: 30.6575

Epoch 139: val_loss did not improve from 30.22476
196/196 - 42s - loss: 30.4012 - MinusLogProbMetric: 30.4012 - val_loss: 30.6575 - val_MinusLogProbMetric: 30.6575 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 140/1000
2023-10-26 02:33:08.453 
Epoch 140/1000 
	 loss: 30.6224, MinusLogProbMetric: 30.6224, val_loss: 30.5430, val_MinusLogProbMetric: 30.5430

Epoch 140: val_loss did not improve from 30.22476
196/196 - 42s - loss: 30.6224 - MinusLogProbMetric: 30.6224 - val_loss: 30.5430 - val_MinusLogProbMetric: 30.5430 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 141/1000
2023-10-26 02:33:51.075 
Epoch 141/1000 
	 loss: 30.4390, MinusLogProbMetric: 30.4390, val_loss: 30.1655, val_MinusLogProbMetric: 30.1655

Epoch 141: val_loss improved from 30.22476 to 30.16549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 30.4390 - MinusLogProbMetric: 30.4390 - val_loss: 30.1655 - val_MinusLogProbMetric: 30.1655 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 142/1000
2023-10-26 02:34:34.242 
Epoch 142/1000 
	 loss: 30.3638, MinusLogProbMetric: 30.3638, val_loss: 30.3931, val_MinusLogProbMetric: 30.3931

Epoch 142: val_loss did not improve from 30.16549
196/196 - 42s - loss: 30.3638 - MinusLogProbMetric: 30.3638 - val_loss: 30.3931 - val_MinusLogProbMetric: 30.3931 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 143/1000
2023-10-26 02:35:16.898 
Epoch 143/1000 
	 loss: 30.4892, MinusLogProbMetric: 30.4892, val_loss: 30.8581, val_MinusLogProbMetric: 30.8581

Epoch 143: val_loss did not improve from 30.16549
196/196 - 43s - loss: 30.4892 - MinusLogProbMetric: 30.4892 - val_loss: 30.8581 - val_MinusLogProbMetric: 30.8581 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 144/1000
2023-10-26 02:35:59.959 
Epoch 144/1000 
	 loss: 30.3885, MinusLogProbMetric: 30.3885, val_loss: 30.6233, val_MinusLogProbMetric: 30.6233

Epoch 144: val_loss did not improve from 30.16549
196/196 - 43s - loss: 30.3885 - MinusLogProbMetric: 30.3885 - val_loss: 30.6233 - val_MinusLogProbMetric: 30.6233 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 145/1000
2023-10-26 02:36:42.644 
Epoch 145/1000 
	 loss: 30.4323, MinusLogProbMetric: 30.4323, val_loss: 30.6469, val_MinusLogProbMetric: 30.6469

Epoch 145: val_loss did not improve from 30.16549
196/196 - 43s - loss: 30.4323 - MinusLogProbMetric: 30.4323 - val_loss: 30.6469 - val_MinusLogProbMetric: 30.6469 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 146/1000
2023-10-26 02:37:25.527 
Epoch 146/1000 
	 loss: 30.2680, MinusLogProbMetric: 30.2680, val_loss: 30.2730, val_MinusLogProbMetric: 30.2730

Epoch 146: val_loss did not improve from 30.16549
196/196 - 43s - loss: 30.2680 - MinusLogProbMetric: 30.2680 - val_loss: 30.2730 - val_MinusLogProbMetric: 30.2730 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 147/1000
2023-10-26 02:38:08.057 
Epoch 147/1000 
	 loss: 30.2388, MinusLogProbMetric: 30.2388, val_loss: 30.7726, val_MinusLogProbMetric: 30.7726

Epoch 147: val_loss did not improve from 30.16549
196/196 - 43s - loss: 30.2388 - MinusLogProbMetric: 30.2388 - val_loss: 30.7726 - val_MinusLogProbMetric: 30.7726 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 148/1000
2023-10-26 02:38:50.651 
Epoch 148/1000 
	 loss: 30.3268, MinusLogProbMetric: 30.3268, val_loss: 31.4310, val_MinusLogProbMetric: 31.4310

Epoch 148: val_loss did not improve from 30.16549
196/196 - 43s - loss: 30.3268 - MinusLogProbMetric: 30.3268 - val_loss: 31.4310 - val_MinusLogProbMetric: 31.4310 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 149/1000
2023-10-26 02:39:33.324 
Epoch 149/1000 
	 loss: 30.2396, MinusLogProbMetric: 30.2396, val_loss: 30.7542, val_MinusLogProbMetric: 30.7542

Epoch 149: val_loss did not improve from 30.16549
196/196 - 43s - loss: 30.2396 - MinusLogProbMetric: 30.2396 - val_loss: 30.7542 - val_MinusLogProbMetric: 30.7542 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 150/1000
2023-10-26 02:40:15.783 
Epoch 150/1000 
	 loss: 30.2670, MinusLogProbMetric: 30.2670, val_loss: 31.1193, val_MinusLogProbMetric: 31.1193

Epoch 150: val_loss did not improve from 30.16549
196/196 - 42s - loss: 30.2670 - MinusLogProbMetric: 30.2670 - val_loss: 31.1193 - val_MinusLogProbMetric: 31.1193 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 151/1000
2023-10-26 02:40:58.310 
Epoch 151/1000 
	 loss: 30.3707, MinusLogProbMetric: 30.3707, val_loss: 30.1252, val_MinusLogProbMetric: 30.1252

Epoch 151: val_loss improved from 30.16549 to 30.12523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 30.3707 - MinusLogProbMetric: 30.3707 - val_loss: 30.1252 - val_MinusLogProbMetric: 30.1252 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 152/1000
2023-10-26 02:41:41.522 
Epoch 152/1000 
	 loss: 30.3332, MinusLogProbMetric: 30.3332, val_loss: 30.7679, val_MinusLogProbMetric: 30.7679

Epoch 152: val_loss did not improve from 30.12523
196/196 - 43s - loss: 30.3332 - MinusLogProbMetric: 30.3332 - val_loss: 30.7679 - val_MinusLogProbMetric: 30.7679 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 153/1000
2023-10-26 02:42:23.812 
Epoch 153/1000 
	 loss: 30.1664, MinusLogProbMetric: 30.1664, val_loss: 30.4701, val_MinusLogProbMetric: 30.4701

Epoch 153: val_loss did not improve from 30.12523
196/196 - 42s - loss: 30.1664 - MinusLogProbMetric: 30.1664 - val_loss: 30.4701 - val_MinusLogProbMetric: 30.4701 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 154/1000
2023-10-26 02:43:06.431 
Epoch 154/1000 
	 loss: 30.3207, MinusLogProbMetric: 30.3207, val_loss: 30.1630, val_MinusLogProbMetric: 30.1630

Epoch 154: val_loss did not improve from 30.12523
196/196 - 43s - loss: 30.3207 - MinusLogProbMetric: 30.3207 - val_loss: 30.1630 - val_MinusLogProbMetric: 30.1630 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 155/1000
2023-10-26 02:43:49.525 
Epoch 155/1000 
	 loss: 30.2511, MinusLogProbMetric: 30.2511, val_loss: 32.0450, val_MinusLogProbMetric: 32.0450

Epoch 155: val_loss did not improve from 30.12523
196/196 - 43s - loss: 30.2511 - MinusLogProbMetric: 30.2511 - val_loss: 32.0450 - val_MinusLogProbMetric: 32.0450 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 156/1000
2023-10-26 02:44:32.421 
Epoch 156/1000 
	 loss: 30.3602, MinusLogProbMetric: 30.3602, val_loss: 30.8756, val_MinusLogProbMetric: 30.8756

Epoch 156: val_loss did not improve from 30.12523
196/196 - 43s - loss: 30.3602 - MinusLogProbMetric: 30.3602 - val_loss: 30.8756 - val_MinusLogProbMetric: 30.8756 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 157/1000
2023-10-26 02:45:15.128 
Epoch 157/1000 
	 loss: 30.1911, MinusLogProbMetric: 30.1911, val_loss: 30.0887, val_MinusLogProbMetric: 30.0887

Epoch 157: val_loss improved from 30.12523 to 30.08871, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 30.1911 - MinusLogProbMetric: 30.1911 - val_loss: 30.0887 - val_MinusLogProbMetric: 30.0887 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 158/1000
2023-10-26 02:45:58.524 
Epoch 158/1000 
	 loss: 30.1427, MinusLogProbMetric: 30.1427, val_loss: 30.8670, val_MinusLogProbMetric: 30.8670

Epoch 158: val_loss did not improve from 30.08871
196/196 - 43s - loss: 30.1427 - MinusLogProbMetric: 30.1427 - val_loss: 30.8670 - val_MinusLogProbMetric: 30.8670 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 159/1000
2023-10-26 02:46:40.977 
Epoch 159/1000 
	 loss: 30.1729, MinusLogProbMetric: 30.1729, val_loss: 30.1230, val_MinusLogProbMetric: 30.1230

Epoch 159: val_loss did not improve from 30.08871
196/196 - 42s - loss: 30.1729 - MinusLogProbMetric: 30.1729 - val_loss: 30.1230 - val_MinusLogProbMetric: 30.1230 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 160/1000
2023-10-26 02:47:23.303 
Epoch 160/1000 
	 loss: 30.2092, MinusLogProbMetric: 30.2092, val_loss: 30.0341, val_MinusLogProbMetric: 30.0341

Epoch 160: val_loss improved from 30.08871 to 30.03414, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 30.2092 - MinusLogProbMetric: 30.2092 - val_loss: 30.0341 - val_MinusLogProbMetric: 30.0341 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 161/1000
2023-10-26 02:48:06.600 
Epoch 161/1000 
	 loss: 30.2412, MinusLogProbMetric: 30.2412, val_loss: 30.2235, val_MinusLogProbMetric: 30.2235

Epoch 161: val_loss did not improve from 30.03414
196/196 - 42s - loss: 30.2412 - MinusLogProbMetric: 30.2412 - val_loss: 30.2235 - val_MinusLogProbMetric: 30.2235 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 162/1000
2023-10-26 02:48:49.241 
Epoch 162/1000 
	 loss: 30.0813, MinusLogProbMetric: 30.0813, val_loss: 30.9453, val_MinusLogProbMetric: 30.9453

Epoch 162: val_loss did not improve from 30.03414
196/196 - 43s - loss: 30.0813 - MinusLogProbMetric: 30.0813 - val_loss: 30.9453 - val_MinusLogProbMetric: 30.9453 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 163/1000
2023-10-26 02:49:32.250 
Epoch 163/1000 
	 loss: 30.1377, MinusLogProbMetric: 30.1377, val_loss: 30.1522, val_MinusLogProbMetric: 30.1522

Epoch 163: val_loss did not improve from 30.03414
196/196 - 43s - loss: 30.1377 - MinusLogProbMetric: 30.1377 - val_loss: 30.1522 - val_MinusLogProbMetric: 30.1522 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 164/1000
2023-10-26 02:50:14.914 
Epoch 164/1000 
	 loss: 30.1184, MinusLogProbMetric: 30.1184, val_loss: 29.9982, val_MinusLogProbMetric: 29.9982

Epoch 164: val_loss improved from 30.03414 to 29.99825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 30.1184 - MinusLogProbMetric: 30.1184 - val_loss: 29.9982 - val_MinusLogProbMetric: 29.9982 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 165/1000
2023-10-26 02:50:58.396 
Epoch 165/1000 
	 loss: 30.1314, MinusLogProbMetric: 30.1314, val_loss: 30.4675, val_MinusLogProbMetric: 30.4675

Epoch 165: val_loss did not improve from 29.99825
196/196 - 43s - loss: 30.1314 - MinusLogProbMetric: 30.1314 - val_loss: 30.4675 - val_MinusLogProbMetric: 30.4675 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 166/1000
2023-10-26 02:51:40.918 
Epoch 166/1000 
	 loss: 30.1607, MinusLogProbMetric: 30.1607, val_loss: 30.5075, val_MinusLogProbMetric: 30.5075

Epoch 166: val_loss did not improve from 29.99825
196/196 - 43s - loss: 30.1607 - MinusLogProbMetric: 30.1607 - val_loss: 30.5075 - val_MinusLogProbMetric: 30.5075 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 167/1000
2023-10-26 02:52:23.665 
Epoch 167/1000 
	 loss: 30.0183, MinusLogProbMetric: 30.0183, val_loss: 30.6574, val_MinusLogProbMetric: 30.6574

Epoch 167: val_loss did not improve from 29.99825
196/196 - 43s - loss: 30.0183 - MinusLogProbMetric: 30.0183 - val_loss: 30.6574 - val_MinusLogProbMetric: 30.6574 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 168/1000
2023-10-26 02:53:06.839 
Epoch 168/1000 
	 loss: 30.1958, MinusLogProbMetric: 30.1958, val_loss: 30.8799, val_MinusLogProbMetric: 30.8799

Epoch 168: val_loss did not improve from 29.99825
196/196 - 43s - loss: 30.1958 - MinusLogProbMetric: 30.1958 - val_loss: 30.8799 - val_MinusLogProbMetric: 30.8799 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 169/1000
2023-10-26 02:53:49.776 
Epoch 169/1000 
	 loss: 29.9817, MinusLogProbMetric: 29.9817, val_loss: 29.7692, val_MinusLogProbMetric: 29.7692

Epoch 169: val_loss improved from 29.99825 to 29.76917, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 29.9817 - MinusLogProbMetric: 29.9817 - val_loss: 29.7692 - val_MinusLogProbMetric: 29.7692 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 170/1000
2023-10-26 02:54:33.360 
Epoch 170/1000 
	 loss: 30.2931, MinusLogProbMetric: 30.2931, val_loss: 30.5673, val_MinusLogProbMetric: 30.5673

Epoch 170: val_loss did not improve from 29.76917
196/196 - 43s - loss: 30.2931 - MinusLogProbMetric: 30.2931 - val_loss: 30.5673 - val_MinusLogProbMetric: 30.5673 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 171/1000
2023-10-26 02:55:16.289 
Epoch 171/1000 
	 loss: 30.0067, MinusLogProbMetric: 30.0067, val_loss: 30.8903, val_MinusLogProbMetric: 30.8903

Epoch 171: val_loss did not improve from 29.76917
196/196 - 43s - loss: 30.0067 - MinusLogProbMetric: 30.0067 - val_loss: 30.8903 - val_MinusLogProbMetric: 30.8903 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 172/1000
2023-10-26 02:55:59.296 
Epoch 172/1000 
	 loss: 30.0712, MinusLogProbMetric: 30.0712, val_loss: 30.8630, val_MinusLogProbMetric: 30.8630

Epoch 172: val_loss did not improve from 29.76917
196/196 - 43s - loss: 30.0712 - MinusLogProbMetric: 30.0712 - val_loss: 30.8630 - val_MinusLogProbMetric: 30.8630 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 173/1000
2023-10-26 02:56:42.102 
Epoch 173/1000 
	 loss: 29.9797, MinusLogProbMetric: 29.9797, val_loss: 30.2428, val_MinusLogProbMetric: 30.2428

Epoch 173: val_loss did not improve from 29.76917
196/196 - 43s - loss: 29.9797 - MinusLogProbMetric: 29.9797 - val_loss: 30.2428 - val_MinusLogProbMetric: 30.2428 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 174/1000
2023-10-26 02:57:24.815 
Epoch 174/1000 
	 loss: 30.0406, MinusLogProbMetric: 30.0406, val_loss: 29.9729, val_MinusLogProbMetric: 29.9729

Epoch 174: val_loss did not improve from 29.76917
196/196 - 43s - loss: 30.0406 - MinusLogProbMetric: 30.0406 - val_loss: 29.9729 - val_MinusLogProbMetric: 29.9729 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 175/1000
2023-10-26 02:58:07.619 
Epoch 175/1000 
	 loss: 30.0090, MinusLogProbMetric: 30.0090, val_loss: 30.0826, val_MinusLogProbMetric: 30.0826

Epoch 175: val_loss did not improve from 29.76917
196/196 - 43s - loss: 30.0090 - MinusLogProbMetric: 30.0090 - val_loss: 30.0826 - val_MinusLogProbMetric: 30.0826 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 176/1000
2023-10-26 02:58:50.630 
Epoch 176/1000 
	 loss: 29.9441, MinusLogProbMetric: 29.9441, val_loss: 30.3392, val_MinusLogProbMetric: 30.3392

Epoch 176: val_loss did not improve from 29.76917
196/196 - 43s - loss: 29.9441 - MinusLogProbMetric: 29.9441 - val_loss: 30.3392 - val_MinusLogProbMetric: 30.3392 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 177/1000
2023-10-26 02:59:33.814 
Epoch 177/1000 
	 loss: 30.0373, MinusLogProbMetric: 30.0373, val_loss: 31.1544, val_MinusLogProbMetric: 31.1544

Epoch 177: val_loss did not improve from 29.76917
196/196 - 43s - loss: 30.0373 - MinusLogProbMetric: 30.0373 - val_loss: 31.1544 - val_MinusLogProbMetric: 31.1544 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 178/1000
2023-10-26 03:00:16.627 
Epoch 178/1000 
	 loss: 30.0386, MinusLogProbMetric: 30.0386, val_loss: 30.8142, val_MinusLogProbMetric: 30.8142

Epoch 178: val_loss did not improve from 29.76917
196/196 - 43s - loss: 30.0386 - MinusLogProbMetric: 30.0386 - val_loss: 30.8142 - val_MinusLogProbMetric: 30.8142 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 179/1000
2023-10-26 03:00:59.467 
Epoch 179/1000 
	 loss: 29.9488, MinusLogProbMetric: 29.9488, val_loss: 30.6245, val_MinusLogProbMetric: 30.6245

Epoch 179: val_loss did not improve from 29.76917
196/196 - 43s - loss: 29.9488 - MinusLogProbMetric: 29.9488 - val_loss: 30.6245 - val_MinusLogProbMetric: 30.6245 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 180/1000
2023-10-26 03:01:41.919 
Epoch 180/1000 
	 loss: 29.9070, MinusLogProbMetric: 29.9070, val_loss: 30.2727, val_MinusLogProbMetric: 30.2727

Epoch 180: val_loss did not improve from 29.76917
196/196 - 42s - loss: 29.9070 - MinusLogProbMetric: 29.9070 - val_loss: 30.2727 - val_MinusLogProbMetric: 30.2727 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 181/1000
2023-10-26 03:02:24.921 
Epoch 181/1000 
	 loss: 30.0705, MinusLogProbMetric: 30.0705, val_loss: 30.6222, val_MinusLogProbMetric: 30.6222

Epoch 181: val_loss did not improve from 29.76917
196/196 - 43s - loss: 30.0705 - MinusLogProbMetric: 30.0705 - val_loss: 30.6222 - val_MinusLogProbMetric: 30.6222 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 182/1000
2023-10-26 03:03:07.742 
Epoch 182/1000 
	 loss: 30.0073, MinusLogProbMetric: 30.0073, val_loss: 29.9216, val_MinusLogProbMetric: 29.9216

Epoch 182: val_loss did not improve from 29.76917
196/196 - 43s - loss: 30.0073 - MinusLogProbMetric: 30.0073 - val_loss: 29.9216 - val_MinusLogProbMetric: 29.9216 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 183/1000
2023-10-26 03:03:50.165 
Epoch 183/1000 
	 loss: 29.8653, MinusLogProbMetric: 29.8653, val_loss: 30.9943, val_MinusLogProbMetric: 30.9943

Epoch 183: val_loss did not improve from 29.76917
196/196 - 42s - loss: 29.8653 - MinusLogProbMetric: 29.8653 - val_loss: 30.9943 - val_MinusLogProbMetric: 30.9943 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 184/1000
2023-10-26 03:04:32.956 
Epoch 184/1000 
	 loss: 30.1363, MinusLogProbMetric: 30.1363, val_loss: 30.1949, val_MinusLogProbMetric: 30.1949

Epoch 184: val_loss did not improve from 29.76917
196/196 - 43s - loss: 30.1363 - MinusLogProbMetric: 30.1363 - val_loss: 30.1949 - val_MinusLogProbMetric: 30.1949 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 185/1000
2023-10-26 03:05:15.481 
Epoch 185/1000 
	 loss: 29.9099, MinusLogProbMetric: 29.9099, val_loss: 31.0650, val_MinusLogProbMetric: 31.0650

Epoch 185: val_loss did not improve from 29.76917
196/196 - 43s - loss: 29.9099 - MinusLogProbMetric: 29.9099 - val_loss: 31.0650 - val_MinusLogProbMetric: 31.0650 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 186/1000
2023-10-26 03:05:58.282 
Epoch 186/1000 
	 loss: 29.8962, MinusLogProbMetric: 29.8962, val_loss: 29.9550, val_MinusLogProbMetric: 29.9550

Epoch 186: val_loss did not improve from 29.76917
196/196 - 43s - loss: 29.8962 - MinusLogProbMetric: 29.8962 - val_loss: 29.9550 - val_MinusLogProbMetric: 29.9550 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 187/1000
2023-10-26 03:06:41.237 
Epoch 187/1000 
	 loss: 29.8868, MinusLogProbMetric: 29.8868, val_loss: 30.2746, val_MinusLogProbMetric: 30.2746

Epoch 187: val_loss did not improve from 29.76917
196/196 - 43s - loss: 29.8868 - MinusLogProbMetric: 29.8868 - val_loss: 30.2746 - val_MinusLogProbMetric: 30.2746 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 188/1000
2023-10-26 03:07:23.838 
Epoch 188/1000 
	 loss: 29.9986, MinusLogProbMetric: 29.9986, val_loss: 30.6522, val_MinusLogProbMetric: 30.6522

Epoch 188: val_loss did not improve from 29.76917
196/196 - 43s - loss: 29.9986 - MinusLogProbMetric: 29.9986 - val_loss: 30.6522 - val_MinusLogProbMetric: 30.6522 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 189/1000
2023-10-26 03:08:06.985 
Epoch 189/1000 
	 loss: 29.8837, MinusLogProbMetric: 29.8837, val_loss: 29.8186, val_MinusLogProbMetric: 29.8186

Epoch 189: val_loss did not improve from 29.76917
196/196 - 43s - loss: 29.8837 - MinusLogProbMetric: 29.8837 - val_loss: 29.8186 - val_MinusLogProbMetric: 29.8186 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 190/1000
2023-10-26 03:08:49.689 
Epoch 190/1000 
	 loss: 29.8634, MinusLogProbMetric: 29.8634, val_loss: 29.8420, val_MinusLogProbMetric: 29.8420

Epoch 190: val_loss did not improve from 29.76917
196/196 - 43s - loss: 29.8634 - MinusLogProbMetric: 29.8634 - val_loss: 29.8420 - val_MinusLogProbMetric: 29.8420 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 191/1000
2023-10-26 03:09:33.227 
Epoch 191/1000 
	 loss: 29.8531, MinusLogProbMetric: 29.8531, val_loss: 29.7440, val_MinusLogProbMetric: 29.7440

Epoch 191: val_loss improved from 29.76917 to 29.74405, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 29.8531 - MinusLogProbMetric: 29.8531 - val_loss: 29.7440 - val_MinusLogProbMetric: 29.7440 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 192/1000
2023-10-26 03:10:14.344 
Epoch 192/1000 
	 loss: 29.8184, MinusLogProbMetric: 29.8184, val_loss: 29.9255, val_MinusLogProbMetric: 29.9255

Epoch 192: val_loss did not improve from 29.74405
196/196 - 40s - loss: 29.8184 - MinusLogProbMetric: 29.8184 - val_loss: 29.9255 - val_MinusLogProbMetric: 29.9255 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 193/1000
2023-10-26 03:10:52.373 
Epoch 193/1000 
	 loss: 29.8444, MinusLogProbMetric: 29.8444, val_loss: 30.3527, val_MinusLogProbMetric: 30.3527

Epoch 193: val_loss did not improve from 29.74405
196/196 - 38s - loss: 29.8444 - MinusLogProbMetric: 29.8444 - val_loss: 30.3527 - val_MinusLogProbMetric: 30.3527 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 194/1000
2023-10-26 03:11:33.312 
Epoch 194/1000 
	 loss: 30.0957, MinusLogProbMetric: 30.0957, val_loss: 30.2065, val_MinusLogProbMetric: 30.2065

Epoch 194: val_loss did not improve from 29.74405
196/196 - 41s - loss: 30.0957 - MinusLogProbMetric: 30.0957 - val_loss: 30.2065 - val_MinusLogProbMetric: 30.2065 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 195/1000
2023-10-26 03:12:16.154 
Epoch 195/1000 
	 loss: 29.8680, MinusLogProbMetric: 29.8680, val_loss: 30.1008, val_MinusLogProbMetric: 30.1008

Epoch 195: val_loss did not improve from 29.74405
196/196 - 43s - loss: 29.8680 - MinusLogProbMetric: 29.8680 - val_loss: 30.1008 - val_MinusLogProbMetric: 30.1008 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 196/1000
2023-10-26 03:12:59.282 
Epoch 196/1000 
	 loss: 29.8529, MinusLogProbMetric: 29.8529, val_loss: 30.6305, val_MinusLogProbMetric: 30.6305

Epoch 196: val_loss did not improve from 29.74405
196/196 - 43s - loss: 29.8529 - MinusLogProbMetric: 29.8529 - val_loss: 30.6305 - val_MinusLogProbMetric: 30.6305 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 197/1000
2023-10-26 03:13:42.226 
Epoch 197/1000 
	 loss: 29.8903, MinusLogProbMetric: 29.8903, val_loss: 30.2001, val_MinusLogProbMetric: 30.2001

Epoch 197: val_loss did not improve from 29.74405
196/196 - 43s - loss: 29.8903 - MinusLogProbMetric: 29.8903 - val_loss: 30.2001 - val_MinusLogProbMetric: 30.2001 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 198/1000
2023-10-26 03:14:25.173 
Epoch 198/1000 
	 loss: 29.7759, MinusLogProbMetric: 29.7759, val_loss: 29.7263, val_MinusLogProbMetric: 29.7263

Epoch 198: val_loss improved from 29.74405 to 29.72633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 29.7759 - MinusLogProbMetric: 29.7759 - val_loss: 29.7263 - val_MinusLogProbMetric: 29.7263 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 199/1000
2023-10-26 03:15:09.048 
Epoch 199/1000 
	 loss: 29.8838, MinusLogProbMetric: 29.8838, val_loss: 30.2139, val_MinusLogProbMetric: 30.2139

Epoch 199: val_loss did not improve from 29.72633
196/196 - 43s - loss: 29.8838 - MinusLogProbMetric: 29.8838 - val_loss: 30.2139 - val_MinusLogProbMetric: 30.2139 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 200/1000
2023-10-26 03:15:52.047 
Epoch 200/1000 
	 loss: 29.9108, MinusLogProbMetric: 29.9108, val_loss: 29.5757, val_MinusLogProbMetric: 29.5757

Epoch 200: val_loss improved from 29.72633 to 29.57570, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 29.9108 - MinusLogProbMetric: 29.9108 - val_loss: 29.5757 - val_MinusLogProbMetric: 29.5757 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 201/1000
2023-10-26 03:16:35.884 
Epoch 201/1000 
	 loss: 29.7097, MinusLogProbMetric: 29.7097, val_loss: 29.5711, val_MinusLogProbMetric: 29.5711

Epoch 201: val_loss improved from 29.57570 to 29.57114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 29.7097 - MinusLogProbMetric: 29.7097 - val_loss: 29.5711 - val_MinusLogProbMetric: 29.5711 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 202/1000
2023-10-26 03:17:19.292 
Epoch 202/1000 
	 loss: 29.7294, MinusLogProbMetric: 29.7294, val_loss: 29.7611, val_MinusLogProbMetric: 29.7611

Epoch 202: val_loss did not improve from 29.57114
196/196 - 43s - loss: 29.7294 - MinusLogProbMetric: 29.7294 - val_loss: 29.7611 - val_MinusLogProbMetric: 29.7611 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 203/1000
2023-10-26 03:18:01.991 
Epoch 203/1000 
	 loss: 29.9358, MinusLogProbMetric: 29.9358, val_loss: 29.5622, val_MinusLogProbMetric: 29.5622

Epoch 203: val_loss improved from 29.57114 to 29.56220, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 29.9358 - MinusLogProbMetric: 29.9358 - val_loss: 29.5622 - val_MinusLogProbMetric: 29.5622 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 204/1000
2023-10-26 03:18:44.993 
Epoch 204/1000 
	 loss: 29.7537, MinusLogProbMetric: 29.7537, val_loss: 29.5508, val_MinusLogProbMetric: 29.5508

Epoch 204: val_loss improved from 29.56220 to 29.55079, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 29.7537 - MinusLogProbMetric: 29.7537 - val_loss: 29.5508 - val_MinusLogProbMetric: 29.5508 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 205/1000
2023-10-26 03:19:28.637 
Epoch 205/1000 
	 loss: 29.7602, MinusLogProbMetric: 29.7602, val_loss: 30.0036, val_MinusLogProbMetric: 30.0036

Epoch 205: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.7602 - MinusLogProbMetric: 29.7602 - val_loss: 30.0036 - val_MinusLogProbMetric: 30.0036 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 206/1000
2023-10-26 03:20:11.526 
Epoch 206/1000 
	 loss: 29.7626, MinusLogProbMetric: 29.7626, val_loss: 30.9016, val_MinusLogProbMetric: 30.9016

Epoch 206: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.7626 - MinusLogProbMetric: 29.7626 - val_loss: 30.9016 - val_MinusLogProbMetric: 30.9016 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 207/1000
2023-10-26 03:20:54.641 
Epoch 207/1000 
	 loss: 29.8764, MinusLogProbMetric: 29.8764, val_loss: 30.1684, val_MinusLogProbMetric: 30.1684

Epoch 207: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.8764 - MinusLogProbMetric: 29.8764 - val_loss: 30.1684 - val_MinusLogProbMetric: 30.1684 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 208/1000
2023-10-26 03:21:37.487 
Epoch 208/1000 
	 loss: 29.7940, MinusLogProbMetric: 29.7940, val_loss: 29.7754, val_MinusLogProbMetric: 29.7754

Epoch 208: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.7940 - MinusLogProbMetric: 29.7940 - val_loss: 29.7754 - val_MinusLogProbMetric: 29.7754 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 209/1000
2023-10-26 03:22:20.034 
Epoch 209/1000 
	 loss: 29.8395, MinusLogProbMetric: 29.8395, val_loss: 30.6064, val_MinusLogProbMetric: 30.6064

Epoch 209: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.8395 - MinusLogProbMetric: 29.8395 - val_loss: 30.6064 - val_MinusLogProbMetric: 30.6064 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 210/1000
2023-10-26 03:23:02.816 
Epoch 210/1000 
	 loss: 29.9443, MinusLogProbMetric: 29.9443, val_loss: 29.6412, val_MinusLogProbMetric: 29.6412

Epoch 210: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.9443 - MinusLogProbMetric: 29.9443 - val_loss: 29.6412 - val_MinusLogProbMetric: 29.6412 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 211/1000
2023-10-26 03:23:45.630 
Epoch 211/1000 
	 loss: 29.6850, MinusLogProbMetric: 29.6850, val_loss: 30.1017, val_MinusLogProbMetric: 30.1017

Epoch 211: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.6850 - MinusLogProbMetric: 29.6850 - val_loss: 30.1017 - val_MinusLogProbMetric: 30.1017 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 212/1000
2023-10-26 03:24:28.420 
Epoch 212/1000 
	 loss: 29.9602, MinusLogProbMetric: 29.9602, val_loss: 29.5630, val_MinusLogProbMetric: 29.5630

Epoch 212: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.9602 - MinusLogProbMetric: 29.9602 - val_loss: 29.5630 - val_MinusLogProbMetric: 29.5630 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 213/1000
2023-10-26 03:25:11.029 
Epoch 213/1000 
	 loss: 29.7510, MinusLogProbMetric: 29.7510, val_loss: 30.1435, val_MinusLogProbMetric: 30.1435

Epoch 213: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.7510 - MinusLogProbMetric: 29.7510 - val_loss: 30.1435 - val_MinusLogProbMetric: 30.1435 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 214/1000
2023-10-26 03:25:53.758 
Epoch 214/1000 
	 loss: 29.7330, MinusLogProbMetric: 29.7330, val_loss: 30.1450, val_MinusLogProbMetric: 30.1450

Epoch 214: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.7330 - MinusLogProbMetric: 29.7330 - val_loss: 30.1450 - val_MinusLogProbMetric: 30.1450 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 215/1000
2023-10-26 03:26:36.674 
Epoch 215/1000 
	 loss: 29.7678, MinusLogProbMetric: 29.7678, val_loss: 30.2347, val_MinusLogProbMetric: 30.2347

Epoch 215: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.7678 - MinusLogProbMetric: 29.7678 - val_loss: 30.2347 - val_MinusLogProbMetric: 30.2347 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 216/1000
2023-10-26 03:27:19.542 
Epoch 216/1000 
	 loss: 29.7078, MinusLogProbMetric: 29.7078, val_loss: 29.8669, val_MinusLogProbMetric: 29.8669

Epoch 216: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.7078 - MinusLogProbMetric: 29.7078 - val_loss: 29.8669 - val_MinusLogProbMetric: 29.8669 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 217/1000
2023-10-26 03:28:02.640 
Epoch 217/1000 
	 loss: 29.6747, MinusLogProbMetric: 29.6747, val_loss: 30.0186, val_MinusLogProbMetric: 30.0186

Epoch 217: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.6747 - MinusLogProbMetric: 29.6747 - val_loss: 30.0186 - val_MinusLogProbMetric: 30.0186 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 218/1000
2023-10-26 03:28:45.958 
Epoch 218/1000 
	 loss: 29.7339, MinusLogProbMetric: 29.7339, val_loss: 29.9401, val_MinusLogProbMetric: 29.9401

Epoch 218: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.7339 - MinusLogProbMetric: 29.7339 - val_loss: 29.9401 - val_MinusLogProbMetric: 29.9401 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 219/1000
2023-10-26 03:29:29.048 
Epoch 219/1000 
	 loss: 29.6920, MinusLogProbMetric: 29.6920, val_loss: 31.6925, val_MinusLogProbMetric: 31.6925

Epoch 219: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.6920 - MinusLogProbMetric: 29.6920 - val_loss: 31.6925 - val_MinusLogProbMetric: 31.6925 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 220/1000
2023-10-26 03:30:12.028 
Epoch 220/1000 
	 loss: 29.8100, MinusLogProbMetric: 29.8100, val_loss: 29.9407, val_MinusLogProbMetric: 29.9407

Epoch 220: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.8100 - MinusLogProbMetric: 29.8100 - val_loss: 29.9407 - val_MinusLogProbMetric: 29.9407 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 221/1000
2023-10-26 03:30:55.536 
Epoch 221/1000 
	 loss: 29.6839, MinusLogProbMetric: 29.6839, val_loss: 29.7769, val_MinusLogProbMetric: 29.7769

Epoch 221: val_loss did not improve from 29.55079
196/196 - 44s - loss: 29.6839 - MinusLogProbMetric: 29.6839 - val_loss: 29.7769 - val_MinusLogProbMetric: 29.7769 - lr: 0.0010 - 44s/epoch - 222ms/step
Epoch 222/1000
2023-10-26 03:31:38.231 
Epoch 222/1000 
	 loss: 29.6598, MinusLogProbMetric: 29.6598, val_loss: 29.7581, val_MinusLogProbMetric: 29.7581

Epoch 222: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.6598 - MinusLogProbMetric: 29.6598 - val_loss: 29.7581 - val_MinusLogProbMetric: 29.7581 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 223/1000
2023-10-26 03:32:21.084 
Epoch 223/1000 
	 loss: 29.6319, MinusLogProbMetric: 29.6319, val_loss: 31.5424, val_MinusLogProbMetric: 31.5424

Epoch 223: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.6319 - MinusLogProbMetric: 29.6319 - val_loss: 31.5424 - val_MinusLogProbMetric: 31.5424 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 224/1000
2023-10-26 03:33:03.817 
Epoch 224/1000 
	 loss: 29.7391, MinusLogProbMetric: 29.7391, val_loss: 29.9867, val_MinusLogProbMetric: 29.9867

Epoch 224: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.7391 - MinusLogProbMetric: 29.7391 - val_loss: 29.9867 - val_MinusLogProbMetric: 29.9867 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 225/1000
2023-10-26 03:33:46.604 
Epoch 225/1000 
	 loss: 29.6349, MinusLogProbMetric: 29.6349, val_loss: 29.8103, val_MinusLogProbMetric: 29.8103

Epoch 225: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.6349 - MinusLogProbMetric: 29.6349 - val_loss: 29.8103 - val_MinusLogProbMetric: 29.8103 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 226/1000
2023-10-26 03:34:29.401 
Epoch 226/1000 
	 loss: 29.6198, MinusLogProbMetric: 29.6198, val_loss: 29.7745, val_MinusLogProbMetric: 29.7745

Epoch 226: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.6198 - MinusLogProbMetric: 29.6198 - val_loss: 29.7745 - val_MinusLogProbMetric: 29.7745 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 227/1000
2023-10-26 03:35:12.490 
Epoch 227/1000 
	 loss: 29.6376, MinusLogProbMetric: 29.6376, val_loss: 30.1672, val_MinusLogProbMetric: 30.1672

Epoch 227: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.6376 - MinusLogProbMetric: 29.6376 - val_loss: 30.1672 - val_MinusLogProbMetric: 30.1672 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 228/1000
2023-10-26 03:35:55.588 
Epoch 228/1000 
	 loss: 29.6686, MinusLogProbMetric: 29.6686, val_loss: 29.6891, val_MinusLogProbMetric: 29.6891

Epoch 228: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.6686 - MinusLogProbMetric: 29.6686 - val_loss: 29.6891 - val_MinusLogProbMetric: 29.6891 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 229/1000
2023-10-26 03:36:38.642 
Epoch 229/1000 
	 loss: 29.8206, MinusLogProbMetric: 29.8206, val_loss: 29.6278, val_MinusLogProbMetric: 29.6278

Epoch 229: val_loss did not improve from 29.55079
196/196 - 43s - loss: 29.8206 - MinusLogProbMetric: 29.8206 - val_loss: 29.6278 - val_MinusLogProbMetric: 29.6278 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 230/1000
2023-10-26 03:37:21.944 
Epoch 230/1000 
	 loss: 29.7511, MinusLogProbMetric: 29.7511, val_loss: 29.4435, val_MinusLogProbMetric: 29.4435

Epoch 230: val_loss improved from 29.55079 to 29.44345, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 29.7511 - MinusLogProbMetric: 29.7511 - val_loss: 29.4435 - val_MinusLogProbMetric: 29.4435 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 231/1000
2023-10-26 03:38:05.825 
Epoch 231/1000 
	 loss: 29.5918, MinusLogProbMetric: 29.5918, val_loss: 29.9136, val_MinusLogProbMetric: 29.9136

Epoch 231: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.5918 - MinusLogProbMetric: 29.5918 - val_loss: 29.9136 - val_MinusLogProbMetric: 29.9136 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 232/1000
2023-10-26 03:38:49.488 
Epoch 232/1000 
	 loss: 29.5736, MinusLogProbMetric: 29.5736, val_loss: 30.6956, val_MinusLogProbMetric: 30.6956

Epoch 232: val_loss did not improve from 29.44345
196/196 - 44s - loss: 29.5736 - MinusLogProbMetric: 29.5736 - val_loss: 30.6956 - val_MinusLogProbMetric: 30.6956 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 233/1000
2023-10-26 03:39:32.340 
Epoch 233/1000 
	 loss: 29.5180, MinusLogProbMetric: 29.5180, val_loss: 29.5617, val_MinusLogProbMetric: 29.5617

Epoch 233: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.5180 - MinusLogProbMetric: 29.5180 - val_loss: 29.5617 - val_MinusLogProbMetric: 29.5617 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 234/1000
2023-10-26 03:40:15.610 
Epoch 234/1000 
	 loss: 29.9060, MinusLogProbMetric: 29.9060, val_loss: 29.9933, val_MinusLogProbMetric: 29.9933

Epoch 234: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.9060 - MinusLogProbMetric: 29.9060 - val_loss: 29.9933 - val_MinusLogProbMetric: 29.9933 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 235/1000
2023-10-26 03:40:58.795 
Epoch 235/1000 
	 loss: 29.6698, MinusLogProbMetric: 29.6698, val_loss: 30.6742, val_MinusLogProbMetric: 30.6742

Epoch 235: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.6698 - MinusLogProbMetric: 29.6698 - val_loss: 30.6742 - val_MinusLogProbMetric: 30.6742 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 236/1000
2023-10-26 03:41:42.273 
Epoch 236/1000 
	 loss: 29.5110, MinusLogProbMetric: 29.5110, val_loss: 29.9663, val_MinusLogProbMetric: 29.9663

Epoch 236: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.5110 - MinusLogProbMetric: 29.5110 - val_loss: 29.9663 - val_MinusLogProbMetric: 29.9663 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 237/1000
2023-10-26 03:42:25.060 
Epoch 237/1000 
	 loss: 29.6993, MinusLogProbMetric: 29.6993, val_loss: 29.7426, val_MinusLogProbMetric: 29.7426

Epoch 237: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.6993 - MinusLogProbMetric: 29.6993 - val_loss: 29.7426 - val_MinusLogProbMetric: 29.7426 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 238/1000
2023-10-26 03:43:07.994 
Epoch 238/1000 
	 loss: 29.5472, MinusLogProbMetric: 29.5472, val_loss: 29.9144, val_MinusLogProbMetric: 29.9144

Epoch 238: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.5472 - MinusLogProbMetric: 29.5472 - val_loss: 29.9144 - val_MinusLogProbMetric: 29.9144 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 239/1000
2023-10-26 03:43:51.274 
Epoch 239/1000 
	 loss: 29.5765, MinusLogProbMetric: 29.5765, val_loss: 29.5869, val_MinusLogProbMetric: 29.5869

Epoch 239: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.5765 - MinusLogProbMetric: 29.5765 - val_loss: 29.5869 - val_MinusLogProbMetric: 29.5869 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 240/1000
2023-10-26 03:44:34.317 
Epoch 240/1000 
	 loss: 29.5366, MinusLogProbMetric: 29.5366, val_loss: 30.2563, val_MinusLogProbMetric: 30.2563

Epoch 240: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.5366 - MinusLogProbMetric: 29.5366 - val_loss: 30.2563 - val_MinusLogProbMetric: 30.2563 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 241/1000
2023-10-26 03:45:17.352 
Epoch 241/1000 
	 loss: 29.5283, MinusLogProbMetric: 29.5283, val_loss: 30.5081, val_MinusLogProbMetric: 30.5081

Epoch 241: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.5283 - MinusLogProbMetric: 29.5283 - val_loss: 30.5081 - val_MinusLogProbMetric: 30.5081 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 242/1000
2023-10-26 03:46:00.766 
Epoch 242/1000 
	 loss: 29.5191, MinusLogProbMetric: 29.5191, val_loss: 30.2757, val_MinusLogProbMetric: 30.2757

Epoch 242: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.5191 - MinusLogProbMetric: 29.5191 - val_loss: 30.2757 - val_MinusLogProbMetric: 30.2757 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 243/1000
2023-10-26 03:46:43.666 
Epoch 243/1000 
	 loss: 29.4839, MinusLogProbMetric: 29.4839, val_loss: 30.7890, val_MinusLogProbMetric: 30.7890

Epoch 243: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.4839 - MinusLogProbMetric: 29.4839 - val_loss: 30.7890 - val_MinusLogProbMetric: 30.7890 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 244/1000
2023-10-26 03:47:26.605 
Epoch 244/1000 
	 loss: 29.7028, MinusLogProbMetric: 29.7028, val_loss: 31.0251, val_MinusLogProbMetric: 31.0251

Epoch 244: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.7028 - MinusLogProbMetric: 29.7028 - val_loss: 31.0251 - val_MinusLogProbMetric: 31.0251 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 245/1000
2023-10-26 03:48:09.459 
Epoch 245/1000 
	 loss: 29.4500, MinusLogProbMetric: 29.4500, val_loss: 30.0179, val_MinusLogProbMetric: 30.0179

Epoch 245: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.4500 - MinusLogProbMetric: 29.4500 - val_loss: 30.0179 - val_MinusLogProbMetric: 30.0179 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 246/1000
2023-10-26 03:48:52.545 
Epoch 246/1000 
	 loss: 29.6088, MinusLogProbMetric: 29.6088, val_loss: 30.1637, val_MinusLogProbMetric: 30.1637

Epoch 246: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.6088 - MinusLogProbMetric: 29.6088 - val_loss: 30.1637 - val_MinusLogProbMetric: 30.1637 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 247/1000
2023-10-26 03:49:35.544 
Epoch 247/1000 
	 loss: 29.6003, MinusLogProbMetric: 29.6003, val_loss: 29.7343, val_MinusLogProbMetric: 29.7343

Epoch 247: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.6003 - MinusLogProbMetric: 29.6003 - val_loss: 29.7343 - val_MinusLogProbMetric: 29.7343 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 248/1000
2023-10-26 03:50:18.389 
Epoch 248/1000 
	 loss: 29.5134, MinusLogProbMetric: 29.5134, val_loss: 30.0425, val_MinusLogProbMetric: 30.0425

Epoch 248: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.5134 - MinusLogProbMetric: 29.5134 - val_loss: 30.0425 - val_MinusLogProbMetric: 30.0425 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 249/1000
2023-10-26 03:51:01.513 
Epoch 249/1000 
	 loss: 29.4535, MinusLogProbMetric: 29.4535, val_loss: 30.0044, val_MinusLogProbMetric: 30.0044

Epoch 249: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.4535 - MinusLogProbMetric: 29.4535 - val_loss: 30.0044 - val_MinusLogProbMetric: 30.0044 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 250/1000
2023-10-26 03:51:43.993 
Epoch 250/1000 
	 loss: 29.4789, MinusLogProbMetric: 29.4789, val_loss: 31.0743, val_MinusLogProbMetric: 31.0743

Epoch 250: val_loss did not improve from 29.44345
196/196 - 42s - loss: 29.4789 - MinusLogProbMetric: 29.4789 - val_loss: 31.0743 - val_MinusLogProbMetric: 31.0743 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 251/1000
2023-10-26 03:52:26.817 
Epoch 251/1000 
	 loss: 29.5216, MinusLogProbMetric: 29.5216, val_loss: 30.1320, val_MinusLogProbMetric: 30.1320

Epoch 251: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.5216 - MinusLogProbMetric: 29.5216 - val_loss: 30.1320 - val_MinusLogProbMetric: 30.1320 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 252/1000
2023-10-26 03:53:09.632 
Epoch 252/1000 
	 loss: 29.5728, MinusLogProbMetric: 29.5728, val_loss: 30.0315, val_MinusLogProbMetric: 30.0315

Epoch 252: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.5728 - MinusLogProbMetric: 29.5728 - val_loss: 30.0315 - val_MinusLogProbMetric: 30.0315 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 253/1000
2023-10-26 03:53:51.812 
Epoch 253/1000 
	 loss: 29.5740, MinusLogProbMetric: 29.5740, val_loss: 29.7640, val_MinusLogProbMetric: 29.7640

Epoch 253: val_loss did not improve from 29.44345
196/196 - 42s - loss: 29.5740 - MinusLogProbMetric: 29.5740 - val_loss: 29.7640 - val_MinusLogProbMetric: 29.7640 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 254/1000
2023-10-26 03:54:34.577 
Epoch 254/1000 
	 loss: 29.4453, MinusLogProbMetric: 29.4453, val_loss: 29.4588, val_MinusLogProbMetric: 29.4588

Epoch 254: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.4453 - MinusLogProbMetric: 29.4453 - val_loss: 29.4588 - val_MinusLogProbMetric: 29.4588 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 255/1000
2023-10-26 03:55:17.631 
Epoch 255/1000 
	 loss: 29.4815, MinusLogProbMetric: 29.4815, val_loss: 29.5006, val_MinusLogProbMetric: 29.5006

Epoch 255: val_loss did not improve from 29.44345
196/196 - 43s - loss: 29.4815 - MinusLogProbMetric: 29.4815 - val_loss: 29.5006 - val_MinusLogProbMetric: 29.5006 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 256/1000
2023-10-26 03:56:00.382 
Epoch 256/1000 
	 loss: 29.5139, MinusLogProbMetric: 29.5139, val_loss: 29.4404, val_MinusLogProbMetric: 29.4404

Epoch 256: val_loss improved from 29.44345 to 29.44036, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 29.5139 - MinusLogProbMetric: 29.5139 - val_loss: 29.4404 - val_MinusLogProbMetric: 29.4404 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 257/1000
2023-10-26 03:56:44.145 
Epoch 257/1000 
	 loss: 29.5093, MinusLogProbMetric: 29.5093, val_loss: 29.3433, val_MinusLogProbMetric: 29.3433

Epoch 257: val_loss improved from 29.44036 to 29.34330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 29.5093 - MinusLogProbMetric: 29.5093 - val_loss: 29.3433 - val_MinusLogProbMetric: 29.3433 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 258/1000
2023-10-26 03:57:27.569 
Epoch 258/1000 
	 loss: 29.4228, MinusLogProbMetric: 29.4228, val_loss: 30.7490, val_MinusLogProbMetric: 30.7490

Epoch 258: val_loss did not improve from 29.34330
196/196 - 43s - loss: 29.4228 - MinusLogProbMetric: 29.4228 - val_loss: 30.7490 - val_MinusLogProbMetric: 30.7490 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 259/1000
2023-10-26 03:58:10.286 
Epoch 259/1000 
	 loss: 29.4846, MinusLogProbMetric: 29.4846, val_loss: 30.2166, val_MinusLogProbMetric: 30.2166

Epoch 259: val_loss did not improve from 29.34330
196/196 - 43s - loss: 29.4846 - MinusLogProbMetric: 29.4846 - val_loss: 30.2166 - val_MinusLogProbMetric: 30.2166 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 260/1000
2023-10-26 03:58:52.817 
Epoch 260/1000 
	 loss: 29.5093, MinusLogProbMetric: 29.5093, val_loss: 29.5945, val_MinusLogProbMetric: 29.5945

Epoch 260: val_loss did not improve from 29.34330
196/196 - 43s - loss: 29.5093 - MinusLogProbMetric: 29.5093 - val_loss: 29.5945 - val_MinusLogProbMetric: 29.5945 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 261/1000
2023-10-26 03:59:35.943 
Epoch 261/1000 
	 loss: 29.3865, MinusLogProbMetric: 29.3865, val_loss: 30.8300, val_MinusLogProbMetric: 30.8300

Epoch 261: val_loss did not improve from 29.34330
196/196 - 43s - loss: 29.3865 - MinusLogProbMetric: 29.3865 - val_loss: 30.8300 - val_MinusLogProbMetric: 30.8300 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 262/1000
2023-10-26 04:00:18.689 
Epoch 262/1000 
	 loss: 29.5793, MinusLogProbMetric: 29.5793, val_loss: 29.9816, val_MinusLogProbMetric: 29.9816

Epoch 262: val_loss did not improve from 29.34330
196/196 - 43s - loss: 29.5793 - MinusLogProbMetric: 29.5793 - val_loss: 29.9816 - val_MinusLogProbMetric: 29.9816 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 263/1000
2023-10-26 04:01:01.789 
Epoch 263/1000 
	 loss: 29.6690, MinusLogProbMetric: 29.6690, val_loss: 29.1799, val_MinusLogProbMetric: 29.1799

Epoch 263: val_loss improved from 29.34330 to 29.17986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 29.6690 - MinusLogProbMetric: 29.6690 - val_loss: 29.1799 - val_MinusLogProbMetric: 29.1799 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 264/1000
2023-10-26 04:01:45.316 
Epoch 264/1000 
	 loss: 29.3260, MinusLogProbMetric: 29.3260, val_loss: 31.6442, val_MinusLogProbMetric: 31.6442

Epoch 264: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.3260 - MinusLogProbMetric: 29.3260 - val_loss: 31.6442 - val_MinusLogProbMetric: 31.6442 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 265/1000
2023-10-26 04:02:28.120 
Epoch 265/1000 
	 loss: 29.5011, MinusLogProbMetric: 29.5011, val_loss: 31.1625, val_MinusLogProbMetric: 31.1625

Epoch 265: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.5011 - MinusLogProbMetric: 29.5011 - val_loss: 31.1625 - val_MinusLogProbMetric: 31.1625 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 266/1000
2023-10-26 04:03:10.926 
Epoch 266/1000 
	 loss: 29.4972, MinusLogProbMetric: 29.4972, val_loss: 29.7030, val_MinusLogProbMetric: 29.7030

Epoch 266: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.4972 - MinusLogProbMetric: 29.4972 - val_loss: 29.7030 - val_MinusLogProbMetric: 29.7030 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 267/1000
2023-10-26 04:03:53.705 
Epoch 267/1000 
	 loss: 29.3320, MinusLogProbMetric: 29.3320, val_loss: 29.6666, val_MinusLogProbMetric: 29.6666

Epoch 267: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.3320 - MinusLogProbMetric: 29.3320 - val_loss: 29.6666 - val_MinusLogProbMetric: 29.6666 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 268/1000
2023-10-26 04:04:36.560 
Epoch 268/1000 
	 loss: 29.4137, MinusLogProbMetric: 29.4137, val_loss: 29.5866, val_MinusLogProbMetric: 29.5866

Epoch 268: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.4137 - MinusLogProbMetric: 29.4137 - val_loss: 29.5866 - val_MinusLogProbMetric: 29.5866 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 269/1000
2023-10-26 04:05:19.635 
Epoch 269/1000 
	 loss: 29.3810, MinusLogProbMetric: 29.3810, val_loss: 30.1114, val_MinusLogProbMetric: 30.1114

Epoch 269: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.3810 - MinusLogProbMetric: 29.3810 - val_loss: 30.1114 - val_MinusLogProbMetric: 30.1114 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 270/1000
2023-10-26 04:06:02.838 
Epoch 270/1000 
	 loss: 29.3698, MinusLogProbMetric: 29.3698, val_loss: 29.5540, val_MinusLogProbMetric: 29.5540

Epoch 270: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.3698 - MinusLogProbMetric: 29.3698 - val_loss: 29.5540 - val_MinusLogProbMetric: 29.5540 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 271/1000
2023-10-26 04:06:45.846 
Epoch 271/1000 
	 loss: 29.4081, MinusLogProbMetric: 29.4081, val_loss: 29.4734, val_MinusLogProbMetric: 29.4734

Epoch 271: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.4081 - MinusLogProbMetric: 29.4081 - val_loss: 29.4734 - val_MinusLogProbMetric: 29.4734 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 272/1000
2023-10-26 04:07:28.371 
Epoch 272/1000 
	 loss: 29.4336, MinusLogProbMetric: 29.4336, val_loss: 30.0669, val_MinusLogProbMetric: 30.0669

Epoch 272: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.4336 - MinusLogProbMetric: 29.4336 - val_loss: 30.0669 - val_MinusLogProbMetric: 30.0669 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 273/1000
2023-10-26 04:08:11.337 
Epoch 273/1000 
	 loss: 29.4299, MinusLogProbMetric: 29.4299, val_loss: 30.0027, val_MinusLogProbMetric: 30.0027

Epoch 273: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.4299 - MinusLogProbMetric: 29.4299 - val_loss: 30.0027 - val_MinusLogProbMetric: 30.0027 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 274/1000
2023-10-26 04:08:53.906 
Epoch 274/1000 
	 loss: 29.4495, MinusLogProbMetric: 29.4495, val_loss: 29.6507, val_MinusLogProbMetric: 29.6507

Epoch 274: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.4495 - MinusLogProbMetric: 29.4495 - val_loss: 29.6507 - val_MinusLogProbMetric: 29.6507 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 275/1000
2023-10-26 04:09:36.788 
Epoch 275/1000 
	 loss: 29.3548, MinusLogProbMetric: 29.3548, val_loss: 29.6513, val_MinusLogProbMetric: 29.6513

Epoch 275: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.3548 - MinusLogProbMetric: 29.3548 - val_loss: 29.6513 - val_MinusLogProbMetric: 29.6513 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 276/1000
2023-10-26 04:10:19.867 
Epoch 276/1000 
	 loss: 29.3941, MinusLogProbMetric: 29.3941, val_loss: 29.3924, val_MinusLogProbMetric: 29.3924

Epoch 276: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.3941 - MinusLogProbMetric: 29.3941 - val_loss: 29.3924 - val_MinusLogProbMetric: 29.3924 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 277/1000
2023-10-26 04:11:02.712 
Epoch 277/1000 
	 loss: 29.4884, MinusLogProbMetric: 29.4884, val_loss: 30.4062, val_MinusLogProbMetric: 30.4062

Epoch 277: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.4884 - MinusLogProbMetric: 29.4884 - val_loss: 30.4062 - val_MinusLogProbMetric: 30.4062 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 278/1000
2023-10-26 04:11:44.854 
Epoch 278/1000 
	 loss: 29.4916, MinusLogProbMetric: 29.4916, val_loss: 29.7142, val_MinusLogProbMetric: 29.7142

Epoch 278: val_loss did not improve from 29.17986
196/196 - 42s - loss: 29.4916 - MinusLogProbMetric: 29.4916 - val_loss: 29.7142 - val_MinusLogProbMetric: 29.7142 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 279/1000
2023-10-26 04:12:27.758 
Epoch 279/1000 
	 loss: 29.2957, MinusLogProbMetric: 29.2957, val_loss: 30.0273, val_MinusLogProbMetric: 30.0273

Epoch 279: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.2957 - MinusLogProbMetric: 29.2957 - val_loss: 30.0273 - val_MinusLogProbMetric: 30.0273 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 280/1000
2023-10-26 04:13:07.337 
Epoch 280/1000 
	 loss: 29.3155, MinusLogProbMetric: 29.3155, val_loss: 30.1871, val_MinusLogProbMetric: 30.1871

Epoch 280: val_loss did not improve from 29.17986
196/196 - 40s - loss: 29.3155 - MinusLogProbMetric: 29.3155 - val_loss: 30.1871 - val_MinusLogProbMetric: 30.1871 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 281/1000
2023-10-26 04:13:45.321 
Epoch 281/1000 
	 loss: 29.3649, MinusLogProbMetric: 29.3649, val_loss: 30.5674, val_MinusLogProbMetric: 30.5674

Epoch 281: val_loss did not improve from 29.17986
196/196 - 38s - loss: 29.3649 - MinusLogProbMetric: 29.3649 - val_loss: 30.5674 - val_MinusLogProbMetric: 30.5674 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 282/1000
2023-10-26 04:14:28.234 
Epoch 282/1000 
	 loss: 29.2881, MinusLogProbMetric: 29.2881, val_loss: 29.6397, val_MinusLogProbMetric: 29.6397

Epoch 282: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.2881 - MinusLogProbMetric: 29.2881 - val_loss: 29.6397 - val_MinusLogProbMetric: 29.6397 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 283/1000
2023-10-26 04:15:11.538 
Epoch 283/1000 
	 loss: 29.4183, MinusLogProbMetric: 29.4183, val_loss: 29.4634, val_MinusLogProbMetric: 29.4634

Epoch 283: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.4183 - MinusLogProbMetric: 29.4183 - val_loss: 29.4634 - val_MinusLogProbMetric: 29.4634 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 284/1000
2023-10-26 04:15:54.636 
Epoch 284/1000 
	 loss: 29.1825, MinusLogProbMetric: 29.1825, val_loss: 29.2520, val_MinusLogProbMetric: 29.2520

Epoch 284: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.1825 - MinusLogProbMetric: 29.1825 - val_loss: 29.2520 - val_MinusLogProbMetric: 29.2520 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 285/1000
2023-10-26 04:16:37.402 
Epoch 285/1000 
	 loss: 29.2563, MinusLogProbMetric: 29.2563, val_loss: 29.4139, val_MinusLogProbMetric: 29.4139

Epoch 285: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.2563 - MinusLogProbMetric: 29.2563 - val_loss: 29.4139 - val_MinusLogProbMetric: 29.4139 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 286/1000
2023-10-26 04:17:19.245 
Epoch 286/1000 
	 loss: 29.4704, MinusLogProbMetric: 29.4704, val_loss: 29.8788, val_MinusLogProbMetric: 29.8788

Epoch 286: val_loss did not improve from 29.17986
196/196 - 42s - loss: 29.4704 - MinusLogProbMetric: 29.4704 - val_loss: 29.8788 - val_MinusLogProbMetric: 29.8788 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 287/1000
2023-10-26 04:18:01.882 
Epoch 287/1000 
	 loss: 29.2867, MinusLogProbMetric: 29.2867, val_loss: 29.5643, val_MinusLogProbMetric: 29.5643

Epoch 287: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.2867 - MinusLogProbMetric: 29.2867 - val_loss: 29.5643 - val_MinusLogProbMetric: 29.5643 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 288/1000
2023-10-26 04:18:45.282 
Epoch 288/1000 
	 loss: 29.3410, MinusLogProbMetric: 29.3410, val_loss: 31.8294, val_MinusLogProbMetric: 31.8294

Epoch 288: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.3410 - MinusLogProbMetric: 29.3410 - val_loss: 31.8294 - val_MinusLogProbMetric: 31.8294 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 289/1000
2023-10-26 04:19:28.439 
Epoch 289/1000 
	 loss: 29.3039, MinusLogProbMetric: 29.3039, val_loss: 29.6592, val_MinusLogProbMetric: 29.6592

Epoch 289: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.3039 - MinusLogProbMetric: 29.3039 - val_loss: 29.6592 - val_MinusLogProbMetric: 29.6592 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 290/1000
2023-10-26 04:20:11.143 
Epoch 290/1000 
	 loss: 29.3905, MinusLogProbMetric: 29.3905, val_loss: 29.3670, val_MinusLogProbMetric: 29.3670

Epoch 290: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.3905 - MinusLogProbMetric: 29.3905 - val_loss: 29.3670 - val_MinusLogProbMetric: 29.3670 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 291/1000
2023-10-26 04:20:54.435 
Epoch 291/1000 
	 loss: 29.2857, MinusLogProbMetric: 29.2857, val_loss: 29.5568, val_MinusLogProbMetric: 29.5568

Epoch 291: val_loss did not improve from 29.17986
196/196 - 43s - loss: 29.2857 - MinusLogProbMetric: 29.2857 - val_loss: 29.5568 - val_MinusLogProbMetric: 29.5568 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 292/1000
2023-10-26 04:21:37.435 
Epoch 292/1000 
	 loss: 29.2907, MinusLogProbMetric: 29.2907, val_loss: 29.1521, val_MinusLogProbMetric: 29.1521

Epoch 292: val_loss improved from 29.17986 to 29.15213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 29.2907 - MinusLogProbMetric: 29.2907 - val_loss: 29.1521 - val_MinusLogProbMetric: 29.1521 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 293/1000
2023-10-26 04:22:21.164 
Epoch 293/1000 
	 loss: 29.2354, MinusLogProbMetric: 29.2354, val_loss: 29.4458, val_MinusLogProbMetric: 29.4458

Epoch 293: val_loss did not improve from 29.15213
196/196 - 43s - loss: 29.2354 - MinusLogProbMetric: 29.2354 - val_loss: 29.4458 - val_MinusLogProbMetric: 29.4458 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 294/1000
2023-10-26 04:23:02.593 
Epoch 294/1000 
	 loss: 29.2555, MinusLogProbMetric: 29.2555, val_loss: 29.6735, val_MinusLogProbMetric: 29.6735

Epoch 294: val_loss did not improve from 29.15213
196/196 - 41s - loss: 29.2555 - MinusLogProbMetric: 29.2555 - val_loss: 29.6735 - val_MinusLogProbMetric: 29.6735 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 295/1000
2023-10-26 04:23:36.604 
Epoch 295/1000 
	 loss: 29.4316, MinusLogProbMetric: 29.4316, val_loss: 33.5804, val_MinusLogProbMetric: 33.5804

Epoch 295: val_loss did not improve from 29.15213
196/196 - 34s - loss: 29.4316 - MinusLogProbMetric: 29.4316 - val_loss: 33.5804 - val_MinusLogProbMetric: 33.5804 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 296/1000
2023-10-26 04:24:12.981 
Epoch 296/1000 
	 loss: 29.3743, MinusLogProbMetric: 29.3743, val_loss: 29.5873, val_MinusLogProbMetric: 29.5873

Epoch 296: val_loss did not improve from 29.15213
196/196 - 36s - loss: 29.3743 - MinusLogProbMetric: 29.3743 - val_loss: 29.5873 - val_MinusLogProbMetric: 29.5873 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 297/1000
2023-10-26 04:24:54.559 
Epoch 297/1000 
	 loss: 29.2168, MinusLogProbMetric: 29.2168, val_loss: 29.6814, val_MinusLogProbMetric: 29.6814

Epoch 297: val_loss did not improve from 29.15213
196/196 - 42s - loss: 29.2168 - MinusLogProbMetric: 29.2168 - val_loss: 29.6814 - val_MinusLogProbMetric: 29.6814 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 298/1000
2023-10-26 04:25:32.980 
Epoch 298/1000 
	 loss: 29.3245, MinusLogProbMetric: 29.3245, val_loss: 29.5496, val_MinusLogProbMetric: 29.5496

Epoch 298: val_loss did not improve from 29.15213
196/196 - 38s - loss: 29.3245 - MinusLogProbMetric: 29.3245 - val_loss: 29.5496 - val_MinusLogProbMetric: 29.5496 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 299/1000
2023-10-26 04:26:06.951 
Epoch 299/1000 
	 loss: 29.2379, MinusLogProbMetric: 29.2379, val_loss: 29.5251, val_MinusLogProbMetric: 29.5251

Epoch 299: val_loss did not improve from 29.15213
196/196 - 34s - loss: 29.2379 - MinusLogProbMetric: 29.2379 - val_loss: 29.5251 - val_MinusLogProbMetric: 29.5251 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 300/1000
2023-10-26 04:26:41.604 
Epoch 300/1000 
	 loss: 29.3513, MinusLogProbMetric: 29.3513, val_loss: 30.2157, val_MinusLogProbMetric: 30.2157

Epoch 300: val_loss did not improve from 29.15213
196/196 - 35s - loss: 29.3513 - MinusLogProbMetric: 29.3513 - val_loss: 30.2157 - val_MinusLogProbMetric: 30.2157 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 301/1000
2023-10-26 04:27:22.324 
Epoch 301/1000 
	 loss: 29.2655, MinusLogProbMetric: 29.2655, val_loss: 29.8315, val_MinusLogProbMetric: 29.8315

Epoch 301: val_loss did not improve from 29.15213
196/196 - 41s - loss: 29.2655 - MinusLogProbMetric: 29.2655 - val_loss: 29.8315 - val_MinusLogProbMetric: 29.8315 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 302/1000
2023-10-26 04:28:00.240 
Epoch 302/1000 
	 loss: 29.3962, MinusLogProbMetric: 29.3962, val_loss: 29.4341, val_MinusLogProbMetric: 29.4341

Epoch 302: val_loss did not improve from 29.15213
196/196 - 38s - loss: 29.3962 - MinusLogProbMetric: 29.3962 - val_loss: 29.4341 - val_MinusLogProbMetric: 29.4341 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 303/1000
2023-10-26 04:28:34.120 
Epoch 303/1000 
	 loss: 29.2892, MinusLogProbMetric: 29.2892, val_loss: 31.9063, val_MinusLogProbMetric: 31.9063

Epoch 303: val_loss did not improve from 29.15213
196/196 - 34s - loss: 29.2892 - MinusLogProbMetric: 29.2892 - val_loss: 31.9063 - val_MinusLogProbMetric: 31.9063 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 304/1000
2023-10-26 04:29:08.148 
Epoch 304/1000 
	 loss: 29.2808, MinusLogProbMetric: 29.2808, val_loss: 29.1433, val_MinusLogProbMetric: 29.1433

Epoch 304: val_loss improved from 29.15213 to 29.14326, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 35s - loss: 29.2808 - MinusLogProbMetric: 29.2808 - val_loss: 29.1433 - val_MinusLogProbMetric: 29.1433 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 305/1000
2023-10-26 04:29:46.803 
Epoch 305/1000 
	 loss: 29.1522, MinusLogProbMetric: 29.1522, val_loss: 29.6723, val_MinusLogProbMetric: 29.6723

Epoch 305: val_loss did not improve from 29.14326
196/196 - 38s - loss: 29.1522 - MinusLogProbMetric: 29.1522 - val_loss: 29.6723 - val_MinusLogProbMetric: 29.6723 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 306/1000
2023-10-26 04:30:20.636 
Epoch 306/1000 
	 loss: 29.3182, MinusLogProbMetric: 29.3182, val_loss: 29.7267, val_MinusLogProbMetric: 29.7267

Epoch 306: val_loss did not improve from 29.14326
196/196 - 34s - loss: 29.3182 - MinusLogProbMetric: 29.3182 - val_loss: 29.7267 - val_MinusLogProbMetric: 29.7267 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 307/1000
2023-10-26 04:30:55.309 
Epoch 307/1000 
	 loss: 29.2010, MinusLogProbMetric: 29.2010, val_loss: 29.9415, val_MinusLogProbMetric: 29.9415

Epoch 307: val_loss did not improve from 29.14326
196/196 - 35s - loss: 29.2010 - MinusLogProbMetric: 29.2010 - val_loss: 29.9415 - val_MinusLogProbMetric: 29.9415 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 308/1000
2023-10-26 04:31:31.567 
Epoch 308/1000 
	 loss: 29.2923, MinusLogProbMetric: 29.2923, val_loss: 29.0311, val_MinusLogProbMetric: 29.0311

Epoch 308: val_loss improved from 29.14326 to 29.03106, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 37s - loss: 29.2923 - MinusLogProbMetric: 29.2923 - val_loss: 29.0311 - val_MinusLogProbMetric: 29.0311 - lr: 0.0010 - 37s/epoch - 188ms/step
Epoch 309/1000
2023-10-26 04:32:08.737 
Epoch 309/1000 
	 loss: 29.2647, MinusLogProbMetric: 29.2647, val_loss: 29.1026, val_MinusLogProbMetric: 29.1026

Epoch 309: val_loss did not improve from 29.03106
196/196 - 36s - loss: 29.2647 - MinusLogProbMetric: 29.2647 - val_loss: 29.1026 - val_MinusLogProbMetric: 29.1026 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 310/1000
2023-10-26 04:32:45.977 
Epoch 310/1000 
	 loss: 29.5853, MinusLogProbMetric: 29.5853, val_loss: 29.3123, val_MinusLogProbMetric: 29.3123

Epoch 310: val_loss did not improve from 29.03106
196/196 - 37s - loss: 29.5853 - MinusLogProbMetric: 29.5853 - val_loss: 29.3123 - val_MinusLogProbMetric: 29.3123 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 311/1000
2023-10-26 04:33:19.967 
Epoch 311/1000 
	 loss: 29.3043, MinusLogProbMetric: 29.3043, val_loss: 29.5736, val_MinusLogProbMetric: 29.5736

Epoch 311: val_loss did not improve from 29.03106
196/196 - 34s - loss: 29.3043 - MinusLogProbMetric: 29.3043 - val_loss: 29.5736 - val_MinusLogProbMetric: 29.5736 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 312/1000
2023-10-26 04:33:56.956 
Epoch 312/1000 
	 loss: 29.1812, MinusLogProbMetric: 29.1812, val_loss: 29.3495, val_MinusLogProbMetric: 29.3495

Epoch 312: val_loss did not improve from 29.03106
196/196 - 37s - loss: 29.1812 - MinusLogProbMetric: 29.1812 - val_loss: 29.3495 - val_MinusLogProbMetric: 29.3495 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 313/1000
2023-10-26 04:34:38.653 
Epoch 313/1000 
	 loss: 29.2698, MinusLogProbMetric: 29.2698, val_loss: 29.4380, val_MinusLogProbMetric: 29.4380

Epoch 313: val_loss did not improve from 29.03106
196/196 - 42s - loss: 29.2698 - MinusLogProbMetric: 29.2698 - val_loss: 29.4380 - val_MinusLogProbMetric: 29.4380 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 314/1000
2023-10-26 04:35:14.290 
Epoch 314/1000 
	 loss: 29.2200, MinusLogProbMetric: 29.2200, val_loss: 29.7690, val_MinusLogProbMetric: 29.7690

Epoch 314: val_loss did not improve from 29.03106
196/196 - 36s - loss: 29.2200 - MinusLogProbMetric: 29.2200 - val_loss: 29.7690 - val_MinusLogProbMetric: 29.7690 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 315/1000
2023-10-26 04:35:48.393 
Epoch 315/1000 
	 loss: 29.1999, MinusLogProbMetric: 29.1999, val_loss: 29.2314, val_MinusLogProbMetric: 29.2314

Epoch 315: val_loss did not improve from 29.03106
196/196 - 34s - loss: 29.1999 - MinusLogProbMetric: 29.1999 - val_loss: 29.2314 - val_MinusLogProbMetric: 29.2314 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 316/1000
2023-10-26 04:36:27.931 
Epoch 316/1000 
	 loss: 29.1788, MinusLogProbMetric: 29.1788, val_loss: 29.8841, val_MinusLogProbMetric: 29.8841

Epoch 316: val_loss did not improve from 29.03106
196/196 - 40s - loss: 29.1788 - MinusLogProbMetric: 29.1788 - val_loss: 29.8841 - val_MinusLogProbMetric: 29.8841 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 317/1000
2023-10-26 04:37:07.860 
Epoch 317/1000 
	 loss: 29.2139, MinusLogProbMetric: 29.2139, val_loss: 29.7392, val_MinusLogProbMetric: 29.7392

Epoch 317: val_loss did not improve from 29.03106
196/196 - 40s - loss: 29.2139 - MinusLogProbMetric: 29.2139 - val_loss: 29.7392 - val_MinusLogProbMetric: 29.7392 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 318/1000
2023-10-26 04:37:43.170 
Epoch 318/1000 
	 loss: 29.2605, MinusLogProbMetric: 29.2605, val_loss: 29.2357, val_MinusLogProbMetric: 29.2357

Epoch 318: val_loss did not improve from 29.03106
196/196 - 35s - loss: 29.2605 - MinusLogProbMetric: 29.2605 - val_loss: 29.2357 - val_MinusLogProbMetric: 29.2357 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 319/1000
2023-10-26 04:38:17.557 
Epoch 319/1000 
	 loss: 29.2570, MinusLogProbMetric: 29.2570, val_loss: 30.6419, val_MinusLogProbMetric: 30.6419

Epoch 319: val_loss did not improve from 29.03106
196/196 - 34s - loss: 29.2570 - MinusLogProbMetric: 29.2570 - val_loss: 30.6419 - val_MinusLogProbMetric: 30.6419 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 320/1000
2023-10-26 04:38:57.385 
Epoch 320/1000 
	 loss: 29.1730, MinusLogProbMetric: 29.1730, val_loss: 29.3923, val_MinusLogProbMetric: 29.3923

Epoch 320: val_loss did not improve from 29.03106
196/196 - 40s - loss: 29.1730 - MinusLogProbMetric: 29.1730 - val_loss: 29.3923 - val_MinusLogProbMetric: 29.3923 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 321/1000
2023-10-26 04:39:37.202 
Epoch 321/1000 
	 loss: 29.3392, MinusLogProbMetric: 29.3392, val_loss: 29.2433, val_MinusLogProbMetric: 29.2433

Epoch 321: val_loss did not improve from 29.03106
196/196 - 40s - loss: 29.3392 - MinusLogProbMetric: 29.3392 - val_loss: 29.2433 - val_MinusLogProbMetric: 29.2433 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 322/1000
2023-10-26 04:40:11.348 
Epoch 322/1000 
	 loss: 29.1531, MinusLogProbMetric: 29.1531, val_loss: 30.3618, val_MinusLogProbMetric: 30.3618

Epoch 322: val_loss did not improve from 29.03106
196/196 - 34s - loss: 29.1531 - MinusLogProbMetric: 29.1531 - val_loss: 30.3618 - val_MinusLogProbMetric: 30.3618 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 323/1000
2023-10-26 04:40:45.380 
Epoch 323/1000 
	 loss: 29.1640, MinusLogProbMetric: 29.1640, val_loss: 30.8376, val_MinusLogProbMetric: 30.8376

Epoch 323: val_loss did not improve from 29.03106
196/196 - 34s - loss: 29.1640 - MinusLogProbMetric: 29.1640 - val_loss: 30.8376 - val_MinusLogProbMetric: 30.8376 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 324/1000
2023-10-26 04:41:25.878 
Epoch 324/1000 
	 loss: 29.1630, MinusLogProbMetric: 29.1630, val_loss: 29.3788, val_MinusLogProbMetric: 29.3788

Epoch 324: val_loss did not improve from 29.03106
196/196 - 40s - loss: 29.1630 - MinusLogProbMetric: 29.1630 - val_loss: 29.3788 - val_MinusLogProbMetric: 29.3788 - lr: 0.0010 - 40s/epoch - 207ms/step
Epoch 325/1000
2023-10-26 04:42:06.545 
Epoch 325/1000 
	 loss: 29.0887, MinusLogProbMetric: 29.0887, val_loss: 29.2614, val_MinusLogProbMetric: 29.2614

Epoch 325: val_loss did not improve from 29.03106
196/196 - 41s - loss: 29.0887 - MinusLogProbMetric: 29.0887 - val_loss: 29.2614 - val_MinusLogProbMetric: 29.2614 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 326/1000
2023-10-26 04:42:41.162 
Epoch 326/1000 
	 loss: 29.2896, MinusLogProbMetric: 29.2896, val_loss: 29.2220, val_MinusLogProbMetric: 29.2220

Epoch 326: val_loss did not improve from 29.03106
196/196 - 35s - loss: 29.2896 - MinusLogProbMetric: 29.2896 - val_loss: 29.2220 - val_MinusLogProbMetric: 29.2220 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 327/1000
2023-10-26 04:43:15.673 
Epoch 327/1000 
	 loss: 29.1856, MinusLogProbMetric: 29.1856, val_loss: 29.0014, val_MinusLogProbMetric: 29.0014

Epoch 327: val_loss improved from 29.03106 to 29.00137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 35s - loss: 29.1856 - MinusLogProbMetric: 29.1856 - val_loss: 29.0014 - val_MinusLogProbMetric: 29.0014 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 328/1000
2023-10-26 04:43:55.999 
Epoch 328/1000 
	 loss: 29.2715, MinusLogProbMetric: 29.2715, val_loss: 29.1665, val_MinusLogProbMetric: 29.1665

Epoch 328: val_loss did not improve from 29.00137
196/196 - 40s - loss: 29.2715 - MinusLogProbMetric: 29.2715 - val_loss: 29.1665 - val_MinusLogProbMetric: 29.1665 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 329/1000
2023-10-26 04:44:37.417 
Epoch 329/1000 
	 loss: 29.1062, MinusLogProbMetric: 29.1062, val_loss: 30.2800, val_MinusLogProbMetric: 30.2800

Epoch 329: val_loss did not improve from 29.00137
196/196 - 41s - loss: 29.1062 - MinusLogProbMetric: 29.1062 - val_loss: 30.2800 - val_MinusLogProbMetric: 30.2800 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 330/1000
2023-10-26 04:45:11.824 
Epoch 330/1000 
	 loss: 29.2287, MinusLogProbMetric: 29.2287, val_loss: 29.4106, val_MinusLogProbMetric: 29.4106

Epoch 330: val_loss did not improve from 29.00137
196/196 - 34s - loss: 29.2287 - MinusLogProbMetric: 29.2287 - val_loss: 29.4106 - val_MinusLogProbMetric: 29.4106 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 331/1000
2023-10-26 04:45:46.172 
Epoch 331/1000 
	 loss: 29.2547, MinusLogProbMetric: 29.2547, val_loss: 29.3351, val_MinusLogProbMetric: 29.3351

Epoch 331: val_loss did not improve from 29.00137
196/196 - 34s - loss: 29.2547 - MinusLogProbMetric: 29.2547 - val_loss: 29.3351 - val_MinusLogProbMetric: 29.3351 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 332/1000
2023-10-26 04:46:23.734 
Epoch 332/1000 
	 loss: 29.1021, MinusLogProbMetric: 29.1021, val_loss: 29.5712, val_MinusLogProbMetric: 29.5712

Epoch 332: val_loss did not improve from 29.00137
196/196 - 38s - loss: 29.1021 - MinusLogProbMetric: 29.1021 - val_loss: 29.5712 - val_MinusLogProbMetric: 29.5712 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 333/1000
2023-10-26 04:47:00.911 
Epoch 333/1000 
	 loss: 29.2511, MinusLogProbMetric: 29.2511, val_loss: 29.9262, val_MinusLogProbMetric: 29.9262

Epoch 333: val_loss did not improve from 29.00137
196/196 - 37s - loss: 29.2511 - MinusLogProbMetric: 29.2511 - val_loss: 29.9262 - val_MinusLogProbMetric: 29.9262 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 334/1000
2023-10-26 04:47:35.213 
Epoch 334/1000 
	 loss: 29.1132, MinusLogProbMetric: 29.1132, val_loss: 29.2546, val_MinusLogProbMetric: 29.2546

Epoch 334: val_loss did not improve from 29.00137
196/196 - 34s - loss: 29.1132 - MinusLogProbMetric: 29.1132 - val_loss: 29.2546 - val_MinusLogProbMetric: 29.2546 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 335/1000
2023-10-26 04:48:09.569 
Epoch 335/1000 
	 loss: 29.0892, MinusLogProbMetric: 29.0892, val_loss: 30.5546, val_MinusLogProbMetric: 30.5546

Epoch 335: val_loss did not improve from 29.00137
196/196 - 34s - loss: 29.0892 - MinusLogProbMetric: 29.0892 - val_loss: 30.5546 - val_MinusLogProbMetric: 30.5546 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 336/1000
2023-10-26 04:48:49.524 
Epoch 336/1000 
	 loss: 29.1258, MinusLogProbMetric: 29.1258, val_loss: 29.5122, val_MinusLogProbMetric: 29.5122

Epoch 336: val_loss did not improve from 29.00137
196/196 - 40s - loss: 29.1258 - MinusLogProbMetric: 29.1258 - val_loss: 29.5122 - val_MinusLogProbMetric: 29.5122 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 337/1000
2023-10-26 04:49:26.171 
Epoch 337/1000 
	 loss: 29.2509, MinusLogProbMetric: 29.2509, val_loss: 29.6602, val_MinusLogProbMetric: 29.6602

Epoch 337: val_loss did not improve from 29.00137
196/196 - 37s - loss: 29.2509 - MinusLogProbMetric: 29.2509 - val_loss: 29.6602 - val_MinusLogProbMetric: 29.6602 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 338/1000
2023-10-26 04:50:01.548 
Epoch 338/1000 
	 loss: 29.1339, MinusLogProbMetric: 29.1339, val_loss: 29.5529, val_MinusLogProbMetric: 29.5529

Epoch 338: val_loss did not improve from 29.00137
196/196 - 35s - loss: 29.1339 - MinusLogProbMetric: 29.1339 - val_loss: 29.5529 - val_MinusLogProbMetric: 29.5529 - lr: 0.0010 - 35s/epoch - 180ms/step
Epoch 339/1000
2023-10-26 04:50:36.251 
Epoch 339/1000 
	 loss: 29.2503, MinusLogProbMetric: 29.2503, val_loss: 29.9916, val_MinusLogProbMetric: 29.9916

Epoch 339: val_loss did not improve from 29.00137
196/196 - 35s - loss: 29.2503 - MinusLogProbMetric: 29.2503 - val_loss: 29.9916 - val_MinusLogProbMetric: 29.9916 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 340/1000
2023-10-26 04:51:18.294 
Epoch 340/1000 
	 loss: 29.1211, MinusLogProbMetric: 29.1211, val_loss: 29.6715, val_MinusLogProbMetric: 29.6715

Epoch 340: val_loss did not improve from 29.00137
196/196 - 42s - loss: 29.1211 - MinusLogProbMetric: 29.1211 - val_loss: 29.6715 - val_MinusLogProbMetric: 29.6715 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 341/1000
2023-10-26 04:51:57.289 
Epoch 341/1000 
	 loss: 29.2132, MinusLogProbMetric: 29.2132, val_loss: 31.2627, val_MinusLogProbMetric: 31.2627

Epoch 341: val_loss did not improve from 29.00137
196/196 - 39s - loss: 29.2132 - MinusLogProbMetric: 29.2132 - val_loss: 31.2627 - val_MinusLogProbMetric: 31.2627 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 342/1000
2023-10-26 04:52:30.389 
Epoch 342/1000 
	 loss: 29.0954, MinusLogProbMetric: 29.0954, val_loss: 29.1041, val_MinusLogProbMetric: 29.1041

Epoch 342: val_loss did not improve from 29.00137
196/196 - 33s - loss: 29.0954 - MinusLogProbMetric: 29.0954 - val_loss: 29.1041 - val_MinusLogProbMetric: 29.1041 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 343/1000
2023-10-26 04:53:03.596 
Epoch 343/1000 
	 loss: 29.1113, MinusLogProbMetric: 29.1113, val_loss: 29.2903, val_MinusLogProbMetric: 29.2903

Epoch 343: val_loss did not improve from 29.00137
196/196 - 33s - loss: 29.1113 - MinusLogProbMetric: 29.1113 - val_loss: 29.2903 - val_MinusLogProbMetric: 29.2903 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 344/1000
2023-10-26 04:53:36.571 
Epoch 344/1000 
	 loss: 29.0842, MinusLogProbMetric: 29.0842, val_loss: 30.4028, val_MinusLogProbMetric: 30.4028

Epoch 344: val_loss did not improve from 29.00137
196/196 - 33s - loss: 29.0842 - MinusLogProbMetric: 29.0842 - val_loss: 30.4028 - val_MinusLogProbMetric: 30.4028 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 345/1000
2023-10-26 04:54:11.502 
Epoch 345/1000 
	 loss: 29.1776, MinusLogProbMetric: 29.1776, val_loss: 29.3741, val_MinusLogProbMetric: 29.3741

Epoch 345: val_loss did not improve from 29.00137
196/196 - 35s - loss: 29.1776 - MinusLogProbMetric: 29.1776 - val_loss: 29.3741 - val_MinusLogProbMetric: 29.3741 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 346/1000
2023-10-26 04:54:47.261 
Epoch 346/1000 
	 loss: 29.1013, MinusLogProbMetric: 29.1013, val_loss: 29.0760, val_MinusLogProbMetric: 29.0760

Epoch 346: val_loss did not improve from 29.00137
196/196 - 36s - loss: 29.1013 - MinusLogProbMetric: 29.1013 - val_loss: 29.0760 - val_MinusLogProbMetric: 29.0760 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 347/1000
2023-10-26 04:55:20.582 
Epoch 347/1000 
	 loss: 29.1022, MinusLogProbMetric: 29.1022, val_loss: 31.5621, val_MinusLogProbMetric: 31.5621

Epoch 347: val_loss did not improve from 29.00137
196/196 - 33s - loss: 29.1022 - MinusLogProbMetric: 29.1022 - val_loss: 31.5621 - val_MinusLogProbMetric: 31.5621 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 348/1000
2023-10-26 04:55:53.523 
Epoch 348/1000 
	 loss: 29.1695, MinusLogProbMetric: 29.1695, val_loss: 30.7403, val_MinusLogProbMetric: 30.7403

Epoch 348: val_loss did not improve from 29.00137
196/196 - 33s - loss: 29.1695 - MinusLogProbMetric: 29.1695 - val_loss: 30.7403 - val_MinusLogProbMetric: 30.7403 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 349/1000
2023-10-26 04:56:26.522 
Epoch 349/1000 
	 loss: 29.0597, MinusLogProbMetric: 29.0597, val_loss: 30.4839, val_MinusLogProbMetric: 30.4839

Epoch 349: val_loss did not improve from 29.00137
196/196 - 33s - loss: 29.0597 - MinusLogProbMetric: 29.0597 - val_loss: 30.4839 - val_MinusLogProbMetric: 30.4839 - lr: 0.0010 - 33s/epoch - 168ms/step
Epoch 350/1000
2023-10-26 04:57:05.439 
Epoch 350/1000 
	 loss: 29.0631, MinusLogProbMetric: 29.0631, val_loss: 29.0865, val_MinusLogProbMetric: 29.0865

Epoch 350: val_loss did not improve from 29.00137
196/196 - 39s - loss: 29.0631 - MinusLogProbMetric: 29.0631 - val_loss: 29.0865 - val_MinusLogProbMetric: 29.0865 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 351/1000
2023-10-26 04:57:43.704 
Epoch 351/1000 
	 loss: 29.1126, MinusLogProbMetric: 29.1126, val_loss: 29.2146, val_MinusLogProbMetric: 29.2146

Epoch 351: val_loss did not improve from 29.00137
196/196 - 38s - loss: 29.1126 - MinusLogProbMetric: 29.1126 - val_loss: 29.2146 - val_MinusLogProbMetric: 29.2146 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 352/1000
2023-10-26 04:58:23.580 
Epoch 352/1000 
	 loss: 29.2166, MinusLogProbMetric: 29.2166, val_loss: 29.1688, val_MinusLogProbMetric: 29.1688

Epoch 352: val_loss did not improve from 29.00137
196/196 - 40s - loss: 29.2166 - MinusLogProbMetric: 29.2166 - val_loss: 29.1688 - val_MinusLogProbMetric: 29.1688 - lr: 0.0010 - 40s/epoch - 203ms/step
Epoch 353/1000
2023-10-26 04:59:00.317 
Epoch 353/1000 
	 loss: 29.1187, MinusLogProbMetric: 29.1187, val_loss: 29.6457, val_MinusLogProbMetric: 29.6457

Epoch 353: val_loss did not improve from 29.00137
196/196 - 37s - loss: 29.1187 - MinusLogProbMetric: 29.1187 - val_loss: 29.6457 - val_MinusLogProbMetric: 29.6457 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 354/1000
2023-10-26 04:59:36.959 
Epoch 354/1000 
	 loss: 29.0751, MinusLogProbMetric: 29.0751, val_loss: 29.6746, val_MinusLogProbMetric: 29.6746

Epoch 354: val_loss did not improve from 29.00137
196/196 - 37s - loss: 29.0751 - MinusLogProbMetric: 29.0751 - val_loss: 29.6746 - val_MinusLogProbMetric: 29.6746 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 355/1000
2023-10-26 05:00:11.320 
Epoch 355/1000 
	 loss: 29.0809, MinusLogProbMetric: 29.0809, val_loss: 29.1115, val_MinusLogProbMetric: 29.1115

Epoch 355: val_loss did not improve from 29.00137
196/196 - 34s - loss: 29.0809 - MinusLogProbMetric: 29.0809 - val_loss: 29.1115 - val_MinusLogProbMetric: 29.1115 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 356/1000
2023-10-26 05:00:45.157 
Epoch 356/1000 
	 loss: 29.0576, MinusLogProbMetric: 29.0576, val_loss: 29.0667, val_MinusLogProbMetric: 29.0667

Epoch 356: val_loss did not improve from 29.00137
196/196 - 34s - loss: 29.0576 - MinusLogProbMetric: 29.0576 - val_loss: 29.0667 - val_MinusLogProbMetric: 29.0667 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 357/1000
2023-10-26 05:01:24.666 
Epoch 357/1000 
	 loss: 29.0338, MinusLogProbMetric: 29.0338, val_loss: 29.4380, val_MinusLogProbMetric: 29.4380

Epoch 357: val_loss did not improve from 29.00137
196/196 - 40s - loss: 29.0338 - MinusLogProbMetric: 29.0338 - val_loss: 29.4380 - val_MinusLogProbMetric: 29.4380 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 358/1000
2023-10-26 05:02:00.186 
Epoch 358/1000 
	 loss: 29.0481, MinusLogProbMetric: 29.0481, val_loss: 29.1849, val_MinusLogProbMetric: 29.1849

Epoch 358: val_loss did not improve from 29.00137
196/196 - 36s - loss: 29.0481 - MinusLogProbMetric: 29.0481 - val_loss: 29.1849 - val_MinusLogProbMetric: 29.1849 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 359/1000
2023-10-26 05:02:33.974 
Epoch 359/1000 
	 loss: 28.9731, MinusLogProbMetric: 28.9731, val_loss: 29.7601, val_MinusLogProbMetric: 29.7601

Epoch 359: val_loss did not improve from 29.00137
196/196 - 34s - loss: 28.9731 - MinusLogProbMetric: 28.9731 - val_loss: 29.7601 - val_MinusLogProbMetric: 29.7601 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 360/1000
2023-10-26 05:03:07.289 
Epoch 360/1000 
	 loss: 29.0197, MinusLogProbMetric: 29.0197, val_loss: 29.9250, val_MinusLogProbMetric: 29.9250

Epoch 360: val_loss did not improve from 29.00137
196/196 - 33s - loss: 29.0197 - MinusLogProbMetric: 29.0197 - val_loss: 29.9250 - val_MinusLogProbMetric: 29.9250 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 361/1000
2023-10-26 05:03:41.314 
Epoch 361/1000 
	 loss: 29.1169, MinusLogProbMetric: 29.1169, val_loss: 29.3764, val_MinusLogProbMetric: 29.3764

Epoch 361: val_loss did not improve from 29.00137
196/196 - 34s - loss: 29.1169 - MinusLogProbMetric: 29.1169 - val_loss: 29.3764 - val_MinusLogProbMetric: 29.3764 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 362/1000
2023-10-26 05:04:21.405 
Epoch 362/1000 
	 loss: 29.1165, MinusLogProbMetric: 29.1165, val_loss: 30.1067, val_MinusLogProbMetric: 30.1067

Epoch 362: val_loss did not improve from 29.00137
196/196 - 40s - loss: 29.1165 - MinusLogProbMetric: 29.1165 - val_loss: 30.1067 - val_MinusLogProbMetric: 30.1067 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 363/1000
2023-10-26 05:04:59.102 
Epoch 363/1000 
	 loss: 29.0655, MinusLogProbMetric: 29.0655, val_loss: 29.1872, val_MinusLogProbMetric: 29.1872

Epoch 363: val_loss did not improve from 29.00137
196/196 - 38s - loss: 29.0655 - MinusLogProbMetric: 29.0655 - val_loss: 29.1872 - val_MinusLogProbMetric: 29.1872 - lr: 0.0010 - 38s/epoch - 192ms/step
Epoch 364/1000
2023-10-26 05:05:35.239 
Epoch 364/1000 
	 loss: 29.0676, MinusLogProbMetric: 29.0676, val_loss: 29.5907, val_MinusLogProbMetric: 29.5907

Epoch 364: val_loss did not improve from 29.00137
196/196 - 36s - loss: 29.0676 - MinusLogProbMetric: 29.0676 - val_loss: 29.5907 - val_MinusLogProbMetric: 29.5907 - lr: 0.0010 - 36s/epoch - 184ms/step
Epoch 365/1000
2023-10-26 05:06:08.630 
Epoch 365/1000 
	 loss: 28.9905, MinusLogProbMetric: 28.9905, val_loss: 29.3310, val_MinusLogProbMetric: 29.3310

Epoch 365: val_loss did not improve from 29.00137
196/196 - 33s - loss: 28.9905 - MinusLogProbMetric: 28.9905 - val_loss: 29.3310 - val_MinusLogProbMetric: 29.3310 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 366/1000
2023-10-26 05:06:42.115 
Epoch 366/1000 
	 loss: 28.9622, MinusLogProbMetric: 28.9622, val_loss: 29.3309, val_MinusLogProbMetric: 29.3309

Epoch 366: val_loss did not improve from 29.00137
196/196 - 33s - loss: 28.9622 - MinusLogProbMetric: 28.9622 - val_loss: 29.3309 - val_MinusLogProbMetric: 29.3309 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 367/1000
2023-10-26 05:07:20.223 
Epoch 367/1000 
	 loss: 29.2552, MinusLogProbMetric: 29.2552, val_loss: 31.4499, val_MinusLogProbMetric: 31.4499

Epoch 367: val_loss did not improve from 29.00137
196/196 - 38s - loss: 29.2552 - MinusLogProbMetric: 29.2552 - val_loss: 31.4499 - val_MinusLogProbMetric: 31.4499 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 368/1000
2023-10-26 05:07:58.605 
Epoch 368/1000 
	 loss: 29.0828, MinusLogProbMetric: 29.0828, val_loss: 29.0762, val_MinusLogProbMetric: 29.0762

Epoch 368: val_loss did not improve from 29.00137
196/196 - 38s - loss: 29.0828 - MinusLogProbMetric: 29.0828 - val_loss: 29.0762 - val_MinusLogProbMetric: 29.0762 - lr: 0.0010 - 38s/epoch - 196ms/step
Epoch 369/1000
2023-10-26 05:08:35.189 
Epoch 369/1000 
	 loss: 29.0674, MinusLogProbMetric: 29.0674, val_loss: 29.6812, val_MinusLogProbMetric: 29.6812

Epoch 369: val_loss did not improve from 29.00137
196/196 - 37s - loss: 29.0674 - MinusLogProbMetric: 29.0674 - val_loss: 29.6812 - val_MinusLogProbMetric: 29.6812 - lr: 0.0010 - 37s/epoch - 187ms/step
Epoch 370/1000
2023-10-26 05:09:08.923 
Epoch 370/1000 
	 loss: 29.0550, MinusLogProbMetric: 29.0550, val_loss: 28.9330, val_MinusLogProbMetric: 28.9330

Epoch 370: val_loss improved from 29.00137 to 28.93299, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 34s - loss: 29.0550 - MinusLogProbMetric: 29.0550 - val_loss: 28.9330 - val_MinusLogProbMetric: 28.9330 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 371/1000
2023-10-26 05:09:43.192 
Epoch 371/1000 
	 loss: 28.9220, MinusLogProbMetric: 28.9220, val_loss: 29.1932, val_MinusLogProbMetric: 29.1932

Epoch 371: val_loss did not improve from 28.93299
196/196 - 34s - loss: 28.9220 - MinusLogProbMetric: 28.9220 - val_loss: 29.1932 - val_MinusLogProbMetric: 29.1932 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 372/1000
2023-10-26 05:10:16.290 
Epoch 372/1000 
	 loss: 29.0301, MinusLogProbMetric: 29.0301, val_loss: 29.0840, val_MinusLogProbMetric: 29.0840

Epoch 372: val_loss did not improve from 28.93299
196/196 - 33s - loss: 29.0301 - MinusLogProbMetric: 29.0301 - val_loss: 29.0840 - val_MinusLogProbMetric: 29.0840 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 373/1000
2023-10-26 05:10:56.779 
Epoch 373/1000 
	 loss: 29.0558, MinusLogProbMetric: 29.0558, val_loss: 29.4788, val_MinusLogProbMetric: 29.4788

Epoch 373: val_loss did not improve from 28.93299
196/196 - 40s - loss: 29.0558 - MinusLogProbMetric: 29.0558 - val_loss: 29.4788 - val_MinusLogProbMetric: 29.4788 - lr: 0.0010 - 40s/epoch - 207ms/step
Epoch 374/1000
2023-10-26 05:11:33.315 
Epoch 374/1000 
	 loss: 29.0166, MinusLogProbMetric: 29.0166, val_loss: 29.6175, val_MinusLogProbMetric: 29.6175

Epoch 374: val_loss did not improve from 28.93299
196/196 - 37s - loss: 29.0166 - MinusLogProbMetric: 29.0166 - val_loss: 29.6175 - val_MinusLogProbMetric: 29.6175 - lr: 0.0010 - 37s/epoch - 186ms/step
Epoch 375/1000
2023-10-26 05:12:07.019 
Epoch 375/1000 
	 loss: 29.1094, MinusLogProbMetric: 29.1094, val_loss: 29.0875, val_MinusLogProbMetric: 29.0875

Epoch 375: val_loss did not improve from 28.93299
196/196 - 34s - loss: 29.1094 - MinusLogProbMetric: 29.1094 - val_loss: 29.0875 - val_MinusLogProbMetric: 29.0875 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 376/1000
2023-10-26 05:12:40.941 
Epoch 376/1000 
	 loss: 29.0988, MinusLogProbMetric: 29.0988, val_loss: 29.0346, val_MinusLogProbMetric: 29.0346

Epoch 376: val_loss did not improve from 28.93299
196/196 - 34s - loss: 29.0988 - MinusLogProbMetric: 29.0988 - val_loss: 29.0346 - val_MinusLogProbMetric: 29.0346 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 377/1000
2023-10-26 05:13:14.611 
Epoch 377/1000 
	 loss: 29.0322, MinusLogProbMetric: 29.0322, val_loss: 29.6455, val_MinusLogProbMetric: 29.6455

Epoch 377: val_loss did not improve from 28.93299
196/196 - 34s - loss: 29.0322 - MinusLogProbMetric: 29.0322 - val_loss: 29.6455 - val_MinusLogProbMetric: 29.6455 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 378/1000
2023-10-26 05:13:53.340 
Epoch 378/1000 
	 loss: 29.0199, MinusLogProbMetric: 29.0199, val_loss: 29.6159, val_MinusLogProbMetric: 29.6159

Epoch 378: val_loss did not improve from 28.93299
196/196 - 39s - loss: 29.0199 - MinusLogProbMetric: 29.0199 - val_loss: 29.6159 - val_MinusLogProbMetric: 29.6159 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 379/1000
2023-10-26 05:14:34.129 
Epoch 379/1000 
	 loss: 28.9877, MinusLogProbMetric: 28.9877, val_loss: 29.4876, val_MinusLogProbMetric: 29.4876

Epoch 379: val_loss did not improve from 28.93299
196/196 - 41s - loss: 28.9877 - MinusLogProbMetric: 28.9877 - val_loss: 29.4876 - val_MinusLogProbMetric: 29.4876 - lr: 0.0010 - 41s/epoch - 208ms/step
Epoch 380/1000
2023-10-26 05:15:11.171 
Epoch 380/1000 
	 loss: 28.9655, MinusLogProbMetric: 28.9655, val_loss: 29.0827, val_MinusLogProbMetric: 29.0827

Epoch 380: val_loss did not improve from 28.93299
196/196 - 37s - loss: 28.9655 - MinusLogProbMetric: 28.9655 - val_loss: 29.0827 - val_MinusLogProbMetric: 29.0827 - lr: 0.0010 - 37s/epoch - 189ms/step
Epoch 381/1000
2023-10-26 05:15:44.860 
Epoch 381/1000 
	 loss: 28.9051, MinusLogProbMetric: 28.9051, val_loss: 29.2399, val_MinusLogProbMetric: 29.2399

Epoch 381: val_loss did not improve from 28.93299
196/196 - 34s - loss: 28.9051 - MinusLogProbMetric: 28.9051 - val_loss: 29.2399 - val_MinusLogProbMetric: 29.2399 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 382/1000
2023-10-26 05:16:17.572 
Epoch 382/1000 
	 loss: 29.0565, MinusLogProbMetric: 29.0565, val_loss: 29.3402, val_MinusLogProbMetric: 29.3402

Epoch 382: val_loss did not improve from 28.93299
196/196 - 33s - loss: 29.0565 - MinusLogProbMetric: 29.0565 - val_loss: 29.3402 - val_MinusLogProbMetric: 29.3402 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 383/1000
2023-10-26 05:16:50.728 
Epoch 383/1000 
	 loss: 28.9366, MinusLogProbMetric: 28.9366, val_loss: 29.0043, val_MinusLogProbMetric: 29.0043

Epoch 383: val_loss did not improve from 28.93299
196/196 - 33s - loss: 28.9366 - MinusLogProbMetric: 28.9366 - val_loss: 29.0043 - val_MinusLogProbMetric: 29.0043 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 384/1000
2023-10-26 05:17:32.068 
Epoch 384/1000 
	 loss: 28.9913, MinusLogProbMetric: 28.9913, val_loss: 29.0428, val_MinusLogProbMetric: 29.0428

Epoch 384: val_loss did not improve from 28.93299
196/196 - 41s - loss: 28.9913 - MinusLogProbMetric: 28.9913 - val_loss: 29.0428 - val_MinusLogProbMetric: 29.0428 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 385/1000
2023-10-26 05:18:10.070 
Epoch 385/1000 
	 loss: 29.2871, MinusLogProbMetric: 29.2871, val_loss: 30.5814, val_MinusLogProbMetric: 30.5814

Epoch 385: val_loss did not improve from 28.93299
196/196 - 38s - loss: 29.2871 - MinusLogProbMetric: 29.2871 - val_loss: 30.5814 - val_MinusLogProbMetric: 30.5814 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 386/1000
2023-10-26 05:18:42.832 
Epoch 386/1000 
	 loss: 28.9769, MinusLogProbMetric: 28.9769, val_loss: 31.6128, val_MinusLogProbMetric: 31.6128

Epoch 386: val_loss did not improve from 28.93299
196/196 - 33s - loss: 28.9769 - MinusLogProbMetric: 28.9769 - val_loss: 31.6128 - val_MinusLogProbMetric: 31.6128 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 387/1000
2023-10-26 05:19:15.507 
Epoch 387/1000 
	 loss: 28.9780, MinusLogProbMetric: 28.9780, val_loss: 29.4535, val_MinusLogProbMetric: 29.4535

Epoch 387: val_loss did not improve from 28.93299
196/196 - 33s - loss: 28.9780 - MinusLogProbMetric: 28.9780 - val_loss: 29.4535 - val_MinusLogProbMetric: 29.4535 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 388/1000
2023-10-26 05:19:50.060 
Epoch 388/1000 
	 loss: 29.1078, MinusLogProbMetric: 29.1078, val_loss: 29.2574, val_MinusLogProbMetric: 29.2574

Epoch 388: val_loss did not improve from 28.93299
196/196 - 35s - loss: 29.1078 - MinusLogProbMetric: 29.1078 - val_loss: 29.2574 - val_MinusLogProbMetric: 29.2574 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 389/1000
2023-10-26 05:20:26.389 
Epoch 389/1000 
	 loss: 28.9895, MinusLogProbMetric: 28.9895, val_loss: 30.0858, val_MinusLogProbMetric: 30.0858

Epoch 389: val_loss did not improve from 28.93299
196/196 - 36s - loss: 28.9895 - MinusLogProbMetric: 28.9895 - val_loss: 30.0858 - val_MinusLogProbMetric: 30.0858 - lr: 0.0010 - 36s/epoch - 185ms/step
Epoch 390/1000
2023-10-26 05:21:05.235 
Epoch 390/1000 
	 loss: 28.9678, MinusLogProbMetric: 28.9678, val_loss: 28.9087, val_MinusLogProbMetric: 28.9087

Epoch 390: val_loss improved from 28.93299 to 28.90873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 40s - loss: 28.9678 - MinusLogProbMetric: 28.9678 - val_loss: 28.9087 - val_MinusLogProbMetric: 28.9087 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 391/1000
2023-10-26 05:21:44.510 
Epoch 391/1000 
	 loss: 28.9797, MinusLogProbMetric: 28.9797, val_loss: 30.0894, val_MinusLogProbMetric: 30.0894

Epoch 391: val_loss did not improve from 28.90873
196/196 - 39s - loss: 28.9797 - MinusLogProbMetric: 28.9797 - val_loss: 30.0894 - val_MinusLogProbMetric: 30.0894 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 392/1000
2023-10-26 05:22:17.739 
Epoch 392/1000 
	 loss: 29.1401, MinusLogProbMetric: 29.1401, val_loss: 29.7667, val_MinusLogProbMetric: 29.7667

Epoch 392: val_loss did not improve from 28.90873
196/196 - 33s - loss: 29.1401 - MinusLogProbMetric: 29.1401 - val_loss: 29.7667 - val_MinusLogProbMetric: 29.7667 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 393/1000
2023-10-26 05:22:51.793 
Epoch 393/1000 
	 loss: 28.9533, MinusLogProbMetric: 28.9533, val_loss: 29.2462, val_MinusLogProbMetric: 29.2462

Epoch 393: val_loss did not improve from 28.90873
196/196 - 34s - loss: 28.9533 - MinusLogProbMetric: 28.9533 - val_loss: 29.2462 - val_MinusLogProbMetric: 29.2462 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 394/1000
2023-10-26 05:23:24.859 
Epoch 394/1000 
	 loss: 29.0231, MinusLogProbMetric: 29.0231, val_loss: 29.2699, val_MinusLogProbMetric: 29.2699

Epoch 394: val_loss did not improve from 28.90873
196/196 - 33s - loss: 29.0231 - MinusLogProbMetric: 29.0231 - val_loss: 29.2699 - val_MinusLogProbMetric: 29.2699 - lr: 0.0010 - 33s/epoch - 169ms/step
Epoch 395/1000
2023-10-26 05:24:03.173 
Epoch 395/1000 
	 loss: 29.0860, MinusLogProbMetric: 29.0860, val_loss: 28.8950, val_MinusLogProbMetric: 28.8950

Epoch 395: val_loss improved from 28.90873 to 28.89497, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 39s - loss: 29.0860 - MinusLogProbMetric: 29.0860 - val_loss: 28.8950 - val_MinusLogProbMetric: 28.8950 - lr: 0.0010 - 39s/epoch - 199ms/step
Epoch 396/1000
2023-10-26 05:24:42.445 
Epoch 396/1000 
	 loss: 28.9056, MinusLogProbMetric: 28.9056, val_loss: 29.1048, val_MinusLogProbMetric: 29.1048

Epoch 396: val_loss did not improve from 28.89497
196/196 - 39s - loss: 28.9056 - MinusLogProbMetric: 28.9056 - val_loss: 29.1048 - val_MinusLogProbMetric: 29.1048 - lr: 0.0010 - 39s/epoch - 197ms/step
Epoch 397/1000
2023-10-26 05:25:18.035 
Epoch 397/1000 
	 loss: 29.0998, MinusLogProbMetric: 29.0998, val_loss: 29.1989, val_MinusLogProbMetric: 29.1989

Epoch 397: val_loss did not improve from 28.89497
196/196 - 36s - loss: 29.0998 - MinusLogProbMetric: 29.0998 - val_loss: 29.1989 - val_MinusLogProbMetric: 29.1989 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 398/1000
2023-10-26 05:25:51.628 
Epoch 398/1000 
	 loss: 28.8828, MinusLogProbMetric: 28.8828, val_loss: 29.0932, val_MinusLogProbMetric: 29.0932

Epoch 398: val_loss did not improve from 28.89497
196/196 - 34s - loss: 28.8828 - MinusLogProbMetric: 28.8828 - val_loss: 29.0932 - val_MinusLogProbMetric: 29.0932 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 399/1000
2023-10-26 05:26:25.853 
Epoch 399/1000 
	 loss: 28.9767, MinusLogProbMetric: 28.9767, val_loss: 29.8798, val_MinusLogProbMetric: 29.8798

Epoch 399: val_loss did not improve from 28.89497
196/196 - 34s - loss: 28.9767 - MinusLogProbMetric: 28.9767 - val_loss: 29.8798 - val_MinusLogProbMetric: 29.8798 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 400/1000
2023-10-26 05:26:58.426 
Epoch 400/1000 
	 loss: 29.1151, MinusLogProbMetric: 29.1151, val_loss: 29.1331, val_MinusLogProbMetric: 29.1331

Epoch 400: val_loss did not improve from 28.89497
196/196 - 33s - loss: 29.1151 - MinusLogProbMetric: 29.1151 - val_loss: 29.1331 - val_MinusLogProbMetric: 29.1331 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 401/1000
2023-10-26 05:27:32.790 
Epoch 401/1000 
	 loss: 28.9695, MinusLogProbMetric: 28.9695, val_loss: 28.8532, val_MinusLogProbMetric: 28.8532

Epoch 401: val_loss improved from 28.89497 to 28.85317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 35s - loss: 28.9695 - MinusLogProbMetric: 28.9695 - val_loss: 28.8532 - val_MinusLogProbMetric: 28.8532 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 402/1000
2023-10-26 05:28:06.676 
Epoch 402/1000 
	 loss: 28.9085, MinusLogProbMetric: 28.9085, val_loss: 30.2747, val_MinusLogProbMetric: 30.2747

Epoch 402: val_loss did not improve from 28.85317
196/196 - 33s - loss: 28.9085 - MinusLogProbMetric: 28.9085 - val_loss: 30.2747 - val_MinusLogProbMetric: 30.2747 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 403/1000
2023-10-26 05:28:43.907 
Epoch 403/1000 
	 loss: 29.0211, MinusLogProbMetric: 29.0211, val_loss: 29.3126, val_MinusLogProbMetric: 29.3126

Epoch 403: val_loss did not improve from 28.85317
196/196 - 37s - loss: 29.0211 - MinusLogProbMetric: 29.0211 - val_loss: 29.3126 - val_MinusLogProbMetric: 29.3126 - lr: 0.0010 - 37s/epoch - 190ms/step
Epoch 404/1000
2023-10-26 05:29:20.275 
Epoch 404/1000 
	 loss: 28.9885, MinusLogProbMetric: 28.9885, val_loss: 30.2336, val_MinusLogProbMetric: 30.2336

Epoch 404: val_loss did not improve from 28.85317
196/196 - 36s - loss: 28.9885 - MinusLogProbMetric: 28.9885 - val_loss: 30.2336 - val_MinusLogProbMetric: 30.2336 - lr: 0.0010 - 36s/epoch - 186ms/step
Epoch 405/1000
2023-10-26 05:29:53.856 
Epoch 405/1000 
	 loss: 28.9723, MinusLogProbMetric: 28.9723, val_loss: 29.3612, val_MinusLogProbMetric: 29.3612

Epoch 405: val_loss did not improve from 28.85317
196/196 - 34s - loss: 28.9723 - MinusLogProbMetric: 28.9723 - val_loss: 29.3612 - val_MinusLogProbMetric: 29.3612 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 406/1000
2023-10-26 05:30:27.593 
Epoch 406/1000 
	 loss: 28.8684, MinusLogProbMetric: 28.8684, val_loss: 29.3363, val_MinusLogProbMetric: 29.3363

Epoch 406: val_loss did not improve from 28.85317
196/196 - 34s - loss: 28.8684 - MinusLogProbMetric: 28.8684 - val_loss: 29.3363 - val_MinusLogProbMetric: 29.3363 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 407/1000
2023-10-26 05:31:01.065 
Epoch 407/1000 
	 loss: 28.8981, MinusLogProbMetric: 28.8981, val_loss: 29.3092, val_MinusLogProbMetric: 29.3092

Epoch 407: val_loss did not improve from 28.85317
196/196 - 33s - loss: 28.8981 - MinusLogProbMetric: 28.8981 - val_loss: 29.3092 - val_MinusLogProbMetric: 29.3092 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 408/1000
2023-10-26 05:31:39.153 
Epoch 408/1000 
	 loss: 28.8904, MinusLogProbMetric: 28.8904, val_loss: 29.1518, val_MinusLogProbMetric: 29.1518

Epoch 408: val_loss did not improve from 28.85317
196/196 - 38s - loss: 28.8904 - MinusLogProbMetric: 28.8904 - val_loss: 29.1518 - val_MinusLogProbMetric: 29.1518 - lr: 0.0010 - 38s/epoch - 194ms/step
Epoch 409/1000
2023-10-26 05:32:13.830 
Epoch 409/1000 
	 loss: 29.0156, MinusLogProbMetric: 29.0156, val_loss: 28.9193, val_MinusLogProbMetric: 28.9193

Epoch 409: val_loss did not improve from 28.85317
196/196 - 35s - loss: 29.0156 - MinusLogProbMetric: 29.0156 - val_loss: 28.9193 - val_MinusLogProbMetric: 28.9193 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 410/1000
2023-10-26 05:32:52.584 
Epoch 410/1000 
	 loss: 28.9708, MinusLogProbMetric: 28.9708, val_loss: 29.5748, val_MinusLogProbMetric: 29.5748

Epoch 410: val_loss did not improve from 28.85317
196/196 - 39s - loss: 28.9708 - MinusLogProbMetric: 28.9708 - val_loss: 29.5748 - val_MinusLogProbMetric: 29.5748 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 411/1000
2023-10-26 05:33:26.030 
Epoch 411/1000 
	 loss: 28.9430, MinusLogProbMetric: 28.9430, val_loss: 29.0252, val_MinusLogProbMetric: 29.0252

Epoch 411: val_loss did not improve from 28.85317
196/196 - 33s - loss: 28.9430 - MinusLogProbMetric: 28.9430 - val_loss: 29.0252 - val_MinusLogProbMetric: 29.0252 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 412/1000
2023-10-26 05:34:00.011 
Epoch 412/1000 
	 loss: 29.2273, MinusLogProbMetric: 29.2273, val_loss: 29.4681, val_MinusLogProbMetric: 29.4681

Epoch 412: val_loss did not improve from 28.85317
196/196 - 34s - loss: 29.2273 - MinusLogProbMetric: 29.2273 - val_loss: 29.4681 - val_MinusLogProbMetric: 29.4681 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 413/1000
2023-10-26 05:34:32.673 
Epoch 413/1000 
	 loss: 28.9211, MinusLogProbMetric: 28.9211, val_loss: 29.9928, val_MinusLogProbMetric: 29.9928

Epoch 413: val_loss did not improve from 28.85317
196/196 - 33s - loss: 28.9211 - MinusLogProbMetric: 28.9211 - val_loss: 29.9928 - val_MinusLogProbMetric: 29.9928 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 414/1000
2023-10-26 05:35:07.676 
Epoch 414/1000 
	 loss: 28.8408, MinusLogProbMetric: 28.8408, val_loss: 29.6800, val_MinusLogProbMetric: 29.6800

Epoch 414: val_loss did not improve from 28.85317
196/196 - 35s - loss: 28.8408 - MinusLogProbMetric: 28.8408 - val_loss: 29.6800 - val_MinusLogProbMetric: 29.6800 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 415/1000
2023-10-26 05:35:42.136 
Epoch 415/1000 
	 loss: 28.9759, MinusLogProbMetric: 28.9759, val_loss: 28.8976, val_MinusLogProbMetric: 28.8976

Epoch 415: val_loss did not improve from 28.85317
196/196 - 34s - loss: 28.9759 - MinusLogProbMetric: 28.9759 - val_loss: 28.8976 - val_MinusLogProbMetric: 28.8976 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 416/1000
2023-10-26 05:36:21.044 
Epoch 416/1000 
	 loss: 28.9658, MinusLogProbMetric: 28.9658, val_loss: 29.9617, val_MinusLogProbMetric: 29.9617

Epoch 416: val_loss did not improve from 28.85317
196/196 - 39s - loss: 28.9658 - MinusLogProbMetric: 28.9658 - val_loss: 29.9617 - val_MinusLogProbMetric: 29.9617 - lr: 0.0010 - 39s/epoch - 198ms/step
Epoch 417/1000
2023-10-26 05:36:56.573 
Epoch 417/1000 
	 loss: 29.0287, MinusLogProbMetric: 29.0287, val_loss: 29.1682, val_MinusLogProbMetric: 29.1682

Epoch 417: val_loss did not improve from 28.85317
196/196 - 36s - loss: 29.0287 - MinusLogProbMetric: 29.0287 - val_loss: 29.1682 - val_MinusLogProbMetric: 29.1682 - lr: 0.0010 - 36s/epoch - 181ms/step
Epoch 418/1000
2023-10-26 05:37:33.947 
Epoch 418/1000 
	 loss: 29.0707, MinusLogProbMetric: 29.0707, val_loss: 29.2292, val_MinusLogProbMetric: 29.2292

Epoch 418: val_loss did not improve from 28.85317
196/196 - 37s - loss: 29.0707 - MinusLogProbMetric: 29.0707 - val_loss: 29.2292 - val_MinusLogProbMetric: 29.2292 - lr: 0.0010 - 37s/epoch - 191ms/step
Epoch 419/1000
2023-10-26 05:38:07.439 
Epoch 419/1000 
	 loss: 28.8172, MinusLogProbMetric: 28.8172, val_loss: 29.7492, val_MinusLogProbMetric: 29.7492

Epoch 419: val_loss did not improve from 28.85317
196/196 - 33s - loss: 28.8172 - MinusLogProbMetric: 28.8172 - val_loss: 29.7492 - val_MinusLogProbMetric: 29.7492 - lr: 0.0010 - 33s/epoch - 171ms/step
Epoch 420/1000
2023-10-26 05:38:40.848 
Epoch 420/1000 
	 loss: 28.9684, MinusLogProbMetric: 28.9684, val_loss: 29.5876, val_MinusLogProbMetric: 29.5876

Epoch 420: val_loss did not improve from 28.85317
196/196 - 33s - loss: 28.9684 - MinusLogProbMetric: 28.9684 - val_loss: 29.5876 - val_MinusLogProbMetric: 29.5876 - lr: 0.0010 - 33s/epoch - 170ms/step
Epoch 421/1000
2023-10-26 05:39:13.655 
Epoch 421/1000 
	 loss: 28.9160, MinusLogProbMetric: 28.9160, val_loss: 29.3810, val_MinusLogProbMetric: 29.3810

Epoch 421: val_loss did not improve from 28.85317
196/196 - 33s - loss: 28.9160 - MinusLogProbMetric: 28.9160 - val_loss: 29.3810 - val_MinusLogProbMetric: 29.3810 - lr: 0.0010 - 33s/epoch - 167ms/step
Epoch 422/1000
2023-10-26 05:39:48.133 
Epoch 422/1000 
	 loss: 29.0693, MinusLogProbMetric: 29.0693, val_loss: 30.1581, val_MinusLogProbMetric: 30.1581

Epoch 422: val_loss did not improve from 28.85317
196/196 - 34s - loss: 29.0693 - MinusLogProbMetric: 29.0693 - val_loss: 30.1581 - val_MinusLogProbMetric: 30.1581 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 423/1000
2023-10-26 05:40:22.965 
Epoch 423/1000 
	 loss: 28.9635, MinusLogProbMetric: 28.9635, val_loss: 31.6309, val_MinusLogProbMetric: 31.6309

Epoch 423: val_loss did not improve from 28.85317
196/196 - 35s - loss: 28.9635 - MinusLogProbMetric: 28.9635 - val_loss: 31.6309 - val_MinusLogProbMetric: 31.6309 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 424/1000
2023-10-26 05:40:58.068 
Epoch 424/1000 
	 loss: 28.8630, MinusLogProbMetric: 28.8630, val_loss: 29.4304, val_MinusLogProbMetric: 29.4304

Epoch 424: val_loss did not improve from 28.85317
196/196 - 35s - loss: 28.8630 - MinusLogProbMetric: 28.8630 - val_loss: 29.4304 - val_MinusLogProbMetric: 29.4304 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 425/1000
2023-10-26 05:41:30.671 
Epoch 425/1000 
	 loss: 28.9268, MinusLogProbMetric: 28.9268, val_loss: 29.2526, val_MinusLogProbMetric: 29.2526

Epoch 425: val_loss did not improve from 28.85317
196/196 - 33s - loss: 28.9268 - MinusLogProbMetric: 28.9268 - val_loss: 29.2526 - val_MinusLogProbMetric: 29.2526 - lr: 0.0010 - 33s/epoch - 166ms/step
Epoch 426/1000
2023-10-26 05:42:04.175 
Epoch 426/1000 
	 loss: 28.8151, MinusLogProbMetric: 28.8151, val_loss: 29.0955, val_MinusLogProbMetric: 29.0955

Epoch 426: val_loss did not improve from 28.85317
196/196 - 34s - loss: 28.8151 - MinusLogProbMetric: 28.8151 - val_loss: 29.0955 - val_MinusLogProbMetric: 29.0955 - lr: 0.0010 - 34s/epoch - 171ms/step
Epoch 427/1000
2023-10-26 05:42:37.594 
Epoch 427/1000 
	 loss: 28.8500, MinusLogProbMetric: 28.8500, val_loss: 28.8136, val_MinusLogProbMetric: 28.8136

Epoch 427: val_loss improved from 28.85317 to 28.81360, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 34s - loss: 28.8500 - MinusLogProbMetric: 28.8500 - val_loss: 28.8136 - val_MinusLogProbMetric: 28.8136 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 428/1000
2023-10-26 05:43:17.325 
Epoch 428/1000 
	 loss: 28.8644, MinusLogProbMetric: 28.8644, val_loss: 29.2992, val_MinusLogProbMetric: 29.2992

Epoch 428: val_loss did not improve from 28.81360
196/196 - 39s - loss: 28.8644 - MinusLogProbMetric: 28.8644 - val_loss: 29.2992 - val_MinusLogProbMetric: 29.2992 - lr: 0.0010 - 39s/epoch - 200ms/step
Epoch 429/1000
2023-10-26 05:43:58.474 
Epoch 429/1000 
	 loss: 28.7550, MinusLogProbMetric: 28.7550, val_loss: 29.2380, val_MinusLogProbMetric: 29.2380

Epoch 429: val_loss did not improve from 28.81360
196/196 - 41s - loss: 28.7550 - MinusLogProbMetric: 28.7550 - val_loss: 29.2380 - val_MinusLogProbMetric: 29.2380 - lr: 0.0010 - 41s/epoch - 210ms/step
Epoch 430/1000
2023-10-26 05:44:39.886 
Epoch 430/1000 
	 loss: 28.9608, MinusLogProbMetric: 28.9608, val_loss: 29.4131, val_MinusLogProbMetric: 29.4131

Epoch 430: val_loss did not improve from 28.81360
196/196 - 41s - loss: 28.9608 - MinusLogProbMetric: 28.9608 - val_loss: 29.4131 - val_MinusLogProbMetric: 29.4131 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 431/1000
2023-10-26 05:45:20.860 
Epoch 431/1000 
	 loss: 28.7665, MinusLogProbMetric: 28.7665, val_loss: 29.7277, val_MinusLogProbMetric: 29.7277

Epoch 431: val_loss did not improve from 28.81360
196/196 - 41s - loss: 28.7665 - MinusLogProbMetric: 28.7665 - val_loss: 29.7277 - val_MinusLogProbMetric: 29.7277 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 432/1000
2023-10-26 05:46:03.156 
Epoch 432/1000 
	 loss: 28.9361, MinusLogProbMetric: 28.9361, val_loss: 28.9525, val_MinusLogProbMetric: 28.9525

Epoch 432: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.9361 - MinusLogProbMetric: 28.9361 - val_loss: 28.9525 - val_MinusLogProbMetric: 28.9525 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 433/1000
2023-10-26 05:46:45.410 
Epoch 433/1000 
	 loss: 28.8806, MinusLogProbMetric: 28.8806, val_loss: 29.6380, val_MinusLogProbMetric: 29.6380

Epoch 433: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8806 - MinusLogProbMetric: 28.8806 - val_loss: 29.6380 - val_MinusLogProbMetric: 29.6380 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 434/1000
2023-10-26 05:47:27.615 
Epoch 434/1000 
	 loss: 28.8806, MinusLogProbMetric: 28.8806, val_loss: 28.9540, val_MinusLogProbMetric: 28.9540

Epoch 434: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8806 - MinusLogProbMetric: 28.8806 - val_loss: 28.9540 - val_MinusLogProbMetric: 28.9540 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 435/1000
2023-10-26 05:48:09.103 
Epoch 435/1000 
	 loss: 28.7473, MinusLogProbMetric: 28.7473, val_loss: 29.2867, val_MinusLogProbMetric: 29.2867

Epoch 435: val_loss did not improve from 28.81360
196/196 - 41s - loss: 28.7473 - MinusLogProbMetric: 28.7473 - val_loss: 29.2867 - val_MinusLogProbMetric: 29.2867 - lr: 0.0010 - 41s/epoch - 212ms/step
Epoch 436/1000
2023-10-26 05:48:50.875 
Epoch 436/1000 
	 loss: 28.9674, MinusLogProbMetric: 28.9674, val_loss: 29.5304, val_MinusLogProbMetric: 29.5304

Epoch 436: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.9674 - MinusLogProbMetric: 28.9674 - val_loss: 29.5304 - val_MinusLogProbMetric: 29.5304 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 437/1000
2023-10-26 05:49:33.191 
Epoch 437/1000 
	 loss: 28.7378, MinusLogProbMetric: 28.7378, val_loss: 29.6984, val_MinusLogProbMetric: 29.6984

Epoch 437: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.7378 - MinusLogProbMetric: 28.7378 - val_loss: 29.6984 - val_MinusLogProbMetric: 29.6984 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 438/1000
2023-10-26 05:50:15.222 
Epoch 438/1000 
	 loss: 28.8057, MinusLogProbMetric: 28.8057, val_loss: 29.3998, val_MinusLogProbMetric: 29.3998

Epoch 438: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8057 - MinusLogProbMetric: 28.8057 - val_loss: 29.3998 - val_MinusLogProbMetric: 29.3998 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 439/1000
2023-10-26 05:50:57.300 
Epoch 439/1000 
	 loss: 28.9386, MinusLogProbMetric: 28.9386, val_loss: 29.1159, val_MinusLogProbMetric: 29.1159

Epoch 439: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.9386 - MinusLogProbMetric: 28.9386 - val_loss: 29.1159 - val_MinusLogProbMetric: 29.1159 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 440/1000
2023-10-26 05:51:39.540 
Epoch 440/1000 
	 loss: 28.8510, MinusLogProbMetric: 28.8510, val_loss: 29.1297, val_MinusLogProbMetric: 29.1297

Epoch 440: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8510 - MinusLogProbMetric: 28.8510 - val_loss: 29.1297 - val_MinusLogProbMetric: 29.1297 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 441/1000
2023-10-26 05:52:21.290 
Epoch 441/1000 
	 loss: 28.8456, MinusLogProbMetric: 28.8456, val_loss: 29.7871, val_MinusLogProbMetric: 29.7871

Epoch 441: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8456 - MinusLogProbMetric: 28.8456 - val_loss: 29.7871 - val_MinusLogProbMetric: 29.7871 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 442/1000
2023-10-26 05:53:03.854 
Epoch 442/1000 
	 loss: 28.8710, MinusLogProbMetric: 28.8710, val_loss: 29.2866, val_MinusLogProbMetric: 29.2866

Epoch 442: val_loss did not improve from 28.81360
196/196 - 43s - loss: 28.8710 - MinusLogProbMetric: 28.8710 - val_loss: 29.2866 - val_MinusLogProbMetric: 29.2866 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 443/1000
2023-10-26 05:53:45.702 
Epoch 443/1000 
	 loss: 28.8158, MinusLogProbMetric: 28.8158, val_loss: 29.1980, val_MinusLogProbMetric: 29.1980

Epoch 443: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8158 - MinusLogProbMetric: 28.8158 - val_loss: 29.1980 - val_MinusLogProbMetric: 29.1980 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 444/1000
2023-10-26 05:54:28.125 
Epoch 444/1000 
	 loss: 28.8816, MinusLogProbMetric: 28.8816, val_loss: 29.0557, val_MinusLogProbMetric: 29.0557

Epoch 444: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8816 - MinusLogProbMetric: 28.8816 - val_loss: 29.0557 - val_MinusLogProbMetric: 29.0557 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 445/1000
2023-10-26 05:55:10.311 
Epoch 445/1000 
	 loss: 29.2148, MinusLogProbMetric: 29.2148, val_loss: 29.1167, val_MinusLogProbMetric: 29.1167

Epoch 445: val_loss did not improve from 28.81360
196/196 - 42s - loss: 29.2148 - MinusLogProbMetric: 29.2148 - val_loss: 29.1167 - val_MinusLogProbMetric: 29.1167 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 446/1000
2023-10-26 05:55:51.965 
Epoch 446/1000 
	 loss: 28.7854, MinusLogProbMetric: 28.7854, val_loss: 28.8932, val_MinusLogProbMetric: 28.8932

Epoch 446: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.7854 - MinusLogProbMetric: 28.7854 - val_loss: 28.8932 - val_MinusLogProbMetric: 28.8932 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 447/1000
2023-10-26 05:56:34.285 
Epoch 447/1000 
	 loss: 28.7638, MinusLogProbMetric: 28.7638, val_loss: 28.9995, val_MinusLogProbMetric: 28.9995

Epoch 447: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.7638 - MinusLogProbMetric: 28.7638 - val_loss: 28.9995 - val_MinusLogProbMetric: 28.9995 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 448/1000
2023-10-26 05:57:16.568 
Epoch 448/1000 
	 loss: 28.8346, MinusLogProbMetric: 28.8346, val_loss: 29.0915, val_MinusLogProbMetric: 29.0915

Epoch 448: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8346 - MinusLogProbMetric: 28.8346 - val_loss: 29.0915 - val_MinusLogProbMetric: 29.0915 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 449/1000
2023-10-26 05:57:59.151 
Epoch 449/1000 
	 loss: 28.9328, MinusLogProbMetric: 28.9328, val_loss: 29.1086, val_MinusLogProbMetric: 29.1086

Epoch 449: val_loss did not improve from 28.81360
196/196 - 43s - loss: 28.9328 - MinusLogProbMetric: 28.9328 - val_loss: 29.1086 - val_MinusLogProbMetric: 29.1086 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 450/1000
2023-10-26 05:58:41.368 
Epoch 450/1000 
	 loss: 28.8142, MinusLogProbMetric: 28.8142, val_loss: 28.8451, val_MinusLogProbMetric: 28.8451

Epoch 450: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8142 - MinusLogProbMetric: 28.8142 - val_loss: 28.8451 - val_MinusLogProbMetric: 28.8451 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 451/1000
2023-10-26 05:59:23.736 
Epoch 451/1000 
	 loss: 28.8781, MinusLogProbMetric: 28.8781, val_loss: 29.3718, val_MinusLogProbMetric: 29.3718

Epoch 451: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8781 - MinusLogProbMetric: 28.8781 - val_loss: 29.3718 - val_MinusLogProbMetric: 29.3718 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 452/1000
2023-10-26 06:00:06.024 
Epoch 452/1000 
	 loss: 28.8263, MinusLogProbMetric: 28.8263, val_loss: 28.9764, val_MinusLogProbMetric: 28.9764

Epoch 452: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8263 - MinusLogProbMetric: 28.8263 - val_loss: 28.9764 - val_MinusLogProbMetric: 28.9764 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 453/1000
2023-10-26 06:00:48.151 
Epoch 453/1000 
	 loss: 28.8460, MinusLogProbMetric: 28.8460, val_loss: 29.3671, val_MinusLogProbMetric: 29.3671

Epoch 453: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8460 - MinusLogProbMetric: 28.8460 - val_loss: 29.3671 - val_MinusLogProbMetric: 29.3671 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 454/1000
2023-10-26 06:01:30.694 
Epoch 454/1000 
	 loss: 28.8616, MinusLogProbMetric: 28.8616, val_loss: 28.9696, val_MinusLogProbMetric: 28.9696

Epoch 454: val_loss did not improve from 28.81360
196/196 - 43s - loss: 28.8616 - MinusLogProbMetric: 28.8616 - val_loss: 28.9696 - val_MinusLogProbMetric: 28.9696 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 455/1000
2023-10-26 06:02:12.230 
Epoch 455/1000 
	 loss: 28.9337, MinusLogProbMetric: 28.9337, val_loss: 28.8696, val_MinusLogProbMetric: 28.8696

Epoch 455: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.9337 - MinusLogProbMetric: 28.9337 - val_loss: 28.8696 - val_MinusLogProbMetric: 28.8696 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 456/1000
2023-10-26 06:02:54.617 
Epoch 456/1000 
	 loss: 28.7815, MinusLogProbMetric: 28.7815, val_loss: 28.9987, val_MinusLogProbMetric: 28.9987

Epoch 456: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.7815 - MinusLogProbMetric: 28.7815 - val_loss: 28.9987 - val_MinusLogProbMetric: 28.9987 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 457/1000
2023-10-26 06:03:36.966 
Epoch 457/1000 
	 loss: 28.7550, MinusLogProbMetric: 28.7550, val_loss: 29.3628, val_MinusLogProbMetric: 29.3628

Epoch 457: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.7550 - MinusLogProbMetric: 28.7550 - val_loss: 29.3628 - val_MinusLogProbMetric: 29.3628 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 458/1000
2023-10-26 06:04:19.237 
Epoch 458/1000 
	 loss: 28.8516, MinusLogProbMetric: 28.8516, val_loss: 29.2347, val_MinusLogProbMetric: 29.2347

Epoch 458: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8516 - MinusLogProbMetric: 28.8516 - val_loss: 29.2347 - val_MinusLogProbMetric: 29.2347 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 459/1000
2023-10-26 06:05:01.572 
Epoch 459/1000 
	 loss: 28.8174, MinusLogProbMetric: 28.8174, val_loss: 28.9125, val_MinusLogProbMetric: 28.9125

Epoch 459: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.8174 - MinusLogProbMetric: 28.8174 - val_loss: 28.9125 - val_MinusLogProbMetric: 28.9125 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 460/1000
2023-10-26 06:05:43.862 
Epoch 460/1000 
	 loss: 28.7699, MinusLogProbMetric: 28.7699, val_loss: 29.4268, val_MinusLogProbMetric: 29.4268

Epoch 460: val_loss did not improve from 28.81360
196/196 - 42s - loss: 28.7699 - MinusLogProbMetric: 28.7699 - val_loss: 29.4268 - val_MinusLogProbMetric: 29.4268 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 461/1000
2023-10-26 06:06:26.413 
Epoch 461/1000 
	 loss: 28.8565, MinusLogProbMetric: 28.8565, val_loss: 28.7769, val_MinusLogProbMetric: 28.7769

Epoch 461: val_loss improved from 28.81360 to 28.77693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 28.8565 - MinusLogProbMetric: 28.8565 - val_loss: 28.7769 - val_MinusLogProbMetric: 28.7769 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 462/1000
2023-10-26 06:07:08.154 
Epoch 462/1000 
	 loss: 28.8689, MinusLogProbMetric: 28.8689, val_loss: 29.5955, val_MinusLogProbMetric: 29.5955

Epoch 462: val_loss did not improve from 28.77693
196/196 - 41s - loss: 28.8689 - MinusLogProbMetric: 28.8689 - val_loss: 29.5955 - val_MinusLogProbMetric: 29.5955 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 463/1000
2023-10-26 06:07:50.468 
Epoch 463/1000 
	 loss: 28.7311, MinusLogProbMetric: 28.7311, val_loss: 29.0129, val_MinusLogProbMetric: 29.0129

Epoch 463: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.7311 - MinusLogProbMetric: 28.7311 - val_loss: 29.0129 - val_MinusLogProbMetric: 29.0129 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 464/1000
2023-10-26 06:08:32.844 
Epoch 464/1000 
	 loss: 28.8101, MinusLogProbMetric: 28.8101, val_loss: 28.9545, val_MinusLogProbMetric: 28.9545

Epoch 464: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.8101 - MinusLogProbMetric: 28.8101 - val_loss: 28.9545 - val_MinusLogProbMetric: 28.9545 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 465/1000
2023-10-26 06:09:15.364 
Epoch 465/1000 
	 loss: 28.7681, MinusLogProbMetric: 28.7681, val_loss: 28.8964, val_MinusLogProbMetric: 28.8964

Epoch 465: val_loss did not improve from 28.77693
196/196 - 43s - loss: 28.7681 - MinusLogProbMetric: 28.7681 - val_loss: 28.8964 - val_MinusLogProbMetric: 28.8964 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 466/1000
2023-10-26 06:09:57.128 
Epoch 466/1000 
	 loss: 28.8266, MinusLogProbMetric: 28.8266, val_loss: 28.7811, val_MinusLogProbMetric: 28.7811

Epoch 466: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.8266 - MinusLogProbMetric: 28.8266 - val_loss: 28.7811 - val_MinusLogProbMetric: 28.7811 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 467/1000
2023-10-26 06:10:39.360 
Epoch 467/1000 
	 loss: 28.8348, MinusLogProbMetric: 28.8348, val_loss: 29.2203, val_MinusLogProbMetric: 29.2203

Epoch 467: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.8348 - MinusLogProbMetric: 28.8348 - val_loss: 29.2203 - val_MinusLogProbMetric: 29.2203 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 468/1000
2023-10-26 06:11:21.928 
Epoch 468/1000 
	 loss: 28.8389, MinusLogProbMetric: 28.8389, val_loss: 29.5043, val_MinusLogProbMetric: 29.5043

Epoch 468: val_loss did not improve from 28.77693
196/196 - 43s - loss: 28.8389 - MinusLogProbMetric: 28.8389 - val_loss: 29.5043 - val_MinusLogProbMetric: 29.5043 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 469/1000
2023-10-26 06:12:04.184 
Epoch 469/1000 
	 loss: 28.8466, MinusLogProbMetric: 28.8466, val_loss: 28.8198, val_MinusLogProbMetric: 28.8198

Epoch 469: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.8466 - MinusLogProbMetric: 28.8466 - val_loss: 28.8198 - val_MinusLogProbMetric: 28.8198 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 470/1000
2023-10-26 06:12:46.756 
Epoch 470/1000 
	 loss: 28.7326, MinusLogProbMetric: 28.7326, val_loss: 29.5481, val_MinusLogProbMetric: 29.5481

Epoch 470: val_loss did not improve from 28.77693
196/196 - 43s - loss: 28.7326 - MinusLogProbMetric: 28.7326 - val_loss: 29.5481 - val_MinusLogProbMetric: 29.5481 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 471/1000
2023-10-26 06:13:28.842 
Epoch 471/1000 
	 loss: 28.6842, MinusLogProbMetric: 28.6842, val_loss: 28.8686, val_MinusLogProbMetric: 28.8686

Epoch 471: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.6842 - MinusLogProbMetric: 28.6842 - val_loss: 28.8686 - val_MinusLogProbMetric: 28.8686 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 472/1000
2023-10-26 06:14:10.629 
Epoch 472/1000 
	 loss: 28.7370, MinusLogProbMetric: 28.7370, val_loss: 29.0988, val_MinusLogProbMetric: 29.0988

Epoch 472: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.7370 - MinusLogProbMetric: 28.7370 - val_loss: 29.0988 - val_MinusLogProbMetric: 29.0988 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 473/1000
2023-10-26 06:14:53.070 
Epoch 473/1000 
	 loss: 28.7822, MinusLogProbMetric: 28.7822, val_loss: 28.9357, val_MinusLogProbMetric: 28.9357

Epoch 473: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.7822 - MinusLogProbMetric: 28.7822 - val_loss: 28.9357 - val_MinusLogProbMetric: 28.9357 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 474/1000
2023-10-26 06:15:35.055 
Epoch 474/1000 
	 loss: 28.8330, MinusLogProbMetric: 28.8330, val_loss: 29.2868, val_MinusLogProbMetric: 29.2868

Epoch 474: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.8330 - MinusLogProbMetric: 28.8330 - val_loss: 29.2868 - val_MinusLogProbMetric: 29.2868 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 475/1000
2023-10-26 06:16:17.336 
Epoch 475/1000 
	 loss: 28.7043, MinusLogProbMetric: 28.7043, val_loss: 29.4261, val_MinusLogProbMetric: 29.4261

Epoch 475: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.7043 - MinusLogProbMetric: 28.7043 - val_loss: 29.4261 - val_MinusLogProbMetric: 29.4261 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 476/1000
2023-10-26 06:16:59.595 
Epoch 476/1000 
	 loss: 28.7396, MinusLogProbMetric: 28.7396, val_loss: 29.0486, val_MinusLogProbMetric: 29.0486

Epoch 476: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.7396 - MinusLogProbMetric: 28.7396 - val_loss: 29.0486 - val_MinusLogProbMetric: 29.0486 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 477/1000
2023-10-26 06:17:42.227 
Epoch 477/1000 
	 loss: 28.8764, MinusLogProbMetric: 28.8764, val_loss: 29.5359, val_MinusLogProbMetric: 29.5359

Epoch 477: val_loss did not improve from 28.77693
196/196 - 43s - loss: 28.8764 - MinusLogProbMetric: 28.8764 - val_loss: 29.5359 - val_MinusLogProbMetric: 29.5359 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 478/1000
2023-10-26 06:18:24.665 
Epoch 478/1000 
	 loss: 28.7481, MinusLogProbMetric: 28.7481, val_loss: 29.2192, val_MinusLogProbMetric: 29.2192

Epoch 478: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.7481 - MinusLogProbMetric: 28.7481 - val_loss: 29.2192 - val_MinusLogProbMetric: 29.2192 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 479/1000
2023-10-26 06:19:06.168 
Epoch 479/1000 
	 loss: 28.8004, MinusLogProbMetric: 28.8004, val_loss: 29.0092, val_MinusLogProbMetric: 29.0092

Epoch 479: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.8004 - MinusLogProbMetric: 28.8004 - val_loss: 29.0092 - val_MinusLogProbMetric: 29.0092 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 480/1000
2023-10-26 06:19:48.363 
Epoch 480/1000 
	 loss: 28.7437, MinusLogProbMetric: 28.7437, val_loss: 28.9313, val_MinusLogProbMetric: 28.9313

Epoch 480: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.7437 - MinusLogProbMetric: 28.7437 - val_loss: 28.9313 - val_MinusLogProbMetric: 28.9313 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 481/1000
2023-10-26 06:20:30.763 
Epoch 481/1000 
	 loss: 28.9179, MinusLogProbMetric: 28.9179, val_loss: 29.1286, val_MinusLogProbMetric: 29.1286

Epoch 481: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.9179 - MinusLogProbMetric: 28.9179 - val_loss: 29.1286 - val_MinusLogProbMetric: 29.1286 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 482/1000
2023-10-26 06:21:13.177 
Epoch 482/1000 
	 loss: 28.7325, MinusLogProbMetric: 28.7325, val_loss: 28.8848, val_MinusLogProbMetric: 28.8848

Epoch 482: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.7325 - MinusLogProbMetric: 28.7325 - val_loss: 28.8848 - val_MinusLogProbMetric: 28.8848 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 483/1000
2023-10-26 06:21:55.475 
Epoch 483/1000 
	 loss: 28.7680, MinusLogProbMetric: 28.7680, val_loss: 28.8497, val_MinusLogProbMetric: 28.8497

Epoch 483: val_loss did not improve from 28.77693
196/196 - 42s - loss: 28.7680 - MinusLogProbMetric: 28.7680 - val_loss: 28.8497 - val_MinusLogProbMetric: 28.8497 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 484/1000
2023-10-26 06:22:37.531 
Epoch 484/1000 
	 loss: 28.7575, MinusLogProbMetric: 28.7575, val_loss: 28.7644, val_MinusLogProbMetric: 28.7644

Epoch 484: val_loss improved from 28.77693 to 28.76439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 28.7575 - MinusLogProbMetric: 28.7575 - val_loss: 28.7644 - val_MinusLogProbMetric: 28.7644 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 485/1000
2023-10-26 06:23:20.637 
Epoch 485/1000 
	 loss: 28.8537, MinusLogProbMetric: 28.8537, val_loss: 29.1145, val_MinusLogProbMetric: 29.1145

Epoch 485: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.8537 - MinusLogProbMetric: 28.8537 - val_loss: 29.1145 - val_MinusLogProbMetric: 29.1145 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 486/1000
2023-10-26 06:24:02.942 
Epoch 486/1000 
	 loss: 28.6804, MinusLogProbMetric: 28.6804, val_loss: 29.1489, val_MinusLogProbMetric: 29.1489

Epoch 486: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.6804 - MinusLogProbMetric: 28.6804 - val_loss: 29.1489 - val_MinusLogProbMetric: 29.1489 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 487/1000
2023-10-26 06:24:45.200 
Epoch 487/1000 
	 loss: 28.7570, MinusLogProbMetric: 28.7570, val_loss: 29.1273, val_MinusLogProbMetric: 29.1273

Epoch 487: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7570 - MinusLogProbMetric: 28.7570 - val_loss: 29.1273 - val_MinusLogProbMetric: 29.1273 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 488/1000
2023-10-26 06:25:27.258 
Epoch 488/1000 
	 loss: 28.7791, MinusLogProbMetric: 28.7791, val_loss: 29.8226, val_MinusLogProbMetric: 29.8226

Epoch 488: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7791 - MinusLogProbMetric: 28.7791 - val_loss: 29.8226 - val_MinusLogProbMetric: 29.8226 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 489/1000
2023-10-26 06:26:09.397 
Epoch 489/1000 
	 loss: 28.7203, MinusLogProbMetric: 28.7203, val_loss: 28.9658, val_MinusLogProbMetric: 28.9658

Epoch 489: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7203 - MinusLogProbMetric: 28.7203 - val_loss: 28.9658 - val_MinusLogProbMetric: 28.9658 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 490/1000
2023-10-26 06:26:51.835 
Epoch 490/1000 
	 loss: 28.7200, MinusLogProbMetric: 28.7200, val_loss: 29.2726, val_MinusLogProbMetric: 29.2726

Epoch 490: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7200 - MinusLogProbMetric: 28.7200 - val_loss: 29.2726 - val_MinusLogProbMetric: 29.2726 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 491/1000
2023-10-26 06:27:33.802 
Epoch 491/1000 
	 loss: 28.7440, MinusLogProbMetric: 28.7440, val_loss: 29.1968, val_MinusLogProbMetric: 29.1968

Epoch 491: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7440 - MinusLogProbMetric: 28.7440 - val_loss: 29.1968 - val_MinusLogProbMetric: 29.1968 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 492/1000
2023-10-26 06:28:15.632 
Epoch 492/1000 
	 loss: 28.8783, MinusLogProbMetric: 28.8783, val_loss: 29.0568, val_MinusLogProbMetric: 29.0568

Epoch 492: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.8783 - MinusLogProbMetric: 28.8783 - val_loss: 29.0568 - val_MinusLogProbMetric: 29.0568 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 493/1000
2023-10-26 06:28:57.804 
Epoch 493/1000 
	 loss: 28.8360, MinusLogProbMetric: 28.8360, val_loss: 30.4393, val_MinusLogProbMetric: 30.4393

Epoch 493: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.8360 - MinusLogProbMetric: 28.8360 - val_loss: 30.4393 - val_MinusLogProbMetric: 30.4393 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 494/1000
2023-10-26 06:29:40.593 
Epoch 494/1000 
	 loss: 28.7010, MinusLogProbMetric: 28.7010, val_loss: 28.9893, val_MinusLogProbMetric: 28.9893

Epoch 494: val_loss did not improve from 28.76439
196/196 - 43s - loss: 28.7010 - MinusLogProbMetric: 28.7010 - val_loss: 28.9893 - val_MinusLogProbMetric: 28.9893 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 495/1000
2023-10-26 06:30:22.612 
Epoch 495/1000 
	 loss: 28.8156, MinusLogProbMetric: 28.8156, val_loss: 29.0599, val_MinusLogProbMetric: 29.0599

Epoch 495: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.8156 - MinusLogProbMetric: 28.8156 - val_loss: 29.0599 - val_MinusLogProbMetric: 29.0599 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 496/1000
2023-10-26 06:31:04.690 
Epoch 496/1000 
	 loss: 28.6771, MinusLogProbMetric: 28.6771, val_loss: 29.1856, val_MinusLogProbMetric: 29.1856

Epoch 496: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.6771 - MinusLogProbMetric: 28.6771 - val_loss: 29.1856 - val_MinusLogProbMetric: 29.1856 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 497/1000
2023-10-26 06:31:46.909 
Epoch 497/1000 
	 loss: 28.8647, MinusLogProbMetric: 28.8647, val_loss: 29.2412, val_MinusLogProbMetric: 29.2412

Epoch 497: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.8647 - MinusLogProbMetric: 28.8647 - val_loss: 29.2412 - val_MinusLogProbMetric: 29.2412 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 498/1000
2023-10-26 06:32:29.214 
Epoch 498/1000 
	 loss: 28.6306, MinusLogProbMetric: 28.6306, val_loss: 29.8646, val_MinusLogProbMetric: 29.8646

Epoch 498: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.6306 - MinusLogProbMetric: 28.6306 - val_loss: 29.8646 - val_MinusLogProbMetric: 29.8646 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 499/1000
2023-10-26 06:33:11.196 
Epoch 499/1000 
	 loss: 28.8162, MinusLogProbMetric: 28.8162, val_loss: 29.2402, val_MinusLogProbMetric: 29.2402

Epoch 499: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.8162 - MinusLogProbMetric: 28.8162 - val_loss: 29.2402 - val_MinusLogProbMetric: 29.2402 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 500/1000
2023-10-26 06:33:53.725 
Epoch 500/1000 
	 loss: 28.6599, MinusLogProbMetric: 28.6599, val_loss: 28.9911, val_MinusLogProbMetric: 28.9911

Epoch 500: val_loss did not improve from 28.76439
196/196 - 43s - loss: 28.6599 - MinusLogProbMetric: 28.6599 - val_loss: 28.9911 - val_MinusLogProbMetric: 28.9911 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 501/1000
2023-10-26 06:34:36.043 
Epoch 501/1000 
	 loss: 28.7775, MinusLogProbMetric: 28.7775, val_loss: 29.5356, val_MinusLogProbMetric: 29.5356

Epoch 501: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7775 - MinusLogProbMetric: 28.7775 - val_loss: 29.5356 - val_MinusLogProbMetric: 29.5356 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 502/1000
2023-10-26 06:35:18.897 
Epoch 502/1000 
	 loss: 28.8030, MinusLogProbMetric: 28.8030, val_loss: 29.1689, val_MinusLogProbMetric: 29.1689

Epoch 502: val_loss did not improve from 28.76439
196/196 - 43s - loss: 28.8030 - MinusLogProbMetric: 28.8030 - val_loss: 29.1689 - val_MinusLogProbMetric: 29.1689 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 503/1000
2023-10-26 06:36:01.135 
Epoch 503/1000 
	 loss: 28.6850, MinusLogProbMetric: 28.6850, val_loss: 28.9555, val_MinusLogProbMetric: 28.9555

Epoch 503: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.6850 - MinusLogProbMetric: 28.6850 - val_loss: 28.9555 - val_MinusLogProbMetric: 28.9555 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 504/1000
2023-10-26 06:36:43.038 
Epoch 504/1000 
	 loss: 28.6756, MinusLogProbMetric: 28.6756, val_loss: 29.0235, val_MinusLogProbMetric: 29.0235

Epoch 504: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.6756 - MinusLogProbMetric: 28.6756 - val_loss: 29.0235 - val_MinusLogProbMetric: 29.0235 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 505/1000
2023-10-26 06:37:25.737 
Epoch 505/1000 
	 loss: 28.6600, MinusLogProbMetric: 28.6600, val_loss: 28.7651, val_MinusLogProbMetric: 28.7651

Epoch 505: val_loss did not improve from 28.76439
196/196 - 43s - loss: 28.6600 - MinusLogProbMetric: 28.6600 - val_loss: 28.7651 - val_MinusLogProbMetric: 28.7651 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 506/1000
2023-10-26 06:38:08.135 
Epoch 506/1000 
	 loss: 28.7796, MinusLogProbMetric: 28.7796, val_loss: 29.3181, val_MinusLogProbMetric: 29.3181

Epoch 506: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7796 - MinusLogProbMetric: 28.7796 - val_loss: 29.3181 - val_MinusLogProbMetric: 29.3181 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 507/1000
2023-10-26 06:38:50.380 
Epoch 507/1000 
	 loss: 28.7119, MinusLogProbMetric: 28.7119, val_loss: 28.8278, val_MinusLogProbMetric: 28.8278

Epoch 507: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7119 - MinusLogProbMetric: 28.7119 - val_loss: 28.8278 - val_MinusLogProbMetric: 28.8278 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 508/1000
2023-10-26 06:39:32.767 
Epoch 508/1000 
	 loss: 28.8906, MinusLogProbMetric: 28.8906, val_loss: 29.2265, val_MinusLogProbMetric: 29.2265

Epoch 508: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.8906 - MinusLogProbMetric: 28.8906 - val_loss: 29.2265 - val_MinusLogProbMetric: 29.2265 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 509/1000
2023-10-26 06:40:15.286 
Epoch 509/1000 
	 loss: 28.6889, MinusLogProbMetric: 28.6889, val_loss: 29.2227, val_MinusLogProbMetric: 29.2227

Epoch 509: val_loss did not improve from 28.76439
196/196 - 43s - loss: 28.6889 - MinusLogProbMetric: 28.6889 - val_loss: 29.2227 - val_MinusLogProbMetric: 29.2227 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 510/1000
2023-10-26 06:40:57.527 
Epoch 510/1000 
	 loss: 28.7862, MinusLogProbMetric: 28.7862, val_loss: 29.5222, val_MinusLogProbMetric: 29.5222

Epoch 510: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7862 - MinusLogProbMetric: 28.7862 - val_loss: 29.5222 - val_MinusLogProbMetric: 29.5222 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 511/1000
2023-10-26 06:41:39.856 
Epoch 511/1000 
	 loss: 28.6982, MinusLogProbMetric: 28.6982, val_loss: 29.6821, val_MinusLogProbMetric: 29.6821

Epoch 511: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.6982 - MinusLogProbMetric: 28.6982 - val_loss: 29.6821 - val_MinusLogProbMetric: 29.6821 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 512/1000
2023-10-26 06:42:22.220 
Epoch 512/1000 
	 loss: 28.9000, MinusLogProbMetric: 28.9000, val_loss: 28.8922, val_MinusLogProbMetric: 28.8922

Epoch 512: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.9000 - MinusLogProbMetric: 28.9000 - val_loss: 28.8922 - val_MinusLogProbMetric: 28.8922 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 513/1000
2023-10-26 06:43:04.316 
Epoch 513/1000 
	 loss: 28.5709, MinusLogProbMetric: 28.5709, val_loss: 29.2660, val_MinusLogProbMetric: 29.2660

Epoch 513: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.5709 - MinusLogProbMetric: 28.5709 - val_loss: 29.2660 - val_MinusLogProbMetric: 29.2660 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 514/1000
2023-10-26 06:43:46.611 
Epoch 514/1000 
	 loss: 28.7005, MinusLogProbMetric: 28.7005, val_loss: 29.4233, val_MinusLogProbMetric: 29.4233

Epoch 514: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7005 - MinusLogProbMetric: 28.7005 - val_loss: 29.4233 - val_MinusLogProbMetric: 29.4233 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 515/1000
2023-10-26 06:44:29.015 
Epoch 515/1000 
	 loss: 28.9278, MinusLogProbMetric: 28.9278, val_loss: 29.0433, val_MinusLogProbMetric: 29.0433

Epoch 515: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.9278 - MinusLogProbMetric: 28.9278 - val_loss: 29.0433 - val_MinusLogProbMetric: 29.0433 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 516/1000
2023-10-26 06:45:11.948 
Epoch 516/1000 
	 loss: 28.6223, MinusLogProbMetric: 28.6223, val_loss: 29.2575, val_MinusLogProbMetric: 29.2575

Epoch 516: val_loss did not improve from 28.76439
196/196 - 43s - loss: 28.6223 - MinusLogProbMetric: 28.6223 - val_loss: 29.2575 - val_MinusLogProbMetric: 29.2575 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 517/1000
2023-10-26 06:45:54.316 
Epoch 517/1000 
	 loss: 28.6345, MinusLogProbMetric: 28.6345, val_loss: 28.8531, val_MinusLogProbMetric: 28.8531

Epoch 517: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.6345 - MinusLogProbMetric: 28.6345 - val_loss: 28.8531 - val_MinusLogProbMetric: 28.8531 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 518/1000
2023-10-26 06:46:36.121 
Epoch 518/1000 
	 loss: 28.7378, MinusLogProbMetric: 28.7378, val_loss: 29.0193, val_MinusLogProbMetric: 29.0193

Epoch 518: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7378 - MinusLogProbMetric: 28.7378 - val_loss: 29.0193 - val_MinusLogProbMetric: 29.0193 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 519/1000
2023-10-26 06:47:18.570 
Epoch 519/1000 
	 loss: 28.6484, MinusLogProbMetric: 28.6484, val_loss: 29.0716, val_MinusLogProbMetric: 29.0716

Epoch 519: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.6484 - MinusLogProbMetric: 28.6484 - val_loss: 29.0716 - val_MinusLogProbMetric: 29.0716 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 520/1000
2023-10-26 06:48:00.748 
Epoch 520/1000 
	 loss: 28.7592, MinusLogProbMetric: 28.7592, val_loss: 29.7489, val_MinusLogProbMetric: 29.7489

Epoch 520: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7592 - MinusLogProbMetric: 28.7592 - val_loss: 29.7489 - val_MinusLogProbMetric: 29.7489 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 521/1000
2023-10-26 06:48:43.212 
Epoch 521/1000 
	 loss: 28.6896, MinusLogProbMetric: 28.6896, val_loss: 29.1725, val_MinusLogProbMetric: 29.1725

Epoch 521: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.6896 - MinusLogProbMetric: 28.6896 - val_loss: 29.1725 - val_MinusLogProbMetric: 29.1725 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 522/1000
2023-10-26 06:49:25.525 
Epoch 522/1000 
	 loss: 28.6152, MinusLogProbMetric: 28.6152, val_loss: 28.8507, val_MinusLogProbMetric: 28.8507

Epoch 522: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.6152 - MinusLogProbMetric: 28.6152 - val_loss: 28.8507 - val_MinusLogProbMetric: 28.8507 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 523/1000
2023-10-26 06:50:07.801 
Epoch 523/1000 
	 loss: 28.6601, MinusLogProbMetric: 28.6601, val_loss: 31.2307, val_MinusLogProbMetric: 31.2307

Epoch 523: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.6601 - MinusLogProbMetric: 28.6601 - val_loss: 31.2307 - val_MinusLogProbMetric: 31.2307 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 524/1000
2023-10-26 06:50:49.711 
Epoch 524/1000 
	 loss: 28.7493, MinusLogProbMetric: 28.7493, val_loss: 29.0499, val_MinusLogProbMetric: 29.0499

Epoch 524: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7493 - MinusLogProbMetric: 28.7493 - val_loss: 29.0499 - val_MinusLogProbMetric: 29.0499 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 525/1000
2023-10-26 06:51:32.009 
Epoch 525/1000 
	 loss: 28.6637, MinusLogProbMetric: 28.6637, val_loss: 29.0570, val_MinusLogProbMetric: 29.0570

Epoch 525: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.6637 - MinusLogProbMetric: 28.6637 - val_loss: 29.0570 - val_MinusLogProbMetric: 29.0570 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 526/1000
2023-10-26 06:52:12.077 
Epoch 526/1000 
	 loss: 28.7684, MinusLogProbMetric: 28.7684, val_loss: 28.8134, val_MinusLogProbMetric: 28.8134

Epoch 526: val_loss did not improve from 28.76439
196/196 - 40s - loss: 28.7684 - MinusLogProbMetric: 28.7684 - val_loss: 28.8134 - val_MinusLogProbMetric: 28.8134 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 527/1000
2023-10-26 06:52:53.822 
Epoch 527/1000 
	 loss: 28.7433, MinusLogProbMetric: 28.7433, val_loss: 29.0741, val_MinusLogProbMetric: 29.0741

Epoch 527: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.7433 - MinusLogProbMetric: 28.7433 - val_loss: 29.0741 - val_MinusLogProbMetric: 29.0741 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 528/1000
2023-10-26 06:53:35.172 
Epoch 528/1000 
	 loss: 28.6703, MinusLogProbMetric: 28.6703, val_loss: 28.7950, val_MinusLogProbMetric: 28.7950

Epoch 528: val_loss did not improve from 28.76439
196/196 - 41s - loss: 28.6703 - MinusLogProbMetric: 28.6703 - val_loss: 28.7950 - val_MinusLogProbMetric: 28.7950 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 529/1000
2023-10-26 06:54:15.827 
Epoch 529/1000 
	 loss: 28.6025, MinusLogProbMetric: 28.6025, val_loss: 29.2937, val_MinusLogProbMetric: 29.2937

Epoch 529: val_loss did not improve from 28.76439
196/196 - 41s - loss: 28.6025 - MinusLogProbMetric: 28.6025 - val_loss: 29.2937 - val_MinusLogProbMetric: 29.2937 - lr: 0.0010 - 41s/epoch - 207ms/step
Epoch 530/1000
2023-10-26 06:54:58.231 
Epoch 530/1000 
	 loss: 28.8939, MinusLogProbMetric: 28.8939, val_loss: 30.0089, val_MinusLogProbMetric: 30.0089

Epoch 530: val_loss did not improve from 28.76439
196/196 - 42s - loss: 28.8939 - MinusLogProbMetric: 28.8939 - val_loss: 30.0089 - val_MinusLogProbMetric: 30.0089 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 531/1000
2023-10-26 06:55:38.971 
Epoch 531/1000 
	 loss: 28.6548, MinusLogProbMetric: 28.6548, val_loss: 28.7376, val_MinusLogProbMetric: 28.7376

Epoch 531: val_loss improved from 28.76439 to 28.73765, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 42s - loss: 28.6548 - MinusLogProbMetric: 28.6548 - val_loss: 28.7376 - val_MinusLogProbMetric: 28.7376 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 532/1000
2023-10-26 06:56:21.722 
Epoch 532/1000 
	 loss: 28.5853, MinusLogProbMetric: 28.5853, val_loss: 29.2544, val_MinusLogProbMetric: 29.2544

Epoch 532: val_loss did not improve from 28.73765
196/196 - 42s - loss: 28.5853 - MinusLogProbMetric: 28.5853 - val_loss: 29.2544 - val_MinusLogProbMetric: 29.2544 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 533/1000
2023-10-26 06:57:01.221 
Epoch 533/1000 
	 loss: 28.6389, MinusLogProbMetric: 28.6389, val_loss: 28.9795, val_MinusLogProbMetric: 28.9795

Epoch 533: val_loss did not improve from 28.73765
196/196 - 39s - loss: 28.6389 - MinusLogProbMetric: 28.6389 - val_loss: 28.9795 - val_MinusLogProbMetric: 28.9795 - lr: 0.0010 - 39s/epoch - 202ms/step
Epoch 534/1000
2023-10-26 06:57:41.177 
Epoch 534/1000 
	 loss: 28.7100, MinusLogProbMetric: 28.7100, val_loss: 30.6163, val_MinusLogProbMetric: 30.6163

Epoch 534: val_loss did not improve from 28.73765
196/196 - 40s - loss: 28.7100 - MinusLogProbMetric: 28.7100 - val_loss: 30.6163 - val_MinusLogProbMetric: 30.6163 - lr: 0.0010 - 40s/epoch - 204ms/step
Epoch 535/1000
2023-10-26 06:58:23.733 
Epoch 535/1000 
	 loss: 28.7085, MinusLogProbMetric: 28.7085, val_loss: 29.2166, val_MinusLogProbMetric: 29.2166

Epoch 535: val_loss did not improve from 28.73765
196/196 - 43s - loss: 28.7085 - MinusLogProbMetric: 28.7085 - val_loss: 29.2166 - val_MinusLogProbMetric: 29.2166 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 536/1000
2023-10-26 06:59:06.374 
Epoch 536/1000 
	 loss: 28.5754, MinusLogProbMetric: 28.5754, val_loss: 29.2558, val_MinusLogProbMetric: 29.2558

Epoch 536: val_loss did not improve from 28.73765
196/196 - 43s - loss: 28.5754 - MinusLogProbMetric: 28.5754 - val_loss: 29.2558 - val_MinusLogProbMetric: 29.2558 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 537/1000
2023-10-26 06:59:48.227 
Epoch 537/1000 
	 loss: 28.6653, MinusLogProbMetric: 28.6653, val_loss: 28.8231, val_MinusLogProbMetric: 28.8231

Epoch 537: val_loss did not improve from 28.73765
196/196 - 42s - loss: 28.6653 - MinusLogProbMetric: 28.6653 - val_loss: 28.8231 - val_MinusLogProbMetric: 28.8231 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 538/1000
2023-10-26 07:00:30.628 
Epoch 538/1000 
	 loss: 28.6604, MinusLogProbMetric: 28.6604, val_loss: 28.8211, val_MinusLogProbMetric: 28.8211

Epoch 538: val_loss did not improve from 28.73765
196/196 - 42s - loss: 28.6604 - MinusLogProbMetric: 28.6604 - val_loss: 28.8211 - val_MinusLogProbMetric: 28.8211 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 539/1000
2023-10-26 07:01:13.117 
Epoch 539/1000 
	 loss: 28.5682, MinusLogProbMetric: 28.5682, val_loss: 28.8415, val_MinusLogProbMetric: 28.8415

Epoch 539: val_loss did not improve from 28.73765
196/196 - 42s - loss: 28.5682 - MinusLogProbMetric: 28.5682 - val_loss: 28.8415 - val_MinusLogProbMetric: 28.8415 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 540/1000
2023-10-26 07:01:56.049 
Epoch 540/1000 
	 loss: 28.7425, MinusLogProbMetric: 28.7425, val_loss: 29.1212, val_MinusLogProbMetric: 29.1212

Epoch 540: val_loss did not improve from 28.73765
196/196 - 43s - loss: 28.7425 - MinusLogProbMetric: 28.7425 - val_loss: 29.1212 - val_MinusLogProbMetric: 29.1212 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 541/1000
2023-10-26 07:02:39.006 
Epoch 541/1000 
	 loss: 28.5919, MinusLogProbMetric: 28.5919, val_loss: 28.7792, val_MinusLogProbMetric: 28.7792

Epoch 541: val_loss did not improve from 28.73765
196/196 - 43s - loss: 28.5919 - MinusLogProbMetric: 28.5919 - val_loss: 28.7792 - val_MinusLogProbMetric: 28.7792 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 542/1000
2023-10-26 07:03:21.774 
Epoch 542/1000 
	 loss: 28.6482, MinusLogProbMetric: 28.6482, val_loss: 29.6800, val_MinusLogProbMetric: 29.6800

Epoch 542: val_loss did not improve from 28.73765
196/196 - 43s - loss: 28.6482 - MinusLogProbMetric: 28.6482 - val_loss: 29.6800 - val_MinusLogProbMetric: 29.6800 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 543/1000
2023-10-26 07:04:04.895 
Epoch 543/1000 
	 loss: 28.7070, MinusLogProbMetric: 28.7070, val_loss: 28.8039, val_MinusLogProbMetric: 28.8039

Epoch 543: val_loss did not improve from 28.73765
196/196 - 43s - loss: 28.7070 - MinusLogProbMetric: 28.7070 - val_loss: 28.8039 - val_MinusLogProbMetric: 28.8039 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 544/1000
2023-10-26 07:04:47.759 
Epoch 544/1000 
	 loss: 28.7501, MinusLogProbMetric: 28.7501, val_loss: 29.1435, val_MinusLogProbMetric: 29.1435

Epoch 544: val_loss did not improve from 28.73765
196/196 - 43s - loss: 28.7501 - MinusLogProbMetric: 28.7501 - val_loss: 29.1435 - val_MinusLogProbMetric: 29.1435 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 545/1000
2023-10-26 07:05:30.604 
Epoch 545/1000 
	 loss: 28.5876, MinusLogProbMetric: 28.5876, val_loss: 28.8154, val_MinusLogProbMetric: 28.8154

Epoch 545: val_loss did not improve from 28.73765
196/196 - 43s - loss: 28.5876 - MinusLogProbMetric: 28.5876 - val_loss: 28.8154 - val_MinusLogProbMetric: 28.8154 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 546/1000
2023-10-26 07:06:13.242 
Epoch 546/1000 
	 loss: 28.6082, MinusLogProbMetric: 28.6082, val_loss: 28.8140, val_MinusLogProbMetric: 28.8140

Epoch 546: val_loss did not improve from 28.73765
196/196 - 43s - loss: 28.6082 - MinusLogProbMetric: 28.6082 - val_loss: 28.8140 - val_MinusLogProbMetric: 28.8140 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 547/1000
2023-10-26 07:06:55.808 
Epoch 547/1000 
	 loss: 28.5491, MinusLogProbMetric: 28.5491, val_loss: 28.6917, val_MinusLogProbMetric: 28.6917

Epoch 547: val_loss improved from 28.73765 to 28.69170, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 28.5491 - MinusLogProbMetric: 28.5491 - val_loss: 28.6917 - val_MinusLogProbMetric: 28.6917 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 548/1000
2023-10-26 07:07:39.104 
Epoch 548/1000 
	 loss: 28.7128, MinusLogProbMetric: 28.7128, val_loss: 30.6152, val_MinusLogProbMetric: 30.6152

Epoch 548: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.7128 - MinusLogProbMetric: 28.7128 - val_loss: 30.6152 - val_MinusLogProbMetric: 30.6152 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 549/1000
2023-10-26 07:08:21.776 
Epoch 549/1000 
	 loss: 28.6617, MinusLogProbMetric: 28.6617, val_loss: 29.3168, val_MinusLogProbMetric: 29.3168

Epoch 549: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.6617 - MinusLogProbMetric: 28.6617 - val_loss: 29.3168 - val_MinusLogProbMetric: 29.3168 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 550/1000
2023-10-26 07:09:04.655 
Epoch 550/1000 
	 loss: 28.6232, MinusLogProbMetric: 28.6232, val_loss: 28.9591, val_MinusLogProbMetric: 28.9591

Epoch 550: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.6232 - MinusLogProbMetric: 28.6232 - val_loss: 28.9591 - val_MinusLogProbMetric: 28.9591 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 551/1000
2023-10-26 07:09:47.455 
Epoch 551/1000 
	 loss: 28.5310, MinusLogProbMetric: 28.5310, val_loss: 29.4145, val_MinusLogProbMetric: 29.4145

Epoch 551: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.5310 - MinusLogProbMetric: 28.5310 - val_loss: 29.4145 - val_MinusLogProbMetric: 29.4145 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 552/1000
2023-10-26 07:10:30.462 
Epoch 552/1000 
	 loss: 28.7540, MinusLogProbMetric: 28.7540, val_loss: 28.8983, val_MinusLogProbMetric: 28.8983

Epoch 552: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.7540 - MinusLogProbMetric: 28.7540 - val_loss: 28.8983 - val_MinusLogProbMetric: 28.8983 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 553/1000
2023-10-26 07:11:13.024 
Epoch 553/1000 
	 loss: 28.6677, MinusLogProbMetric: 28.6677, val_loss: 28.8883, val_MinusLogProbMetric: 28.8883

Epoch 553: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.6677 - MinusLogProbMetric: 28.6677 - val_loss: 28.8883 - val_MinusLogProbMetric: 28.8883 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 554/1000
2023-10-26 07:11:55.843 
Epoch 554/1000 
	 loss: 28.6946, MinusLogProbMetric: 28.6946, val_loss: 30.0672, val_MinusLogProbMetric: 30.0672

Epoch 554: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.6946 - MinusLogProbMetric: 28.6946 - val_loss: 30.0672 - val_MinusLogProbMetric: 30.0672 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 555/1000
2023-10-26 07:12:38.955 
Epoch 555/1000 
	 loss: 28.5934, MinusLogProbMetric: 28.5934, val_loss: 29.6267, val_MinusLogProbMetric: 29.6267

Epoch 555: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.5934 - MinusLogProbMetric: 28.5934 - val_loss: 29.6267 - val_MinusLogProbMetric: 29.6267 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 556/1000
2023-10-26 07:13:21.757 
Epoch 556/1000 
	 loss: 28.7970, MinusLogProbMetric: 28.7970, val_loss: 28.9499, val_MinusLogProbMetric: 28.9499

Epoch 556: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.7970 - MinusLogProbMetric: 28.7970 - val_loss: 28.9499 - val_MinusLogProbMetric: 28.9499 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 557/1000
2023-10-26 07:14:04.711 
Epoch 557/1000 
	 loss: 28.6616, MinusLogProbMetric: 28.6616, val_loss: 28.8429, val_MinusLogProbMetric: 28.8429

Epoch 557: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.6616 - MinusLogProbMetric: 28.6616 - val_loss: 28.8429 - val_MinusLogProbMetric: 28.8429 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 558/1000
2023-10-26 07:14:47.690 
Epoch 558/1000 
	 loss: 28.5539, MinusLogProbMetric: 28.5539, val_loss: 29.5309, val_MinusLogProbMetric: 29.5309

Epoch 558: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.5539 - MinusLogProbMetric: 28.5539 - val_loss: 29.5309 - val_MinusLogProbMetric: 29.5309 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 559/1000
2023-10-26 07:15:30.584 
Epoch 559/1000 
	 loss: 28.6735, MinusLogProbMetric: 28.6735, val_loss: 28.8585, val_MinusLogProbMetric: 28.8585

Epoch 559: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.6735 - MinusLogProbMetric: 28.6735 - val_loss: 28.8585 - val_MinusLogProbMetric: 28.8585 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 560/1000
2023-10-26 07:16:13.577 
Epoch 560/1000 
	 loss: 28.5597, MinusLogProbMetric: 28.5597, val_loss: 28.8214, val_MinusLogProbMetric: 28.8214

Epoch 560: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.5597 - MinusLogProbMetric: 28.5597 - val_loss: 28.8214 - val_MinusLogProbMetric: 28.8214 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 561/1000
2023-10-26 07:16:56.320 
Epoch 561/1000 
	 loss: 28.6570, MinusLogProbMetric: 28.6570, val_loss: 28.9475, val_MinusLogProbMetric: 28.9475

Epoch 561: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.6570 - MinusLogProbMetric: 28.6570 - val_loss: 28.9475 - val_MinusLogProbMetric: 28.9475 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 562/1000
2023-10-26 07:17:39.287 
Epoch 562/1000 
	 loss: 28.5907, MinusLogProbMetric: 28.5907, val_loss: 28.9535, val_MinusLogProbMetric: 28.9535

Epoch 562: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.5907 - MinusLogProbMetric: 28.5907 - val_loss: 28.9535 - val_MinusLogProbMetric: 28.9535 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 563/1000
2023-10-26 07:18:22.286 
Epoch 563/1000 
	 loss: 28.6150, MinusLogProbMetric: 28.6150, val_loss: 29.3092, val_MinusLogProbMetric: 29.3092

Epoch 563: val_loss did not improve from 28.69170
196/196 - 43s - loss: 28.6150 - MinusLogProbMetric: 28.6150 - val_loss: 29.3092 - val_MinusLogProbMetric: 29.3092 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 564/1000
2023-10-26 07:19:05.522 
Epoch 564/1000 
	 loss: 28.6212, MinusLogProbMetric: 28.6212, val_loss: 28.5758, val_MinusLogProbMetric: 28.5758

Epoch 564: val_loss improved from 28.69170 to 28.57581, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 28.6212 - MinusLogProbMetric: 28.6212 - val_loss: 28.5758 - val_MinusLogProbMetric: 28.5758 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 565/1000
2023-10-26 07:19:48.815 
Epoch 565/1000 
	 loss: 28.7097, MinusLogProbMetric: 28.7097, val_loss: 29.4617, val_MinusLogProbMetric: 29.4617

Epoch 565: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.7097 - MinusLogProbMetric: 28.7097 - val_loss: 29.4617 - val_MinusLogProbMetric: 29.4617 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 566/1000
2023-10-26 07:20:31.929 
Epoch 566/1000 
	 loss: 28.5311, MinusLogProbMetric: 28.5311, val_loss: 29.1868, val_MinusLogProbMetric: 29.1868

Epoch 566: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5311 - MinusLogProbMetric: 28.5311 - val_loss: 29.1868 - val_MinusLogProbMetric: 29.1868 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 567/1000
2023-10-26 07:21:14.803 
Epoch 567/1000 
	 loss: 28.7265, MinusLogProbMetric: 28.7265, val_loss: 28.9306, val_MinusLogProbMetric: 28.9306

Epoch 567: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.7265 - MinusLogProbMetric: 28.7265 - val_loss: 28.9306 - val_MinusLogProbMetric: 28.9306 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 568/1000
2023-10-26 07:21:57.897 
Epoch 568/1000 
	 loss: 28.6722, MinusLogProbMetric: 28.6722, val_loss: 29.4711, val_MinusLogProbMetric: 29.4711

Epoch 568: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.6722 - MinusLogProbMetric: 28.6722 - val_loss: 29.4711 - val_MinusLogProbMetric: 29.4711 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 569/1000
2023-10-26 07:22:40.691 
Epoch 569/1000 
	 loss: 28.5222, MinusLogProbMetric: 28.5222, val_loss: 28.9398, val_MinusLogProbMetric: 28.9398

Epoch 569: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5222 - MinusLogProbMetric: 28.5222 - val_loss: 28.9398 - val_MinusLogProbMetric: 28.9398 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 570/1000
2023-10-26 07:23:23.561 
Epoch 570/1000 
	 loss: 28.7130, MinusLogProbMetric: 28.7130, val_loss: 29.2255, val_MinusLogProbMetric: 29.2255

Epoch 570: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.7130 - MinusLogProbMetric: 28.7130 - val_loss: 29.2255 - val_MinusLogProbMetric: 29.2255 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 571/1000
2023-10-26 07:24:06.320 
Epoch 571/1000 
	 loss: 28.5971, MinusLogProbMetric: 28.5971, val_loss: 28.8191, val_MinusLogProbMetric: 28.8191

Epoch 571: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5971 - MinusLogProbMetric: 28.5971 - val_loss: 28.8191 - val_MinusLogProbMetric: 28.8191 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 572/1000
2023-10-26 07:24:49.018 
Epoch 572/1000 
	 loss: 28.6558, MinusLogProbMetric: 28.6558, val_loss: 28.8977, val_MinusLogProbMetric: 28.8977

Epoch 572: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.6558 - MinusLogProbMetric: 28.6558 - val_loss: 28.8977 - val_MinusLogProbMetric: 28.8977 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 573/1000
2023-10-26 07:25:31.777 
Epoch 573/1000 
	 loss: 28.5700, MinusLogProbMetric: 28.5700, val_loss: 28.7566, val_MinusLogProbMetric: 28.7566

Epoch 573: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5700 - MinusLogProbMetric: 28.5700 - val_loss: 28.7566 - val_MinusLogProbMetric: 28.7566 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 574/1000
2023-10-26 07:26:14.778 
Epoch 574/1000 
	 loss: 28.6938, MinusLogProbMetric: 28.6938, val_loss: 29.1473, val_MinusLogProbMetric: 29.1473

Epoch 574: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.6938 - MinusLogProbMetric: 28.6938 - val_loss: 29.1473 - val_MinusLogProbMetric: 29.1473 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 575/1000
2023-10-26 07:26:57.860 
Epoch 575/1000 
	 loss: 28.4865, MinusLogProbMetric: 28.4865, val_loss: 29.2572, val_MinusLogProbMetric: 29.2572

Epoch 575: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.4865 - MinusLogProbMetric: 28.4865 - val_loss: 29.2572 - val_MinusLogProbMetric: 29.2572 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 576/1000
2023-10-26 07:27:40.876 
Epoch 576/1000 
	 loss: 28.5269, MinusLogProbMetric: 28.5269, val_loss: 29.1484, val_MinusLogProbMetric: 29.1484

Epoch 576: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5269 - MinusLogProbMetric: 28.5269 - val_loss: 29.1484 - val_MinusLogProbMetric: 29.1484 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 577/1000
2023-10-26 07:28:23.425 
Epoch 577/1000 
	 loss: 28.6271, MinusLogProbMetric: 28.6271, val_loss: 28.8840, val_MinusLogProbMetric: 28.8840

Epoch 577: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.6271 - MinusLogProbMetric: 28.6271 - val_loss: 28.8840 - val_MinusLogProbMetric: 28.8840 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 578/1000
2023-10-26 07:29:06.768 
Epoch 578/1000 
	 loss: 28.7464, MinusLogProbMetric: 28.7464, val_loss: 30.4213, val_MinusLogProbMetric: 30.4213

Epoch 578: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.7464 - MinusLogProbMetric: 28.7464 - val_loss: 30.4213 - val_MinusLogProbMetric: 30.4213 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 579/1000
2023-10-26 07:29:49.558 
Epoch 579/1000 
	 loss: 28.5781, MinusLogProbMetric: 28.5781, val_loss: 28.5921, val_MinusLogProbMetric: 28.5921

Epoch 579: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5781 - MinusLogProbMetric: 28.5781 - val_loss: 28.5921 - val_MinusLogProbMetric: 28.5921 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 580/1000
2023-10-26 07:30:31.741 
Epoch 580/1000 
	 loss: 28.6401, MinusLogProbMetric: 28.6401, val_loss: 28.8817, val_MinusLogProbMetric: 28.8817

Epoch 580: val_loss did not improve from 28.57581
196/196 - 42s - loss: 28.6401 - MinusLogProbMetric: 28.6401 - val_loss: 28.8817 - val_MinusLogProbMetric: 28.8817 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 581/1000
2023-10-26 07:31:11.845 
Epoch 581/1000 
	 loss: 28.5611, MinusLogProbMetric: 28.5611, val_loss: 29.0596, val_MinusLogProbMetric: 29.0596

Epoch 581: val_loss did not improve from 28.57581
196/196 - 40s - loss: 28.5611 - MinusLogProbMetric: 28.5611 - val_loss: 29.0596 - val_MinusLogProbMetric: 29.0596 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 582/1000
2023-10-26 07:31:51.538 
Epoch 582/1000 
	 loss: 28.5296, MinusLogProbMetric: 28.5296, val_loss: 28.6997, val_MinusLogProbMetric: 28.6997

Epoch 582: val_loss did not improve from 28.57581
196/196 - 40s - loss: 28.5296 - MinusLogProbMetric: 28.5296 - val_loss: 28.6997 - val_MinusLogProbMetric: 28.6997 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 583/1000
2023-10-26 07:32:34.414 
Epoch 583/1000 
	 loss: 28.6005, MinusLogProbMetric: 28.6005, val_loss: 29.5346, val_MinusLogProbMetric: 29.5346

Epoch 583: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.6005 - MinusLogProbMetric: 28.6005 - val_loss: 29.5346 - val_MinusLogProbMetric: 29.5346 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 584/1000
2023-10-26 07:33:17.068 
Epoch 584/1000 
	 loss: 28.5749, MinusLogProbMetric: 28.5749, val_loss: 28.7569, val_MinusLogProbMetric: 28.7569

Epoch 584: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5749 - MinusLogProbMetric: 28.5749 - val_loss: 28.7569 - val_MinusLogProbMetric: 28.7569 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 585/1000
2023-10-26 07:33:59.454 
Epoch 585/1000 
	 loss: 28.5335, MinusLogProbMetric: 28.5335, val_loss: 30.2384, val_MinusLogProbMetric: 30.2384

Epoch 585: val_loss did not improve from 28.57581
196/196 - 42s - loss: 28.5335 - MinusLogProbMetric: 28.5335 - val_loss: 30.2384 - val_MinusLogProbMetric: 30.2384 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 586/1000
2023-10-26 07:34:41.845 
Epoch 586/1000 
	 loss: 28.6131, MinusLogProbMetric: 28.6131, val_loss: 29.5273, val_MinusLogProbMetric: 29.5273

Epoch 586: val_loss did not improve from 28.57581
196/196 - 42s - loss: 28.6131 - MinusLogProbMetric: 28.6131 - val_loss: 29.5273 - val_MinusLogProbMetric: 29.5273 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 587/1000
2023-10-26 07:35:24.539 
Epoch 587/1000 
	 loss: 28.6124, MinusLogProbMetric: 28.6124, val_loss: 28.9211, val_MinusLogProbMetric: 28.9211

Epoch 587: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.6124 - MinusLogProbMetric: 28.6124 - val_loss: 28.9211 - val_MinusLogProbMetric: 28.9211 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 588/1000
2023-10-26 07:36:07.186 
Epoch 588/1000 
	 loss: 28.6201, MinusLogProbMetric: 28.6201, val_loss: 28.7758, val_MinusLogProbMetric: 28.7758

Epoch 588: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.6201 - MinusLogProbMetric: 28.6201 - val_loss: 28.7758 - val_MinusLogProbMetric: 28.7758 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 589/1000
2023-10-26 07:36:49.389 
Epoch 589/1000 
	 loss: 28.5091, MinusLogProbMetric: 28.5091, val_loss: 28.8849, val_MinusLogProbMetric: 28.8849

Epoch 589: val_loss did not improve from 28.57581
196/196 - 42s - loss: 28.5091 - MinusLogProbMetric: 28.5091 - val_loss: 28.8849 - val_MinusLogProbMetric: 28.8849 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 590/1000
2023-10-26 07:37:32.430 
Epoch 590/1000 
	 loss: 28.5922, MinusLogProbMetric: 28.5922, val_loss: 28.6966, val_MinusLogProbMetric: 28.6966

Epoch 590: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5922 - MinusLogProbMetric: 28.5922 - val_loss: 28.6966 - val_MinusLogProbMetric: 28.6966 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 591/1000
2023-10-26 07:38:14.970 
Epoch 591/1000 
	 loss: 28.6280, MinusLogProbMetric: 28.6280, val_loss: 28.6552, val_MinusLogProbMetric: 28.6552

Epoch 591: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.6280 - MinusLogProbMetric: 28.6280 - val_loss: 28.6552 - val_MinusLogProbMetric: 28.6552 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 592/1000
2023-10-26 07:38:57.499 
Epoch 592/1000 
	 loss: 28.7179, MinusLogProbMetric: 28.7179, val_loss: 28.9120, val_MinusLogProbMetric: 28.9120

Epoch 592: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.7179 - MinusLogProbMetric: 28.7179 - val_loss: 28.9120 - val_MinusLogProbMetric: 28.9120 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 593/1000
2023-10-26 07:39:39.998 
Epoch 593/1000 
	 loss: 28.5308, MinusLogProbMetric: 28.5308, val_loss: 29.0226, val_MinusLogProbMetric: 29.0226

Epoch 593: val_loss did not improve from 28.57581
196/196 - 42s - loss: 28.5308 - MinusLogProbMetric: 28.5308 - val_loss: 29.0226 - val_MinusLogProbMetric: 29.0226 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 594/1000
2023-10-26 07:40:21.903 
Epoch 594/1000 
	 loss: 28.4980, MinusLogProbMetric: 28.4980, val_loss: 29.4854, val_MinusLogProbMetric: 29.4854

Epoch 594: val_loss did not improve from 28.57581
196/196 - 42s - loss: 28.4980 - MinusLogProbMetric: 28.4980 - val_loss: 29.4854 - val_MinusLogProbMetric: 29.4854 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 595/1000
2023-10-26 07:41:04.676 
Epoch 595/1000 
	 loss: 28.5823, MinusLogProbMetric: 28.5823, val_loss: 28.6691, val_MinusLogProbMetric: 28.6691

Epoch 595: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5823 - MinusLogProbMetric: 28.5823 - val_loss: 28.6691 - val_MinusLogProbMetric: 28.6691 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 596/1000
2023-10-26 07:41:47.422 
Epoch 596/1000 
	 loss: 28.6229, MinusLogProbMetric: 28.6229, val_loss: 29.3414, val_MinusLogProbMetric: 29.3414

Epoch 596: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.6229 - MinusLogProbMetric: 28.6229 - val_loss: 29.3414 - val_MinusLogProbMetric: 29.3414 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 597/1000
2023-10-26 07:42:30.035 
Epoch 597/1000 
	 loss: 28.7528, MinusLogProbMetric: 28.7528, val_loss: 28.9623, val_MinusLogProbMetric: 28.9623

Epoch 597: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.7528 - MinusLogProbMetric: 28.7528 - val_loss: 28.9623 - val_MinusLogProbMetric: 28.9623 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 598/1000
2023-10-26 07:43:12.612 
Epoch 598/1000 
	 loss: 28.4520, MinusLogProbMetric: 28.4520, val_loss: 28.6932, val_MinusLogProbMetric: 28.6932

Epoch 598: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.4520 - MinusLogProbMetric: 28.4520 - val_loss: 28.6932 - val_MinusLogProbMetric: 28.6932 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 599/1000
2023-10-26 07:43:55.267 
Epoch 599/1000 
	 loss: 28.5987, MinusLogProbMetric: 28.5987, val_loss: 28.9450, val_MinusLogProbMetric: 28.9450

Epoch 599: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5987 - MinusLogProbMetric: 28.5987 - val_loss: 28.9450 - val_MinusLogProbMetric: 28.9450 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 600/1000
2023-10-26 07:44:37.810 
Epoch 600/1000 
	 loss: 28.5753, MinusLogProbMetric: 28.5753, val_loss: 29.1533, val_MinusLogProbMetric: 29.1533

Epoch 600: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5753 - MinusLogProbMetric: 28.5753 - val_loss: 29.1533 - val_MinusLogProbMetric: 29.1533 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 601/1000
2023-10-26 07:45:20.577 
Epoch 601/1000 
	 loss: 28.5309, MinusLogProbMetric: 28.5309, val_loss: 28.8715, val_MinusLogProbMetric: 28.8715

Epoch 601: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5309 - MinusLogProbMetric: 28.5309 - val_loss: 28.8715 - val_MinusLogProbMetric: 28.8715 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 602/1000
2023-10-26 07:46:03.171 
Epoch 602/1000 
	 loss: 28.5623, MinusLogProbMetric: 28.5623, val_loss: 29.4313, val_MinusLogProbMetric: 29.4313

Epoch 602: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5623 - MinusLogProbMetric: 28.5623 - val_loss: 29.4313 - val_MinusLogProbMetric: 29.4313 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 603/1000
2023-10-26 07:46:45.835 
Epoch 603/1000 
	 loss: 28.5636, MinusLogProbMetric: 28.5636, val_loss: 29.1404, val_MinusLogProbMetric: 29.1404

Epoch 603: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5636 - MinusLogProbMetric: 28.5636 - val_loss: 29.1404 - val_MinusLogProbMetric: 29.1404 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 604/1000
2023-10-26 07:47:28.113 
Epoch 604/1000 
	 loss: 28.5830, MinusLogProbMetric: 28.5830, val_loss: 28.6722, val_MinusLogProbMetric: 28.6722

Epoch 604: val_loss did not improve from 28.57581
196/196 - 42s - loss: 28.5830 - MinusLogProbMetric: 28.5830 - val_loss: 28.6722 - val_MinusLogProbMetric: 28.6722 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 605/1000
2023-10-26 07:48:10.656 
Epoch 605/1000 
	 loss: 28.5247, MinusLogProbMetric: 28.5247, val_loss: 28.7406, val_MinusLogProbMetric: 28.7406

Epoch 605: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5247 - MinusLogProbMetric: 28.5247 - val_loss: 28.7406 - val_MinusLogProbMetric: 28.7406 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 606/1000
2023-10-26 07:48:53.413 
Epoch 606/1000 
	 loss: 28.6434, MinusLogProbMetric: 28.6434, val_loss: 28.6709, val_MinusLogProbMetric: 28.6709

Epoch 606: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.6434 - MinusLogProbMetric: 28.6434 - val_loss: 28.6709 - val_MinusLogProbMetric: 28.6709 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 607/1000
2023-10-26 07:49:35.950 
Epoch 607/1000 
	 loss: 28.5657, MinusLogProbMetric: 28.5657, val_loss: 29.2691, val_MinusLogProbMetric: 29.2691

Epoch 607: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5657 - MinusLogProbMetric: 28.5657 - val_loss: 29.2691 - val_MinusLogProbMetric: 29.2691 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 608/1000
2023-10-26 07:50:18.126 
Epoch 608/1000 
	 loss: 28.5336, MinusLogProbMetric: 28.5336, val_loss: 29.4397, val_MinusLogProbMetric: 29.4397

Epoch 608: val_loss did not improve from 28.57581
196/196 - 42s - loss: 28.5336 - MinusLogProbMetric: 28.5336 - val_loss: 29.4397 - val_MinusLogProbMetric: 29.4397 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 609/1000
2023-10-26 07:50:59.165 
Epoch 609/1000 
	 loss: 28.6134, MinusLogProbMetric: 28.6134, val_loss: 28.9234, val_MinusLogProbMetric: 28.9234

Epoch 609: val_loss did not improve from 28.57581
196/196 - 41s - loss: 28.6134 - MinusLogProbMetric: 28.6134 - val_loss: 28.9234 - val_MinusLogProbMetric: 28.9234 - lr: 0.0010 - 41s/epoch - 209ms/step
Epoch 610/1000
2023-10-26 07:51:39.287 
Epoch 610/1000 
	 loss: 28.5495, MinusLogProbMetric: 28.5495, val_loss: 28.8068, val_MinusLogProbMetric: 28.8068

Epoch 610: val_loss did not improve from 28.57581
196/196 - 40s - loss: 28.5495 - MinusLogProbMetric: 28.5495 - val_loss: 28.8068 - val_MinusLogProbMetric: 28.8068 - lr: 0.0010 - 40s/epoch - 205ms/step
Epoch 611/1000
2023-10-26 07:52:21.133 
Epoch 611/1000 
	 loss: 28.5515, MinusLogProbMetric: 28.5515, val_loss: 28.7923, val_MinusLogProbMetric: 28.7923

Epoch 611: val_loss did not improve from 28.57581
196/196 - 42s - loss: 28.5515 - MinusLogProbMetric: 28.5515 - val_loss: 28.7923 - val_MinusLogProbMetric: 28.7923 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 612/1000
2023-10-26 07:53:02.781 
Epoch 612/1000 
	 loss: 28.5584, MinusLogProbMetric: 28.5584, val_loss: 30.3945, val_MinusLogProbMetric: 30.3945

Epoch 612: val_loss did not improve from 28.57581
196/196 - 42s - loss: 28.5584 - MinusLogProbMetric: 28.5584 - val_loss: 30.3945 - val_MinusLogProbMetric: 30.3945 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 613/1000
2023-10-26 07:53:45.484 
Epoch 613/1000 
	 loss: 28.5580, MinusLogProbMetric: 28.5580, val_loss: 29.3468, val_MinusLogProbMetric: 29.3468

Epoch 613: val_loss did not improve from 28.57581
196/196 - 43s - loss: 28.5580 - MinusLogProbMetric: 28.5580 - val_loss: 29.3468 - val_MinusLogProbMetric: 29.3468 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 614/1000
2023-10-26 07:54:27.946 
Epoch 614/1000 
	 loss: 28.5469, MinusLogProbMetric: 28.5469, val_loss: 29.0955, val_MinusLogProbMetric: 29.0955

Epoch 614: val_loss did not improve from 28.57581
196/196 - 42s - loss: 28.5469 - MinusLogProbMetric: 28.5469 - val_loss: 29.0955 - val_MinusLogProbMetric: 29.0955 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 615/1000
2023-10-26 07:55:10.311 
Epoch 615/1000 
	 loss: 27.9569, MinusLogProbMetric: 27.9569, val_loss: 28.3353, val_MinusLogProbMetric: 28.3353

Epoch 615: val_loss improved from 28.57581 to 28.33534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.9569 - MinusLogProbMetric: 27.9569 - val_loss: 28.3353 - val_MinusLogProbMetric: 28.3353 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 616/1000
2023-10-26 07:55:53.569 
Epoch 616/1000 
	 loss: 27.9560, MinusLogProbMetric: 27.9560, val_loss: 28.8125, val_MinusLogProbMetric: 28.8125

Epoch 616: val_loss did not improve from 28.33534
196/196 - 43s - loss: 27.9560 - MinusLogProbMetric: 27.9560 - val_loss: 28.8125 - val_MinusLogProbMetric: 28.8125 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 617/1000
2023-10-26 07:56:36.518 
Epoch 617/1000 
	 loss: 27.9156, MinusLogProbMetric: 27.9156, val_loss: 28.8929, val_MinusLogProbMetric: 28.8929

Epoch 617: val_loss did not improve from 28.33534
196/196 - 43s - loss: 27.9156 - MinusLogProbMetric: 27.9156 - val_loss: 28.8929 - val_MinusLogProbMetric: 28.8929 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 618/1000
2023-10-26 07:57:19.178 
Epoch 618/1000 
	 loss: 27.9770, MinusLogProbMetric: 27.9770, val_loss: 28.3912, val_MinusLogProbMetric: 28.3912

Epoch 618: val_loss did not improve from 28.33534
196/196 - 43s - loss: 27.9770 - MinusLogProbMetric: 27.9770 - val_loss: 28.3912 - val_MinusLogProbMetric: 28.3912 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 619/1000
2023-10-26 07:58:01.185 
Epoch 619/1000 
	 loss: 27.9670, MinusLogProbMetric: 27.9670, val_loss: 28.3100, val_MinusLogProbMetric: 28.3100

Epoch 619: val_loss improved from 28.33534 to 28.31000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.9670 - MinusLogProbMetric: 27.9670 - val_loss: 28.3100 - val_MinusLogProbMetric: 28.3100 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 620/1000
2023-10-26 07:58:38.266 
Epoch 620/1000 
	 loss: 28.0064, MinusLogProbMetric: 28.0064, val_loss: 28.4234, val_MinusLogProbMetric: 28.4234

Epoch 620: val_loss did not improve from 28.31000
196/196 - 37s - loss: 28.0064 - MinusLogProbMetric: 28.0064 - val_loss: 28.4234 - val_MinusLogProbMetric: 28.4234 - lr: 5.0000e-04 - 37s/epoch - 186ms/step
Epoch 621/1000
2023-10-26 07:59:15.038 
Epoch 621/1000 
	 loss: 27.9266, MinusLogProbMetric: 27.9266, val_loss: 28.2982, val_MinusLogProbMetric: 28.2982

Epoch 621: val_loss improved from 28.31000 to 28.29823, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 37s - loss: 27.9266 - MinusLogProbMetric: 27.9266 - val_loss: 28.2982 - val_MinusLogProbMetric: 28.2982 - lr: 5.0000e-04 - 37s/epoch - 191ms/step
Epoch 622/1000
2023-10-26 07:59:52.332 
Epoch 622/1000 
	 loss: 27.9737, MinusLogProbMetric: 27.9737, val_loss: 28.3263, val_MinusLogProbMetric: 28.3263

Epoch 622: val_loss did not improve from 28.29823
196/196 - 37s - loss: 27.9737 - MinusLogProbMetric: 27.9737 - val_loss: 28.3263 - val_MinusLogProbMetric: 28.3263 - lr: 5.0000e-04 - 37s/epoch - 187ms/step
Epoch 623/1000
2023-10-26 08:00:32.488 
Epoch 623/1000 
	 loss: 27.9285, MinusLogProbMetric: 27.9285, val_loss: 28.2663, val_MinusLogProbMetric: 28.2663

Epoch 623: val_loss improved from 28.29823 to 28.26629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 41s - loss: 27.9285 - MinusLogProbMetric: 27.9285 - val_loss: 28.2663 - val_MinusLogProbMetric: 28.2663 - lr: 5.0000e-04 - 41s/epoch - 208ms/step
Epoch 624/1000
2023-10-26 08:01:15.482 
Epoch 624/1000 
	 loss: 27.9546, MinusLogProbMetric: 27.9546, val_loss: 28.2221, val_MinusLogProbMetric: 28.2221

Epoch 624: val_loss improved from 28.26629 to 28.22205, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.9546 - MinusLogProbMetric: 27.9546 - val_loss: 28.2221 - val_MinusLogProbMetric: 28.2221 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 625/1000
2023-10-26 08:01:57.808 
Epoch 625/1000 
	 loss: 27.9476, MinusLogProbMetric: 27.9476, val_loss: 28.4605, val_MinusLogProbMetric: 28.4605

Epoch 625: val_loss did not improve from 28.22205
196/196 - 42s - loss: 27.9476 - MinusLogProbMetric: 27.9476 - val_loss: 28.4605 - val_MinusLogProbMetric: 28.4605 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 626/1000
2023-10-26 08:02:40.496 
Epoch 626/1000 
	 loss: 28.0041, MinusLogProbMetric: 28.0041, val_loss: 28.8141, val_MinusLogProbMetric: 28.8141

Epoch 626: val_loss did not improve from 28.22205
196/196 - 43s - loss: 28.0041 - MinusLogProbMetric: 28.0041 - val_loss: 28.8141 - val_MinusLogProbMetric: 28.8141 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 627/1000
2023-10-26 08:03:22.787 
Epoch 627/1000 
	 loss: 28.0111, MinusLogProbMetric: 28.0111, val_loss: 28.4078, val_MinusLogProbMetric: 28.4078

Epoch 627: val_loss did not improve from 28.22205
196/196 - 42s - loss: 28.0111 - MinusLogProbMetric: 28.0111 - val_loss: 28.4078 - val_MinusLogProbMetric: 28.4078 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 628/1000
2023-10-26 08:04:05.077 
Epoch 628/1000 
	 loss: 28.0230, MinusLogProbMetric: 28.0230, val_loss: 28.3887, val_MinusLogProbMetric: 28.3887

Epoch 628: val_loss did not improve from 28.22205
196/196 - 42s - loss: 28.0230 - MinusLogProbMetric: 28.0230 - val_loss: 28.3887 - val_MinusLogProbMetric: 28.3887 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 629/1000
2023-10-26 08:04:47.604 
Epoch 629/1000 
	 loss: 27.9611, MinusLogProbMetric: 27.9611, val_loss: 28.3403, val_MinusLogProbMetric: 28.3403

Epoch 629: val_loss did not improve from 28.22205
196/196 - 43s - loss: 27.9611 - MinusLogProbMetric: 27.9611 - val_loss: 28.3403 - val_MinusLogProbMetric: 28.3403 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 630/1000
2023-10-26 08:05:29.861 
Epoch 630/1000 
	 loss: 27.9837, MinusLogProbMetric: 27.9837, val_loss: 28.3411, val_MinusLogProbMetric: 28.3411

Epoch 630: val_loss did not improve from 28.22205
196/196 - 42s - loss: 27.9837 - MinusLogProbMetric: 27.9837 - val_loss: 28.3411 - val_MinusLogProbMetric: 28.3411 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 631/1000
2023-10-26 08:06:12.489 
Epoch 631/1000 
	 loss: 27.9678, MinusLogProbMetric: 27.9678, val_loss: 28.8740, val_MinusLogProbMetric: 28.8740

Epoch 631: val_loss did not improve from 28.22205
196/196 - 43s - loss: 27.9678 - MinusLogProbMetric: 27.9678 - val_loss: 28.8740 - val_MinusLogProbMetric: 28.8740 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 632/1000
2023-10-26 08:06:54.830 
Epoch 632/1000 
	 loss: 27.9168, MinusLogProbMetric: 27.9168, val_loss: 28.7407, val_MinusLogProbMetric: 28.7407

Epoch 632: val_loss did not improve from 28.22205
196/196 - 42s - loss: 27.9168 - MinusLogProbMetric: 27.9168 - val_loss: 28.7407 - val_MinusLogProbMetric: 28.7407 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 633/1000
2023-10-26 08:07:37.313 
Epoch 633/1000 
	 loss: 28.0080, MinusLogProbMetric: 28.0080, val_loss: 28.1882, val_MinusLogProbMetric: 28.1882

Epoch 633: val_loss improved from 28.22205 to 28.18821, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 28.0080 - MinusLogProbMetric: 28.0080 - val_loss: 28.1882 - val_MinusLogProbMetric: 28.1882 - lr: 5.0000e-04 - 43s/epoch - 220ms/step
Epoch 634/1000
2023-10-26 08:08:20.971 
Epoch 634/1000 
	 loss: 28.0400, MinusLogProbMetric: 28.0400, val_loss: 28.6443, val_MinusLogProbMetric: 28.6443

Epoch 634: val_loss did not improve from 28.18821
196/196 - 43s - loss: 28.0400 - MinusLogProbMetric: 28.0400 - val_loss: 28.6443 - val_MinusLogProbMetric: 28.6443 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 635/1000
2023-10-26 08:09:03.969 
Epoch 635/1000 
	 loss: 27.9606, MinusLogProbMetric: 27.9606, val_loss: 28.2449, val_MinusLogProbMetric: 28.2449

Epoch 635: val_loss did not improve from 28.18821
196/196 - 43s - loss: 27.9606 - MinusLogProbMetric: 27.9606 - val_loss: 28.2449 - val_MinusLogProbMetric: 28.2449 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 636/1000
2023-10-26 08:09:46.826 
Epoch 636/1000 
	 loss: 27.9661, MinusLogProbMetric: 27.9661, val_loss: 28.3977, val_MinusLogProbMetric: 28.3977

Epoch 636: val_loss did not improve from 28.18821
196/196 - 43s - loss: 27.9661 - MinusLogProbMetric: 27.9661 - val_loss: 28.3977 - val_MinusLogProbMetric: 28.3977 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 637/1000
2023-10-26 08:10:25.920 
Epoch 637/1000 
	 loss: 27.9384, MinusLogProbMetric: 27.9384, val_loss: 28.5564, val_MinusLogProbMetric: 28.5564

Epoch 637: val_loss did not improve from 28.18821
196/196 - 39s - loss: 27.9384 - MinusLogProbMetric: 27.9384 - val_loss: 28.5564 - val_MinusLogProbMetric: 28.5564 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 638/1000
2023-10-26 08:11:03.487 
Epoch 638/1000 
	 loss: 27.9687, MinusLogProbMetric: 27.9687, val_loss: 28.4162, val_MinusLogProbMetric: 28.4162

Epoch 638: val_loss did not improve from 28.18821
196/196 - 38s - loss: 27.9687 - MinusLogProbMetric: 27.9687 - val_loss: 28.4162 - val_MinusLogProbMetric: 28.4162 - lr: 5.0000e-04 - 38s/epoch - 192ms/step
Epoch 639/1000
2023-10-26 08:11:43.130 
Epoch 639/1000 
	 loss: 27.9940, MinusLogProbMetric: 27.9940, val_loss: 28.5160, val_MinusLogProbMetric: 28.5160

Epoch 639: val_loss did not improve from 28.18821
196/196 - 40s - loss: 27.9940 - MinusLogProbMetric: 27.9940 - val_loss: 28.5160 - val_MinusLogProbMetric: 28.5160 - lr: 5.0000e-04 - 40s/epoch - 202ms/step
Epoch 640/1000
2023-10-26 08:12:21.205 
Epoch 640/1000 
	 loss: 27.9648, MinusLogProbMetric: 27.9648, val_loss: 28.4867, val_MinusLogProbMetric: 28.4867

Epoch 640: val_loss did not improve from 28.18821
196/196 - 38s - loss: 27.9648 - MinusLogProbMetric: 27.9648 - val_loss: 28.4867 - val_MinusLogProbMetric: 28.4867 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 641/1000
2023-10-26 08:13:01.040 
Epoch 641/1000 
	 loss: 27.9270, MinusLogProbMetric: 27.9270, val_loss: 28.6130, val_MinusLogProbMetric: 28.6130

Epoch 641: val_loss did not improve from 28.18821
196/196 - 40s - loss: 27.9270 - MinusLogProbMetric: 27.9270 - val_loss: 28.6130 - val_MinusLogProbMetric: 28.6130 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 642/1000
2023-10-26 08:13:38.973 
Epoch 642/1000 
	 loss: 27.8748, MinusLogProbMetric: 27.8748, val_loss: 28.3696, val_MinusLogProbMetric: 28.3696

Epoch 642: val_loss did not improve from 28.18821
196/196 - 38s - loss: 27.8748 - MinusLogProbMetric: 27.8748 - val_loss: 28.3696 - val_MinusLogProbMetric: 28.3696 - lr: 5.0000e-04 - 38s/epoch - 194ms/step
Epoch 643/1000
2023-10-26 08:14:19.271 
Epoch 643/1000 
	 loss: 28.0034, MinusLogProbMetric: 28.0034, val_loss: 28.4585, val_MinusLogProbMetric: 28.4585

Epoch 643: val_loss did not improve from 28.18821
196/196 - 40s - loss: 28.0034 - MinusLogProbMetric: 28.0034 - val_loss: 28.4585 - val_MinusLogProbMetric: 28.4585 - lr: 5.0000e-04 - 40s/epoch - 206ms/step
Epoch 644/1000
2023-10-26 08:14:59.824 
Epoch 644/1000 
	 loss: 27.9118, MinusLogProbMetric: 27.9118, val_loss: 28.3547, val_MinusLogProbMetric: 28.3547

Epoch 644: val_loss did not improve from 28.18821
196/196 - 41s - loss: 27.9118 - MinusLogProbMetric: 27.9118 - val_loss: 28.3547 - val_MinusLogProbMetric: 28.3547 - lr: 5.0000e-04 - 41s/epoch - 207ms/step
Epoch 645/1000
2023-10-26 08:15:37.447 
Epoch 645/1000 
	 loss: 27.9230, MinusLogProbMetric: 27.9230, val_loss: 28.3149, val_MinusLogProbMetric: 28.3149

Epoch 645: val_loss did not improve from 28.18821
196/196 - 38s - loss: 27.9230 - MinusLogProbMetric: 27.9230 - val_loss: 28.3149 - val_MinusLogProbMetric: 28.3149 - lr: 5.0000e-04 - 38s/epoch - 192ms/step
Epoch 646/1000
2023-10-26 08:16:19.129 
Epoch 646/1000 
	 loss: 28.0074, MinusLogProbMetric: 28.0074, val_loss: 28.2614, val_MinusLogProbMetric: 28.2614

Epoch 646: val_loss did not improve from 28.18821
196/196 - 42s - loss: 28.0074 - MinusLogProbMetric: 28.0074 - val_loss: 28.2614 - val_MinusLogProbMetric: 28.2614 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 647/1000
2023-10-26 08:16:55.985 
Epoch 647/1000 
	 loss: 27.9710, MinusLogProbMetric: 27.9710, val_loss: 28.2887, val_MinusLogProbMetric: 28.2887

Epoch 647: val_loss did not improve from 28.18821
196/196 - 37s - loss: 27.9710 - MinusLogProbMetric: 27.9710 - val_loss: 28.2887 - val_MinusLogProbMetric: 28.2887 - lr: 5.0000e-04 - 37s/epoch - 188ms/step
Epoch 648/1000
2023-10-26 08:17:35.081 
Epoch 648/1000 
	 loss: 27.8984, MinusLogProbMetric: 27.8984, val_loss: 28.2626, val_MinusLogProbMetric: 28.2626

Epoch 648: val_loss did not improve from 28.18821
196/196 - 39s - loss: 27.8984 - MinusLogProbMetric: 27.8984 - val_loss: 28.2626 - val_MinusLogProbMetric: 28.2626 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 649/1000
2023-10-26 08:18:15.104 
Epoch 649/1000 
	 loss: 27.9555, MinusLogProbMetric: 27.9555, val_loss: 28.4727, val_MinusLogProbMetric: 28.4727

Epoch 649: val_loss did not improve from 28.18821
196/196 - 40s - loss: 27.9555 - MinusLogProbMetric: 27.9555 - val_loss: 28.4727 - val_MinusLogProbMetric: 28.4727 - lr: 5.0000e-04 - 40s/epoch - 204ms/step
Epoch 650/1000
2023-10-26 08:18:53.446 
Epoch 650/1000 
	 loss: 27.9762, MinusLogProbMetric: 27.9762, val_loss: 28.3165, val_MinusLogProbMetric: 28.3165

Epoch 650: val_loss did not improve from 28.18821
196/196 - 38s - loss: 27.9762 - MinusLogProbMetric: 27.9762 - val_loss: 28.3165 - val_MinusLogProbMetric: 28.3165 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 651/1000
2023-10-26 08:19:36.040 
Epoch 651/1000 
	 loss: 27.9091, MinusLogProbMetric: 27.9091, val_loss: 28.2639, val_MinusLogProbMetric: 28.2639

Epoch 651: val_loss did not improve from 28.18821
196/196 - 43s - loss: 27.9091 - MinusLogProbMetric: 27.9091 - val_loss: 28.2639 - val_MinusLogProbMetric: 28.2639 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 652/1000
2023-10-26 08:20:12.557 
Epoch 652/1000 
	 loss: 27.9422, MinusLogProbMetric: 27.9422, val_loss: 28.9518, val_MinusLogProbMetric: 28.9518

Epoch 652: val_loss did not improve from 28.18821
196/196 - 37s - loss: 27.9422 - MinusLogProbMetric: 27.9422 - val_loss: 28.9518 - val_MinusLogProbMetric: 28.9518 - lr: 5.0000e-04 - 37s/epoch - 186ms/step
Epoch 653/1000
2023-10-26 08:20:54.327 
Epoch 653/1000 
	 loss: 27.9396, MinusLogProbMetric: 27.9396, val_loss: 28.2859, val_MinusLogProbMetric: 28.2859

Epoch 653: val_loss did not improve from 28.18821
196/196 - 42s - loss: 27.9396 - MinusLogProbMetric: 27.9396 - val_loss: 28.2859 - val_MinusLogProbMetric: 28.2859 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 654/1000
2023-10-26 08:21:32.667 
Epoch 654/1000 
	 loss: 27.9773, MinusLogProbMetric: 27.9773, val_loss: 28.5402, val_MinusLogProbMetric: 28.5402

Epoch 654: val_loss did not improve from 28.18821
196/196 - 38s - loss: 27.9773 - MinusLogProbMetric: 27.9773 - val_loss: 28.5402 - val_MinusLogProbMetric: 28.5402 - lr: 5.0000e-04 - 38s/epoch - 196ms/step
Epoch 655/1000
2023-10-26 08:22:09.782 
Epoch 655/1000 
	 loss: 27.9549, MinusLogProbMetric: 27.9549, val_loss: 28.3370, val_MinusLogProbMetric: 28.3370

Epoch 655: val_loss did not improve from 28.18821
196/196 - 37s - loss: 27.9549 - MinusLogProbMetric: 27.9549 - val_loss: 28.3370 - val_MinusLogProbMetric: 28.3370 - lr: 5.0000e-04 - 37s/epoch - 189ms/step
Epoch 656/1000
2023-10-26 08:22:51.021 
Epoch 656/1000 
	 loss: 27.8951, MinusLogProbMetric: 27.8951, val_loss: 28.7263, val_MinusLogProbMetric: 28.7263

Epoch 656: val_loss did not improve from 28.18821
196/196 - 41s - loss: 27.8951 - MinusLogProbMetric: 27.8951 - val_loss: 28.7263 - val_MinusLogProbMetric: 28.7263 - lr: 5.0000e-04 - 41s/epoch - 210ms/step
Epoch 657/1000
2023-10-26 08:23:29.306 
Epoch 657/1000 
	 loss: 27.9590, MinusLogProbMetric: 27.9590, val_loss: 28.4114, val_MinusLogProbMetric: 28.4114

Epoch 657: val_loss did not improve from 28.18821
196/196 - 38s - loss: 27.9590 - MinusLogProbMetric: 27.9590 - val_loss: 28.4114 - val_MinusLogProbMetric: 28.4114 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 658/1000
2023-10-26 08:24:07.497 
Epoch 658/1000 
	 loss: 27.8996, MinusLogProbMetric: 27.8996, val_loss: 28.2269, val_MinusLogProbMetric: 28.2269

Epoch 658: val_loss did not improve from 28.18821
196/196 - 38s - loss: 27.8996 - MinusLogProbMetric: 27.8996 - val_loss: 28.2269 - val_MinusLogProbMetric: 28.2269 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 659/1000
2023-10-26 08:24:46.623 
Epoch 659/1000 
	 loss: 27.9537, MinusLogProbMetric: 27.9537, val_loss: 28.4965, val_MinusLogProbMetric: 28.4965

Epoch 659: val_loss did not improve from 28.18821
196/196 - 39s - loss: 27.9537 - MinusLogProbMetric: 27.9537 - val_loss: 28.4965 - val_MinusLogProbMetric: 28.4965 - lr: 5.0000e-04 - 39s/epoch - 200ms/step
Epoch 660/1000
2023-10-26 08:25:25.704 
Epoch 660/1000 
	 loss: 27.9909, MinusLogProbMetric: 27.9909, val_loss: 28.3433, val_MinusLogProbMetric: 28.3433

Epoch 660: val_loss did not improve from 28.18821
196/196 - 39s - loss: 27.9909 - MinusLogProbMetric: 27.9909 - val_loss: 28.3433 - val_MinusLogProbMetric: 28.3433 - lr: 5.0000e-04 - 39s/epoch - 199ms/step
Epoch 661/1000
2023-10-26 08:26:05.847 
Epoch 661/1000 
	 loss: 27.9211, MinusLogProbMetric: 27.9211, val_loss: 28.2352, val_MinusLogProbMetric: 28.2352

Epoch 661: val_loss did not improve from 28.18821
196/196 - 40s - loss: 27.9211 - MinusLogProbMetric: 27.9211 - val_loss: 28.2352 - val_MinusLogProbMetric: 28.2352 - lr: 5.0000e-04 - 40s/epoch - 205ms/step
Epoch 662/1000
2023-10-26 08:26:44.102 
Epoch 662/1000 
	 loss: 27.9186, MinusLogProbMetric: 27.9186, val_loss: 28.4839, val_MinusLogProbMetric: 28.4839

Epoch 662: val_loss did not improve from 28.18821
196/196 - 38s - loss: 27.9186 - MinusLogProbMetric: 27.9186 - val_loss: 28.4839 - val_MinusLogProbMetric: 28.4839 - lr: 5.0000e-04 - 38s/epoch - 195ms/step
Epoch 663/1000
2023-10-26 08:27:25.473 
Epoch 663/1000 
	 loss: 28.0614, MinusLogProbMetric: 28.0614, val_loss: 28.6853, val_MinusLogProbMetric: 28.6853

Epoch 663: val_loss did not improve from 28.18821
196/196 - 41s - loss: 28.0614 - MinusLogProbMetric: 28.0614 - val_loss: 28.6853 - val_MinusLogProbMetric: 28.6853 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 664/1000
2023-10-26 08:28:03.330 
Epoch 664/1000 
	 loss: 27.9076, MinusLogProbMetric: 27.9076, val_loss: 28.3687, val_MinusLogProbMetric: 28.3687

Epoch 664: val_loss did not improve from 28.18821
196/196 - 38s - loss: 27.9076 - MinusLogProbMetric: 27.9076 - val_loss: 28.3687 - val_MinusLogProbMetric: 28.3687 - lr: 5.0000e-04 - 38s/epoch - 193ms/step
Epoch 665/1000
2023-10-26 08:28:43.078 
Epoch 665/1000 
	 loss: 27.9302, MinusLogProbMetric: 27.9302, val_loss: 28.6711, val_MinusLogProbMetric: 28.6711

Epoch 665: val_loss did not improve from 28.18821
196/196 - 40s - loss: 27.9302 - MinusLogProbMetric: 27.9302 - val_loss: 28.6711 - val_MinusLogProbMetric: 28.6711 - lr: 5.0000e-04 - 40s/epoch - 203ms/step
Epoch 666/1000
2023-10-26 08:29:25.403 
Epoch 666/1000 
	 loss: 27.9012, MinusLogProbMetric: 27.9012, val_loss: 28.3521, val_MinusLogProbMetric: 28.3521

Epoch 666: val_loss did not improve from 28.18821
196/196 - 42s - loss: 27.9012 - MinusLogProbMetric: 27.9012 - val_loss: 28.3521 - val_MinusLogProbMetric: 28.3521 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 667/1000
2023-10-26 08:30:06.816 
Epoch 667/1000 
	 loss: 27.9112, MinusLogProbMetric: 27.9112, val_loss: 28.4690, val_MinusLogProbMetric: 28.4690

Epoch 667: val_loss did not improve from 28.18821
196/196 - 41s - loss: 27.9112 - MinusLogProbMetric: 27.9112 - val_loss: 28.4690 - val_MinusLogProbMetric: 28.4690 - lr: 5.0000e-04 - 41s/epoch - 211ms/step
Epoch 668/1000
2023-10-26 08:30:49.604 
Epoch 668/1000 
	 loss: 27.9511, MinusLogProbMetric: 27.9511, val_loss: 28.3188, val_MinusLogProbMetric: 28.3188

Epoch 668: val_loss did not improve from 28.18821
196/196 - 43s - loss: 27.9511 - MinusLogProbMetric: 27.9511 - val_loss: 28.3188 - val_MinusLogProbMetric: 28.3188 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 669/1000
2023-10-26 08:31:32.160 
Epoch 669/1000 
	 loss: 27.9037, MinusLogProbMetric: 27.9037, val_loss: 29.3685, val_MinusLogProbMetric: 29.3685

Epoch 669: val_loss did not improve from 28.18821
196/196 - 43s - loss: 27.9037 - MinusLogProbMetric: 27.9037 - val_loss: 29.3685 - val_MinusLogProbMetric: 29.3685 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 670/1000
2023-10-26 08:32:14.661 
Epoch 670/1000 
	 loss: 27.9324, MinusLogProbMetric: 27.9324, val_loss: 28.4528, val_MinusLogProbMetric: 28.4528

Epoch 670: val_loss did not improve from 28.18821
196/196 - 42s - loss: 27.9324 - MinusLogProbMetric: 27.9324 - val_loss: 28.4528 - val_MinusLogProbMetric: 28.4528 - lr: 5.0000e-04 - 42s/epoch - 217ms/step
Epoch 671/1000
2023-10-26 08:32:56.131 
Epoch 671/1000 
	 loss: 27.9155, MinusLogProbMetric: 27.9155, val_loss: 28.2429, val_MinusLogProbMetric: 28.2429

Epoch 671: val_loss did not improve from 28.18821
196/196 - 41s - loss: 27.9155 - MinusLogProbMetric: 27.9155 - val_loss: 28.2429 - val_MinusLogProbMetric: 28.2429 - lr: 5.0000e-04 - 41s/epoch - 212ms/step
Epoch 672/1000
2023-10-26 08:33:38.127 
Epoch 672/1000 
	 loss: 27.9633, MinusLogProbMetric: 27.9633, val_loss: 28.2380, val_MinusLogProbMetric: 28.2380

Epoch 672: val_loss did not improve from 28.18821
196/196 - 42s - loss: 27.9633 - MinusLogProbMetric: 27.9633 - val_loss: 28.2380 - val_MinusLogProbMetric: 28.2380 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 673/1000
2023-10-26 08:34:20.348 
Epoch 673/1000 
	 loss: 27.9436, MinusLogProbMetric: 27.9436, val_loss: 28.2447, val_MinusLogProbMetric: 28.2447

Epoch 673: val_loss did not improve from 28.18821
196/196 - 42s - loss: 27.9436 - MinusLogProbMetric: 27.9436 - val_loss: 28.2447 - val_MinusLogProbMetric: 28.2447 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 674/1000
2023-10-26 08:35:02.635 
Epoch 674/1000 
	 loss: 27.9429, MinusLogProbMetric: 27.9429, val_loss: 28.3542, val_MinusLogProbMetric: 28.3542

Epoch 674: val_loss did not improve from 28.18821
196/196 - 42s - loss: 27.9429 - MinusLogProbMetric: 27.9429 - val_loss: 28.3542 - val_MinusLogProbMetric: 28.3542 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 675/1000
2023-10-26 08:35:45.163 
Epoch 675/1000 
	 loss: 27.9493, MinusLogProbMetric: 27.9493, val_loss: 28.4611, val_MinusLogProbMetric: 28.4611

Epoch 675: val_loss did not improve from 28.18821
196/196 - 43s - loss: 27.9493 - MinusLogProbMetric: 27.9493 - val_loss: 28.4611 - val_MinusLogProbMetric: 28.4611 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 676/1000
2023-10-26 08:36:27.479 
Epoch 676/1000 
	 loss: 27.9362, MinusLogProbMetric: 27.9362, val_loss: 28.4184, val_MinusLogProbMetric: 28.4184

Epoch 676: val_loss did not improve from 28.18821
196/196 - 42s - loss: 27.9362 - MinusLogProbMetric: 27.9362 - val_loss: 28.4184 - val_MinusLogProbMetric: 28.4184 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 677/1000
2023-10-26 08:37:10.170 
Epoch 677/1000 
	 loss: 27.8788, MinusLogProbMetric: 27.8788, val_loss: 28.2908, val_MinusLogProbMetric: 28.2908

Epoch 677: val_loss did not improve from 28.18821
196/196 - 43s - loss: 27.8788 - MinusLogProbMetric: 27.8788 - val_loss: 28.2908 - val_MinusLogProbMetric: 28.2908 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 678/1000
2023-10-26 08:37:52.691 
Epoch 678/1000 
	 loss: 27.9633, MinusLogProbMetric: 27.9633, val_loss: 28.3470, val_MinusLogProbMetric: 28.3470

Epoch 678: val_loss did not improve from 28.18821
196/196 - 43s - loss: 27.9633 - MinusLogProbMetric: 27.9633 - val_loss: 28.3470 - val_MinusLogProbMetric: 28.3470 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 679/1000
2023-10-26 08:38:35.147 
Epoch 679/1000 
	 loss: 27.9108, MinusLogProbMetric: 27.9108, val_loss: 28.3879, val_MinusLogProbMetric: 28.3879

Epoch 679: val_loss did not improve from 28.18821
196/196 - 42s - loss: 27.9108 - MinusLogProbMetric: 27.9108 - val_loss: 28.3879 - val_MinusLogProbMetric: 28.3879 - lr: 5.0000e-04 - 42s/epoch - 217ms/step
Epoch 680/1000
2023-10-26 08:39:17.538 
Epoch 680/1000 
	 loss: 28.0242, MinusLogProbMetric: 28.0242, val_loss: 28.3505, val_MinusLogProbMetric: 28.3505

Epoch 680: val_loss did not improve from 28.18821
196/196 - 42s - loss: 28.0242 - MinusLogProbMetric: 28.0242 - val_loss: 28.3505 - val_MinusLogProbMetric: 28.3505 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 681/1000
2023-10-26 08:40:00.174 
Epoch 681/1000 
	 loss: 27.9181, MinusLogProbMetric: 27.9181, val_loss: 28.2566, val_MinusLogProbMetric: 28.2566

Epoch 681: val_loss did not improve from 28.18821
196/196 - 43s - loss: 27.9181 - MinusLogProbMetric: 27.9181 - val_loss: 28.2566 - val_MinusLogProbMetric: 28.2566 - lr: 5.0000e-04 - 43s/epoch - 218ms/step
Epoch 682/1000
2023-10-26 08:40:43.129 
Epoch 682/1000 
	 loss: 27.9378, MinusLogProbMetric: 27.9378, val_loss: 28.5256, val_MinusLogProbMetric: 28.5256

Epoch 682: val_loss did not improve from 28.18821
196/196 - 43s - loss: 27.9378 - MinusLogProbMetric: 27.9378 - val_loss: 28.5256 - val_MinusLogProbMetric: 28.5256 - lr: 5.0000e-04 - 43s/epoch - 219ms/step
Epoch 683/1000
2023-10-26 08:41:25.462 
Epoch 683/1000 
	 loss: 27.9273, MinusLogProbMetric: 27.9273, val_loss: 28.2514, val_MinusLogProbMetric: 28.2514

Epoch 683: val_loss did not improve from 28.18821
196/196 - 42s - loss: 27.9273 - MinusLogProbMetric: 27.9273 - val_loss: 28.2514 - val_MinusLogProbMetric: 28.2514 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 684/1000
2023-10-26 08:42:07.334 
Epoch 684/1000 
	 loss: 27.6568, MinusLogProbMetric: 27.6568, val_loss: 28.0588, val_MinusLogProbMetric: 28.0588

Epoch 684: val_loss improved from 28.18821 to 28.05878, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.6568 - MinusLogProbMetric: 27.6568 - val_loss: 28.0588 - val_MinusLogProbMetric: 28.0588 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 685/1000
2023-10-26 08:42:51.359 
Epoch 685/1000 
	 loss: 27.6685, MinusLogProbMetric: 27.6685, val_loss: 28.1583, val_MinusLogProbMetric: 28.1583

Epoch 685: val_loss did not improve from 28.05878
196/196 - 43s - loss: 27.6685 - MinusLogProbMetric: 27.6685 - val_loss: 28.1583 - val_MinusLogProbMetric: 28.1583 - lr: 2.5000e-04 - 43s/epoch - 221ms/step
Epoch 686/1000
2023-10-26 08:43:33.825 
Epoch 686/1000 
	 loss: 27.6729, MinusLogProbMetric: 27.6729, val_loss: 28.0870, val_MinusLogProbMetric: 28.0870

Epoch 686: val_loss did not improve from 28.05878
196/196 - 42s - loss: 27.6729 - MinusLogProbMetric: 27.6729 - val_loss: 28.0870 - val_MinusLogProbMetric: 28.0870 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 687/1000
2023-10-26 08:44:16.517 
Epoch 687/1000 
	 loss: 27.6569, MinusLogProbMetric: 27.6569, val_loss: 28.1231, val_MinusLogProbMetric: 28.1231

Epoch 687: val_loss did not improve from 28.05878
196/196 - 43s - loss: 27.6569 - MinusLogProbMetric: 27.6569 - val_loss: 28.1231 - val_MinusLogProbMetric: 28.1231 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 688/1000
2023-10-26 08:44:58.707 
Epoch 688/1000 
	 loss: 27.6616, MinusLogProbMetric: 27.6616, val_loss: 28.1161, val_MinusLogProbMetric: 28.1161

Epoch 688: val_loss did not improve from 28.05878
196/196 - 42s - loss: 27.6616 - MinusLogProbMetric: 27.6616 - val_loss: 28.1161 - val_MinusLogProbMetric: 28.1161 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 689/1000
2023-10-26 08:45:40.312 
Epoch 689/1000 
	 loss: 27.6597, MinusLogProbMetric: 27.6597, val_loss: 28.1478, val_MinusLogProbMetric: 28.1478

Epoch 689: val_loss did not improve from 28.05878
196/196 - 42s - loss: 27.6597 - MinusLogProbMetric: 27.6597 - val_loss: 28.1478 - val_MinusLogProbMetric: 28.1478 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 690/1000
2023-10-26 08:46:21.694 
Epoch 690/1000 
	 loss: 27.6752, MinusLogProbMetric: 27.6752, val_loss: 28.1360, val_MinusLogProbMetric: 28.1360

Epoch 690: val_loss did not improve from 28.05878
196/196 - 41s - loss: 27.6752 - MinusLogProbMetric: 27.6752 - val_loss: 28.1360 - val_MinusLogProbMetric: 28.1360 - lr: 2.5000e-04 - 41s/epoch - 211ms/step
Epoch 691/1000
2023-10-26 08:47:04.005 
Epoch 691/1000 
	 loss: 27.6621, MinusLogProbMetric: 27.6621, val_loss: 28.1867, val_MinusLogProbMetric: 28.1867

Epoch 691: val_loss did not improve from 28.05878
196/196 - 42s - loss: 27.6621 - MinusLogProbMetric: 27.6621 - val_loss: 28.1867 - val_MinusLogProbMetric: 28.1867 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 692/1000
2023-10-26 08:47:46.496 
Epoch 692/1000 
	 loss: 27.6629, MinusLogProbMetric: 27.6629, val_loss: 28.0945, val_MinusLogProbMetric: 28.0945

Epoch 692: val_loss did not improve from 28.05878
196/196 - 42s - loss: 27.6629 - MinusLogProbMetric: 27.6629 - val_loss: 28.0945 - val_MinusLogProbMetric: 28.0945 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 693/1000
2023-10-26 08:48:28.818 
Epoch 693/1000 
	 loss: 27.6582, MinusLogProbMetric: 27.6582, val_loss: 28.1069, val_MinusLogProbMetric: 28.1069

Epoch 693: val_loss did not improve from 28.05878
196/196 - 42s - loss: 27.6582 - MinusLogProbMetric: 27.6582 - val_loss: 28.1069 - val_MinusLogProbMetric: 28.1069 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 694/1000
2023-10-26 08:49:11.094 
Epoch 694/1000 
	 loss: 27.6398, MinusLogProbMetric: 27.6398, val_loss: 28.1729, val_MinusLogProbMetric: 28.1729

Epoch 694: val_loss did not improve from 28.05878
196/196 - 42s - loss: 27.6398 - MinusLogProbMetric: 27.6398 - val_loss: 28.1729 - val_MinusLogProbMetric: 28.1729 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 695/1000
2023-10-26 08:49:51.707 
Epoch 695/1000 
	 loss: 27.6524, MinusLogProbMetric: 27.6524, val_loss: 28.1284, val_MinusLogProbMetric: 28.1284

Epoch 695: val_loss did not improve from 28.05878
196/196 - 41s - loss: 27.6524 - MinusLogProbMetric: 27.6524 - val_loss: 28.1284 - val_MinusLogProbMetric: 28.1284 - lr: 2.5000e-04 - 41s/epoch - 207ms/step
Epoch 696/1000
2023-10-26 08:50:33.960 
Epoch 696/1000 
	 loss: 27.6642, MinusLogProbMetric: 27.6642, val_loss: 28.1084, val_MinusLogProbMetric: 28.1084

Epoch 696: val_loss did not improve from 28.05878
196/196 - 42s - loss: 27.6642 - MinusLogProbMetric: 27.6642 - val_loss: 28.1084 - val_MinusLogProbMetric: 28.1084 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 697/1000
2023-10-26 08:51:16.125 
Epoch 697/1000 
	 loss: 27.6542, MinusLogProbMetric: 27.6542, val_loss: 28.1200, val_MinusLogProbMetric: 28.1200

Epoch 697: val_loss did not improve from 28.05878
196/196 - 42s - loss: 27.6542 - MinusLogProbMetric: 27.6542 - val_loss: 28.1200 - val_MinusLogProbMetric: 28.1200 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 698/1000
2023-10-26 08:51:58.807 
Epoch 698/1000 
	 loss: 27.6754, MinusLogProbMetric: 27.6754, val_loss: 28.1271, val_MinusLogProbMetric: 28.1271

Epoch 698: val_loss did not improve from 28.05878
196/196 - 43s - loss: 27.6754 - MinusLogProbMetric: 27.6754 - val_loss: 28.1271 - val_MinusLogProbMetric: 28.1271 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 699/1000
2023-10-26 08:52:41.049 
Epoch 699/1000 
	 loss: 27.6429, MinusLogProbMetric: 27.6429, val_loss: 28.1127, val_MinusLogProbMetric: 28.1127

Epoch 699: val_loss did not improve from 28.05878
196/196 - 42s - loss: 27.6429 - MinusLogProbMetric: 27.6429 - val_loss: 28.1127 - val_MinusLogProbMetric: 28.1127 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 700/1000
2023-10-26 08:53:23.797 
Epoch 700/1000 
	 loss: 27.6905, MinusLogProbMetric: 27.6905, val_loss: 28.1136, val_MinusLogProbMetric: 28.1136

Epoch 700: val_loss did not improve from 28.05878
196/196 - 43s - loss: 27.6905 - MinusLogProbMetric: 27.6905 - val_loss: 28.1136 - val_MinusLogProbMetric: 28.1136 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 701/1000
2023-10-26 08:54:06.762 
Epoch 701/1000 
	 loss: 27.6447, MinusLogProbMetric: 27.6447, val_loss: 28.1778, val_MinusLogProbMetric: 28.1778

Epoch 701: val_loss did not improve from 28.05878
196/196 - 43s - loss: 27.6447 - MinusLogProbMetric: 27.6447 - val_loss: 28.1778 - val_MinusLogProbMetric: 28.1778 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 702/1000
2023-10-26 08:54:49.417 
Epoch 702/1000 
	 loss: 27.6510, MinusLogProbMetric: 27.6510, val_loss: 28.1168, val_MinusLogProbMetric: 28.1168

Epoch 702: val_loss did not improve from 28.05878
196/196 - 43s - loss: 27.6510 - MinusLogProbMetric: 27.6510 - val_loss: 28.1168 - val_MinusLogProbMetric: 28.1168 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 703/1000
2023-10-26 08:55:32.169 
Epoch 703/1000 
	 loss: 27.6533, MinusLogProbMetric: 27.6533, val_loss: 28.0382, val_MinusLogProbMetric: 28.0382

Epoch 703: val_loss improved from 28.05878 to 28.03825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.6533 - MinusLogProbMetric: 27.6533 - val_loss: 28.0382 - val_MinusLogProbMetric: 28.0382 - lr: 2.5000e-04 - 43s/epoch - 222ms/step
Epoch 704/1000
2023-10-26 08:56:15.564 
Epoch 704/1000 
	 loss: 27.6767, MinusLogProbMetric: 27.6767, val_loss: 28.2716, val_MinusLogProbMetric: 28.2716

Epoch 704: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6767 - MinusLogProbMetric: 27.6767 - val_loss: 28.2716 - val_MinusLogProbMetric: 28.2716 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 705/1000
2023-10-26 08:56:57.401 
Epoch 705/1000 
	 loss: 27.6521, MinusLogProbMetric: 27.6521, val_loss: 28.1781, val_MinusLogProbMetric: 28.1781

Epoch 705: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6521 - MinusLogProbMetric: 27.6521 - val_loss: 28.1781 - val_MinusLogProbMetric: 28.1781 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 706/1000
2023-10-26 08:57:40.062 
Epoch 706/1000 
	 loss: 27.6636, MinusLogProbMetric: 27.6636, val_loss: 28.1886, val_MinusLogProbMetric: 28.1886

Epoch 706: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6636 - MinusLogProbMetric: 27.6636 - val_loss: 28.1886 - val_MinusLogProbMetric: 28.1886 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 707/1000
2023-10-26 08:58:22.162 
Epoch 707/1000 
	 loss: 27.6558, MinusLogProbMetric: 27.6558, val_loss: 28.0557, val_MinusLogProbMetric: 28.0557

Epoch 707: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6558 - MinusLogProbMetric: 27.6558 - val_loss: 28.0557 - val_MinusLogProbMetric: 28.0557 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 708/1000
2023-10-26 08:59:04.651 
Epoch 708/1000 
	 loss: 27.6563, MinusLogProbMetric: 27.6563, val_loss: 28.1206, val_MinusLogProbMetric: 28.1206

Epoch 708: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6563 - MinusLogProbMetric: 27.6563 - val_loss: 28.1206 - val_MinusLogProbMetric: 28.1206 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 709/1000
2023-10-26 08:59:47.307 
Epoch 709/1000 
	 loss: 27.6402, MinusLogProbMetric: 27.6402, val_loss: 28.1719, val_MinusLogProbMetric: 28.1719

Epoch 709: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6402 - MinusLogProbMetric: 27.6402 - val_loss: 28.1719 - val_MinusLogProbMetric: 28.1719 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 710/1000
2023-10-26 09:00:28.939 
Epoch 710/1000 
	 loss: 27.6518, MinusLogProbMetric: 27.6518, val_loss: 28.1112, val_MinusLogProbMetric: 28.1112

Epoch 710: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6518 - MinusLogProbMetric: 27.6518 - val_loss: 28.1112 - val_MinusLogProbMetric: 28.1112 - lr: 2.5000e-04 - 42s/epoch - 212ms/step
Epoch 711/1000
2023-10-26 09:01:11.492 
Epoch 711/1000 
	 loss: 27.6479, MinusLogProbMetric: 27.6479, val_loss: 28.1430, val_MinusLogProbMetric: 28.1430

Epoch 711: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6479 - MinusLogProbMetric: 27.6479 - val_loss: 28.1430 - val_MinusLogProbMetric: 28.1430 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 712/1000
2023-10-26 09:01:54.084 
Epoch 712/1000 
	 loss: 27.6614, MinusLogProbMetric: 27.6614, val_loss: 28.0846, val_MinusLogProbMetric: 28.0846

Epoch 712: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6614 - MinusLogProbMetric: 27.6614 - val_loss: 28.0846 - val_MinusLogProbMetric: 28.0846 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 713/1000
2023-10-26 09:02:36.311 
Epoch 713/1000 
	 loss: 27.6501, MinusLogProbMetric: 27.6501, val_loss: 28.0806, val_MinusLogProbMetric: 28.0806

Epoch 713: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6501 - MinusLogProbMetric: 27.6501 - val_loss: 28.0806 - val_MinusLogProbMetric: 28.0806 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 714/1000
2023-10-26 09:03:18.055 
Epoch 714/1000 
	 loss: 27.6433, MinusLogProbMetric: 27.6433, val_loss: 28.0946, val_MinusLogProbMetric: 28.0946

Epoch 714: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6433 - MinusLogProbMetric: 27.6433 - val_loss: 28.0946 - val_MinusLogProbMetric: 28.0946 - lr: 2.5000e-04 - 42s/epoch - 213ms/step
Epoch 715/1000
2023-10-26 09:04:00.259 
Epoch 715/1000 
	 loss: 27.6409, MinusLogProbMetric: 27.6409, val_loss: 28.0705, val_MinusLogProbMetric: 28.0705

Epoch 715: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6409 - MinusLogProbMetric: 27.6409 - val_loss: 28.0705 - val_MinusLogProbMetric: 28.0705 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 716/1000
2023-10-26 09:04:43.247 
Epoch 716/1000 
	 loss: 27.6537, MinusLogProbMetric: 27.6537, val_loss: 28.0682, val_MinusLogProbMetric: 28.0682

Epoch 716: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6537 - MinusLogProbMetric: 27.6537 - val_loss: 28.0682 - val_MinusLogProbMetric: 28.0682 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 717/1000
2023-10-26 09:05:25.121 
Epoch 717/1000 
	 loss: 27.6714, MinusLogProbMetric: 27.6714, val_loss: 28.1093, val_MinusLogProbMetric: 28.1093

Epoch 717: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6714 - MinusLogProbMetric: 27.6714 - val_loss: 28.1093 - val_MinusLogProbMetric: 28.1093 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 718/1000
2023-10-26 09:06:07.889 
Epoch 718/1000 
	 loss: 27.6586, MinusLogProbMetric: 27.6586, val_loss: 28.3524, val_MinusLogProbMetric: 28.3524

Epoch 718: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6586 - MinusLogProbMetric: 27.6586 - val_loss: 28.3524 - val_MinusLogProbMetric: 28.3524 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 719/1000
2023-10-26 09:06:50.162 
Epoch 719/1000 
	 loss: 27.6436, MinusLogProbMetric: 27.6436, val_loss: 28.3270, val_MinusLogProbMetric: 28.3270

Epoch 719: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6436 - MinusLogProbMetric: 27.6436 - val_loss: 28.3270 - val_MinusLogProbMetric: 28.3270 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 720/1000
2023-10-26 09:07:32.679 
Epoch 720/1000 
	 loss: 27.6574, MinusLogProbMetric: 27.6574, val_loss: 28.1247, val_MinusLogProbMetric: 28.1247

Epoch 720: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6574 - MinusLogProbMetric: 27.6574 - val_loss: 28.1247 - val_MinusLogProbMetric: 28.1247 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 721/1000
2023-10-26 09:08:14.965 
Epoch 721/1000 
	 loss: 27.6819, MinusLogProbMetric: 27.6819, val_loss: 28.2161, val_MinusLogProbMetric: 28.2161

Epoch 721: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6819 - MinusLogProbMetric: 27.6819 - val_loss: 28.2161 - val_MinusLogProbMetric: 28.2161 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 722/1000
2023-10-26 09:08:57.279 
Epoch 722/1000 
	 loss: 27.6640, MinusLogProbMetric: 27.6640, val_loss: 28.1093, val_MinusLogProbMetric: 28.1093

Epoch 722: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6640 - MinusLogProbMetric: 27.6640 - val_loss: 28.1093 - val_MinusLogProbMetric: 28.1093 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 723/1000
2023-10-26 09:09:39.324 
Epoch 723/1000 
	 loss: 27.6414, MinusLogProbMetric: 27.6414, val_loss: 28.1230, val_MinusLogProbMetric: 28.1230

Epoch 723: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6414 - MinusLogProbMetric: 27.6414 - val_loss: 28.1230 - val_MinusLogProbMetric: 28.1230 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 724/1000
2023-10-26 09:10:22.126 
Epoch 724/1000 
	 loss: 27.6337, MinusLogProbMetric: 27.6337, val_loss: 28.0672, val_MinusLogProbMetric: 28.0672

Epoch 724: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6337 - MinusLogProbMetric: 27.6337 - val_loss: 28.0672 - val_MinusLogProbMetric: 28.0672 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 725/1000
2023-10-26 09:11:05.048 
Epoch 725/1000 
	 loss: 27.6760, MinusLogProbMetric: 27.6760, val_loss: 28.1468, val_MinusLogProbMetric: 28.1468

Epoch 725: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6760 - MinusLogProbMetric: 27.6760 - val_loss: 28.1468 - val_MinusLogProbMetric: 28.1468 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 726/1000
2023-10-26 09:11:47.607 
Epoch 726/1000 
	 loss: 27.6494, MinusLogProbMetric: 27.6494, val_loss: 28.2560, val_MinusLogProbMetric: 28.2560

Epoch 726: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6494 - MinusLogProbMetric: 27.6494 - val_loss: 28.2560 - val_MinusLogProbMetric: 28.2560 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 727/1000
2023-10-26 09:12:30.214 
Epoch 727/1000 
	 loss: 27.6235, MinusLogProbMetric: 27.6235, val_loss: 28.3049, val_MinusLogProbMetric: 28.3049

Epoch 727: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6235 - MinusLogProbMetric: 27.6235 - val_loss: 28.3049 - val_MinusLogProbMetric: 28.3049 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 728/1000
2023-10-26 09:13:12.548 
Epoch 728/1000 
	 loss: 27.6545, MinusLogProbMetric: 27.6545, val_loss: 28.1396, val_MinusLogProbMetric: 28.1396

Epoch 728: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6545 - MinusLogProbMetric: 27.6545 - val_loss: 28.1396 - val_MinusLogProbMetric: 28.1396 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 729/1000
2023-10-26 09:13:53.798 
Epoch 729/1000 
	 loss: 27.6487, MinusLogProbMetric: 27.6487, val_loss: 28.1746, val_MinusLogProbMetric: 28.1746

Epoch 729: val_loss did not improve from 28.03825
196/196 - 41s - loss: 27.6487 - MinusLogProbMetric: 27.6487 - val_loss: 28.1746 - val_MinusLogProbMetric: 28.1746 - lr: 2.5000e-04 - 41s/epoch - 210ms/step
Epoch 730/1000
2023-10-26 09:14:35.886 
Epoch 730/1000 
	 loss: 27.6241, MinusLogProbMetric: 27.6241, val_loss: 28.2577, val_MinusLogProbMetric: 28.2577

Epoch 730: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6241 - MinusLogProbMetric: 27.6241 - val_loss: 28.2577 - val_MinusLogProbMetric: 28.2577 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 731/1000
2023-10-26 09:15:18.566 
Epoch 731/1000 
	 loss: 27.6166, MinusLogProbMetric: 27.6166, val_loss: 28.1419, val_MinusLogProbMetric: 28.1419

Epoch 731: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6166 - MinusLogProbMetric: 27.6166 - val_loss: 28.1419 - val_MinusLogProbMetric: 28.1419 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 732/1000
2023-10-26 09:16:01.195 
Epoch 732/1000 
	 loss: 27.6564, MinusLogProbMetric: 27.6564, val_loss: 28.0444, val_MinusLogProbMetric: 28.0444

Epoch 732: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6564 - MinusLogProbMetric: 27.6564 - val_loss: 28.0444 - val_MinusLogProbMetric: 28.0444 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 733/1000
2023-10-26 09:16:44.196 
Epoch 733/1000 
	 loss: 27.6238, MinusLogProbMetric: 27.6238, val_loss: 28.1294, val_MinusLogProbMetric: 28.1294

Epoch 733: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6238 - MinusLogProbMetric: 27.6238 - val_loss: 28.1294 - val_MinusLogProbMetric: 28.1294 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 734/1000
2023-10-26 09:17:26.681 
Epoch 734/1000 
	 loss: 27.6271, MinusLogProbMetric: 27.6271, val_loss: 28.1366, val_MinusLogProbMetric: 28.1366

Epoch 734: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6271 - MinusLogProbMetric: 27.6271 - val_loss: 28.1366 - val_MinusLogProbMetric: 28.1366 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 735/1000
2023-10-26 09:18:09.118 
Epoch 735/1000 
	 loss: 27.6277, MinusLogProbMetric: 27.6277, val_loss: 28.1272, val_MinusLogProbMetric: 28.1272

Epoch 735: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6277 - MinusLogProbMetric: 27.6277 - val_loss: 28.1272 - val_MinusLogProbMetric: 28.1272 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 736/1000
2023-10-26 09:18:51.212 
Epoch 736/1000 
	 loss: 27.6473, MinusLogProbMetric: 27.6473, val_loss: 28.2181, val_MinusLogProbMetric: 28.2181

Epoch 736: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6473 - MinusLogProbMetric: 27.6473 - val_loss: 28.2181 - val_MinusLogProbMetric: 28.2181 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 737/1000
2023-10-26 09:19:33.601 
Epoch 737/1000 
	 loss: 27.6670, MinusLogProbMetric: 27.6670, val_loss: 28.1400, val_MinusLogProbMetric: 28.1400

Epoch 737: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6670 - MinusLogProbMetric: 27.6670 - val_loss: 28.1400 - val_MinusLogProbMetric: 28.1400 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 738/1000
2023-10-26 09:20:16.589 
Epoch 738/1000 
	 loss: 27.6563, MinusLogProbMetric: 27.6563, val_loss: 28.0548, val_MinusLogProbMetric: 28.0548

Epoch 738: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6563 - MinusLogProbMetric: 27.6563 - val_loss: 28.0548 - val_MinusLogProbMetric: 28.0548 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 739/1000
2023-10-26 09:20:59.460 
Epoch 739/1000 
	 loss: 27.6373, MinusLogProbMetric: 27.6373, val_loss: 28.1059, val_MinusLogProbMetric: 28.1059

Epoch 739: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6373 - MinusLogProbMetric: 27.6373 - val_loss: 28.1059 - val_MinusLogProbMetric: 28.1059 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 740/1000
2023-10-26 09:21:41.458 
Epoch 740/1000 
	 loss: 27.6560, MinusLogProbMetric: 27.6560, val_loss: 28.2805, val_MinusLogProbMetric: 28.2805

Epoch 740: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6560 - MinusLogProbMetric: 27.6560 - val_loss: 28.2805 - val_MinusLogProbMetric: 28.2805 - lr: 2.5000e-04 - 42s/epoch - 214ms/step
Epoch 741/1000
2023-10-26 09:22:23.822 
Epoch 741/1000 
	 loss: 27.6457, MinusLogProbMetric: 27.6457, val_loss: 28.1229, val_MinusLogProbMetric: 28.1229

Epoch 741: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6457 - MinusLogProbMetric: 27.6457 - val_loss: 28.1229 - val_MinusLogProbMetric: 28.1229 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 742/1000
2023-10-26 09:23:06.619 
Epoch 742/1000 
	 loss: 27.6684, MinusLogProbMetric: 27.6684, val_loss: 28.0812, val_MinusLogProbMetric: 28.0812

Epoch 742: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6684 - MinusLogProbMetric: 27.6684 - val_loss: 28.0812 - val_MinusLogProbMetric: 28.0812 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 743/1000
2023-10-26 09:23:49.202 
Epoch 743/1000 
	 loss: 27.6259, MinusLogProbMetric: 27.6259, val_loss: 28.1229, val_MinusLogProbMetric: 28.1229

Epoch 743: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6259 - MinusLogProbMetric: 27.6259 - val_loss: 28.1229 - val_MinusLogProbMetric: 28.1229 - lr: 2.5000e-04 - 43s/epoch - 217ms/step
Epoch 744/1000
2023-10-26 09:24:31.543 
Epoch 744/1000 
	 loss: 27.6606, MinusLogProbMetric: 27.6606, val_loss: 28.0746, val_MinusLogProbMetric: 28.0746

Epoch 744: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6606 - MinusLogProbMetric: 27.6606 - val_loss: 28.0746 - val_MinusLogProbMetric: 28.0746 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 745/1000
2023-10-26 09:25:13.926 
Epoch 745/1000 
	 loss: 27.6451, MinusLogProbMetric: 27.6451, val_loss: 28.0929, val_MinusLogProbMetric: 28.0929

Epoch 745: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6451 - MinusLogProbMetric: 27.6451 - val_loss: 28.0929 - val_MinusLogProbMetric: 28.0929 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 746/1000
2023-10-26 09:25:56.193 
Epoch 746/1000 
	 loss: 27.6454, MinusLogProbMetric: 27.6454, val_loss: 28.1789, val_MinusLogProbMetric: 28.1789

Epoch 746: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6454 - MinusLogProbMetric: 27.6454 - val_loss: 28.1789 - val_MinusLogProbMetric: 28.1789 - lr: 2.5000e-04 - 42s/epoch - 216ms/step
Epoch 747/1000
2023-10-26 09:26:38.640 
Epoch 747/1000 
	 loss: 27.6341, MinusLogProbMetric: 27.6341, val_loss: 28.0712, val_MinusLogProbMetric: 28.0712

Epoch 747: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6341 - MinusLogProbMetric: 27.6341 - val_loss: 28.0712 - val_MinusLogProbMetric: 28.0712 - lr: 2.5000e-04 - 42s/epoch - 217ms/step
Epoch 748/1000
2023-10-26 09:27:21.530 
Epoch 748/1000 
	 loss: 27.6516, MinusLogProbMetric: 27.6516, val_loss: 28.2349, val_MinusLogProbMetric: 28.2349

Epoch 748: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6516 - MinusLogProbMetric: 27.6516 - val_loss: 28.2349 - val_MinusLogProbMetric: 28.2349 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 749/1000
2023-10-26 09:28:04.294 
Epoch 749/1000 
	 loss: 27.6388, MinusLogProbMetric: 27.6388, val_loss: 28.1596, val_MinusLogProbMetric: 28.1596

Epoch 749: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6388 - MinusLogProbMetric: 27.6388 - val_loss: 28.1596 - val_MinusLogProbMetric: 28.1596 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 750/1000
2023-10-26 09:28:47.124 
Epoch 750/1000 
	 loss: 27.6352, MinusLogProbMetric: 27.6352, val_loss: 28.2093, val_MinusLogProbMetric: 28.2093

Epoch 750: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6352 - MinusLogProbMetric: 27.6352 - val_loss: 28.2093 - val_MinusLogProbMetric: 28.2093 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 751/1000
2023-10-26 09:29:30.027 
Epoch 751/1000 
	 loss: 27.6593, MinusLogProbMetric: 27.6593, val_loss: 28.1203, val_MinusLogProbMetric: 28.1203

Epoch 751: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6593 - MinusLogProbMetric: 27.6593 - val_loss: 28.1203 - val_MinusLogProbMetric: 28.1203 - lr: 2.5000e-04 - 43s/epoch - 219ms/step
Epoch 752/1000
2023-10-26 09:30:12.195 
Epoch 752/1000 
	 loss: 27.6448, MinusLogProbMetric: 27.6448, val_loss: 28.0756, val_MinusLogProbMetric: 28.0756

Epoch 752: val_loss did not improve from 28.03825
196/196 - 42s - loss: 27.6448 - MinusLogProbMetric: 27.6448 - val_loss: 28.0756 - val_MinusLogProbMetric: 28.0756 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 753/1000
2023-10-26 09:30:54.901 
Epoch 753/1000 
	 loss: 27.6290, MinusLogProbMetric: 27.6290, val_loss: 28.1019, val_MinusLogProbMetric: 28.1019

Epoch 753: val_loss did not improve from 28.03825
196/196 - 43s - loss: 27.6290 - MinusLogProbMetric: 27.6290 - val_loss: 28.1019 - val_MinusLogProbMetric: 28.1019 - lr: 2.5000e-04 - 43s/epoch - 218ms/step
Epoch 754/1000
2023-10-26 09:31:37.920 
Epoch 754/1000 
	 loss: 27.5430, MinusLogProbMetric: 27.5430, val_loss: 28.0161, val_MinusLogProbMetric: 28.0161

Epoch 754: val_loss improved from 28.03825 to 28.01606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 27.5430 - MinusLogProbMetric: 27.5430 - val_loss: 28.0161 - val_MinusLogProbMetric: 28.0161 - lr: 1.2500e-04 - 44s/epoch - 224ms/step
Epoch 755/1000
2023-10-26 09:32:21.427 
Epoch 755/1000 
	 loss: 27.5383, MinusLogProbMetric: 27.5383, val_loss: 28.0091, val_MinusLogProbMetric: 28.0091

Epoch 755: val_loss improved from 28.01606 to 28.00908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.5383 - MinusLogProbMetric: 27.5383 - val_loss: 28.0091 - val_MinusLogProbMetric: 28.0091 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 756/1000
2023-10-26 09:33:04.178 
Epoch 756/1000 
	 loss: 27.5360, MinusLogProbMetric: 27.5360, val_loss: 28.0539, val_MinusLogProbMetric: 28.0539

Epoch 756: val_loss did not improve from 28.00908
196/196 - 42s - loss: 27.5360 - MinusLogProbMetric: 27.5360 - val_loss: 28.0539 - val_MinusLogProbMetric: 28.0539 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 757/1000
2023-10-26 09:33:46.840 
Epoch 757/1000 
	 loss: 27.5463, MinusLogProbMetric: 27.5463, val_loss: 28.0447, val_MinusLogProbMetric: 28.0447

Epoch 757: val_loss did not improve from 28.00908
196/196 - 43s - loss: 27.5463 - MinusLogProbMetric: 27.5463 - val_loss: 28.0447 - val_MinusLogProbMetric: 28.0447 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 758/1000
2023-10-26 09:34:29.770 
Epoch 758/1000 
	 loss: 27.5390, MinusLogProbMetric: 27.5390, val_loss: 28.0386, val_MinusLogProbMetric: 28.0386

Epoch 758: val_loss did not improve from 28.00908
196/196 - 43s - loss: 27.5390 - MinusLogProbMetric: 27.5390 - val_loss: 28.0386 - val_MinusLogProbMetric: 28.0386 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 759/1000
2023-10-26 09:35:11.914 
Epoch 759/1000 
	 loss: 27.5467, MinusLogProbMetric: 27.5467, val_loss: 28.0653, val_MinusLogProbMetric: 28.0653

Epoch 759: val_loss did not improve from 28.00908
196/196 - 42s - loss: 27.5467 - MinusLogProbMetric: 27.5467 - val_loss: 28.0653 - val_MinusLogProbMetric: 28.0653 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 760/1000
2023-10-26 09:35:54.217 
Epoch 760/1000 
	 loss: 27.5355, MinusLogProbMetric: 27.5355, val_loss: 28.0686, val_MinusLogProbMetric: 28.0686

Epoch 760: val_loss did not improve from 28.00908
196/196 - 42s - loss: 27.5355 - MinusLogProbMetric: 27.5355 - val_loss: 28.0686 - val_MinusLogProbMetric: 28.0686 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 761/1000
2023-10-26 09:36:37.092 
Epoch 761/1000 
	 loss: 27.5408, MinusLogProbMetric: 27.5408, val_loss: 28.0324, val_MinusLogProbMetric: 28.0324

Epoch 761: val_loss did not improve from 28.00908
196/196 - 43s - loss: 27.5408 - MinusLogProbMetric: 27.5408 - val_loss: 28.0324 - val_MinusLogProbMetric: 28.0324 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 762/1000
2023-10-26 09:37:19.352 
Epoch 762/1000 
	 loss: 27.5434, MinusLogProbMetric: 27.5434, val_loss: 28.0051, val_MinusLogProbMetric: 28.0051

Epoch 762: val_loss improved from 28.00908 to 28.00510, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.5434 - MinusLogProbMetric: 27.5434 - val_loss: 28.0051 - val_MinusLogProbMetric: 28.0051 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 763/1000
2023-10-26 09:38:01.998 
Epoch 763/1000 
	 loss: 27.5433, MinusLogProbMetric: 27.5433, val_loss: 28.0087, val_MinusLogProbMetric: 28.0087

Epoch 763: val_loss did not improve from 28.00510
196/196 - 42s - loss: 27.5433 - MinusLogProbMetric: 27.5433 - val_loss: 28.0087 - val_MinusLogProbMetric: 28.0087 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 764/1000
2023-10-26 09:38:43.571 
Epoch 764/1000 
	 loss: 27.5440, MinusLogProbMetric: 27.5440, val_loss: 28.0273, val_MinusLogProbMetric: 28.0273

Epoch 764: val_loss did not improve from 28.00510
196/196 - 42s - loss: 27.5440 - MinusLogProbMetric: 27.5440 - val_loss: 28.0273 - val_MinusLogProbMetric: 28.0273 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 765/1000
2023-10-26 09:39:26.062 
Epoch 765/1000 
	 loss: 27.5360, MinusLogProbMetric: 27.5360, val_loss: 28.0326, val_MinusLogProbMetric: 28.0326

Epoch 765: val_loss did not improve from 28.00510
196/196 - 42s - loss: 27.5360 - MinusLogProbMetric: 27.5360 - val_loss: 28.0326 - val_MinusLogProbMetric: 28.0326 - lr: 1.2500e-04 - 42s/epoch - 217ms/step
Epoch 766/1000
2023-10-26 09:40:08.153 
Epoch 766/1000 
	 loss: 27.5464, MinusLogProbMetric: 27.5464, val_loss: 28.0690, val_MinusLogProbMetric: 28.0690

Epoch 766: val_loss did not improve from 28.00510
196/196 - 42s - loss: 27.5464 - MinusLogProbMetric: 27.5464 - val_loss: 28.0690 - val_MinusLogProbMetric: 28.0690 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 767/1000
2023-10-26 09:40:50.475 
Epoch 767/1000 
	 loss: 27.5418, MinusLogProbMetric: 27.5418, val_loss: 28.0152, val_MinusLogProbMetric: 28.0152

Epoch 767: val_loss did not improve from 28.00510
196/196 - 42s - loss: 27.5418 - MinusLogProbMetric: 27.5418 - val_loss: 28.0152 - val_MinusLogProbMetric: 28.0152 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 768/1000
2023-10-26 09:41:32.625 
Epoch 768/1000 
	 loss: 27.5463, MinusLogProbMetric: 27.5463, val_loss: 28.0036, val_MinusLogProbMetric: 28.0036

Epoch 768: val_loss improved from 28.00510 to 28.00359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.5463 - MinusLogProbMetric: 27.5463 - val_loss: 28.0036 - val_MinusLogProbMetric: 28.0036 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 769/1000
2023-10-26 09:42:16.154 
Epoch 769/1000 
	 loss: 27.5350, MinusLogProbMetric: 27.5350, val_loss: 28.0127, val_MinusLogProbMetric: 28.0127

Epoch 769: val_loss did not improve from 28.00359
196/196 - 43s - loss: 27.5350 - MinusLogProbMetric: 27.5350 - val_loss: 28.0127 - val_MinusLogProbMetric: 28.0127 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 770/1000
2023-10-26 09:42:58.603 
Epoch 770/1000 
	 loss: 27.5482, MinusLogProbMetric: 27.5482, val_loss: 28.0435, val_MinusLogProbMetric: 28.0435

Epoch 770: val_loss did not improve from 28.00359
196/196 - 42s - loss: 27.5482 - MinusLogProbMetric: 27.5482 - val_loss: 28.0435 - val_MinusLogProbMetric: 28.0435 - lr: 1.2500e-04 - 42s/epoch - 217ms/step
Epoch 771/1000
2023-10-26 09:43:40.945 
Epoch 771/1000 
	 loss: 27.5480, MinusLogProbMetric: 27.5480, val_loss: 28.0277, val_MinusLogProbMetric: 28.0277

Epoch 771: val_loss did not improve from 28.00359
196/196 - 42s - loss: 27.5480 - MinusLogProbMetric: 27.5480 - val_loss: 28.0277 - val_MinusLogProbMetric: 28.0277 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 772/1000
2023-10-26 09:44:23.024 
Epoch 772/1000 
	 loss: 27.5415, MinusLogProbMetric: 27.5415, val_loss: 28.1862, val_MinusLogProbMetric: 28.1862

Epoch 772: val_loss did not improve from 28.00359
196/196 - 42s - loss: 27.5415 - MinusLogProbMetric: 27.5415 - val_loss: 28.1862 - val_MinusLogProbMetric: 28.1862 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 773/1000
2023-10-26 09:45:04.998 
Epoch 773/1000 
	 loss: 27.5464, MinusLogProbMetric: 27.5464, val_loss: 28.0305, val_MinusLogProbMetric: 28.0305

Epoch 773: val_loss did not improve from 28.00359
196/196 - 42s - loss: 27.5464 - MinusLogProbMetric: 27.5464 - val_loss: 28.0305 - val_MinusLogProbMetric: 28.0305 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 774/1000
2023-10-26 09:45:47.912 
Epoch 774/1000 
	 loss: 27.5415, MinusLogProbMetric: 27.5415, val_loss: 28.0035, val_MinusLogProbMetric: 28.0035

Epoch 774: val_loss improved from 28.00359 to 28.00354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.5415 - MinusLogProbMetric: 27.5415 - val_loss: 28.0035 - val_MinusLogProbMetric: 28.0035 - lr: 1.2500e-04 - 43s/epoch - 221ms/step
Epoch 775/1000
2023-10-26 09:46:30.832 
Epoch 775/1000 
	 loss: 27.5438, MinusLogProbMetric: 27.5438, val_loss: 28.0157, val_MinusLogProbMetric: 28.0157

Epoch 775: val_loss did not improve from 28.00354
196/196 - 42s - loss: 27.5438 - MinusLogProbMetric: 27.5438 - val_loss: 28.0157 - val_MinusLogProbMetric: 28.0157 - lr: 1.2500e-04 - 42s/epoch - 217ms/step
Epoch 776/1000
2023-10-26 09:47:13.029 
Epoch 776/1000 
	 loss: 27.5384, MinusLogProbMetric: 27.5384, val_loss: 28.0792, val_MinusLogProbMetric: 28.0792

Epoch 776: val_loss did not improve from 28.00354
196/196 - 42s - loss: 27.5384 - MinusLogProbMetric: 27.5384 - val_loss: 28.0792 - val_MinusLogProbMetric: 28.0792 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 777/1000
2023-10-26 09:47:54.891 
Epoch 777/1000 
	 loss: 27.5441, MinusLogProbMetric: 27.5441, val_loss: 28.1236, val_MinusLogProbMetric: 28.1236

Epoch 777: val_loss did not improve from 28.00354
196/196 - 42s - loss: 27.5441 - MinusLogProbMetric: 27.5441 - val_loss: 28.1236 - val_MinusLogProbMetric: 28.1236 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 778/1000
2023-10-26 09:48:35.897 
Epoch 778/1000 
	 loss: 27.5382, MinusLogProbMetric: 27.5382, val_loss: 28.0807, val_MinusLogProbMetric: 28.0807

Epoch 778: val_loss did not improve from 28.00354
196/196 - 41s - loss: 27.5382 - MinusLogProbMetric: 27.5382 - val_loss: 28.0807 - val_MinusLogProbMetric: 28.0807 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 779/1000
2023-10-26 09:49:18.076 
Epoch 779/1000 
	 loss: 27.5556, MinusLogProbMetric: 27.5556, val_loss: 28.0694, val_MinusLogProbMetric: 28.0694

Epoch 779: val_loss did not improve from 28.00354
196/196 - 42s - loss: 27.5556 - MinusLogProbMetric: 27.5556 - val_loss: 28.0694 - val_MinusLogProbMetric: 28.0694 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 780/1000
2023-10-26 09:50:00.492 
Epoch 780/1000 
	 loss: 27.5459, MinusLogProbMetric: 27.5459, val_loss: 28.1286, val_MinusLogProbMetric: 28.1286

Epoch 780: val_loss did not improve from 28.00354
196/196 - 42s - loss: 27.5459 - MinusLogProbMetric: 27.5459 - val_loss: 28.1286 - val_MinusLogProbMetric: 28.1286 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 781/1000
2023-10-26 09:50:43.035 
Epoch 781/1000 
	 loss: 27.5459, MinusLogProbMetric: 27.5459, val_loss: 28.0410, val_MinusLogProbMetric: 28.0410

Epoch 781: val_loss did not improve from 28.00354
196/196 - 43s - loss: 27.5459 - MinusLogProbMetric: 27.5459 - val_loss: 28.0410 - val_MinusLogProbMetric: 28.0410 - lr: 1.2500e-04 - 43s/epoch - 217ms/step
Epoch 782/1000
2023-10-26 09:51:25.228 
Epoch 782/1000 
	 loss: 27.5354, MinusLogProbMetric: 27.5354, val_loss: 28.0644, val_MinusLogProbMetric: 28.0644

Epoch 782: val_loss did not improve from 28.00354
196/196 - 42s - loss: 27.5354 - MinusLogProbMetric: 27.5354 - val_loss: 28.0644 - val_MinusLogProbMetric: 28.0644 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 783/1000
2023-10-26 09:52:07.253 
Epoch 783/1000 
	 loss: 27.5464, MinusLogProbMetric: 27.5464, val_loss: 28.0293, val_MinusLogProbMetric: 28.0293

Epoch 783: val_loss did not improve from 28.00354
196/196 - 42s - loss: 27.5464 - MinusLogProbMetric: 27.5464 - val_loss: 28.0293 - val_MinusLogProbMetric: 28.0293 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 784/1000
2023-10-26 09:52:49.240 
Epoch 784/1000 
	 loss: 27.5382, MinusLogProbMetric: 27.5382, val_loss: 28.0224, val_MinusLogProbMetric: 28.0224

Epoch 784: val_loss did not improve from 28.00354
196/196 - 42s - loss: 27.5382 - MinusLogProbMetric: 27.5382 - val_loss: 28.0224 - val_MinusLogProbMetric: 28.0224 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 785/1000
2023-10-26 09:53:31.211 
Epoch 785/1000 
	 loss: 27.5367, MinusLogProbMetric: 27.5367, val_loss: 28.0883, val_MinusLogProbMetric: 28.0883

Epoch 785: val_loss did not improve from 28.00354
196/196 - 42s - loss: 27.5367 - MinusLogProbMetric: 27.5367 - val_loss: 28.0883 - val_MinusLogProbMetric: 28.0883 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 786/1000
2023-10-26 09:54:13.893 
Epoch 786/1000 
	 loss: 27.5438, MinusLogProbMetric: 27.5438, val_loss: 28.1105, val_MinusLogProbMetric: 28.1105

Epoch 786: val_loss did not improve from 28.00354
196/196 - 43s - loss: 27.5438 - MinusLogProbMetric: 27.5438 - val_loss: 28.1105 - val_MinusLogProbMetric: 28.1105 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 787/1000
2023-10-26 09:54:56.468 
Epoch 787/1000 
	 loss: 27.5451, MinusLogProbMetric: 27.5451, val_loss: 28.0174, val_MinusLogProbMetric: 28.0174

Epoch 787: val_loss did not improve from 28.00354
196/196 - 43s - loss: 27.5451 - MinusLogProbMetric: 27.5451 - val_loss: 28.0174 - val_MinusLogProbMetric: 28.0174 - lr: 1.2500e-04 - 43s/epoch - 217ms/step
Epoch 788/1000
2023-10-26 09:55:38.739 
Epoch 788/1000 
	 loss: 27.5296, MinusLogProbMetric: 27.5296, val_loss: 28.0052, val_MinusLogProbMetric: 28.0052

Epoch 788: val_loss did not improve from 28.00354
196/196 - 42s - loss: 27.5296 - MinusLogProbMetric: 27.5296 - val_loss: 28.0052 - val_MinusLogProbMetric: 28.0052 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 789/1000
2023-10-26 09:56:21.262 
Epoch 789/1000 
	 loss: 27.5374, MinusLogProbMetric: 27.5374, val_loss: 28.0177, val_MinusLogProbMetric: 28.0177

Epoch 789: val_loss did not improve from 28.00354
196/196 - 43s - loss: 27.5374 - MinusLogProbMetric: 27.5374 - val_loss: 28.0177 - val_MinusLogProbMetric: 28.0177 - lr: 1.2500e-04 - 43s/epoch - 217ms/step
Epoch 790/1000
2023-10-26 09:57:03.709 
Epoch 790/1000 
	 loss: 27.5336, MinusLogProbMetric: 27.5336, val_loss: 28.0364, val_MinusLogProbMetric: 28.0364

Epoch 790: val_loss did not improve from 28.00354
196/196 - 42s - loss: 27.5336 - MinusLogProbMetric: 27.5336 - val_loss: 28.0364 - val_MinusLogProbMetric: 28.0364 - lr: 1.2500e-04 - 42s/epoch - 217ms/step
Epoch 791/1000
2023-10-26 09:57:46.654 
Epoch 791/1000 
	 loss: 27.5329, MinusLogProbMetric: 27.5329, val_loss: 27.9976, val_MinusLogProbMetric: 27.9976

Epoch 791: val_loss improved from 28.00354 to 27.99760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 27.5329 - MinusLogProbMetric: 27.5329 - val_loss: 27.9976 - val_MinusLogProbMetric: 27.9976 - lr: 1.2500e-04 - 44s/epoch - 223ms/step
Epoch 792/1000
2023-10-26 09:58:29.418 
Epoch 792/1000 
	 loss: 27.5243, MinusLogProbMetric: 27.5243, val_loss: 28.0060, val_MinusLogProbMetric: 28.0060

Epoch 792: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5243 - MinusLogProbMetric: 27.5243 - val_loss: 28.0060 - val_MinusLogProbMetric: 28.0060 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 793/1000
2023-10-26 09:59:12.501 
Epoch 793/1000 
	 loss: 27.5334, MinusLogProbMetric: 27.5334, val_loss: 28.0176, val_MinusLogProbMetric: 28.0176

Epoch 793: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5334 - MinusLogProbMetric: 27.5334 - val_loss: 28.0176 - val_MinusLogProbMetric: 28.0176 - lr: 1.2500e-04 - 43s/epoch - 220ms/step
Epoch 794/1000
2023-10-26 09:59:53.098 
Epoch 794/1000 
	 loss: 27.5312, MinusLogProbMetric: 27.5312, val_loss: 28.0295, val_MinusLogProbMetric: 28.0295

Epoch 794: val_loss did not improve from 27.99760
196/196 - 41s - loss: 27.5312 - MinusLogProbMetric: 27.5312 - val_loss: 28.0295 - val_MinusLogProbMetric: 28.0295 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 795/1000
2023-10-26 10:00:34.094 
Epoch 795/1000 
	 loss: 27.5402, MinusLogProbMetric: 27.5402, val_loss: 28.0367, val_MinusLogProbMetric: 28.0367

Epoch 795: val_loss did not improve from 27.99760
196/196 - 41s - loss: 27.5402 - MinusLogProbMetric: 27.5402 - val_loss: 28.0367 - val_MinusLogProbMetric: 28.0367 - lr: 1.2500e-04 - 41s/epoch - 209ms/step
Epoch 796/1000
2023-10-26 10:01:11.638 
Epoch 796/1000 
	 loss: 27.5277, MinusLogProbMetric: 27.5277, val_loss: 28.0229, val_MinusLogProbMetric: 28.0229

Epoch 796: val_loss did not improve from 27.99760
196/196 - 38s - loss: 27.5277 - MinusLogProbMetric: 27.5277 - val_loss: 28.0229 - val_MinusLogProbMetric: 28.0229 - lr: 1.2500e-04 - 38s/epoch - 192ms/step
Epoch 797/1000
2023-10-26 10:01:49.238 
Epoch 797/1000 
	 loss: 27.5346, MinusLogProbMetric: 27.5346, val_loss: 28.0681, val_MinusLogProbMetric: 28.0681

Epoch 797: val_loss did not improve from 27.99760
196/196 - 38s - loss: 27.5346 - MinusLogProbMetric: 27.5346 - val_loss: 28.0681 - val_MinusLogProbMetric: 28.0681 - lr: 1.2500e-04 - 38s/epoch - 192ms/step
Epoch 798/1000
2023-10-26 10:02:29.554 
Epoch 798/1000 
	 loss: 27.5426, MinusLogProbMetric: 27.5426, val_loss: 28.0662, val_MinusLogProbMetric: 28.0662

Epoch 798: val_loss did not improve from 27.99760
196/196 - 40s - loss: 27.5426 - MinusLogProbMetric: 27.5426 - val_loss: 28.0662 - val_MinusLogProbMetric: 28.0662 - lr: 1.2500e-04 - 40s/epoch - 206ms/step
Epoch 799/1000
2023-10-26 10:03:12.483 
Epoch 799/1000 
	 loss: 27.5382, MinusLogProbMetric: 27.5382, val_loss: 28.0677, val_MinusLogProbMetric: 28.0677

Epoch 799: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5382 - MinusLogProbMetric: 27.5382 - val_loss: 28.0677 - val_MinusLogProbMetric: 28.0677 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 800/1000
2023-10-26 10:03:54.460 
Epoch 800/1000 
	 loss: 27.5380, MinusLogProbMetric: 27.5380, val_loss: 28.0029, val_MinusLogProbMetric: 28.0029

Epoch 800: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5380 - MinusLogProbMetric: 27.5380 - val_loss: 28.0029 - val_MinusLogProbMetric: 28.0029 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 801/1000
2023-10-26 10:04:37.079 
Epoch 801/1000 
	 loss: 27.5345, MinusLogProbMetric: 27.5345, val_loss: 28.0164, val_MinusLogProbMetric: 28.0164

Epoch 801: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5345 - MinusLogProbMetric: 27.5345 - val_loss: 28.0164 - val_MinusLogProbMetric: 28.0164 - lr: 1.2500e-04 - 43s/epoch - 217ms/step
Epoch 802/1000
2023-10-26 10:05:19.774 
Epoch 802/1000 
	 loss: 27.5370, MinusLogProbMetric: 27.5370, val_loss: 28.0293, val_MinusLogProbMetric: 28.0293

Epoch 802: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5370 - MinusLogProbMetric: 27.5370 - val_loss: 28.0293 - val_MinusLogProbMetric: 28.0293 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 803/1000
2023-10-26 10:06:01.903 
Epoch 803/1000 
	 loss: 27.5365, MinusLogProbMetric: 27.5365, val_loss: 28.1416, val_MinusLogProbMetric: 28.1416

Epoch 803: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5365 - MinusLogProbMetric: 27.5365 - val_loss: 28.1416 - val_MinusLogProbMetric: 28.1416 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 804/1000
2023-10-26 10:06:43.594 
Epoch 804/1000 
	 loss: 27.5456, MinusLogProbMetric: 27.5456, val_loss: 28.1080, val_MinusLogProbMetric: 28.1080

Epoch 804: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5456 - MinusLogProbMetric: 27.5456 - val_loss: 28.1080 - val_MinusLogProbMetric: 28.1080 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 805/1000
2023-10-26 10:07:25.836 
Epoch 805/1000 
	 loss: 27.5353, MinusLogProbMetric: 27.5353, val_loss: 28.0264, val_MinusLogProbMetric: 28.0264

Epoch 805: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5353 - MinusLogProbMetric: 27.5353 - val_loss: 28.0264 - val_MinusLogProbMetric: 28.0264 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 806/1000
2023-10-26 10:08:08.770 
Epoch 806/1000 
	 loss: 27.5420, MinusLogProbMetric: 27.5420, val_loss: 28.1638, val_MinusLogProbMetric: 28.1638

Epoch 806: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5420 - MinusLogProbMetric: 27.5420 - val_loss: 28.1638 - val_MinusLogProbMetric: 28.1638 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 807/1000
2023-10-26 10:08:50.895 
Epoch 807/1000 
	 loss: 27.5406, MinusLogProbMetric: 27.5406, val_loss: 28.0599, val_MinusLogProbMetric: 28.0599

Epoch 807: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5406 - MinusLogProbMetric: 27.5406 - val_loss: 28.0599 - val_MinusLogProbMetric: 28.0599 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 808/1000
2023-10-26 10:09:32.688 
Epoch 808/1000 
	 loss: 27.5347, MinusLogProbMetric: 27.5347, val_loss: 28.0856, val_MinusLogProbMetric: 28.0856

Epoch 808: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5347 - MinusLogProbMetric: 27.5347 - val_loss: 28.0856 - val_MinusLogProbMetric: 28.0856 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 809/1000
2023-10-26 10:10:14.748 
Epoch 809/1000 
	 loss: 27.5495, MinusLogProbMetric: 27.5495, val_loss: 28.0372, val_MinusLogProbMetric: 28.0372

Epoch 809: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5495 - MinusLogProbMetric: 27.5495 - val_loss: 28.0372 - val_MinusLogProbMetric: 28.0372 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 810/1000
2023-10-26 10:10:56.308 
Epoch 810/1000 
	 loss: 27.5301, MinusLogProbMetric: 27.5301, val_loss: 28.0599, val_MinusLogProbMetric: 28.0599

Epoch 810: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5301 - MinusLogProbMetric: 27.5301 - val_loss: 28.0599 - val_MinusLogProbMetric: 28.0599 - lr: 1.2500e-04 - 42s/epoch - 212ms/step
Epoch 811/1000
2023-10-26 10:11:38.198 
Epoch 811/1000 
	 loss: 27.5319, MinusLogProbMetric: 27.5319, val_loss: 28.0200, val_MinusLogProbMetric: 28.0200

Epoch 811: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5319 - MinusLogProbMetric: 27.5319 - val_loss: 28.0200 - val_MinusLogProbMetric: 28.0200 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 812/1000
2023-10-26 10:12:20.796 
Epoch 812/1000 
	 loss: 27.5299, MinusLogProbMetric: 27.5299, val_loss: 28.0005, val_MinusLogProbMetric: 28.0005

Epoch 812: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5299 - MinusLogProbMetric: 27.5299 - val_loss: 28.0005 - val_MinusLogProbMetric: 28.0005 - lr: 1.2500e-04 - 43s/epoch - 217ms/step
Epoch 813/1000
2023-10-26 10:13:03.478 
Epoch 813/1000 
	 loss: 27.5331, MinusLogProbMetric: 27.5331, val_loss: 28.0371, val_MinusLogProbMetric: 28.0371

Epoch 813: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5331 - MinusLogProbMetric: 27.5331 - val_loss: 28.0371 - val_MinusLogProbMetric: 28.0371 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 814/1000
2023-10-26 10:13:46.206 
Epoch 814/1000 
	 loss: 27.5307, MinusLogProbMetric: 27.5307, val_loss: 28.0340, val_MinusLogProbMetric: 28.0340

Epoch 814: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5307 - MinusLogProbMetric: 27.5307 - val_loss: 28.0340 - val_MinusLogProbMetric: 28.0340 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 815/1000
2023-10-26 10:14:28.424 
Epoch 815/1000 
	 loss: 27.5366, MinusLogProbMetric: 27.5366, val_loss: 28.0145, val_MinusLogProbMetric: 28.0145

Epoch 815: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5366 - MinusLogProbMetric: 27.5366 - val_loss: 28.0145 - val_MinusLogProbMetric: 28.0145 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 816/1000
2023-10-26 10:15:11.089 
Epoch 816/1000 
	 loss: 27.5311, MinusLogProbMetric: 27.5311, val_loss: 28.0319, val_MinusLogProbMetric: 28.0319

Epoch 816: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5311 - MinusLogProbMetric: 27.5311 - val_loss: 28.0319 - val_MinusLogProbMetric: 28.0319 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 817/1000
2023-10-26 10:15:52.817 
Epoch 817/1000 
	 loss: 27.5392, MinusLogProbMetric: 27.5392, val_loss: 28.0108, val_MinusLogProbMetric: 28.0108

Epoch 817: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5392 - MinusLogProbMetric: 27.5392 - val_loss: 28.0108 - val_MinusLogProbMetric: 28.0108 - lr: 1.2500e-04 - 42s/epoch - 213ms/step
Epoch 818/1000
2023-10-26 10:16:34.195 
Epoch 818/1000 
	 loss: 27.5338, MinusLogProbMetric: 27.5338, val_loss: 28.0226, val_MinusLogProbMetric: 28.0226

Epoch 818: val_loss did not improve from 27.99760
196/196 - 41s - loss: 27.5338 - MinusLogProbMetric: 27.5338 - val_loss: 28.0226 - val_MinusLogProbMetric: 28.0226 - lr: 1.2500e-04 - 41s/epoch - 211ms/step
Epoch 819/1000
2023-10-26 10:17:17.158 
Epoch 819/1000 
	 loss: 27.5371, MinusLogProbMetric: 27.5371, val_loss: 28.0302, val_MinusLogProbMetric: 28.0302

Epoch 819: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5371 - MinusLogProbMetric: 27.5371 - val_loss: 28.0302 - val_MinusLogProbMetric: 28.0302 - lr: 1.2500e-04 - 43s/epoch - 219ms/step
Epoch 820/1000
2023-10-26 10:17:59.117 
Epoch 820/1000 
	 loss: 27.5318, MinusLogProbMetric: 27.5318, val_loss: 28.0069, val_MinusLogProbMetric: 28.0069

Epoch 820: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5318 - MinusLogProbMetric: 27.5318 - val_loss: 28.0069 - val_MinusLogProbMetric: 28.0069 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 821/1000
2023-10-26 10:18:41.650 
Epoch 821/1000 
	 loss: 27.5255, MinusLogProbMetric: 27.5255, val_loss: 28.0411, val_MinusLogProbMetric: 28.0411

Epoch 821: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5255 - MinusLogProbMetric: 27.5255 - val_loss: 28.0411 - val_MinusLogProbMetric: 28.0411 - lr: 1.2500e-04 - 43s/epoch - 217ms/step
Epoch 822/1000
2023-10-26 10:19:27.753 
Epoch 822/1000 
	 loss: 27.5321, MinusLogProbMetric: 27.5321, val_loss: 28.0337, val_MinusLogProbMetric: 28.0337

Epoch 822: val_loss did not improve from 27.99760
196/196 - 46s - loss: 27.5321 - MinusLogProbMetric: 27.5321 - val_loss: 28.0337 - val_MinusLogProbMetric: 28.0337 - lr: 1.2500e-04 - 46s/epoch - 235ms/step
Epoch 823/1000
2023-10-26 10:20:10.421 
Epoch 823/1000 
	 loss: 27.5278, MinusLogProbMetric: 27.5278, val_loss: 28.0463, val_MinusLogProbMetric: 28.0463

Epoch 823: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5278 - MinusLogProbMetric: 27.5278 - val_loss: 28.0463 - val_MinusLogProbMetric: 28.0463 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 824/1000
2023-10-26 10:20:53.088 
Epoch 824/1000 
	 loss: 27.5337, MinusLogProbMetric: 27.5337, val_loss: 28.0151, val_MinusLogProbMetric: 28.0151

Epoch 824: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5337 - MinusLogProbMetric: 27.5337 - val_loss: 28.0151 - val_MinusLogProbMetric: 28.0151 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 825/1000
2023-10-26 10:21:34.975 
Epoch 825/1000 
	 loss: 27.5459, MinusLogProbMetric: 27.5459, val_loss: 28.0480, val_MinusLogProbMetric: 28.0480

Epoch 825: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5459 - MinusLogProbMetric: 27.5459 - val_loss: 28.0480 - val_MinusLogProbMetric: 28.0480 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 826/1000
2023-10-26 10:22:17.659 
Epoch 826/1000 
	 loss: 27.5267, MinusLogProbMetric: 27.5267, val_loss: 28.0116, val_MinusLogProbMetric: 28.0116

Epoch 826: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5267 - MinusLogProbMetric: 27.5267 - val_loss: 28.0116 - val_MinusLogProbMetric: 28.0116 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 827/1000
2023-10-26 10:22:59.936 
Epoch 827/1000 
	 loss: 27.5313, MinusLogProbMetric: 27.5313, val_loss: 28.0013, val_MinusLogProbMetric: 28.0013

Epoch 827: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5313 - MinusLogProbMetric: 27.5313 - val_loss: 28.0013 - val_MinusLogProbMetric: 28.0013 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 828/1000
2023-10-26 10:23:41.996 
Epoch 828/1000 
	 loss: 27.5385, MinusLogProbMetric: 27.5385, val_loss: 28.0128, val_MinusLogProbMetric: 28.0128

Epoch 828: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5385 - MinusLogProbMetric: 27.5385 - val_loss: 28.0128 - val_MinusLogProbMetric: 28.0128 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 829/1000
2023-10-26 10:24:24.126 
Epoch 829/1000 
	 loss: 27.5365, MinusLogProbMetric: 27.5365, val_loss: 28.0454, val_MinusLogProbMetric: 28.0454

Epoch 829: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5365 - MinusLogProbMetric: 27.5365 - val_loss: 28.0454 - val_MinusLogProbMetric: 28.0454 - lr: 1.2500e-04 - 42s/epoch - 215ms/step
Epoch 830/1000
2023-10-26 10:25:06.434 
Epoch 830/1000 
	 loss: 27.5418, MinusLogProbMetric: 27.5418, val_loss: 28.0633, val_MinusLogProbMetric: 28.0633

Epoch 830: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5418 - MinusLogProbMetric: 27.5418 - val_loss: 28.0633 - val_MinusLogProbMetric: 28.0633 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 831/1000
2023-10-26 10:25:48.860 
Epoch 831/1000 
	 loss: 27.5370, MinusLogProbMetric: 27.5370, val_loss: 28.0621, val_MinusLogProbMetric: 28.0621

Epoch 831: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5370 - MinusLogProbMetric: 27.5370 - val_loss: 28.0621 - val_MinusLogProbMetric: 28.0621 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 832/1000
2023-10-26 10:26:30.731 
Epoch 832/1000 
	 loss: 27.5448, MinusLogProbMetric: 27.5448, val_loss: 28.0094, val_MinusLogProbMetric: 28.0094

Epoch 832: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5448 - MinusLogProbMetric: 27.5448 - val_loss: 28.0094 - val_MinusLogProbMetric: 28.0094 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 833/1000
2023-10-26 10:27:13.134 
Epoch 833/1000 
	 loss: 27.5300, MinusLogProbMetric: 27.5300, val_loss: 28.0502, val_MinusLogProbMetric: 28.0502

Epoch 833: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5300 - MinusLogProbMetric: 27.5300 - val_loss: 28.0502 - val_MinusLogProbMetric: 28.0502 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 834/1000
2023-10-26 10:27:55.476 
Epoch 834/1000 
	 loss: 27.5329, MinusLogProbMetric: 27.5329, val_loss: 28.1608, val_MinusLogProbMetric: 28.1608

Epoch 834: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5329 - MinusLogProbMetric: 27.5329 - val_loss: 28.1608 - val_MinusLogProbMetric: 28.1608 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 835/1000
2023-10-26 10:28:38.212 
Epoch 835/1000 
	 loss: 27.5395, MinusLogProbMetric: 27.5395, val_loss: 28.0541, val_MinusLogProbMetric: 28.0541

Epoch 835: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5395 - MinusLogProbMetric: 27.5395 - val_loss: 28.0541 - val_MinusLogProbMetric: 28.0541 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 836/1000
2023-10-26 10:29:20.541 
Epoch 836/1000 
	 loss: 27.5356, MinusLogProbMetric: 27.5356, val_loss: 28.0603, val_MinusLogProbMetric: 28.0603

Epoch 836: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5356 - MinusLogProbMetric: 27.5356 - val_loss: 28.0603 - val_MinusLogProbMetric: 28.0603 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 837/1000
2023-10-26 10:30:02.911 
Epoch 837/1000 
	 loss: 27.5275, MinusLogProbMetric: 27.5275, val_loss: 28.0142, val_MinusLogProbMetric: 28.0142

Epoch 837: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5275 - MinusLogProbMetric: 27.5275 - val_loss: 28.0142 - val_MinusLogProbMetric: 28.0142 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 838/1000
2023-10-26 10:30:45.160 
Epoch 838/1000 
	 loss: 27.5344, MinusLogProbMetric: 27.5344, val_loss: 28.1291, val_MinusLogProbMetric: 28.1291

Epoch 838: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5344 - MinusLogProbMetric: 27.5344 - val_loss: 28.1291 - val_MinusLogProbMetric: 28.1291 - lr: 1.2500e-04 - 42s/epoch - 216ms/step
Epoch 839/1000
2023-10-26 10:31:27.039 
Epoch 839/1000 
	 loss: 27.5379, MinusLogProbMetric: 27.5379, val_loss: 28.0821, val_MinusLogProbMetric: 28.0821

Epoch 839: val_loss did not improve from 27.99760
196/196 - 42s - loss: 27.5379 - MinusLogProbMetric: 27.5379 - val_loss: 28.0821 - val_MinusLogProbMetric: 28.0821 - lr: 1.2500e-04 - 42s/epoch - 214ms/step
Epoch 840/1000
2023-10-26 10:32:09.623 
Epoch 840/1000 
	 loss: 27.5383, MinusLogProbMetric: 27.5383, val_loss: 28.0527, val_MinusLogProbMetric: 28.0527

Epoch 840: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5383 - MinusLogProbMetric: 27.5383 - val_loss: 28.0527 - val_MinusLogProbMetric: 28.0527 - lr: 1.2500e-04 - 43s/epoch - 217ms/step
Epoch 841/1000
2023-10-26 10:32:52.313 
Epoch 841/1000 
	 loss: 27.5280, MinusLogProbMetric: 27.5280, val_loss: 28.0306, val_MinusLogProbMetric: 28.0306

Epoch 841: val_loss did not improve from 27.99760
196/196 - 43s - loss: 27.5280 - MinusLogProbMetric: 27.5280 - val_loss: 28.0306 - val_MinusLogProbMetric: 28.0306 - lr: 1.2500e-04 - 43s/epoch - 218ms/step
Epoch 842/1000
2023-10-26 10:33:34.850 
Epoch 842/1000 
	 loss: 27.4938, MinusLogProbMetric: 27.4938, val_loss: 27.9780, val_MinusLogProbMetric: 27.9780

Epoch 842: val_loss improved from 27.99760 to 27.97797, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.4938 - MinusLogProbMetric: 27.4938 - val_loss: 27.9780 - val_MinusLogProbMetric: 27.9780 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 843/1000
2023-10-26 10:34:18.460 
Epoch 843/1000 
	 loss: 27.4879, MinusLogProbMetric: 27.4879, val_loss: 27.9781, val_MinusLogProbMetric: 27.9781

Epoch 843: val_loss did not improve from 27.97797
196/196 - 43s - loss: 27.4879 - MinusLogProbMetric: 27.4879 - val_loss: 27.9781 - val_MinusLogProbMetric: 27.9781 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 844/1000
2023-10-26 10:35:01.046 
Epoch 844/1000 
	 loss: 27.4926, MinusLogProbMetric: 27.4926, val_loss: 27.9931, val_MinusLogProbMetric: 27.9931

Epoch 844: val_loss did not improve from 27.97797
196/196 - 43s - loss: 27.4926 - MinusLogProbMetric: 27.4926 - val_loss: 27.9931 - val_MinusLogProbMetric: 27.9931 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 845/1000
2023-10-26 10:35:43.216 
Epoch 845/1000 
	 loss: 27.4912, MinusLogProbMetric: 27.4912, val_loss: 27.9972, val_MinusLogProbMetric: 27.9972

Epoch 845: val_loss did not improve from 27.97797
196/196 - 42s - loss: 27.4912 - MinusLogProbMetric: 27.4912 - val_loss: 27.9972 - val_MinusLogProbMetric: 27.9972 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 846/1000
2023-10-26 10:36:26.171 
Epoch 846/1000 
	 loss: 27.4900, MinusLogProbMetric: 27.4900, val_loss: 27.9859, val_MinusLogProbMetric: 27.9859

Epoch 846: val_loss did not improve from 27.97797
196/196 - 43s - loss: 27.4900 - MinusLogProbMetric: 27.4900 - val_loss: 27.9859 - val_MinusLogProbMetric: 27.9859 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 847/1000
2023-10-26 10:37:08.776 
Epoch 847/1000 
	 loss: 27.4912, MinusLogProbMetric: 27.4912, val_loss: 27.9858, val_MinusLogProbMetric: 27.9858

Epoch 847: val_loss did not improve from 27.97797
196/196 - 43s - loss: 27.4912 - MinusLogProbMetric: 27.4912 - val_loss: 27.9858 - val_MinusLogProbMetric: 27.9858 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 848/1000
2023-10-26 10:37:50.307 
Epoch 848/1000 
	 loss: 27.4945, MinusLogProbMetric: 27.4945, val_loss: 28.0195, val_MinusLogProbMetric: 28.0195

Epoch 848: val_loss did not improve from 27.97797
196/196 - 42s - loss: 27.4945 - MinusLogProbMetric: 27.4945 - val_loss: 28.0195 - val_MinusLogProbMetric: 28.0195 - lr: 6.2500e-05 - 42s/epoch - 212ms/step
Epoch 849/1000
2023-10-26 10:38:33.130 
Epoch 849/1000 
	 loss: 27.4936, MinusLogProbMetric: 27.4936, val_loss: 27.9859, val_MinusLogProbMetric: 27.9859

Epoch 849: val_loss did not improve from 27.97797
196/196 - 43s - loss: 27.4936 - MinusLogProbMetric: 27.4936 - val_loss: 27.9859 - val_MinusLogProbMetric: 27.9859 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 850/1000
2023-10-26 10:39:15.340 
Epoch 850/1000 
	 loss: 27.4859, MinusLogProbMetric: 27.4859, val_loss: 28.0041, val_MinusLogProbMetric: 28.0041

Epoch 850: val_loss did not improve from 27.97797
196/196 - 42s - loss: 27.4859 - MinusLogProbMetric: 27.4859 - val_loss: 28.0041 - val_MinusLogProbMetric: 28.0041 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 851/1000
2023-10-26 10:39:57.375 
Epoch 851/1000 
	 loss: 27.4860, MinusLogProbMetric: 27.4860, val_loss: 27.9818, val_MinusLogProbMetric: 27.9818

Epoch 851: val_loss did not improve from 27.97797
196/196 - 42s - loss: 27.4860 - MinusLogProbMetric: 27.4860 - val_loss: 27.9818 - val_MinusLogProbMetric: 27.9818 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 852/1000
2023-10-26 10:40:39.508 
Epoch 852/1000 
	 loss: 27.4871, MinusLogProbMetric: 27.4871, val_loss: 27.9889, val_MinusLogProbMetric: 27.9889

Epoch 852: val_loss did not improve from 27.97797
196/196 - 42s - loss: 27.4871 - MinusLogProbMetric: 27.4871 - val_loss: 27.9889 - val_MinusLogProbMetric: 27.9889 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 853/1000
2023-10-26 10:41:22.258 
Epoch 853/1000 
	 loss: 27.4852, MinusLogProbMetric: 27.4852, val_loss: 27.9861, val_MinusLogProbMetric: 27.9861

Epoch 853: val_loss did not improve from 27.97797
196/196 - 43s - loss: 27.4852 - MinusLogProbMetric: 27.4852 - val_loss: 27.9861 - val_MinusLogProbMetric: 27.9861 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 854/1000
2023-10-26 10:42:04.972 
Epoch 854/1000 
	 loss: 27.4846, MinusLogProbMetric: 27.4846, val_loss: 27.9798, val_MinusLogProbMetric: 27.9798

Epoch 854: val_loss did not improve from 27.97797
196/196 - 43s - loss: 27.4846 - MinusLogProbMetric: 27.4846 - val_loss: 27.9798 - val_MinusLogProbMetric: 27.9798 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 855/1000
2023-10-26 10:42:47.937 
Epoch 855/1000 
	 loss: 27.4883, MinusLogProbMetric: 27.4883, val_loss: 27.9758, val_MinusLogProbMetric: 27.9758

Epoch 855: val_loss improved from 27.97797 to 27.97579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 44s - loss: 27.4883 - MinusLogProbMetric: 27.4883 - val_loss: 27.9758 - val_MinusLogProbMetric: 27.9758 - lr: 6.2500e-05 - 44s/epoch - 222ms/step
Epoch 856/1000
2023-10-26 10:43:30.938 
Epoch 856/1000 
	 loss: 27.4846, MinusLogProbMetric: 27.4846, val_loss: 27.9894, val_MinusLogProbMetric: 27.9894

Epoch 856: val_loss did not improve from 27.97579
196/196 - 42s - loss: 27.4846 - MinusLogProbMetric: 27.4846 - val_loss: 27.9894 - val_MinusLogProbMetric: 27.9894 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 857/1000
2023-10-26 10:44:12.881 
Epoch 857/1000 
	 loss: 27.4841, MinusLogProbMetric: 27.4841, val_loss: 27.9840, val_MinusLogProbMetric: 27.9840

Epoch 857: val_loss did not improve from 27.97579
196/196 - 42s - loss: 27.4841 - MinusLogProbMetric: 27.4841 - val_loss: 27.9840 - val_MinusLogProbMetric: 27.9840 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 858/1000
2023-10-26 10:44:55.399 
Epoch 858/1000 
	 loss: 27.4888, MinusLogProbMetric: 27.4888, val_loss: 27.9914, val_MinusLogProbMetric: 27.9914

Epoch 858: val_loss did not improve from 27.97579
196/196 - 43s - loss: 27.4888 - MinusLogProbMetric: 27.4888 - val_loss: 27.9914 - val_MinusLogProbMetric: 27.9914 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 859/1000
2023-10-26 10:45:37.969 
Epoch 859/1000 
	 loss: 27.4848, MinusLogProbMetric: 27.4848, val_loss: 27.9815, val_MinusLogProbMetric: 27.9815

Epoch 859: val_loss did not improve from 27.97579
196/196 - 43s - loss: 27.4848 - MinusLogProbMetric: 27.4848 - val_loss: 27.9815 - val_MinusLogProbMetric: 27.9815 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 860/1000
2023-10-26 10:46:20.252 
Epoch 860/1000 
	 loss: 27.4875, MinusLogProbMetric: 27.4875, val_loss: 27.9732, val_MinusLogProbMetric: 27.9732

Epoch 860: val_loss improved from 27.97579 to 27.97318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.4875 - MinusLogProbMetric: 27.4875 - val_loss: 27.9732 - val_MinusLogProbMetric: 27.9732 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 861/1000
2023-10-26 10:47:03.115 
Epoch 861/1000 
	 loss: 27.4902, MinusLogProbMetric: 27.4902, val_loss: 27.9916, val_MinusLogProbMetric: 27.9916

Epoch 861: val_loss did not improve from 27.97318
196/196 - 42s - loss: 27.4902 - MinusLogProbMetric: 27.4902 - val_loss: 27.9916 - val_MinusLogProbMetric: 27.9916 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 862/1000
2023-10-26 10:47:44.905 
Epoch 862/1000 
	 loss: 27.4858, MinusLogProbMetric: 27.4858, val_loss: 27.9948, val_MinusLogProbMetric: 27.9948

Epoch 862: val_loss did not improve from 27.97318
196/196 - 42s - loss: 27.4858 - MinusLogProbMetric: 27.4858 - val_loss: 27.9948 - val_MinusLogProbMetric: 27.9948 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 863/1000
2023-10-26 10:48:27.532 
Epoch 863/1000 
	 loss: 27.4882, MinusLogProbMetric: 27.4882, val_loss: 27.9839, val_MinusLogProbMetric: 27.9839

Epoch 863: val_loss did not improve from 27.97318
196/196 - 43s - loss: 27.4882 - MinusLogProbMetric: 27.4882 - val_loss: 27.9839 - val_MinusLogProbMetric: 27.9839 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 864/1000
2023-10-26 10:49:09.970 
Epoch 864/1000 
	 loss: 27.4902, MinusLogProbMetric: 27.4902, val_loss: 27.9826, val_MinusLogProbMetric: 27.9826

Epoch 864: val_loss did not improve from 27.97318
196/196 - 42s - loss: 27.4902 - MinusLogProbMetric: 27.4902 - val_loss: 27.9826 - val_MinusLogProbMetric: 27.9826 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 865/1000
2023-10-26 10:49:52.303 
Epoch 865/1000 
	 loss: 27.4916, MinusLogProbMetric: 27.4916, val_loss: 27.9962, val_MinusLogProbMetric: 27.9962

Epoch 865: val_loss did not improve from 27.97318
196/196 - 42s - loss: 27.4916 - MinusLogProbMetric: 27.4916 - val_loss: 27.9962 - val_MinusLogProbMetric: 27.9962 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 866/1000
2023-10-26 10:50:34.747 
Epoch 866/1000 
	 loss: 27.4890, MinusLogProbMetric: 27.4890, val_loss: 28.0014, val_MinusLogProbMetric: 28.0014

Epoch 866: val_loss did not improve from 27.97318
196/196 - 42s - loss: 27.4890 - MinusLogProbMetric: 27.4890 - val_loss: 28.0014 - val_MinusLogProbMetric: 28.0014 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 867/1000
2023-10-26 10:51:16.977 
Epoch 867/1000 
	 loss: 27.4911, MinusLogProbMetric: 27.4911, val_loss: 27.9886, val_MinusLogProbMetric: 27.9886

Epoch 867: val_loss did not improve from 27.97318
196/196 - 42s - loss: 27.4911 - MinusLogProbMetric: 27.4911 - val_loss: 27.9886 - val_MinusLogProbMetric: 27.9886 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 868/1000
2023-10-26 10:51:58.666 
Epoch 868/1000 
	 loss: 27.4923, MinusLogProbMetric: 27.4923, val_loss: 27.9794, val_MinusLogProbMetric: 27.9794

Epoch 868: val_loss did not improve from 27.97318
196/196 - 42s - loss: 27.4923 - MinusLogProbMetric: 27.4923 - val_loss: 27.9794 - val_MinusLogProbMetric: 27.9794 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 869/1000
2023-10-26 10:52:41.410 
Epoch 869/1000 
	 loss: 27.4926, MinusLogProbMetric: 27.4926, val_loss: 27.9810, val_MinusLogProbMetric: 27.9810

Epoch 869: val_loss did not improve from 27.97318
196/196 - 43s - loss: 27.4926 - MinusLogProbMetric: 27.4926 - val_loss: 27.9810 - val_MinusLogProbMetric: 27.9810 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 870/1000
2023-10-26 10:53:24.140 
Epoch 870/1000 
	 loss: 27.4864, MinusLogProbMetric: 27.4864, val_loss: 27.9751, val_MinusLogProbMetric: 27.9751

Epoch 870: val_loss did not improve from 27.97318
196/196 - 43s - loss: 27.4864 - MinusLogProbMetric: 27.4864 - val_loss: 27.9751 - val_MinusLogProbMetric: 27.9751 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 871/1000
2023-10-26 10:54:06.681 
Epoch 871/1000 
	 loss: 27.4895, MinusLogProbMetric: 27.4895, val_loss: 27.9945, val_MinusLogProbMetric: 27.9945

Epoch 871: val_loss did not improve from 27.97318
196/196 - 43s - loss: 27.4895 - MinusLogProbMetric: 27.4895 - val_loss: 27.9945 - val_MinusLogProbMetric: 27.9945 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 872/1000
2023-10-26 10:54:49.192 
Epoch 872/1000 
	 loss: 27.4914, MinusLogProbMetric: 27.4914, val_loss: 27.9897, val_MinusLogProbMetric: 27.9897

Epoch 872: val_loss did not improve from 27.97318
196/196 - 43s - loss: 27.4914 - MinusLogProbMetric: 27.4914 - val_loss: 27.9897 - val_MinusLogProbMetric: 27.9897 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 873/1000
2023-10-26 10:55:32.010 
Epoch 873/1000 
	 loss: 27.4845, MinusLogProbMetric: 27.4845, val_loss: 27.9765, val_MinusLogProbMetric: 27.9765

Epoch 873: val_loss did not improve from 27.97318
196/196 - 43s - loss: 27.4845 - MinusLogProbMetric: 27.4845 - val_loss: 27.9765 - val_MinusLogProbMetric: 27.9765 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 874/1000
2023-10-26 10:56:14.724 
Epoch 874/1000 
	 loss: 27.4896, MinusLogProbMetric: 27.4896, val_loss: 28.0034, val_MinusLogProbMetric: 28.0034

Epoch 874: val_loss did not improve from 27.97318
196/196 - 43s - loss: 27.4896 - MinusLogProbMetric: 27.4896 - val_loss: 28.0034 - val_MinusLogProbMetric: 28.0034 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 875/1000
2023-10-26 10:56:56.821 
Epoch 875/1000 
	 loss: 27.4876, MinusLogProbMetric: 27.4876, val_loss: 27.9730, val_MinusLogProbMetric: 27.9730

Epoch 875: val_loss improved from 27.97318 to 27.97300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.4876 - MinusLogProbMetric: 27.4876 - val_loss: 27.9730 - val_MinusLogProbMetric: 27.9730 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 876/1000
2023-10-26 10:57:40.222 
Epoch 876/1000 
	 loss: 27.4873, MinusLogProbMetric: 27.4873, val_loss: 27.9832, val_MinusLogProbMetric: 27.9832

Epoch 876: val_loss did not improve from 27.97300
196/196 - 43s - loss: 27.4873 - MinusLogProbMetric: 27.4873 - val_loss: 27.9832 - val_MinusLogProbMetric: 27.9832 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 877/1000
2023-10-26 10:58:22.693 
Epoch 877/1000 
	 loss: 27.4851, MinusLogProbMetric: 27.4851, val_loss: 27.9773, val_MinusLogProbMetric: 27.9773

Epoch 877: val_loss did not improve from 27.97300
196/196 - 42s - loss: 27.4851 - MinusLogProbMetric: 27.4851 - val_loss: 27.9773 - val_MinusLogProbMetric: 27.9773 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 878/1000
2023-10-26 10:59:03.376 
Epoch 878/1000 
	 loss: 27.4853, MinusLogProbMetric: 27.4853, val_loss: 27.9841, val_MinusLogProbMetric: 27.9841

Epoch 878: val_loss did not improve from 27.97300
196/196 - 41s - loss: 27.4853 - MinusLogProbMetric: 27.4853 - val_loss: 27.9841 - val_MinusLogProbMetric: 27.9841 - lr: 6.2500e-05 - 41s/epoch - 208ms/step
Epoch 879/1000
2023-10-26 10:59:45.851 
Epoch 879/1000 
	 loss: 27.4910, MinusLogProbMetric: 27.4910, val_loss: 28.0164, val_MinusLogProbMetric: 28.0164

Epoch 879: val_loss did not improve from 27.97300
196/196 - 42s - loss: 27.4910 - MinusLogProbMetric: 27.4910 - val_loss: 28.0164 - val_MinusLogProbMetric: 28.0164 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 880/1000
2023-10-26 11:00:28.036 
Epoch 880/1000 
	 loss: 27.4875, MinusLogProbMetric: 27.4875, val_loss: 28.0246, val_MinusLogProbMetric: 28.0246

Epoch 880: val_loss did not improve from 27.97300
196/196 - 42s - loss: 27.4875 - MinusLogProbMetric: 27.4875 - val_loss: 28.0246 - val_MinusLogProbMetric: 28.0246 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 881/1000
2023-10-26 11:01:10.581 
Epoch 881/1000 
	 loss: 27.4918, MinusLogProbMetric: 27.4918, val_loss: 27.9888, val_MinusLogProbMetric: 27.9888

Epoch 881: val_loss did not improve from 27.97300
196/196 - 43s - loss: 27.4918 - MinusLogProbMetric: 27.4918 - val_loss: 27.9888 - val_MinusLogProbMetric: 27.9888 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 882/1000
2023-10-26 11:01:53.164 
Epoch 882/1000 
	 loss: 27.4896, MinusLogProbMetric: 27.4896, val_loss: 27.9870, val_MinusLogProbMetric: 27.9870

Epoch 882: val_loss did not improve from 27.97300
196/196 - 43s - loss: 27.4896 - MinusLogProbMetric: 27.4896 - val_loss: 27.9870 - val_MinusLogProbMetric: 27.9870 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 883/1000
2023-10-26 11:02:35.886 
Epoch 883/1000 
	 loss: 27.4846, MinusLogProbMetric: 27.4846, val_loss: 28.0004, val_MinusLogProbMetric: 28.0004

Epoch 883: val_loss did not improve from 27.97300
196/196 - 43s - loss: 27.4846 - MinusLogProbMetric: 27.4846 - val_loss: 28.0004 - val_MinusLogProbMetric: 28.0004 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 884/1000
2023-10-26 11:03:18.889 
Epoch 884/1000 
	 loss: 27.4929, MinusLogProbMetric: 27.4929, val_loss: 27.9882, val_MinusLogProbMetric: 27.9882

Epoch 884: val_loss did not improve from 27.97300
196/196 - 43s - loss: 27.4929 - MinusLogProbMetric: 27.4929 - val_loss: 27.9882 - val_MinusLogProbMetric: 27.9882 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 885/1000
2023-10-26 11:04:02.033 
Epoch 885/1000 
	 loss: 27.4841, MinusLogProbMetric: 27.4841, val_loss: 27.9871, val_MinusLogProbMetric: 27.9871

Epoch 885: val_loss did not improve from 27.97300
196/196 - 43s - loss: 27.4841 - MinusLogProbMetric: 27.4841 - val_loss: 27.9871 - val_MinusLogProbMetric: 27.9871 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 886/1000
2023-10-26 11:04:44.482 
Epoch 886/1000 
	 loss: 27.4857, MinusLogProbMetric: 27.4857, val_loss: 27.9750, val_MinusLogProbMetric: 27.9750

Epoch 886: val_loss did not improve from 27.97300
196/196 - 42s - loss: 27.4857 - MinusLogProbMetric: 27.4857 - val_loss: 27.9750 - val_MinusLogProbMetric: 27.9750 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 887/1000
2023-10-26 11:05:26.795 
Epoch 887/1000 
	 loss: 27.4814, MinusLogProbMetric: 27.4814, val_loss: 27.9780, val_MinusLogProbMetric: 27.9780

Epoch 887: val_loss did not improve from 27.97300
196/196 - 42s - loss: 27.4814 - MinusLogProbMetric: 27.4814 - val_loss: 27.9780 - val_MinusLogProbMetric: 27.9780 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 888/1000
2023-10-26 11:06:09.291 
Epoch 888/1000 
	 loss: 27.4883, MinusLogProbMetric: 27.4883, val_loss: 27.9791, val_MinusLogProbMetric: 27.9791

Epoch 888: val_loss did not improve from 27.97300
196/196 - 42s - loss: 27.4883 - MinusLogProbMetric: 27.4883 - val_loss: 27.9791 - val_MinusLogProbMetric: 27.9791 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 889/1000
2023-10-26 11:06:51.758 
Epoch 889/1000 
	 loss: 27.4839, MinusLogProbMetric: 27.4839, val_loss: 27.9842, val_MinusLogProbMetric: 27.9842

Epoch 889: val_loss did not improve from 27.97300
196/196 - 42s - loss: 27.4839 - MinusLogProbMetric: 27.4839 - val_loss: 27.9842 - val_MinusLogProbMetric: 27.9842 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 890/1000
2023-10-26 11:07:34.251 
Epoch 890/1000 
	 loss: 27.4840, MinusLogProbMetric: 27.4840, val_loss: 28.0407, val_MinusLogProbMetric: 28.0407

Epoch 890: val_loss did not improve from 27.97300
196/196 - 42s - loss: 27.4840 - MinusLogProbMetric: 27.4840 - val_loss: 28.0407 - val_MinusLogProbMetric: 28.0407 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 891/1000
2023-10-26 11:08:17.053 
Epoch 891/1000 
	 loss: 27.4892, MinusLogProbMetric: 27.4892, val_loss: 28.0211, val_MinusLogProbMetric: 28.0211

Epoch 891: val_loss did not improve from 27.97300
196/196 - 43s - loss: 27.4892 - MinusLogProbMetric: 27.4892 - val_loss: 28.0211 - val_MinusLogProbMetric: 28.0211 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 892/1000
2023-10-26 11:09:00.004 
Epoch 892/1000 
	 loss: 27.4854, MinusLogProbMetric: 27.4854, val_loss: 28.0660, val_MinusLogProbMetric: 28.0660

Epoch 892: val_loss did not improve from 27.97300
196/196 - 43s - loss: 27.4854 - MinusLogProbMetric: 27.4854 - val_loss: 28.0660 - val_MinusLogProbMetric: 28.0660 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 893/1000
2023-10-26 11:09:42.288 
Epoch 893/1000 
	 loss: 27.4904, MinusLogProbMetric: 27.4904, val_loss: 27.9983, val_MinusLogProbMetric: 27.9983

Epoch 893: val_loss did not improve from 27.97300
196/196 - 42s - loss: 27.4904 - MinusLogProbMetric: 27.4904 - val_loss: 27.9983 - val_MinusLogProbMetric: 27.9983 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 894/1000
2023-10-26 11:10:24.667 
Epoch 894/1000 
	 loss: 27.4852, MinusLogProbMetric: 27.4852, val_loss: 27.9792, val_MinusLogProbMetric: 27.9792

Epoch 894: val_loss did not improve from 27.97300
196/196 - 42s - loss: 27.4852 - MinusLogProbMetric: 27.4852 - val_loss: 27.9792 - val_MinusLogProbMetric: 27.9792 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 895/1000
2023-10-26 11:11:07.108 
Epoch 895/1000 
	 loss: 27.4849, MinusLogProbMetric: 27.4849, val_loss: 27.9725, val_MinusLogProbMetric: 27.9725

Epoch 895: val_loss improved from 27.97300 to 27.97249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.4849 - MinusLogProbMetric: 27.4849 - val_loss: 27.9725 - val_MinusLogProbMetric: 27.9725 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 896/1000
2023-10-26 11:11:50.472 
Epoch 896/1000 
	 loss: 27.4885, MinusLogProbMetric: 27.4885, val_loss: 27.9848, val_MinusLogProbMetric: 27.9848

Epoch 896: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4885 - MinusLogProbMetric: 27.4885 - val_loss: 27.9848 - val_MinusLogProbMetric: 27.9848 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 897/1000
2023-10-26 11:12:32.809 
Epoch 897/1000 
	 loss: 27.4849, MinusLogProbMetric: 27.4849, val_loss: 27.9810, val_MinusLogProbMetric: 27.9810

Epoch 897: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4849 - MinusLogProbMetric: 27.4849 - val_loss: 27.9810 - val_MinusLogProbMetric: 27.9810 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 898/1000
2023-10-26 11:13:14.670 
Epoch 898/1000 
	 loss: 27.4860, MinusLogProbMetric: 27.4860, val_loss: 28.0005, val_MinusLogProbMetric: 28.0005

Epoch 898: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4860 - MinusLogProbMetric: 27.4860 - val_loss: 28.0005 - val_MinusLogProbMetric: 28.0005 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 899/1000
2023-10-26 11:13:56.090 
Epoch 899/1000 
	 loss: 27.4840, MinusLogProbMetric: 27.4840, val_loss: 27.9769, val_MinusLogProbMetric: 27.9769

Epoch 899: val_loss did not improve from 27.97249
196/196 - 41s - loss: 27.4840 - MinusLogProbMetric: 27.4840 - val_loss: 27.9769 - val_MinusLogProbMetric: 27.9769 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 900/1000
2023-10-26 11:14:38.124 
Epoch 900/1000 
	 loss: 27.4869, MinusLogProbMetric: 27.4869, val_loss: 27.9850, val_MinusLogProbMetric: 27.9850

Epoch 900: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4869 - MinusLogProbMetric: 27.4869 - val_loss: 27.9850 - val_MinusLogProbMetric: 27.9850 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 901/1000
2023-10-26 11:15:20.496 
Epoch 901/1000 
	 loss: 27.4819, MinusLogProbMetric: 27.4819, val_loss: 27.9812, val_MinusLogProbMetric: 27.9812

Epoch 901: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4819 - MinusLogProbMetric: 27.4819 - val_loss: 27.9812 - val_MinusLogProbMetric: 27.9812 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 902/1000
2023-10-26 11:16:02.728 
Epoch 902/1000 
	 loss: 27.4817, MinusLogProbMetric: 27.4817, val_loss: 27.9858, val_MinusLogProbMetric: 27.9858

Epoch 902: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4817 - MinusLogProbMetric: 27.4817 - val_loss: 27.9858 - val_MinusLogProbMetric: 27.9858 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 903/1000
2023-10-26 11:16:44.924 
Epoch 903/1000 
	 loss: 27.4875, MinusLogProbMetric: 27.4875, val_loss: 27.9924, val_MinusLogProbMetric: 27.9924

Epoch 903: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4875 - MinusLogProbMetric: 27.4875 - val_loss: 27.9924 - val_MinusLogProbMetric: 27.9924 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 904/1000
2023-10-26 11:17:27.034 
Epoch 904/1000 
	 loss: 27.4839, MinusLogProbMetric: 27.4839, val_loss: 27.9847, val_MinusLogProbMetric: 27.9847

Epoch 904: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4839 - MinusLogProbMetric: 27.4839 - val_loss: 27.9847 - val_MinusLogProbMetric: 27.9847 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 905/1000
2023-10-26 11:18:09.782 
Epoch 905/1000 
	 loss: 27.4820, MinusLogProbMetric: 27.4820, val_loss: 28.0231, val_MinusLogProbMetric: 28.0231

Epoch 905: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4820 - MinusLogProbMetric: 27.4820 - val_loss: 28.0231 - val_MinusLogProbMetric: 28.0231 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 906/1000
2023-10-26 11:18:51.982 
Epoch 906/1000 
	 loss: 27.4855, MinusLogProbMetric: 27.4855, val_loss: 27.9752, val_MinusLogProbMetric: 27.9752

Epoch 906: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4855 - MinusLogProbMetric: 27.4855 - val_loss: 27.9752 - val_MinusLogProbMetric: 27.9752 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 907/1000
2023-10-26 11:19:34.865 
Epoch 907/1000 
	 loss: 27.4849, MinusLogProbMetric: 27.4849, val_loss: 27.9839, val_MinusLogProbMetric: 27.9839

Epoch 907: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4849 - MinusLogProbMetric: 27.4849 - val_loss: 27.9839 - val_MinusLogProbMetric: 27.9839 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 908/1000
2023-10-26 11:20:17.398 
Epoch 908/1000 
	 loss: 27.4849, MinusLogProbMetric: 27.4849, val_loss: 28.0103, val_MinusLogProbMetric: 28.0103

Epoch 908: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4849 - MinusLogProbMetric: 27.4849 - val_loss: 28.0103 - val_MinusLogProbMetric: 28.0103 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 909/1000
2023-10-26 11:20:59.800 
Epoch 909/1000 
	 loss: 27.4858, MinusLogProbMetric: 27.4858, val_loss: 27.9731, val_MinusLogProbMetric: 27.9731

Epoch 909: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4858 - MinusLogProbMetric: 27.4858 - val_loss: 27.9731 - val_MinusLogProbMetric: 27.9731 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 910/1000
2023-10-26 11:21:42.631 
Epoch 910/1000 
	 loss: 27.4820, MinusLogProbMetric: 27.4820, val_loss: 27.9923, val_MinusLogProbMetric: 27.9923

Epoch 910: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4820 - MinusLogProbMetric: 27.4820 - val_loss: 27.9923 - val_MinusLogProbMetric: 27.9923 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 911/1000
2023-10-26 11:22:24.970 
Epoch 911/1000 
	 loss: 27.4823, MinusLogProbMetric: 27.4823, val_loss: 27.9725, val_MinusLogProbMetric: 27.9725

Epoch 911: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4823 - MinusLogProbMetric: 27.4823 - val_loss: 27.9725 - val_MinusLogProbMetric: 27.9725 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 912/1000
2023-10-26 11:23:07.131 
Epoch 912/1000 
	 loss: 27.4826, MinusLogProbMetric: 27.4826, val_loss: 28.0269, val_MinusLogProbMetric: 28.0269

Epoch 912: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4826 - MinusLogProbMetric: 27.4826 - val_loss: 28.0269 - val_MinusLogProbMetric: 28.0269 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 913/1000
2023-10-26 11:23:50.285 
Epoch 913/1000 
	 loss: 27.4832, MinusLogProbMetric: 27.4832, val_loss: 27.9855, val_MinusLogProbMetric: 27.9855

Epoch 913: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4832 - MinusLogProbMetric: 27.4832 - val_loss: 27.9855 - val_MinusLogProbMetric: 27.9855 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 914/1000
2023-10-26 11:24:32.894 
Epoch 914/1000 
	 loss: 27.4846, MinusLogProbMetric: 27.4846, val_loss: 27.9775, val_MinusLogProbMetric: 27.9775

Epoch 914: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4846 - MinusLogProbMetric: 27.4846 - val_loss: 27.9775 - val_MinusLogProbMetric: 27.9775 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 915/1000
2023-10-26 11:25:15.901 
Epoch 915/1000 
	 loss: 27.4835, MinusLogProbMetric: 27.4835, val_loss: 27.9735, val_MinusLogProbMetric: 27.9735

Epoch 915: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4835 - MinusLogProbMetric: 27.4835 - val_loss: 27.9735 - val_MinusLogProbMetric: 27.9735 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 916/1000
2023-10-26 11:25:58.062 
Epoch 916/1000 
	 loss: 27.4857, MinusLogProbMetric: 27.4857, val_loss: 27.9815, val_MinusLogProbMetric: 27.9815

Epoch 916: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4857 - MinusLogProbMetric: 27.4857 - val_loss: 27.9815 - val_MinusLogProbMetric: 27.9815 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 917/1000
2023-10-26 11:26:41.322 
Epoch 917/1000 
	 loss: 27.4821, MinusLogProbMetric: 27.4821, val_loss: 27.9811, val_MinusLogProbMetric: 27.9811

Epoch 917: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4821 - MinusLogProbMetric: 27.4821 - val_loss: 27.9811 - val_MinusLogProbMetric: 27.9811 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 918/1000
2023-10-26 11:27:24.214 
Epoch 918/1000 
	 loss: 27.4861, MinusLogProbMetric: 27.4861, val_loss: 27.9921, val_MinusLogProbMetric: 27.9921

Epoch 918: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4861 - MinusLogProbMetric: 27.4861 - val_loss: 27.9921 - val_MinusLogProbMetric: 27.9921 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 919/1000
2023-10-26 11:28:06.897 
Epoch 919/1000 
	 loss: 27.4877, MinusLogProbMetric: 27.4877, val_loss: 27.9969, val_MinusLogProbMetric: 27.9969

Epoch 919: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4877 - MinusLogProbMetric: 27.4877 - val_loss: 27.9969 - val_MinusLogProbMetric: 27.9969 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 920/1000
2023-10-26 11:28:49.135 
Epoch 920/1000 
	 loss: 27.4894, MinusLogProbMetric: 27.4894, val_loss: 28.0001, val_MinusLogProbMetric: 28.0001

Epoch 920: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4894 - MinusLogProbMetric: 27.4894 - val_loss: 28.0001 - val_MinusLogProbMetric: 28.0001 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 921/1000
2023-10-26 11:29:32.135 
Epoch 921/1000 
	 loss: 27.4832, MinusLogProbMetric: 27.4832, val_loss: 28.0708, val_MinusLogProbMetric: 28.0708

Epoch 921: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4832 - MinusLogProbMetric: 27.4832 - val_loss: 28.0708 - val_MinusLogProbMetric: 28.0708 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 922/1000
2023-10-26 11:30:14.495 
Epoch 922/1000 
	 loss: 27.4811, MinusLogProbMetric: 27.4811, val_loss: 27.9831, val_MinusLogProbMetric: 27.9831

Epoch 922: val_loss did not improve from 27.97249
196/196 - 42s - loss: 27.4811 - MinusLogProbMetric: 27.4811 - val_loss: 27.9831 - val_MinusLogProbMetric: 27.9831 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 923/1000
2023-10-26 11:30:57.292 
Epoch 923/1000 
	 loss: 27.4836, MinusLogProbMetric: 27.4836, val_loss: 27.9759, val_MinusLogProbMetric: 27.9759

Epoch 923: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4836 - MinusLogProbMetric: 27.4836 - val_loss: 27.9759 - val_MinusLogProbMetric: 27.9759 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 924/1000
2023-10-26 11:31:40.196 
Epoch 924/1000 
	 loss: 27.4811, MinusLogProbMetric: 27.4811, val_loss: 27.9856, val_MinusLogProbMetric: 27.9856

Epoch 924: val_loss did not improve from 27.97249
196/196 - 43s - loss: 27.4811 - MinusLogProbMetric: 27.4811 - val_loss: 27.9856 - val_MinusLogProbMetric: 27.9856 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 925/1000
2023-10-26 11:32:22.606 
Epoch 925/1000 
	 loss: 27.4820, MinusLogProbMetric: 27.4820, val_loss: 27.9666, val_MinusLogProbMetric: 27.9666

Epoch 925: val_loss improved from 27.97249 to 27.96662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.4820 - MinusLogProbMetric: 27.4820 - val_loss: 27.9666 - val_MinusLogProbMetric: 27.9666 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 926/1000
2023-10-26 11:33:06.087 
Epoch 926/1000 
	 loss: 27.4871, MinusLogProbMetric: 27.4871, val_loss: 27.9959, val_MinusLogProbMetric: 27.9959

Epoch 926: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4871 - MinusLogProbMetric: 27.4871 - val_loss: 27.9959 - val_MinusLogProbMetric: 27.9959 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 927/1000
2023-10-26 11:33:48.563 
Epoch 927/1000 
	 loss: 27.4848, MinusLogProbMetric: 27.4848, val_loss: 27.9933, val_MinusLogProbMetric: 27.9933

Epoch 927: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4848 - MinusLogProbMetric: 27.4848 - val_loss: 27.9933 - val_MinusLogProbMetric: 27.9933 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 928/1000
2023-10-26 11:34:31.335 
Epoch 928/1000 
	 loss: 27.4832, MinusLogProbMetric: 27.4832, val_loss: 27.9731, val_MinusLogProbMetric: 27.9731

Epoch 928: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4832 - MinusLogProbMetric: 27.4832 - val_loss: 27.9731 - val_MinusLogProbMetric: 27.9731 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 929/1000
2023-10-26 11:35:13.578 
Epoch 929/1000 
	 loss: 27.4783, MinusLogProbMetric: 27.4783, val_loss: 27.9910, val_MinusLogProbMetric: 27.9910

Epoch 929: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4783 - MinusLogProbMetric: 27.4783 - val_loss: 27.9910 - val_MinusLogProbMetric: 27.9910 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 930/1000
2023-10-26 11:35:55.977 
Epoch 930/1000 
	 loss: 27.4848, MinusLogProbMetric: 27.4848, val_loss: 27.9785, val_MinusLogProbMetric: 27.9785

Epoch 930: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4848 - MinusLogProbMetric: 27.4848 - val_loss: 27.9785 - val_MinusLogProbMetric: 27.9785 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 931/1000
2023-10-26 11:36:39.039 
Epoch 931/1000 
	 loss: 27.4809, MinusLogProbMetric: 27.4809, val_loss: 27.9815, val_MinusLogProbMetric: 27.9815

Epoch 931: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4809 - MinusLogProbMetric: 27.4809 - val_loss: 27.9815 - val_MinusLogProbMetric: 27.9815 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 932/1000
2023-10-26 11:37:21.963 
Epoch 932/1000 
	 loss: 27.4841, MinusLogProbMetric: 27.4841, val_loss: 27.9977, val_MinusLogProbMetric: 27.9977

Epoch 932: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4841 - MinusLogProbMetric: 27.4841 - val_loss: 27.9977 - val_MinusLogProbMetric: 27.9977 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 933/1000
2023-10-26 11:38:04.767 
Epoch 933/1000 
	 loss: 27.4855, MinusLogProbMetric: 27.4855, val_loss: 27.9944, val_MinusLogProbMetric: 27.9944

Epoch 933: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4855 - MinusLogProbMetric: 27.4855 - val_loss: 27.9944 - val_MinusLogProbMetric: 27.9944 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 934/1000
2023-10-26 11:38:47.528 
Epoch 934/1000 
	 loss: 27.4807, MinusLogProbMetric: 27.4807, val_loss: 27.9999, val_MinusLogProbMetric: 27.9999

Epoch 934: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4807 - MinusLogProbMetric: 27.4807 - val_loss: 27.9999 - val_MinusLogProbMetric: 27.9999 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 935/1000
2023-10-26 11:39:29.602 
Epoch 935/1000 
	 loss: 27.4825, MinusLogProbMetric: 27.4825, val_loss: 27.9949, val_MinusLogProbMetric: 27.9949

Epoch 935: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4825 - MinusLogProbMetric: 27.4825 - val_loss: 27.9949 - val_MinusLogProbMetric: 27.9949 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 936/1000
2023-10-26 11:40:12.119 
Epoch 936/1000 
	 loss: 27.4833, MinusLogProbMetric: 27.4833, val_loss: 28.0025, val_MinusLogProbMetric: 28.0025

Epoch 936: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4833 - MinusLogProbMetric: 27.4833 - val_loss: 28.0025 - val_MinusLogProbMetric: 28.0025 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 937/1000
2023-10-26 11:40:54.719 
Epoch 937/1000 
	 loss: 27.4898, MinusLogProbMetric: 27.4898, val_loss: 27.9821, val_MinusLogProbMetric: 27.9821

Epoch 937: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4898 - MinusLogProbMetric: 27.4898 - val_loss: 27.9821 - val_MinusLogProbMetric: 27.9821 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 938/1000
2023-10-26 11:41:36.039 
Epoch 938/1000 
	 loss: 27.4825, MinusLogProbMetric: 27.4825, val_loss: 28.0537, val_MinusLogProbMetric: 28.0537

Epoch 938: val_loss did not improve from 27.96662
196/196 - 41s - loss: 27.4825 - MinusLogProbMetric: 27.4825 - val_loss: 28.0537 - val_MinusLogProbMetric: 28.0537 - lr: 6.2500e-05 - 41s/epoch - 211ms/step
Epoch 939/1000
2023-10-26 11:42:18.703 
Epoch 939/1000 
	 loss: 27.4825, MinusLogProbMetric: 27.4825, val_loss: 27.9815, val_MinusLogProbMetric: 27.9815

Epoch 939: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4825 - MinusLogProbMetric: 27.4825 - val_loss: 27.9815 - val_MinusLogProbMetric: 27.9815 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 940/1000
2023-10-26 11:43:01.055 
Epoch 940/1000 
	 loss: 27.4793, MinusLogProbMetric: 27.4793, val_loss: 27.9871, val_MinusLogProbMetric: 27.9871

Epoch 940: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4793 - MinusLogProbMetric: 27.4793 - val_loss: 27.9871 - val_MinusLogProbMetric: 27.9871 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 941/1000
2023-10-26 11:43:43.841 
Epoch 941/1000 
	 loss: 27.4844, MinusLogProbMetric: 27.4844, val_loss: 27.9867, val_MinusLogProbMetric: 27.9867

Epoch 941: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4844 - MinusLogProbMetric: 27.4844 - val_loss: 27.9867 - val_MinusLogProbMetric: 27.9867 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 942/1000
2023-10-26 11:44:27.212 
Epoch 942/1000 
	 loss: 27.4824, MinusLogProbMetric: 27.4824, val_loss: 27.9833, val_MinusLogProbMetric: 27.9833

Epoch 942: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4824 - MinusLogProbMetric: 27.4824 - val_loss: 27.9833 - val_MinusLogProbMetric: 27.9833 - lr: 6.2500e-05 - 43s/epoch - 221ms/step
Epoch 943/1000
2023-10-26 11:45:09.757 
Epoch 943/1000 
	 loss: 27.4830, MinusLogProbMetric: 27.4830, val_loss: 27.9840, val_MinusLogProbMetric: 27.9840

Epoch 943: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4830 - MinusLogProbMetric: 27.4830 - val_loss: 27.9840 - val_MinusLogProbMetric: 27.9840 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 944/1000
2023-10-26 11:45:52.220 
Epoch 944/1000 
	 loss: 27.4847, MinusLogProbMetric: 27.4847, val_loss: 28.0034, val_MinusLogProbMetric: 28.0034

Epoch 944: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4847 - MinusLogProbMetric: 27.4847 - val_loss: 28.0034 - val_MinusLogProbMetric: 28.0034 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 945/1000
2023-10-26 11:46:34.219 
Epoch 945/1000 
	 loss: 27.4857, MinusLogProbMetric: 27.4857, val_loss: 27.9930, val_MinusLogProbMetric: 27.9930

Epoch 945: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4857 - MinusLogProbMetric: 27.4857 - val_loss: 27.9930 - val_MinusLogProbMetric: 27.9930 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 946/1000
2023-10-26 11:47:16.912 
Epoch 946/1000 
	 loss: 27.4870, MinusLogProbMetric: 27.4870, val_loss: 27.9839, val_MinusLogProbMetric: 27.9839

Epoch 946: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4870 - MinusLogProbMetric: 27.4870 - val_loss: 27.9839 - val_MinusLogProbMetric: 27.9839 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 947/1000
2023-10-26 11:47:59.438 
Epoch 947/1000 
	 loss: 27.4809, MinusLogProbMetric: 27.4809, val_loss: 27.9777, val_MinusLogProbMetric: 27.9777

Epoch 947: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4809 - MinusLogProbMetric: 27.4809 - val_loss: 27.9777 - val_MinusLogProbMetric: 27.9777 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 948/1000
2023-10-26 11:48:41.953 
Epoch 948/1000 
	 loss: 27.4815, MinusLogProbMetric: 27.4815, val_loss: 28.0025, val_MinusLogProbMetric: 28.0025

Epoch 948: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4815 - MinusLogProbMetric: 27.4815 - val_loss: 28.0025 - val_MinusLogProbMetric: 28.0025 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 949/1000
2023-10-26 11:49:24.986 
Epoch 949/1000 
	 loss: 27.4786, MinusLogProbMetric: 27.4786, val_loss: 27.9991, val_MinusLogProbMetric: 27.9991

Epoch 949: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4786 - MinusLogProbMetric: 27.4786 - val_loss: 27.9991 - val_MinusLogProbMetric: 27.9991 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 950/1000
2023-10-26 11:50:07.344 
Epoch 950/1000 
	 loss: 27.4844, MinusLogProbMetric: 27.4844, val_loss: 27.9813, val_MinusLogProbMetric: 27.9813

Epoch 950: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4844 - MinusLogProbMetric: 27.4844 - val_loss: 27.9813 - val_MinusLogProbMetric: 27.9813 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 951/1000
2023-10-26 11:50:49.997 
Epoch 951/1000 
	 loss: 27.4821, MinusLogProbMetric: 27.4821, val_loss: 28.0019, val_MinusLogProbMetric: 28.0019

Epoch 951: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4821 - MinusLogProbMetric: 27.4821 - val_loss: 28.0019 - val_MinusLogProbMetric: 28.0019 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 952/1000
2023-10-26 11:51:32.777 
Epoch 952/1000 
	 loss: 27.4827, MinusLogProbMetric: 27.4827, val_loss: 27.9772, val_MinusLogProbMetric: 27.9772

Epoch 952: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4827 - MinusLogProbMetric: 27.4827 - val_loss: 27.9772 - val_MinusLogProbMetric: 27.9772 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 953/1000
2023-10-26 11:52:15.616 
Epoch 953/1000 
	 loss: 27.4798, MinusLogProbMetric: 27.4798, val_loss: 27.9791, val_MinusLogProbMetric: 27.9791

Epoch 953: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4798 - MinusLogProbMetric: 27.4798 - val_loss: 27.9791 - val_MinusLogProbMetric: 27.9791 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 954/1000
2023-10-26 11:52:57.993 
Epoch 954/1000 
	 loss: 27.4769, MinusLogProbMetric: 27.4769, val_loss: 27.9831, val_MinusLogProbMetric: 27.9831

Epoch 954: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4769 - MinusLogProbMetric: 27.4769 - val_loss: 27.9831 - val_MinusLogProbMetric: 27.9831 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 955/1000
2023-10-26 11:53:40.128 
Epoch 955/1000 
	 loss: 27.4802, MinusLogProbMetric: 27.4802, val_loss: 28.0036, val_MinusLogProbMetric: 28.0036

Epoch 955: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4802 - MinusLogProbMetric: 27.4802 - val_loss: 28.0036 - val_MinusLogProbMetric: 28.0036 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 956/1000
2023-10-26 11:54:22.849 
Epoch 956/1000 
	 loss: 27.4809, MinusLogProbMetric: 27.4809, val_loss: 27.9774, val_MinusLogProbMetric: 27.9774

Epoch 956: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4809 - MinusLogProbMetric: 27.4809 - val_loss: 27.9774 - val_MinusLogProbMetric: 27.9774 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 957/1000
2023-10-26 11:55:05.769 
Epoch 957/1000 
	 loss: 27.4749, MinusLogProbMetric: 27.4749, val_loss: 27.9785, val_MinusLogProbMetric: 27.9785

Epoch 957: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4749 - MinusLogProbMetric: 27.4749 - val_loss: 27.9785 - val_MinusLogProbMetric: 27.9785 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 958/1000
2023-10-26 11:55:47.918 
Epoch 958/1000 
	 loss: 27.4766, MinusLogProbMetric: 27.4766, val_loss: 27.9803, val_MinusLogProbMetric: 27.9803

Epoch 958: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4766 - MinusLogProbMetric: 27.4766 - val_loss: 27.9803 - val_MinusLogProbMetric: 27.9803 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 959/1000
2023-10-26 11:56:30.255 
Epoch 959/1000 
	 loss: 27.4759, MinusLogProbMetric: 27.4759, val_loss: 27.9962, val_MinusLogProbMetric: 27.9962

Epoch 959: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4759 - MinusLogProbMetric: 27.4759 - val_loss: 27.9962 - val_MinusLogProbMetric: 27.9962 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 960/1000
2023-10-26 11:57:12.585 
Epoch 960/1000 
	 loss: 27.4791, MinusLogProbMetric: 27.4791, val_loss: 27.9758, val_MinusLogProbMetric: 27.9758

Epoch 960: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4791 - MinusLogProbMetric: 27.4791 - val_loss: 27.9758 - val_MinusLogProbMetric: 27.9758 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 961/1000
2023-10-26 11:57:55.594 
Epoch 961/1000 
	 loss: 27.4737, MinusLogProbMetric: 27.4737, val_loss: 28.0202, val_MinusLogProbMetric: 28.0202

Epoch 961: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4737 - MinusLogProbMetric: 27.4737 - val_loss: 28.0202 - val_MinusLogProbMetric: 28.0202 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 962/1000
2023-10-26 11:58:38.062 
Epoch 962/1000 
	 loss: 27.4784, MinusLogProbMetric: 27.4784, val_loss: 27.9742, val_MinusLogProbMetric: 27.9742

Epoch 962: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4784 - MinusLogProbMetric: 27.4784 - val_loss: 27.9742 - val_MinusLogProbMetric: 27.9742 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 963/1000
2023-10-26 11:59:20.181 
Epoch 963/1000 
	 loss: 27.4776, MinusLogProbMetric: 27.4776, val_loss: 27.9900, val_MinusLogProbMetric: 27.9900

Epoch 963: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4776 - MinusLogProbMetric: 27.4776 - val_loss: 27.9900 - val_MinusLogProbMetric: 27.9900 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 964/1000
2023-10-26 12:00:02.774 
Epoch 964/1000 
	 loss: 27.4797, MinusLogProbMetric: 27.4797, val_loss: 27.9670, val_MinusLogProbMetric: 27.9670

Epoch 964: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4797 - MinusLogProbMetric: 27.4797 - val_loss: 27.9670 - val_MinusLogProbMetric: 27.9670 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 965/1000
2023-10-26 12:00:45.439 
Epoch 965/1000 
	 loss: 27.4755, MinusLogProbMetric: 27.4755, val_loss: 27.9741, val_MinusLogProbMetric: 27.9741

Epoch 965: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4755 - MinusLogProbMetric: 27.4755 - val_loss: 27.9741 - val_MinusLogProbMetric: 27.9741 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 966/1000
2023-10-26 12:01:28.476 
Epoch 966/1000 
	 loss: 27.4753, MinusLogProbMetric: 27.4753, val_loss: 27.9767, val_MinusLogProbMetric: 27.9767

Epoch 966: val_loss did not improve from 27.96662
196/196 - 43s - loss: 27.4753 - MinusLogProbMetric: 27.4753 - val_loss: 27.9767 - val_MinusLogProbMetric: 27.9767 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 967/1000
2023-10-26 12:02:10.615 
Epoch 967/1000 
	 loss: 27.4768, MinusLogProbMetric: 27.4768, val_loss: 27.9828, val_MinusLogProbMetric: 27.9828

Epoch 967: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4768 - MinusLogProbMetric: 27.4768 - val_loss: 27.9828 - val_MinusLogProbMetric: 27.9828 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 968/1000
2023-10-26 12:02:52.629 
Epoch 968/1000 
	 loss: 27.4795, MinusLogProbMetric: 27.4795, val_loss: 27.9946, val_MinusLogProbMetric: 27.9946

Epoch 968: val_loss did not improve from 27.96662
196/196 - 42s - loss: 27.4795 - MinusLogProbMetric: 27.4795 - val_loss: 27.9946 - val_MinusLogProbMetric: 27.9946 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 969/1000
2023-10-26 12:03:34.626 
Epoch 969/1000 
	 loss: 27.4804, MinusLogProbMetric: 27.4804, val_loss: 27.9641, val_MinusLogProbMetric: 27.9641

Epoch 969: val_loss improved from 27.96662 to 27.96405, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_371/weights/best_weights.h5
196/196 - 43s - loss: 27.4804 - MinusLogProbMetric: 27.4804 - val_loss: 27.9641 - val_MinusLogProbMetric: 27.9641 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 970/1000
2023-10-26 12:04:17.761 
Epoch 970/1000 
	 loss: 27.4788, MinusLogProbMetric: 27.4788, val_loss: 27.9821, val_MinusLogProbMetric: 27.9821

Epoch 970: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4788 - MinusLogProbMetric: 27.4788 - val_loss: 27.9821 - val_MinusLogProbMetric: 27.9821 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 971/1000
2023-10-26 12:05:00.816 
Epoch 971/1000 
	 loss: 27.4772, MinusLogProbMetric: 27.4772, val_loss: 27.9851, val_MinusLogProbMetric: 27.9851

Epoch 971: val_loss did not improve from 27.96405
196/196 - 43s - loss: 27.4772 - MinusLogProbMetric: 27.4772 - val_loss: 27.9851 - val_MinusLogProbMetric: 27.9851 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 972/1000
2023-10-26 12:05:43.474 
Epoch 972/1000 
	 loss: 27.4767, MinusLogProbMetric: 27.4767, val_loss: 27.9991, val_MinusLogProbMetric: 27.9991

Epoch 972: val_loss did not improve from 27.96405
196/196 - 43s - loss: 27.4767 - MinusLogProbMetric: 27.4767 - val_loss: 27.9991 - val_MinusLogProbMetric: 27.9991 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 973/1000
2023-10-26 12:06:25.789 
Epoch 973/1000 
	 loss: 27.4801, MinusLogProbMetric: 27.4801, val_loss: 27.9770, val_MinusLogProbMetric: 27.9770

Epoch 973: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4801 - MinusLogProbMetric: 27.4801 - val_loss: 27.9770 - val_MinusLogProbMetric: 27.9770 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 974/1000
2023-10-26 12:07:08.055 
Epoch 974/1000 
	 loss: 27.4793, MinusLogProbMetric: 27.4793, val_loss: 27.9803, val_MinusLogProbMetric: 27.9803

Epoch 974: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4793 - MinusLogProbMetric: 27.4793 - val_loss: 27.9803 - val_MinusLogProbMetric: 27.9803 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 975/1000
2023-10-26 12:07:50.440 
Epoch 975/1000 
	 loss: 27.4764, MinusLogProbMetric: 27.4764, val_loss: 27.9824, val_MinusLogProbMetric: 27.9824

Epoch 975: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4764 - MinusLogProbMetric: 27.4764 - val_loss: 27.9824 - val_MinusLogProbMetric: 27.9824 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 976/1000
2023-10-26 12:08:32.665 
Epoch 976/1000 
	 loss: 27.4785, MinusLogProbMetric: 27.4785, val_loss: 28.0042, val_MinusLogProbMetric: 28.0042

Epoch 976: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4785 - MinusLogProbMetric: 27.4785 - val_loss: 28.0042 - val_MinusLogProbMetric: 28.0042 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 977/1000
2023-10-26 12:09:15.551 
Epoch 977/1000 
	 loss: 27.4835, MinusLogProbMetric: 27.4835, val_loss: 28.0026, val_MinusLogProbMetric: 28.0026

Epoch 977: val_loss did not improve from 27.96405
196/196 - 43s - loss: 27.4835 - MinusLogProbMetric: 27.4835 - val_loss: 28.0026 - val_MinusLogProbMetric: 28.0026 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 978/1000
2023-10-26 12:09:58.345 
Epoch 978/1000 
	 loss: 27.4781, MinusLogProbMetric: 27.4781, val_loss: 27.9976, val_MinusLogProbMetric: 27.9976

Epoch 978: val_loss did not improve from 27.96405
196/196 - 43s - loss: 27.4781 - MinusLogProbMetric: 27.4781 - val_loss: 27.9976 - val_MinusLogProbMetric: 27.9976 - lr: 6.2500e-05 - 43s/epoch - 218ms/step
Epoch 979/1000
2023-10-26 12:10:40.968 
Epoch 979/1000 
	 loss: 27.4769, MinusLogProbMetric: 27.4769, val_loss: 28.0099, val_MinusLogProbMetric: 28.0099

Epoch 979: val_loss did not improve from 27.96405
196/196 - 43s - loss: 27.4769 - MinusLogProbMetric: 27.4769 - val_loss: 28.0099 - val_MinusLogProbMetric: 28.0099 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 980/1000
2023-10-26 12:11:23.563 
Epoch 980/1000 
	 loss: 27.4802, MinusLogProbMetric: 27.4802, val_loss: 28.0034, val_MinusLogProbMetric: 28.0034

Epoch 980: val_loss did not improve from 27.96405
196/196 - 43s - loss: 27.4802 - MinusLogProbMetric: 27.4802 - val_loss: 28.0034 - val_MinusLogProbMetric: 28.0034 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 981/1000
2023-10-26 12:12:05.360 
Epoch 981/1000 
	 loss: 27.4808, MinusLogProbMetric: 27.4808, val_loss: 27.9726, val_MinusLogProbMetric: 27.9726

Epoch 981: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4808 - MinusLogProbMetric: 27.4808 - val_loss: 27.9726 - val_MinusLogProbMetric: 27.9726 - lr: 6.2500e-05 - 42s/epoch - 213ms/step
Epoch 982/1000
2023-10-26 12:12:46.430 
Epoch 982/1000 
	 loss: 27.4815, MinusLogProbMetric: 27.4815, val_loss: 27.9917, val_MinusLogProbMetric: 27.9917

Epoch 982: val_loss did not improve from 27.96405
196/196 - 41s - loss: 27.4815 - MinusLogProbMetric: 27.4815 - val_loss: 27.9917 - val_MinusLogProbMetric: 27.9917 - lr: 6.2500e-05 - 41s/epoch - 210ms/step
Epoch 983/1000
2023-10-26 12:13:28.689 
Epoch 983/1000 
	 loss: 27.4822, MinusLogProbMetric: 27.4822, val_loss: 28.0047, val_MinusLogProbMetric: 28.0047

Epoch 983: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4822 - MinusLogProbMetric: 27.4822 - val_loss: 28.0047 - val_MinusLogProbMetric: 28.0047 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 984/1000
2023-10-26 12:14:11.008 
Epoch 984/1000 
	 loss: 27.4791, MinusLogProbMetric: 27.4791, val_loss: 27.9880, val_MinusLogProbMetric: 27.9880

Epoch 984: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4791 - MinusLogProbMetric: 27.4791 - val_loss: 27.9880 - val_MinusLogProbMetric: 27.9880 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 985/1000
2023-10-26 12:14:53.457 
Epoch 985/1000 
	 loss: 27.4754, MinusLogProbMetric: 27.4754, val_loss: 28.0160, val_MinusLogProbMetric: 28.0160

Epoch 985: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4754 - MinusLogProbMetric: 27.4754 - val_loss: 28.0160 - val_MinusLogProbMetric: 28.0160 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 986/1000
2023-10-26 12:15:35.479 
Epoch 986/1000 
	 loss: 27.4810, MinusLogProbMetric: 27.4810, val_loss: 27.9897, val_MinusLogProbMetric: 27.9897

Epoch 986: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4810 - MinusLogProbMetric: 27.4810 - val_loss: 27.9897 - val_MinusLogProbMetric: 27.9897 - lr: 6.2500e-05 - 42s/epoch - 214ms/step
Epoch 987/1000
2023-10-26 12:16:17.604 
Epoch 987/1000 
	 loss: 27.4793, MinusLogProbMetric: 27.4793, val_loss: 27.9900, val_MinusLogProbMetric: 27.9900

Epoch 987: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4793 - MinusLogProbMetric: 27.4793 - val_loss: 27.9900 - val_MinusLogProbMetric: 27.9900 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 988/1000
2023-10-26 12:16:59.701 
Epoch 988/1000 
	 loss: 27.4768, MinusLogProbMetric: 27.4768, val_loss: 27.9910, val_MinusLogProbMetric: 27.9910

Epoch 988: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4768 - MinusLogProbMetric: 27.4768 - val_loss: 27.9910 - val_MinusLogProbMetric: 27.9910 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 989/1000
2023-10-26 12:17:41.954 
Epoch 989/1000 
	 loss: 27.4862, MinusLogProbMetric: 27.4862, val_loss: 27.9883, val_MinusLogProbMetric: 27.9883

Epoch 989: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4862 - MinusLogProbMetric: 27.4862 - val_loss: 27.9883 - val_MinusLogProbMetric: 27.9883 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 990/1000
2023-10-26 12:18:24.200 
Epoch 990/1000 
	 loss: 27.4782, MinusLogProbMetric: 27.4782, val_loss: 27.9802, val_MinusLogProbMetric: 27.9802

Epoch 990: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4782 - MinusLogProbMetric: 27.4782 - val_loss: 27.9802 - val_MinusLogProbMetric: 27.9802 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Epoch 991/1000
2023-10-26 12:19:07.320 
Epoch 991/1000 
	 loss: 27.4790, MinusLogProbMetric: 27.4790, val_loss: 27.9825, val_MinusLogProbMetric: 27.9825

Epoch 991: val_loss did not improve from 27.96405
196/196 - 43s - loss: 27.4790 - MinusLogProbMetric: 27.4790 - val_loss: 27.9825 - val_MinusLogProbMetric: 27.9825 - lr: 6.2500e-05 - 43s/epoch - 220ms/step
Epoch 992/1000
2023-10-26 12:19:49.908 
Epoch 992/1000 
	 loss: 27.4814, MinusLogProbMetric: 27.4814, val_loss: 27.9952, val_MinusLogProbMetric: 27.9952

Epoch 992: val_loss did not improve from 27.96405
196/196 - 43s - loss: 27.4814 - MinusLogProbMetric: 27.4814 - val_loss: 27.9952 - val_MinusLogProbMetric: 27.9952 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 993/1000
2023-10-26 12:20:32.539 
Epoch 993/1000 
	 loss: 27.4774, MinusLogProbMetric: 27.4774, val_loss: 27.9784, val_MinusLogProbMetric: 27.9784

Epoch 993: val_loss did not improve from 27.96405
196/196 - 43s - loss: 27.4774 - MinusLogProbMetric: 27.4774 - val_loss: 27.9784 - val_MinusLogProbMetric: 27.9784 - lr: 6.2500e-05 - 43s/epoch - 217ms/step
Epoch 994/1000
2023-10-26 12:21:15.410 
Epoch 994/1000 
	 loss: 27.4762, MinusLogProbMetric: 27.4762, val_loss: 27.9939, val_MinusLogProbMetric: 27.9939

Epoch 994: val_loss did not improve from 27.96405
196/196 - 43s - loss: 27.4762 - MinusLogProbMetric: 27.4762 - val_loss: 27.9939 - val_MinusLogProbMetric: 27.9939 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 995/1000
2023-10-26 12:21:57.618 
Epoch 995/1000 
	 loss: 27.4794, MinusLogProbMetric: 27.4794, val_loss: 27.9923, val_MinusLogProbMetric: 27.9923

Epoch 995: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4794 - MinusLogProbMetric: 27.4794 - val_loss: 27.9923 - val_MinusLogProbMetric: 27.9923 - lr: 6.2500e-05 - 42s/epoch - 215ms/step
Epoch 996/1000
2023-10-26 12:22:40.459 
Epoch 996/1000 
	 loss: 27.4782, MinusLogProbMetric: 27.4782, val_loss: 27.9879, val_MinusLogProbMetric: 27.9879

Epoch 996: val_loss did not improve from 27.96405
196/196 - 43s - loss: 27.4782 - MinusLogProbMetric: 27.4782 - val_loss: 27.9879 - val_MinusLogProbMetric: 27.9879 - lr: 6.2500e-05 - 43s/epoch - 219ms/step
Epoch 997/1000
2023-10-26 12:23:22.956 
Epoch 997/1000 
	 loss: 27.4775, MinusLogProbMetric: 27.4775, val_loss: 27.9781, val_MinusLogProbMetric: 27.9781

Epoch 997: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4775 - MinusLogProbMetric: 27.4775 - val_loss: 27.9781 - val_MinusLogProbMetric: 27.9781 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 998/1000
2023-10-26 12:24:05.416 
Epoch 998/1000 
	 loss: 27.4761, MinusLogProbMetric: 27.4761, val_loss: 27.9799, val_MinusLogProbMetric: 27.9799

Epoch 998: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4761 - MinusLogProbMetric: 27.4761 - val_loss: 27.9799 - val_MinusLogProbMetric: 27.9799 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 999/1000
2023-10-26 12:24:47.910 
Epoch 999/1000 
	 loss: 27.4779, MinusLogProbMetric: 27.4779, val_loss: 27.9767, val_MinusLogProbMetric: 27.9767

Epoch 999: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4779 - MinusLogProbMetric: 27.4779 - val_loss: 27.9767 - val_MinusLogProbMetric: 27.9767 - lr: 6.2500e-05 - 42s/epoch - 217ms/step
Epoch 1000/1000
2023-10-26 12:25:30.157 
Epoch 1000/1000 
	 loss: 27.4777, MinusLogProbMetric: 27.4777, val_loss: 27.9796, val_MinusLogProbMetric: 27.9796

Epoch 1000: val_loss did not improve from 27.96405
196/196 - 42s - loss: 27.4777 - MinusLogProbMetric: 27.4777 - val_loss: 27.9796 - val_MinusLogProbMetric: 27.9796 - lr: 6.2500e-05 - 42s/epoch - 216ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 721.
Model trained in 41622.59 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.79 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.09 s.
===========
Run 371/720 done in 41630.01 s.
===========

Directory ../../results/CsplineN_new/run_372/ already exists.
Skipping it.
===========
Run 372/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_373/ already exists.
Skipping it.
===========
Run 373/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_374/ already exists.
Skipping it.
===========
Run 374/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_375/ already exists.
Skipping it.
===========
Run 375/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_376/ already exists.
Skipping it.
===========
Run 376/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_377/ already exists.
Skipping it.
===========
Run 377/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_378/ already exists.
Skipping it.
===========
Run 378/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_379/ already exists.
Skipping it.
===========
Run 379/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_380/ already exists.
Skipping it.
===========
Run 380/720 already exists. Skipping it.
===========

===========
Generating train data for run 381.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_381
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_71"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_72 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f2d46f0ffa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2d4c76bac0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2d4c76bac0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f33ecab83d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2d46f43790>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2d46f43d00>, <keras.callbacks.ModelCheckpoint object at 0x7f2d46f43dc0>, <keras.callbacks.EarlyStopping object at 0x7f2d46f43fd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2d46f43cd0>, <keras.callbacks.TerminateOnNaN object at 0x7f2d46f43f70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_381/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 381/720 with hyperparameters:
timestamp = 2023-10-26 12:25:40.081981
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:27:53.746 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7508.3936, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 133s - loss: nan - MinusLogProbMetric: 7508.3936 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 133s/epoch - 681ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 381.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_381
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_82"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_83 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f2fb8413d90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f303024d750>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f303024d750>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2f587cc0a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2eac5d0250>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2eac5d07c0>, <keras.callbacks.ModelCheckpoint object at 0x7f2eac5d0880>, <keras.callbacks.EarlyStopping object at 0x7f2eac5d0af0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2eac5d0b20>, <keras.callbacks.TerminateOnNaN object at 0x7f2eac5d0760>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_381/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 381/720 with hyperparameters:
timestamp = 2023-10-26 12:28:02.791860
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:30:10.892 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7502.2856, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 128s - loss: nan - MinusLogProbMetric: 7502.2856 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 128s/epoch - 653ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0001111111111111111.
===========
Generating train data for run 381.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_381
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_93"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_94 (InputLayer)       [(None, 64)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f2e407855d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2ee81eae30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2ee81eae30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2f982bffa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2e0c5179d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2e0c517f40>, <keras.callbacks.ModelCheckpoint object at 0x7f2e0c517ee0>, <keras.callbacks.EarlyStopping object at 0x7f2e0c517f10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2e0c5d00d0>, <keras.callbacks.TerminateOnNaN object at 0x7f2e0c5d02b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_381/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 381/720 with hyperparameters:
timestamp = 2023-10-26 12:30:19.755447
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:32:29.049 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7496.0752, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 129s - loss: nan - MinusLogProbMetric: 7496.0752 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 129s/epoch - 658ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 3.703703703703703e-05.
===========
Generating train data for run 381.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_381
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_104"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_105 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  1321920   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f2bdcdab400>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2bdc9127a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2bdc9127a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2bdc96bf10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2bdc1de440>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2bdc1de9b0>, <keras.callbacks.ModelCheckpoint object at 0x7f2bdc1dea70>, <keras.callbacks.EarlyStopping object at 0x7f2bdc1dece0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2bdc1ded10>, <keras.callbacks.TerminateOnNaN object at 0x7f2bdc1de950>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_381/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 381/720 with hyperparameters:
timestamp = 2023-10-26 12:32:36.335336
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:34:42.927 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7490.6055, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 126s - loss: nan - MinusLogProbMetric: 7490.6055 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 126s/epoch - 645ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.2345679012345677e-05.
===========
Generating train data for run 381.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_381
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_115"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_116 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f2e705237c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2bd834e740>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2bd834e740>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2d9c3bc520>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2d9c3948e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2d9c394e50>, <keras.callbacks.ModelCheckpoint object at 0x7f2d9c394f10>, <keras.callbacks.EarlyStopping object at 0x7f2d9c395180>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2d9c3951b0>, <keras.callbacks.TerminateOnNaN object at 0x7f2d9c394df0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_381/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 381/720 with hyperparameters:
timestamp = 2023-10-26 12:34:52.458346
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:37:02.824 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7504.1787, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 130s - loss: nan - MinusLogProbMetric: 7504.1787 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 130s/epoch - 664ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.115226337448558e-06.
===========
Generating train data for run 381.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_381
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_126"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_127 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f2bd81b3f40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2b8954b7f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2b8954b7f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2bdd15c400>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2bb8420700>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2bb8420c70>, <keras.callbacks.ModelCheckpoint object at 0x7f2bb8420d30>, <keras.callbacks.EarlyStopping object at 0x7f2bb8420fa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2bb8420fd0>, <keras.callbacks.TerminateOnNaN object at 0x7f2bb8420c10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_381/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 381/720 with hyperparameters:
timestamp = 2023-10-26 12:37:10.294345
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:39:18.020 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7490.8267, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 128s - loss: nan - MinusLogProbMetric: 7490.8267 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 128s/epoch - 651ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.3717421124828526e-06.
===========
Generating train data for run 381.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_381
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_137"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_138 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f2bd80f2b00>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2b9477b850>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2b9477b850>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2f9820fc10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2f3829bdc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2f38214370>, <keras.callbacks.ModelCheckpoint object at 0x7f2f38214430>, <keras.callbacks.EarlyStopping object at 0x7f2f382146a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2f382146d0>, <keras.callbacks.TerminateOnNaN object at 0x7f2f38214310>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_381/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 381/720 with hyperparameters:
timestamp = 2023-10-26 12:39:24.878153
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:41:32.753 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7498.5215, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 128s - loss: nan - MinusLogProbMetric: 7498.5215 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 128s/epoch - 652ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.572473708276175e-07.
===========
Generating train data for run 381.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_381
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_148"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_149 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f2b88a2f370>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2bdd068fa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2bdd068fa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b88b37490>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2b7dfad7b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2b7dfadd20>, <keras.callbacks.ModelCheckpoint object at 0x7f2b7dfadde0>, <keras.callbacks.EarlyStopping object at 0x7f2b7dfae050>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2b7dfae080>, <keras.callbacks.TerminateOnNaN object at 0x7f2b7dfadcc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_381/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 381/720 with hyperparameters:
timestamp = 2023-10-26 12:41:41.004450
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:43:49.160 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7501.2461, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 128s - loss: nan - MinusLogProbMetric: 7501.2461 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 128s/epoch - 653ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.524157902758725e-07.
===========
Generating train data for run 381.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_381
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_159"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_160 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f2f982db490>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2e1c6744f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2e1c6744f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b7d6ca9b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2ffc1ce1d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2ffc1ce740>, <keras.callbacks.ModelCheckpoint object at 0x7f2ffc1ce800>, <keras.callbacks.EarlyStopping object at 0x7f2ffc1cea70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2ffc1ceaa0>, <keras.callbacks.TerminateOnNaN object at 0x7f2ffc1ce6e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_381/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 381/720 with hyperparameters:
timestamp = 2023-10-26 12:43:59.042715
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x7f2c20185750> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:46:06.509 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7497.1821, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 127s - loss: nan - MinusLogProbMetric: 7497.1821 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 127s/epoch - 649ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.0805263425290834e-08.
===========
Generating train data for run 381.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_381
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_170"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_171 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f2ba10a7e80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2b7c80b490>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2b7c80b490>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2ba143e9b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2ba10061d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2ba1006740>, <keras.callbacks.ModelCheckpoint object at 0x7f2ba1006800>, <keras.callbacks.EarlyStopping object at 0x7f2ba1006a70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2ba1006aa0>, <keras.callbacks.TerminateOnNaN object at 0x7f2ba10066e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_381/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 381/720 with hyperparameters:
timestamp = 2023-10-26 12:46:14.108214
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7f2d4c20bbe0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:48:17.810 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7495.2051, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 123s - loss: nan - MinusLogProbMetric: 7495.2051 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 123s/epoch - 630ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.6935087808430278e-08.
===========
Generating train data for run 381.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_381/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_381
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_181"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_182 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  1321920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,321,920
Trainable params: 1,321,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f2bdc4c4670>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2bdc48a5c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2bdc48a5c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b7dc95ba0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2b7dc52050>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_381/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2b7dc525c0>, <keras.callbacks.ModelCheckpoint object at 0x7f2b7dc52680>, <keras.callbacks.EarlyStopping object at 0x7f2b7dc528f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2b7dc52920>, <keras.callbacks.TerminateOnNaN object at 0x7f2b7dc52560>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_381/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 381/720 with hyperparameters:
timestamp = 2023-10-26 12:48:27.846359
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1321920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:50:34.441 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7503.7476, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 126s - loss: nan - MinusLogProbMetric: 7503.7476 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 126s/epoch - 645ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.645029269476759e-09.
===========
Run 381/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

Directory ../../results/CsplineN_new/run_382/ already exists.
Skipping it.
===========
Run 382/720 already exists. Skipping it.
===========

===========
Generating train data for run 383.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_192"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_193 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f2bdccb8640>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2dbc51d840>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2dbc51d840>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b8921e530>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2bdc5b3730>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2bdc5b3d00>, <keras.callbacks.ModelCheckpoint object at 0x7f2bdc5b1ae0>, <keras.callbacks.EarlyStopping object at 0x7f2bdc5b3280>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2bdc5b3e80>, <keras.callbacks.TerminateOnNaN object at 0x7f2bdc5b24a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-26 12:50:44.607936
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:53:23.364 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 159s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 159s/epoch - 809ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 383.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_203"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_204 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f2d45fba7d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f29c41b0640>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f29c41b0640>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2d459e7f70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2d45715de0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2d45716350>, <keras.callbacks.ModelCheckpoint object at 0x7f2d45716410>, <keras.callbacks.EarlyStopping object at 0x7f2d45716680>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2d457166b0>, <keras.callbacks.TerminateOnNaN object at 0x7f2d457162f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-26 12:53:36.031484
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:56:16.207 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 160s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 160s/epoch - 815ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0001111111111111111.
===========
Generating train data for run 383.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_214"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_215 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f2b6c630550>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2b05fa0dc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2b05fa0dc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b756029b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2b6c833bb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2b6c8b8160>, <keras.callbacks.ModelCheckpoint object at 0x7f2b6c8b8220>, <keras.callbacks.EarlyStopping object at 0x7f2b6c8b8490>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2b6c8b84c0>, <keras.callbacks.TerminateOnNaN object at 0x7f2b6c8b8100>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-26 12:56:28.403358
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 12:59:05.560 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 157s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 157s/epoch - 800ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 3.703703703703703e-05.
===========
Generating train data for run 383.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_225"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_226 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f2d44bef730>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f29bc79feb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f29bc79feb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2d44bc3340>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f298f755fc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f298f756530>, <keras.callbacks.ModelCheckpoint object at 0x7f298f7565f0>, <keras.callbacks.EarlyStopping object at 0x7f298f756860>, <keras.callbacks.ReduceLROnPlateau object at 0x7f298f756890>, <keras.callbacks.TerminateOnNaN object at 0x7f298f7564d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-26 12:59:16.126754
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:01:58.088 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 162s/epoch - 825ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.2345679012345677e-05.
===========
Generating train data for run 383.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_236"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_237 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f2b5dbb2b30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2b047450f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2b047450f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b0c5453f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2b5db99270>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2b5db997e0>, <keras.callbacks.ModelCheckpoint object at 0x7f2b5db998a0>, <keras.callbacks.EarlyStopping object at 0x7f2b5db99b10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2b5db99b40>, <keras.callbacks.TerminateOnNaN object at 0x7f2b5db99780>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-26 13:02:07.187397
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:04:59.107 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 172s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 172s/epoch - 876ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.115226337448558e-06.
===========
Generating train data for run 383.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_247"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_248 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f2d4529e7a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2d453d8040>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2d453d8040>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b6d380a60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2d452d0ee0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2d452d1450>, <keras.callbacks.ModelCheckpoint object at 0x7f2d452d1510>, <keras.callbacks.EarlyStopping object at 0x7f2d452d1780>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2d452d17b0>, <keras.callbacks.TerminateOnNaN object at 0x7f2d452d13f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-26 13:05:11.600158
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:07:58.789 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 167s/epoch - 851ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.3717421124828526e-06.
===========
Generating train data for run 383.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_258"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_259 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f29beaeb1f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2d1be3bfa0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2d1be3bfa0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2d1bf26ad0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2d1b99d870>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2d1b99dde0>, <keras.callbacks.ModelCheckpoint object at 0x7f2d1b99dea0>, <keras.callbacks.EarlyStopping object at 0x7f2d1b99e110>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2d1b99e140>, <keras.callbacks.TerminateOnNaN object at 0x7f2d1b99dd80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-26 13:08:10.879725
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:10:52.949 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 162s/epoch - 826ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.572473708276175e-07.
===========
Generating train data for run 383.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_269"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_270 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f29bfe12dd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2d451eeda0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2d451eeda0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b0514d300>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2d1ad2ca60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2d1ad2cfd0>, <keras.callbacks.ModelCheckpoint object at 0x7f2d1ad2d090>, <keras.callbacks.EarlyStopping object at 0x7f2d1ad2d300>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2d1ad2d330>, <keras.callbacks.TerminateOnNaN object at 0x7f2d1ad2cf70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-26 13:11:03.304392
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:13:47.137 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 164s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 164s/epoch - 835ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.524157902758725e-07.
===========
Generating train data for run 383.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_280"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_281 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f2d1a8fd960>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f29bd080dc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f29bd080dc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b7d3fca30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2d1a1a8460>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2d1a1a89d0>, <keras.callbacks.ModelCheckpoint object at 0x7f2d1a1a8a90>, <keras.callbacks.EarlyStopping object at 0x7f2d1a1a8d00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2d1a1a8d30>, <keras.callbacks.TerminateOnNaN object at 0x7f2d1a1a8970>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-26 13:13:59.325552
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:16:43.352 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 164s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 164s/epoch - 836ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.0805263425290834e-08.
===========
Generating train data for run 383.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_291"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_292 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f2d18b1dbd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f29c4e4dd80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f29c4e4dd80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b056a7cd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2d195ffac0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2d195fffd0>, <keras.callbacks.ModelCheckpoint object at 0x7f2d19f68130>, <keras.callbacks.EarlyStopping object at 0x7f2d19f683a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2d19f683d0>, <keras.callbacks.TerminateOnNaN object at 0x7f2d19f68040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-26 13:16:54.852111
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:19:39.277 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 164s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 164s/epoch - 838ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.6935087808430278e-08.
===========
Generating train data for run 383.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_383/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_383
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_302"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_303 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f2b0df3a7d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2d01b7b910>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2d01b7b910>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b0df39a20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2d01996c50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_383/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2d019971c0>, <keras.callbacks.ModelCheckpoint object at 0x7f2d01997280>, <keras.callbacks.EarlyStopping object at 0x7f2d019974f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2d01997520>, <keras.callbacks.TerminateOnNaN object at 0x7f2d01997160>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_383/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 383/720 with hyperparameters:
timestamp = 2023-10-26 13:19:51.445630
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:22:43.610 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7972.3237, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 172s - loss: nan - MinusLogProbMetric: 7972.3237 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 172s/epoch - 878ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.645029269476759e-09.
===========
Run 383/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 384.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_384
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_313"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_314 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f2b6c53bf40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2b401d7790>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2b401d7790>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2d4c56f610>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2986f16230>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2986f167a0>, <keras.callbacks.ModelCheckpoint object at 0x7f2986f16860>, <keras.callbacks.EarlyStopping object at 0x7f2986f16ad0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2986f16b00>, <keras.callbacks.TerminateOnNaN object at 0x7f2986f16740>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_384/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 384/720 with hyperparameters:
timestamp = 2023-10-26 13:22:56.617526
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:25:44.102 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6534.4248, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 6534.4248 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 167s/epoch - 854ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 384.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_384
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_324"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_325 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f2d00de2d10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2b6c08ffd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2b6c08ffd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2ce7fce500>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2ce7e058d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2ce7e05e40>, <keras.callbacks.ModelCheckpoint object at 0x7f2ce7e05f00>, <keras.callbacks.EarlyStopping object at 0x7f2ce7e06170>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2ce7e061a0>, <keras.callbacks.TerminateOnNaN object at 0x7f2ce7e05de0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_384/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 384/720 with hyperparameters:
timestamp = 2023-10-26 13:25:56.538423
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 44: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:28:56.981 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 4752.3965, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 180s - loss: nan - MinusLogProbMetric: 4752.3965 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 180s/epoch - 920ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0001111111111111111.
===========
Generating train data for run 384.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_384
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_335"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_336 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f2d191f3790>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2b75a47d30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2b75a47d30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b95d1a470>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2b7c2dcfa0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2b7c2dd510>, <keras.callbacks.ModelCheckpoint object at 0x7f2b7c2dd5d0>, <keras.callbacks.EarlyStopping object at 0x7f2b7c2dd840>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2b7c2dd870>, <keras.callbacks.TerminateOnNaN object at 0x7f2b7c2dd4b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_384/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 384/720 with hyperparameters:
timestamp = 2023-10-26 13:29:07.835264
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 6: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:31:54.767 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6551.9668, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 6551.9668 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 167s/epoch - 851ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 3.703703703703703e-05.
===========
Generating train data for run 384.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_384
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_346"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_347 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f2b88c154e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2b51ddfd90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2b51ddfd90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b89162ad0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2b51a741f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2b51a74760>, <keras.callbacks.ModelCheckpoint object at 0x7f2b51a74820>, <keras.callbacks.EarlyStopping object at 0x7f2b51a74a90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2b51a74ac0>, <keras.callbacks.TerminateOnNaN object at 0x7f2b51a74700>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_384/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 384/720 with hyperparameters:
timestamp = 2023-10-26 13:32:06.644436
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 4: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:34:58.227 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6619.5820, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 171s - loss: nan - MinusLogProbMetric: 6619.5820 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 171s/epoch - 874ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.2345679012345677e-05.
===========
Generating train data for run 384.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_384
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_357"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_358 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f2e404598d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2bdbd60a60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2bdbd60a60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b7c15b9a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2b95ed6d40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2b95ed5630>, <keras.callbacks.ModelCheckpoint object at 0x7f2b95ed76d0>, <keras.callbacks.EarlyStopping object at 0x7f2b95ed7a90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2b95ed5a50>, <keras.callbacks.TerminateOnNaN object at 0x7f2b95ed7640>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_384/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 384/720 with hyperparameters:
timestamp = 2023-10-26 13:35:11.352896
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 4: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:38:01.163 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6623.8540, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 169s - loss: nan - MinusLogProbMetric: 6623.8540 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 169s/epoch - 864ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.115226337448558e-06.
===========
Generating train data for run 384.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_384
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_368"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_369 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f2ce6df79a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2ce723bd00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2ce723bd00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2ce7763670>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2ce6c2e9b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2ce6c2ef20>, <keras.callbacks.ModelCheckpoint object at 0x7f2ce6c2efe0>, <keras.callbacks.EarlyStopping object at 0x7f2ce6c2f250>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2ce6c2f280>, <keras.callbacks.TerminateOnNaN object at 0x7f2ce6c2eec0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_384/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 384/720 with hyperparameters:
timestamp = 2023-10-26 13:38:13.907312
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:41:22.949 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6634.2319, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 189s - loss: nan - MinusLogProbMetric: 6634.2319 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 189s/epoch - 962ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.3717421124828526e-06.
===========
Generating train data for run 384.
===========
Train data generated in 0.28 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_384
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_379"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_380 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f2954861600>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f295d2f5a80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f295d2f5a80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f295cca80a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f29757f4df0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f29757f5360>, <keras.callbacks.ModelCheckpoint object at 0x7f29757f5420>, <keras.callbacks.EarlyStopping object at 0x7f29757f5690>, <keras.callbacks.ReduceLROnPlateau object at 0x7f29757f56c0>, <keras.callbacks.TerminateOnNaN object at 0x7f29757f5300>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_384/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 384/720 with hyperparameters:
timestamp = 2023-10-26 13:41:33.609430
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:44:51.042 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6643.1562, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 197s - loss: nan - MinusLogProbMetric: 6643.1562 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 197s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 4.572473708276175e-07.
===========
Generating train data for run 384.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_384
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_390"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_391 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f2b75af2560>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2977d7c250>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2977d7c250>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2d9055a770>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2987091e10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2987092380>, <keras.callbacks.ModelCheckpoint object at 0x7f2987092440>, <keras.callbacks.EarlyStopping object at 0x7f29870926b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f29870926e0>, <keras.callbacks.TerminateOnNaN object at 0x7f2987092320>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_384/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 384/720 with hyperparameters:
timestamp = 2023-10-26 13:45:03.256092
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 5: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:48:06.552 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6626.6064, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 183s - loss: nan - MinusLogProbMetric: 6626.6064 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 183s/epoch - 935ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.524157902758725e-07.
===========
Generating train data for run 384.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_384
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_401"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_402 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f2ce66a2860>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2ce60489a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2ce60489a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f293f7b6980>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2ce5b762c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2ce5b76830>, <keras.callbacks.ModelCheckpoint object at 0x7f2ce5b768f0>, <keras.callbacks.EarlyStopping object at 0x7f2ce5b76b60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2ce5b76b90>, <keras.callbacks.TerminateOnNaN object at 0x7f2ce5b767d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_384/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 384/720 with hyperparameters:
timestamp = 2023-10-26 13:48:19.085785
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:51:35.411 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6642.9209, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 196s - loss: nan - MinusLogProbMetric: 6642.9209 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 196s/epoch - 1000ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.0805263425290834e-08.
===========
Generating train data for run 384.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_384
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_412"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_413 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f29746c5bd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f2bfc322260>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f2bfc322260>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f29559e4040>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f294e5f4550>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f294e5f4ac0>, <keras.callbacks.ModelCheckpoint object at 0x7f294e5f4b80>, <keras.callbacks.EarlyStopping object at 0x7f294e5f4df0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f294e5f4e20>, <keras.callbacks.TerminateOnNaN object at 0x7f294e5f4a60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_384/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 384/720 with hyperparameters:
timestamp = 2023-10-26 13:51:47.941459
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 5: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:54:47.886 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6624.3770, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 180s - loss: nan - MinusLogProbMetric: 6624.3770 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 180s/epoch - 918ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 1.6935087808430278e-08.
===========
Generating train data for run 384.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_384/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_384
self.data_kwargs: {'seed': 869}
self.x_data: [[ 6.514898    2.847786    6.255653   ...  3.6510358   4.425651
   2.1592255 ]
 [ 6.9207306   2.8537555   6.075634   ...  3.3624277   3.311129
   1.6233965 ]
 [ 6.4017973   3.0749886   6.172165   ...  2.955763    4.1152744
   1.3646827 ]
 ...
 [ 1.2767403   5.669613    7.5353622  ...  6.398686   -0.23885484
   2.5814815 ]
 [ 6.5017424   2.903797    6.162057   ...  3.9584637   2.3525772
   1.5695707 ]
 [ 6.560207    2.9788766   6.2347674  ...  3.186725    4.042962
   1.7694858 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_423"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_424 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f297d7e2410>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f295c832da0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f295c832da0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b50d5e200>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2b7d9effd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_384/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2b51b78580>, <keras.callbacks.ModelCheckpoint object at 0x7f2b51b78640>, <keras.callbacks.EarlyStopping object at 0x7f2b51b788b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2b51b788e0>, <keras.callbacks.TerminateOnNaN object at 0x7f2b51b78520>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.914401  , 2.8740964 , 6.197989  , ..., 3.3622417 , 2.8613076 ,
        2.0475917 ],
       [5.4126844 , 7.541518  , 5.3427024 , ..., 1.3038243 , 7.0230465 ,
        1.3282784 ],
       [5.585955  , 6.5243144 , 5.1275916 , ..., 1.2703401 , 6.15361   ,
        1.3179724 ],
       ...,
       [3.0932174 , 3.1785257 , 7.533167  , ..., 5.713445  , 0.1608001 ,
        4.308157  ],
       [5.4948845 , 7.5376387 , 6.3699913 , ..., 0.17478478, 6.7631593 ,
        1.439661  ],
       [6.5755196 , 2.6756127 , 6.179314  , ..., 3.751392  , 4.149127  ,
        1.9367172 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_384/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 384/720 with hyperparameters:
timestamp = 2023-10-26 13:55:01.303911
ndims = 64
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.514898    2.847786    6.255653    4.3194256   1.4973812   3.1341035
  5.0228505   6.1194158   5.916825    6.0264635   6.508691    4.724786
  9.036075    2.7496574   4.5507812   9.340518    7.6978188   7.6711087
  0.8399726   9.2822895   6.8571315   9.722106    2.197008    8.719921
  2.8436491   6.131179    1.4479048   8.114116    7.6586547   6.3509684
  4.0610533   0.6821992   8.149007    5.037877    7.3046746   9.524578
  9.765952    8.719091    0.35098916  4.6243057   7.163706    1.6523881
  5.6271005   0.58072436  2.70707    -0.32627347  7.868687    2.5209217
  4.148258    8.0176115   6.9998198   0.4588151   2.8661127   6.7648273
  5.7348933   2.2856503   9.749809    4.471522    5.854378    4.4448867
  7.1426215   3.6510358   4.425651    2.1592255 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 2: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:58:05.113 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6643.9727, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 183s - loss: nan - MinusLogProbMetric: 6643.9727 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 183s/epoch - 936ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 5.645029269476759e-09.
===========
Run 384/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 385.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_385/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_385/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_385/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_385
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_429"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_430 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f2b5d44a9e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f340e3d1450>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f340e3d1450>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b5d454580>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2ce7a90400>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f2b754b0340>, <keras.callbacks.ModelCheckpoint object at 0x7f2ffc1e7a90>, <keras.callbacks.EarlyStopping object at 0x7f2986ed7fd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2ba162ffa0>, <keras.callbacks.TerminateOnNaN object at 0x7f30847ef7f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_385/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 385/720 with hyperparameters:
timestamp = 2023-10-26 13:58:21.736907
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 27: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 13:59:44.589 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 4087.5474, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 83s - loss: nan - MinusLogProbMetric: 4087.5474 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 83s/epoch - 422ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0003333333333333333.
===========
Generating train data for run 385.
===========
Train data generated in 0.33 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_385/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_385/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_385/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_385
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_435"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_436 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f29c4f3aad0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f29bfab7340>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f29bfab7340>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b88d89060>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f298f57b3a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f298f57a5c0>, <keras.callbacks.ModelCheckpoint object at 0x7f298f5790c0>, <keras.callbacks.EarlyStopping object at 0x7f298f579a20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f298f579330>, <keras.callbacks.TerminateOnNaN object at 0x7f298f579ae0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_385/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 385/720 with hyperparameters:
timestamp = 2023-10-26 13:59:49.746762
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 3: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-26 14:00:55.321 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5930.6626, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 65s - loss: nan - MinusLogProbMetric: 5930.6626 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 65s/epoch - 334ms/step
The loss history contains NaN values.
Training failed: trying again with seed 638742 and lr 0.0001111111111111111.
===========
Generating train data for run 385.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_385/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 926}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_385/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_385/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_385
self.data_kwargs: {'seed': 926}
self.x_data: [[ 5.493947    7.7498794   5.880344   ...  1.2520857   6.580984
   1.2785633 ]
 [ 5.476005    8.571384    5.8171277  ...  1.1546116   7.79415
   1.4973618 ]
 [ 2.0623596   4.1273804   7.8748827  ...  5.938621    0.4107541
   2.9289248 ]
 ...
 [ 2.7513845   3.6992078   8.902546   ...  5.669202    1.1540179
   4.335299  ]
 [ 0.95716524  3.7573647   8.454903   ...  7.024189   -0.24022801
   3.8549995 ]
 [ 5.3094444   6.0836673   4.9564004  ...  1.811027    5.7610536
   1.5005304 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_441"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_442 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  660960    
 ayer)                                                           
                                                                 
=================================================================
Total params: 660,960
Trainable params: 660,960
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f2956437280>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f29569036a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f29569036a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f2b741edf30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f2956615480>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f29566159f0>, <keras.callbacks.ModelCheckpoint object at 0x7f2956615ab0>, <keras.callbacks.EarlyStopping object at 0x7f2956615d20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f2956615d50>, <keras.callbacks.TerminateOnNaN object at 0x7f2956615990>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4360485 ,  3.0547888 ,  6.2520843 , ...,  3.593781  ,
         4.1715846 ,  1.6065981 ],
       [ 6.8557706 ,  2.654025  ,  6.262237  , ...,  3.0459085 ,
         4.890991  ,  3.4169316 ],
       [ 1.655875  ,  2.784504  ,  5.821641  , ...,  6.034958  ,
        -0.65044   ,  2.2794065 ],
       ...,
       [ 1.9296709 ,  3.5623934 ,  8.419179  , ...,  5.4428782 ,
        -0.30274814,  2.6514096 ],
       [ 6.999009  ,  3.1260176 ,  6.232393  , ...,  3.2721472 ,
         4.207871  ,  2.8154526 ],
       [ 6.697628  ,  2.9338238 ,  6.1403036 , ...,  3.0996253 ,
         5.5472813 ,  1.5184094 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_385/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 385/720 with hyperparameters:
timestamp = 2023-10-26 14:01:00.763374
ndims = 64
seed_train = 926
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 660960
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.493947    7.7498794   5.880344    5.0118876   4.626097    6.704342
  4.369556    8.558859    9.480334    4.010617    7.448444    5.3736415
  5.667812    9.277698    1.0501947   0.8586999   0.5442954   7.2538667
  7.000578    8.641741    9.464584    7.9817486   4.6149635   6.8414574
  1.2289732   6.2369795   2.3831377   9.398327    4.8008513   2.5804245
  2.6253817   7.150032    4.4531236   3.8899019   0.5365164   6.4274006
  6.176794    5.6127276   9.615258    6.784686    3.5889976   4.3399954
  6.9008403   1.1400609   6.7047997   6.587667    2.1630335   1.0544555
  3.5621717   3.593693    5.7803855   4.3460145  10.207765    0.94548833
  2.366539    1.6003813   6.438226    2.5612001   4.618742    3.4830515
  0.8740866   1.2520857   6.580984    1.2785633 ]
Epoch 1/1000
2023-10-26 14:02:44.195 
Epoch 1/1000 
	 loss: 3222.8572, MinusLogProbMetric: 3222.8572, val_loss: 1285.1017, val_MinusLogProbMetric: 1285.1017

Epoch 1: val_loss improved from inf to 1285.10168, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 104s - loss: 3222.8572 - MinusLogProbMetric: 3222.8572 - val_loss: 1285.1017 - val_MinusLogProbMetric: 1285.1017 - lr: 1.1111e-04 - 104s/epoch - 530ms/step
Epoch 2/1000
2023-10-26 14:03:22.048 
Epoch 2/1000 
	 loss: 764.6309, MinusLogProbMetric: 764.6309, val_loss: 522.1213, val_MinusLogProbMetric: 522.1213

Epoch 2: val_loss improved from 1285.10168 to 522.12134, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 38s - loss: 764.6309 - MinusLogProbMetric: 764.6309 - val_loss: 522.1213 - val_MinusLogProbMetric: 522.1213 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 3/1000
2023-10-26 14:03:59.369 
Epoch 3/1000 
	 loss: 440.0263, MinusLogProbMetric: 440.0263, val_loss: 369.1774, val_MinusLogProbMetric: 369.1774

Epoch 3: val_loss improved from 522.12134 to 369.17743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 440.0263 - MinusLogProbMetric: 440.0263 - val_loss: 369.1774 - val_MinusLogProbMetric: 369.1774 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 4/1000
2023-10-26 14:04:36.346 
Epoch 4/1000 
	 loss: 342.1752, MinusLogProbMetric: 342.1752, val_loss: 320.7291, val_MinusLogProbMetric: 320.7291

Epoch 4: val_loss improved from 369.17743 to 320.72910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 342.1752 - MinusLogProbMetric: 342.1752 - val_loss: 320.7291 - val_MinusLogProbMetric: 320.7291 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 5/1000
2023-10-26 14:05:14.163 
Epoch 5/1000 
	 loss: 308.8367, MinusLogProbMetric: 308.8367, val_loss: 296.3801, val_MinusLogProbMetric: 296.3801

Epoch 5: val_loss improved from 320.72910 to 296.38007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 38s - loss: 308.8367 - MinusLogProbMetric: 308.8367 - val_loss: 296.3801 - val_MinusLogProbMetric: 296.3801 - lr: 1.1111e-04 - 38s/epoch - 193ms/step
Epoch 6/1000
2023-10-26 14:05:51.465 
Epoch 6/1000 
	 loss: 288.6165, MinusLogProbMetric: 288.6165, val_loss: 277.0023, val_MinusLogProbMetric: 277.0023

Epoch 6: val_loss improved from 296.38007 to 277.00232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 288.6165 - MinusLogProbMetric: 288.6165 - val_loss: 277.0023 - val_MinusLogProbMetric: 277.0023 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 7/1000
2023-10-26 14:06:28.690 
Epoch 7/1000 
	 loss: 269.7702, MinusLogProbMetric: 269.7702, val_loss: 260.1764, val_MinusLogProbMetric: 260.1764

Epoch 7: val_loss improved from 277.00232 to 260.17639, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 269.7702 - MinusLogProbMetric: 269.7702 - val_loss: 260.1764 - val_MinusLogProbMetric: 260.1764 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 8/1000
2023-10-26 14:07:06.295 
Epoch 8/1000 
	 loss: 256.8560, MinusLogProbMetric: 256.8560, val_loss: 249.5083, val_MinusLogProbMetric: 249.5083

Epoch 8: val_loss improved from 260.17639 to 249.50832, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 38s - loss: 256.8560 - MinusLogProbMetric: 256.8560 - val_loss: 249.5083 - val_MinusLogProbMetric: 249.5083 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 9/1000
2023-10-26 14:07:43.842 
Epoch 9/1000 
	 loss: 240.7410, MinusLogProbMetric: 240.7410, val_loss: 236.0226, val_MinusLogProbMetric: 236.0226

Epoch 9: val_loss improved from 249.50832 to 236.02258, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 38s - loss: 240.7410 - MinusLogProbMetric: 240.7410 - val_loss: 236.0226 - val_MinusLogProbMetric: 236.0226 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 10/1000
2023-10-26 14:08:21.022 
Epoch 10/1000 
	 loss: 228.4651, MinusLogProbMetric: 228.4651, val_loss: 222.3468, val_MinusLogProbMetric: 222.3468

Epoch 10: val_loss improved from 236.02258 to 222.34683, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 228.4651 - MinusLogProbMetric: 228.4651 - val_loss: 222.3468 - val_MinusLogProbMetric: 222.3468 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 11/1000
2023-10-26 14:08:57.568 
Epoch 11/1000 
	 loss: 220.0225, MinusLogProbMetric: 220.0225, val_loss: 215.9328, val_MinusLogProbMetric: 215.9328

Epoch 11: val_loss improved from 222.34683 to 215.93277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 220.0225 - MinusLogProbMetric: 220.0225 - val_loss: 215.9328 - val_MinusLogProbMetric: 215.9328 - lr: 1.1111e-04 - 37s/epoch - 186ms/step
Epoch 12/1000
2023-10-26 14:09:34.561 
Epoch 12/1000 
	 loss: 212.0638, MinusLogProbMetric: 212.0638, val_loss: 208.5244, val_MinusLogProbMetric: 208.5244

Epoch 12: val_loss improved from 215.93277 to 208.52438, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 212.0638 - MinusLogProbMetric: 212.0638 - val_loss: 208.5244 - val_MinusLogProbMetric: 208.5244 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 13/1000
2023-10-26 14:10:06.887 
Epoch 13/1000 
	 loss: 204.5503, MinusLogProbMetric: 204.5503, val_loss: 200.4399, val_MinusLogProbMetric: 200.4399

Epoch 13: val_loss improved from 208.52438 to 200.43988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 204.5503 - MinusLogProbMetric: 204.5503 - val_loss: 200.4399 - val_MinusLogProbMetric: 200.4399 - lr: 1.1111e-04 - 32s/epoch - 164ms/step
Epoch 14/1000
2023-10-26 14:10:37.125 
Epoch 14/1000 
	 loss: 198.1455, MinusLogProbMetric: 198.1455, val_loss: 195.1669, val_MinusLogProbMetric: 195.1669

Epoch 14: val_loss improved from 200.43988 to 195.16692, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 198.1455 - MinusLogProbMetric: 198.1455 - val_loss: 195.1669 - val_MinusLogProbMetric: 195.1669 - lr: 1.1111e-04 - 30s/epoch - 154ms/step
Epoch 15/1000
2023-10-26 14:11:08.354 
Epoch 15/1000 
	 loss: 192.4410, MinusLogProbMetric: 192.4410, val_loss: 189.2805, val_MinusLogProbMetric: 189.2805

Epoch 15: val_loss improved from 195.16692 to 189.28047, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 192.4410 - MinusLogProbMetric: 192.4410 - val_loss: 189.2805 - val_MinusLogProbMetric: 189.2805 - lr: 1.1111e-04 - 31s/epoch - 159ms/step
Epoch 16/1000
2023-10-26 14:11:45.036 
Epoch 16/1000 
	 loss: 187.1432, MinusLogProbMetric: 187.1432, val_loss: 184.9903, val_MinusLogProbMetric: 184.9903

Epoch 16: val_loss improved from 189.28047 to 184.99033, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 187.1432 - MinusLogProbMetric: 187.1432 - val_loss: 184.9903 - val_MinusLogProbMetric: 184.9903 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 17/1000
2023-10-26 14:12:22.125 
Epoch 17/1000 
	 loss: 182.3775, MinusLogProbMetric: 182.3775, val_loss: 179.5294, val_MinusLogProbMetric: 179.5294

Epoch 17: val_loss improved from 184.99033 to 179.52945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 182.3775 - MinusLogProbMetric: 182.3775 - val_loss: 179.5294 - val_MinusLogProbMetric: 179.5294 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 18/1000
2023-10-26 14:12:58.547 
Epoch 18/1000 
	 loss: 177.5683, MinusLogProbMetric: 177.5683, val_loss: 177.1021, val_MinusLogProbMetric: 177.1021

Epoch 18: val_loss improved from 179.52945 to 177.10207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 177.5683 - MinusLogProbMetric: 177.5683 - val_loss: 177.1021 - val_MinusLogProbMetric: 177.1021 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 19/1000
2023-10-26 14:13:35.199 
Epoch 19/1000 
	 loss: 171.2239, MinusLogProbMetric: 171.2239, val_loss: 148.5355, val_MinusLogProbMetric: 148.5355

Epoch 19: val_loss improved from 177.10207 to 148.53552, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 171.2239 - MinusLogProbMetric: 171.2239 - val_loss: 148.5355 - val_MinusLogProbMetric: 148.5355 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 20/1000
2023-10-26 14:14:11.778 
Epoch 20/1000 
	 loss: 142.5527, MinusLogProbMetric: 142.5527, val_loss: 140.1922, val_MinusLogProbMetric: 140.1922

Epoch 20: val_loss improved from 148.53552 to 140.19215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 142.5527 - MinusLogProbMetric: 142.5527 - val_loss: 140.1922 - val_MinusLogProbMetric: 140.1922 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 21/1000
2023-10-26 14:14:48.728 
Epoch 21/1000 
	 loss: 137.8341, MinusLogProbMetric: 137.8341, val_loss: 136.3867, val_MinusLogProbMetric: 136.3867

Epoch 21: val_loss improved from 140.19215 to 136.38666, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 137.8341 - MinusLogProbMetric: 137.8341 - val_loss: 136.3867 - val_MinusLogProbMetric: 136.3867 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 22/1000
2023-10-26 14:15:25.308 
Epoch 22/1000 
	 loss: 133.3237, MinusLogProbMetric: 133.3237, val_loss: 131.5140, val_MinusLogProbMetric: 131.5140

Epoch 22: val_loss improved from 136.38666 to 131.51396, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 133.3237 - MinusLogProbMetric: 133.3237 - val_loss: 131.5140 - val_MinusLogProbMetric: 131.5140 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 23/1000
2023-10-26 14:16:01.357 
Epoch 23/1000 
	 loss: 129.6235, MinusLogProbMetric: 129.6235, val_loss: 128.5059, val_MinusLogProbMetric: 128.5059

Epoch 23: val_loss improved from 131.51396 to 128.50591, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 129.6235 - MinusLogProbMetric: 129.6235 - val_loss: 128.5059 - val_MinusLogProbMetric: 128.5059 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 24/1000
2023-10-26 14:16:37.418 
Epoch 24/1000 
	 loss: 126.1698, MinusLogProbMetric: 126.1698, val_loss: 125.9572, val_MinusLogProbMetric: 125.9572

Epoch 24: val_loss improved from 128.50591 to 125.95721, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 126.1698 - MinusLogProbMetric: 126.1698 - val_loss: 125.9572 - val_MinusLogProbMetric: 125.9572 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 25/1000
2023-10-26 14:17:14.518 
Epoch 25/1000 
	 loss: 123.4937, MinusLogProbMetric: 123.4937, val_loss: 122.1126, val_MinusLogProbMetric: 122.1126

Epoch 25: val_loss improved from 125.95721 to 122.11263, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 123.4937 - MinusLogProbMetric: 123.4937 - val_loss: 122.1126 - val_MinusLogProbMetric: 122.1126 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 26/1000
2023-10-26 14:17:52.707 
Epoch 26/1000 
	 loss: 120.5066, MinusLogProbMetric: 120.5066, val_loss: 119.0901, val_MinusLogProbMetric: 119.0901

Epoch 26: val_loss improved from 122.11263 to 119.09014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 38s - loss: 120.5066 - MinusLogProbMetric: 120.5066 - val_loss: 119.0901 - val_MinusLogProbMetric: 119.0901 - lr: 1.1111e-04 - 38s/epoch - 195ms/step
Epoch 27/1000
2023-10-26 14:18:23.110 
Epoch 27/1000 
	 loss: 118.0417, MinusLogProbMetric: 118.0417, val_loss: 116.9694, val_MinusLogProbMetric: 116.9694

Epoch 27: val_loss improved from 119.09014 to 116.96942, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 118.0417 - MinusLogProbMetric: 118.0417 - val_loss: 116.9694 - val_MinusLogProbMetric: 116.9694 - lr: 1.1111e-04 - 30s/epoch - 155ms/step
Epoch 28/1000
2023-10-26 14:18:53.098 
Epoch 28/1000 
	 loss: 115.6445, MinusLogProbMetric: 115.6445, val_loss: 114.5659, val_MinusLogProbMetric: 114.5659

Epoch 28: val_loss improved from 116.96942 to 114.56588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 115.6445 - MinusLogProbMetric: 115.6445 - val_loss: 114.5659 - val_MinusLogProbMetric: 114.5659 - lr: 1.1111e-04 - 30s/epoch - 153ms/step
Epoch 29/1000
2023-10-26 14:19:26.115 
Epoch 29/1000 
	 loss: 113.5862, MinusLogProbMetric: 113.5862, val_loss: 112.3496, val_MinusLogProbMetric: 112.3496

Epoch 29: val_loss improved from 114.56588 to 112.34964, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 113.5862 - MinusLogProbMetric: 113.5862 - val_loss: 112.3496 - val_MinusLogProbMetric: 112.3496 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 30/1000
2023-10-26 14:20:02.656 
Epoch 30/1000 
	 loss: 111.3077, MinusLogProbMetric: 111.3077, val_loss: 111.6062, val_MinusLogProbMetric: 111.6062

Epoch 30: val_loss improved from 112.34964 to 111.60622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 111.3077 - MinusLogProbMetric: 111.3077 - val_loss: 111.6062 - val_MinusLogProbMetric: 111.6062 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 31/1000
2023-10-26 14:20:39.875 
Epoch 31/1000 
	 loss: 109.3005, MinusLogProbMetric: 109.3005, val_loss: 109.0655, val_MinusLogProbMetric: 109.0655

Epoch 31: val_loss improved from 111.60622 to 109.06545, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 109.3005 - MinusLogProbMetric: 109.3005 - val_loss: 109.0655 - val_MinusLogProbMetric: 109.0655 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 32/1000
2023-10-26 14:21:16.254 
Epoch 32/1000 
	 loss: 107.5251, MinusLogProbMetric: 107.5251, val_loss: 106.8258, val_MinusLogProbMetric: 106.8258

Epoch 32: val_loss improved from 109.06545 to 106.82575, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 107.5251 - MinusLogProbMetric: 107.5251 - val_loss: 106.8258 - val_MinusLogProbMetric: 106.8258 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 33/1000
2023-10-26 14:21:53.483 
Epoch 33/1000 
	 loss: 105.5778, MinusLogProbMetric: 105.5778, val_loss: 105.0095, val_MinusLogProbMetric: 105.0095

Epoch 33: val_loss improved from 106.82575 to 105.00945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 105.5778 - MinusLogProbMetric: 105.5778 - val_loss: 105.0095 - val_MinusLogProbMetric: 105.0095 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 34/1000
2023-10-26 14:22:30.633 
Epoch 34/1000 
	 loss: 103.8853, MinusLogProbMetric: 103.8853, val_loss: 103.7692, val_MinusLogProbMetric: 103.7692

Epoch 34: val_loss improved from 105.00945 to 103.76919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 103.8853 - MinusLogProbMetric: 103.8853 - val_loss: 103.7692 - val_MinusLogProbMetric: 103.7692 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 35/1000
2023-10-26 14:23:08.452 
Epoch 35/1000 
	 loss: 102.0968, MinusLogProbMetric: 102.0968, val_loss: 101.1973, val_MinusLogProbMetric: 101.1973

Epoch 35: val_loss improved from 103.76919 to 101.19732, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 38s - loss: 102.0968 - MinusLogProbMetric: 102.0968 - val_loss: 101.1973 - val_MinusLogProbMetric: 101.1973 - lr: 1.1111e-04 - 38s/epoch - 193ms/step
Epoch 36/1000
2023-10-26 14:23:45.322 
Epoch 36/1000 
	 loss: 100.7234, MinusLogProbMetric: 100.7234, val_loss: 99.9410, val_MinusLogProbMetric: 99.9410

Epoch 36: val_loss improved from 101.19732 to 99.94102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 100.7234 - MinusLogProbMetric: 100.7234 - val_loss: 99.9410 - val_MinusLogProbMetric: 99.9410 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 37/1000
2023-10-26 14:24:21.627 
Epoch 37/1000 
	 loss: 98.8276, MinusLogProbMetric: 98.8276, val_loss: 98.9754, val_MinusLogProbMetric: 98.9754

Epoch 37: val_loss improved from 99.94102 to 98.97543, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 98.8276 - MinusLogProbMetric: 98.8276 - val_loss: 98.9754 - val_MinusLogProbMetric: 98.9754 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 38/1000
2023-10-26 14:24:58.722 
Epoch 38/1000 
	 loss: 97.3056, MinusLogProbMetric: 97.3056, val_loss: 96.7203, val_MinusLogProbMetric: 96.7203

Epoch 38: val_loss improved from 98.97543 to 96.72029, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 97.3056 - MinusLogProbMetric: 97.3056 - val_loss: 96.7203 - val_MinusLogProbMetric: 96.7203 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 39/1000
2023-10-26 14:25:35.577 
Epoch 39/1000 
	 loss: 95.9013, MinusLogProbMetric: 95.9013, val_loss: 95.4855, val_MinusLogProbMetric: 95.4855

Epoch 39: val_loss improved from 96.72029 to 95.48553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 95.9013 - MinusLogProbMetric: 95.9013 - val_loss: 95.4855 - val_MinusLogProbMetric: 95.4855 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 40/1000
2023-10-26 14:26:12.044 
Epoch 40/1000 
	 loss: 94.5514, MinusLogProbMetric: 94.5514, val_loss: 94.0932, val_MinusLogProbMetric: 94.0932

Epoch 40: val_loss improved from 95.48553 to 94.09325, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 94.5514 - MinusLogProbMetric: 94.5514 - val_loss: 94.0932 - val_MinusLogProbMetric: 94.0932 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 41/1000
2023-10-26 14:26:48.631 
Epoch 41/1000 
	 loss: 93.3243, MinusLogProbMetric: 93.3243, val_loss: 92.7217, val_MinusLogProbMetric: 92.7217

Epoch 41: val_loss improved from 94.09325 to 92.72172, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 93.3243 - MinusLogProbMetric: 93.3243 - val_loss: 92.7217 - val_MinusLogProbMetric: 92.7217 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 42/1000
2023-10-26 14:27:25.551 
Epoch 42/1000 
	 loss: 92.1232, MinusLogProbMetric: 92.1232, val_loss: 92.1307, val_MinusLogProbMetric: 92.1307

Epoch 42: val_loss improved from 92.72172 to 92.13073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 92.1232 - MinusLogProbMetric: 92.1232 - val_loss: 92.1307 - val_MinusLogProbMetric: 92.1307 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 43/1000
2023-10-26 14:28:01.339 
Epoch 43/1000 
	 loss: 90.8745, MinusLogProbMetric: 90.8745, val_loss: 90.5755, val_MinusLogProbMetric: 90.5755

Epoch 43: val_loss improved from 92.13073 to 90.57547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 90.8745 - MinusLogProbMetric: 90.8745 - val_loss: 90.5755 - val_MinusLogProbMetric: 90.5755 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 44/1000
2023-10-26 14:28:37.878 
Epoch 44/1000 
	 loss: 89.8320, MinusLogProbMetric: 89.8320, val_loss: 89.3472, val_MinusLogProbMetric: 89.3472

Epoch 44: val_loss improved from 90.57547 to 89.34718, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 89.8320 - MinusLogProbMetric: 89.8320 - val_loss: 89.3472 - val_MinusLogProbMetric: 89.3472 - lr: 1.1111e-04 - 37s/epoch - 186ms/step
Epoch 45/1000
2023-10-26 14:29:15.038 
Epoch 45/1000 
	 loss: 88.7101, MinusLogProbMetric: 88.7101, val_loss: 89.0186, val_MinusLogProbMetric: 89.0186

Epoch 45: val_loss improved from 89.34718 to 89.01860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 88.7101 - MinusLogProbMetric: 88.7101 - val_loss: 89.0186 - val_MinusLogProbMetric: 89.0186 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 46/1000
2023-10-26 14:29:52.209 
Epoch 46/1000 
	 loss: 87.5531, MinusLogProbMetric: 87.5531, val_loss: 87.3170, val_MinusLogProbMetric: 87.3170

Epoch 46: val_loss improved from 89.01860 to 87.31696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 87.5531 - MinusLogProbMetric: 87.5531 - val_loss: 87.3170 - val_MinusLogProbMetric: 87.3170 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 47/1000
2023-10-26 14:30:29.330 
Epoch 47/1000 
	 loss: 86.5636, MinusLogProbMetric: 86.5636, val_loss: 88.1808, val_MinusLogProbMetric: 88.1808

Epoch 47: val_loss did not improve from 87.31696
196/196 - 36s - loss: 86.5636 - MinusLogProbMetric: 86.5636 - val_loss: 88.1808 - val_MinusLogProbMetric: 88.1808 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 48/1000
2023-10-26 14:31:05.575 
Epoch 48/1000 
	 loss: 85.8366, MinusLogProbMetric: 85.8366, val_loss: 85.5451, val_MinusLogProbMetric: 85.5451

Epoch 48: val_loss improved from 87.31696 to 85.54507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 85.8366 - MinusLogProbMetric: 85.8366 - val_loss: 85.5451 - val_MinusLogProbMetric: 85.5451 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 49/1000
2023-10-26 14:31:42.746 
Epoch 49/1000 
	 loss: 84.7296, MinusLogProbMetric: 84.7296, val_loss: 84.5905, val_MinusLogProbMetric: 84.5905

Epoch 49: val_loss improved from 85.54507 to 84.59048, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 84.7296 - MinusLogProbMetric: 84.7296 - val_loss: 84.5905 - val_MinusLogProbMetric: 84.5905 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 50/1000
2023-10-26 14:32:19.270 
Epoch 50/1000 
	 loss: 83.7523, MinusLogProbMetric: 83.7523, val_loss: 83.3425, val_MinusLogProbMetric: 83.3425

Epoch 50: val_loss improved from 84.59048 to 83.34254, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 83.7523 - MinusLogProbMetric: 83.7523 - val_loss: 83.3425 - val_MinusLogProbMetric: 83.3425 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 51/1000
2023-10-26 14:32:56.588 
Epoch 51/1000 
	 loss: 83.0115, MinusLogProbMetric: 83.0115, val_loss: 83.1234, val_MinusLogProbMetric: 83.1234

Epoch 51: val_loss improved from 83.34254 to 83.12337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 83.0115 - MinusLogProbMetric: 83.0115 - val_loss: 83.1234 - val_MinusLogProbMetric: 83.1234 - lr: 1.1111e-04 - 37s/epoch - 191ms/step
Epoch 52/1000
2023-10-26 14:33:33.482 
Epoch 52/1000 
	 loss: 82.0102, MinusLogProbMetric: 82.0102, val_loss: 82.9205, val_MinusLogProbMetric: 82.9205

Epoch 52: val_loss improved from 83.12337 to 82.92052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 82.0102 - MinusLogProbMetric: 82.0102 - val_loss: 82.9205 - val_MinusLogProbMetric: 82.9205 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 53/1000
2023-10-26 14:34:10.427 
Epoch 53/1000 
	 loss: 81.7326, MinusLogProbMetric: 81.7326, val_loss: 80.9172, val_MinusLogProbMetric: 80.9172

Epoch 53: val_loss improved from 82.92052 to 80.91717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 81.7326 - MinusLogProbMetric: 81.7326 - val_loss: 80.9172 - val_MinusLogProbMetric: 80.9172 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 54/1000
2023-10-26 14:34:47.043 
Epoch 54/1000 
	 loss: 80.4179, MinusLogProbMetric: 80.4179, val_loss: 80.5975, val_MinusLogProbMetric: 80.5975

Epoch 54: val_loss improved from 80.91717 to 80.59747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 80.4179 - MinusLogProbMetric: 80.4179 - val_loss: 80.5975 - val_MinusLogProbMetric: 80.5975 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 55/1000
2023-10-26 14:35:22.873 
Epoch 55/1000 
	 loss: 79.5856, MinusLogProbMetric: 79.5856, val_loss: 79.4901, val_MinusLogProbMetric: 79.4901

Epoch 55: val_loss improved from 80.59747 to 79.49007, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 79.5856 - MinusLogProbMetric: 79.5856 - val_loss: 79.4901 - val_MinusLogProbMetric: 79.4901 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 56/1000
2023-10-26 14:35:59.243 
Epoch 56/1000 
	 loss: 78.7831, MinusLogProbMetric: 78.7831, val_loss: 79.0519, val_MinusLogProbMetric: 79.0519

Epoch 56: val_loss improved from 79.49007 to 79.05194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 78.7831 - MinusLogProbMetric: 78.7831 - val_loss: 79.0519 - val_MinusLogProbMetric: 79.0519 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 57/1000
2023-10-26 14:36:35.721 
Epoch 57/1000 
	 loss: 77.9621, MinusLogProbMetric: 77.9621, val_loss: 77.7046, val_MinusLogProbMetric: 77.7046

Epoch 57: val_loss improved from 79.05194 to 77.70461, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 77.9621 - MinusLogProbMetric: 77.9621 - val_loss: 77.7046 - val_MinusLogProbMetric: 77.7046 - lr: 1.1111e-04 - 37s/epoch - 186ms/step
Epoch 58/1000
2023-10-26 14:37:12.282 
Epoch 58/1000 
	 loss: 77.1424, MinusLogProbMetric: 77.1424, val_loss: 77.0685, val_MinusLogProbMetric: 77.0685

Epoch 58: val_loss improved from 77.70461 to 77.06853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 77.1424 - MinusLogProbMetric: 77.1424 - val_loss: 77.0685 - val_MinusLogProbMetric: 77.0685 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 59/1000
2023-10-26 14:37:49.195 
Epoch 59/1000 
	 loss: 76.4796, MinusLogProbMetric: 76.4796, val_loss: 76.1952, val_MinusLogProbMetric: 76.1952

Epoch 59: val_loss improved from 77.06853 to 76.19516, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 76.4796 - MinusLogProbMetric: 76.4796 - val_loss: 76.1952 - val_MinusLogProbMetric: 76.1952 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 60/1000
2023-10-26 14:38:26.285 
Epoch 60/1000 
	 loss: 75.6038, MinusLogProbMetric: 75.6038, val_loss: 75.4540, val_MinusLogProbMetric: 75.4540

Epoch 60: val_loss improved from 76.19516 to 75.45399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 75.6038 - MinusLogProbMetric: 75.6038 - val_loss: 75.4540 - val_MinusLogProbMetric: 75.4540 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 61/1000
2023-10-26 14:39:03.649 
Epoch 61/1000 
	 loss: 74.9370, MinusLogProbMetric: 74.9370, val_loss: 74.7147, val_MinusLogProbMetric: 74.7147

Epoch 61: val_loss improved from 75.45399 to 74.71468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 74.9370 - MinusLogProbMetric: 74.9370 - val_loss: 74.7147 - val_MinusLogProbMetric: 74.7147 - lr: 1.1111e-04 - 37s/epoch - 191ms/step
Epoch 62/1000
2023-10-26 14:39:40.581 
Epoch 62/1000 
	 loss: 74.3699, MinusLogProbMetric: 74.3699, val_loss: 74.5063, val_MinusLogProbMetric: 74.5063

Epoch 62: val_loss improved from 74.71468 to 74.50626, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 74.3699 - MinusLogProbMetric: 74.3699 - val_loss: 74.5063 - val_MinusLogProbMetric: 74.5063 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 63/1000
2023-10-26 14:40:16.771 
Epoch 63/1000 
	 loss: 73.5498, MinusLogProbMetric: 73.5498, val_loss: 74.3645, val_MinusLogProbMetric: 74.3645

Epoch 63: val_loss improved from 74.50626 to 74.36455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 73.5498 - MinusLogProbMetric: 73.5498 - val_loss: 74.3645 - val_MinusLogProbMetric: 74.3645 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 64/1000
2023-10-26 14:40:51.893 
Epoch 64/1000 
	 loss: 72.8815, MinusLogProbMetric: 72.8815, val_loss: 72.7378, val_MinusLogProbMetric: 72.7378

Epoch 64: val_loss improved from 74.36455 to 72.73775, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 72.8815 - MinusLogProbMetric: 72.8815 - val_loss: 72.7378 - val_MinusLogProbMetric: 72.7378 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 65/1000
2023-10-26 14:41:28.923 
Epoch 65/1000 
	 loss: 72.1879, MinusLogProbMetric: 72.1879, val_loss: 72.1023, val_MinusLogProbMetric: 72.1023

Epoch 65: val_loss improved from 72.73775 to 72.10229, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 72.1879 - MinusLogProbMetric: 72.1879 - val_loss: 72.1023 - val_MinusLogProbMetric: 72.1023 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 66/1000
2023-10-26 14:42:04.331 
Epoch 66/1000 
	 loss: 71.5515, MinusLogProbMetric: 71.5515, val_loss: 71.4698, val_MinusLogProbMetric: 71.4698

Epoch 66: val_loss improved from 72.10229 to 71.46982, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 71.5515 - MinusLogProbMetric: 71.5515 - val_loss: 71.4698 - val_MinusLogProbMetric: 71.4698 - lr: 1.1111e-04 - 35s/epoch - 181ms/step
Epoch 67/1000
2023-10-26 14:42:41.441 
Epoch 67/1000 
	 loss: 70.8892, MinusLogProbMetric: 70.8892, val_loss: 70.9894, val_MinusLogProbMetric: 70.9894

Epoch 67: val_loss improved from 71.46982 to 70.98938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 70.8892 - MinusLogProbMetric: 70.8892 - val_loss: 70.9894 - val_MinusLogProbMetric: 70.9894 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 68/1000
2023-10-26 14:43:18.734 
Epoch 68/1000 
	 loss: 70.4129, MinusLogProbMetric: 70.4129, val_loss: 70.5736, val_MinusLogProbMetric: 70.5736

Epoch 68: val_loss improved from 70.98938 to 70.57355, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 70.4129 - MinusLogProbMetric: 70.4129 - val_loss: 70.5736 - val_MinusLogProbMetric: 70.5736 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 69/1000
2023-10-26 14:43:55.202 
Epoch 69/1000 
	 loss: 69.6803, MinusLogProbMetric: 69.6803, val_loss: 69.7062, val_MinusLogProbMetric: 69.7062

Epoch 69: val_loss improved from 70.57355 to 69.70617, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 69.6803 - MinusLogProbMetric: 69.6803 - val_loss: 69.7062 - val_MinusLogProbMetric: 69.7062 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 70/1000
2023-10-26 14:44:31.160 
Epoch 70/1000 
	 loss: 69.0280, MinusLogProbMetric: 69.0280, val_loss: 69.4185, val_MinusLogProbMetric: 69.4185

Epoch 70: val_loss improved from 69.70617 to 69.41849, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 69.0280 - MinusLogProbMetric: 69.0280 - val_loss: 69.4185 - val_MinusLogProbMetric: 69.4185 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 71/1000
2023-10-26 14:45:08.468 
Epoch 71/1000 
	 loss: 68.5573, MinusLogProbMetric: 68.5573, val_loss: 68.7967, val_MinusLogProbMetric: 68.7967

Epoch 71: val_loss improved from 69.41849 to 68.79672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 68.5573 - MinusLogProbMetric: 68.5573 - val_loss: 68.7967 - val_MinusLogProbMetric: 68.7967 - lr: 1.1111e-04 - 37s/epoch - 191ms/step
Epoch 72/1000
2023-10-26 14:45:45.140 
Epoch 72/1000 
	 loss: 68.1555, MinusLogProbMetric: 68.1555, val_loss: 67.8630, val_MinusLogProbMetric: 67.8630

Epoch 72: val_loss improved from 68.79672 to 67.86298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 68.1555 - MinusLogProbMetric: 68.1555 - val_loss: 67.8630 - val_MinusLogProbMetric: 67.8630 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 73/1000
2023-10-26 14:46:22.153 
Epoch 73/1000 
	 loss: 67.5013, MinusLogProbMetric: 67.5013, val_loss: 67.5886, val_MinusLogProbMetric: 67.5886

Epoch 73: val_loss improved from 67.86298 to 67.58855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 67.5013 - MinusLogProbMetric: 67.5013 - val_loss: 67.5886 - val_MinusLogProbMetric: 67.5886 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 74/1000
2023-10-26 14:46:59.124 
Epoch 74/1000 
	 loss: 66.8967, MinusLogProbMetric: 66.8967, val_loss: 66.8887, val_MinusLogProbMetric: 66.8887

Epoch 74: val_loss improved from 67.58855 to 66.88869, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 66.8967 - MinusLogProbMetric: 66.8967 - val_loss: 66.8887 - val_MinusLogProbMetric: 66.8887 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 75/1000
2023-10-26 14:47:35.987 
Epoch 75/1000 
	 loss: 66.3395, MinusLogProbMetric: 66.3395, val_loss: 66.7311, val_MinusLogProbMetric: 66.7311

Epoch 75: val_loss improved from 66.88869 to 66.73110, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 66.3395 - MinusLogProbMetric: 66.3395 - val_loss: 66.7311 - val_MinusLogProbMetric: 66.7311 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 76/1000
2023-10-26 14:48:13.495 
Epoch 76/1000 
	 loss: 65.8764, MinusLogProbMetric: 65.8764, val_loss: 65.7740, val_MinusLogProbMetric: 65.7740

Epoch 76: val_loss improved from 66.73110 to 65.77399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 65.8764 - MinusLogProbMetric: 65.8764 - val_loss: 65.7740 - val_MinusLogProbMetric: 65.7740 - lr: 1.1111e-04 - 37s/epoch - 191ms/step
Epoch 77/1000
2023-10-26 14:48:50.276 
Epoch 77/1000 
	 loss: 65.3134, MinusLogProbMetric: 65.3134, val_loss: 65.4992, val_MinusLogProbMetric: 65.4992

Epoch 77: val_loss improved from 65.77399 to 65.49920, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 65.3134 - MinusLogProbMetric: 65.3134 - val_loss: 65.4992 - val_MinusLogProbMetric: 65.4992 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 78/1000
2023-10-26 14:49:27.588 
Epoch 78/1000 
	 loss: 64.8974, MinusLogProbMetric: 64.8974, val_loss: 64.9578, val_MinusLogProbMetric: 64.9578

Epoch 78: val_loss improved from 65.49920 to 64.95776, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 64.8974 - MinusLogProbMetric: 64.8974 - val_loss: 64.9578 - val_MinusLogProbMetric: 64.9578 - lr: 1.1111e-04 - 37s/epoch - 191ms/step
Epoch 79/1000
2023-10-26 14:50:04.671 
Epoch 79/1000 
	 loss: 64.4003, MinusLogProbMetric: 64.4003, val_loss: 64.4650, val_MinusLogProbMetric: 64.4650

Epoch 79: val_loss improved from 64.95776 to 64.46499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 64.4003 - MinusLogProbMetric: 64.4003 - val_loss: 64.4650 - val_MinusLogProbMetric: 64.4650 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 80/1000
2023-10-26 14:50:41.914 
Epoch 80/1000 
	 loss: 63.8761, MinusLogProbMetric: 63.8761, val_loss: 64.0579, val_MinusLogProbMetric: 64.0579

Epoch 80: val_loss improved from 64.46499 to 64.05793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 38s - loss: 63.8761 - MinusLogProbMetric: 63.8761 - val_loss: 64.0579 - val_MinusLogProbMetric: 64.0579 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 81/1000
2023-10-26 14:51:18.429 
Epoch 81/1000 
	 loss: 63.4079, MinusLogProbMetric: 63.4079, val_loss: 63.6387, val_MinusLogProbMetric: 63.6387

Epoch 81: val_loss improved from 64.05793 to 63.63873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 63.4079 - MinusLogProbMetric: 63.4079 - val_loss: 63.6387 - val_MinusLogProbMetric: 63.6387 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 82/1000
2023-10-26 14:51:55.476 
Epoch 82/1000 
	 loss: 62.9281, MinusLogProbMetric: 62.9281, val_loss: 62.8499, val_MinusLogProbMetric: 62.8499

Epoch 82: val_loss improved from 63.63873 to 62.84993, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 62.9281 - MinusLogProbMetric: 62.9281 - val_loss: 62.8499 - val_MinusLogProbMetric: 62.8499 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 83/1000
2023-10-26 14:52:32.203 
Epoch 83/1000 
	 loss: 62.5289, MinusLogProbMetric: 62.5289, val_loss: 62.5296, val_MinusLogProbMetric: 62.5296

Epoch 83: val_loss improved from 62.84993 to 62.52961, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 62.5289 - MinusLogProbMetric: 62.5289 - val_loss: 62.5296 - val_MinusLogProbMetric: 62.5296 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 84/1000
2023-10-26 14:53:09.148 
Epoch 84/1000 
	 loss: 62.0807, MinusLogProbMetric: 62.0807, val_loss: 62.1307, val_MinusLogProbMetric: 62.1307

Epoch 84: val_loss improved from 62.52961 to 62.13068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 62.0807 - MinusLogProbMetric: 62.0807 - val_loss: 62.1307 - val_MinusLogProbMetric: 62.1307 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 85/1000
2023-10-26 14:53:46.795 
Epoch 85/1000 
	 loss: 61.6475, MinusLogProbMetric: 61.6475, val_loss: 61.8694, val_MinusLogProbMetric: 61.8694

Epoch 85: val_loss improved from 62.13068 to 61.86943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 38s - loss: 61.6475 - MinusLogProbMetric: 61.6475 - val_loss: 61.8694 - val_MinusLogProbMetric: 61.8694 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 86/1000
2023-10-26 14:54:23.602 
Epoch 86/1000 
	 loss: 61.2129, MinusLogProbMetric: 61.2129, val_loss: 61.4297, val_MinusLogProbMetric: 61.4297

Epoch 86: val_loss improved from 61.86943 to 61.42971, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 61.2129 - MinusLogProbMetric: 61.2129 - val_loss: 61.4297 - val_MinusLogProbMetric: 61.4297 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 87/1000
2023-10-26 14:55:00.784 
Epoch 87/1000 
	 loss: 60.8344, MinusLogProbMetric: 60.8344, val_loss: 60.9392, val_MinusLogProbMetric: 60.9392

Epoch 87: val_loss improved from 61.42971 to 60.93924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 60.8344 - MinusLogProbMetric: 60.8344 - val_loss: 60.9392 - val_MinusLogProbMetric: 60.9392 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 88/1000
2023-10-26 14:55:36.800 
Epoch 88/1000 
	 loss: 60.3929, MinusLogProbMetric: 60.3929, val_loss: 60.4058, val_MinusLogProbMetric: 60.4058

Epoch 88: val_loss improved from 60.93924 to 60.40577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 60.3929 - MinusLogProbMetric: 60.3929 - val_loss: 60.4058 - val_MinusLogProbMetric: 60.4058 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 89/1000
2023-10-26 14:56:14.238 
Epoch 89/1000 
	 loss: 59.9841, MinusLogProbMetric: 59.9841, val_loss: 60.0707, val_MinusLogProbMetric: 60.0707

Epoch 89: val_loss improved from 60.40577 to 60.07071, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 59.9841 - MinusLogProbMetric: 59.9841 - val_loss: 60.0707 - val_MinusLogProbMetric: 60.0707 - lr: 1.1111e-04 - 37s/epoch - 191ms/step
Epoch 90/1000
2023-10-26 14:56:51.821 
Epoch 90/1000 
	 loss: 59.6291, MinusLogProbMetric: 59.6291, val_loss: 59.5931, val_MinusLogProbMetric: 59.5931

Epoch 90: val_loss improved from 60.07071 to 59.59306, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 59.6291 - MinusLogProbMetric: 59.6291 - val_loss: 59.5931 - val_MinusLogProbMetric: 59.5931 - lr: 1.1111e-04 - 37s/epoch - 191ms/step
Epoch 91/1000
2023-10-26 14:57:28.730 
Epoch 91/1000 
	 loss: 59.3780, MinusLogProbMetric: 59.3780, val_loss: 59.7628, val_MinusLogProbMetric: 59.7628

Epoch 91: val_loss did not improve from 59.59306
196/196 - 36s - loss: 59.3780 - MinusLogProbMetric: 59.3780 - val_loss: 59.7628 - val_MinusLogProbMetric: 59.7628 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 92/1000
2023-10-26 14:58:05.615 
Epoch 92/1000 
	 loss: 58.8551, MinusLogProbMetric: 58.8551, val_loss: 58.8221, val_MinusLogProbMetric: 58.8221

Epoch 92: val_loss improved from 59.59306 to 58.82215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 58.8551 - MinusLogProbMetric: 58.8551 - val_loss: 58.8221 - val_MinusLogProbMetric: 58.8221 - lr: 1.1111e-04 - 37s/epoch - 191ms/step
Epoch 93/1000
2023-10-26 14:58:42.780 
Epoch 93/1000 
	 loss: 58.4686, MinusLogProbMetric: 58.4686, val_loss: 58.4977, val_MinusLogProbMetric: 58.4977

Epoch 93: val_loss improved from 58.82215 to 58.49775, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 58.4686 - MinusLogProbMetric: 58.4686 - val_loss: 58.4977 - val_MinusLogProbMetric: 58.4977 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 94/1000
2023-10-26 14:59:19.905 
Epoch 94/1000 
	 loss: 58.1340, MinusLogProbMetric: 58.1340, val_loss: 58.6656, val_MinusLogProbMetric: 58.6656

Epoch 94: val_loss did not improve from 58.49775
196/196 - 36s - loss: 58.1340 - MinusLogProbMetric: 58.1340 - val_loss: 58.6656 - val_MinusLogProbMetric: 58.6656 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 95/1000
2023-10-26 14:59:56.113 
Epoch 95/1000 
	 loss: 57.8518, MinusLogProbMetric: 57.8518, val_loss: 58.0931, val_MinusLogProbMetric: 58.0931

Epoch 95: val_loss improved from 58.49775 to 58.09311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 57.8518 - MinusLogProbMetric: 57.8518 - val_loss: 58.0931 - val_MinusLogProbMetric: 58.0931 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 96/1000
2023-10-26 15:00:33.287 
Epoch 96/1000 
	 loss: 57.5580, MinusLogProbMetric: 57.5580, val_loss: 57.8821, val_MinusLogProbMetric: 57.8821

Epoch 96: val_loss improved from 58.09311 to 57.88213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 57.5580 - MinusLogProbMetric: 57.5580 - val_loss: 57.8821 - val_MinusLogProbMetric: 57.8821 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 97/1000
2023-10-26 15:01:10.287 
Epoch 97/1000 
	 loss: 57.0920, MinusLogProbMetric: 57.0920, val_loss: 57.2721, val_MinusLogProbMetric: 57.2721

Epoch 97: val_loss improved from 57.88213 to 57.27208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 57.0920 - MinusLogProbMetric: 57.0920 - val_loss: 57.2721 - val_MinusLogProbMetric: 57.2721 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 98/1000
2023-10-26 15:01:44.897 
Epoch 98/1000 
	 loss: 56.8047, MinusLogProbMetric: 56.8047, val_loss: 56.7652, val_MinusLogProbMetric: 56.7652

Epoch 98: val_loss improved from 57.27208 to 56.76524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 56.8047 - MinusLogProbMetric: 56.8047 - val_loss: 56.7652 - val_MinusLogProbMetric: 56.7652 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 99/1000
2023-10-26 15:02:15.182 
Epoch 99/1000 
	 loss: 56.5093, MinusLogProbMetric: 56.5093, val_loss: 56.5192, val_MinusLogProbMetric: 56.5192

Epoch 99: val_loss improved from 56.76524 to 56.51925, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 56.5093 - MinusLogProbMetric: 56.5093 - val_loss: 56.5192 - val_MinusLogProbMetric: 56.5192 - lr: 1.1111e-04 - 30s/epoch - 154ms/step
Epoch 100/1000
2023-10-26 15:02:45.719 
Epoch 100/1000 
	 loss: 56.1590, MinusLogProbMetric: 56.1590, val_loss: 56.2539, val_MinusLogProbMetric: 56.2539

Epoch 100: val_loss improved from 56.51925 to 56.25393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 56.1590 - MinusLogProbMetric: 56.1590 - val_loss: 56.2539 - val_MinusLogProbMetric: 56.2539 - lr: 1.1111e-04 - 31s/epoch - 156ms/step
Epoch 101/1000
2023-10-26 15:03:20.455 
Epoch 101/1000 
	 loss: 55.8742, MinusLogProbMetric: 55.8742, val_loss: 55.9710, val_MinusLogProbMetric: 55.9710

Epoch 101: val_loss improved from 56.25393 to 55.97098, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 55.8742 - MinusLogProbMetric: 55.8742 - val_loss: 55.9710 - val_MinusLogProbMetric: 55.9710 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 102/1000
2023-10-26 15:03:57.723 
Epoch 102/1000 
	 loss: 55.4846, MinusLogProbMetric: 55.4846, val_loss: 55.7632, val_MinusLogProbMetric: 55.7632

Epoch 102: val_loss improved from 55.97098 to 55.76316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 55.4846 - MinusLogProbMetric: 55.4846 - val_loss: 55.7632 - val_MinusLogProbMetric: 55.7632 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 103/1000
2023-10-26 15:04:34.500 
Epoch 103/1000 
	 loss: 55.1804, MinusLogProbMetric: 55.1804, val_loss: 55.3818, val_MinusLogProbMetric: 55.3818

Epoch 103: val_loss improved from 55.76316 to 55.38176, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 55.1804 - MinusLogProbMetric: 55.1804 - val_loss: 55.3818 - val_MinusLogProbMetric: 55.3818 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 104/1000
2023-10-26 15:05:11.383 
Epoch 104/1000 
	 loss: 54.9777, MinusLogProbMetric: 54.9777, val_loss: 55.1164, val_MinusLogProbMetric: 55.1164

Epoch 104: val_loss improved from 55.38176 to 55.11643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 54.9777 - MinusLogProbMetric: 54.9777 - val_loss: 55.1164 - val_MinusLogProbMetric: 55.1164 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 105/1000
2023-10-26 15:05:48.384 
Epoch 105/1000 
	 loss: 54.6189, MinusLogProbMetric: 54.6189, val_loss: 54.7076, val_MinusLogProbMetric: 54.7076

Epoch 105: val_loss improved from 55.11643 to 54.70759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 54.6189 - MinusLogProbMetric: 54.6189 - val_loss: 54.7076 - val_MinusLogProbMetric: 54.7076 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 106/1000
2023-10-26 15:06:25.391 
Epoch 106/1000 
	 loss: 54.4966, MinusLogProbMetric: 54.4966, val_loss: 54.4431, val_MinusLogProbMetric: 54.4431

Epoch 106: val_loss improved from 54.70759 to 54.44313, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 54.4966 - MinusLogProbMetric: 54.4966 - val_loss: 54.4431 - val_MinusLogProbMetric: 54.4431 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 107/1000
2023-10-26 15:07:02.609 
Epoch 107/1000 
	 loss: 54.0709, MinusLogProbMetric: 54.0709, val_loss: 54.4251, val_MinusLogProbMetric: 54.4251

Epoch 107: val_loss improved from 54.44313 to 54.42506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 54.0709 - MinusLogProbMetric: 54.0709 - val_loss: 54.4251 - val_MinusLogProbMetric: 54.4251 - lr: 1.1111e-04 - 37s/epoch - 189ms/step
Epoch 108/1000
2023-10-26 15:07:39.878 
Epoch 108/1000 
	 loss: 53.8449, MinusLogProbMetric: 53.8449, val_loss: 53.9888, val_MinusLogProbMetric: 53.9888

Epoch 108: val_loss improved from 54.42506 to 53.98877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 53.8449 - MinusLogProbMetric: 53.8449 - val_loss: 53.9888 - val_MinusLogProbMetric: 53.9888 - lr: 1.1111e-04 - 37s/epoch - 190ms/step
Epoch 109/1000
2023-10-26 15:08:17.425 
Epoch 109/1000 
	 loss: 53.5876, MinusLogProbMetric: 53.5876, val_loss: 53.7729, val_MinusLogProbMetric: 53.7729

Epoch 109: val_loss improved from 53.98877 to 53.77288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 38s - loss: 53.5876 - MinusLogProbMetric: 53.5876 - val_loss: 53.7729 - val_MinusLogProbMetric: 53.7729 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 110/1000
2023-10-26 15:08:55.125 
Epoch 110/1000 
	 loss: 53.3002, MinusLogProbMetric: 53.3002, val_loss: 53.5605, val_MinusLogProbMetric: 53.5605

Epoch 110: val_loss improved from 53.77288 to 53.56049, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 38s - loss: 53.3002 - MinusLogProbMetric: 53.3002 - val_loss: 53.5605 - val_MinusLogProbMetric: 53.5605 - lr: 1.1111e-04 - 38s/epoch - 192ms/step
Epoch 111/1000
2023-10-26 15:09:26.420 
Epoch 111/1000 
	 loss: 53.0346, MinusLogProbMetric: 53.0346, val_loss: 53.2429, val_MinusLogProbMetric: 53.2429

Epoch 111: val_loss improved from 53.56049 to 53.24291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 53.0346 - MinusLogProbMetric: 53.0346 - val_loss: 53.2429 - val_MinusLogProbMetric: 53.2429 - lr: 1.1111e-04 - 31s/epoch - 159ms/step
Epoch 112/1000
2023-10-26 15:09:54.829 
Epoch 112/1000 
	 loss: 52.7991, MinusLogProbMetric: 52.7991, val_loss: 53.0479, val_MinusLogProbMetric: 53.0479

Epoch 112: val_loss improved from 53.24291 to 53.04788, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 52.7991 - MinusLogProbMetric: 52.7991 - val_loss: 53.0479 - val_MinusLogProbMetric: 53.0479 - lr: 1.1111e-04 - 29s/epoch - 146ms/step
Epoch 113/1000
2023-10-26 15:10:30.933 
Epoch 113/1000 
	 loss: 52.5803, MinusLogProbMetric: 52.5803, val_loss: 52.6890, val_MinusLogProbMetric: 52.6890

Epoch 113: val_loss improved from 53.04788 to 52.68900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 52.5803 - MinusLogProbMetric: 52.5803 - val_loss: 52.6890 - val_MinusLogProbMetric: 52.6890 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 114/1000
2023-10-26 15:11:06.400 
Epoch 114/1000 
	 loss: 52.2661, MinusLogProbMetric: 52.2661, val_loss: 52.5364, val_MinusLogProbMetric: 52.5364

Epoch 114: val_loss improved from 52.68900 to 52.53641, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 52.2661 - MinusLogProbMetric: 52.2661 - val_loss: 52.5364 - val_MinusLogProbMetric: 52.5364 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 115/1000
2023-10-26 15:11:42.709 
Epoch 115/1000 
	 loss: 52.0022, MinusLogProbMetric: 52.0022, val_loss: 52.2481, val_MinusLogProbMetric: 52.2481

Epoch 115: val_loss improved from 52.53641 to 52.24813, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 52.0022 - MinusLogProbMetric: 52.0022 - val_loss: 52.2481 - val_MinusLogProbMetric: 52.2481 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 116/1000
2023-10-26 15:12:19.208 
Epoch 116/1000 
	 loss: 51.8680, MinusLogProbMetric: 51.8680, val_loss: 51.9844, val_MinusLogProbMetric: 51.9844

Epoch 116: val_loss improved from 52.24813 to 51.98443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 51.8680 - MinusLogProbMetric: 51.8680 - val_loss: 51.9844 - val_MinusLogProbMetric: 51.9844 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 117/1000
2023-10-26 15:12:55.495 
Epoch 117/1000 
	 loss: 51.5354, MinusLogProbMetric: 51.5354, val_loss: 51.8056, val_MinusLogProbMetric: 51.8056

Epoch 117: val_loss improved from 51.98443 to 51.80561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 51.5354 - MinusLogProbMetric: 51.5354 - val_loss: 51.8056 - val_MinusLogProbMetric: 51.8056 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 118/1000
2023-10-26 15:13:31.849 
Epoch 118/1000 
	 loss: 51.5867, MinusLogProbMetric: 51.5867, val_loss: 51.6271, val_MinusLogProbMetric: 51.6271

Epoch 118: val_loss improved from 51.80561 to 51.62712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 51.5867 - MinusLogProbMetric: 51.5867 - val_loss: 51.6271 - val_MinusLogProbMetric: 51.6271 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 119/1000
2023-10-26 15:14:07.337 
Epoch 119/1000 
	 loss: 51.2113, MinusLogProbMetric: 51.2113, val_loss: 51.6450, val_MinusLogProbMetric: 51.6450

Epoch 119: val_loss did not improve from 51.62712
196/196 - 35s - loss: 51.2113 - MinusLogProbMetric: 51.2113 - val_loss: 51.6450 - val_MinusLogProbMetric: 51.6450 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 120/1000
2023-10-26 15:14:42.807 
Epoch 120/1000 
	 loss: 50.9692, MinusLogProbMetric: 50.9692, val_loss: 51.3635, val_MinusLogProbMetric: 51.3635

Epoch 120: val_loss improved from 51.62712 to 51.36351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 50.9692 - MinusLogProbMetric: 50.9692 - val_loss: 51.3635 - val_MinusLogProbMetric: 51.3635 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 121/1000
2023-10-26 15:15:18.715 
Epoch 121/1000 
	 loss: 50.6755, MinusLogProbMetric: 50.6755, val_loss: 50.9794, val_MinusLogProbMetric: 50.9794

Epoch 121: val_loss improved from 51.36351 to 50.97942, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 50.6755 - MinusLogProbMetric: 50.6755 - val_loss: 50.9794 - val_MinusLogProbMetric: 50.9794 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 122/1000
2023-10-26 15:15:54.917 
Epoch 122/1000 
	 loss: 50.5384, MinusLogProbMetric: 50.5384, val_loss: 50.8076, val_MinusLogProbMetric: 50.8076

Epoch 122: val_loss improved from 50.97942 to 50.80759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 50.5384 - MinusLogProbMetric: 50.5384 - val_loss: 50.8076 - val_MinusLogProbMetric: 50.8076 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 123/1000
2023-10-26 15:16:31.156 
Epoch 123/1000 
	 loss: 50.3139, MinusLogProbMetric: 50.3139, val_loss: 50.8769, val_MinusLogProbMetric: 50.8769

Epoch 123: val_loss did not improve from 50.80759
196/196 - 36s - loss: 50.3139 - MinusLogProbMetric: 50.3139 - val_loss: 50.8769 - val_MinusLogProbMetric: 50.8769 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 124/1000
2023-10-26 15:17:06.737 
Epoch 124/1000 
	 loss: 50.2844, MinusLogProbMetric: 50.2844, val_loss: 50.2613, val_MinusLogProbMetric: 50.2613

Epoch 124: val_loss improved from 50.80759 to 50.26130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 50.2844 - MinusLogProbMetric: 50.2844 - val_loss: 50.2613 - val_MinusLogProbMetric: 50.2613 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 125/1000
2023-10-26 15:17:42.940 
Epoch 125/1000 
	 loss: 49.8418, MinusLogProbMetric: 49.8418, val_loss: 50.0488, val_MinusLogProbMetric: 50.0488

Epoch 125: val_loss improved from 50.26130 to 50.04881, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 49.8418 - MinusLogProbMetric: 49.8418 - val_loss: 50.0488 - val_MinusLogProbMetric: 50.0488 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 126/1000
2023-10-26 15:18:19.070 
Epoch 126/1000 
	 loss: 49.7041, MinusLogProbMetric: 49.7041, val_loss: 49.8668, val_MinusLogProbMetric: 49.8668

Epoch 126: val_loss improved from 50.04881 to 49.86678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 49.7041 - MinusLogProbMetric: 49.7041 - val_loss: 49.8668 - val_MinusLogProbMetric: 49.8668 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 127/1000
2023-10-26 15:18:54.586 
Epoch 127/1000 
	 loss: 49.6905, MinusLogProbMetric: 49.6905, val_loss: 49.9460, val_MinusLogProbMetric: 49.9460

Epoch 127: val_loss did not improve from 49.86678
196/196 - 35s - loss: 49.6905 - MinusLogProbMetric: 49.6905 - val_loss: 49.9460 - val_MinusLogProbMetric: 49.9460 - lr: 1.1111e-04 - 35s/epoch - 179ms/step
Epoch 128/1000
2023-10-26 15:19:30.238 
Epoch 128/1000 
	 loss: 49.3713, MinusLogProbMetric: 49.3713, val_loss: 49.7576, val_MinusLogProbMetric: 49.7576

Epoch 128: val_loss improved from 49.86678 to 49.75756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 49.3713 - MinusLogProbMetric: 49.3713 - val_loss: 49.7576 - val_MinusLogProbMetric: 49.7576 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 129/1000
2023-10-26 15:20:05.523 
Epoch 129/1000 
	 loss: 49.3105, MinusLogProbMetric: 49.3105, val_loss: 49.3099, val_MinusLogProbMetric: 49.3099

Epoch 129: val_loss improved from 49.75756 to 49.30989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 49.3105 - MinusLogProbMetric: 49.3105 - val_loss: 49.3099 - val_MinusLogProbMetric: 49.3099 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 130/1000
2023-10-26 15:20:41.665 
Epoch 130/1000 
	 loss: 49.0275, MinusLogProbMetric: 49.0275, val_loss: 49.3150, val_MinusLogProbMetric: 49.3150

Epoch 130: val_loss did not improve from 49.30989
196/196 - 36s - loss: 49.0275 - MinusLogProbMetric: 49.0275 - val_loss: 49.3150 - val_MinusLogProbMetric: 49.3150 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 131/1000
2023-10-26 15:21:17.340 
Epoch 131/1000 
	 loss: 48.7322, MinusLogProbMetric: 48.7322, val_loss: 48.9531, val_MinusLogProbMetric: 48.9531

Epoch 131: val_loss improved from 49.30989 to 48.95314, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 48.7322 - MinusLogProbMetric: 48.7322 - val_loss: 48.9531 - val_MinusLogProbMetric: 48.9531 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 132/1000
2023-10-26 15:21:52.141 
Epoch 132/1000 
	 loss: 48.6291, MinusLogProbMetric: 48.6291, val_loss: 48.7929, val_MinusLogProbMetric: 48.7929

Epoch 132: val_loss improved from 48.95314 to 48.79290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 48.6291 - MinusLogProbMetric: 48.6291 - val_loss: 48.7929 - val_MinusLogProbMetric: 48.7929 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 133/1000
2023-10-26 15:22:28.306 
Epoch 133/1000 
	 loss: 48.3869, MinusLogProbMetric: 48.3869, val_loss: 48.5360, val_MinusLogProbMetric: 48.5360

Epoch 133: val_loss improved from 48.79290 to 48.53599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 48.3869 - MinusLogProbMetric: 48.3869 - val_loss: 48.5360 - val_MinusLogProbMetric: 48.5360 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 134/1000
2023-10-26 15:23:03.824 
Epoch 134/1000 
	 loss: 48.2378, MinusLogProbMetric: 48.2378, val_loss: 48.5124, val_MinusLogProbMetric: 48.5124

Epoch 134: val_loss improved from 48.53599 to 48.51239, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 48.2378 - MinusLogProbMetric: 48.2378 - val_loss: 48.5124 - val_MinusLogProbMetric: 48.5124 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 135/1000
2023-10-26 15:23:40.020 
Epoch 135/1000 
	 loss: 48.1885, MinusLogProbMetric: 48.1885, val_loss: 48.3138, val_MinusLogProbMetric: 48.3138

Epoch 135: val_loss improved from 48.51239 to 48.31376, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 48.1885 - MinusLogProbMetric: 48.1885 - val_loss: 48.3138 - val_MinusLogProbMetric: 48.3138 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 136/1000
2023-10-26 15:24:15.958 
Epoch 136/1000 
	 loss: 47.9703, MinusLogProbMetric: 47.9703, val_loss: 48.4423, val_MinusLogProbMetric: 48.4423

Epoch 136: val_loss did not improve from 48.31376
196/196 - 35s - loss: 47.9703 - MinusLogProbMetric: 47.9703 - val_loss: 48.4423 - val_MinusLogProbMetric: 48.4423 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 137/1000
2023-10-26 15:24:50.727 
Epoch 137/1000 
	 loss: 47.6927, MinusLogProbMetric: 47.6927, val_loss: 48.0950, val_MinusLogProbMetric: 48.0950

Epoch 137: val_loss improved from 48.31376 to 48.09495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 47.6927 - MinusLogProbMetric: 47.6927 - val_loss: 48.0950 - val_MinusLogProbMetric: 48.0950 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 138/1000
2023-10-26 15:25:26.540 
Epoch 138/1000 
	 loss: 47.5819, MinusLogProbMetric: 47.5819, val_loss: 47.8048, val_MinusLogProbMetric: 47.8048

Epoch 138: val_loss improved from 48.09495 to 47.80484, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 47.5819 - MinusLogProbMetric: 47.5819 - val_loss: 47.8048 - val_MinusLogProbMetric: 47.8048 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 139/1000
2023-10-26 15:26:02.333 
Epoch 139/1000 
	 loss: 47.4498, MinusLogProbMetric: 47.4498, val_loss: 47.9083, val_MinusLogProbMetric: 47.9083

Epoch 139: val_loss did not improve from 47.80484
196/196 - 35s - loss: 47.4498 - MinusLogProbMetric: 47.4498 - val_loss: 47.9083 - val_MinusLogProbMetric: 47.9083 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 140/1000
2023-10-26 15:26:33.514 
Epoch 140/1000 
	 loss: 47.2981, MinusLogProbMetric: 47.2981, val_loss: 47.6878, val_MinusLogProbMetric: 47.6878

Epoch 140: val_loss improved from 47.80484 to 47.68781, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 47.2981 - MinusLogProbMetric: 47.2981 - val_loss: 47.6878 - val_MinusLogProbMetric: 47.6878 - lr: 1.1111e-04 - 32s/epoch - 161ms/step
Epoch 141/1000
2023-10-26 15:27:04.106 
Epoch 141/1000 
	 loss: 47.3814, MinusLogProbMetric: 47.3814, val_loss: 47.4567, val_MinusLogProbMetric: 47.4567

Epoch 141: val_loss improved from 47.68781 to 47.45669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 47.3814 - MinusLogProbMetric: 47.3814 - val_loss: 47.4567 - val_MinusLogProbMetric: 47.4567 - lr: 1.1111e-04 - 31s/epoch - 156ms/step
Epoch 142/1000
2023-10-26 15:27:34.483 
Epoch 142/1000 
	 loss: 47.0362, MinusLogProbMetric: 47.0362, val_loss: 47.4678, val_MinusLogProbMetric: 47.4678

Epoch 142: val_loss did not improve from 47.45669
196/196 - 30s - loss: 47.0362 - MinusLogProbMetric: 47.0362 - val_loss: 47.4678 - val_MinusLogProbMetric: 47.4678 - lr: 1.1111e-04 - 30s/epoch - 153ms/step
Epoch 143/1000
2023-10-26 15:28:07.982 
Epoch 143/1000 
	 loss: 46.8171, MinusLogProbMetric: 46.8171, val_loss: 47.5592, val_MinusLogProbMetric: 47.5592

Epoch 143: val_loss did not improve from 47.45669
196/196 - 33s - loss: 46.8171 - MinusLogProbMetric: 46.8171 - val_loss: 47.5592 - val_MinusLogProbMetric: 47.5592 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 144/1000
2023-10-26 15:28:41.909 
Epoch 144/1000 
	 loss: 46.8561, MinusLogProbMetric: 46.8561, val_loss: 46.9752, val_MinusLogProbMetric: 46.9752

Epoch 144: val_loss improved from 47.45669 to 46.97523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 46.8561 - MinusLogProbMetric: 46.8561 - val_loss: 46.9752 - val_MinusLogProbMetric: 46.9752 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 145/1000
2023-10-26 15:29:15.972 
Epoch 145/1000 
	 loss: 46.5504, MinusLogProbMetric: 46.5504, val_loss: 46.6463, val_MinusLogProbMetric: 46.6463

Epoch 145: val_loss improved from 46.97523 to 46.64629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 46.5504 - MinusLogProbMetric: 46.5504 - val_loss: 46.6463 - val_MinusLogProbMetric: 46.6463 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 146/1000
2023-10-26 15:29:50.498 
Epoch 146/1000 
	 loss: 46.3820, MinusLogProbMetric: 46.3820, val_loss: 46.6769, val_MinusLogProbMetric: 46.6769

Epoch 146: val_loss did not improve from 46.64629
196/196 - 34s - loss: 46.3820 - MinusLogProbMetric: 46.3820 - val_loss: 46.6769 - val_MinusLogProbMetric: 46.6769 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 147/1000
2023-10-26 15:30:20.632 
Epoch 147/1000 
	 loss: 46.3237, MinusLogProbMetric: 46.3237, val_loss: 46.7719, val_MinusLogProbMetric: 46.7719

Epoch 147: val_loss did not improve from 46.64629
196/196 - 30s - loss: 46.3237 - MinusLogProbMetric: 46.3237 - val_loss: 46.7719 - val_MinusLogProbMetric: 46.7719 - lr: 1.1111e-04 - 30s/epoch - 154ms/step
Epoch 148/1000
2023-10-26 15:30:50.895 
Epoch 148/1000 
	 loss: 46.2411, MinusLogProbMetric: 46.2411, val_loss: 46.2999, val_MinusLogProbMetric: 46.2999

Epoch 148: val_loss improved from 46.64629 to 46.29986, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 46.2411 - MinusLogProbMetric: 46.2411 - val_loss: 46.2999 - val_MinusLogProbMetric: 46.2999 - lr: 1.1111e-04 - 31s/epoch - 157ms/step
Epoch 149/1000
2023-10-26 15:31:23.594 
Epoch 149/1000 
	 loss: 46.0230, MinusLogProbMetric: 46.0230, val_loss: 46.3785, val_MinusLogProbMetric: 46.3785

Epoch 149: val_loss did not improve from 46.29986
196/196 - 32s - loss: 46.0230 - MinusLogProbMetric: 46.0230 - val_loss: 46.3785 - val_MinusLogProbMetric: 46.3785 - lr: 1.1111e-04 - 32s/epoch - 164ms/step
Epoch 150/1000
2023-10-26 15:31:57.088 
Epoch 150/1000 
	 loss: 45.8514, MinusLogProbMetric: 45.8514, val_loss: 46.4296, val_MinusLogProbMetric: 46.4296

Epoch 150: val_loss did not improve from 46.29986
196/196 - 33s - loss: 45.8514 - MinusLogProbMetric: 45.8514 - val_loss: 46.4296 - val_MinusLogProbMetric: 46.4296 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 151/1000
2023-10-26 15:32:29.141 
Epoch 151/1000 
	 loss: 45.7320, MinusLogProbMetric: 45.7320, val_loss: 45.9907, val_MinusLogProbMetric: 45.9907

Epoch 151: val_loss improved from 46.29986 to 45.99068, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 45.7320 - MinusLogProbMetric: 45.7320 - val_loss: 45.9907 - val_MinusLogProbMetric: 45.9907 - lr: 1.1111e-04 - 32s/epoch - 166ms/step
Epoch 152/1000
2023-10-26 15:32:59.000 
Epoch 152/1000 
	 loss: 45.5703, MinusLogProbMetric: 45.5703, val_loss: 46.3785, val_MinusLogProbMetric: 46.3785

Epoch 152: val_loss did not improve from 45.99068
196/196 - 29s - loss: 45.5703 - MinusLogProbMetric: 45.5703 - val_loss: 46.3785 - val_MinusLogProbMetric: 46.3785 - lr: 1.1111e-04 - 29s/epoch - 150ms/step
Epoch 153/1000
2023-10-26 15:33:29.035 
Epoch 153/1000 
	 loss: 45.4985, MinusLogProbMetric: 45.4985, val_loss: 45.7659, val_MinusLogProbMetric: 45.7659

Epoch 153: val_loss improved from 45.99068 to 45.76587, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 45.4985 - MinusLogProbMetric: 45.4985 - val_loss: 45.7659 - val_MinusLogProbMetric: 45.7659 - lr: 1.1111e-04 - 31s/epoch - 156ms/step
Epoch 154/1000
2023-10-26 15:34:03.847 
Epoch 154/1000 
	 loss: 45.4404, MinusLogProbMetric: 45.4404, val_loss: 45.7472, val_MinusLogProbMetric: 45.7472

Epoch 154: val_loss improved from 45.76587 to 45.74721, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 45.4404 - MinusLogProbMetric: 45.4404 - val_loss: 45.7472 - val_MinusLogProbMetric: 45.7472 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 155/1000
2023-10-26 15:34:38.430 
Epoch 155/1000 
	 loss: 45.2213, MinusLogProbMetric: 45.2213, val_loss: 45.4849, val_MinusLogProbMetric: 45.4849

Epoch 155: val_loss improved from 45.74721 to 45.48491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 45.2213 - MinusLogProbMetric: 45.2213 - val_loss: 45.4849 - val_MinusLogProbMetric: 45.4849 - lr: 1.1111e-04 - 34s/epoch - 176ms/step
Epoch 156/1000
2023-10-26 15:35:09.376 
Epoch 156/1000 
	 loss: 45.1637, MinusLogProbMetric: 45.1637, val_loss: 45.8786, val_MinusLogProbMetric: 45.8786

Epoch 156: val_loss did not improve from 45.48491
196/196 - 30s - loss: 45.1637 - MinusLogProbMetric: 45.1637 - val_loss: 45.8786 - val_MinusLogProbMetric: 45.8786 - lr: 1.1111e-04 - 30s/epoch - 156ms/step
Epoch 157/1000
2023-10-26 15:35:39.408 
Epoch 157/1000 
	 loss: 45.2070, MinusLogProbMetric: 45.2070, val_loss: 45.3251, val_MinusLogProbMetric: 45.3251

Epoch 157: val_loss improved from 45.48491 to 45.32507, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 45.2070 - MinusLogProbMetric: 45.2070 - val_loss: 45.3251 - val_MinusLogProbMetric: 45.3251 - lr: 1.1111e-04 - 31s/epoch - 156ms/step
Epoch 158/1000
2023-10-26 15:36:10.908 
Epoch 158/1000 
	 loss: 44.8460, MinusLogProbMetric: 44.8460, val_loss: 45.0660, val_MinusLogProbMetric: 45.0660

Epoch 158: val_loss improved from 45.32507 to 45.06605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 44.8460 - MinusLogProbMetric: 44.8460 - val_loss: 45.0660 - val_MinusLogProbMetric: 45.0660 - lr: 1.1111e-04 - 31s/epoch - 160ms/step
Epoch 159/1000
2023-10-26 15:36:46.349 
Epoch 159/1000 
	 loss: 44.7836, MinusLogProbMetric: 44.7836, val_loss: 44.8804, val_MinusLogProbMetric: 44.8804

Epoch 159: val_loss improved from 45.06605 to 44.88041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 44.7836 - MinusLogProbMetric: 44.7836 - val_loss: 44.8804 - val_MinusLogProbMetric: 44.8804 - lr: 1.1111e-04 - 35s/epoch - 181ms/step
Epoch 160/1000
2023-10-26 15:37:18.856 
Epoch 160/1000 
	 loss: 44.6441, MinusLogProbMetric: 44.6441, val_loss: 44.8070, val_MinusLogProbMetric: 44.8070

Epoch 160: val_loss improved from 44.88041 to 44.80696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 44.6441 - MinusLogProbMetric: 44.6441 - val_loss: 44.8070 - val_MinusLogProbMetric: 44.8070 - lr: 1.1111e-04 - 33s/epoch - 166ms/step
Epoch 161/1000
2023-10-26 15:37:49.280 
Epoch 161/1000 
	 loss: 44.5405, MinusLogProbMetric: 44.5405, val_loss: 44.6756, val_MinusLogProbMetric: 44.6756

Epoch 161: val_loss improved from 44.80696 to 44.67557, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 44.5405 - MinusLogProbMetric: 44.5405 - val_loss: 44.6756 - val_MinusLogProbMetric: 44.6756 - lr: 1.1111e-04 - 30s/epoch - 155ms/step
Epoch 162/1000
2023-10-26 15:38:24.860 
Epoch 162/1000 
	 loss: 44.4364, MinusLogProbMetric: 44.4364, val_loss: 44.9267, val_MinusLogProbMetric: 44.9267

Epoch 162: val_loss did not improve from 44.67557
196/196 - 35s - loss: 44.4364 - MinusLogProbMetric: 44.4364 - val_loss: 44.9267 - val_MinusLogProbMetric: 44.9267 - lr: 1.1111e-04 - 35s/epoch - 179ms/step
Epoch 163/1000
2023-10-26 15:38:56.404 
Epoch 163/1000 
	 loss: 44.3603, MinusLogProbMetric: 44.3603, val_loss: 44.5694, val_MinusLogProbMetric: 44.5694

Epoch 163: val_loss improved from 44.67557 to 44.56944, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 44.3603 - MinusLogProbMetric: 44.3603 - val_loss: 44.5694 - val_MinusLogProbMetric: 44.5694 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 164/1000
2023-10-26 15:39:28.010 
Epoch 164/1000 
	 loss: 44.2021, MinusLogProbMetric: 44.2021, val_loss: 44.3587, val_MinusLogProbMetric: 44.3587

Epoch 164: val_loss improved from 44.56944 to 44.35866, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 44.2021 - MinusLogProbMetric: 44.2021 - val_loss: 44.3587 - val_MinusLogProbMetric: 44.3587 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 165/1000
2023-10-26 15:40:01.563 
Epoch 165/1000 
	 loss: 44.2453, MinusLogProbMetric: 44.2453, val_loss: 44.2920, val_MinusLogProbMetric: 44.2920

Epoch 165: val_loss improved from 44.35866 to 44.29203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 44.2453 - MinusLogProbMetric: 44.2453 - val_loss: 44.2920 - val_MinusLogProbMetric: 44.2920 - lr: 1.1111e-04 - 33s/epoch - 170ms/step
Epoch 166/1000
2023-10-26 15:40:31.904 
Epoch 166/1000 
	 loss: 43.9588, MinusLogProbMetric: 43.9588, val_loss: 44.1460, val_MinusLogProbMetric: 44.1460

Epoch 166: val_loss improved from 44.29203 to 44.14605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 43.9588 - MinusLogProbMetric: 43.9588 - val_loss: 44.1460 - val_MinusLogProbMetric: 44.1460 - lr: 1.1111e-04 - 30s/epoch - 155ms/step
Epoch 167/1000
2023-10-26 15:41:05.177 
Epoch 167/1000 
	 loss: 44.0388, MinusLogProbMetric: 44.0388, val_loss: 44.2886, val_MinusLogProbMetric: 44.2886

Epoch 167: val_loss did not improve from 44.14605
196/196 - 33s - loss: 44.0388 - MinusLogProbMetric: 44.0388 - val_loss: 44.2886 - val_MinusLogProbMetric: 44.2886 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 168/1000
2023-10-26 15:41:37.881 
Epoch 168/1000 
	 loss: 43.9618, MinusLogProbMetric: 43.9618, val_loss: 44.4187, val_MinusLogProbMetric: 44.4187

Epoch 168: val_loss did not improve from 44.14605
196/196 - 33s - loss: 43.9618 - MinusLogProbMetric: 43.9618 - val_loss: 44.4187 - val_MinusLogProbMetric: 44.4187 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 169/1000
2023-10-26 15:42:07.445 
Epoch 169/1000 
	 loss: 43.8646, MinusLogProbMetric: 43.8646, val_loss: 44.1302, val_MinusLogProbMetric: 44.1302

Epoch 169: val_loss improved from 44.14605 to 44.13024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 43.8646 - MinusLogProbMetric: 43.8646 - val_loss: 44.1302 - val_MinusLogProbMetric: 44.1302 - lr: 1.1111e-04 - 30s/epoch - 153ms/step
Epoch 170/1000
2023-10-26 15:42:42.594 
Epoch 170/1000 
	 loss: 45.0025, MinusLogProbMetric: 45.0025, val_loss: 45.7023, val_MinusLogProbMetric: 45.7023

Epoch 170: val_loss did not improve from 44.13024
196/196 - 35s - loss: 45.0025 - MinusLogProbMetric: 45.0025 - val_loss: 45.7023 - val_MinusLogProbMetric: 45.7023 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 171/1000
2023-10-26 15:43:14.679 
Epoch 171/1000 
	 loss: 44.7352, MinusLogProbMetric: 44.7352, val_loss: 44.7922, val_MinusLogProbMetric: 44.7922

Epoch 171: val_loss did not improve from 44.13024
196/196 - 32s - loss: 44.7352 - MinusLogProbMetric: 44.7352 - val_loss: 44.7922 - val_MinusLogProbMetric: 44.7922 - lr: 1.1111e-04 - 32s/epoch - 164ms/step
Epoch 172/1000
2023-10-26 15:43:45.888 
Epoch 172/1000 
	 loss: 43.9658, MinusLogProbMetric: 43.9658, val_loss: 43.8044, val_MinusLogProbMetric: 43.8044

Epoch 172: val_loss improved from 44.13024 to 43.80440, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 43.9658 - MinusLogProbMetric: 43.9658 - val_loss: 43.8044 - val_MinusLogProbMetric: 43.8044 - lr: 1.1111e-04 - 32s/epoch - 162ms/step
Epoch 173/1000
2023-10-26 15:44:21.335 
Epoch 173/1000 
	 loss: 43.2944, MinusLogProbMetric: 43.2944, val_loss: 43.4776, val_MinusLogProbMetric: 43.4776

Epoch 173: val_loss improved from 43.80440 to 43.47761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 43.2944 - MinusLogProbMetric: 43.2944 - val_loss: 43.4776 - val_MinusLogProbMetric: 43.4776 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 174/1000
2023-10-26 15:44:52.476 
Epoch 174/1000 
	 loss: 43.2226, MinusLogProbMetric: 43.2226, val_loss: 43.3929, val_MinusLogProbMetric: 43.3929

Epoch 174: val_loss improved from 43.47761 to 43.39292, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 43.2226 - MinusLogProbMetric: 43.2226 - val_loss: 43.3929 - val_MinusLogProbMetric: 43.3929 - lr: 1.1111e-04 - 31s/epoch - 159ms/step
Epoch 175/1000
2023-10-26 15:45:24.943 
Epoch 175/1000 
	 loss: 43.1238, MinusLogProbMetric: 43.1238, val_loss: 43.4652, val_MinusLogProbMetric: 43.4652

Epoch 175: val_loss did not improve from 43.39292
196/196 - 32s - loss: 43.1238 - MinusLogProbMetric: 43.1238 - val_loss: 43.4652 - val_MinusLogProbMetric: 43.4652 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 176/1000
2023-10-26 15:45:58.726 
Epoch 176/1000 
	 loss: 43.1806, MinusLogProbMetric: 43.1806, val_loss: 43.5917, val_MinusLogProbMetric: 43.5917

Epoch 176: val_loss did not improve from 43.39292
196/196 - 34s - loss: 43.1806 - MinusLogProbMetric: 43.1806 - val_loss: 43.5917 - val_MinusLogProbMetric: 43.5917 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 177/1000
2023-10-26 15:46:28.745 
Epoch 177/1000 
	 loss: 42.9025, MinusLogProbMetric: 42.9025, val_loss: 43.4979, val_MinusLogProbMetric: 43.4979

Epoch 177: val_loss did not improve from 43.39292
196/196 - 30s - loss: 42.9025 - MinusLogProbMetric: 42.9025 - val_loss: 43.4979 - val_MinusLogProbMetric: 43.4979 - lr: 1.1111e-04 - 30s/epoch - 153ms/step
Epoch 178/1000
2023-10-26 15:47:03.242 
Epoch 178/1000 
	 loss: 42.8157, MinusLogProbMetric: 42.8157, val_loss: 43.0636, val_MinusLogProbMetric: 43.0636

Epoch 178: val_loss improved from 43.39292 to 43.06364, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 42.8157 - MinusLogProbMetric: 42.8157 - val_loss: 43.0636 - val_MinusLogProbMetric: 43.0636 - lr: 1.1111e-04 - 35s/epoch - 179ms/step
Epoch 179/1000
2023-10-26 15:47:34.807 
Epoch 179/1000 
	 loss: 42.6944, MinusLogProbMetric: 42.6944, val_loss: 42.8906, val_MinusLogProbMetric: 42.8906

Epoch 179: val_loss improved from 43.06364 to 42.89064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 42.6944 - MinusLogProbMetric: 42.6944 - val_loss: 42.8906 - val_MinusLogProbMetric: 42.8906 - lr: 1.1111e-04 - 31s/epoch - 161ms/step
Epoch 180/1000
2023-10-26 15:48:05.611 
Epoch 180/1000 
	 loss: 42.8115, MinusLogProbMetric: 42.8115, val_loss: 43.2511, val_MinusLogProbMetric: 43.2511

Epoch 180: val_loss did not improve from 42.89064
196/196 - 30s - loss: 42.8115 - MinusLogProbMetric: 42.8115 - val_loss: 43.2511 - val_MinusLogProbMetric: 43.2511 - lr: 1.1111e-04 - 30s/epoch - 155ms/step
Epoch 181/1000
2023-10-26 15:48:40.789 
Epoch 181/1000 
	 loss: 42.5782, MinusLogProbMetric: 42.5782, val_loss: 42.9365, val_MinusLogProbMetric: 42.9365

Epoch 181: val_loss did not improve from 42.89064
196/196 - 35s - loss: 42.5782 - MinusLogProbMetric: 42.5782 - val_loss: 42.9365 - val_MinusLogProbMetric: 42.9365 - lr: 1.1111e-04 - 35s/epoch - 179ms/step
Epoch 182/1000
2023-10-26 15:49:12.768 
Epoch 182/1000 
	 loss: 42.4318, MinusLogProbMetric: 42.4318, val_loss: 42.6087, val_MinusLogProbMetric: 42.6087

Epoch 182: val_loss improved from 42.89064 to 42.60868, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 42.4318 - MinusLogProbMetric: 42.4318 - val_loss: 42.6087 - val_MinusLogProbMetric: 42.6087 - lr: 1.1111e-04 - 32s/epoch - 165ms/step
Epoch 183/1000
2023-10-26 15:49:44.794 
Epoch 183/1000 
	 loss: 42.3495, MinusLogProbMetric: 42.3495, val_loss: 42.5325, val_MinusLogProbMetric: 42.5325

Epoch 183: val_loss improved from 42.60868 to 42.53251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 42.3495 - MinusLogProbMetric: 42.3495 - val_loss: 42.5325 - val_MinusLogProbMetric: 42.5325 - lr: 1.1111e-04 - 32s/epoch - 164ms/step
Epoch 184/1000
2023-10-26 15:50:20.274 
Epoch 184/1000 
	 loss: 42.3563, MinusLogProbMetric: 42.3563, val_loss: 42.8881, val_MinusLogProbMetric: 42.8881

Epoch 184: val_loss did not improve from 42.53251
196/196 - 35s - loss: 42.3563 - MinusLogProbMetric: 42.3563 - val_loss: 42.8881 - val_MinusLogProbMetric: 42.8881 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 185/1000
2023-10-26 15:50:50.257 
Epoch 185/1000 
	 loss: 42.2745, MinusLogProbMetric: 42.2745, val_loss: 42.5534, val_MinusLogProbMetric: 42.5534

Epoch 185: val_loss did not improve from 42.53251
196/196 - 30s - loss: 42.2745 - MinusLogProbMetric: 42.2745 - val_loss: 42.5534 - val_MinusLogProbMetric: 42.5534 - lr: 1.1111e-04 - 30s/epoch - 153ms/step
Epoch 186/1000
2023-10-26 15:51:21.896 
Epoch 186/1000 
	 loss: 42.2499, MinusLogProbMetric: 42.2499, val_loss: 42.5673, val_MinusLogProbMetric: 42.5673

Epoch 186: val_loss did not improve from 42.53251
196/196 - 32s - loss: 42.2499 - MinusLogProbMetric: 42.2499 - val_loss: 42.5673 - val_MinusLogProbMetric: 42.5673 - lr: 1.1111e-04 - 32s/epoch - 161ms/step
Epoch 187/1000
2023-10-26 15:51:56.221 
Epoch 187/1000 
	 loss: 42.0129, MinusLogProbMetric: 42.0129, val_loss: 43.0629, val_MinusLogProbMetric: 43.0629

Epoch 187: val_loss did not improve from 42.53251
196/196 - 34s - loss: 42.0129 - MinusLogProbMetric: 42.0129 - val_loss: 43.0629 - val_MinusLogProbMetric: 43.0629 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 188/1000
2023-10-26 15:52:26.810 
Epoch 188/1000 
	 loss: 42.2183, MinusLogProbMetric: 42.2183, val_loss: 42.8246, val_MinusLogProbMetric: 42.8246

Epoch 188: val_loss did not improve from 42.53251
196/196 - 31s - loss: 42.2183 - MinusLogProbMetric: 42.2183 - val_loss: 42.8246 - val_MinusLogProbMetric: 42.8246 - lr: 1.1111e-04 - 31s/epoch - 156ms/step
Epoch 189/1000
2023-10-26 15:52:59.237 
Epoch 189/1000 
	 loss: 41.8418, MinusLogProbMetric: 41.8418, val_loss: 42.0707, val_MinusLogProbMetric: 42.0707

Epoch 189: val_loss improved from 42.53251 to 42.07074, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 41.8418 - MinusLogProbMetric: 41.8418 - val_loss: 42.0707 - val_MinusLogProbMetric: 42.0707 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 190/1000
2023-10-26 15:53:36.818 
Epoch 190/1000 
	 loss: 41.9685, MinusLogProbMetric: 41.9685, val_loss: 42.1333, val_MinusLogProbMetric: 42.1333

Epoch 190: val_loss did not improve from 42.07074
196/196 - 37s - loss: 41.9685 - MinusLogProbMetric: 41.9685 - val_loss: 42.1333 - val_MinusLogProbMetric: 42.1333 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 191/1000
2023-10-26 15:54:07.979 
Epoch 191/1000 
	 loss: 41.9235, MinusLogProbMetric: 41.9235, val_loss: 43.3724, val_MinusLogProbMetric: 43.3724

Epoch 191: val_loss did not improve from 42.07074
196/196 - 31s - loss: 41.9235 - MinusLogProbMetric: 41.9235 - val_loss: 43.3724 - val_MinusLogProbMetric: 43.3724 - lr: 1.1111e-04 - 31s/epoch - 159ms/step
Epoch 192/1000
2023-10-26 15:54:40.475 
Epoch 192/1000 
	 loss: 41.7858, MinusLogProbMetric: 41.7858, val_loss: 41.9456, val_MinusLogProbMetric: 41.9456

Epoch 192: val_loss improved from 42.07074 to 41.94561, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 41.7858 - MinusLogProbMetric: 41.7858 - val_loss: 41.9456 - val_MinusLogProbMetric: 41.9456 - lr: 1.1111e-04 - 33s/epoch - 169ms/step
Epoch 193/1000
2023-10-26 15:55:17.519 
Epoch 193/1000 
	 loss: 41.9537, MinusLogProbMetric: 41.9537, val_loss: 42.0933, val_MinusLogProbMetric: 42.0933

Epoch 193: val_loss did not improve from 41.94561
196/196 - 36s - loss: 41.9537 - MinusLogProbMetric: 41.9537 - val_loss: 42.0933 - val_MinusLogProbMetric: 42.0933 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 194/1000
2023-10-26 15:55:53.277 
Epoch 194/1000 
	 loss: 41.5096, MinusLogProbMetric: 41.5096, val_loss: 42.0240, val_MinusLogProbMetric: 42.0240

Epoch 194: val_loss did not improve from 41.94561
196/196 - 36s - loss: 41.5096 - MinusLogProbMetric: 41.5096 - val_loss: 42.0240 - val_MinusLogProbMetric: 42.0240 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 195/1000
2023-10-26 15:56:29.748 
Epoch 195/1000 
	 loss: 41.7835, MinusLogProbMetric: 41.7835, val_loss: 42.0558, val_MinusLogProbMetric: 42.0558

Epoch 195: val_loss did not improve from 41.94561
196/196 - 36s - loss: 41.7835 - MinusLogProbMetric: 41.7835 - val_loss: 42.0558 - val_MinusLogProbMetric: 42.0558 - lr: 1.1111e-04 - 36s/epoch - 186ms/step
Epoch 196/1000
2023-10-26 15:57:07.678 
Epoch 196/1000 
	 loss: 41.3897, MinusLogProbMetric: 41.3897, val_loss: 41.6294, val_MinusLogProbMetric: 41.6294

Epoch 196: val_loss improved from 41.94561 to 41.62942, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 38s - loss: 41.3897 - MinusLogProbMetric: 41.3897 - val_loss: 41.6294 - val_MinusLogProbMetric: 41.6294 - lr: 1.1111e-04 - 38s/epoch - 196ms/step
Epoch 197/1000
2023-10-26 15:57:44.952 
Epoch 197/1000 
	 loss: 41.3141, MinusLogProbMetric: 41.3141, val_loss: 41.9094, val_MinusLogProbMetric: 41.9094

Epoch 197: val_loss did not improve from 41.62942
196/196 - 37s - loss: 41.3141 - MinusLogProbMetric: 41.3141 - val_loss: 41.9094 - val_MinusLogProbMetric: 41.9094 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 198/1000
2023-10-26 15:58:17.352 
Epoch 198/1000 
	 loss: 41.2298, MinusLogProbMetric: 41.2298, val_loss: 41.4730, val_MinusLogProbMetric: 41.4730

Epoch 198: val_loss improved from 41.62942 to 41.47300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 41.2298 - MinusLogProbMetric: 41.2298 - val_loss: 41.4730 - val_MinusLogProbMetric: 41.4730 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 199/1000
2023-10-26 15:58:48.364 
Epoch 199/1000 
	 loss: 41.5209, MinusLogProbMetric: 41.5209, val_loss: 41.7780, val_MinusLogProbMetric: 41.7780

Epoch 199: val_loss did not improve from 41.47300
196/196 - 30s - loss: 41.5209 - MinusLogProbMetric: 41.5209 - val_loss: 41.7780 - val_MinusLogProbMetric: 41.7780 - lr: 1.1111e-04 - 30s/epoch - 155ms/step
Epoch 200/1000
2023-10-26 15:59:19.000 
Epoch 200/1000 
	 loss: 41.4988, MinusLogProbMetric: 41.4988, val_loss: 41.4442, val_MinusLogProbMetric: 41.4442

Epoch 200: val_loss improved from 41.47300 to 41.44420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 41.4988 - MinusLogProbMetric: 41.4988 - val_loss: 41.4442 - val_MinusLogProbMetric: 41.4442 - lr: 1.1111e-04 - 31s/epoch - 159ms/step
Epoch 201/1000
2023-10-26 15:59:49.389 
Epoch 201/1000 
	 loss: 41.0758, MinusLogProbMetric: 41.0758, val_loss: 42.6810, val_MinusLogProbMetric: 42.6810

Epoch 201: val_loss did not improve from 41.44420
196/196 - 30s - loss: 41.0758 - MinusLogProbMetric: 41.0758 - val_loss: 42.6810 - val_MinusLogProbMetric: 42.6810 - lr: 1.1111e-04 - 30s/epoch - 152ms/step
Epoch 202/1000
2023-10-26 16:00:18.996 
Epoch 202/1000 
	 loss: 41.1199, MinusLogProbMetric: 41.1199, val_loss: 41.2174, val_MinusLogProbMetric: 41.2174

Epoch 202: val_loss improved from 41.44420 to 41.21741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 41.1199 - MinusLogProbMetric: 41.1199 - val_loss: 41.2174 - val_MinusLogProbMetric: 41.2174 - lr: 1.1111e-04 - 30s/epoch - 154ms/step
Epoch 203/1000
2023-10-26 16:00:51.011 
Epoch 203/1000 
	 loss: 40.9619, MinusLogProbMetric: 40.9619, val_loss: 41.5794, val_MinusLogProbMetric: 41.5794

Epoch 203: val_loss did not improve from 41.21741
196/196 - 31s - loss: 40.9619 - MinusLogProbMetric: 40.9619 - val_loss: 41.5794 - val_MinusLogProbMetric: 41.5794 - lr: 1.1111e-04 - 31s/epoch - 160ms/step
Epoch 204/1000
2023-10-26 16:01:25.544 
Epoch 204/1000 
	 loss: 40.9301, MinusLogProbMetric: 40.9301, val_loss: 41.7827, val_MinusLogProbMetric: 41.7827

Epoch 204: val_loss did not improve from 41.21741
196/196 - 35s - loss: 40.9301 - MinusLogProbMetric: 40.9301 - val_loss: 41.7827 - val_MinusLogProbMetric: 41.7827 - lr: 1.1111e-04 - 35s/epoch - 176ms/step
Epoch 205/1000
2023-10-26 16:01:55.281 
Epoch 205/1000 
	 loss: 40.8950, MinusLogProbMetric: 40.8950, val_loss: 41.1501, val_MinusLogProbMetric: 41.1501

Epoch 205: val_loss improved from 41.21741 to 41.15009, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 30s - loss: 40.8950 - MinusLogProbMetric: 40.8950 - val_loss: 41.1501 - val_MinusLogProbMetric: 41.1501 - lr: 1.1111e-04 - 30s/epoch - 154ms/step
Epoch 206/1000
2023-10-26 16:02:23.700 
Epoch 206/1000 
	 loss: 40.7353, MinusLogProbMetric: 40.7353, val_loss: 40.9896, val_MinusLogProbMetric: 40.9896

Epoch 206: val_loss improved from 41.15009 to 40.98958, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 28s - loss: 40.7353 - MinusLogProbMetric: 40.7353 - val_loss: 40.9896 - val_MinusLogProbMetric: 40.9896 - lr: 1.1111e-04 - 28s/epoch - 145ms/step
Epoch 207/1000
2023-10-26 16:02:51.836 
Epoch 207/1000 
	 loss: 42.0273, MinusLogProbMetric: 42.0273, val_loss: 43.3041, val_MinusLogProbMetric: 43.3041

Epoch 207: val_loss did not improve from 40.98958
196/196 - 28s - loss: 42.0273 - MinusLogProbMetric: 42.0273 - val_loss: 43.3041 - val_MinusLogProbMetric: 43.3041 - lr: 1.1111e-04 - 28s/epoch - 141ms/step
Epoch 208/1000
2023-10-26 16:03:22.003 
Epoch 208/1000 
	 loss: 42.2077, MinusLogProbMetric: 42.2077, val_loss: 42.0097, val_MinusLogProbMetric: 42.0097

Epoch 208: val_loss did not improve from 40.98958
196/196 - 30s - loss: 42.2077 - MinusLogProbMetric: 42.2077 - val_loss: 42.0097 - val_MinusLogProbMetric: 42.0097 - lr: 1.1111e-04 - 30s/epoch - 154ms/step
Epoch 209/1000
2023-10-26 16:03:54.651 
Epoch 209/1000 
	 loss: 42.3438, MinusLogProbMetric: 42.3438, val_loss: 42.5583, val_MinusLogProbMetric: 42.5583

Epoch 209: val_loss did not improve from 40.98958
196/196 - 33s - loss: 42.3438 - MinusLogProbMetric: 42.3438 - val_loss: 42.5583 - val_MinusLogProbMetric: 42.5583 - lr: 1.1111e-04 - 33s/epoch - 167ms/step
Epoch 210/1000
2023-10-26 16:04:25.684 
Epoch 210/1000 
	 loss: 41.7055, MinusLogProbMetric: 41.7055, val_loss: 42.0019, val_MinusLogProbMetric: 42.0019

Epoch 210: val_loss did not improve from 40.98958
196/196 - 31s - loss: 41.7055 - MinusLogProbMetric: 41.7055 - val_loss: 42.0019 - val_MinusLogProbMetric: 42.0019 - lr: 1.1111e-04 - 31s/epoch - 158ms/step
Epoch 211/1000
2023-10-26 16:04:53.685 
Epoch 211/1000 
	 loss: 41.0101, MinusLogProbMetric: 41.0101, val_loss: 40.8604, val_MinusLogProbMetric: 40.8604

Epoch 211: val_loss improved from 40.98958 to 40.86039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 28s - loss: 41.0101 - MinusLogProbMetric: 41.0101 - val_loss: 40.8604 - val_MinusLogProbMetric: 40.8604 - lr: 1.1111e-04 - 28s/epoch - 145ms/step
Epoch 212/1000
2023-10-26 16:05:23.107 
Epoch 212/1000 
	 loss: 41.1922, MinusLogProbMetric: 41.1922, val_loss: 40.7185, val_MinusLogProbMetric: 40.7185

Epoch 212: val_loss improved from 40.86039 to 40.71853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 29s - loss: 41.1922 - MinusLogProbMetric: 41.1922 - val_loss: 40.7185 - val_MinusLogProbMetric: 40.7185 - lr: 1.1111e-04 - 29s/epoch - 150ms/step
Epoch 213/1000
2023-10-26 16:05:53.923 
Epoch 213/1000 
	 loss: 40.3727, MinusLogProbMetric: 40.3727, val_loss: 40.7065, val_MinusLogProbMetric: 40.7065

Epoch 213: val_loss improved from 40.71853 to 40.70649, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 31s - loss: 40.3727 - MinusLogProbMetric: 40.3727 - val_loss: 40.7065 - val_MinusLogProbMetric: 40.7065 - lr: 1.1111e-04 - 31s/epoch - 158ms/step
Epoch 214/1000
2023-10-26 16:06:28.051 
Epoch 214/1000 
	 loss: 40.4331, MinusLogProbMetric: 40.4331, val_loss: 40.6271, val_MinusLogProbMetric: 40.6271

Epoch 214: val_loss improved from 40.70649 to 40.62712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 34s - loss: 40.4331 - MinusLogProbMetric: 40.4331 - val_loss: 40.6271 - val_MinusLogProbMetric: 40.6271 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 215/1000
2023-10-26 16:07:04.303 
Epoch 215/1000 
	 loss: 40.2854, MinusLogProbMetric: 40.2854, val_loss: 41.1104, val_MinusLogProbMetric: 41.1104

Epoch 215: val_loss did not improve from 40.62712
196/196 - 36s - loss: 40.2854 - MinusLogProbMetric: 40.2854 - val_loss: 41.1104 - val_MinusLogProbMetric: 41.1104 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 216/1000
2023-10-26 16:07:37.235 
Epoch 216/1000 
	 loss: 40.1695, MinusLogProbMetric: 40.1695, val_loss: 40.6052, val_MinusLogProbMetric: 40.6052

Epoch 216: val_loss improved from 40.62712 to 40.60520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 40.1695 - MinusLogProbMetric: 40.1695 - val_loss: 40.6052 - val_MinusLogProbMetric: 40.6052 - lr: 1.1111e-04 - 33s/epoch - 171ms/step
Epoch 217/1000
2023-10-26 16:08:10.279 
Epoch 217/1000 
	 loss: 40.2766, MinusLogProbMetric: 40.2766, val_loss: 40.6124, val_MinusLogProbMetric: 40.6124

Epoch 217: val_loss did not improve from 40.60520
196/196 - 32s - loss: 40.2766 - MinusLogProbMetric: 40.2766 - val_loss: 40.6124 - val_MinusLogProbMetric: 40.6124 - lr: 1.1111e-04 - 32s/epoch - 166ms/step
Epoch 218/1000
2023-10-26 16:08:45.423 
Epoch 218/1000 
	 loss: 40.0959, MinusLogProbMetric: 40.0959, val_loss: 40.2438, val_MinusLogProbMetric: 40.2438

Epoch 218: val_loss improved from 40.60520 to 40.24379, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 40.0959 - MinusLogProbMetric: 40.0959 - val_loss: 40.2438 - val_MinusLogProbMetric: 40.2438 - lr: 1.1111e-04 - 36s/epoch - 183ms/step
Epoch 219/1000
2023-10-26 16:09:21.631 
Epoch 219/1000 
	 loss: 40.2174, MinusLogProbMetric: 40.2174, val_loss: 40.2520, val_MinusLogProbMetric: 40.2520

Epoch 219: val_loss did not improve from 40.24379
196/196 - 36s - loss: 40.2174 - MinusLogProbMetric: 40.2174 - val_loss: 40.2520 - val_MinusLogProbMetric: 40.2520 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 220/1000
2023-10-26 16:09:57.339 
Epoch 220/1000 
	 loss: 39.9325, MinusLogProbMetric: 39.9325, val_loss: 40.1997, val_MinusLogProbMetric: 40.1997

Epoch 220: val_loss improved from 40.24379 to 40.19968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 39.9325 - MinusLogProbMetric: 39.9325 - val_loss: 40.1997 - val_MinusLogProbMetric: 40.1997 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 221/1000
2023-10-26 16:10:31.564 
Epoch 221/1000 
	 loss: 39.8357, MinusLogProbMetric: 39.8357, val_loss: 40.3406, val_MinusLogProbMetric: 40.3406

Epoch 221: val_loss did not improve from 40.19968
196/196 - 34s - loss: 39.8357 - MinusLogProbMetric: 39.8357 - val_loss: 40.3406 - val_MinusLogProbMetric: 40.3406 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 222/1000
2023-10-26 16:11:05.197 
Epoch 222/1000 
	 loss: 39.8156, MinusLogProbMetric: 39.8156, val_loss: 40.2861, val_MinusLogProbMetric: 40.2861

Epoch 222: val_loss did not improve from 40.19968
196/196 - 34s - loss: 39.8156 - MinusLogProbMetric: 39.8156 - val_loss: 40.2861 - val_MinusLogProbMetric: 40.2861 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 223/1000
2023-10-26 16:11:40.945 
Epoch 223/1000 
	 loss: 39.8049, MinusLogProbMetric: 39.8049, val_loss: 40.1927, val_MinusLogProbMetric: 40.1927

Epoch 223: val_loss improved from 40.19968 to 40.19267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 39.8049 - MinusLogProbMetric: 39.8049 - val_loss: 40.1927 - val_MinusLogProbMetric: 40.1927 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 224/1000
2023-10-26 16:12:17.070 
Epoch 224/1000 
	 loss: 39.8586, MinusLogProbMetric: 39.8586, val_loss: 40.1275, val_MinusLogProbMetric: 40.1275

Epoch 224: val_loss improved from 40.19267 to 40.12750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 39.8586 - MinusLogProbMetric: 39.8586 - val_loss: 40.1275 - val_MinusLogProbMetric: 40.1275 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 225/1000
2023-10-26 16:12:52.671 
Epoch 225/1000 
	 loss: 40.3011, MinusLogProbMetric: 40.3011, val_loss: 40.1150, val_MinusLogProbMetric: 40.1150

Epoch 225: val_loss improved from 40.12750 to 40.11499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 40.3011 - MinusLogProbMetric: 40.3011 - val_loss: 40.1150 - val_MinusLogProbMetric: 40.1150 - lr: 1.1111e-04 - 35s/epoch - 181ms/step
Epoch 226/1000
2023-10-26 16:13:28.399 
Epoch 226/1000 
	 loss: 39.6333, MinusLogProbMetric: 39.6333, val_loss: 39.8340, val_MinusLogProbMetric: 39.8340

Epoch 226: val_loss improved from 40.11499 to 39.83395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 39.6333 - MinusLogProbMetric: 39.6333 - val_loss: 39.8340 - val_MinusLogProbMetric: 39.8340 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 227/1000
2023-10-26 16:14:03.721 
Epoch 227/1000 
	 loss: 40.0713, MinusLogProbMetric: 40.0713, val_loss: 40.5472, val_MinusLogProbMetric: 40.5472

Epoch 227: val_loss did not improve from 39.83395
196/196 - 35s - loss: 40.0713 - MinusLogProbMetric: 40.0713 - val_loss: 40.5472 - val_MinusLogProbMetric: 40.5472 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 228/1000
2023-10-26 16:14:37.367 
Epoch 228/1000 
	 loss: 39.6406, MinusLogProbMetric: 39.6406, val_loss: 40.0339, val_MinusLogProbMetric: 40.0339

Epoch 228: val_loss did not improve from 39.83395
196/196 - 34s - loss: 39.6406 - MinusLogProbMetric: 39.6406 - val_loss: 40.0339 - val_MinusLogProbMetric: 40.0339 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 229/1000
2023-10-26 16:15:13.596 
Epoch 229/1000 
	 loss: 39.5915, MinusLogProbMetric: 39.5915, val_loss: 39.8099, val_MinusLogProbMetric: 39.8099

Epoch 229: val_loss improved from 39.83395 to 39.80991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 37s - loss: 39.5915 - MinusLogProbMetric: 39.5915 - val_loss: 39.8099 - val_MinusLogProbMetric: 39.8099 - lr: 1.1111e-04 - 37s/epoch - 188ms/step
Epoch 230/1000
2023-10-26 16:15:49.616 
Epoch 230/1000 
	 loss: 39.5388, MinusLogProbMetric: 39.5388, val_loss: 39.5796, val_MinusLogProbMetric: 39.5796

Epoch 230: val_loss improved from 39.80991 to 39.57963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 39.5388 - MinusLogProbMetric: 39.5388 - val_loss: 39.5796 - val_MinusLogProbMetric: 39.5796 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 231/1000
2023-10-26 16:16:25.829 
Epoch 231/1000 
	 loss: 39.5845, MinusLogProbMetric: 39.5845, val_loss: 41.6412, val_MinusLogProbMetric: 41.6412

Epoch 231: val_loss did not improve from 39.57963
196/196 - 36s - loss: 39.5845 - MinusLogProbMetric: 39.5845 - val_loss: 41.6412 - val_MinusLogProbMetric: 41.6412 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 232/1000
2023-10-26 16:16:59.763 
Epoch 232/1000 
	 loss: 39.9592, MinusLogProbMetric: 39.9592, val_loss: 39.6925, val_MinusLogProbMetric: 39.6925

Epoch 232: val_loss did not improve from 39.57963
196/196 - 34s - loss: 39.9592 - MinusLogProbMetric: 39.9592 - val_loss: 39.6925 - val_MinusLogProbMetric: 39.6925 - lr: 1.1111e-04 - 34s/epoch - 173ms/step
Epoch 233/1000
2023-10-26 16:17:32.749 
Epoch 233/1000 
	 loss: 39.3443, MinusLogProbMetric: 39.3443, val_loss: 39.7416, val_MinusLogProbMetric: 39.7416

Epoch 233: val_loss did not improve from 39.57963
196/196 - 33s - loss: 39.3443 - MinusLogProbMetric: 39.3443 - val_loss: 39.7416 - val_MinusLogProbMetric: 39.7416 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 234/1000
2023-10-26 16:18:08.436 
Epoch 234/1000 
	 loss: 39.3560, MinusLogProbMetric: 39.3560, val_loss: 39.7287, val_MinusLogProbMetric: 39.7287

Epoch 234: val_loss did not improve from 39.57963
196/196 - 36s - loss: 39.3560 - MinusLogProbMetric: 39.3560 - val_loss: 39.7287 - val_MinusLogProbMetric: 39.7287 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 235/1000
2023-10-26 16:18:43.326 
Epoch 235/1000 
	 loss: 39.2855, MinusLogProbMetric: 39.2855, val_loss: 39.4150, val_MinusLogProbMetric: 39.4150

Epoch 235: val_loss improved from 39.57963 to 39.41500, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 35s - loss: 39.2855 - MinusLogProbMetric: 39.2855 - val_loss: 39.4150 - val_MinusLogProbMetric: 39.4150 - lr: 1.1111e-04 - 35s/epoch - 181ms/step
Epoch 236/1000
2023-10-26 16:19:19.269 
Epoch 236/1000 
	 loss: 40.6349, MinusLogProbMetric: 40.6349, val_loss: 40.4404, val_MinusLogProbMetric: 40.4404

Epoch 236: val_loss did not improve from 39.41500
196/196 - 35s - loss: 40.6349 - MinusLogProbMetric: 40.6349 - val_loss: 40.4404 - val_MinusLogProbMetric: 40.4404 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 237/1000
2023-10-26 16:19:54.407 
Epoch 237/1000 
	 loss: 39.4094, MinusLogProbMetric: 39.4094, val_loss: 39.5177, val_MinusLogProbMetric: 39.5177

Epoch 237: val_loss did not improve from 39.41500
196/196 - 35s - loss: 39.4094 - MinusLogProbMetric: 39.4094 - val_loss: 39.5177 - val_MinusLogProbMetric: 39.5177 - lr: 1.1111e-04 - 35s/epoch - 179ms/step
Epoch 238/1000
2023-10-26 16:20:28.677 
Epoch 238/1000 
	 loss: 39.1036, MinusLogProbMetric: 39.1036, val_loss: 40.0052, val_MinusLogProbMetric: 40.0052

Epoch 238: val_loss did not improve from 39.41500
196/196 - 34s - loss: 39.1036 - MinusLogProbMetric: 39.1036 - val_loss: 40.0052 - val_MinusLogProbMetric: 40.0052 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 239/1000
2023-10-26 16:21:04.024 
Epoch 239/1000 
	 loss: 39.1813, MinusLogProbMetric: 39.1813, val_loss: 39.7606, val_MinusLogProbMetric: 39.7606

Epoch 239: val_loss did not improve from 39.41500
196/196 - 35s - loss: 39.1813 - MinusLogProbMetric: 39.1813 - val_loss: 39.7606 - val_MinusLogProbMetric: 39.7606 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 240/1000
2023-10-26 16:21:39.776 
Epoch 240/1000 
	 loss: 39.2376, MinusLogProbMetric: 39.2376, val_loss: 39.2023, val_MinusLogProbMetric: 39.2023

Epoch 240: val_loss improved from 39.41500 to 39.20227, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 39.2376 - MinusLogProbMetric: 39.2376 - val_loss: 39.2023 - val_MinusLogProbMetric: 39.2023 - lr: 1.1111e-04 - 36s/epoch - 185ms/step
Epoch 241/1000
2023-10-26 16:22:15.074 
Epoch 241/1000 
	 loss: 38.9788, MinusLogProbMetric: 38.9788, val_loss: 39.3863, val_MinusLogProbMetric: 39.3863

Epoch 241: val_loss did not improve from 39.20227
196/196 - 35s - loss: 38.9788 - MinusLogProbMetric: 38.9788 - val_loss: 39.3863 - val_MinusLogProbMetric: 39.3863 - lr: 1.1111e-04 - 35s/epoch - 177ms/step
Epoch 242/1000
2023-10-26 16:22:48.771 
Epoch 242/1000 
	 loss: 39.0187, MinusLogProbMetric: 39.0187, val_loss: 39.6575, val_MinusLogProbMetric: 39.6575

Epoch 242: val_loss did not improve from 39.20227
196/196 - 34s - loss: 39.0187 - MinusLogProbMetric: 39.0187 - val_loss: 39.6575 - val_MinusLogProbMetric: 39.6575 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 243/1000
2023-10-26 16:23:23.028 
Epoch 243/1000 
	 loss: 39.1408, MinusLogProbMetric: 39.1408, val_loss: 39.4262, val_MinusLogProbMetric: 39.4262

Epoch 243: val_loss did not improve from 39.20227
196/196 - 34s - loss: 39.1408 - MinusLogProbMetric: 39.1408 - val_loss: 39.4262 - val_MinusLogProbMetric: 39.4262 - lr: 1.1111e-04 - 34s/epoch - 175ms/step
Epoch 244/1000
2023-10-26 16:23:56.534 
Epoch 244/1000 
	 loss: 38.8618, MinusLogProbMetric: 38.8618, val_loss: 39.9965, val_MinusLogProbMetric: 39.9965

Epoch 244: val_loss did not improve from 39.20227
196/196 - 34s - loss: 38.8618 - MinusLogProbMetric: 38.8618 - val_loss: 39.9965 - val_MinusLogProbMetric: 39.9965 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 245/1000
2023-10-26 16:24:30.051 
Epoch 245/1000 
	 loss: 38.8539, MinusLogProbMetric: 38.8539, val_loss: 39.4691, val_MinusLogProbMetric: 39.4691

Epoch 245: val_loss did not improve from 39.20227
196/196 - 34s - loss: 38.8539 - MinusLogProbMetric: 38.8539 - val_loss: 39.4691 - val_MinusLogProbMetric: 39.4691 - lr: 1.1111e-04 - 34s/epoch - 171ms/step
Epoch 246/1000
2023-10-26 16:24:58.490 
Epoch 246/1000 
	 loss: 39.0962, MinusLogProbMetric: 39.0962, val_loss: 40.7410, val_MinusLogProbMetric: 40.7410

Epoch 246: val_loss did not improve from 39.20227
196/196 - 28s - loss: 39.0962 - MinusLogProbMetric: 39.0962 - val_loss: 40.7410 - val_MinusLogProbMetric: 40.7410 - lr: 1.1111e-04 - 28s/epoch - 145ms/step
Epoch 247/1000
2023-10-26 16:25:26.528 
Epoch 247/1000 
	 loss: 38.9176, MinusLogProbMetric: 38.9176, val_loss: 39.3799, val_MinusLogProbMetric: 39.3799

Epoch 247: val_loss did not improve from 39.20227
196/196 - 28s - loss: 38.9176 - MinusLogProbMetric: 38.9176 - val_loss: 39.3799 - val_MinusLogProbMetric: 39.3799 - lr: 1.1111e-04 - 28s/epoch - 143ms/step
Epoch 248/1000
2023-10-26 16:25:57.975 
Epoch 248/1000 
	 loss: 38.7541, MinusLogProbMetric: 38.7541, val_loss: 38.9018, val_MinusLogProbMetric: 38.9018

Epoch 248: val_loss improved from 39.20227 to 38.90177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 32s - loss: 38.7541 - MinusLogProbMetric: 38.7541 - val_loss: 38.9018 - val_MinusLogProbMetric: 38.9018 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 249/1000
2023-10-26 16:26:34.087 
Epoch 249/1000 
	 loss: 38.8463, MinusLogProbMetric: 38.8463, val_loss: 39.0354, val_MinusLogProbMetric: 39.0354

Epoch 249: val_loss did not improve from 38.90177
196/196 - 36s - loss: 38.8463 - MinusLogProbMetric: 38.8463 - val_loss: 39.0354 - val_MinusLogProbMetric: 39.0354 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 250/1000
2023-10-26 16:27:09.003 
Epoch 250/1000 
	 loss: 42.5554, MinusLogProbMetric: 42.5554, val_loss: 41.3723, val_MinusLogProbMetric: 41.3723

Epoch 250: val_loss did not improve from 38.90177
196/196 - 35s - loss: 42.5554 - MinusLogProbMetric: 42.5554 - val_loss: 41.3723 - val_MinusLogProbMetric: 41.3723 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 251/1000
2023-10-26 16:27:42.686 
Epoch 251/1000 
	 loss: 40.4869, MinusLogProbMetric: 40.4869, val_loss: 40.3381, val_MinusLogProbMetric: 40.3381

Epoch 251: val_loss did not improve from 38.90177
196/196 - 34s - loss: 40.4869 - MinusLogProbMetric: 40.4869 - val_loss: 40.3381 - val_MinusLogProbMetric: 40.3381 - lr: 1.1111e-04 - 34s/epoch - 172ms/step
Epoch 252/1000
2023-10-26 16:28:17.958 
Epoch 252/1000 
	 loss: 40.1811, MinusLogProbMetric: 40.1811, val_loss: 41.5811, val_MinusLogProbMetric: 41.5811

Epoch 252: val_loss did not improve from 38.90177
196/196 - 35s - loss: 40.1811 - MinusLogProbMetric: 40.1811 - val_loss: 41.5811 - val_MinusLogProbMetric: 41.5811 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 253/1000
2023-10-26 16:28:53.466 
Epoch 253/1000 
	 loss: 40.1322, MinusLogProbMetric: 40.1322, val_loss: 40.0534, val_MinusLogProbMetric: 40.0534

Epoch 253: val_loss did not improve from 38.90177
196/196 - 36s - loss: 40.1322 - MinusLogProbMetric: 40.1322 - val_loss: 40.0534 - val_MinusLogProbMetric: 40.0534 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 254/1000
2023-10-26 16:29:28.364 
Epoch 254/1000 
	 loss: 39.8731, MinusLogProbMetric: 39.8731, val_loss: 40.3006, val_MinusLogProbMetric: 40.3006

Epoch 254: val_loss did not improve from 38.90177
196/196 - 35s - loss: 39.8731 - MinusLogProbMetric: 39.8731 - val_loss: 40.3006 - val_MinusLogProbMetric: 40.3006 - lr: 1.1111e-04 - 35s/epoch - 178ms/step
Epoch 255/1000
2023-10-26 16:30:00.285 
Epoch 255/1000 
	 loss: 39.7488, MinusLogProbMetric: 39.7488, val_loss: 40.1560, val_MinusLogProbMetric: 40.1560

Epoch 255: val_loss did not improve from 38.90177
196/196 - 32s - loss: 39.7488 - MinusLogProbMetric: 39.7488 - val_loss: 40.1560 - val_MinusLogProbMetric: 40.1560 - lr: 1.1111e-04 - 32s/epoch - 163ms/step
Epoch 256/1000
2023-10-26 16:30:35.945 
Epoch 256/1000 
	 loss: 39.7546, MinusLogProbMetric: 39.7546, val_loss: 40.4106, val_MinusLogProbMetric: 40.4106

Epoch 256: val_loss did not improve from 38.90177
196/196 - 36s - loss: 39.7546 - MinusLogProbMetric: 39.7546 - val_loss: 40.4106 - val_MinusLogProbMetric: 40.4106 - lr: 1.1111e-04 - 36s/epoch - 182ms/step
Epoch 257/1000
2023-10-26 16:31:11.251 
Epoch 257/1000 
	 loss: 39.7504, MinusLogProbMetric: 39.7504, val_loss: 39.7632, val_MinusLogProbMetric: 39.7632

Epoch 257: val_loss did not improve from 38.90177
196/196 - 35s - loss: 39.7504 - MinusLogProbMetric: 39.7504 - val_loss: 39.7632 - val_MinusLogProbMetric: 39.7632 - lr: 1.1111e-04 - 35s/epoch - 180ms/step
Epoch 258/1000
2023-10-26 16:31:45.420 
Epoch 258/1000 
	 loss: 39.5329, MinusLogProbMetric: 39.5329, val_loss: 40.1390, val_MinusLogProbMetric: 40.1390

Epoch 258: val_loss did not improve from 38.90177
196/196 - 34s - loss: 39.5329 - MinusLogProbMetric: 39.5329 - val_loss: 40.1390 - val_MinusLogProbMetric: 40.1390 - lr: 1.1111e-04 - 34s/epoch - 174ms/step
Epoch 259/1000
2023-10-26 16:32:20.928 
Epoch 259/1000 
	 loss: 39.5908, MinusLogProbMetric: 39.5908, val_loss: 39.5546, val_MinusLogProbMetric: 39.5546

Epoch 259: val_loss did not improve from 38.90177
196/196 - 36s - loss: 39.5908 - MinusLogProbMetric: 39.5908 - val_loss: 39.5546 - val_MinusLogProbMetric: 39.5546 - lr: 1.1111e-04 - 36s/epoch - 181ms/step
Epoch 260/1000
2023-10-26 16:32:56.327 
Epoch 260/1000 
	 loss: 38.8198, MinusLogProbMetric: 38.8198, val_loss: 38.6916, val_MinusLogProbMetric: 38.6916

Epoch 260: val_loss improved from 38.90177 to 38.69157, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 36s - loss: 38.8198 - MinusLogProbMetric: 38.8198 - val_loss: 38.6916 - val_MinusLogProbMetric: 38.6916 - lr: 1.1111e-04 - 36s/epoch - 184ms/step
Epoch 261/1000
2023-10-26 16:33:29.408 
Epoch 261/1000 
	 loss: 38.5272, MinusLogProbMetric: 38.5272, val_loss: 38.5455, val_MinusLogProbMetric: 38.5455

Epoch 261: val_loss improved from 38.69157 to 38.54551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_385/weights/best_weights.h5
196/196 - 33s - loss: 38.5272 - MinusLogProbMetric: 38.5272 - val_loss: 38.5455 - val_MinusLogProbMetric: 38.5455 - lr: 1.1111e-04 - 33s/epoch - 168ms/step
Epoch 262/1000
2023-10-26 16:34:06.608 
Epoch 262/1000 
	 loss: 38.3545, MinusLogProbMetric: 38.3545, val_loss: 38.7140, val_MinusLogProbMetric: 38.7140

Epoch 262: val_loss did not improve from 38.54551
196/196 - 37s - loss: 38.3545 - MinusLogProbMetric: 38.3545 - val_loss: 38.7140 - val_MinusLogProbMetric: 38.7140 - lr: 1.1111e-04 - 37s/epoch - 187ms/step
Epoch 263/1000
