2023-09-26 13:22:57.100525: Importing os...
2023-09-26 13:22:57.100592: Importing sys...
2023-09-26 13:22:57.100606: Importing and initializing argparse...
Visible devices: [0]
2023-09-26 13:22:57.117513: Importing timer from timeit...
2023-09-26 13:22:57.118101: Setting env variables for tf import (only device [0] will be available)...
2023-09-26 13:22:57.118158: Importing numpy...
2023-09-26 13:22:57.276729: Importing pandas...
2023-09-26 13:22:57.475750: Importing shutil...
2023-09-26 13:22:57.475779: Importing subprocess...
2023-09-26 13:22:57.475786: Importing tensorflow...
Tensorflow version: 2.12.0
2023-09-26 13:22:59.755625: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-09-26 13:23:00.149631: Importing textwrap...
2023-09-26 13:23:00.149664: Importing timeit...
2023-09-26 13:23:00.149674: Importing traceback...
2023-09-26 13:23:00.149680: Importing typing...
2023-09-26 13:23:00.149691: Setting tf configs...
2023-09-26 13:23:00.486274: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-09-26 13:23:01.809411: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

===========
Generating train data for run 285.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_285/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_285/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_285/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_285
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.5424666   4.160326    8.947518   ...  7.3513613   3.2319064
   2.0238907 ]
 [ 1.223466    3.6614168   7.44358    ...  7.1275983   2.889486
   2.1263318 ]
 [ 2.8113456   3.2582755   7.9323573  ...  7.1118317   2.55147
   1.6773428 ]
 ...
 [ 4.8257422   5.760828    0.99088824 ...  1.1221942   6.224573
   1.3870366 ]
 [ 2.8796694   4.5901284   7.9563813  ...  7.221806    3.399475
   1.8678449 ]
 [ 4.724557    5.899211   -0.68043184 ...  1.4107605   6.846419
   1.3825576 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_10"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  826720    
 r)                                                              
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f928047e440>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f9282866440>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f9282866440>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f92704501c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f92704aa110>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f92704aa680>, <keras.callbacks.ModelCheckpoint object at 0x7f92704aa7d0>, <keras.callbacks.EarlyStopping object at 0x7f92704aa9e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f92704aaa10>, <keras.callbacks.TerminateOnNaN object at 0x7f92704aa740>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_285/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 285/720 with hyperparameters:
timestamp = 2023-09-26 13:23:11.428352
ndims = 32
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.5424666   4.160326    8.947518    1.0939208   7.617727   -0.37824863
  9.755269    4.70193     9.9485855   6.084102    7.916266    0.31372035
  2.6670911   1.177147    2.493954    1.2494209   3.149602    6.587495
  1.3128376   6.9324527   5.4146237   2.7642589   4.1612015   1.0985487
  4.086754    9.263963    3.806251    5.6873355   2.3875675   7.3513613
  3.2319064   2.0238907 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 51: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-26 13:25:16.123 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1888.2954, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 125s - loss: nan - MinusLogProbMetric: 1888.2954 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 125s/epoch - 636ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 285.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_285/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_285/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_285/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_285
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.5424666   4.160326    8.947518   ...  7.3513613   3.2319064
   2.0238907 ]
 [ 1.223466    3.6614168   7.44358    ...  7.1275983   2.889486
   2.1263318 ]
 [ 2.8113456   3.2582755   7.9323573  ...  7.1118317   2.55147
   1.6773428 ]
 ...
 [ 4.8257422   5.760828    0.99088824 ...  1.1221942   6.224573
   1.3870366 ]
 [ 2.8796694   4.5901284   7.9563813  ...  7.221806    3.399475
   1.8678449 ]
 [ 4.724557    5.899211   -0.68043184 ...  1.4107605   6.846419
   1.3825576 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_21"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_22 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f96218ca950>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8f84560940>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8f84560940>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f9621e07f10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f96215a8c40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f96215a91b0>, <keras.callbacks.ModelCheckpoint object at 0x7f96215a9270>, <keras.callbacks.EarlyStopping object at 0x7f96215a94e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f96215a9510>, <keras.callbacks.TerminateOnNaN object at 0x7f96215a9150>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_285/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 285/720 with hyperparameters:
timestamp = 2023-09-26 13:25:25.480175
ndims = 32
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.5424666   4.160326    8.947518    1.0939208   7.617727   -0.37824863
  9.755269    4.70193     9.9485855   6.084102    7.916266    0.31372035
  2.6670911   1.177147    2.493954    1.2494209   3.149602    6.587495
  1.3128376   6.9324527   5.4146237   2.7642589   4.1612015   1.0985487
  4.086754    9.263963    3.806251    5.6873355   2.3875675   7.3513613
  3.2319064   2.0238907 ]
Epoch 1/1000
2023-09-26 13:28:24.964 
Epoch 1/1000 
	 loss: 1113.3613, MinusLogProbMetric: 1113.3613, val_loss: 444.2080, val_MinusLogProbMetric: 444.2080

Epoch 1: val_loss improved from inf to 444.20801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 179s - loss: 1113.3613 - MinusLogProbMetric: 1113.3613 - val_loss: 444.2080 - val_MinusLogProbMetric: 444.2080 - lr: 3.3333e-04 - 179s/epoch - 916ms/step
Epoch 2/1000
2023-09-26 13:29:33.546 
Epoch 2/1000 
	 loss: 431.9600, MinusLogProbMetric: 431.9600, val_loss: 452.8618, val_MinusLogProbMetric: 452.8618

Epoch 2: val_loss did not improve from 444.20801
196/196 - 68s - loss: 431.9600 - MinusLogProbMetric: 431.9600 - val_loss: 452.8618 - val_MinusLogProbMetric: 452.8618 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 3/1000
2023-09-26 13:30:40.651 
Epoch 3/1000 
	 loss: 344.7994, MinusLogProbMetric: 344.7994, val_loss: 305.7759, val_MinusLogProbMetric: 305.7759

Epoch 3: val_loss improved from 444.20801 to 305.77594, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 68s - loss: 344.7994 - MinusLogProbMetric: 344.7994 - val_loss: 305.7759 - val_MinusLogProbMetric: 305.7759 - lr: 3.3333e-04 - 68s/epoch - 347ms/step
Epoch 4/1000
2023-09-26 13:31:48.839 
Epoch 4/1000 
	 loss: 292.5099, MinusLogProbMetric: 292.5099, val_loss: 240.1438, val_MinusLogProbMetric: 240.1438

Epoch 4: val_loss improved from 305.77594 to 240.14380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 68s - loss: 292.5099 - MinusLogProbMetric: 292.5099 - val_loss: 240.1438 - val_MinusLogProbMetric: 240.1438 - lr: 3.3333e-04 - 68s/epoch - 348ms/step
Epoch 5/1000
2023-09-26 13:32:55.480 
Epoch 5/1000 
	 loss: 232.5220, MinusLogProbMetric: 232.5220, val_loss: 206.2823, val_MinusLogProbMetric: 206.2823

Epoch 5: val_loss improved from 240.14380 to 206.28226, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 232.5220 - MinusLogProbMetric: 232.5220 - val_loss: 206.2823 - val_MinusLogProbMetric: 206.2823 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 6/1000
2023-09-26 13:34:03.343 
Epoch 6/1000 
	 loss: 189.0712, MinusLogProbMetric: 189.0712, val_loss: 178.7938, val_MinusLogProbMetric: 178.7938

Epoch 6: val_loss improved from 206.28226 to 178.79382, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 68s - loss: 189.0712 - MinusLogProbMetric: 189.0712 - val_loss: 178.7938 - val_MinusLogProbMetric: 178.7938 - lr: 3.3333e-04 - 68s/epoch - 346ms/step
Epoch 7/1000
2023-09-26 13:35:11.097 
Epoch 7/1000 
	 loss: 165.5067, MinusLogProbMetric: 165.5067, val_loss: 154.0472, val_MinusLogProbMetric: 154.0472

Epoch 7: val_loss improved from 178.79382 to 154.04720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 68s - loss: 165.5067 - MinusLogProbMetric: 165.5067 - val_loss: 154.0472 - val_MinusLogProbMetric: 154.0472 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 8/1000
2023-09-26 13:36:18.308 
Epoch 8/1000 
	 loss: 152.5106, MinusLogProbMetric: 152.5106, val_loss: 144.1214, val_MinusLogProbMetric: 144.1214

Epoch 8: val_loss improved from 154.04720 to 144.12138, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 152.5106 - MinusLogProbMetric: 152.5106 - val_loss: 144.1214 - val_MinusLogProbMetric: 144.1214 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 9/1000
2023-09-26 13:37:25.647 
Epoch 9/1000 
	 loss: 141.7287, MinusLogProbMetric: 141.7287, val_loss: 140.5064, val_MinusLogProbMetric: 140.5064

Epoch 9: val_loss improved from 144.12138 to 140.50642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 141.7287 - MinusLogProbMetric: 141.7287 - val_loss: 140.5064 - val_MinusLogProbMetric: 140.5064 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 10/1000
2023-09-26 13:38:34.593 
Epoch 10/1000 
	 loss: 131.5445, MinusLogProbMetric: 131.5445, val_loss: 124.8559, val_MinusLogProbMetric: 124.8559

Epoch 10: val_loss improved from 140.50642 to 124.85588, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 69s - loss: 131.5445 - MinusLogProbMetric: 131.5445 - val_loss: 124.8559 - val_MinusLogProbMetric: 124.8559 - lr: 3.3333e-04 - 69s/epoch - 353ms/step
Epoch 11/1000
2023-09-26 13:39:43.511 
Epoch 11/1000 
	 loss: 203.9390, MinusLogProbMetric: 203.9390, val_loss: 164.9742, val_MinusLogProbMetric: 164.9742

Epoch 11: val_loss did not improve from 124.85588
196/196 - 68s - loss: 203.9390 - MinusLogProbMetric: 203.9390 - val_loss: 164.9742 - val_MinusLogProbMetric: 164.9742 - lr: 3.3333e-04 - 68s/epoch - 346ms/step
Epoch 12/1000
2023-09-26 13:40:50.639 
Epoch 12/1000 
	 loss: 147.3549, MinusLogProbMetric: 147.3549, val_loss: 134.3382, val_MinusLogProbMetric: 134.3382

Epoch 12: val_loss did not improve from 124.85588
196/196 - 67s - loss: 147.3549 - MinusLogProbMetric: 147.3549 - val_loss: 134.3382 - val_MinusLogProbMetric: 134.3382 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 13/1000
2023-09-26 13:41:57.061 
Epoch 13/1000 
	 loss: 128.6559, MinusLogProbMetric: 128.6559, val_loss: 121.1433, val_MinusLogProbMetric: 121.1433

Epoch 13: val_loss improved from 124.85588 to 121.14327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 128.6559 - MinusLogProbMetric: 128.6559 - val_loss: 121.1433 - val_MinusLogProbMetric: 121.1433 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 14/1000
2023-09-26 13:43:03.855 
Epoch 14/1000 
	 loss: 117.9673, MinusLogProbMetric: 117.9673, val_loss: 113.0278, val_MinusLogProbMetric: 113.0278

Epoch 14: val_loss improved from 121.14327 to 113.02782, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 117.9673 - MinusLogProbMetric: 117.9673 - val_loss: 113.0278 - val_MinusLogProbMetric: 113.0278 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 15/1000
2023-09-26 13:44:10.201 
Epoch 15/1000 
	 loss: 107.1313, MinusLogProbMetric: 107.1313, val_loss: 103.4042, val_MinusLogProbMetric: 103.4042

Epoch 15: val_loss improved from 113.02782 to 103.40424, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 66s - loss: 107.1313 - MinusLogProbMetric: 107.1313 - val_loss: 103.4042 - val_MinusLogProbMetric: 103.4042 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 16/1000
2023-09-26 13:45:16.862 
Epoch 16/1000 
	 loss: 102.0464, MinusLogProbMetric: 102.0464, val_loss: 99.2174, val_MinusLogProbMetric: 99.2174

Epoch 16: val_loss improved from 103.40424 to 99.21742, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 102.0464 - MinusLogProbMetric: 102.0464 - val_loss: 99.2174 - val_MinusLogProbMetric: 99.2174 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 17/1000
2023-09-26 13:46:23.838 
Epoch 17/1000 
	 loss: 97.0965, MinusLogProbMetric: 97.0965, val_loss: 96.1917, val_MinusLogProbMetric: 96.1917

Epoch 17: val_loss improved from 99.21742 to 96.19171, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 97.0965 - MinusLogProbMetric: 97.0965 - val_loss: 96.1917 - val_MinusLogProbMetric: 96.1917 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 18/1000
2023-09-26 13:47:31.402 
Epoch 18/1000 
	 loss: 93.1363, MinusLogProbMetric: 93.1363, val_loss: 91.8338, val_MinusLogProbMetric: 91.8338

Epoch 18: val_loss improved from 96.19171 to 91.83379, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 93.1363 - MinusLogProbMetric: 93.1363 - val_loss: 91.8338 - val_MinusLogProbMetric: 91.8338 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 19/1000
2023-09-26 13:48:38.192 
Epoch 19/1000 
	 loss: 89.5235, MinusLogProbMetric: 89.5235, val_loss: 91.9894, val_MinusLogProbMetric: 91.9894

Epoch 19: val_loss did not improve from 91.83379
196/196 - 66s - loss: 89.5235 - MinusLogProbMetric: 89.5235 - val_loss: 91.9894 - val_MinusLogProbMetric: 91.9894 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 20/1000
2023-09-26 13:49:43.804 
Epoch 20/1000 
	 loss: 112.1255, MinusLogProbMetric: 112.1255, val_loss: 96.0635, val_MinusLogProbMetric: 96.0635

Epoch 20: val_loss did not improve from 91.83379
196/196 - 66s - loss: 112.1255 - MinusLogProbMetric: 112.1255 - val_loss: 96.0635 - val_MinusLogProbMetric: 96.0635 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 21/1000
2023-09-26 13:50:49.422 
Epoch 21/1000 
	 loss: 91.9660, MinusLogProbMetric: 91.9660, val_loss: 89.6761, val_MinusLogProbMetric: 89.6761

Epoch 21: val_loss improved from 91.83379 to 89.67606, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 91.9660 - MinusLogProbMetric: 91.9660 - val_loss: 89.6761 - val_MinusLogProbMetric: 89.6761 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 22/1000
2023-09-26 13:51:56.051 
Epoch 22/1000 
	 loss: 218.9494, MinusLogProbMetric: 218.9494, val_loss: 188.1156, val_MinusLogProbMetric: 188.1156

Epoch 22: val_loss did not improve from 89.67606
196/196 - 66s - loss: 218.9494 - MinusLogProbMetric: 218.9494 - val_loss: 188.1156 - val_MinusLogProbMetric: 188.1156 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 23/1000
2023-09-26 13:53:01.193 
Epoch 23/1000 
	 loss: 156.9201, MinusLogProbMetric: 156.9201, val_loss: 143.0645, val_MinusLogProbMetric: 143.0645

Epoch 23: val_loss did not improve from 89.67606
196/196 - 65s - loss: 156.9201 - MinusLogProbMetric: 156.9201 - val_loss: 143.0645 - val_MinusLogProbMetric: 143.0645 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 24/1000
2023-09-26 13:54:07.357 
Epoch 24/1000 
	 loss: 133.2644, MinusLogProbMetric: 133.2644, val_loss: 121.0683, val_MinusLogProbMetric: 121.0683

Epoch 24: val_loss did not improve from 89.67606
196/196 - 66s - loss: 133.2644 - MinusLogProbMetric: 133.2644 - val_loss: 121.0683 - val_MinusLogProbMetric: 121.0683 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 25/1000
2023-09-26 13:55:13.294 
Epoch 25/1000 
	 loss: 115.9321, MinusLogProbMetric: 115.9321, val_loss: 112.5568, val_MinusLogProbMetric: 112.5568

Epoch 25: val_loss did not improve from 89.67606
196/196 - 66s - loss: 115.9321 - MinusLogProbMetric: 115.9321 - val_loss: 112.5568 - val_MinusLogProbMetric: 112.5568 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 26/1000
2023-09-26 13:56:19.955 
Epoch 26/1000 
	 loss: 111.5098, MinusLogProbMetric: 111.5098, val_loss: 106.5355, val_MinusLogProbMetric: 106.5355

Epoch 26: val_loss did not improve from 89.67606
196/196 - 67s - loss: 111.5098 - MinusLogProbMetric: 111.5098 - val_loss: 106.5355 - val_MinusLogProbMetric: 106.5355 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 27/1000
2023-09-26 13:57:25.918 
Epoch 27/1000 
	 loss: 104.3235, MinusLogProbMetric: 104.3235, val_loss: 104.4063, val_MinusLogProbMetric: 104.4063

Epoch 27: val_loss did not improve from 89.67606
196/196 - 66s - loss: 104.3235 - MinusLogProbMetric: 104.3235 - val_loss: 104.4063 - val_MinusLogProbMetric: 104.4063 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 28/1000
2023-09-26 13:58:31.532 
Epoch 28/1000 
	 loss: 100.0131, MinusLogProbMetric: 100.0131, val_loss: 97.6291, val_MinusLogProbMetric: 97.6291

Epoch 28: val_loss did not improve from 89.67606
196/196 - 66s - loss: 100.0131 - MinusLogProbMetric: 100.0131 - val_loss: 97.6291 - val_MinusLogProbMetric: 97.6291 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 29/1000
2023-09-26 13:59:36.974 
Epoch 29/1000 
	 loss: 96.6415, MinusLogProbMetric: 96.6415, val_loss: 95.6423, val_MinusLogProbMetric: 95.6423

Epoch 29: val_loss did not improve from 89.67606
196/196 - 65s - loss: 96.6415 - MinusLogProbMetric: 96.6415 - val_loss: 95.6423 - val_MinusLogProbMetric: 95.6423 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 30/1000
2023-09-26 14:00:42.639 
Epoch 30/1000 
	 loss: 95.5323, MinusLogProbMetric: 95.5323, val_loss: 94.7776, val_MinusLogProbMetric: 94.7776

Epoch 30: val_loss did not improve from 89.67606
196/196 - 66s - loss: 95.5323 - MinusLogProbMetric: 95.5323 - val_loss: 94.7776 - val_MinusLogProbMetric: 94.7776 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 31/1000
2023-09-26 14:01:48.769 
Epoch 31/1000 
	 loss: 92.6325, MinusLogProbMetric: 92.6325, val_loss: 90.2622, val_MinusLogProbMetric: 90.2622

Epoch 31: val_loss did not improve from 89.67606
196/196 - 66s - loss: 92.6325 - MinusLogProbMetric: 92.6325 - val_loss: 90.2622 - val_MinusLogProbMetric: 90.2622 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 32/1000
2023-09-26 14:02:54.569 
Epoch 32/1000 
	 loss: 102.2214, MinusLogProbMetric: 102.2214, val_loss: 105.7394, val_MinusLogProbMetric: 105.7394

Epoch 32: val_loss did not improve from 89.67606
196/196 - 66s - loss: 102.2214 - MinusLogProbMetric: 102.2214 - val_loss: 105.7394 - val_MinusLogProbMetric: 105.7394 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 33/1000
2023-09-26 14:04:00.078 
Epoch 33/1000 
	 loss: 95.4714, MinusLogProbMetric: 95.4714, val_loss: 88.8257, val_MinusLogProbMetric: 88.8257

Epoch 33: val_loss improved from 89.67606 to 88.82572, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 95.4714 - MinusLogProbMetric: 95.4714 - val_loss: 88.8257 - val_MinusLogProbMetric: 88.8257 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 34/1000
2023-09-26 14:05:06.835 
Epoch 34/1000 
	 loss: 87.2281, MinusLogProbMetric: 87.2281, val_loss: 85.8318, val_MinusLogProbMetric: 85.8318

Epoch 34: val_loss improved from 88.82572 to 85.83177, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 87.2281 - MinusLogProbMetric: 87.2281 - val_loss: 85.8318 - val_MinusLogProbMetric: 85.8318 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 35/1000
2023-09-26 14:06:13.497 
Epoch 35/1000 
	 loss: 84.9684, MinusLogProbMetric: 84.9684, val_loss: 89.1427, val_MinusLogProbMetric: 89.1427

Epoch 35: val_loss did not improve from 85.83177
196/196 - 66s - loss: 84.9684 - MinusLogProbMetric: 84.9684 - val_loss: 89.1427 - val_MinusLogProbMetric: 89.1427 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 36/1000
2023-09-26 14:07:18.910 
Epoch 36/1000 
	 loss: 83.4276, MinusLogProbMetric: 83.4276, val_loss: 82.2224, val_MinusLogProbMetric: 82.2224

Epoch 36: val_loss improved from 85.83177 to 82.22237, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 66s - loss: 83.4276 - MinusLogProbMetric: 83.4276 - val_loss: 82.2224 - val_MinusLogProbMetric: 82.2224 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 37/1000
2023-09-26 14:08:25.832 
Epoch 37/1000 
	 loss: 81.6924, MinusLogProbMetric: 81.6924, val_loss: 80.5286, val_MinusLogProbMetric: 80.5286

Epoch 37: val_loss improved from 82.22237 to 80.52859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 81.6924 - MinusLogProbMetric: 81.6924 - val_loss: 80.5286 - val_MinusLogProbMetric: 80.5286 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 38/1000
2023-09-26 14:09:33.160 
Epoch 38/1000 
	 loss: 79.8401, MinusLogProbMetric: 79.8401, val_loss: 79.4835, val_MinusLogProbMetric: 79.4835

Epoch 38: val_loss improved from 80.52859 to 79.48345, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 79.8401 - MinusLogProbMetric: 79.8401 - val_loss: 79.4835 - val_MinusLogProbMetric: 79.4835 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 39/1000
2023-09-26 14:10:39.801 
Epoch 39/1000 
	 loss: 78.7241, MinusLogProbMetric: 78.7241, val_loss: 78.0215, val_MinusLogProbMetric: 78.0215

Epoch 39: val_loss improved from 79.48345 to 78.02155, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 78.7241 - MinusLogProbMetric: 78.7241 - val_loss: 78.0215 - val_MinusLogProbMetric: 78.0215 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 40/1000
2023-09-26 14:11:46.613 
Epoch 40/1000 
	 loss: 78.1170, MinusLogProbMetric: 78.1170, val_loss: 77.0066, val_MinusLogProbMetric: 77.0066

Epoch 40: val_loss improved from 78.02155 to 77.00656, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 78.1170 - MinusLogProbMetric: 78.1170 - val_loss: 77.0066 - val_MinusLogProbMetric: 77.0066 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 41/1000
2023-09-26 14:12:52.904 
Epoch 41/1000 
	 loss: 79.3938, MinusLogProbMetric: 79.3938, val_loss: 77.8382, val_MinusLogProbMetric: 77.8382

Epoch 41: val_loss did not improve from 77.00656
196/196 - 65s - loss: 79.3938 - MinusLogProbMetric: 79.3938 - val_loss: 77.8382 - val_MinusLogProbMetric: 77.8382 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 42/1000
2023-09-26 14:13:59.810 
Epoch 42/1000 
	 loss: 76.7516, MinusLogProbMetric: 76.7516, val_loss: 75.7463, val_MinusLogProbMetric: 75.7463

Epoch 42: val_loss improved from 77.00656 to 75.74632, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 68s - loss: 76.7516 - MinusLogProbMetric: 76.7516 - val_loss: 75.7463 - val_MinusLogProbMetric: 75.7463 - lr: 3.3333e-04 - 68s/epoch - 347ms/step
Epoch 43/1000
2023-09-26 14:15:06.460 
Epoch 43/1000 
	 loss: 75.5307, MinusLogProbMetric: 75.5307, val_loss: 75.4089, val_MinusLogProbMetric: 75.4089

Epoch 43: val_loss improved from 75.74632 to 75.40894, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 75.5307 - MinusLogProbMetric: 75.5307 - val_loss: 75.4089 - val_MinusLogProbMetric: 75.4089 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 44/1000
2023-09-26 14:16:12.991 
Epoch 44/1000 
	 loss: 77.3968, MinusLogProbMetric: 77.3968, val_loss: 74.2363, val_MinusLogProbMetric: 74.2363

Epoch 44: val_loss improved from 75.40894 to 74.23627, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 66s - loss: 77.3968 - MinusLogProbMetric: 77.3968 - val_loss: 74.2363 - val_MinusLogProbMetric: 74.2363 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 45/1000
2023-09-26 14:17:19.748 
Epoch 45/1000 
	 loss: 73.8009, MinusLogProbMetric: 73.8009, val_loss: 73.6214, val_MinusLogProbMetric: 73.6214

Epoch 45: val_loss improved from 74.23627 to 73.62136, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 73.8009 - MinusLogProbMetric: 73.8009 - val_loss: 73.6214 - val_MinusLogProbMetric: 73.6214 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 46/1000
2023-09-26 14:18:25.838 
Epoch 46/1000 
	 loss: 72.8971, MinusLogProbMetric: 72.8971, val_loss: 73.1145, val_MinusLogProbMetric: 73.1145

Epoch 46: val_loss improved from 73.62136 to 73.11453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 66s - loss: 72.8971 - MinusLogProbMetric: 72.8971 - val_loss: 73.1145 - val_MinusLogProbMetric: 73.1145 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 47/1000
2023-09-26 14:19:31.978 
Epoch 47/1000 
	 loss: 72.8902, MinusLogProbMetric: 72.8902, val_loss: 72.6558, val_MinusLogProbMetric: 72.6558

Epoch 47: val_loss improved from 73.11453 to 72.65580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 66s - loss: 72.8902 - MinusLogProbMetric: 72.8902 - val_loss: 72.6558 - val_MinusLogProbMetric: 72.6558 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 48/1000
2023-09-26 14:20:36.586 
Epoch 48/1000 
	 loss: 79.3342, MinusLogProbMetric: 79.3342, val_loss: 83.9813, val_MinusLogProbMetric: 83.9813

Epoch 48: val_loss did not improve from 72.65580
196/196 - 64s - loss: 79.3342 - MinusLogProbMetric: 79.3342 - val_loss: 83.9813 - val_MinusLogProbMetric: 83.9813 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 49/1000
2023-09-26 14:21:40.188 
Epoch 49/1000 
	 loss: 75.2388, MinusLogProbMetric: 75.2388, val_loss: 73.2578, val_MinusLogProbMetric: 73.2578

Epoch 49: val_loss did not improve from 72.65580
196/196 - 64s - loss: 75.2388 - MinusLogProbMetric: 75.2388 - val_loss: 73.2578 - val_MinusLogProbMetric: 73.2578 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 50/1000
2023-09-26 14:22:44.828 
Epoch 50/1000 
	 loss: 73.5354, MinusLogProbMetric: 73.5354, val_loss: 73.1930, val_MinusLogProbMetric: 73.1930

Epoch 50: val_loss did not improve from 72.65580
196/196 - 65s - loss: 73.5354 - MinusLogProbMetric: 73.5354 - val_loss: 73.1930 - val_MinusLogProbMetric: 73.1930 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 51/1000
2023-09-26 14:23:50.685 
Epoch 51/1000 
	 loss: 72.5442, MinusLogProbMetric: 72.5442, val_loss: 71.7921, val_MinusLogProbMetric: 71.7921

Epoch 51: val_loss improved from 72.65580 to 71.79214, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 67s - loss: 72.5442 - MinusLogProbMetric: 72.5442 - val_loss: 71.7921 - val_MinusLogProbMetric: 71.7921 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 52/1000
2023-09-26 14:24:56.224 
Epoch 52/1000 
	 loss: 103.3041, MinusLogProbMetric: 103.3041, val_loss: 151.7946, val_MinusLogProbMetric: 151.7946

Epoch 52: val_loss did not improve from 71.79214
196/196 - 65s - loss: 103.3041 - MinusLogProbMetric: 103.3041 - val_loss: 151.7946 - val_MinusLogProbMetric: 151.7946 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 53/1000
2023-09-26 14:25:59.671 
Epoch 53/1000 
	 loss: 112.9279, MinusLogProbMetric: 112.9279, val_loss: 95.6734, val_MinusLogProbMetric: 95.6734

Epoch 53: val_loss did not improve from 71.79214
196/196 - 63s - loss: 112.9279 - MinusLogProbMetric: 112.9279 - val_loss: 95.6734 - val_MinusLogProbMetric: 95.6734 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 54/1000
2023-09-26 14:27:04.320 
Epoch 54/1000 
	 loss: 107.6777, MinusLogProbMetric: 107.6777, val_loss: 105.8365, val_MinusLogProbMetric: 105.8365

Epoch 54: val_loss did not improve from 71.79214
196/196 - 65s - loss: 107.6777 - MinusLogProbMetric: 107.6777 - val_loss: 105.8365 - val_MinusLogProbMetric: 105.8365 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 55/1000
2023-09-26 14:28:08.119 
Epoch 55/1000 
	 loss: 102.6590, MinusLogProbMetric: 102.6590, val_loss: 95.3746, val_MinusLogProbMetric: 95.3746

Epoch 55: val_loss did not improve from 71.79214
196/196 - 64s - loss: 102.6590 - MinusLogProbMetric: 102.6590 - val_loss: 95.3746 - val_MinusLogProbMetric: 95.3746 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 56/1000
2023-09-26 14:29:12.120 
Epoch 56/1000 
	 loss: 92.5332, MinusLogProbMetric: 92.5332, val_loss: 91.0242, val_MinusLogProbMetric: 91.0242

Epoch 56: val_loss did not improve from 71.79214
196/196 - 64s - loss: 92.5332 - MinusLogProbMetric: 92.5332 - val_loss: 91.0242 - val_MinusLogProbMetric: 91.0242 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 57/1000
2023-09-26 14:30:17.210 
Epoch 57/1000 
	 loss: 84.9081, MinusLogProbMetric: 84.9081, val_loss: 81.9740, val_MinusLogProbMetric: 81.9740

Epoch 57: val_loss did not improve from 71.79214
196/196 - 65s - loss: 84.9081 - MinusLogProbMetric: 84.9081 - val_loss: 81.9740 - val_MinusLogProbMetric: 81.9740 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 58/1000
2023-09-26 14:31:21.746 
Epoch 58/1000 
	 loss: 81.8418, MinusLogProbMetric: 81.8418, val_loss: 78.9908, val_MinusLogProbMetric: 78.9908

Epoch 58: val_loss did not improve from 71.79214
196/196 - 65s - loss: 81.8418 - MinusLogProbMetric: 81.8418 - val_loss: 78.9908 - val_MinusLogProbMetric: 78.9908 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 59/1000
2023-09-26 14:32:25.724 
Epoch 59/1000 
	 loss: 88.3064, MinusLogProbMetric: 88.3064, val_loss: 78.4408, val_MinusLogProbMetric: 78.4408

Epoch 59: val_loss did not improve from 71.79214
196/196 - 64s - loss: 88.3064 - MinusLogProbMetric: 88.3064 - val_loss: 78.4408 - val_MinusLogProbMetric: 78.4408 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 60/1000
2023-09-26 14:33:30.766 
Epoch 60/1000 
	 loss: 76.9753, MinusLogProbMetric: 76.9753, val_loss: 75.9522, val_MinusLogProbMetric: 75.9522

Epoch 60: val_loss did not improve from 71.79214
196/196 - 65s - loss: 76.9753 - MinusLogProbMetric: 76.9753 - val_loss: 75.9522 - val_MinusLogProbMetric: 75.9522 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 61/1000
2023-09-26 14:34:35.424 
Epoch 61/1000 
	 loss: 74.4838, MinusLogProbMetric: 74.4838, val_loss: 74.1967, val_MinusLogProbMetric: 74.1967

Epoch 61: val_loss did not improve from 71.79214
196/196 - 65s - loss: 74.4838 - MinusLogProbMetric: 74.4838 - val_loss: 74.1967 - val_MinusLogProbMetric: 74.1967 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 62/1000
2023-09-26 14:35:39.965 
Epoch 62/1000 
	 loss: 73.0095, MinusLogProbMetric: 73.0095, val_loss: 72.8118, val_MinusLogProbMetric: 72.8118

Epoch 62: val_loss did not improve from 71.79214
196/196 - 65s - loss: 73.0095 - MinusLogProbMetric: 73.0095 - val_loss: 72.8118 - val_MinusLogProbMetric: 72.8118 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 63/1000
2023-09-26 14:36:44.445 
Epoch 63/1000 
	 loss: 71.7293, MinusLogProbMetric: 71.7293, val_loss: 70.7086, val_MinusLogProbMetric: 70.7086

Epoch 63: val_loss improved from 71.79214 to 70.70860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 71.7293 - MinusLogProbMetric: 71.7293 - val_loss: 70.7086 - val_MinusLogProbMetric: 70.7086 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 64/1000
2023-09-26 14:37:49.728 
Epoch 64/1000 
	 loss: 70.5069, MinusLogProbMetric: 70.5069, val_loss: 70.7628, val_MinusLogProbMetric: 70.7628

Epoch 64: val_loss did not improve from 70.70860
196/196 - 64s - loss: 70.5069 - MinusLogProbMetric: 70.5069 - val_loss: 70.7628 - val_MinusLogProbMetric: 70.7628 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 65/1000
2023-09-26 14:38:53.618 
Epoch 65/1000 
	 loss: 69.6065, MinusLogProbMetric: 69.6065, val_loss: 69.9032, val_MinusLogProbMetric: 69.9032

Epoch 65: val_loss improved from 70.70860 to 69.90321, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 69.6065 - MinusLogProbMetric: 69.6065 - val_loss: 69.9032 - val_MinusLogProbMetric: 69.9032 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 66/1000
2023-09-26 14:39:58.412 
Epoch 66/1000 
	 loss: 73.4790, MinusLogProbMetric: 73.4790, val_loss: 72.4956, val_MinusLogProbMetric: 72.4956

Epoch 66: val_loss did not improve from 69.90321
196/196 - 64s - loss: 73.4790 - MinusLogProbMetric: 73.4790 - val_loss: 72.4956 - val_MinusLogProbMetric: 72.4956 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 67/1000
2023-09-26 14:41:02.814 
Epoch 67/1000 
	 loss: 70.0719, MinusLogProbMetric: 70.0719, val_loss: 67.6527, val_MinusLogProbMetric: 67.6527

Epoch 67: val_loss improved from 69.90321 to 67.65275, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 70.0719 - MinusLogProbMetric: 70.0719 - val_loss: 67.6527 - val_MinusLogProbMetric: 67.6527 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 68/1000
2023-09-26 14:42:08.204 
Epoch 68/1000 
	 loss: 67.9129, MinusLogProbMetric: 67.9129, val_loss: 69.2541, val_MinusLogProbMetric: 69.2541

Epoch 68: val_loss did not improve from 67.65275
196/196 - 64s - loss: 67.9129 - MinusLogProbMetric: 67.9129 - val_loss: 69.2541 - val_MinusLogProbMetric: 69.2541 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 69/1000
2023-09-26 14:43:12.234 
Epoch 69/1000 
	 loss: 67.1236, MinusLogProbMetric: 67.1236, val_loss: 66.5535, val_MinusLogProbMetric: 66.5535

Epoch 69: val_loss improved from 67.65275 to 66.55352, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 67.1236 - MinusLogProbMetric: 67.1236 - val_loss: 66.5535 - val_MinusLogProbMetric: 66.5535 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 70/1000
2023-09-26 14:44:17.094 
Epoch 70/1000 
	 loss: 66.6603, MinusLogProbMetric: 66.6603, val_loss: 66.0831, val_MinusLogProbMetric: 66.0831

Epoch 70: val_loss improved from 66.55352 to 66.08313, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 66.6603 - MinusLogProbMetric: 66.6603 - val_loss: 66.0831 - val_MinusLogProbMetric: 66.0831 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 71/1000
2023-09-26 14:45:22.459 
Epoch 71/1000 
	 loss: 66.1161, MinusLogProbMetric: 66.1161, val_loss: 66.7245, val_MinusLogProbMetric: 66.7245

Epoch 71: val_loss did not improve from 66.08313
196/196 - 64s - loss: 66.1161 - MinusLogProbMetric: 66.1161 - val_loss: 66.7245 - val_MinusLogProbMetric: 66.7245 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 72/1000
2023-09-26 14:46:26.812 
Epoch 72/1000 
	 loss: 65.8440, MinusLogProbMetric: 65.8440, val_loss: 65.4863, val_MinusLogProbMetric: 65.4863

Epoch 72: val_loss improved from 66.08313 to 65.48631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 65.8440 - MinusLogProbMetric: 65.8440 - val_loss: 65.4863 - val_MinusLogProbMetric: 65.4863 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 73/1000
2023-09-26 14:47:31.763 
Epoch 73/1000 
	 loss: 65.2439, MinusLogProbMetric: 65.2439, val_loss: 64.8715, val_MinusLogProbMetric: 64.8715

Epoch 73: val_loss improved from 65.48631 to 64.87148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 65.2439 - MinusLogProbMetric: 65.2439 - val_loss: 64.8715 - val_MinusLogProbMetric: 64.8715 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 74/1000
2023-09-26 14:48:36.677 
Epoch 74/1000 
	 loss: 64.8396, MinusLogProbMetric: 64.8396, val_loss: 64.8684, val_MinusLogProbMetric: 64.8684

Epoch 74: val_loss improved from 64.87148 to 64.86838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 64.8396 - MinusLogProbMetric: 64.8396 - val_loss: 64.8684 - val_MinusLogProbMetric: 64.8684 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 75/1000
2023-09-26 14:49:41.712 
Epoch 75/1000 
	 loss: 65.0177, MinusLogProbMetric: 65.0177, val_loss: 64.9166, val_MinusLogProbMetric: 64.9166

Epoch 75: val_loss did not improve from 64.86838
196/196 - 64s - loss: 65.0177 - MinusLogProbMetric: 65.0177 - val_loss: 64.9166 - val_MinusLogProbMetric: 64.9166 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 76/1000
2023-09-26 14:50:46.292 
Epoch 76/1000 
	 loss: 64.3821, MinusLogProbMetric: 64.3821, val_loss: 63.9772, val_MinusLogProbMetric: 63.9772

Epoch 76: val_loss improved from 64.86838 to 63.97717, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 66s - loss: 64.3821 - MinusLogProbMetric: 64.3821 - val_loss: 63.9772 - val_MinusLogProbMetric: 63.9772 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 77/1000
2023-09-26 14:51:51.340 
Epoch 77/1000 
	 loss: 64.0334, MinusLogProbMetric: 64.0334, val_loss: 63.9454, val_MinusLogProbMetric: 63.9454

Epoch 77: val_loss improved from 63.97717 to 63.94537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 64.0334 - MinusLogProbMetric: 64.0334 - val_loss: 63.9454 - val_MinusLogProbMetric: 63.9454 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 78/1000
2023-09-26 14:52:56.096 
Epoch 78/1000 
	 loss: 63.7227, MinusLogProbMetric: 63.7227, val_loss: 63.7987, val_MinusLogProbMetric: 63.7987

Epoch 78: val_loss improved from 63.94537 to 63.79869, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 63.7227 - MinusLogProbMetric: 63.7227 - val_loss: 63.7987 - val_MinusLogProbMetric: 63.7987 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 79/1000
2023-09-26 14:54:01.588 
Epoch 79/1000 
	 loss: 63.4868, MinusLogProbMetric: 63.4868, val_loss: 63.3187, val_MinusLogProbMetric: 63.3187

Epoch 79: val_loss improved from 63.79869 to 63.31874, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 66s - loss: 63.4868 - MinusLogProbMetric: 63.4868 - val_loss: 63.3187 - val_MinusLogProbMetric: 63.3187 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 80/1000
2023-09-26 14:55:07.367 
Epoch 80/1000 
	 loss: 63.2700, MinusLogProbMetric: 63.2700, val_loss: 63.0568, val_MinusLogProbMetric: 63.0568

Epoch 80: val_loss improved from 63.31874 to 63.05679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 66s - loss: 63.2700 - MinusLogProbMetric: 63.2700 - val_loss: 63.0568 - val_MinusLogProbMetric: 63.0568 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 81/1000
2023-09-26 14:56:12.183 
Epoch 81/1000 
	 loss: 63.4496, MinusLogProbMetric: 63.4496, val_loss: 62.9111, val_MinusLogProbMetric: 62.9111

Epoch 81: val_loss improved from 63.05679 to 62.91112, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 63.4496 - MinusLogProbMetric: 63.4496 - val_loss: 62.9111 - val_MinusLogProbMetric: 62.9111 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 82/1000
2023-09-26 14:57:18.240 
Epoch 82/1000 
	 loss: 73.6944, MinusLogProbMetric: 73.6944, val_loss: 71.6374, val_MinusLogProbMetric: 71.6374

Epoch 82: val_loss did not improve from 62.91112
196/196 - 65s - loss: 73.6944 - MinusLogProbMetric: 73.6944 - val_loss: 71.6374 - val_MinusLogProbMetric: 71.6374 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 83/1000
2023-09-26 14:58:22.694 
Epoch 83/1000 
	 loss: 67.6183, MinusLogProbMetric: 67.6183, val_loss: 67.0006, val_MinusLogProbMetric: 67.0006

Epoch 83: val_loss did not improve from 62.91112
196/196 - 64s - loss: 67.6183 - MinusLogProbMetric: 67.6183 - val_loss: 67.0006 - val_MinusLogProbMetric: 67.0006 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 84/1000
2023-09-26 14:59:26.777 
Epoch 84/1000 
	 loss: 65.2106, MinusLogProbMetric: 65.2106, val_loss: 64.4649, val_MinusLogProbMetric: 64.4649

Epoch 84: val_loss did not improve from 62.91112
196/196 - 64s - loss: 65.2106 - MinusLogProbMetric: 65.2106 - val_loss: 64.4649 - val_MinusLogProbMetric: 64.4649 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 85/1000
2023-09-26 15:00:31.353 
Epoch 85/1000 
	 loss: 63.8474, MinusLogProbMetric: 63.8474, val_loss: 63.4130, val_MinusLogProbMetric: 63.4130

Epoch 85: val_loss did not improve from 62.91112
196/196 - 65s - loss: 63.8474 - MinusLogProbMetric: 63.8474 - val_loss: 63.4130 - val_MinusLogProbMetric: 63.4130 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 86/1000
2023-09-26 15:01:36.108 
Epoch 86/1000 
	 loss: 63.6609, MinusLogProbMetric: 63.6609, val_loss: 63.5868, val_MinusLogProbMetric: 63.5868

Epoch 86: val_loss did not improve from 62.91112
196/196 - 65s - loss: 63.6609 - MinusLogProbMetric: 63.6609 - val_loss: 63.5868 - val_MinusLogProbMetric: 63.5868 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 87/1000
2023-09-26 15:02:39.796 
Epoch 87/1000 
	 loss: 104.9079, MinusLogProbMetric: 104.9079, val_loss: 131.1464, val_MinusLogProbMetric: 131.1464

Epoch 87: val_loss did not improve from 62.91112
196/196 - 64s - loss: 104.9079 - MinusLogProbMetric: 104.9079 - val_loss: 131.1464 - val_MinusLogProbMetric: 131.1464 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 88/1000
2023-09-26 15:03:43.907 
Epoch 88/1000 
	 loss: 89.2801, MinusLogProbMetric: 89.2801, val_loss: 76.2670, val_MinusLogProbMetric: 76.2670

Epoch 88: val_loss did not improve from 62.91112
196/196 - 64s - loss: 89.2801 - MinusLogProbMetric: 89.2801 - val_loss: 76.2670 - val_MinusLogProbMetric: 76.2670 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 89/1000
2023-09-26 15:04:47.832 
Epoch 89/1000 
	 loss: 73.8639, MinusLogProbMetric: 73.8639, val_loss: 71.7465, val_MinusLogProbMetric: 71.7465

Epoch 89: val_loss did not improve from 62.91112
196/196 - 64s - loss: 73.8639 - MinusLogProbMetric: 73.8639 - val_loss: 71.7465 - val_MinusLogProbMetric: 71.7465 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 90/1000
2023-09-26 15:05:51.813 
Epoch 90/1000 
	 loss: 69.8484, MinusLogProbMetric: 69.8484, val_loss: 68.7375, val_MinusLogProbMetric: 68.7375

Epoch 90: val_loss did not improve from 62.91112
196/196 - 64s - loss: 69.8484 - MinusLogProbMetric: 69.8484 - val_loss: 68.7375 - val_MinusLogProbMetric: 68.7375 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 91/1000
2023-09-26 15:06:55.830 
Epoch 91/1000 
	 loss: 67.9777, MinusLogProbMetric: 67.9777, val_loss: 67.3490, val_MinusLogProbMetric: 67.3490

Epoch 91: val_loss did not improve from 62.91112
196/196 - 64s - loss: 67.9777 - MinusLogProbMetric: 67.9777 - val_loss: 67.3490 - val_MinusLogProbMetric: 67.3490 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 92/1000
2023-09-26 15:08:00.114 
Epoch 92/1000 
	 loss: 84.4746, MinusLogProbMetric: 84.4746, val_loss: 257.2131, val_MinusLogProbMetric: 257.2131

Epoch 92: val_loss did not improve from 62.91112
196/196 - 64s - loss: 84.4746 - MinusLogProbMetric: 84.4746 - val_loss: 257.2131 - val_MinusLogProbMetric: 257.2131 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 93/1000
2023-09-26 15:09:04.340 
Epoch 93/1000 
	 loss: 137.7308, MinusLogProbMetric: 137.7308, val_loss: 128.6940, val_MinusLogProbMetric: 128.6940

Epoch 93: val_loss did not improve from 62.91112
196/196 - 64s - loss: 137.7308 - MinusLogProbMetric: 137.7308 - val_loss: 128.6940 - val_MinusLogProbMetric: 128.6940 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 94/1000
2023-09-26 15:10:08.730 
Epoch 94/1000 
	 loss: 78.5589, MinusLogProbMetric: 78.5589, val_loss: 62.6743, val_MinusLogProbMetric: 62.6743

Epoch 94: val_loss improved from 62.91112 to 62.67430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 78.5589 - MinusLogProbMetric: 78.5589 - val_loss: 62.6743 - val_MinusLogProbMetric: 62.6743 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 95/1000
2023-09-26 15:11:13.832 
Epoch 95/1000 
	 loss: 58.7532, MinusLogProbMetric: 58.7532, val_loss: 54.2328, val_MinusLogProbMetric: 54.2328

Epoch 95: val_loss improved from 62.67430 to 54.23283, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 58.7532 - MinusLogProbMetric: 58.7532 - val_loss: 54.2328 - val_MinusLogProbMetric: 54.2328 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 96/1000
2023-09-26 15:12:18.977 
Epoch 96/1000 
	 loss: 52.8027, MinusLogProbMetric: 52.8027, val_loss: 51.6163, val_MinusLogProbMetric: 51.6163

Epoch 96: val_loss improved from 54.23283 to 51.61631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 52.8027 - MinusLogProbMetric: 52.8027 - val_loss: 51.6163 - val_MinusLogProbMetric: 51.6163 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 97/1000
2023-09-26 15:13:24.280 
Epoch 97/1000 
	 loss: 62.7911, MinusLogProbMetric: 62.7911, val_loss: 72.3490, val_MinusLogProbMetric: 72.3490

Epoch 97: val_loss did not improve from 51.61631
196/196 - 64s - loss: 62.7911 - MinusLogProbMetric: 62.7911 - val_loss: 72.3490 - val_MinusLogProbMetric: 72.3490 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 98/1000
2023-09-26 15:14:28.698 
Epoch 98/1000 
	 loss: 56.8443, MinusLogProbMetric: 56.8443, val_loss: 52.8443, val_MinusLogProbMetric: 52.8443

Epoch 98: val_loss did not improve from 51.61631
196/196 - 64s - loss: 56.8443 - MinusLogProbMetric: 56.8443 - val_loss: 52.8443 - val_MinusLogProbMetric: 52.8443 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 99/1000
2023-09-26 15:15:32.894 
Epoch 99/1000 
	 loss: 47.1664, MinusLogProbMetric: 47.1664, val_loss: 44.6424, val_MinusLogProbMetric: 44.6424

Epoch 99: val_loss improved from 51.61631 to 44.64236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 47.1664 - MinusLogProbMetric: 47.1664 - val_loss: 44.6424 - val_MinusLogProbMetric: 44.6424 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 100/1000
2023-09-26 15:16:37.893 
Epoch 100/1000 
	 loss: 42.7618, MinusLogProbMetric: 42.7618, val_loss: 41.6284, val_MinusLogProbMetric: 41.6284

Epoch 100: val_loss improved from 44.64236 to 41.62838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 42.7618 - MinusLogProbMetric: 42.7618 - val_loss: 41.6284 - val_MinusLogProbMetric: 41.6284 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 101/1000
2023-09-26 15:17:42.863 
Epoch 101/1000 
	 loss: 39.9768, MinusLogProbMetric: 39.9768, val_loss: 38.6969, val_MinusLogProbMetric: 38.6969

Epoch 101: val_loss improved from 41.62838 to 38.69695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 39.9768 - MinusLogProbMetric: 39.9768 - val_loss: 38.6969 - val_MinusLogProbMetric: 38.6969 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 102/1000
2023-09-26 15:18:48.342 
Epoch 102/1000 
	 loss: 58.2745, MinusLogProbMetric: 58.2745, val_loss: 59.3184, val_MinusLogProbMetric: 59.3184

Epoch 102: val_loss did not improve from 38.69695
196/196 - 64s - loss: 58.2745 - MinusLogProbMetric: 58.2745 - val_loss: 59.3184 - val_MinusLogProbMetric: 59.3184 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 103/1000
2023-09-26 15:19:52.335 
Epoch 103/1000 
	 loss: 94.8100, MinusLogProbMetric: 94.8100, val_loss: 70.6835, val_MinusLogProbMetric: 70.6835

Epoch 103: val_loss did not improve from 38.69695
196/196 - 64s - loss: 94.8100 - MinusLogProbMetric: 94.8100 - val_loss: 70.6835 - val_MinusLogProbMetric: 70.6835 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 104/1000
2023-09-26 15:20:56.091 
Epoch 104/1000 
	 loss: 63.1150, MinusLogProbMetric: 63.1150, val_loss: 57.9483, val_MinusLogProbMetric: 57.9483

Epoch 104: val_loss did not improve from 38.69695
196/196 - 64s - loss: 63.1150 - MinusLogProbMetric: 63.1150 - val_loss: 57.9483 - val_MinusLogProbMetric: 57.9483 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 105/1000
2023-09-26 15:22:00.316 
Epoch 105/1000 
	 loss: 53.4610, MinusLogProbMetric: 53.4610, val_loss: 50.4401, val_MinusLogProbMetric: 50.4401

Epoch 105: val_loss did not improve from 38.69695
196/196 - 64s - loss: 53.4610 - MinusLogProbMetric: 53.4610 - val_loss: 50.4401 - val_MinusLogProbMetric: 50.4401 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 106/1000
2023-09-26 15:23:04.635 
Epoch 106/1000 
	 loss: 96.3857, MinusLogProbMetric: 96.3857, val_loss: 375.9429, val_MinusLogProbMetric: 375.9429

Epoch 106: val_loss did not improve from 38.69695
196/196 - 64s - loss: 96.3857 - MinusLogProbMetric: 96.3857 - val_loss: 375.9429 - val_MinusLogProbMetric: 375.9429 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 107/1000
2023-09-26 15:24:08.703 
Epoch 107/1000 
	 loss: 169.2832, MinusLogProbMetric: 169.2832, val_loss: 113.6814, val_MinusLogProbMetric: 113.6814

Epoch 107: val_loss did not improve from 38.69695
196/196 - 64s - loss: 169.2832 - MinusLogProbMetric: 169.2832 - val_loss: 113.6814 - val_MinusLogProbMetric: 113.6814 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 108/1000
2023-09-26 15:25:11.210 
Epoch 108/1000 
	 loss: 97.4811, MinusLogProbMetric: 97.4811, val_loss: 86.9731, val_MinusLogProbMetric: 86.9731

Epoch 108: val_loss did not improve from 38.69695
196/196 - 63s - loss: 97.4811 - MinusLogProbMetric: 97.4811 - val_loss: 86.9731 - val_MinusLogProbMetric: 86.9731 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 109/1000
2023-09-26 15:26:12.204 
Epoch 109/1000 
	 loss: 79.8281, MinusLogProbMetric: 79.8281, val_loss: 73.3043, val_MinusLogProbMetric: 73.3043

Epoch 109: val_loss did not improve from 38.69695
196/196 - 61s - loss: 79.8281 - MinusLogProbMetric: 79.8281 - val_loss: 73.3043 - val_MinusLogProbMetric: 73.3043 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 110/1000
2023-09-26 15:27:08.400 
Epoch 110/1000 
	 loss: 69.6721, MinusLogProbMetric: 69.6721, val_loss: 67.6346, val_MinusLogProbMetric: 67.6346

Epoch 110: val_loss did not improve from 38.69695
196/196 - 56s - loss: 69.6721 - MinusLogProbMetric: 69.6721 - val_loss: 67.6346 - val_MinusLogProbMetric: 67.6346 - lr: 3.3333e-04 - 56s/epoch - 287ms/step
Epoch 111/1000
2023-09-26 15:28:09.816 
Epoch 111/1000 
	 loss: 64.6198, MinusLogProbMetric: 64.6198, val_loss: 62.8389, val_MinusLogProbMetric: 62.8389

Epoch 111: val_loss did not improve from 38.69695
196/196 - 61s - loss: 64.6198 - MinusLogProbMetric: 64.6198 - val_loss: 62.8389 - val_MinusLogProbMetric: 62.8389 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 112/1000
2023-09-26 15:29:07.692 
Epoch 112/1000 
	 loss: 61.4542, MinusLogProbMetric: 61.4542, val_loss: 60.0413, val_MinusLogProbMetric: 60.0413

Epoch 112: val_loss did not improve from 38.69695
196/196 - 58s - loss: 61.4542 - MinusLogProbMetric: 61.4542 - val_loss: 60.0413 - val_MinusLogProbMetric: 60.0413 - lr: 3.3333e-04 - 58s/epoch - 295ms/step
Epoch 113/1000
2023-09-26 15:30:02.839 
Epoch 113/1000 
	 loss: 59.0226, MinusLogProbMetric: 59.0226, val_loss: 57.9822, val_MinusLogProbMetric: 57.9822

Epoch 113: val_loss did not improve from 38.69695
196/196 - 55s - loss: 59.0226 - MinusLogProbMetric: 59.0226 - val_loss: 57.9822 - val_MinusLogProbMetric: 57.9822 - lr: 3.3333e-04 - 55s/epoch - 281ms/step
Epoch 114/1000
2023-09-26 15:30:56.980 
Epoch 114/1000 
	 loss: 57.1242, MinusLogProbMetric: 57.1242, val_loss: 56.1497, val_MinusLogProbMetric: 56.1497

Epoch 114: val_loss did not improve from 38.69695
196/196 - 54s - loss: 57.1242 - MinusLogProbMetric: 57.1242 - val_loss: 56.1497 - val_MinusLogProbMetric: 56.1497 - lr: 3.3333e-04 - 54s/epoch - 276ms/step
Epoch 115/1000
2023-09-26 15:31:58.796 
Epoch 115/1000 
	 loss: 55.5117, MinusLogProbMetric: 55.5117, val_loss: 54.2819, val_MinusLogProbMetric: 54.2819

Epoch 115: val_loss did not improve from 38.69695
196/196 - 62s - loss: 55.5117 - MinusLogProbMetric: 55.5117 - val_loss: 54.2819 - val_MinusLogProbMetric: 54.2819 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 116/1000
2023-09-26 15:32:58.465 
Epoch 116/1000 
	 loss: 54.1621, MinusLogProbMetric: 54.1621, val_loss: 53.5558, val_MinusLogProbMetric: 53.5558

Epoch 116: val_loss did not improve from 38.69695
196/196 - 60s - loss: 54.1621 - MinusLogProbMetric: 54.1621 - val_loss: 53.5558 - val_MinusLogProbMetric: 53.5558 - lr: 3.3333e-04 - 60s/epoch - 304ms/step
Epoch 117/1000
2023-09-26 15:33:58.925 
Epoch 117/1000 
	 loss: 52.8630, MinusLogProbMetric: 52.8630, val_loss: 52.5380, val_MinusLogProbMetric: 52.5380

Epoch 117: val_loss did not improve from 38.69695
196/196 - 60s - loss: 52.8630 - MinusLogProbMetric: 52.8630 - val_loss: 52.5380 - val_MinusLogProbMetric: 52.5380 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 118/1000
2023-09-26 15:35:02.341 
Epoch 118/1000 
	 loss: 52.8729, MinusLogProbMetric: 52.8729, val_loss: 51.3890, val_MinusLogProbMetric: 51.3890

Epoch 118: val_loss did not improve from 38.69695
196/196 - 63s - loss: 52.8729 - MinusLogProbMetric: 52.8729 - val_loss: 51.3890 - val_MinusLogProbMetric: 51.3890 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 119/1000
2023-09-26 15:36:06.245 
Epoch 119/1000 
	 loss: 51.9670, MinusLogProbMetric: 51.9670, val_loss: 54.5723, val_MinusLogProbMetric: 54.5723

Epoch 119: val_loss did not improve from 38.69695
196/196 - 64s - loss: 51.9670 - MinusLogProbMetric: 51.9670 - val_loss: 54.5723 - val_MinusLogProbMetric: 54.5723 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 120/1000
2023-09-26 15:37:10.030 
Epoch 120/1000 
	 loss: 50.6977, MinusLogProbMetric: 50.6977, val_loss: 49.6942, val_MinusLogProbMetric: 49.6942

Epoch 120: val_loss did not improve from 38.69695
196/196 - 64s - loss: 50.6977 - MinusLogProbMetric: 50.6977 - val_loss: 49.6942 - val_MinusLogProbMetric: 49.6942 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 121/1000
2023-09-26 15:38:14.162 
Epoch 121/1000 
	 loss: 77.8879, MinusLogProbMetric: 77.8879, val_loss: 61.0962, val_MinusLogProbMetric: 61.0962

Epoch 121: val_loss did not improve from 38.69695
196/196 - 64s - loss: 77.8879 - MinusLogProbMetric: 77.8879 - val_loss: 61.0962 - val_MinusLogProbMetric: 61.0962 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 122/1000
2023-09-26 15:39:18.760 
Epoch 122/1000 
	 loss: 69.9110, MinusLogProbMetric: 69.9110, val_loss: 60.6783, val_MinusLogProbMetric: 60.6783

Epoch 122: val_loss did not improve from 38.69695
196/196 - 65s - loss: 69.9110 - MinusLogProbMetric: 69.9110 - val_loss: 60.6783 - val_MinusLogProbMetric: 60.6783 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 123/1000
2023-09-26 15:40:22.363 
Epoch 123/1000 
	 loss: 58.4173, MinusLogProbMetric: 58.4173, val_loss: 55.8238, val_MinusLogProbMetric: 55.8238

Epoch 123: val_loss did not improve from 38.69695
196/196 - 64s - loss: 58.4173 - MinusLogProbMetric: 58.4173 - val_loss: 55.8238 - val_MinusLogProbMetric: 55.8238 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 124/1000
2023-09-26 15:41:26.024 
Epoch 124/1000 
	 loss: 55.2903, MinusLogProbMetric: 55.2903, val_loss: 54.1770, val_MinusLogProbMetric: 54.1770

Epoch 124: val_loss did not improve from 38.69695
196/196 - 64s - loss: 55.2903 - MinusLogProbMetric: 55.2903 - val_loss: 54.1770 - val_MinusLogProbMetric: 54.1770 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 125/1000
2023-09-26 15:42:29.526 
Epoch 125/1000 
	 loss: 54.5197, MinusLogProbMetric: 54.5197, val_loss: 59.4198, val_MinusLogProbMetric: 59.4198

Epoch 125: val_loss did not improve from 38.69695
196/196 - 64s - loss: 54.5197 - MinusLogProbMetric: 54.5197 - val_loss: 59.4198 - val_MinusLogProbMetric: 59.4198 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 126/1000
2023-09-26 15:43:32.569 
Epoch 126/1000 
	 loss: 53.0509, MinusLogProbMetric: 53.0509, val_loss: 50.2056, val_MinusLogProbMetric: 50.2056

Epoch 126: val_loss did not improve from 38.69695
196/196 - 63s - loss: 53.0509 - MinusLogProbMetric: 53.0509 - val_loss: 50.2056 - val_MinusLogProbMetric: 50.2056 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 127/1000
2023-09-26 15:44:36.102 
Epoch 127/1000 
	 loss: 50.4889, MinusLogProbMetric: 50.4889, val_loss: 48.4704, val_MinusLogProbMetric: 48.4704

Epoch 127: val_loss did not improve from 38.69695
196/196 - 64s - loss: 50.4889 - MinusLogProbMetric: 50.4889 - val_loss: 48.4704 - val_MinusLogProbMetric: 48.4704 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 128/1000
2023-09-26 15:45:40.271 
Epoch 128/1000 
	 loss: 48.8078, MinusLogProbMetric: 48.8078, val_loss: 48.9163, val_MinusLogProbMetric: 48.9163

Epoch 128: val_loss did not improve from 38.69695
196/196 - 64s - loss: 48.8078 - MinusLogProbMetric: 48.8078 - val_loss: 48.9163 - val_MinusLogProbMetric: 48.9163 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 129/1000
2023-09-26 15:46:43.955 
Epoch 129/1000 
	 loss: 48.5883, MinusLogProbMetric: 48.5883, val_loss: 48.2797, val_MinusLogProbMetric: 48.2797

Epoch 129: val_loss did not improve from 38.69695
196/196 - 64s - loss: 48.5883 - MinusLogProbMetric: 48.5883 - val_loss: 48.2797 - val_MinusLogProbMetric: 48.2797 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 130/1000
2023-09-26 15:47:47.665 
Epoch 130/1000 
	 loss: 48.4876, MinusLogProbMetric: 48.4876, val_loss: 47.4061, val_MinusLogProbMetric: 47.4061

Epoch 130: val_loss did not improve from 38.69695
196/196 - 64s - loss: 48.4876 - MinusLogProbMetric: 48.4876 - val_loss: 47.4061 - val_MinusLogProbMetric: 47.4061 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 131/1000
2023-09-26 15:48:51.576 
Epoch 131/1000 
	 loss: 47.2069, MinusLogProbMetric: 47.2069, val_loss: 46.6196, val_MinusLogProbMetric: 46.6196

Epoch 131: val_loss did not improve from 38.69695
196/196 - 64s - loss: 47.2069 - MinusLogProbMetric: 47.2069 - val_loss: 46.6196 - val_MinusLogProbMetric: 46.6196 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 132/1000
2023-09-26 15:49:55.443 
Epoch 132/1000 
	 loss: 45.9871, MinusLogProbMetric: 45.9871, val_loss: 45.6146, val_MinusLogProbMetric: 45.6146

Epoch 132: val_loss did not improve from 38.69695
196/196 - 64s - loss: 45.9871 - MinusLogProbMetric: 45.9871 - val_loss: 45.6146 - val_MinusLogProbMetric: 45.6146 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 133/1000
2023-09-26 15:51:00.005 
Epoch 133/1000 
	 loss: 45.1902, MinusLogProbMetric: 45.1902, val_loss: 44.2272, val_MinusLogProbMetric: 44.2272

Epoch 133: val_loss did not improve from 38.69695
196/196 - 65s - loss: 45.1902 - MinusLogProbMetric: 45.1902 - val_loss: 44.2272 - val_MinusLogProbMetric: 44.2272 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 134/1000
2023-09-26 15:52:04.381 
Epoch 134/1000 
	 loss: 44.6546, MinusLogProbMetric: 44.6546, val_loss: 45.5101, val_MinusLogProbMetric: 45.5101

Epoch 134: val_loss did not improve from 38.69695
196/196 - 64s - loss: 44.6546 - MinusLogProbMetric: 44.6546 - val_loss: 45.5101 - val_MinusLogProbMetric: 45.5101 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 135/1000
2023-09-26 15:53:08.520 
Epoch 135/1000 
	 loss: 43.9876, MinusLogProbMetric: 43.9876, val_loss: 43.6448, val_MinusLogProbMetric: 43.6448

Epoch 135: val_loss did not improve from 38.69695
196/196 - 64s - loss: 43.9876 - MinusLogProbMetric: 43.9876 - val_loss: 43.6448 - val_MinusLogProbMetric: 43.6448 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 136/1000
2023-09-26 15:54:13.053 
Epoch 136/1000 
	 loss: 43.6973, MinusLogProbMetric: 43.6973, val_loss: 42.8613, val_MinusLogProbMetric: 42.8613

Epoch 136: val_loss did not improve from 38.69695
196/196 - 65s - loss: 43.6973 - MinusLogProbMetric: 43.6973 - val_loss: 42.8613 - val_MinusLogProbMetric: 42.8613 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 137/1000
2023-09-26 15:55:16.760 
Epoch 137/1000 
	 loss: 43.1784, MinusLogProbMetric: 43.1784, val_loss: 41.9646, val_MinusLogProbMetric: 41.9646

Epoch 137: val_loss did not improve from 38.69695
196/196 - 64s - loss: 43.1784 - MinusLogProbMetric: 43.1784 - val_loss: 41.9646 - val_MinusLogProbMetric: 41.9646 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 138/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 161: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-26 15:56:10.401 
Epoch 138/1000 
	 loss: nan, MinusLogProbMetric: 210.8524, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 138: val_loss did not improve from 38.69695
196/196 - 54s - loss: nan - MinusLogProbMetric: 210.8524 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 54s/epoch - 274ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0001111111111111111.
===========
Generating train data for run 285.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_285/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_285/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_285/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_285
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.5424666   4.160326    8.947518   ...  7.3513613   3.2319064
   2.0238907 ]
 [ 1.223466    3.6614168   7.44358    ...  7.1275983   2.889486
   2.1263318 ]
 [ 2.8113456   3.2582755   7.9323573  ...  7.1118317   2.55147
   1.6773428 ]
 ...
 [ 4.8257422   5.760828    0.99088824 ...  1.1221942   6.224573
   1.3870366 ]
 [ 2.8796694   4.5901284   7.9563813  ...  7.221806    3.399475
   1.8678449 ]
 [ 4.724557    5.899211   -0.68043184 ...  1.4107605   6.846419
   1.3825576 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_32"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_33 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f962138a800>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f9620d81420>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f9620d81420>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f9620cef580>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f9620c471f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f9620c47760>, <keras.callbacks.ModelCheckpoint object at 0x7f9620c47820>, <keras.callbacks.EarlyStopping object at 0x7f9620c47a90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f9620c47ac0>, <keras.callbacks.TerminateOnNaN object at 0x7f9620c47700>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.4544215 ,  3.75662   ,  8.587604  , ...,  7.2717385 ,
         2.2797804 ,  2.224709  ],
       [ 1.3902065 ,  3.433224  ,  7.9205    , ...,  7.378508  ,
         3.6055362 ,  1.7885203 ],
       [ 2.5590267 ,  3.4328253 ,  7.4715605 , ...,  7.496273  ,
         3.511426  ,  1.7360344 ],
       ...,
       [ 5.120887  ,  6.0070558 , -0.50225353, ...,  0.7586615 ,
         6.522565  ,  1.2877582 ],
       [ 4.695989  ,  6.091708  ,  0.05890873, ...,  1.4941858 ,
         6.0310183 ,  1.5105633 ],
       [ 5.3161554 ,  7.1853566 ,  7.6864433 , ...,  4.199462  ,
         2.6094391 ,  7.597809  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 285/720 with hyperparameters:
timestamp = 2023-09-26 15:56:21.203218
ndims = 32
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.5424666   4.160326    8.947518    1.0939208   7.617727   -0.37824863
  9.755269    4.70193     9.9485855   6.084102    7.916266    0.31372035
  2.6670911   1.177147    2.493954    1.2494209   3.149602    6.587495
  1.3128376   6.9324527   5.4146237   2.7642589   4.1612015   1.0985487
  4.086754    9.263963    3.806251    5.6873355   2.3875675   7.3513613
  3.2319064   2.0238907 ]
Epoch 1/1000
2023-09-26 15:59:28.343 
Epoch 1/1000 
	 loss: 66.3120, MinusLogProbMetric: 66.3120, val_loss: 54.8279, val_MinusLogProbMetric: 54.8279

Epoch 1: val_loss improved from inf to 54.82794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 188s - loss: 66.3120 - MinusLogProbMetric: 66.3120 - val_loss: 54.8279 - val_MinusLogProbMetric: 54.8279 - lr: 1.1111e-04 - 188s/epoch - 957ms/step
Epoch 2/1000
2023-09-26 16:00:33.561 
Epoch 2/1000 
	 loss: 43.3306, MinusLogProbMetric: 43.3306, val_loss: 37.1527, val_MinusLogProbMetric: 37.1527

Epoch 2: val_loss improved from 54.82794 to 37.15268, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 43.3306 - MinusLogProbMetric: 43.3306 - val_loss: 37.1527 - val_MinusLogProbMetric: 37.1527 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 3/1000
2023-09-26 16:01:38.377 
Epoch 3/1000 
	 loss: 38.4918, MinusLogProbMetric: 38.4918, val_loss: 40.1220, val_MinusLogProbMetric: 40.1220

Epoch 3: val_loss did not improve from 37.15268
196/196 - 64s - loss: 38.4918 - MinusLogProbMetric: 38.4918 - val_loss: 40.1220 - val_MinusLogProbMetric: 40.1220 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 4/1000
2023-09-26 16:02:42.447 
Epoch 4/1000 
	 loss: 32.8521, MinusLogProbMetric: 32.8521, val_loss: 30.8027, val_MinusLogProbMetric: 30.8027

Epoch 4: val_loss improved from 37.15268 to 30.80267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 32.8521 - MinusLogProbMetric: 32.8521 - val_loss: 30.8027 - val_MinusLogProbMetric: 30.8027 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 5/1000
2023-09-26 16:03:47.895 
Epoch 5/1000 
	 loss: 32.3303, MinusLogProbMetric: 32.3303, val_loss: 33.2814, val_MinusLogProbMetric: 33.2814

Epoch 5: val_loss did not improve from 30.80267
196/196 - 64s - loss: 32.3303 - MinusLogProbMetric: 32.3303 - val_loss: 33.2814 - val_MinusLogProbMetric: 33.2814 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 6/1000
2023-09-26 16:04:52.659 
Epoch 6/1000 
	 loss: 31.0085, MinusLogProbMetric: 31.0085, val_loss: 29.1795, val_MinusLogProbMetric: 29.1795

Epoch 6: val_loss improved from 30.80267 to 29.17947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 66s - loss: 31.0085 - MinusLogProbMetric: 31.0085 - val_loss: 29.1795 - val_MinusLogProbMetric: 29.1795 - lr: 1.1111e-04 - 66s/epoch - 335ms/step
Epoch 7/1000
2023-09-26 16:05:57.798 
Epoch 7/1000 
	 loss: 34.0507, MinusLogProbMetric: 34.0507, val_loss: 28.9574, val_MinusLogProbMetric: 28.9574

Epoch 7: val_loss improved from 29.17947 to 28.95739, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 34.0507 - MinusLogProbMetric: 34.0507 - val_loss: 28.9574 - val_MinusLogProbMetric: 28.9574 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 8/1000
2023-09-26 16:07:03.119 
Epoch 8/1000 
	 loss: 27.4762, MinusLogProbMetric: 27.4762, val_loss: 30.0536, val_MinusLogProbMetric: 30.0536

Epoch 8: val_loss did not improve from 28.95739
196/196 - 64s - loss: 27.4762 - MinusLogProbMetric: 27.4762 - val_loss: 30.0536 - val_MinusLogProbMetric: 30.0536 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 9/1000
2023-09-26 16:08:07.256 
Epoch 9/1000 
	 loss: 29.8553, MinusLogProbMetric: 29.8553, val_loss: 27.2037, val_MinusLogProbMetric: 27.2037

Epoch 9: val_loss improved from 28.95739 to 27.20374, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 29.8553 - MinusLogProbMetric: 29.8553 - val_loss: 27.2037 - val_MinusLogProbMetric: 27.2037 - lr: 1.1111e-04 - 65s/epoch - 332ms/step
Epoch 10/1000
2023-09-26 16:09:12.619 
Epoch 10/1000 
	 loss: 31.7045, MinusLogProbMetric: 31.7045, val_loss: 33.6115, val_MinusLogProbMetric: 33.6115

Epoch 10: val_loss did not improve from 27.20374
196/196 - 64s - loss: 31.7045 - MinusLogProbMetric: 31.7045 - val_loss: 33.6115 - val_MinusLogProbMetric: 33.6115 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 11/1000
2023-09-26 16:10:17.019 
Epoch 11/1000 
	 loss: 30.3419, MinusLogProbMetric: 30.3419, val_loss: 25.3142, val_MinusLogProbMetric: 25.3142

Epoch 11: val_loss improved from 27.20374 to 25.31419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 30.3419 - MinusLogProbMetric: 30.3419 - val_loss: 25.3142 - val_MinusLogProbMetric: 25.3142 - lr: 1.1111e-04 - 65s/epoch - 333ms/step
Epoch 12/1000
2023-09-26 16:11:22.478 
Epoch 12/1000 
	 loss: 67.2121, MinusLogProbMetric: 67.2121, val_loss: 76.3393, val_MinusLogProbMetric: 76.3393

Epoch 12: val_loss did not improve from 25.31419
196/196 - 64s - loss: 67.2121 - MinusLogProbMetric: 67.2121 - val_loss: 76.3393 - val_MinusLogProbMetric: 76.3393 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 13/1000
2023-09-26 16:12:26.740 
Epoch 13/1000 
	 loss: 68.3270, MinusLogProbMetric: 68.3270, val_loss: 47.8512, val_MinusLogProbMetric: 47.8512

Epoch 13: val_loss did not improve from 25.31419
196/196 - 64s - loss: 68.3270 - MinusLogProbMetric: 68.3270 - val_loss: 47.8512 - val_MinusLogProbMetric: 47.8512 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 14/1000
2023-09-26 16:13:31.016 
Epoch 14/1000 
	 loss: 40.1610, MinusLogProbMetric: 40.1610, val_loss: 35.3833, val_MinusLogProbMetric: 35.3833

Epoch 14: val_loss did not improve from 25.31419
196/196 - 64s - loss: 40.1610 - MinusLogProbMetric: 40.1610 - val_loss: 35.3833 - val_MinusLogProbMetric: 35.3833 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 15/1000
2023-09-26 16:14:35.222 
Epoch 15/1000 
	 loss: 34.3950, MinusLogProbMetric: 34.3950, val_loss: 32.3287, val_MinusLogProbMetric: 32.3287

Epoch 15: val_loss did not improve from 25.31419
196/196 - 64s - loss: 34.3950 - MinusLogProbMetric: 34.3950 - val_loss: 32.3287 - val_MinusLogProbMetric: 32.3287 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 16/1000
2023-09-26 16:15:38.876 
Epoch 16/1000 
	 loss: 31.0323, MinusLogProbMetric: 31.0323, val_loss: 29.4361, val_MinusLogProbMetric: 29.4361

Epoch 16: val_loss did not improve from 25.31419
196/196 - 64s - loss: 31.0323 - MinusLogProbMetric: 31.0323 - val_loss: 29.4361 - val_MinusLogProbMetric: 29.4361 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 17/1000
2023-09-26 16:16:42.766 
Epoch 17/1000 
	 loss: 28.8321, MinusLogProbMetric: 28.8321, val_loss: 28.1817, val_MinusLogProbMetric: 28.1817

Epoch 17: val_loss did not improve from 25.31419
196/196 - 64s - loss: 28.8321 - MinusLogProbMetric: 28.8321 - val_loss: 28.1817 - val_MinusLogProbMetric: 28.1817 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 18/1000
2023-09-26 16:17:46.420 
Epoch 18/1000 
	 loss: 28.3510, MinusLogProbMetric: 28.3510, val_loss: 27.0563, val_MinusLogProbMetric: 27.0563

Epoch 18: val_loss did not improve from 25.31419
196/196 - 64s - loss: 28.3510 - MinusLogProbMetric: 28.3510 - val_loss: 27.0563 - val_MinusLogProbMetric: 27.0563 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 19/1000
2023-09-26 16:18:50.785 
Epoch 19/1000 
	 loss: 27.2855, MinusLogProbMetric: 27.2855, val_loss: 26.3681, val_MinusLogProbMetric: 26.3681

Epoch 19: val_loss did not improve from 25.31419
196/196 - 64s - loss: 27.2855 - MinusLogProbMetric: 27.2855 - val_loss: 26.3681 - val_MinusLogProbMetric: 26.3681 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 20/1000
2023-09-26 16:19:54.680 
Epoch 20/1000 
	 loss: 26.6337, MinusLogProbMetric: 26.6337, val_loss: 27.6431, val_MinusLogProbMetric: 27.6431

Epoch 20: val_loss did not improve from 25.31419
196/196 - 64s - loss: 26.6337 - MinusLogProbMetric: 26.6337 - val_loss: 27.6431 - val_MinusLogProbMetric: 27.6431 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 21/1000
2023-09-26 16:20:58.273 
Epoch 21/1000 
	 loss: 25.6528, MinusLogProbMetric: 25.6528, val_loss: 25.5364, val_MinusLogProbMetric: 25.5364

Epoch 21: val_loss did not improve from 25.31419
196/196 - 64s - loss: 25.6528 - MinusLogProbMetric: 25.6528 - val_loss: 25.5364 - val_MinusLogProbMetric: 25.5364 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 22/1000
2023-09-26 16:22:02.102 
Epoch 22/1000 
	 loss: 25.4001, MinusLogProbMetric: 25.4001, val_loss: 24.8053, val_MinusLogProbMetric: 24.8053

Epoch 22: val_loss improved from 25.31419 to 24.80531, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 25.4001 - MinusLogProbMetric: 25.4001 - val_loss: 24.8053 - val_MinusLogProbMetric: 24.8053 - lr: 1.1111e-04 - 65s/epoch - 331ms/step
Epoch 23/1000
2023-09-26 16:23:06.937 
Epoch 23/1000 
	 loss: 25.2870, MinusLogProbMetric: 25.2870, val_loss: 25.1010, val_MinusLogProbMetric: 25.1010

Epoch 23: val_loss did not improve from 24.80531
196/196 - 64s - loss: 25.2870 - MinusLogProbMetric: 25.2870 - val_loss: 25.1010 - val_MinusLogProbMetric: 25.1010 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 24/1000
2023-09-26 16:24:10.659 
Epoch 24/1000 
	 loss: 24.3368, MinusLogProbMetric: 24.3368, val_loss: 24.1550, val_MinusLogProbMetric: 24.1550

Epoch 24: val_loss improved from 24.80531 to 24.15501, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 24.3368 - MinusLogProbMetric: 24.3368 - val_loss: 24.1550 - val_MinusLogProbMetric: 24.1550 - lr: 1.1111e-04 - 65s/epoch - 330ms/step
Epoch 25/1000
2023-09-26 16:25:14.782 
Epoch 25/1000 
	 loss: 24.0420, MinusLogProbMetric: 24.0420, val_loss: 24.2774, val_MinusLogProbMetric: 24.2774

Epoch 25: val_loss did not improve from 24.15501
196/196 - 63s - loss: 24.0420 - MinusLogProbMetric: 24.0420 - val_loss: 24.2774 - val_MinusLogProbMetric: 24.2774 - lr: 1.1111e-04 - 63s/epoch - 322ms/step
Epoch 26/1000
2023-09-26 16:26:18.593 
Epoch 26/1000 
	 loss: 40.0134, MinusLogProbMetric: 40.0134, val_loss: 35.0653, val_MinusLogProbMetric: 35.0653

Epoch 26: val_loss did not improve from 24.15501
196/196 - 64s - loss: 40.0134 - MinusLogProbMetric: 40.0134 - val_loss: 35.0653 - val_MinusLogProbMetric: 35.0653 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 27/1000
2023-09-26 16:27:22.189 
Epoch 27/1000 
	 loss: 32.1373, MinusLogProbMetric: 32.1373, val_loss: 27.8254, val_MinusLogProbMetric: 27.8254

Epoch 27: val_loss did not improve from 24.15501
196/196 - 64s - loss: 32.1373 - MinusLogProbMetric: 32.1373 - val_loss: 27.8254 - val_MinusLogProbMetric: 27.8254 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 28/1000
2023-09-26 16:28:26.113 
Epoch 28/1000 
	 loss: 26.0900, MinusLogProbMetric: 26.0900, val_loss: 28.8783, val_MinusLogProbMetric: 28.8783

Epoch 28: val_loss did not improve from 24.15501
196/196 - 64s - loss: 26.0900 - MinusLogProbMetric: 26.0900 - val_loss: 28.8783 - val_MinusLogProbMetric: 28.8783 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 29/1000
2023-09-26 16:29:29.652 
Epoch 29/1000 
	 loss: 32.5515, MinusLogProbMetric: 32.5515, val_loss: 30.9291, val_MinusLogProbMetric: 30.9291

Epoch 29: val_loss did not improve from 24.15501
196/196 - 64s - loss: 32.5515 - MinusLogProbMetric: 32.5515 - val_loss: 30.9291 - val_MinusLogProbMetric: 30.9291 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 30/1000
2023-09-26 16:30:33.468 
Epoch 30/1000 
	 loss: 29.6671, MinusLogProbMetric: 29.6671, val_loss: 28.8295, val_MinusLogProbMetric: 28.8295

Epoch 30: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.6671 - MinusLogProbMetric: 29.6671 - val_loss: 28.8295 - val_MinusLogProbMetric: 28.8295 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 31/1000
2023-09-26 16:31:37.573 
Epoch 31/1000 
	 loss: 39.0377, MinusLogProbMetric: 39.0377, val_loss: 50.8317, val_MinusLogProbMetric: 50.8317

Epoch 31: val_loss did not improve from 24.15501
196/196 - 64s - loss: 39.0377 - MinusLogProbMetric: 39.0377 - val_loss: 50.8317 - val_MinusLogProbMetric: 50.8317 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 32/1000
2023-09-26 16:32:41.523 
Epoch 32/1000 
	 loss: 41.1599, MinusLogProbMetric: 41.1599, val_loss: 37.5506, val_MinusLogProbMetric: 37.5506

Epoch 32: val_loss did not improve from 24.15501
196/196 - 64s - loss: 41.1599 - MinusLogProbMetric: 41.1599 - val_loss: 37.5506 - val_MinusLogProbMetric: 37.5506 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 33/1000
2023-09-26 16:33:45.501 
Epoch 33/1000 
	 loss: 36.5289, MinusLogProbMetric: 36.5289, val_loss: 36.2869, val_MinusLogProbMetric: 36.2869

Epoch 33: val_loss did not improve from 24.15501
196/196 - 64s - loss: 36.5289 - MinusLogProbMetric: 36.5289 - val_loss: 36.2869 - val_MinusLogProbMetric: 36.2869 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 34/1000
2023-09-26 16:34:49.065 
Epoch 34/1000 
	 loss: 35.5476, MinusLogProbMetric: 35.5476, val_loss: 34.9401, val_MinusLogProbMetric: 34.9401

Epoch 34: val_loss did not improve from 24.15501
196/196 - 64s - loss: 35.5476 - MinusLogProbMetric: 35.5476 - val_loss: 34.9401 - val_MinusLogProbMetric: 34.9401 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 35/1000
2023-09-26 16:35:52.882 
Epoch 35/1000 
	 loss: 34.6465, MinusLogProbMetric: 34.6465, val_loss: 34.4292, val_MinusLogProbMetric: 34.4292

Epoch 35: val_loss did not improve from 24.15501
196/196 - 64s - loss: 34.6465 - MinusLogProbMetric: 34.6465 - val_loss: 34.4292 - val_MinusLogProbMetric: 34.4292 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 36/1000
2023-09-26 16:36:57.030 
Epoch 36/1000 
	 loss: 34.0198, MinusLogProbMetric: 34.0198, val_loss: 32.4020, val_MinusLogProbMetric: 32.4020

Epoch 36: val_loss did not improve from 24.15501
196/196 - 64s - loss: 34.0198 - MinusLogProbMetric: 34.0198 - val_loss: 32.4020 - val_MinusLogProbMetric: 32.4020 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 37/1000
2023-09-26 16:38:01.030 
Epoch 37/1000 
	 loss: 48.9832, MinusLogProbMetric: 48.9832, val_loss: 44.4832, val_MinusLogProbMetric: 44.4832

Epoch 37: val_loss did not improve from 24.15501
196/196 - 64s - loss: 48.9832 - MinusLogProbMetric: 48.9832 - val_loss: 44.4832 - val_MinusLogProbMetric: 44.4832 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 38/1000
2023-09-26 16:39:05.004 
Epoch 38/1000 
	 loss: 39.6133, MinusLogProbMetric: 39.6133, val_loss: 37.2699, val_MinusLogProbMetric: 37.2699

Epoch 38: val_loss did not improve from 24.15501
196/196 - 64s - loss: 39.6133 - MinusLogProbMetric: 39.6133 - val_loss: 37.2699 - val_MinusLogProbMetric: 37.2699 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 39/1000
2023-09-26 16:40:08.635 
Epoch 39/1000 
	 loss: 36.6280, MinusLogProbMetric: 36.6280, val_loss: 37.4812, val_MinusLogProbMetric: 37.4812

Epoch 39: val_loss did not improve from 24.15501
196/196 - 64s - loss: 36.6280 - MinusLogProbMetric: 36.6280 - val_loss: 37.4812 - val_MinusLogProbMetric: 37.4812 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 40/1000
2023-09-26 16:41:12.551 
Epoch 40/1000 
	 loss: 35.5414, MinusLogProbMetric: 35.5414, val_loss: 34.3685, val_MinusLogProbMetric: 34.3685

Epoch 40: val_loss did not improve from 24.15501
196/196 - 64s - loss: 35.5414 - MinusLogProbMetric: 35.5414 - val_loss: 34.3685 - val_MinusLogProbMetric: 34.3685 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 41/1000
2023-09-26 16:42:16.504 
Epoch 41/1000 
	 loss: 34.5612, MinusLogProbMetric: 34.5612, val_loss: 33.7907, val_MinusLogProbMetric: 33.7907

Epoch 41: val_loss did not improve from 24.15501
196/196 - 64s - loss: 34.5612 - MinusLogProbMetric: 34.5612 - val_loss: 33.7907 - val_MinusLogProbMetric: 33.7907 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 42/1000
2023-09-26 16:43:20.561 
Epoch 42/1000 
	 loss: 33.8254, MinusLogProbMetric: 33.8254, val_loss: 33.2948, val_MinusLogProbMetric: 33.2948

Epoch 42: val_loss did not improve from 24.15501
196/196 - 64s - loss: 33.8254 - MinusLogProbMetric: 33.8254 - val_loss: 33.2948 - val_MinusLogProbMetric: 33.2948 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 43/1000
2023-09-26 16:44:24.401 
Epoch 43/1000 
	 loss: 33.4665, MinusLogProbMetric: 33.4665, val_loss: 33.9787, val_MinusLogProbMetric: 33.9787

Epoch 43: val_loss did not improve from 24.15501
196/196 - 64s - loss: 33.4665 - MinusLogProbMetric: 33.4665 - val_loss: 33.9787 - val_MinusLogProbMetric: 33.9787 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 44/1000
2023-09-26 16:45:27.886 
Epoch 44/1000 
	 loss: 32.7092, MinusLogProbMetric: 32.7092, val_loss: 32.5386, val_MinusLogProbMetric: 32.5386

Epoch 44: val_loss did not improve from 24.15501
196/196 - 63s - loss: 32.7092 - MinusLogProbMetric: 32.7092 - val_loss: 32.5386 - val_MinusLogProbMetric: 32.5386 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 45/1000
2023-09-26 16:46:31.993 
Epoch 45/1000 
	 loss: 32.5523, MinusLogProbMetric: 32.5523, val_loss: 32.9023, val_MinusLogProbMetric: 32.9023

Epoch 45: val_loss did not improve from 24.15501
196/196 - 64s - loss: 32.5523 - MinusLogProbMetric: 32.5523 - val_loss: 32.9023 - val_MinusLogProbMetric: 32.9023 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 46/1000
2023-09-26 16:47:36.008 
Epoch 46/1000 
	 loss: 32.4691, MinusLogProbMetric: 32.4691, val_loss: 31.8757, val_MinusLogProbMetric: 31.8757

Epoch 46: val_loss did not improve from 24.15501
196/196 - 64s - loss: 32.4691 - MinusLogProbMetric: 32.4691 - val_loss: 31.8757 - val_MinusLogProbMetric: 31.8757 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 47/1000
2023-09-26 16:48:40.075 
Epoch 47/1000 
	 loss: 32.2160, MinusLogProbMetric: 32.2160, val_loss: 31.9884, val_MinusLogProbMetric: 31.9884

Epoch 47: val_loss did not improve from 24.15501
196/196 - 64s - loss: 32.2160 - MinusLogProbMetric: 32.2160 - val_loss: 31.9884 - val_MinusLogProbMetric: 31.9884 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 48/1000
2023-09-26 16:49:44.002 
Epoch 48/1000 
	 loss: 32.2227, MinusLogProbMetric: 32.2227, val_loss: 31.8437, val_MinusLogProbMetric: 31.8437

Epoch 48: val_loss did not improve from 24.15501
196/196 - 64s - loss: 32.2227 - MinusLogProbMetric: 32.2227 - val_loss: 31.8437 - val_MinusLogProbMetric: 31.8437 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 49/1000
2023-09-26 16:50:47.799 
Epoch 49/1000 
	 loss: 32.3168, MinusLogProbMetric: 32.3168, val_loss: 31.7660, val_MinusLogProbMetric: 31.7660

Epoch 49: val_loss did not improve from 24.15501
196/196 - 64s - loss: 32.3168 - MinusLogProbMetric: 32.3168 - val_loss: 31.7660 - val_MinusLogProbMetric: 31.7660 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 50/1000
2023-09-26 16:51:51.026 
Epoch 50/1000 
	 loss: 31.7868, MinusLogProbMetric: 31.7868, val_loss: 31.2791, val_MinusLogProbMetric: 31.2791

Epoch 50: val_loss did not improve from 24.15501
196/196 - 63s - loss: 31.7868 - MinusLogProbMetric: 31.7868 - val_loss: 31.2791 - val_MinusLogProbMetric: 31.2791 - lr: 1.1111e-04 - 63s/epoch - 323ms/step
Epoch 51/1000
2023-09-26 16:52:54.895 
Epoch 51/1000 
	 loss: 31.6891, MinusLogProbMetric: 31.6891, val_loss: 31.8747, val_MinusLogProbMetric: 31.8747

Epoch 51: val_loss did not improve from 24.15501
196/196 - 64s - loss: 31.6891 - MinusLogProbMetric: 31.6891 - val_loss: 31.8747 - val_MinusLogProbMetric: 31.8747 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 52/1000
2023-09-26 16:53:58.857 
Epoch 52/1000 
	 loss: 31.4473, MinusLogProbMetric: 31.4473, val_loss: 31.5759, val_MinusLogProbMetric: 31.5759

Epoch 52: val_loss did not improve from 24.15501
196/196 - 64s - loss: 31.4473 - MinusLogProbMetric: 31.4473 - val_loss: 31.5759 - val_MinusLogProbMetric: 31.5759 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 53/1000
2023-09-26 16:55:02.394 
Epoch 53/1000 
	 loss: 31.5720, MinusLogProbMetric: 31.5720, val_loss: 31.2555, val_MinusLogProbMetric: 31.2555

Epoch 53: val_loss did not improve from 24.15501
196/196 - 64s - loss: 31.5720 - MinusLogProbMetric: 31.5720 - val_loss: 31.2555 - val_MinusLogProbMetric: 31.2555 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 54/1000
2023-09-26 16:56:05.989 
Epoch 54/1000 
	 loss: 31.3402, MinusLogProbMetric: 31.3402, val_loss: 31.3074, val_MinusLogProbMetric: 31.3074

Epoch 54: val_loss did not improve from 24.15501
196/196 - 64s - loss: 31.3402 - MinusLogProbMetric: 31.3402 - val_loss: 31.3074 - val_MinusLogProbMetric: 31.3074 - lr: 1.1111e-04 - 64s/epoch - 324ms/step
Epoch 55/1000
2023-09-26 16:57:09.477 
Epoch 55/1000 
	 loss: 31.2394, MinusLogProbMetric: 31.2394, val_loss: 30.7451, val_MinusLogProbMetric: 30.7451

Epoch 55: val_loss did not improve from 24.15501
196/196 - 63s - loss: 31.2394 - MinusLogProbMetric: 31.2394 - val_loss: 30.7451 - val_MinusLogProbMetric: 30.7451 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 56/1000
2023-09-26 16:58:13.572 
Epoch 56/1000 
	 loss: 30.9980, MinusLogProbMetric: 30.9980, val_loss: 31.3884, val_MinusLogProbMetric: 31.3884

Epoch 56: val_loss did not improve from 24.15501
196/196 - 64s - loss: 30.9980 - MinusLogProbMetric: 30.9980 - val_loss: 31.3884 - val_MinusLogProbMetric: 31.3884 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 57/1000
2023-09-26 16:59:17.756 
Epoch 57/1000 
	 loss: 48.7692, MinusLogProbMetric: 48.7692, val_loss: 44.6877, val_MinusLogProbMetric: 44.6877

Epoch 57: val_loss did not improve from 24.15501
196/196 - 64s - loss: 48.7692 - MinusLogProbMetric: 48.7692 - val_loss: 44.6877 - val_MinusLogProbMetric: 44.6877 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 58/1000
2023-09-26 17:00:21.523 
Epoch 58/1000 
	 loss: 38.1432, MinusLogProbMetric: 38.1432, val_loss: 34.8930, val_MinusLogProbMetric: 34.8930

Epoch 58: val_loss did not improve from 24.15501
196/196 - 64s - loss: 38.1432 - MinusLogProbMetric: 38.1432 - val_loss: 34.8930 - val_MinusLogProbMetric: 34.8930 - lr: 1.1111e-04 - 64s/epoch - 325ms/step
Epoch 59/1000
2023-09-26 17:01:25.339 
Epoch 59/1000 
	 loss: 33.9052, MinusLogProbMetric: 33.9052, val_loss: 32.9634, val_MinusLogProbMetric: 32.9634

Epoch 59: val_loss did not improve from 24.15501
196/196 - 64s - loss: 33.9052 - MinusLogProbMetric: 33.9052 - val_loss: 32.9634 - val_MinusLogProbMetric: 32.9634 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 60/1000
2023-09-26 17:02:29.445 
Epoch 60/1000 
	 loss: 32.8539, MinusLogProbMetric: 32.8539, val_loss: 32.3915, val_MinusLogProbMetric: 32.3915

Epoch 60: val_loss did not improve from 24.15501
196/196 - 64s - loss: 32.8539 - MinusLogProbMetric: 32.8539 - val_loss: 32.3915 - val_MinusLogProbMetric: 32.3915 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 61/1000
2023-09-26 17:03:33.662 
Epoch 61/1000 
	 loss: 31.9922, MinusLogProbMetric: 31.9922, val_loss: 31.8477, val_MinusLogProbMetric: 31.8477

Epoch 61: val_loss did not improve from 24.15501
196/196 - 64s - loss: 31.9922 - MinusLogProbMetric: 31.9922 - val_loss: 31.8477 - val_MinusLogProbMetric: 31.8477 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 62/1000
2023-09-26 17:04:37.903 
Epoch 62/1000 
	 loss: 31.8232, MinusLogProbMetric: 31.8232, val_loss: 31.6429, val_MinusLogProbMetric: 31.6429

Epoch 62: val_loss did not improve from 24.15501
196/196 - 64s - loss: 31.8232 - MinusLogProbMetric: 31.8232 - val_loss: 31.6429 - val_MinusLogProbMetric: 31.6429 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 63/1000
2023-09-26 17:05:41.809 
Epoch 63/1000 
	 loss: 31.6096, MinusLogProbMetric: 31.6096, val_loss: 31.2657, val_MinusLogProbMetric: 31.2657

Epoch 63: val_loss did not improve from 24.15501
196/196 - 64s - loss: 31.6096 - MinusLogProbMetric: 31.6096 - val_loss: 31.2657 - val_MinusLogProbMetric: 31.2657 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 64/1000
2023-09-26 17:06:45.775 
Epoch 64/1000 
	 loss: 31.3534, MinusLogProbMetric: 31.3534, val_loss: 31.1433, val_MinusLogProbMetric: 31.1433

Epoch 64: val_loss did not improve from 24.15501
196/196 - 64s - loss: 31.3534 - MinusLogProbMetric: 31.3534 - val_loss: 31.1433 - val_MinusLogProbMetric: 31.1433 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 65/1000
2023-09-26 17:07:49.959 
Epoch 65/1000 
	 loss: 31.4188, MinusLogProbMetric: 31.4188, val_loss: 30.8815, val_MinusLogProbMetric: 30.8815

Epoch 65: val_loss did not improve from 24.15501
196/196 - 64s - loss: 31.4188 - MinusLogProbMetric: 31.4188 - val_loss: 30.8815 - val_MinusLogProbMetric: 30.8815 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 66/1000
2023-09-26 17:08:53.403 
Epoch 66/1000 
	 loss: 31.3920, MinusLogProbMetric: 31.3920, val_loss: 31.5186, val_MinusLogProbMetric: 31.5186

Epoch 66: val_loss did not improve from 24.15501
196/196 - 63s - loss: 31.3920 - MinusLogProbMetric: 31.3920 - val_loss: 31.5186 - val_MinusLogProbMetric: 31.5186 - lr: 1.1111e-04 - 63s/epoch - 324ms/step
Epoch 67/1000
2023-09-26 17:09:57.424 
Epoch 67/1000 
	 loss: 31.1106, MinusLogProbMetric: 31.1106, val_loss: 30.8797, val_MinusLogProbMetric: 30.8797

Epoch 67: val_loss did not improve from 24.15501
196/196 - 64s - loss: 31.1106 - MinusLogProbMetric: 31.1106 - val_loss: 30.8797 - val_MinusLogProbMetric: 30.8797 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 68/1000
2023-09-26 17:11:01.594 
Epoch 68/1000 
	 loss: 30.8573, MinusLogProbMetric: 30.8573, val_loss: 30.7870, val_MinusLogProbMetric: 30.7870

Epoch 68: val_loss did not improve from 24.15501
196/196 - 64s - loss: 30.8573 - MinusLogProbMetric: 30.8573 - val_loss: 30.7870 - val_MinusLogProbMetric: 30.7870 - lr: 1.1111e-04 - 64s/epoch - 327ms/step
Epoch 69/1000
2023-09-26 17:12:06.141 
Epoch 69/1000 
	 loss: 30.7971, MinusLogProbMetric: 30.7971, val_loss: 32.9071, val_MinusLogProbMetric: 32.9071

Epoch 69: val_loss did not improve from 24.15501
196/196 - 65s - loss: 30.7971 - MinusLogProbMetric: 30.7971 - val_loss: 32.9071 - val_MinusLogProbMetric: 32.9071 - lr: 1.1111e-04 - 65s/epoch - 329ms/step
Epoch 70/1000
2023-09-26 17:13:10.520 
Epoch 70/1000 
	 loss: 30.5904, MinusLogProbMetric: 30.5904, val_loss: 30.1978, val_MinusLogProbMetric: 30.1978

Epoch 70: val_loss did not improve from 24.15501
196/196 - 64s - loss: 30.5904 - MinusLogProbMetric: 30.5904 - val_loss: 30.1978 - val_MinusLogProbMetric: 30.1978 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 71/1000
2023-09-26 17:14:14.374 
Epoch 71/1000 
	 loss: 30.5289, MinusLogProbMetric: 30.5289, val_loss: 30.3745, val_MinusLogProbMetric: 30.3745

Epoch 71: val_loss did not improve from 24.15501
196/196 - 64s - loss: 30.5289 - MinusLogProbMetric: 30.5289 - val_loss: 30.3745 - val_MinusLogProbMetric: 30.3745 - lr: 1.1111e-04 - 64s/epoch - 326ms/step
Epoch 72/1000
2023-09-26 17:15:18.861 
Epoch 72/1000 
	 loss: 30.5127, MinusLogProbMetric: 30.5127, val_loss: 30.5600, val_MinusLogProbMetric: 30.5600

Epoch 72: val_loss did not improve from 24.15501
196/196 - 64s - loss: 30.5127 - MinusLogProbMetric: 30.5127 - val_loss: 30.5600 - val_MinusLogProbMetric: 30.5600 - lr: 1.1111e-04 - 64s/epoch - 329ms/step
Epoch 73/1000
2023-09-26 17:16:23.056 
Epoch 73/1000 
	 loss: 30.3604, MinusLogProbMetric: 30.3604, val_loss: 30.8460, val_MinusLogProbMetric: 30.8460

Epoch 73: val_loss did not improve from 24.15501
196/196 - 64s - loss: 30.3604 - MinusLogProbMetric: 30.3604 - val_loss: 30.8460 - val_MinusLogProbMetric: 30.8460 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 74/1000
2023-09-26 17:17:27.271 
Epoch 74/1000 
	 loss: 30.6323, MinusLogProbMetric: 30.6323, val_loss: 29.8508, val_MinusLogProbMetric: 29.8508

Epoch 74: val_loss did not improve from 24.15501
196/196 - 64s - loss: 30.6323 - MinusLogProbMetric: 30.6323 - val_loss: 29.8508 - val_MinusLogProbMetric: 29.8508 - lr: 1.1111e-04 - 64s/epoch - 328ms/step
Epoch 75/1000
2023-09-26 17:18:31.353 
Epoch 75/1000 
	 loss: 29.6484, MinusLogProbMetric: 29.6484, val_loss: 29.7217, val_MinusLogProbMetric: 29.7217

Epoch 75: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.6484 - MinusLogProbMetric: 29.6484 - val_loss: 29.7217 - val_MinusLogProbMetric: 29.7217 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 76/1000
2023-09-26 17:19:35.497 
Epoch 76/1000 
	 loss: 29.6334, MinusLogProbMetric: 29.6334, val_loss: 29.7293, val_MinusLogProbMetric: 29.7293

Epoch 76: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.6334 - MinusLogProbMetric: 29.6334 - val_loss: 29.7293 - val_MinusLogProbMetric: 29.7293 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 77/1000
2023-09-26 17:20:39.556 
Epoch 77/1000 
	 loss: 29.5480, MinusLogProbMetric: 29.5480, val_loss: 29.5972, val_MinusLogProbMetric: 29.5972

Epoch 77: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.5480 - MinusLogProbMetric: 29.5480 - val_loss: 29.5972 - val_MinusLogProbMetric: 29.5972 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 78/1000
2023-09-26 17:21:43.462 
Epoch 78/1000 
	 loss: 29.5430, MinusLogProbMetric: 29.5430, val_loss: 29.4813, val_MinusLogProbMetric: 29.4813

Epoch 78: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.5430 - MinusLogProbMetric: 29.5430 - val_loss: 29.4813 - val_MinusLogProbMetric: 29.4813 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 79/1000
2023-09-26 17:22:47.338 
Epoch 79/1000 
	 loss: 29.5204, MinusLogProbMetric: 29.5204, val_loss: 29.6709, val_MinusLogProbMetric: 29.6709

Epoch 79: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.5204 - MinusLogProbMetric: 29.5204 - val_loss: 29.6709 - val_MinusLogProbMetric: 29.6709 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 80/1000
2023-09-26 17:23:50.756 
Epoch 80/1000 
	 loss: 29.4793, MinusLogProbMetric: 29.4793, val_loss: 29.4846, val_MinusLogProbMetric: 29.4846

Epoch 80: val_loss did not improve from 24.15501
196/196 - 63s - loss: 29.4793 - MinusLogProbMetric: 29.4793 - val_loss: 29.4846 - val_MinusLogProbMetric: 29.4846 - lr: 5.5556e-05 - 63s/epoch - 324ms/step
Epoch 81/1000
2023-09-26 17:24:54.667 
Epoch 81/1000 
	 loss: 29.5029, MinusLogProbMetric: 29.5029, val_loss: 29.4454, val_MinusLogProbMetric: 29.4454

Epoch 81: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.5029 - MinusLogProbMetric: 29.5029 - val_loss: 29.4454 - val_MinusLogProbMetric: 29.4454 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 82/1000
2023-09-26 17:25:58.455 
Epoch 82/1000 
	 loss: 29.4405, MinusLogProbMetric: 29.4405, val_loss: 29.3132, val_MinusLogProbMetric: 29.3132

Epoch 82: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.4405 - MinusLogProbMetric: 29.4405 - val_loss: 29.3132 - val_MinusLogProbMetric: 29.3132 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 83/1000
2023-09-26 17:27:02.509 
Epoch 83/1000 
	 loss: 29.4055, MinusLogProbMetric: 29.4055, val_loss: 29.3877, val_MinusLogProbMetric: 29.3877

Epoch 83: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.4055 - MinusLogProbMetric: 29.4055 - val_loss: 29.3877 - val_MinusLogProbMetric: 29.3877 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 84/1000
2023-09-26 17:28:06.115 
Epoch 84/1000 
	 loss: 29.4096, MinusLogProbMetric: 29.4096, val_loss: 29.2699, val_MinusLogProbMetric: 29.2699

Epoch 84: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.4096 - MinusLogProbMetric: 29.4096 - val_loss: 29.2699 - val_MinusLogProbMetric: 29.2699 - lr: 5.5556e-05 - 64s/epoch - 324ms/step
Epoch 85/1000
2023-09-26 17:29:10.253 
Epoch 85/1000 
	 loss: 29.3775, MinusLogProbMetric: 29.3775, val_loss: 29.4096, val_MinusLogProbMetric: 29.4096

Epoch 85: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.3775 - MinusLogProbMetric: 29.3775 - val_loss: 29.4096 - val_MinusLogProbMetric: 29.4096 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 86/1000
2023-09-26 17:30:14.285 
Epoch 86/1000 
	 loss: 29.3654, MinusLogProbMetric: 29.3654, val_loss: 29.2709, val_MinusLogProbMetric: 29.2709

Epoch 86: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.3654 - MinusLogProbMetric: 29.3654 - val_loss: 29.2709 - val_MinusLogProbMetric: 29.2709 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 87/1000
2023-09-26 17:31:18.143 
Epoch 87/1000 
	 loss: 29.4223, MinusLogProbMetric: 29.4223, val_loss: 29.7252, val_MinusLogProbMetric: 29.7252

Epoch 87: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.4223 - MinusLogProbMetric: 29.4223 - val_loss: 29.7252 - val_MinusLogProbMetric: 29.7252 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 88/1000
2023-09-26 17:32:22.070 
Epoch 88/1000 
	 loss: 29.3510, MinusLogProbMetric: 29.3510, val_loss: 29.2957, val_MinusLogProbMetric: 29.2957

Epoch 88: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.3510 - MinusLogProbMetric: 29.3510 - val_loss: 29.2957 - val_MinusLogProbMetric: 29.2957 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 89/1000
2023-09-26 17:33:26.275 
Epoch 89/1000 
	 loss: 29.3467, MinusLogProbMetric: 29.3467, val_loss: 29.1815, val_MinusLogProbMetric: 29.1815

Epoch 89: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.3467 - MinusLogProbMetric: 29.3467 - val_loss: 29.1815 - val_MinusLogProbMetric: 29.1815 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 90/1000
2023-09-26 17:34:30.173 
Epoch 90/1000 
	 loss: 29.3512, MinusLogProbMetric: 29.3512, val_loss: 29.4061, val_MinusLogProbMetric: 29.4061

Epoch 90: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.3512 - MinusLogProbMetric: 29.3512 - val_loss: 29.4061 - val_MinusLogProbMetric: 29.4061 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 91/1000
2023-09-26 17:35:33.945 
Epoch 91/1000 
	 loss: 29.7458, MinusLogProbMetric: 29.7458, val_loss: 29.3839, val_MinusLogProbMetric: 29.3839

Epoch 91: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.7458 - MinusLogProbMetric: 29.7458 - val_loss: 29.3839 - val_MinusLogProbMetric: 29.3839 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 92/1000
2023-09-26 17:36:38.124 
Epoch 92/1000 
	 loss: 29.2891, MinusLogProbMetric: 29.2891, val_loss: 29.1546, val_MinusLogProbMetric: 29.1546

Epoch 92: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.2891 - MinusLogProbMetric: 29.2891 - val_loss: 29.1546 - val_MinusLogProbMetric: 29.1546 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 93/1000
2023-09-26 17:37:41.956 
Epoch 93/1000 
	 loss: 29.2309, MinusLogProbMetric: 29.2309, val_loss: 29.0622, val_MinusLogProbMetric: 29.0622

Epoch 93: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.2309 - MinusLogProbMetric: 29.2309 - val_loss: 29.0622 - val_MinusLogProbMetric: 29.0622 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 94/1000
2023-09-26 17:38:45.725 
Epoch 94/1000 
	 loss: 29.2611, MinusLogProbMetric: 29.2611, val_loss: 29.2761, val_MinusLogProbMetric: 29.2761

Epoch 94: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.2611 - MinusLogProbMetric: 29.2611 - val_loss: 29.2761 - val_MinusLogProbMetric: 29.2761 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 95/1000
2023-09-26 17:39:50.198 
Epoch 95/1000 
	 loss: 29.2196, MinusLogProbMetric: 29.2196, val_loss: 29.3549, val_MinusLogProbMetric: 29.3549

Epoch 95: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.2196 - MinusLogProbMetric: 29.2196 - val_loss: 29.3549 - val_MinusLogProbMetric: 29.3549 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 96/1000
2023-09-26 17:40:54.424 
Epoch 96/1000 
	 loss: 29.2143, MinusLogProbMetric: 29.2143, val_loss: 29.3636, val_MinusLogProbMetric: 29.3636

Epoch 96: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.2143 - MinusLogProbMetric: 29.2143 - val_loss: 29.3636 - val_MinusLogProbMetric: 29.3636 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 97/1000
2023-09-26 17:41:58.662 
Epoch 97/1000 
	 loss: 29.3161, MinusLogProbMetric: 29.3161, val_loss: 29.4244, val_MinusLogProbMetric: 29.4244

Epoch 97: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.3161 - MinusLogProbMetric: 29.3161 - val_loss: 29.4244 - val_MinusLogProbMetric: 29.4244 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 98/1000
2023-09-26 17:43:02.487 
Epoch 98/1000 
	 loss: 29.1534, MinusLogProbMetric: 29.1534, val_loss: 29.0775, val_MinusLogProbMetric: 29.0775

Epoch 98: val_loss did not improve from 24.15501
196/196 - 64s - loss: 29.1534 - MinusLogProbMetric: 29.1534 - val_loss: 29.0775 - val_MinusLogProbMetric: 29.0775 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 99/1000
2023-09-26 17:44:07.090 
Epoch 99/1000 
	 loss: 29.2114, MinusLogProbMetric: 29.2114, val_loss: 29.4095, val_MinusLogProbMetric: 29.4095

Epoch 99: val_loss did not improve from 24.15501
196/196 - 65s - loss: 29.2114 - MinusLogProbMetric: 29.2114 - val_loss: 29.4095 - val_MinusLogProbMetric: 29.4095 - lr: 5.5556e-05 - 65s/epoch - 330ms/step
Epoch 100/1000
2023-09-26 17:45:11.080 
Epoch 100/1000 
	 loss: 26.1649, MinusLogProbMetric: 26.1649, val_loss: 22.6303, val_MinusLogProbMetric: 22.6303

Epoch 100: val_loss improved from 24.15501 to 22.63029, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 26.1649 - MinusLogProbMetric: 26.1649 - val_loss: 22.6303 - val_MinusLogProbMetric: 22.6303 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 101/1000
2023-09-26 17:46:15.654 
Epoch 101/1000 
	 loss: 21.2588, MinusLogProbMetric: 21.2588, val_loss: 20.7597, val_MinusLogProbMetric: 20.7597

Epoch 101: val_loss improved from 22.63029 to 20.75972, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 21.2588 - MinusLogProbMetric: 21.2588 - val_loss: 20.7597 - val_MinusLogProbMetric: 20.7597 - lr: 5.5556e-05 - 65s/epoch - 330ms/step
Epoch 102/1000
2023-09-26 17:47:20.542 
Epoch 102/1000 
	 loss: 20.6783, MinusLogProbMetric: 20.6783, val_loss: 20.5943, val_MinusLogProbMetric: 20.5943

Epoch 102: val_loss improved from 20.75972 to 20.59426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 20.6783 - MinusLogProbMetric: 20.6783 - val_loss: 20.5943 - val_MinusLogProbMetric: 20.5943 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 103/1000
2023-09-26 17:48:25.496 
Epoch 103/1000 
	 loss: 20.5442, MinusLogProbMetric: 20.5442, val_loss: 20.8333, val_MinusLogProbMetric: 20.8333

Epoch 103: val_loss did not improve from 20.59426
196/196 - 64s - loss: 20.5442 - MinusLogProbMetric: 20.5442 - val_loss: 20.8333 - val_MinusLogProbMetric: 20.8333 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 104/1000
2023-09-26 17:49:29.466 
Epoch 104/1000 
	 loss: 20.3745, MinusLogProbMetric: 20.3745, val_loss: 20.2900, val_MinusLogProbMetric: 20.2900

Epoch 104: val_loss improved from 20.59426 to 20.29000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 20.3745 - MinusLogProbMetric: 20.3745 - val_loss: 20.2900 - val_MinusLogProbMetric: 20.2900 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 105/1000
2023-09-26 17:50:34.324 
Epoch 105/1000 
	 loss: 25.1100, MinusLogProbMetric: 25.1100, val_loss: 27.6677, val_MinusLogProbMetric: 27.6677

Epoch 105: val_loss did not improve from 20.29000
196/196 - 64s - loss: 25.1100 - MinusLogProbMetric: 25.1100 - val_loss: 27.6677 - val_MinusLogProbMetric: 27.6677 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 106/1000
2023-09-26 17:51:38.395 
Epoch 106/1000 
	 loss: 24.5002, MinusLogProbMetric: 24.5002, val_loss: 23.1643, val_MinusLogProbMetric: 23.1643

Epoch 106: val_loss did not improve from 20.29000
196/196 - 64s - loss: 24.5002 - MinusLogProbMetric: 24.5002 - val_loss: 23.1643 - val_MinusLogProbMetric: 23.1643 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 107/1000
2023-09-26 17:52:42.223 
Epoch 107/1000 
	 loss: 22.6952, MinusLogProbMetric: 22.6952, val_loss: 23.0305, val_MinusLogProbMetric: 23.0305

Epoch 107: val_loss did not improve from 20.29000
196/196 - 64s - loss: 22.6952 - MinusLogProbMetric: 22.6952 - val_loss: 23.0305 - val_MinusLogProbMetric: 23.0305 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 108/1000
2023-09-26 17:53:46.324 
Epoch 108/1000 
	 loss: 22.2436, MinusLogProbMetric: 22.2436, val_loss: 21.8499, val_MinusLogProbMetric: 21.8499

Epoch 108: val_loss did not improve from 20.29000
196/196 - 64s - loss: 22.2436 - MinusLogProbMetric: 22.2436 - val_loss: 21.8499 - val_MinusLogProbMetric: 21.8499 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 109/1000
2023-09-26 17:54:49.972 
Epoch 109/1000 
	 loss: 21.8205, MinusLogProbMetric: 21.8205, val_loss: 21.8214, val_MinusLogProbMetric: 21.8214

Epoch 109: val_loss did not improve from 20.29000
196/196 - 64s - loss: 21.8205 - MinusLogProbMetric: 21.8205 - val_loss: 21.8214 - val_MinusLogProbMetric: 21.8214 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 110/1000
2023-09-26 17:55:53.929 
Epoch 110/1000 
	 loss: 21.5448, MinusLogProbMetric: 21.5448, val_loss: 21.1480, val_MinusLogProbMetric: 21.1480

Epoch 110: val_loss did not improve from 20.29000
196/196 - 64s - loss: 21.5448 - MinusLogProbMetric: 21.5448 - val_loss: 21.1480 - val_MinusLogProbMetric: 21.1480 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 111/1000
2023-09-26 17:56:57.831 
Epoch 111/1000 
	 loss: 21.0957, MinusLogProbMetric: 21.0957, val_loss: 21.1238, val_MinusLogProbMetric: 21.1238

Epoch 111: val_loss did not improve from 20.29000
196/196 - 64s - loss: 21.0957 - MinusLogProbMetric: 21.0957 - val_loss: 21.1238 - val_MinusLogProbMetric: 21.1238 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 112/1000
2023-09-26 17:58:02.081 
Epoch 112/1000 
	 loss: 20.9256, MinusLogProbMetric: 20.9256, val_loss: 20.9497, val_MinusLogProbMetric: 20.9497

Epoch 112: val_loss did not improve from 20.29000
196/196 - 64s - loss: 20.9256 - MinusLogProbMetric: 20.9256 - val_loss: 20.9497 - val_MinusLogProbMetric: 20.9497 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 113/1000
2023-09-26 17:59:06.333 
Epoch 113/1000 
	 loss: 20.8375, MinusLogProbMetric: 20.8375, val_loss: 20.7031, val_MinusLogProbMetric: 20.7031

Epoch 113: val_loss did not improve from 20.29000
196/196 - 64s - loss: 20.8375 - MinusLogProbMetric: 20.8375 - val_loss: 20.7031 - val_MinusLogProbMetric: 20.7031 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 114/1000
2023-09-26 18:00:10.615 
Epoch 114/1000 
	 loss: 21.5579, MinusLogProbMetric: 21.5579, val_loss: 20.8523, val_MinusLogProbMetric: 20.8523

Epoch 114: val_loss did not improve from 20.29000
196/196 - 64s - loss: 21.5579 - MinusLogProbMetric: 21.5579 - val_loss: 20.8523 - val_MinusLogProbMetric: 20.8523 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 115/1000
2023-09-26 18:01:14.812 
Epoch 115/1000 
	 loss: 20.6439, MinusLogProbMetric: 20.6439, val_loss: 20.5022, val_MinusLogProbMetric: 20.5022

Epoch 115: val_loss did not improve from 20.29000
196/196 - 64s - loss: 20.6439 - MinusLogProbMetric: 20.6439 - val_loss: 20.5022 - val_MinusLogProbMetric: 20.5022 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 116/1000
2023-09-26 18:02:18.775 
Epoch 116/1000 
	 loss: 20.5017, MinusLogProbMetric: 20.5017, val_loss: 20.4008, val_MinusLogProbMetric: 20.4008

Epoch 116: val_loss did not improve from 20.29000
196/196 - 64s - loss: 20.5017 - MinusLogProbMetric: 20.5017 - val_loss: 20.4008 - val_MinusLogProbMetric: 20.4008 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 117/1000
2023-09-26 18:03:23.321 
Epoch 117/1000 
	 loss: 20.5320, MinusLogProbMetric: 20.5320, val_loss: 20.3331, val_MinusLogProbMetric: 20.3331

Epoch 117: val_loss did not improve from 20.29000
196/196 - 65s - loss: 20.5320 - MinusLogProbMetric: 20.5320 - val_loss: 20.3331 - val_MinusLogProbMetric: 20.3331 - lr: 5.5556e-05 - 65s/epoch - 329ms/step
Epoch 118/1000
2023-09-26 18:04:27.307 
Epoch 118/1000 
	 loss: 20.3629, MinusLogProbMetric: 20.3629, val_loss: 21.2020, val_MinusLogProbMetric: 21.2020

Epoch 118: val_loss did not improve from 20.29000
196/196 - 64s - loss: 20.3629 - MinusLogProbMetric: 20.3629 - val_loss: 21.2020 - val_MinusLogProbMetric: 21.2020 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 119/1000
2023-09-26 18:05:31.248 
Epoch 119/1000 
	 loss: 20.3414, MinusLogProbMetric: 20.3414, val_loss: 20.1819, val_MinusLogProbMetric: 20.1819

Epoch 119: val_loss improved from 20.29000 to 20.18190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 20.3414 - MinusLogProbMetric: 20.3414 - val_loss: 20.1819 - val_MinusLogProbMetric: 20.1819 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 120/1000
2023-09-26 18:06:36.510 
Epoch 120/1000 
	 loss: 20.2326, MinusLogProbMetric: 20.2326, val_loss: 20.3142, val_MinusLogProbMetric: 20.3142

Epoch 120: val_loss did not improve from 20.18190
196/196 - 64s - loss: 20.2326 - MinusLogProbMetric: 20.2326 - val_loss: 20.3142 - val_MinusLogProbMetric: 20.3142 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 121/1000
2023-09-26 18:07:40.653 
Epoch 121/1000 
	 loss: 20.1275, MinusLogProbMetric: 20.1275, val_loss: 20.0100, val_MinusLogProbMetric: 20.0100

Epoch 121: val_loss improved from 20.18190 to 20.00998, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 20.1275 - MinusLogProbMetric: 20.1275 - val_loss: 20.0100 - val_MinusLogProbMetric: 20.0100 - lr: 5.5556e-05 - 65s/epoch - 332ms/step
Epoch 122/1000
2023-09-26 18:08:45.616 
Epoch 122/1000 
	 loss: 20.1007, MinusLogProbMetric: 20.1007, val_loss: 20.3564, val_MinusLogProbMetric: 20.3564

Epoch 122: val_loss did not improve from 20.00998
196/196 - 64s - loss: 20.1007 - MinusLogProbMetric: 20.1007 - val_loss: 20.3564 - val_MinusLogProbMetric: 20.3564 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 123/1000
2023-09-26 18:09:49.937 
Epoch 123/1000 
	 loss: 20.0468, MinusLogProbMetric: 20.0468, val_loss: 20.1662, val_MinusLogProbMetric: 20.1662

Epoch 123: val_loss did not improve from 20.00998
196/196 - 64s - loss: 20.0468 - MinusLogProbMetric: 20.0468 - val_loss: 20.1662 - val_MinusLogProbMetric: 20.1662 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 124/1000
2023-09-26 18:10:53.318 
Epoch 124/1000 
	 loss: 20.4759, MinusLogProbMetric: 20.4759, val_loss: 20.3283, val_MinusLogProbMetric: 20.3283

Epoch 124: val_loss did not improve from 20.00998
196/196 - 63s - loss: 20.4759 - MinusLogProbMetric: 20.4759 - val_loss: 20.3283 - val_MinusLogProbMetric: 20.3283 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 125/1000
2023-09-26 18:11:56.983 
Epoch 125/1000 
	 loss: 19.9737, MinusLogProbMetric: 19.9737, val_loss: 20.1819, val_MinusLogProbMetric: 20.1819

Epoch 125: val_loss did not improve from 20.00998
196/196 - 64s - loss: 19.9737 - MinusLogProbMetric: 19.9737 - val_loss: 20.1819 - val_MinusLogProbMetric: 20.1819 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 126/1000
2023-09-26 18:13:01.028 
Epoch 126/1000 
	 loss: 20.0123, MinusLogProbMetric: 20.0123, val_loss: 20.1659, val_MinusLogProbMetric: 20.1659

Epoch 126: val_loss did not improve from 20.00998
196/196 - 64s - loss: 20.0123 - MinusLogProbMetric: 20.0123 - val_loss: 20.1659 - val_MinusLogProbMetric: 20.1659 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 127/1000
2023-09-26 18:14:05.104 
Epoch 127/1000 
	 loss: 19.9015, MinusLogProbMetric: 19.9015, val_loss: 20.0632, val_MinusLogProbMetric: 20.0632

Epoch 127: val_loss did not improve from 20.00998
196/196 - 64s - loss: 19.9015 - MinusLogProbMetric: 19.9015 - val_loss: 20.0632 - val_MinusLogProbMetric: 20.0632 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 128/1000
2023-09-26 18:15:08.683 
Epoch 128/1000 
	 loss: 19.8954, MinusLogProbMetric: 19.8954, val_loss: 19.7860, val_MinusLogProbMetric: 19.7860

Epoch 128: val_loss improved from 20.00998 to 19.78599, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 64s - loss: 19.8954 - MinusLogProbMetric: 19.8954 - val_loss: 19.7860 - val_MinusLogProbMetric: 19.7860 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 129/1000
2023-09-26 18:16:13.607 
Epoch 129/1000 
	 loss: 19.8710, MinusLogProbMetric: 19.8710, val_loss: 20.1358, val_MinusLogProbMetric: 20.1358

Epoch 129: val_loss did not improve from 19.78599
196/196 - 64s - loss: 19.8710 - MinusLogProbMetric: 19.8710 - val_loss: 20.1358 - val_MinusLogProbMetric: 20.1358 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 130/1000
2023-09-26 18:17:17.918 
Epoch 130/1000 
	 loss: 32.0408, MinusLogProbMetric: 32.0408, val_loss: 77.2115, val_MinusLogProbMetric: 77.2115

Epoch 130: val_loss did not improve from 19.78599
196/196 - 64s - loss: 32.0408 - MinusLogProbMetric: 32.0408 - val_loss: 77.2115 - val_MinusLogProbMetric: 77.2115 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 131/1000
2023-09-26 18:18:22.388 
Epoch 131/1000 
	 loss: 52.5675, MinusLogProbMetric: 52.5675, val_loss: 41.4563, val_MinusLogProbMetric: 41.4563

Epoch 131: val_loss did not improve from 19.78599
196/196 - 64s - loss: 52.5675 - MinusLogProbMetric: 52.5675 - val_loss: 41.4563 - val_MinusLogProbMetric: 41.4563 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 132/1000
2023-09-26 18:19:26.136 
Epoch 132/1000 
	 loss: 29.6133, MinusLogProbMetric: 29.6133, val_loss: 22.4623, val_MinusLogProbMetric: 22.4623

Epoch 132: val_loss did not improve from 19.78599
196/196 - 64s - loss: 29.6133 - MinusLogProbMetric: 29.6133 - val_loss: 22.4623 - val_MinusLogProbMetric: 22.4623 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 133/1000
2023-09-26 18:20:30.478 
Epoch 133/1000 
	 loss: 22.6482, MinusLogProbMetric: 22.6482, val_loss: 22.1670, val_MinusLogProbMetric: 22.1670

Epoch 133: val_loss did not improve from 19.78599
196/196 - 64s - loss: 22.6482 - MinusLogProbMetric: 22.6482 - val_loss: 22.1670 - val_MinusLogProbMetric: 22.1670 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 134/1000
2023-09-26 18:21:33.953 
Epoch 134/1000 
	 loss: 21.9111, MinusLogProbMetric: 21.9111, val_loss: 21.8377, val_MinusLogProbMetric: 21.8377

Epoch 134: val_loss did not improve from 19.78599
196/196 - 63s - loss: 21.9111 - MinusLogProbMetric: 21.9111 - val_loss: 21.8377 - val_MinusLogProbMetric: 21.8377 - lr: 5.5556e-05 - 63s/epoch - 324ms/step
Epoch 135/1000
2023-09-26 18:22:38.417 
Epoch 135/1000 
	 loss: 21.2852, MinusLogProbMetric: 21.2852, val_loss: 21.2986, val_MinusLogProbMetric: 21.2986

Epoch 135: val_loss did not improve from 19.78599
196/196 - 64s - loss: 21.2852 - MinusLogProbMetric: 21.2852 - val_loss: 21.2986 - val_MinusLogProbMetric: 21.2986 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 136/1000
2023-09-26 18:23:42.699 
Epoch 136/1000 
	 loss: 21.2262, MinusLogProbMetric: 21.2262, val_loss: 21.0512, val_MinusLogProbMetric: 21.0512

Epoch 136: val_loss did not improve from 19.78599
196/196 - 64s - loss: 21.2262 - MinusLogProbMetric: 21.2262 - val_loss: 21.0512 - val_MinusLogProbMetric: 21.0512 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 137/1000
2023-09-26 18:24:47.177 
Epoch 137/1000 
	 loss: 20.9893, MinusLogProbMetric: 20.9893, val_loss: 20.9471, val_MinusLogProbMetric: 20.9471

Epoch 137: val_loss did not improve from 19.78599
196/196 - 64s - loss: 20.9893 - MinusLogProbMetric: 20.9893 - val_loss: 20.9471 - val_MinusLogProbMetric: 20.9471 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 138/1000
2023-09-26 18:25:51.031 
Epoch 138/1000 
	 loss: 26.4888, MinusLogProbMetric: 26.4888, val_loss: 22.3053, val_MinusLogProbMetric: 22.3053

Epoch 138: val_loss did not improve from 19.78599
196/196 - 64s - loss: 26.4888 - MinusLogProbMetric: 26.4888 - val_loss: 22.3053 - val_MinusLogProbMetric: 22.3053 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 139/1000
2023-09-26 18:26:55.421 
Epoch 139/1000 
	 loss: 21.9547, MinusLogProbMetric: 21.9547, val_loss: 24.0787, val_MinusLogProbMetric: 24.0787

Epoch 139: val_loss did not improve from 19.78599
196/196 - 64s - loss: 21.9547 - MinusLogProbMetric: 21.9547 - val_loss: 24.0787 - val_MinusLogProbMetric: 24.0787 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 140/1000
2023-09-26 18:27:59.283 
Epoch 140/1000 
	 loss: 22.0274, MinusLogProbMetric: 22.0274, val_loss: 21.5572, val_MinusLogProbMetric: 21.5572

Epoch 140: val_loss did not improve from 19.78599
196/196 - 64s - loss: 22.0274 - MinusLogProbMetric: 22.0274 - val_loss: 21.5572 - val_MinusLogProbMetric: 21.5572 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 141/1000
2023-09-26 18:29:02.994 
Epoch 141/1000 
	 loss: 21.3848, MinusLogProbMetric: 21.3848, val_loss: 21.0473, val_MinusLogProbMetric: 21.0473

Epoch 141: val_loss did not improve from 19.78599
196/196 - 64s - loss: 21.3848 - MinusLogProbMetric: 21.3848 - val_loss: 21.0473 - val_MinusLogProbMetric: 21.0473 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 142/1000
2023-09-26 18:30:07.007 
Epoch 142/1000 
	 loss: 21.0913, MinusLogProbMetric: 21.0913, val_loss: 21.1207, val_MinusLogProbMetric: 21.1207

Epoch 142: val_loss did not improve from 19.78599
196/196 - 64s - loss: 21.0913 - MinusLogProbMetric: 21.0913 - val_loss: 21.1207 - val_MinusLogProbMetric: 21.1207 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 143/1000
2023-09-26 18:31:11.169 
Epoch 143/1000 
	 loss: 20.9859, MinusLogProbMetric: 20.9859, val_loss: 21.0808, val_MinusLogProbMetric: 21.0808

Epoch 143: val_loss did not improve from 19.78599
196/196 - 64s - loss: 20.9859 - MinusLogProbMetric: 20.9859 - val_loss: 21.0808 - val_MinusLogProbMetric: 21.0808 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 144/1000
2023-09-26 18:32:14.946 
Epoch 144/1000 
	 loss: 20.8152, MinusLogProbMetric: 20.8152, val_loss: 20.7728, val_MinusLogProbMetric: 20.7728

Epoch 144: val_loss did not improve from 19.78599
196/196 - 64s - loss: 20.8152 - MinusLogProbMetric: 20.8152 - val_loss: 20.7728 - val_MinusLogProbMetric: 20.7728 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 145/1000
2023-09-26 18:33:18.994 
Epoch 145/1000 
	 loss: 20.7893, MinusLogProbMetric: 20.7893, val_loss: 20.8431, val_MinusLogProbMetric: 20.8431

Epoch 145: val_loss did not improve from 19.78599
196/196 - 64s - loss: 20.7893 - MinusLogProbMetric: 20.7893 - val_loss: 20.8431 - val_MinusLogProbMetric: 20.8431 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 146/1000
2023-09-26 18:34:23.328 
Epoch 146/1000 
	 loss: 20.6726, MinusLogProbMetric: 20.6726, val_loss: 20.4751, val_MinusLogProbMetric: 20.4751

Epoch 146: val_loss did not improve from 19.78599
196/196 - 64s - loss: 20.6726 - MinusLogProbMetric: 20.6726 - val_loss: 20.4751 - val_MinusLogProbMetric: 20.4751 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 147/1000
2023-09-26 18:35:27.377 
Epoch 147/1000 
	 loss: 20.4518, MinusLogProbMetric: 20.4518, val_loss: 21.1219, val_MinusLogProbMetric: 21.1219

Epoch 147: val_loss did not improve from 19.78599
196/196 - 64s - loss: 20.4518 - MinusLogProbMetric: 20.4518 - val_loss: 21.1219 - val_MinusLogProbMetric: 21.1219 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 148/1000
2023-09-26 18:36:30.997 
Epoch 148/1000 
	 loss: 20.6412, MinusLogProbMetric: 20.6412, val_loss: 20.1558, val_MinusLogProbMetric: 20.1558

Epoch 148: val_loss did not improve from 19.78599
196/196 - 64s - loss: 20.6412 - MinusLogProbMetric: 20.6412 - val_loss: 20.1558 - val_MinusLogProbMetric: 20.1558 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 149/1000
2023-09-26 18:37:34.772 
Epoch 149/1000 
	 loss: 23.4356, MinusLogProbMetric: 23.4356, val_loss: 21.6975, val_MinusLogProbMetric: 21.6975

Epoch 149: val_loss did not improve from 19.78599
196/196 - 64s - loss: 23.4356 - MinusLogProbMetric: 23.4356 - val_loss: 21.6975 - val_MinusLogProbMetric: 21.6975 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 150/1000
2023-09-26 18:38:38.799 
Epoch 150/1000 
	 loss: 20.8404, MinusLogProbMetric: 20.8404, val_loss: 20.7308, val_MinusLogProbMetric: 20.7308

Epoch 150: val_loss did not improve from 19.78599
196/196 - 64s - loss: 20.8404 - MinusLogProbMetric: 20.8404 - val_loss: 20.7308 - val_MinusLogProbMetric: 20.7308 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 151/1000
2023-09-26 18:39:43.144 
Epoch 151/1000 
	 loss: 32.6422, MinusLogProbMetric: 32.6422, val_loss: 27.5123, val_MinusLogProbMetric: 27.5123

Epoch 151: val_loss did not improve from 19.78599
196/196 - 64s - loss: 32.6422 - MinusLogProbMetric: 32.6422 - val_loss: 27.5123 - val_MinusLogProbMetric: 27.5123 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 152/1000
2023-09-26 18:40:47.767 
Epoch 152/1000 
	 loss: 26.4888, MinusLogProbMetric: 26.4888, val_loss: 25.5356, val_MinusLogProbMetric: 25.5356

Epoch 152: val_loss did not improve from 19.78599
196/196 - 65s - loss: 26.4888 - MinusLogProbMetric: 26.4888 - val_loss: 25.5356 - val_MinusLogProbMetric: 25.5356 - lr: 5.5556e-05 - 65s/epoch - 330ms/step
Epoch 153/1000
2023-09-26 18:41:51.911 
Epoch 153/1000 
	 loss: 22.8130, MinusLogProbMetric: 22.8130, val_loss: 21.2191, val_MinusLogProbMetric: 21.2191

Epoch 153: val_loss did not improve from 19.78599
196/196 - 64s - loss: 22.8130 - MinusLogProbMetric: 22.8130 - val_loss: 21.2191 - val_MinusLogProbMetric: 21.2191 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 154/1000
2023-09-26 18:42:55.648 
Epoch 154/1000 
	 loss: 21.2571, MinusLogProbMetric: 21.2571, val_loss: 20.4460, val_MinusLogProbMetric: 20.4460

Epoch 154: val_loss did not improve from 19.78599
196/196 - 64s - loss: 21.2571 - MinusLogProbMetric: 21.2571 - val_loss: 20.4460 - val_MinusLogProbMetric: 20.4460 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 155/1000
2023-09-26 18:43:59.707 
Epoch 155/1000 
	 loss: 20.4797, MinusLogProbMetric: 20.4797, val_loss: 20.0503, val_MinusLogProbMetric: 20.0503

Epoch 155: val_loss did not improve from 19.78599
196/196 - 64s - loss: 20.4797 - MinusLogProbMetric: 20.4797 - val_loss: 20.0503 - val_MinusLogProbMetric: 20.0503 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 156/1000
2023-09-26 18:45:04.302 
Epoch 156/1000 
	 loss: 20.3436, MinusLogProbMetric: 20.3436, val_loss: 20.5231, val_MinusLogProbMetric: 20.5231

Epoch 156: val_loss did not improve from 19.78599
196/196 - 65s - loss: 20.3436 - MinusLogProbMetric: 20.3436 - val_loss: 20.5231 - val_MinusLogProbMetric: 20.5231 - lr: 5.5556e-05 - 65s/epoch - 330ms/step
Epoch 157/1000
2023-09-26 18:46:08.482 
Epoch 157/1000 
	 loss: 19.9615, MinusLogProbMetric: 19.9615, val_loss: 23.6845, val_MinusLogProbMetric: 23.6845

Epoch 157: val_loss did not improve from 19.78599
196/196 - 64s - loss: 19.9615 - MinusLogProbMetric: 19.9615 - val_loss: 23.6845 - val_MinusLogProbMetric: 23.6845 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 158/1000
2023-09-26 18:47:13.099 
Epoch 158/1000 
	 loss: 20.1768, MinusLogProbMetric: 20.1768, val_loss: 19.6711, val_MinusLogProbMetric: 19.6711

Epoch 158: val_loss improved from 19.78599 to 19.67108, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 20.1768 - MinusLogProbMetric: 20.1768 - val_loss: 19.6711 - val_MinusLogProbMetric: 19.6711 - lr: 5.5556e-05 - 65s/epoch - 334ms/step
Epoch 159/1000
2023-09-26 18:48:17.864 
Epoch 159/1000 
	 loss: 19.7089, MinusLogProbMetric: 19.7089, val_loss: 20.2251, val_MinusLogProbMetric: 20.2251

Epoch 159: val_loss did not improve from 19.67108
196/196 - 64s - loss: 19.7089 - MinusLogProbMetric: 19.7089 - val_loss: 20.2251 - val_MinusLogProbMetric: 20.2251 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 160/1000
2023-09-26 18:49:21.817 
Epoch 160/1000 
	 loss: 19.8048, MinusLogProbMetric: 19.8048, val_loss: 20.1736, val_MinusLogProbMetric: 20.1736

Epoch 160: val_loss did not improve from 19.67108
196/196 - 64s - loss: 19.8048 - MinusLogProbMetric: 19.8048 - val_loss: 20.1736 - val_MinusLogProbMetric: 20.1736 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 161/1000
2023-09-26 18:50:26.305 
Epoch 161/1000 
	 loss: 19.9313, MinusLogProbMetric: 19.9313, val_loss: 19.7560, val_MinusLogProbMetric: 19.7560

Epoch 161: val_loss did not improve from 19.67108
196/196 - 64s - loss: 19.9313 - MinusLogProbMetric: 19.9313 - val_loss: 19.7560 - val_MinusLogProbMetric: 19.7560 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 162/1000
2023-09-26 18:51:30.257 
Epoch 162/1000 
	 loss: 20.6691, MinusLogProbMetric: 20.6691, val_loss: 19.9484, val_MinusLogProbMetric: 19.9484

Epoch 162: val_loss did not improve from 19.67108
196/196 - 64s - loss: 20.6691 - MinusLogProbMetric: 20.6691 - val_loss: 19.9484 - val_MinusLogProbMetric: 19.9484 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 163/1000
2023-09-26 18:52:34.389 
Epoch 163/1000 
	 loss: 19.5113, MinusLogProbMetric: 19.5113, val_loss: 19.4312, val_MinusLogProbMetric: 19.4312

Epoch 163: val_loss improved from 19.67108 to 19.43117, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 19.5113 - MinusLogProbMetric: 19.5113 - val_loss: 19.4312 - val_MinusLogProbMetric: 19.4312 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 164/1000
2023-09-26 18:53:38.925 
Epoch 164/1000 
	 loss: 19.5370, MinusLogProbMetric: 19.5370, val_loss: 19.5783, val_MinusLogProbMetric: 19.5783

Epoch 164: val_loss did not improve from 19.43117
196/196 - 64s - loss: 19.5370 - MinusLogProbMetric: 19.5370 - val_loss: 19.5783 - val_MinusLogProbMetric: 19.5783 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 165/1000
2023-09-26 18:54:43.027 
Epoch 165/1000 
	 loss: 19.5509, MinusLogProbMetric: 19.5509, val_loss: 19.7283, val_MinusLogProbMetric: 19.7283

Epoch 165: val_loss did not improve from 19.43117
196/196 - 64s - loss: 19.5509 - MinusLogProbMetric: 19.5509 - val_loss: 19.7283 - val_MinusLogProbMetric: 19.7283 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 166/1000
2023-09-26 18:55:47.333 
Epoch 166/1000 
	 loss: 19.5744, MinusLogProbMetric: 19.5744, val_loss: 19.9057, val_MinusLogProbMetric: 19.9057

Epoch 166: val_loss did not improve from 19.43117
196/196 - 64s - loss: 19.5744 - MinusLogProbMetric: 19.5744 - val_loss: 19.9057 - val_MinusLogProbMetric: 19.9057 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 167/1000
2023-09-26 18:56:51.452 
Epoch 167/1000 
	 loss: 20.7342, MinusLogProbMetric: 20.7342, val_loss: 35.1869, val_MinusLogProbMetric: 35.1869

Epoch 167: val_loss did not improve from 19.43117
196/196 - 64s - loss: 20.7342 - MinusLogProbMetric: 20.7342 - val_loss: 35.1869 - val_MinusLogProbMetric: 35.1869 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 168/1000
2023-09-26 18:57:55.277 
Epoch 168/1000 
	 loss: 22.8031, MinusLogProbMetric: 22.8031, val_loss: 21.0759, val_MinusLogProbMetric: 21.0759

Epoch 168: val_loss did not improve from 19.43117
196/196 - 64s - loss: 22.8031 - MinusLogProbMetric: 22.8031 - val_loss: 21.0759 - val_MinusLogProbMetric: 21.0759 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 169/1000
2023-09-26 18:58:59.146 
Epoch 169/1000 
	 loss: 20.4082, MinusLogProbMetric: 20.4082, val_loss: 20.2234, val_MinusLogProbMetric: 20.2234

Epoch 169: val_loss did not improve from 19.43117
196/196 - 64s - loss: 20.4082 - MinusLogProbMetric: 20.4082 - val_loss: 20.2234 - val_MinusLogProbMetric: 20.2234 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 170/1000
2023-09-26 19:00:03.408 
Epoch 170/1000 
	 loss: 20.0883, MinusLogProbMetric: 20.0883, val_loss: 21.1905, val_MinusLogProbMetric: 21.1905

Epoch 170: val_loss did not improve from 19.43117
196/196 - 64s - loss: 20.0883 - MinusLogProbMetric: 20.0883 - val_loss: 21.1905 - val_MinusLogProbMetric: 21.1905 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 171/1000
2023-09-26 19:01:07.826 
Epoch 171/1000 
	 loss: 20.4063, MinusLogProbMetric: 20.4063, val_loss: 19.7800, val_MinusLogProbMetric: 19.7800

Epoch 171: val_loss did not improve from 19.43117
196/196 - 64s - loss: 20.4063 - MinusLogProbMetric: 20.4063 - val_loss: 19.7800 - val_MinusLogProbMetric: 19.7800 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 172/1000
2023-09-26 19:02:11.971 
Epoch 172/1000 
	 loss: 19.5168, MinusLogProbMetric: 19.5168, val_loss: 19.4325, val_MinusLogProbMetric: 19.4325

Epoch 172: val_loss did not improve from 19.43117
196/196 - 64s - loss: 19.5168 - MinusLogProbMetric: 19.5168 - val_loss: 19.4325 - val_MinusLogProbMetric: 19.4325 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 173/1000
2023-09-26 19:03:16.115 
Epoch 173/1000 
	 loss: 19.5397, MinusLogProbMetric: 19.5397, val_loss: 19.5272, val_MinusLogProbMetric: 19.5272

Epoch 173: val_loss did not improve from 19.43117
196/196 - 64s - loss: 19.5397 - MinusLogProbMetric: 19.5397 - val_loss: 19.5272 - val_MinusLogProbMetric: 19.5272 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 174/1000
2023-09-26 19:04:20.651 
Epoch 174/1000 
	 loss: 19.9529, MinusLogProbMetric: 19.9529, val_loss: 19.4429, val_MinusLogProbMetric: 19.4429

Epoch 174: val_loss did not improve from 19.43117
196/196 - 65s - loss: 19.9529 - MinusLogProbMetric: 19.9529 - val_loss: 19.4429 - val_MinusLogProbMetric: 19.4429 - lr: 5.5556e-05 - 65s/epoch - 329ms/step
Epoch 175/1000
2023-09-26 19:05:25.272 
Epoch 175/1000 
	 loss: 19.7070, MinusLogProbMetric: 19.7070, val_loss: 19.3004, val_MinusLogProbMetric: 19.3004

Epoch 175: val_loss improved from 19.43117 to 19.30042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 66s - loss: 19.7070 - MinusLogProbMetric: 19.7070 - val_loss: 19.3004 - val_MinusLogProbMetric: 19.3004 - lr: 5.5556e-05 - 66s/epoch - 334ms/step
Epoch 176/1000
2023-09-26 19:06:31.070 
Epoch 176/1000 
	 loss: 19.5897, MinusLogProbMetric: 19.5897, val_loss: 19.6601, val_MinusLogProbMetric: 19.6601

Epoch 176: val_loss did not improve from 19.30042
196/196 - 65s - loss: 19.5897 - MinusLogProbMetric: 19.5897 - val_loss: 19.6601 - val_MinusLogProbMetric: 19.6601 - lr: 5.5556e-05 - 65s/epoch - 331ms/step
Epoch 177/1000
2023-09-26 19:07:34.970 
Epoch 177/1000 
	 loss: 35.6722, MinusLogProbMetric: 35.6722, val_loss: 51.4402, val_MinusLogProbMetric: 51.4402

Epoch 177: val_loss did not improve from 19.30042
196/196 - 64s - loss: 35.6722 - MinusLogProbMetric: 35.6722 - val_loss: 51.4402 - val_MinusLogProbMetric: 51.4402 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 178/1000
2023-09-26 19:08:38.907 
Epoch 178/1000 
	 loss: 44.6513, MinusLogProbMetric: 44.6513, val_loss: 40.5670, val_MinusLogProbMetric: 40.5670

Epoch 178: val_loss did not improve from 19.30042
196/196 - 64s - loss: 44.6513 - MinusLogProbMetric: 44.6513 - val_loss: 40.5670 - val_MinusLogProbMetric: 40.5670 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 179/1000
2023-09-26 19:09:42.999 
Epoch 179/1000 
	 loss: 35.3412, MinusLogProbMetric: 35.3412, val_loss: 27.8186, val_MinusLogProbMetric: 27.8186

Epoch 179: val_loss did not improve from 19.30042
196/196 - 64s - loss: 35.3412 - MinusLogProbMetric: 35.3412 - val_loss: 27.8186 - val_MinusLogProbMetric: 27.8186 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 180/1000
2023-09-26 19:10:47.431 
Epoch 180/1000 
	 loss: 26.7278, MinusLogProbMetric: 26.7278, val_loss: 26.0450, val_MinusLogProbMetric: 26.0450

Epoch 180: val_loss did not improve from 19.30042
196/196 - 64s - loss: 26.7278 - MinusLogProbMetric: 26.7278 - val_loss: 26.0450 - val_MinusLogProbMetric: 26.0450 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 181/1000
2023-09-26 19:11:51.549 
Epoch 181/1000 
	 loss: 25.7552, MinusLogProbMetric: 25.7552, val_loss: 25.4104, val_MinusLogProbMetric: 25.4104

Epoch 181: val_loss did not improve from 19.30042
196/196 - 64s - loss: 25.7552 - MinusLogProbMetric: 25.7552 - val_loss: 25.4104 - val_MinusLogProbMetric: 25.4104 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 182/1000
2023-09-26 19:12:56.086 
Epoch 182/1000 
	 loss: 25.0922, MinusLogProbMetric: 25.0922, val_loss: 25.0612, val_MinusLogProbMetric: 25.0612

Epoch 182: val_loss did not improve from 19.30042
196/196 - 65s - loss: 25.0922 - MinusLogProbMetric: 25.0922 - val_loss: 25.0612 - val_MinusLogProbMetric: 25.0612 - lr: 5.5556e-05 - 65s/epoch - 329ms/step
Epoch 183/1000
2023-09-26 19:14:00.508 
Epoch 183/1000 
	 loss: 24.7586, MinusLogProbMetric: 24.7586, val_loss: 24.2885, val_MinusLogProbMetric: 24.2885

Epoch 183: val_loss did not improve from 19.30042
196/196 - 64s - loss: 24.7586 - MinusLogProbMetric: 24.7586 - val_loss: 24.2885 - val_MinusLogProbMetric: 24.2885 - lr: 5.5556e-05 - 64s/epoch - 329ms/step
Epoch 184/1000
2023-09-26 19:15:04.537 
Epoch 184/1000 
	 loss: 24.1661, MinusLogProbMetric: 24.1661, val_loss: 24.1255, val_MinusLogProbMetric: 24.1255

Epoch 184: val_loss did not improve from 19.30042
196/196 - 64s - loss: 24.1661 - MinusLogProbMetric: 24.1661 - val_loss: 24.1255 - val_MinusLogProbMetric: 24.1255 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 185/1000
2023-09-26 19:16:08.689 
Epoch 185/1000 
	 loss: 24.5538, MinusLogProbMetric: 24.5538, val_loss: 25.0819, val_MinusLogProbMetric: 25.0819

Epoch 185: val_loss did not improve from 19.30042
196/196 - 64s - loss: 24.5538 - MinusLogProbMetric: 24.5538 - val_loss: 25.0819 - val_MinusLogProbMetric: 25.0819 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 186/1000
2023-09-26 19:17:13.018 
Epoch 186/1000 
	 loss: 24.3012, MinusLogProbMetric: 24.3012, val_loss: 23.6740, val_MinusLogProbMetric: 23.6740

Epoch 186: val_loss did not improve from 19.30042
196/196 - 64s - loss: 24.3012 - MinusLogProbMetric: 24.3012 - val_loss: 23.6740 - val_MinusLogProbMetric: 23.6740 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 187/1000
2023-09-26 19:18:15.986 
Epoch 187/1000 
	 loss: 24.3069, MinusLogProbMetric: 24.3069, val_loss: 26.4876, val_MinusLogProbMetric: 26.4876

Epoch 187: val_loss did not improve from 19.30042
196/196 - 63s - loss: 24.3069 - MinusLogProbMetric: 24.3069 - val_loss: 26.4876 - val_MinusLogProbMetric: 26.4876 - lr: 5.5556e-05 - 63s/epoch - 321ms/step
Epoch 188/1000
2023-09-26 19:19:12.964 
Epoch 188/1000 
	 loss: 23.5939, MinusLogProbMetric: 23.5939, val_loss: 23.9562, val_MinusLogProbMetric: 23.9562

Epoch 188: val_loss did not improve from 19.30042
196/196 - 57s - loss: 23.5939 - MinusLogProbMetric: 23.5939 - val_loss: 23.9562 - val_MinusLogProbMetric: 23.9562 - lr: 5.5556e-05 - 57s/epoch - 291ms/step
Epoch 189/1000
2023-09-26 19:20:06.724 
Epoch 189/1000 
	 loss: 24.4849, MinusLogProbMetric: 24.4849, val_loss: 23.6923, val_MinusLogProbMetric: 23.6923

Epoch 189: val_loss did not improve from 19.30042
196/196 - 54s - loss: 24.4849 - MinusLogProbMetric: 24.4849 - val_loss: 23.6923 - val_MinusLogProbMetric: 23.6923 - lr: 5.5556e-05 - 54s/epoch - 274ms/step
Epoch 190/1000
2023-09-26 19:21:08.988 
Epoch 190/1000 
	 loss: 23.2531, MinusLogProbMetric: 23.2531, val_loss: 28.0753, val_MinusLogProbMetric: 28.0753

Epoch 190: val_loss did not improve from 19.30042
196/196 - 62s - loss: 23.2531 - MinusLogProbMetric: 23.2531 - val_loss: 28.0753 - val_MinusLogProbMetric: 28.0753 - lr: 5.5556e-05 - 62s/epoch - 318ms/step
Epoch 191/1000
2023-09-26 19:22:06.161 
Epoch 191/1000 
	 loss: 23.7663, MinusLogProbMetric: 23.7663, val_loss: 22.9035, val_MinusLogProbMetric: 22.9035

Epoch 191: val_loss did not improve from 19.30042
196/196 - 57s - loss: 23.7663 - MinusLogProbMetric: 23.7663 - val_loss: 22.9035 - val_MinusLogProbMetric: 22.9035 - lr: 5.5556e-05 - 57s/epoch - 292ms/step
Epoch 192/1000
2023-09-26 19:23:01.374 
Epoch 192/1000 
	 loss: 23.8514, MinusLogProbMetric: 23.8514, val_loss: 22.6990, val_MinusLogProbMetric: 22.6990

Epoch 192: val_loss did not improve from 19.30042
196/196 - 55s - loss: 23.8514 - MinusLogProbMetric: 23.8514 - val_loss: 22.6990 - val_MinusLogProbMetric: 22.6990 - lr: 5.5556e-05 - 55s/epoch - 282ms/step
Epoch 193/1000
2023-09-26 19:23:59.150 
Epoch 193/1000 
	 loss: 23.9610, MinusLogProbMetric: 23.9610, val_loss: 23.3747, val_MinusLogProbMetric: 23.3747

Epoch 193: val_loss did not improve from 19.30042
196/196 - 58s - loss: 23.9610 - MinusLogProbMetric: 23.9610 - val_loss: 23.3747 - val_MinusLogProbMetric: 23.3747 - lr: 5.5556e-05 - 58s/epoch - 295ms/step
Epoch 194/1000
2023-09-26 19:25:02.672 
Epoch 194/1000 
	 loss: 22.7753, MinusLogProbMetric: 22.7753, val_loss: 23.1207, val_MinusLogProbMetric: 23.1207

Epoch 194: val_loss did not improve from 19.30042
196/196 - 64s - loss: 22.7753 - MinusLogProbMetric: 22.7753 - val_loss: 23.1207 - val_MinusLogProbMetric: 23.1207 - lr: 5.5556e-05 - 64s/epoch - 324ms/step
Epoch 195/1000
2023-09-26 19:26:05.160 
Epoch 195/1000 
	 loss: 23.8629, MinusLogProbMetric: 23.8629, val_loss: 22.4725, val_MinusLogProbMetric: 22.4725

Epoch 195: val_loss did not improve from 19.30042
196/196 - 62s - loss: 23.8629 - MinusLogProbMetric: 23.8629 - val_loss: 22.4725 - val_MinusLogProbMetric: 22.4725 - lr: 5.5556e-05 - 62s/epoch - 319ms/step
Epoch 196/1000
2023-09-26 19:27:08.508 
Epoch 196/1000 
	 loss: 23.7976, MinusLogProbMetric: 23.7976, val_loss: 24.6727, val_MinusLogProbMetric: 24.6727

Epoch 196: val_loss did not improve from 19.30042
196/196 - 63s - loss: 23.7976 - MinusLogProbMetric: 23.7976 - val_loss: 24.6727 - val_MinusLogProbMetric: 24.6727 - lr: 5.5556e-05 - 63s/epoch - 323ms/step
Epoch 197/1000
2023-09-26 19:28:10.359 
Epoch 197/1000 
	 loss: 22.4835, MinusLogProbMetric: 22.4835, val_loss: 22.0297, val_MinusLogProbMetric: 22.0297

Epoch 197: val_loss did not improve from 19.30042
196/196 - 62s - loss: 22.4835 - MinusLogProbMetric: 22.4835 - val_loss: 22.0297 - val_MinusLogProbMetric: 22.0297 - lr: 5.5556e-05 - 62s/epoch - 316ms/step
Epoch 198/1000
2023-09-26 19:29:04.251 
Epoch 198/1000 
	 loss: 23.4163, MinusLogProbMetric: 23.4163, val_loss: 22.1206, val_MinusLogProbMetric: 22.1206

Epoch 198: val_loss did not improve from 19.30042
196/196 - 54s - loss: 23.4163 - MinusLogProbMetric: 23.4163 - val_loss: 22.1206 - val_MinusLogProbMetric: 22.1206 - lr: 5.5556e-05 - 54s/epoch - 275ms/step
Epoch 199/1000
2023-09-26 19:30:02.354 
Epoch 199/1000 
	 loss: 22.1698, MinusLogProbMetric: 22.1698, val_loss: 23.0421, val_MinusLogProbMetric: 23.0421

Epoch 199: val_loss did not improve from 19.30042
196/196 - 58s - loss: 22.1698 - MinusLogProbMetric: 22.1698 - val_loss: 23.0421 - val_MinusLogProbMetric: 23.0421 - lr: 5.5556e-05 - 58s/epoch - 296ms/step
Epoch 200/1000
2023-09-26 19:31:06.163 
Epoch 200/1000 
	 loss: 26.5969, MinusLogProbMetric: 26.5969, val_loss: 23.1701, val_MinusLogProbMetric: 23.1701

Epoch 200: val_loss did not improve from 19.30042
196/196 - 64s - loss: 26.5969 - MinusLogProbMetric: 26.5969 - val_loss: 23.1701 - val_MinusLogProbMetric: 23.1701 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 201/1000
2023-09-26 19:32:07.870 
Epoch 201/1000 
	 loss: 23.5535, MinusLogProbMetric: 23.5535, val_loss: 23.3027, val_MinusLogProbMetric: 23.3027

Epoch 201: val_loss did not improve from 19.30042
196/196 - 62s - loss: 23.5535 - MinusLogProbMetric: 23.5535 - val_loss: 23.3027 - val_MinusLogProbMetric: 23.3027 - lr: 5.5556e-05 - 62s/epoch - 315ms/step
Epoch 202/1000
2023-09-26 19:33:11.657 
Epoch 202/1000 
	 loss: 23.3025, MinusLogProbMetric: 23.3025, val_loss: 23.1130, val_MinusLogProbMetric: 23.1130

Epoch 202: val_loss did not improve from 19.30042
196/196 - 64s - loss: 23.3025 - MinusLogProbMetric: 23.3025 - val_loss: 23.1130 - val_MinusLogProbMetric: 23.1130 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 203/1000
2023-09-26 19:34:15.367 
Epoch 203/1000 
	 loss: 23.8161, MinusLogProbMetric: 23.8161, val_loss: 23.7954, val_MinusLogProbMetric: 23.7954

Epoch 203: val_loss did not improve from 19.30042
196/196 - 64s - loss: 23.8161 - MinusLogProbMetric: 23.8161 - val_loss: 23.7954 - val_MinusLogProbMetric: 23.7954 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 204/1000
2023-09-26 19:35:19.211 
Epoch 204/1000 
	 loss: 22.2423, MinusLogProbMetric: 22.2423, val_loss: 21.9230, val_MinusLogProbMetric: 21.9230

Epoch 204: val_loss did not improve from 19.30042
196/196 - 64s - loss: 22.2423 - MinusLogProbMetric: 22.2423 - val_loss: 21.9230 - val_MinusLogProbMetric: 21.9230 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 205/1000
2023-09-26 19:36:22.983 
Epoch 205/1000 
	 loss: 23.7559, MinusLogProbMetric: 23.7559, val_loss: 21.8467, val_MinusLogProbMetric: 21.8467

Epoch 205: val_loss did not improve from 19.30042
196/196 - 64s - loss: 23.7559 - MinusLogProbMetric: 23.7559 - val_loss: 21.8467 - val_MinusLogProbMetric: 21.8467 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 206/1000
2023-09-26 19:37:26.610 
Epoch 206/1000 
	 loss: 22.7013, MinusLogProbMetric: 22.7013, val_loss: 24.2911, val_MinusLogProbMetric: 24.2911

Epoch 206: val_loss did not improve from 19.30042
196/196 - 64s - loss: 22.7013 - MinusLogProbMetric: 22.7013 - val_loss: 24.2911 - val_MinusLogProbMetric: 24.2911 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 207/1000
2023-09-26 19:38:30.169 
Epoch 207/1000 
	 loss: 22.0354, MinusLogProbMetric: 22.0354, val_loss: 21.5810, val_MinusLogProbMetric: 21.5810

Epoch 207: val_loss did not improve from 19.30042
196/196 - 64s - loss: 22.0354 - MinusLogProbMetric: 22.0354 - val_loss: 21.5810 - val_MinusLogProbMetric: 21.5810 - lr: 5.5556e-05 - 64s/epoch - 324ms/step
Epoch 208/1000
2023-09-26 19:39:33.792 
Epoch 208/1000 
	 loss: 23.0971, MinusLogProbMetric: 23.0971, val_loss: 21.5512, val_MinusLogProbMetric: 21.5512

Epoch 208: val_loss did not improve from 19.30042
196/196 - 64s - loss: 23.0971 - MinusLogProbMetric: 23.0971 - val_loss: 21.5512 - val_MinusLogProbMetric: 21.5512 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 209/1000
2023-09-26 19:40:37.842 
Epoch 209/1000 
	 loss: 21.3358, MinusLogProbMetric: 21.3358, val_loss: 22.6025, val_MinusLogProbMetric: 22.6025

Epoch 209: val_loss did not improve from 19.30042
196/196 - 64s - loss: 21.3358 - MinusLogProbMetric: 21.3358 - val_loss: 22.6025 - val_MinusLogProbMetric: 22.6025 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 210/1000
2023-09-26 19:41:41.649 
Epoch 210/1000 
	 loss: 22.4647, MinusLogProbMetric: 22.4647, val_loss: 21.1186, val_MinusLogProbMetric: 21.1186

Epoch 210: val_loss did not improve from 19.30042
196/196 - 64s - loss: 22.4647 - MinusLogProbMetric: 22.4647 - val_loss: 21.1186 - val_MinusLogProbMetric: 21.1186 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 211/1000
2023-09-26 19:42:45.394 
Epoch 211/1000 
	 loss: 20.4350, MinusLogProbMetric: 20.4350, val_loss: 20.1161, val_MinusLogProbMetric: 20.1161

Epoch 211: val_loss did not improve from 19.30042
196/196 - 64s - loss: 20.4350 - MinusLogProbMetric: 20.4350 - val_loss: 20.1161 - val_MinusLogProbMetric: 20.1161 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 212/1000
2023-09-26 19:43:49.297 
Epoch 212/1000 
	 loss: 20.0425, MinusLogProbMetric: 20.0425, val_loss: 19.9831, val_MinusLogProbMetric: 19.9831

Epoch 212: val_loss did not improve from 19.30042
196/196 - 64s - loss: 20.0425 - MinusLogProbMetric: 20.0425 - val_loss: 19.9831 - val_MinusLogProbMetric: 19.9831 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 213/1000
2023-09-26 19:44:53.061 
Epoch 213/1000 
	 loss: 19.9354, MinusLogProbMetric: 19.9354, val_loss: 19.8555, val_MinusLogProbMetric: 19.8555

Epoch 213: val_loss did not improve from 19.30042
196/196 - 64s - loss: 19.9354 - MinusLogProbMetric: 19.9354 - val_loss: 19.8555 - val_MinusLogProbMetric: 19.8555 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 214/1000
2023-09-26 19:45:56.725 
Epoch 214/1000 
	 loss: 19.8977, MinusLogProbMetric: 19.8977, val_loss: 19.9170, val_MinusLogProbMetric: 19.9170

Epoch 214: val_loss did not improve from 19.30042
196/196 - 64s - loss: 19.8977 - MinusLogProbMetric: 19.8977 - val_loss: 19.9170 - val_MinusLogProbMetric: 19.9170 - lr: 5.5556e-05 - 64s/epoch - 325ms/step
Epoch 215/1000
2023-09-26 19:47:01.019 
Epoch 215/1000 
	 loss: 19.9024, MinusLogProbMetric: 19.9024, val_loss: 19.8108, val_MinusLogProbMetric: 19.8108

Epoch 215: val_loss did not improve from 19.30042
196/196 - 64s - loss: 19.9024 - MinusLogProbMetric: 19.9024 - val_loss: 19.8108 - val_MinusLogProbMetric: 19.8108 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 216/1000
2023-09-26 19:48:04.879 
Epoch 216/1000 
	 loss: 19.6668, MinusLogProbMetric: 19.6668, val_loss: 19.5888, val_MinusLogProbMetric: 19.5888

Epoch 216: val_loss did not improve from 19.30042
196/196 - 64s - loss: 19.6668 - MinusLogProbMetric: 19.6668 - val_loss: 19.5888 - val_MinusLogProbMetric: 19.5888 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 217/1000
2023-09-26 19:49:09.233 
Epoch 217/1000 
	 loss: 19.6395, MinusLogProbMetric: 19.6395, val_loss: 19.5242, val_MinusLogProbMetric: 19.5242

Epoch 217: val_loss did not improve from 19.30042
196/196 - 64s - loss: 19.6395 - MinusLogProbMetric: 19.6395 - val_loss: 19.5242 - val_MinusLogProbMetric: 19.5242 - lr: 5.5556e-05 - 64s/epoch - 328ms/step
Epoch 218/1000
2023-09-26 19:50:13.149 
Epoch 218/1000 
	 loss: 19.5725, MinusLogProbMetric: 19.5725, val_loss: 19.9234, val_MinusLogProbMetric: 19.9234

Epoch 218: val_loss did not improve from 19.30042
196/196 - 64s - loss: 19.5725 - MinusLogProbMetric: 19.5725 - val_loss: 19.9234 - val_MinusLogProbMetric: 19.9234 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 219/1000
2023-09-26 19:51:17.300 
Epoch 219/1000 
	 loss: 19.5904, MinusLogProbMetric: 19.5904, val_loss: 19.4934, val_MinusLogProbMetric: 19.4934

Epoch 219: val_loss did not improve from 19.30042
196/196 - 64s - loss: 19.5904 - MinusLogProbMetric: 19.5904 - val_loss: 19.4934 - val_MinusLogProbMetric: 19.4934 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 220/1000
2023-09-26 19:52:21.463 
Epoch 220/1000 
	 loss: 19.5638, MinusLogProbMetric: 19.5638, val_loss: 19.9427, val_MinusLogProbMetric: 19.9427

Epoch 220: val_loss did not improve from 19.30042
196/196 - 64s - loss: 19.5638 - MinusLogProbMetric: 19.5638 - val_loss: 19.9427 - val_MinusLogProbMetric: 19.9427 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 221/1000
2023-09-26 19:53:25.618 
Epoch 221/1000 
	 loss: 19.5858, MinusLogProbMetric: 19.5858, val_loss: 19.5204, val_MinusLogProbMetric: 19.5204

Epoch 221: val_loss did not improve from 19.30042
196/196 - 64s - loss: 19.5858 - MinusLogProbMetric: 19.5858 - val_loss: 19.5204 - val_MinusLogProbMetric: 19.5204 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 222/1000
2023-09-26 19:54:30.195 
Epoch 222/1000 
	 loss: 19.5337, MinusLogProbMetric: 19.5337, val_loss: 19.3806, val_MinusLogProbMetric: 19.3806

Epoch 222: val_loss did not improve from 19.30042
196/196 - 65s - loss: 19.5337 - MinusLogProbMetric: 19.5337 - val_loss: 19.3806 - val_MinusLogProbMetric: 19.3806 - lr: 5.5556e-05 - 65s/epoch - 329ms/step
Epoch 223/1000
2023-09-26 19:55:34.227 
Epoch 223/1000 
	 loss: 19.4979, MinusLogProbMetric: 19.4979, val_loss: 19.6438, val_MinusLogProbMetric: 19.6438

Epoch 223: val_loss did not improve from 19.30042
196/196 - 64s - loss: 19.4979 - MinusLogProbMetric: 19.4979 - val_loss: 19.6438 - val_MinusLogProbMetric: 19.6438 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 224/1000
2023-09-26 19:56:38.404 
Epoch 224/1000 
	 loss: 19.3942, MinusLogProbMetric: 19.3942, val_loss: 19.7504, val_MinusLogProbMetric: 19.7504

Epoch 224: val_loss did not improve from 19.30042
196/196 - 64s - loss: 19.3942 - MinusLogProbMetric: 19.3942 - val_loss: 19.7504 - val_MinusLogProbMetric: 19.7504 - lr: 5.5556e-05 - 64s/epoch - 327ms/step
Epoch 225/1000
2023-09-26 19:57:42.335 
Epoch 225/1000 
	 loss: 19.4676, MinusLogProbMetric: 19.4676, val_loss: 19.6607, val_MinusLogProbMetric: 19.6607

Epoch 225: val_loss did not improve from 19.30042
196/196 - 64s - loss: 19.4676 - MinusLogProbMetric: 19.4676 - val_loss: 19.6607 - val_MinusLogProbMetric: 19.6607 - lr: 5.5556e-05 - 64s/epoch - 326ms/step
Epoch 226/1000
2023-09-26 19:58:46.475 
Epoch 226/1000 
	 loss: 19.0561, MinusLogProbMetric: 19.0561, val_loss: 19.0158, val_MinusLogProbMetric: 19.0158

Epoch 226: val_loss improved from 19.30042 to 19.01582, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 19.0561 - MinusLogProbMetric: 19.0561 - val_loss: 19.0158 - val_MinusLogProbMetric: 19.0158 - lr: 2.7778e-05 - 65s/epoch - 331ms/step
Epoch 227/1000
2023-09-26 19:59:51.119 
Epoch 227/1000 
	 loss: 19.0327, MinusLogProbMetric: 19.0327, val_loss: 19.0341, val_MinusLogProbMetric: 19.0341

Epoch 227: val_loss did not improve from 19.01582
196/196 - 64s - loss: 19.0327 - MinusLogProbMetric: 19.0327 - val_loss: 19.0341 - val_MinusLogProbMetric: 19.0341 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 228/1000
2023-09-26 20:00:55.489 
Epoch 228/1000 
	 loss: 19.0329, MinusLogProbMetric: 19.0329, val_loss: 18.9948, val_MinusLogProbMetric: 18.9948

Epoch 228: val_loss improved from 19.01582 to 18.99480, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 19.0329 - MinusLogProbMetric: 19.0329 - val_loss: 18.9948 - val_MinusLogProbMetric: 18.9948 - lr: 2.7778e-05 - 65s/epoch - 332ms/step
Epoch 229/1000
2023-09-26 20:01:59.309 
Epoch 229/1000 
	 loss: 19.0419, MinusLogProbMetric: 19.0419, val_loss: 19.0613, val_MinusLogProbMetric: 19.0613

Epoch 229: val_loss did not improve from 18.99480
196/196 - 63s - loss: 19.0419 - MinusLogProbMetric: 19.0419 - val_loss: 19.0613 - val_MinusLogProbMetric: 19.0613 - lr: 2.7778e-05 - 63s/epoch - 322ms/step
Epoch 230/1000
2023-09-26 20:03:03.337 
Epoch 230/1000 
	 loss: 19.0449, MinusLogProbMetric: 19.0449, val_loss: 18.9328, val_MinusLogProbMetric: 18.9328

Epoch 230: val_loss improved from 18.99480 to 18.93278, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 19.0449 - MinusLogProbMetric: 19.0449 - val_loss: 18.9328 - val_MinusLogProbMetric: 18.9328 - lr: 2.7778e-05 - 65s/epoch - 332ms/step
Epoch 231/1000
2023-09-26 20:04:08.437 
Epoch 231/1000 
	 loss: 19.0458, MinusLogProbMetric: 19.0458, val_loss: 19.3332, val_MinusLogProbMetric: 19.3332

Epoch 231: val_loss did not improve from 18.93278
196/196 - 64s - loss: 19.0458 - MinusLogProbMetric: 19.0458 - val_loss: 19.3332 - val_MinusLogProbMetric: 19.3332 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 232/1000
2023-09-26 20:05:11.831 
Epoch 232/1000 
	 loss: 19.0030, MinusLogProbMetric: 19.0030, val_loss: 18.9534, val_MinusLogProbMetric: 18.9534

Epoch 232: val_loss did not improve from 18.93278
196/196 - 63s - loss: 19.0030 - MinusLogProbMetric: 19.0030 - val_loss: 18.9534 - val_MinusLogProbMetric: 18.9534 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 233/1000
2023-09-26 20:06:15.976 
Epoch 233/1000 
	 loss: 19.0478, MinusLogProbMetric: 19.0478, val_loss: 19.0784, val_MinusLogProbMetric: 19.0784

Epoch 233: val_loss did not improve from 18.93278
196/196 - 64s - loss: 19.0478 - MinusLogProbMetric: 19.0478 - val_loss: 19.0784 - val_MinusLogProbMetric: 19.0784 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 234/1000
2023-09-26 20:07:20.037 
Epoch 234/1000 
	 loss: 18.9846, MinusLogProbMetric: 18.9846, val_loss: 19.1351, val_MinusLogProbMetric: 19.1351

Epoch 234: val_loss did not improve from 18.93278
196/196 - 64s - loss: 18.9846 - MinusLogProbMetric: 18.9846 - val_loss: 19.1351 - val_MinusLogProbMetric: 19.1351 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 235/1000
2023-09-26 20:08:23.811 
Epoch 235/1000 
	 loss: 18.9713, MinusLogProbMetric: 18.9713, val_loss: 18.9985, val_MinusLogProbMetric: 18.9985

Epoch 235: val_loss did not improve from 18.93278
196/196 - 64s - loss: 18.9713 - MinusLogProbMetric: 18.9713 - val_loss: 18.9985 - val_MinusLogProbMetric: 18.9985 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 236/1000
2023-09-26 20:09:27.893 
Epoch 236/1000 
	 loss: 19.0046, MinusLogProbMetric: 19.0046, val_loss: 19.0125, val_MinusLogProbMetric: 19.0125

Epoch 236: val_loss did not improve from 18.93278
196/196 - 64s - loss: 19.0046 - MinusLogProbMetric: 19.0046 - val_loss: 19.0125 - val_MinusLogProbMetric: 19.0125 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 237/1000
2023-09-26 20:10:32.408 
Epoch 237/1000 
	 loss: 18.9743, MinusLogProbMetric: 18.9743, val_loss: 19.0720, val_MinusLogProbMetric: 19.0720

Epoch 237: val_loss did not improve from 18.93278
196/196 - 65s - loss: 18.9743 - MinusLogProbMetric: 18.9743 - val_loss: 19.0720 - val_MinusLogProbMetric: 19.0720 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 238/1000
2023-09-26 20:11:36.455 
Epoch 238/1000 
	 loss: 19.0068, MinusLogProbMetric: 19.0068, val_loss: 19.1648, val_MinusLogProbMetric: 19.1648

Epoch 238: val_loss did not improve from 18.93278
196/196 - 64s - loss: 19.0068 - MinusLogProbMetric: 19.0068 - val_loss: 19.1648 - val_MinusLogProbMetric: 19.1648 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 239/1000
2023-09-26 20:12:40.737 
Epoch 239/1000 
	 loss: 18.9483, MinusLogProbMetric: 18.9483, val_loss: 19.0333, val_MinusLogProbMetric: 19.0333

Epoch 239: val_loss did not improve from 18.93278
196/196 - 64s - loss: 18.9483 - MinusLogProbMetric: 18.9483 - val_loss: 19.0333 - val_MinusLogProbMetric: 19.0333 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 240/1000
2023-09-26 20:13:44.885 
Epoch 240/1000 
	 loss: 18.9155, MinusLogProbMetric: 18.9155, val_loss: 18.8720, val_MinusLogProbMetric: 18.8720

Epoch 240: val_loss improved from 18.93278 to 18.87201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.9155 - MinusLogProbMetric: 18.9155 - val_loss: 18.8720 - val_MinusLogProbMetric: 18.8720 - lr: 2.7778e-05 - 65s/epoch - 332ms/step
Epoch 241/1000
2023-09-26 20:14:50.340 
Epoch 241/1000 
	 loss: 18.9939, MinusLogProbMetric: 18.9939, val_loss: 18.9680, val_MinusLogProbMetric: 18.9680

Epoch 241: val_loss did not improve from 18.87201
196/196 - 64s - loss: 18.9939 - MinusLogProbMetric: 18.9939 - val_loss: 18.9680 - val_MinusLogProbMetric: 18.9680 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 242/1000
2023-09-26 20:15:54.443 
Epoch 242/1000 
	 loss: 18.9189, MinusLogProbMetric: 18.9189, val_loss: 18.9586, val_MinusLogProbMetric: 18.9586

Epoch 242: val_loss did not improve from 18.87201
196/196 - 64s - loss: 18.9189 - MinusLogProbMetric: 18.9189 - val_loss: 18.9586 - val_MinusLogProbMetric: 18.9586 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 243/1000
2023-09-26 20:16:58.760 
Epoch 243/1000 
	 loss: 18.9431, MinusLogProbMetric: 18.9431, val_loss: 18.9709, val_MinusLogProbMetric: 18.9709

Epoch 243: val_loss did not improve from 18.87201
196/196 - 64s - loss: 18.9431 - MinusLogProbMetric: 18.9431 - val_loss: 18.9709 - val_MinusLogProbMetric: 18.9709 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 244/1000
2023-09-26 20:18:02.482 
Epoch 244/1000 
	 loss: 18.9569, MinusLogProbMetric: 18.9569, val_loss: 18.9928, val_MinusLogProbMetric: 18.9928

Epoch 244: val_loss did not improve from 18.87201
196/196 - 64s - loss: 18.9569 - MinusLogProbMetric: 18.9569 - val_loss: 18.9928 - val_MinusLogProbMetric: 18.9928 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 245/1000
2023-09-26 20:19:06.641 
Epoch 245/1000 
	 loss: 18.9266, MinusLogProbMetric: 18.9266, val_loss: 18.8594, val_MinusLogProbMetric: 18.8594

Epoch 245: val_loss improved from 18.87201 to 18.85935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.9266 - MinusLogProbMetric: 18.9266 - val_loss: 18.8594 - val_MinusLogProbMetric: 18.8594 - lr: 2.7778e-05 - 65s/epoch - 331ms/step
Epoch 246/1000
2023-09-26 20:20:12.009 
Epoch 246/1000 
	 loss: 18.8568, MinusLogProbMetric: 18.8568, val_loss: 18.9667, val_MinusLogProbMetric: 18.9667

Epoch 246: val_loss did not improve from 18.85935
196/196 - 65s - loss: 18.8568 - MinusLogProbMetric: 18.8568 - val_loss: 18.9667 - val_MinusLogProbMetric: 18.9667 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 247/1000
2023-09-26 20:21:15.850 
Epoch 247/1000 
	 loss: 18.9326, MinusLogProbMetric: 18.9326, val_loss: 19.0203, val_MinusLogProbMetric: 19.0203

Epoch 247: val_loss did not improve from 18.85935
196/196 - 64s - loss: 18.9326 - MinusLogProbMetric: 18.9326 - val_loss: 19.0203 - val_MinusLogProbMetric: 19.0203 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 248/1000
2023-09-26 20:22:20.219 
Epoch 248/1000 
	 loss: 18.9277, MinusLogProbMetric: 18.9277, val_loss: 18.9216, val_MinusLogProbMetric: 18.9216

Epoch 248: val_loss did not improve from 18.85935
196/196 - 64s - loss: 18.9277 - MinusLogProbMetric: 18.9277 - val_loss: 18.9216 - val_MinusLogProbMetric: 18.9216 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 249/1000
2023-09-26 20:23:24.448 
Epoch 249/1000 
	 loss: 18.8739, MinusLogProbMetric: 18.8739, val_loss: 18.7640, val_MinusLogProbMetric: 18.7640

Epoch 249: val_loss improved from 18.85935 to 18.76403, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.8739 - MinusLogProbMetric: 18.8739 - val_loss: 18.7640 - val_MinusLogProbMetric: 18.7640 - lr: 2.7778e-05 - 65s/epoch - 333ms/step
Epoch 250/1000
2023-09-26 20:24:29.634 
Epoch 250/1000 
	 loss: 18.8725, MinusLogProbMetric: 18.8725, val_loss: 18.8181, val_MinusLogProbMetric: 18.8181

Epoch 250: val_loss did not improve from 18.76403
196/196 - 64s - loss: 18.8725 - MinusLogProbMetric: 18.8725 - val_loss: 18.8181 - val_MinusLogProbMetric: 18.8181 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 251/1000
2023-09-26 20:25:33.530 
Epoch 251/1000 
	 loss: 18.8394, MinusLogProbMetric: 18.8394, val_loss: 19.0261, val_MinusLogProbMetric: 19.0261

Epoch 251: val_loss did not improve from 18.76403
196/196 - 64s - loss: 18.8394 - MinusLogProbMetric: 18.8394 - val_loss: 19.0261 - val_MinusLogProbMetric: 19.0261 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 252/1000
2023-09-26 20:26:37.843 
Epoch 252/1000 
	 loss: 18.8688, MinusLogProbMetric: 18.8688, val_loss: 19.0498, val_MinusLogProbMetric: 19.0498

Epoch 252: val_loss did not improve from 18.76403
196/196 - 64s - loss: 18.8688 - MinusLogProbMetric: 18.8688 - val_loss: 19.0498 - val_MinusLogProbMetric: 19.0498 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 253/1000
2023-09-26 20:27:42.158 
Epoch 253/1000 
	 loss: 18.8773, MinusLogProbMetric: 18.8773, val_loss: 18.8925, val_MinusLogProbMetric: 18.8925

Epoch 253: val_loss did not improve from 18.76403
196/196 - 64s - loss: 18.8773 - MinusLogProbMetric: 18.8773 - val_loss: 18.8925 - val_MinusLogProbMetric: 18.8925 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 254/1000
2023-09-26 20:28:46.118 
Epoch 254/1000 
	 loss: 18.8529, MinusLogProbMetric: 18.8529, val_loss: 19.0520, val_MinusLogProbMetric: 19.0520

Epoch 254: val_loss did not improve from 18.76403
196/196 - 64s - loss: 18.8529 - MinusLogProbMetric: 18.8529 - val_loss: 19.0520 - val_MinusLogProbMetric: 19.0520 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 255/1000
2023-09-26 20:29:49.714 
Epoch 255/1000 
	 loss: 18.8874, MinusLogProbMetric: 18.8874, val_loss: 18.8443, val_MinusLogProbMetric: 18.8443

Epoch 255: val_loss did not improve from 18.76403
196/196 - 64s - loss: 18.8874 - MinusLogProbMetric: 18.8874 - val_loss: 18.8443 - val_MinusLogProbMetric: 18.8443 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 256/1000
2023-09-26 20:30:53.613 
Epoch 256/1000 
	 loss: 18.7833, MinusLogProbMetric: 18.7833, val_loss: 18.8214, val_MinusLogProbMetric: 18.8214

Epoch 256: val_loss did not improve from 18.76403
196/196 - 64s - loss: 18.7833 - MinusLogProbMetric: 18.7833 - val_loss: 18.8214 - val_MinusLogProbMetric: 18.8214 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 257/1000
2023-09-26 20:31:57.500 
Epoch 257/1000 
	 loss: 18.8222, MinusLogProbMetric: 18.8222, val_loss: 18.7407, val_MinusLogProbMetric: 18.7407

Epoch 257: val_loss improved from 18.76403 to 18.74073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.8222 - MinusLogProbMetric: 18.8222 - val_loss: 18.7407 - val_MinusLogProbMetric: 18.7407 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 258/1000
2023-09-26 20:33:02.229 
Epoch 258/1000 
	 loss: 18.8623, MinusLogProbMetric: 18.8623, val_loss: 18.7947, val_MinusLogProbMetric: 18.7947

Epoch 258: val_loss did not improve from 18.74073
196/196 - 64s - loss: 18.8623 - MinusLogProbMetric: 18.8623 - val_loss: 18.7947 - val_MinusLogProbMetric: 18.7947 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 259/1000
2023-09-26 20:34:06.548 
Epoch 259/1000 
	 loss: 18.8111, MinusLogProbMetric: 18.8111, val_loss: 18.7297, val_MinusLogProbMetric: 18.7297

Epoch 259: val_loss improved from 18.74073 to 18.72974, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.8111 - MinusLogProbMetric: 18.8111 - val_loss: 18.7297 - val_MinusLogProbMetric: 18.7297 - lr: 2.7778e-05 - 65s/epoch - 333ms/step
Epoch 260/1000
2023-09-26 20:35:11.676 
Epoch 260/1000 
	 loss: 18.8486, MinusLogProbMetric: 18.8486, val_loss: 18.8445, val_MinusLogProbMetric: 18.8445

Epoch 260: val_loss did not improve from 18.72974
196/196 - 64s - loss: 18.8486 - MinusLogProbMetric: 18.8486 - val_loss: 18.8445 - val_MinusLogProbMetric: 18.8445 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 261/1000
2023-09-26 20:36:15.871 
Epoch 261/1000 
	 loss: 18.8067, MinusLogProbMetric: 18.8067, val_loss: 18.8968, val_MinusLogProbMetric: 18.8968

Epoch 261: val_loss did not improve from 18.72974
196/196 - 64s - loss: 18.8067 - MinusLogProbMetric: 18.8067 - val_loss: 18.8968 - val_MinusLogProbMetric: 18.8968 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 262/1000
2023-09-26 20:37:20.606 
Epoch 262/1000 
	 loss: 18.8908, MinusLogProbMetric: 18.8908, val_loss: 18.8646, val_MinusLogProbMetric: 18.8646

Epoch 262: val_loss did not improve from 18.72974
196/196 - 65s - loss: 18.8908 - MinusLogProbMetric: 18.8908 - val_loss: 18.8646 - val_MinusLogProbMetric: 18.8646 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 263/1000
2023-09-26 20:38:24.923 
Epoch 263/1000 
	 loss: 18.7567, MinusLogProbMetric: 18.7567, val_loss: 19.0830, val_MinusLogProbMetric: 19.0830

Epoch 263: val_loss did not improve from 18.72974
196/196 - 64s - loss: 18.7567 - MinusLogProbMetric: 18.7567 - val_loss: 19.0830 - val_MinusLogProbMetric: 19.0830 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 264/1000
2023-09-26 20:39:28.554 
Epoch 264/1000 
	 loss: 18.8582, MinusLogProbMetric: 18.8582, val_loss: 18.7103, val_MinusLogProbMetric: 18.7103

Epoch 264: val_loss improved from 18.72974 to 18.71028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.8582 - MinusLogProbMetric: 18.8582 - val_loss: 18.7103 - val_MinusLogProbMetric: 18.7103 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 265/1000
2023-09-26 20:40:34.024 
Epoch 265/1000 
	 loss: 18.8125, MinusLogProbMetric: 18.8125, val_loss: 18.8417, val_MinusLogProbMetric: 18.8417

Epoch 265: val_loss did not improve from 18.71028
196/196 - 65s - loss: 18.8125 - MinusLogProbMetric: 18.8125 - val_loss: 18.8417 - val_MinusLogProbMetric: 18.8417 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 266/1000
2023-09-26 20:41:38.195 
Epoch 266/1000 
	 loss: 18.7804, MinusLogProbMetric: 18.7804, val_loss: 18.7446, val_MinusLogProbMetric: 18.7446

Epoch 266: val_loss did not improve from 18.71028
196/196 - 64s - loss: 18.7804 - MinusLogProbMetric: 18.7804 - val_loss: 18.7446 - val_MinusLogProbMetric: 18.7446 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 267/1000
2023-09-26 20:42:42.559 
Epoch 267/1000 
	 loss: 18.8308, MinusLogProbMetric: 18.8308, val_loss: 18.8543, val_MinusLogProbMetric: 18.8543

Epoch 267: val_loss did not improve from 18.71028
196/196 - 64s - loss: 18.8308 - MinusLogProbMetric: 18.8308 - val_loss: 18.8543 - val_MinusLogProbMetric: 18.8543 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 268/1000
2023-09-26 20:43:46.207 
Epoch 268/1000 
	 loss: 18.7997, MinusLogProbMetric: 18.7997, val_loss: 18.7763, val_MinusLogProbMetric: 18.7763

Epoch 268: val_loss did not improve from 18.71028
196/196 - 64s - loss: 18.7997 - MinusLogProbMetric: 18.7997 - val_loss: 18.7763 - val_MinusLogProbMetric: 18.7763 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 269/1000
2023-09-26 20:44:50.947 
Epoch 269/1000 
	 loss: 18.7977, MinusLogProbMetric: 18.7977, val_loss: 19.0068, val_MinusLogProbMetric: 19.0068

Epoch 269: val_loss did not improve from 18.71028
196/196 - 65s - loss: 18.7977 - MinusLogProbMetric: 18.7977 - val_loss: 19.0068 - val_MinusLogProbMetric: 19.0068 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 270/1000
2023-09-26 20:45:55.223 
Epoch 270/1000 
	 loss: 18.8297, MinusLogProbMetric: 18.8297, val_loss: 18.7625, val_MinusLogProbMetric: 18.7625

Epoch 270: val_loss did not improve from 18.71028
196/196 - 64s - loss: 18.8297 - MinusLogProbMetric: 18.8297 - val_loss: 18.7625 - val_MinusLogProbMetric: 18.7625 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 271/1000
2023-09-26 20:46:59.608 
Epoch 271/1000 
	 loss: 18.7212, MinusLogProbMetric: 18.7212, val_loss: 18.6906, val_MinusLogProbMetric: 18.6906

Epoch 271: val_loss improved from 18.71028 to 18.69062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.7212 - MinusLogProbMetric: 18.7212 - val_loss: 18.6906 - val_MinusLogProbMetric: 18.6906 - lr: 2.7778e-05 - 65s/epoch - 333ms/step
Epoch 272/1000
2023-09-26 20:48:05.015 
Epoch 272/1000 
	 loss: 18.7760, MinusLogProbMetric: 18.7760, val_loss: 18.9897, val_MinusLogProbMetric: 18.9897

Epoch 272: val_loss did not improve from 18.69062
196/196 - 64s - loss: 18.7760 - MinusLogProbMetric: 18.7760 - val_loss: 18.9897 - val_MinusLogProbMetric: 18.9897 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 273/1000
2023-09-26 20:49:09.312 
Epoch 273/1000 
	 loss: 18.7363, MinusLogProbMetric: 18.7363, val_loss: 18.7514, val_MinusLogProbMetric: 18.7514

Epoch 273: val_loss did not improve from 18.69062
196/196 - 64s - loss: 18.7363 - MinusLogProbMetric: 18.7363 - val_loss: 18.7514 - val_MinusLogProbMetric: 18.7514 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 274/1000
2023-09-26 20:50:13.689 
Epoch 274/1000 
	 loss: 18.8104, MinusLogProbMetric: 18.8104, val_loss: 18.7786, val_MinusLogProbMetric: 18.7786

Epoch 274: val_loss did not improve from 18.69062
196/196 - 64s - loss: 18.8104 - MinusLogProbMetric: 18.8104 - val_loss: 18.7786 - val_MinusLogProbMetric: 18.7786 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 275/1000
2023-09-26 20:51:18.281 
Epoch 275/1000 
	 loss: 18.7533, MinusLogProbMetric: 18.7533, val_loss: 18.8382, val_MinusLogProbMetric: 18.8382

Epoch 275: val_loss did not improve from 18.69062
196/196 - 65s - loss: 18.7533 - MinusLogProbMetric: 18.7533 - val_loss: 18.8382 - val_MinusLogProbMetric: 18.8382 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 276/1000
2023-09-26 20:52:22.718 
Epoch 276/1000 
	 loss: 18.7439, MinusLogProbMetric: 18.7439, val_loss: 18.7130, val_MinusLogProbMetric: 18.7130

Epoch 276: val_loss did not improve from 18.69062
196/196 - 64s - loss: 18.7439 - MinusLogProbMetric: 18.7439 - val_loss: 18.7130 - val_MinusLogProbMetric: 18.7130 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 277/1000
2023-09-26 20:53:27.000 
Epoch 277/1000 
	 loss: 18.8945, MinusLogProbMetric: 18.8945, val_loss: 18.7577, val_MinusLogProbMetric: 18.7577

Epoch 277: val_loss did not improve from 18.69062
196/196 - 64s - loss: 18.8945 - MinusLogProbMetric: 18.8945 - val_loss: 18.7577 - val_MinusLogProbMetric: 18.7577 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 278/1000
2023-09-26 20:54:31.092 
Epoch 278/1000 
	 loss: 18.7301, MinusLogProbMetric: 18.7301, val_loss: 18.6375, val_MinusLogProbMetric: 18.6375

Epoch 278: val_loss improved from 18.69062 to 18.63754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.7301 - MinusLogProbMetric: 18.7301 - val_loss: 18.6375 - val_MinusLogProbMetric: 18.6375 - lr: 2.7778e-05 - 65s/epoch - 332ms/step
Epoch 279/1000
2023-09-26 20:55:36.599 
Epoch 279/1000 
	 loss: 18.7699, MinusLogProbMetric: 18.7699, val_loss: 18.6376, val_MinusLogProbMetric: 18.6376

Epoch 279: val_loss did not improve from 18.63754
196/196 - 65s - loss: 18.7699 - MinusLogProbMetric: 18.7699 - val_loss: 18.6376 - val_MinusLogProbMetric: 18.6376 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 280/1000
2023-09-26 20:56:40.920 
Epoch 280/1000 
	 loss: 18.7379, MinusLogProbMetric: 18.7379, val_loss: 18.6459, val_MinusLogProbMetric: 18.6459

Epoch 280: val_loss did not improve from 18.63754
196/196 - 64s - loss: 18.7379 - MinusLogProbMetric: 18.7379 - val_loss: 18.6459 - val_MinusLogProbMetric: 18.6459 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 281/1000
2023-09-26 20:57:44.673 
Epoch 281/1000 
	 loss: 18.7404, MinusLogProbMetric: 18.7404, val_loss: 18.8383, val_MinusLogProbMetric: 18.8383

Epoch 281: val_loss did not improve from 18.63754
196/196 - 64s - loss: 18.7404 - MinusLogProbMetric: 18.7404 - val_loss: 18.8383 - val_MinusLogProbMetric: 18.8383 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 282/1000
2023-09-26 20:58:48.958 
Epoch 282/1000 
	 loss: 18.6993, MinusLogProbMetric: 18.6993, val_loss: 18.6977, val_MinusLogProbMetric: 18.6977

Epoch 282: val_loss did not improve from 18.63754
196/196 - 64s - loss: 18.6993 - MinusLogProbMetric: 18.6993 - val_loss: 18.6977 - val_MinusLogProbMetric: 18.6977 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 283/1000
2023-09-26 20:59:53.666 
Epoch 283/1000 
	 loss: 18.7925, MinusLogProbMetric: 18.7925, val_loss: 18.7074, val_MinusLogProbMetric: 18.7074

Epoch 283: val_loss did not improve from 18.63754
196/196 - 65s - loss: 18.7925 - MinusLogProbMetric: 18.7925 - val_loss: 18.7074 - val_MinusLogProbMetric: 18.7074 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 284/1000
2023-09-26 21:00:57.793 
Epoch 284/1000 
	 loss: 18.7431, MinusLogProbMetric: 18.7431, val_loss: 18.6346, val_MinusLogProbMetric: 18.6346

Epoch 284: val_loss improved from 18.63754 to 18.63459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.7431 - MinusLogProbMetric: 18.7431 - val_loss: 18.6346 - val_MinusLogProbMetric: 18.6346 - lr: 2.7778e-05 - 65s/epoch - 332ms/step
Epoch 285/1000
2023-09-26 21:02:03.307 
Epoch 285/1000 
	 loss: 18.7078, MinusLogProbMetric: 18.7078, val_loss: 18.7442, val_MinusLogProbMetric: 18.7442

Epoch 285: val_loss did not improve from 18.63459
196/196 - 65s - loss: 18.7078 - MinusLogProbMetric: 18.7078 - val_loss: 18.7442 - val_MinusLogProbMetric: 18.7442 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 286/1000
2023-09-26 21:03:07.487 
Epoch 286/1000 
	 loss: 18.7184, MinusLogProbMetric: 18.7184, val_loss: 18.9065, val_MinusLogProbMetric: 18.9065

Epoch 286: val_loss did not improve from 18.63459
196/196 - 64s - loss: 18.7184 - MinusLogProbMetric: 18.7184 - val_loss: 18.9065 - val_MinusLogProbMetric: 18.9065 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 287/1000
2023-09-26 21:04:11.487 
Epoch 287/1000 
	 loss: 18.6456, MinusLogProbMetric: 18.6456, val_loss: 18.8834, val_MinusLogProbMetric: 18.8834

Epoch 287: val_loss did not improve from 18.63459
196/196 - 64s - loss: 18.6456 - MinusLogProbMetric: 18.6456 - val_loss: 18.8834 - val_MinusLogProbMetric: 18.8834 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 288/1000
2023-09-26 21:05:15.897 
Epoch 288/1000 
	 loss: 18.7253, MinusLogProbMetric: 18.7253, val_loss: 18.9288, val_MinusLogProbMetric: 18.9288

Epoch 288: val_loss did not improve from 18.63459
196/196 - 64s - loss: 18.7253 - MinusLogProbMetric: 18.7253 - val_loss: 18.9288 - val_MinusLogProbMetric: 18.9288 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 289/1000
2023-09-26 21:06:19.960 
Epoch 289/1000 
	 loss: 18.7474, MinusLogProbMetric: 18.7474, val_loss: 18.6436, val_MinusLogProbMetric: 18.6436

Epoch 289: val_loss did not improve from 18.63459
196/196 - 64s - loss: 18.7474 - MinusLogProbMetric: 18.7474 - val_loss: 18.6436 - val_MinusLogProbMetric: 18.6436 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 290/1000
2023-09-26 21:07:24.356 
Epoch 290/1000 
	 loss: 18.7590, MinusLogProbMetric: 18.7590, val_loss: 18.6596, val_MinusLogProbMetric: 18.6596

Epoch 290: val_loss did not improve from 18.63459
196/196 - 64s - loss: 18.7590 - MinusLogProbMetric: 18.7590 - val_loss: 18.6596 - val_MinusLogProbMetric: 18.6596 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 291/1000
2023-09-26 21:08:29.035 
Epoch 291/1000 
	 loss: 18.7323, MinusLogProbMetric: 18.7323, val_loss: 18.9130, val_MinusLogProbMetric: 18.9130

Epoch 291: val_loss did not improve from 18.63459
196/196 - 65s - loss: 18.7323 - MinusLogProbMetric: 18.7323 - val_loss: 18.9130 - val_MinusLogProbMetric: 18.9130 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 292/1000
2023-09-26 21:09:33.368 
Epoch 292/1000 
	 loss: 18.7214, MinusLogProbMetric: 18.7214, val_loss: 18.7570, val_MinusLogProbMetric: 18.7570

Epoch 292: val_loss did not improve from 18.63459
196/196 - 64s - loss: 18.7214 - MinusLogProbMetric: 18.7214 - val_loss: 18.7570 - val_MinusLogProbMetric: 18.7570 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 293/1000
2023-09-26 21:10:37.485 
Epoch 293/1000 
	 loss: 18.6641, MinusLogProbMetric: 18.6641, val_loss: 18.8166, val_MinusLogProbMetric: 18.8166

Epoch 293: val_loss did not improve from 18.63459
196/196 - 64s - loss: 18.6641 - MinusLogProbMetric: 18.6641 - val_loss: 18.8166 - val_MinusLogProbMetric: 18.8166 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 294/1000
2023-09-26 21:11:41.901 
Epoch 294/1000 
	 loss: 18.6399, MinusLogProbMetric: 18.6399, val_loss: 18.6998, val_MinusLogProbMetric: 18.6998

Epoch 294: val_loss did not improve from 18.63459
196/196 - 64s - loss: 18.6399 - MinusLogProbMetric: 18.6399 - val_loss: 18.6998 - val_MinusLogProbMetric: 18.6998 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 295/1000
2023-09-26 21:12:46.024 
Epoch 295/1000 
	 loss: 18.7679, MinusLogProbMetric: 18.7679, val_loss: 18.6994, val_MinusLogProbMetric: 18.6994

Epoch 295: val_loss did not improve from 18.63459
196/196 - 64s - loss: 18.7679 - MinusLogProbMetric: 18.7679 - val_loss: 18.6994 - val_MinusLogProbMetric: 18.6994 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 296/1000
2023-09-26 21:13:49.821 
Epoch 296/1000 
	 loss: 18.6340, MinusLogProbMetric: 18.6340, val_loss: 18.8527, val_MinusLogProbMetric: 18.8527

Epoch 296: val_loss did not improve from 18.63459
196/196 - 64s - loss: 18.6340 - MinusLogProbMetric: 18.6340 - val_loss: 18.8527 - val_MinusLogProbMetric: 18.8527 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 297/1000
2023-09-26 21:14:54.147 
Epoch 297/1000 
	 loss: 18.6534, MinusLogProbMetric: 18.6534, val_loss: 18.9174, val_MinusLogProbMetric: 18.9174

Epoch 297: val_loss did not improve from 18.63459
196/196 - 64s - loss: 18.6534 - MinusLogProbMetric: 18.6534 - val_loss: 18.9174 - val_MinusLogProbMetric: 18.9174 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 298/1000
2023-09-26 21:15:58.315 
Epoch 298/1000 
	 loss: 18.6833, MinusLogProbMetric: 18.6833, val_loss: 18.7538, val_MinusLogProbMetric: 18.7538

Epoch 298: val_loss did not improve from 18.63459
196/196 - 64s - loss: 18.6833 - MinusLogProbMetric: 18.6833 - val_loss: 18.7538 - val_MinusLogProbMetric: 18.7538 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 299/1000
2023-09-26 21:17:02.542 
Epoch 299/1000 
	 loss: 18.6979, MinusLogProbMetric: 18.6979, val_loss: 18.8555, val_MinusLogProbMetric: 18.8555

Epoch 299: val_loss did not improve from 18.63459
196/196 - 64s - loss: 18.6979 - MinusLogProbMetric: 18.6979 - val_loss: 18.8555 - val_MinusLogProbMetric: 18.8555 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 300/1000
2023-09-26 21:18:06.608 
Epoch 300/1000 
	 loss: 18.6372, MinusLogProbMetric: 18.6372, val_loss: 18.5316, val_MinusLogProbMetric: 18.5316

Epoch 300: val_loss improved from 18.63459 to 18.53161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.6372 - MinusLogProbMetric: 18.6372 - val_loss: 18.5316 - val_MinusLogProbMetric: 18.5316 - lr: 2.7778e-05 - 65s/epoch - 332ms/step
Epoch 301/1000
2023-09-26 21:19:11.561 
Epoch 301/1000 
	 loss: 18.6506, MinusLogProbMetric: 18.6506, val_loss: 18.9169, val_MinusLogProbMetric: 18.9169

Epoch 301: val_loss did not improve from 18.53161
196/196 - 64s - loss: 18.6506 - MinusLogProbMetric: 18.6506 - val_loss: 18.9169 - val_MinusLogProbMetric: 18.9169 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 302/1000
2023-09-26 21:20:15.602 
Epoch 302/1000 
	 loss: 18.6860, MinusLogProbMetric: 18.6860, val_loss: 18.5674, val_MinusLogProbMetric: 18.5674

Epoch 302: val_loss did not improve from 18.53161
196/196 - 64s - loss: 18.6860 - MinusLogProbMetric: 18.6860 - val_loss: 18.5674 - val_MinusLogProbMetric: 18.5674 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 303/1000
2023-09-26 21:21:19.925 
Epoch 303/1000 
	 loss: 18.6328, MinusLogProbMetric: 18.6328, val_loss: 18.8101, val_MinusLogProbMetric: 18.8101

Epoch 303: val_loss did not improve from 18.53161
196/196 - 64s - loss: 18.6328 - MinusLogProbMetric: 18.6328 - val_loss: 18.8101 - val_MinusLogProbMetric: 18.8101 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 304/1000
2023-09-26 21:22:24.352 
Epoch 304/1000 
	 loss: 18.6585, MinusLogProbMetric: 18.6585, val_loss: 18.5272, val_MinusLogProbMetric: 18.5272

Epoch 304: val_loss improved from 18.53161 to 18.52723, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.6585 - MinusLogProbMetric: 18.6585 - val_loss: 18.5272 - val_MinusLogProbMetric: 18.5272 - lr: 2.7778e-05 - 65s/epoch - 334ms/step
Epoch 305/1000
2023-09-26 21:23:29.532 
Epoch 305/1000 
	 loss: 18.6278, MinusLogProbMetric: 18.6278, val_loss: 18.9559, val_MinusLogProbMetric: 18.9559

Epoch 305: val_loss did not improve from 18.52723
196/196 - 64s - loss: 18.6278 - MinusLogProbMetric: 18.6278 - val_loss: 18.9559 - val_MinusLogProbMetric: 18.9559 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 306/1000
2023-09-26 21:24:33.801 
Epoch 306/1000 
	 loss: 18.6499, MinusLogProbMetric: 18.6499, val_loss: 18.7731, val_MinusLogProbMetric: 18.7731

Epoch 306: val_loss did not improve from 18.52723
196/196 - 64s - loss: 18.6499 - MinusLogProbMetric: 18.6499 - val_loss: 18.7731 - val_MinusLogProbMetric: 18.7731 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 307/1000
2023-09-26 21:25:37.370 
Epoch 307/1000 
	 loss: 18.6420, MinusLogProbMetric: 18.6420, val_loss: 18.6559, val_MinusLogProbMetric: 18.6559

Epoch 307: val_loss did not improve from 18.52723
196/196 - 64s - loss: 18.6420 - MinusLogProbMetric: 18.6420 - val_loss: 18.6559 - val_MinusLogProbMetric: 18.6559 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 308/1000
2023-09-26 21:26:41.614 
Epoch 308/1000 
	 loss: 18.6316, MinusLogProbMetric: 18.6316, val_loss: 18.5466, val_MinusLogProbMetric: 18.5466

Epoch 308: val_loss did not improve from 18.52723
196/196 - 64s - loss: 18.6316 - MinusLogProbMetric: 18.6316 - val_loss: 18.5466 - val_MinusLogProbMetric: 18.5466 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 309/1000
2023-09-26 21:27:46.287 
Epoch 309/1000 
	 loss: 18.6156, MinusLogProbMetric: 18.6156, val_loss: 18.5942, val_MinusLogProbMetric: 18.5942

Epoch 309: val_loss did not improve from 18.52723
196/196 - 65s - loss: 18.6156 - MinusLogProbMetric: 18.6156 - val_loss: 18.5942 - val_MinusLogProbMetric: 18.5942 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 310/1000
2023-09-26 21:28:50.671 
Epoch 310/1000 
	 loss: 18.7311, MinusLogProbMetric: 18.7311, val_loss: 18.6619, val_MinusLogProbMetric: 18.6619

Epoch 310: val_loss did not improve from 18.52723
196/196 - 64s - loss: 18.7311 - MinusLogProbMetric: 18.7311 - val_loss: 18.6619 - val_MinusLogProbMetric: 18.6619 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 311/1000
2023-09-26 21:29:54.662 
Epoch 311/1000 
	 loss: 18.5779, MinusLogProbMetric: 18.5779, val_loss: 18.6409, val_MinusLogProbMetric: 18.6409

Epoch 311: val_loss did not improve from 18.52723
196/196 - 64s - loss: 18.5779 - MinusLogProbMetric: 18.5779 - val_loss: 18.6409 - val_MinusLogProbMetric: 18.6409 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 312/1000
2023-09-26 21:30:58.431 
Epoch 312/1000 
	 loss: 18.6232, MinusLogProbMetric: 18.6232, val_loss: 18.5615, val_MinusLogProbMetric: 18.5615

Epoch 312: val_loss did not improve from 18.52723
196/196 - 64s - loss: 18.6232 - MinusLogProbMetric: 18.6232 - val_loss: 18.5615 - val_MinusLogProbMetric: 18.5615 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 313/1000
2023-09-26 21:32:02.528 
Epoch 313/1000 
	 loss: 18.6312, MinusLogProbMetric: 18.6312, val_loss: 18.5273, val_MinusLogProbMetric: 18.5273

Epoch 313: val_loss did not improve from 18.52723
196/196 - 64s - loss: 18.6312 - MinusLogProbMetric: 18.6312 - val_loss: 18.5273 - val_MinusLogProbMetric: 18.5273 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 314/1000
2023-09-26 21:33:06.922 
Epoch 314/1000 
	 loss: 18.5624, MinusLogProbMetric: 18.5624, val_loss: 18.5707, val_MinusLogProbMetric: 18.5707

Epoch 314: val_loss did not improve from 18.52723
196/196 - 64s - loss: 18.5624 - MinusLogProbMetric: 18.5624 - val_loss: 18.5707 - val_MinusLogProbMetric: 18.5707 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 315/1000
2023-09-26 21:34:11.512 
Epoch 315/1000 
	 loss: 18.6132, MinusLogProbMetric: 18.6132, val_loss: 18.7960, val_MinusLogProbMetric: 18.7960

Epoch 315: val_loss did not improve from 18.52723
196/196 - 65s - loss: 18.6132 - MinusLogProbMetric: 18.6132 - val_loss: 18.7960 - val_MinusLogProbMetric: 18.7960 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 316/1000
2023-09-26 21:35:15.706 
Epoch 316/1000 
	 loss: 18.6011, MinusLogProbMetric: 18.6011, val_loss: 18.5241, val_MinusLogProbMetric: 18.5241

Epoch 316: val_loss improved from 18.52723 to 18.52410, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.6011 - MinusLogProbMetric: 18.6011 - val_loss: 18.5241 - val_MinusLogProbMetric: 18.5241 - lr: 2.7778e-05 - 65s/epoch - 333ms/step
Epoch 317/1000
2023-09-26 21:36:20.468 
Epoch 317/1000 
	 loss: 18.5840, MinusLogProbMetric: 18.5840, val_loss: 18.5087, val_MinusLogProbMetric: 18.5087

Epoch 317: val_loss improved from 18.52410 to 18.50867, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.5840 - MinusLogProbMetric: 18.5840 - val_loss: 18.5087 - val_MinusLogProbMetric: 18.5087 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 318/1000
2023-09-26 21:37:25.579 
Epoch 318/1000 
	 loss: 18.5887, MinusLogProbMetric: 18.5887, val_loss: 18.5767, val_MinusLogProbMetric: 18.5767

Epoch 318: val_loss did not improve from 18.50867
196/196 - 64s - loss: 18.5887 - MinusLogProbMetric: 18.5887 - val_loss: 18.5767 - val_MinusLogProbMetric: 18.5767 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 319/1000
2023-09-26 21:38:29.767 
Epoch 319/1000 
	 loss: 18.5874, MinusLogProbMetric: 18.5874, val_loss: 18.6514, val_MinusLogProbMetric: 18.6514

Epoch 319: val_loss did not improve from 18.50867
196/196 - 64s - loss: 18.5874 - MinusLogProbMetric: 18.5874 - val_loss: 18.6514 - val_MinusLogProbMetric: 18.6514 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 320/1000
2023-09-26 21:39:34.597 
Epoch 320/1000 
	 loss: 18.5804, MinusLogProbMetric: 18.5804, val_loss: 18.4832, val_MinusLogProbMetric: 18.4832

Epoch 320: val_loss improved from 18.50867 to 18.48320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 66s - loss: 18.5804 - MinusLogProbMetric: 18.5804 - val_loss: 18.4832 - val_MinusLogProbMetric: 18.4832 - lr: 2.7778e-05 - 66s/epoch - 336ms/step
Epoch 321/1000
2023-09-26 21:40:40.221 
Epoch 321/1000 
	 loss: 18.6423, MinusLogProbMetric: 18.6423, val_loss: 18.4698, val_MinusLogProbMetric: 18.4698

Epoch 321: val_loss improved from 18.48320 to 18.46984, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.6423 - MinusLogProbMetric: 18.6423 - val_loss: 18.4698 - val_MinusLogProbMetric: 18.4698 - lr: 2.7778e-05 - 65s/epoch - 334ms/step
Epoch 322/1000
2023-09-26 21:41:45.291 
Epoch 322/1000 
	 loss: 18.6151, MinusLogProbMetric: 18.6151, val_loss: 18.6244, val_MinusLogProbMetric: 18.6244

Epoch 322: val_loss did not improve from 18.46984
196/196 - 64s - loss: 18.6151 - MinusLogProbMetric: 18.6151 - val_loss: 18.6244 - val_MinusLogProbMetric: 18.6244 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 323/1000
2023-09-26 21:42:49.394 
Epoch 323/1000 
	 loss: 18.5931, MinusLogProbMetric: 18.5931, val_loss: 18.5888, val_MinusLogProbMetric: 18.5888

Epoch 323: val_loss did not improve from 18.46984
196/196 - 64s - loss: 18.5931 - MinusLogProbMetric: 18.5931 - val_loss: 18.5888 - val_MinusLogProbMetric: 18.5888 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 324/1000
2023-09-26 21:43:53.475 
Epoch 324/1000 
	 loss: 18.5616, MinusLogProbMetric: 18.5616, val_loss: 18.7482, val_MinusLogProbMetric: 18.7482

Epoch 324: val_loss did not improve from 18.46984
196/196 - 64s - loss: 18.5616 - MinusLogProbMetric: 18.5616 - val_loss: 18.7482 - val_MinusLogProbMetric: 18.7482 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 325/1000
2023-09-26 21:44:57.672 
Epoch 325/1000 
	 loss: 18.5984, MinusLogProbMetric: 18.5984, val_loss: 18.5584, val_MinusLogProbMetric: 18.5584

Epoch 325: val_loss did not improve from 18.46984
196/196 - 64s - loss: 18.5984 - MinusLogProbMetric: 18.5984 - val_loss: 18.5584 - val_MinusLogProbMetric: 18.5584 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 326/1000
2023-09-26 21:46:02.217 
Epoch 326/1000 
	 loss: 18.6110, MinusLogProbMetric: 18.6110, val_loss: 18.6242, val_MinusLogProbMetric: 18.6242

Epoch 326: val_loss did not improve from 18.46984
196/196 - 65s - loss: 18.6110 - MinusLogProbMetric: 18.6110 - val_loss: 18.6242 - val_MinusLogProbMetric: 18.6242 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 327/1000
2023-09-26 21:47:05.857 
Epoch 327/1000 
	 loss: 18.5933, MinusLogProbMetric: 18.5933, val_loss: 18.5062, val_MinusLogProbMetric: 18.5062

Epoch 327: val_loss did not improve from 18.46984
196/196 - 64s - loss: 18.5933 - MinusLogProbMetric: 18.5933 - val_loss: 18.5062 - val_MinusLogProbMetric: 18.5062 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 328/1000
2023-09-26 21:48:10.410 
Epoch 328/1000 
	 loss: 18.5958, MinusLogProbMetric: 18.5958, val_loss: 18.5621, val_MinusLogProbMetric: 18.5621

Epoch 328: val_loss did not improve from 18.46984
196/196 - 65s - loss: 18.5958 - MinusLogProbMetric: 18.5958 - val_loss: 18.5621 - val_MinusLogProbMetric: 18.5621 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 329/1000
2023-09-26 21:49:14.971 
Epoch 329/1000 
	 loss: 18.5609, MinusLogProbMetric: 18.5609, val_loss: 18.6706, val_MinusLogProbMetric: 18.6706

Epoch 329: val_loss did not improve from 18.46984
196/196 - 65s - loss: 18.5609 - MinusLogProbMetric: 18.5609 - val_loss: 18.6706 - val_MinusLogProbMetric: 18.6706 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 330/1000
2023-09-26 21:50:19.278 
Epoch 330/1000 
	 loss: 18.5214, MinusLogProbMetric: 18.5214, val_loss: 18.5464, val_MinusLogProbMetric: 18.5464

Epoch 330: val_loss did not improve from 18.46984
196/196 - 64s - loss: 18.5214 - MinusLogProbMetric: 18.5214 - val_loss: 18.5464 - val_MinusLogProbMetric: 18.5464 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 331/1000
2023-09-26 21:51:23.385 
Epoch 331/1000 
	 loss: 18.5715, MinusLogProbMetric: 18.5715, val_loss: 18.6574, val_MinusLogProbMetric: 18.6574

Epoch 331: val_loss did not improve from 18.46984
196/196 - 64s - loss: 18.5715 - MinusLogProbMetric: 18.5715 - val_loss: 18.6574 - val_MinusLogProbMetric: 18.6574 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 332/1000
2023-09-26 21:52:26.941 
Epoch 332/1000 
	 loss: 18.5984, MinusLogProbMetric: 18.5984, val_loss: 18.5473, val_MinusLogProbMetric: 18.5473

Epoch 332: val_loss did not improve from 18.46984
196/196 - 64s - loss: 18.5984 - MinusLogProbMetric: 18.5984 - val_loss: 18.5473 - val_MinusLogProbMetric: 18.5473 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 333/1000
2023-09-26 21:53:31.431 
Epoch 333/1000 
	 loss: 18.5721, MinusLogProbMetric: 18.5721, val_loss: 18.5051, val_MinusLogProbMetric: 18.5051

Epoch 333: val_loss did not improve from 18.46984
196/196 - 64s - loss: 18.5721 - MinusLogProbMetric: 18.5721 - val_loss: 18.5051 - val_MinusLogProbMetric: 18.5051 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 334/1000
2023-09-26 21:54:36.179 
Epoch 334/1000 
	 loss: 18.5995, MinusLogProbMetric: 18.5995, val_loss: 18.4644, val_MinusLogProbMetric: 18.4644

Epoch 334: val_loss improved from 18.46984 to 18.46435, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 66s - loss: 18.5995 - MinusLogProbMetric: 18.5995 - val_loss: 18.4644 - val_MinusLogProbMetric: 18.4644 - lr: 2.7778e-05 - 66s/epoch - 335ms/step
Epoch 335/1000
2023-09-26 21:55:41.462 
Epoch 335/1000 
	 loss: 18.6070, MinusLogProbMetric: 18.6070, val_loss: 18.6831, val_MinusLogProbMetric: 18.6831

Epoch 335: val_loss did not improve from 18.46435
196/196 - 64s - loss: 18.6070 - MinusLogProbMetric: 18.6070 - val_loss: 18.6831 - val_MinusLogProbMetric: 18.6831 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 336/1000
2023-09-26 21:56:45.873 
Epoch 336/1000 
	 loss: 18.5379, MinusLogProbMetric: 18.5379, val_loss: 18.5558, val_MinusLogProbMetric: 18.5558

Epoch 336: val_loss did not improve from 18.46435
196/196 - 64s - loss: 18.5379 - MinusLogProbMetric: 18.5379 - val_loss: 18.5558 - val_MinusLogProbMetric: 18.5558 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 337/1000
2023-09-26 21:57:49.646 
Epoch 337/1000 
	 loss: 18.5240, MinusLogProbMetric: 18.5240, val_loss: 18.4579, val_MinusLogProbMetric: 18.4579

Epoch 337: val_loss improved from 18.46435 to 18.45793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.5240 - MinusLogProbMetric: 18.5240 - val_loss: 18.4579 - val_MinusLogProbMetric: 18.4579 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 338/1000
2023-09-26 21:58:54.926 
Epoch 338/1000 
	 loss: 18.5442, MinusLogProbMetric: 18.5442, val_loss: 18.4545, val_MinusLogProbMetric: 18.4545

Epoch 338: val_loss improved from 18.45793 to 18.45446, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.5442 - MinusLogProbMetric: 18.5442 - val_loss: 18.4545 - val_MinusLogProbMetric: 18.4545 - lr: 2.7778e-05 - 65s/epoch - 333ms/step
Epoch 339/1000
2023-09-26 22:00:00.029 
Epoch 339/1000 
	 loss: 18.5525, MinusLogProbMetric: 18.5525, val_loss: 18.8357, val_MinusLogProbMetric: 18.8357

Epoch 339: val_loss did not improve from 18.45446
196/196 - 64s - loss: 18.5525 - MinusLogProbMetric: 18.5525 - val_loss: 18.8357 - val_MinusLogProbMetric: 18.8357 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 340/1000
2023-09-26 22:01:04.067 
Epoch 340/1000 
	 loss: 18.5477, MinusLogProbMetric: 18.5477, val_loss: 18.6716, val_MinusLogProbMetric: 18.6716

Epoch 340: val_loss did not improve from 18.45446
196/196 - 64s - loss: 18.5477 - MinusLogProbMetric: 18.5477 - val_loss: 18.6716 - val_MinusLogProbMetric: 18.6716 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 341/1000
2023-09-26 22:02:07.974 
Epoch 341/1000 
	 loss: 18.5073, MinusLogProbMetric: 18.5073, val_loss: 18.8708, val_MinusLogProbMetric: 18.8708

Epoch 341: val_loss did not improve from 18.45446
196/196 - 64s - loss: 18.5073 - MinusLogProbMetric: 18.5073 - val_loss: 18.8708 - val_MinusLogProbMetric: 18.8708 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 342/1000
2023-09-26 22:03:12.360 
Epoch 342/1000 
	 loss: 18.5060, MinusLogProbMetric: 18.5060, val_loss: 18.6461, val_MinusLogProbMetric: 18.6461

Epoch 342: val_loss did not improve from 18.45446
196/196 - 64s - loss: 18.5060 - MinusLogProbMetric: 18.5060 - val_loss: 18.6461 - val_MinusLogProbMetric: 18.6461 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 343/1000
2023-09-26 22:04:16.468 
Epoch 343/1000 
	 loss: 18.5389, MinusLogProbMetric: 18.5389, val_loss: 18.4402, val_MinusLogProbMetric: 18.4402

Epoch 343: val_loss improved from 18.45446 to 18.44021, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.5389 - MinusLogProbMetric: 18.5389 - val_loss: 18.4402 - val_MinusLogProbMetric: 18.4402 - lr: 2.7778e-05 - 65s/epoch - 331ms/step
Epoch 344/1000
2023-09-26 22:05:21.726 
Epoch 344/1000 
	 loss: 18.5492, MinusLogProbMetric: 18.5492, val_loss: 18.7946, val_MinusLogProbMetric: 18.7946

Epoch 344: val_loss did not improve from 18.44021
196/196 - 64s - loss: 18.5492 - MinusLogProbMetric: 18.5492 - val_loss: 18.7946 - val_MinusLogProbMetric: 18.7946 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 345/1000
2023-09-26 22:06:25.952 
Epoch 345/1000 
	 loss: 18.5464, MinusLogProbMetric: 18.5464, val_loss: 18.4968, val_MinusLogProbMetric: 18.4968

Epoch 345: val_loss did not improve from 18.44021
196/196 - 64s - loss: 18.5464 - MinusLogProbMetric: 18.5464 - val_loss: 18.4968 - val_MinusLogProbMetric: 18.4968 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 346/1000
2023-09-26 22:07:30.243 
Epoch 346/1000 
	 loss: 18.4924, MinusLogProbMetric: 18.4924, val_loss: 18.4547, val_MinusLogProbMetric: 18.4547

Epoch 346: val_loss did not improve from 18.44021
196/196 - 64s - loss: 18.4924 - MinusLogProbMetric: 18.4924 - val_loss: 18.4547 - val_MinusLogProbMetric: 18.4547 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 347/1000
2023-09-26 22:08:34.509 
Epoch 347/1000 
	 loss: 18.5576, MinusLogProbMetric: 18.5576, val_loss: 18.7396, val_MinusLogProbMetric: 18.7396

Epoch 347: val_loss did not improve from 18.44021
196/196 - 64s - loss: 18.5576 - MinusLogProbMetric: 18.5576 - val_loss: 18.7396 - val_MinusLogProbMetric: 18.7396 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 348/1000
2023-09-26 22:09:38.747 
Epoch 348/1000 
	 loss: 18.5477, MinusLogProbMetric: 18.5477, val_loss: 18.5927, val_MinusLogProbMetric: 18.5927

Epoch 348: val_loss did not improve from 18.44021
196/196 - 64s - loss: 18.5477 - MinusLogProbMetric: 18.5477 - val_loss: 18.5927 - val_MinusLogProbMetric: 18.5927 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 349/1000
2023-09-26 22:10:42.662 
Epoch 349/1000 
	 loss: 18.5319, MinusLogProbMetric: 18.5319, val_loss: 18.4936, val_MinusLogProbMetric: 18.4936

Epoch 349: val_loss did not improve from 18.44021
196/196 - 64s - loss: 18.5319 - MinusLogProbMetric: 18.5319 - val_loss: 18.4936 - val_MinusLogProbMetric: 18.4936 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 350/1000
2023-09-26 22:11:46.961 
Epoch 350/1000 
	 loss: 18.5291, MinusLogProbMetric: 18.5291, val_loss: 18.6240, val_MinusLogProbMetric: 18.6240

Epoch 350: val_loss did not improve from 18.44021
196/196 - 64s - loss: 18.5291 - MinusLogProbMetric: 18.5291 - val_loss: 18.6240 - val_MinusLogProbMetric: 18.6240 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 351/1000
2023-09-26 22:12:50.805 
Epoch 351/1000 
	 loss: 18.4658, MinusLogProbMetric: 18.4658, val_loss: 18.4010, val_MinusLogProbMetric: 18.4010

Epoch 351: val_loss improved from 18.44021 to 18.40099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.4658 - MinusLogProbMetric: 18.4658 - val_loss: 18.4010 - val_MinusLogProbMetric: 18.4010 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 352/1000
2023-09-26 22:13:55.934 
Epoch 352/1000 
	 loss: 18.4831, MinusLogProbMetric: 18.4831, val_loss: 18.7316, val_MinusLogProbMetric: 18.7316

Epoch 352: val_loss did not improve from 18.40099
196/196 - 64s - loss: 18.4831 - MinusLogProbMetric: 18.4831 - val_loss: 18.7316 - val_MinusLogProbMetric: 18.7316 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 353/1000
2023-09-26 22:15:00.221 
Epoch 353/1000 
	 loss: 18.4855, MinusLogProbMetric: 18.4855, val_loss: 18.4731, val_MinusLogProbMetric: 18.4731

Epoch 353: val_loss did not improve from 18.40099
196/196 - 64s - loss: 18.4855 - MinusLogProbMetric: 18.4855 - val_loss: 18.4731 - val_MinusLogProbMetric: 18.4731 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 354/1000
2023-09-26 22:16:03.698 
Epoch 354/1000 
	 loss: 18.6031, MinusLogProbMetric: 18.6031, val_loss: 18.4748, val_MinusLogProbMetric: 18.4748

Epoch 354: val_loss did not improve from 18.40099
196/196 - 63s - loss: 18.6031 - MinusLogProbMetric: 18.6031 - val_loss: 18.4748 - val_MinusLogProbMetric: 18.4748 - lr: 2.7778e-05 - 63s/epoch - 324ms/step
Epoch 355/1000
2023-09-26 22:17:07.936 
Epoch 355/1000 
	 loss: 18.4691, MinusLogProbMetric: 18.4691, val_loss: 18.4520, val_MinusLogProbMetric: 18.4520

Epoch 355: val_loss did not improve from 18.40099
196/196 - 64s - loss: 18.4691 - MinusLogProbMetric: 18.4691 - val_loss: 18.4520 - val_MinusLogProbMetric: 18.4520 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 356/1000
2023-09-26 22:18:12.376 
Epoch 356/1000 
	 loss: 18.5169, MinusLogProbMetric: 18.5169, val_loss: 18.7578, val_MinusLogProbMetric: 18.7578

Epoch 356: val_loss did not improve from 18.40099
196/196 - 64s - loss: 18.5169 - MinusLogProbMetric: 18.5169 - val_loss: 18.7578 - val_MinusLogProbMetric: 18.7578 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 357/1000
2023-09-26 22:19:16.850 
Epoch 357/1000 
	 loss: 18.6096, MinusLogProbMetric: 18.6096, val_loss: 18.5232, val_MinusLogProbMetric: 18.5232

Epoch 357: val_loss did not improve from 18.40099
196/196 - 64s - loss: 18.6096 - MinusLogProbMetric: 18.6096 - val_loss: 18.5232 - val_MinusLogProbMetric: 18.5232 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 358/1000
2023-09-26 22:20:20.716 
Epoch 358/1000 
	 loss: 18.4563, MinusLogProbMetric: 18.4563, val_loss: 18.5310, val_MinusLogProbMetric: 18.5310

Epoch 358: val_loss did not improve from 18.40099
196/196 - 64s - loss: 18.4563 - MinusLogProbMetric: 18.4563 - val_loss: 18.5310 - val_MinusLogProbMetric: 18.5310 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 359/1000
2023-09-26 22:21:24.689 
Epoch 359/1000 
	 loss: 18.4373, MinusLogProbMetric: 18.4373, val_loss: 18.4066, val_MinusLogProbMetric: 18.4066

Epoch 359: val_loss did not improve from 18.40099
196/196 - 64s - loss: 18.4373 - MinusLogProbMetric: 18.4373 - val_loss: 18.4066 - val_MinusLogProbMetric: 18.4066 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 360/1000
2023-09-26 22:22:28.969 
Epoch 360/1000 
	 loss: 18.5396, MinusLogProbMetric: 18.5396, val_loss: 18.7792, val_MinusLogProbMetric: 18.7792

Epoch 360: val_loss did not improve from 18.40099
196/196 - 64s - loss: 18.5396 - MinusLogProbMetric: 18.5396 - val_loss: 18.7792 - val_MinusLogProbMetric: 18.7792 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 361/1000
2023-09-26 22:23:32.872 
Epoch 361/1000 
	 loss: 18.5511, MinusLogProbMetric: 18.5511, val_loss: 18.4011, val_MinusLogProbMetric: 18.4011

Epoch 361: val_loss did not improve from 18.40099
196/196 - 64s - loss: 18.5511 - MinusLogProbMetric: 18.5511 - val_loss: 18.4011 - val_MinusLogProbMetric: 18.4011 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 362/1000
2023-09-26 22:24:36.625 
Epoch 362/1000 
	 loss: 18.4833, MinusLogProbMetric: 18.4833, val_loss: 18.3932, val_MinusLogProbMetric: 18.3932

Epoch 362: val_loss improved from 18.40099 to 18.39315, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.4833 - MinusLogProbMetric: 18.4833 - val_loss: 18.3932 - val_MinusLogProbMetric: 18.3932 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 363/1000
2023-09-26 22:25:41.529 
Epoch 363/1000 
	 loss: 18.4564, MinusLogProbMetric: 18.4564, val_loss: 18.6639, val_MinusLogProbMetric: 18.6639

Epoch 363: val_loss did not improve from 18.39315
196/196 - 64s - loss: 18.4564 - MinusLogProbMetric: 18.4564 - val_loss: 18.6639 - val_MinusLogProbMetric: 18.6639 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 364/1000
2023-09-26 22:26:45.554 
Epoch 364/1000 
	 loss: 18.4667, MinusLogProbMetric: 18.4667, val_loss: 18.5141, val_MinusLogProbMetric: 18.5141

Epoch 364: val_loss did not improve from 18.39315
196/196 - 64s - loss: 18.4667 - MinusLogProbMetric: 18.4667 - val_loss: 18.5141 - val_MinusLogProbMetric: 18.5141 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 365/1000
2023-09-26 22:27:50.297 
Epoch 365/1000 
	 loss: 18.4627, MinusLogProbMetric: 18.4627, val_loss: 18.3712, val_MinusLogProbMetric: 18.3712

Epoch 365: val_loss improved from 18.39315 to 18.37123, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 66s - loss: 18.4627 - MinusLogProbMetric: 18.4627 - val_loss: 18.3712 - val_MinusLogProbMetric: 18.3712 - lr: 2.7778e-05 - 66s/epoch - 336ms/step
Epoch 366/1000
2023-09-26 22:28:55.882 
Epoch 366/1000 
	 loss: 18.4887, MinusLogProbMetric: 18.4887, val_loss: 18.4811, val_MinusLogProbMetric: 18.4811

Epoch 366: val_loss did not improve from 18.37123
196/196 - 64s - loss: 18.4887 - MinusLogProbMetric: 18.4887 - val_loss: 18.4811 - val_MinusLogProbMetric: 18.4811 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 367/1000
2023-09-26 22:29:59.888 
Epoch 367/1000 
	 loss: 18.4527, MinusLogProbMetric: 18.4527, val_loss: 18.4866, val_MinusLogProbMetric: 18.4866

Epoch 367: val_loss did not improve from 18.37123
196/196 - 64s - loss: 18.4527 - MinusLogProbMetric: 18.4527 - val_loss: 18.4866 - val_MinusLogProbMetric: 18.4866 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 368/1000
2023-09-26 22:31:04.536 
Epoch 368/1000 
	 loss: 18.4631, MinusLogProbMetric: 18.4631, val_loss: 18.5866, val_MinusLogProbMetric: 18.5866

Epoch 368: val_loss did not improve from 18.37123
196/196 - 65s - loss: 18.4631 - MinusLogProbMetric: 18.4631 - val_loss: 18.5866 - val_MinusLogProbMetric: 18.5866 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 369/1000
2023-09-26 22:32:06.459 
Epoch 369/1000 
	 loss: 18.4888, MinusLogProbMetric: 18.4888, val_loss: 18.6413, val_MinusLogProbMetric: 18.6413

Epoch 369: val_loss did not improve from 18.37123
196/196 - 62s - loss: 18.4888 - MinusLogProbMetric: 18.4888 - val_loss: 18.6413 - val_MinusLogProbMetric: 18.6413 - lr: 2.7778e-05 - 62s/epoch - 316ms/step
Epoch 370/1000
2023-09-26 22:33:03.413 
Epoch 370/1000 
	 loss: 18.4659, MinusLogProbMetric: 18.4659, val_loss: 18.6428, val_MinusLogProbMetric: 18.6428

Epoch 370: val_loss did not improve from 18.37123
196/196 - 57s - loss: 18.4659 - MinusLogProbMetric: 18.4659 - val_loss: 18.6428 - val_MinusLogProbMetric: 18.6428 - lr: 2.7778e-05 - 57s/epoch - 291ms/step
Epoch 371/1000
2023-09-26 22:33:55.893 
Epoch 371/1000 
	 loss: 18.4090, MinusLogProbMetric: 18.4090, val_loss: 18.6445, val_MinusLogProbMetric: 18.6445

Epoch 371: val_loss did not improve from 18.37123
196/196 - 52s - loss: 18.4090 - MinusLogProbMetric: 18.4090 - val_loss: 18.6445 - val_MinusLogProbMetric: 18.6445 - lr: 2.7778e-05 - 52s/epoch - 268ms/step
Epoch 372/1000
2023-09-26 22:34:53.996 
Epoch 372/1000 
	 loss: 18.5005, MinusLogProbMetric: 18.5005, val_loss: 18.6073, val_MinusLogProbMetric: 18.6073

Epoch 372: val_loss did not improve from 18.37123
196/196 - 58s - loss: 18.5005 - MinusLogProbMetric: 18.5005 - val_loss: 18.6073 - val_MinusLogProbMetric: 18.6073 - lr: 2.7778e-05 - 58s/epoch - 296ms/step
Epoch 373/1000
2023-09-26 22:35:54.372 
Epoch 373/1000 
	 loss: 18.4359, MinusLogProbMetric: 18.4359, val_loss: 18.4861, val_MinusLogProbMetric: 18.4861

Epoch 373: val_loss did not improve from 18.37123
196/196 - 60s - loss: 18.4359 - MinusLogProbMetric: 18.4359 - val_loss: 18.4861 - val_MinusLogProbMetric: 18.4861 - lr: 2.7778e-05 - 60s/epoch - 308ms/step
Epoch 374/1000
2023-09-26 22:36:46.488 
Epoch 374/1000 
	 loss: 18.4950, MinusLogProbMetric: 18.4950, val_loss: 18.4845, val_MinusLogProbMetric: 18.4845

Epoch 374: val_loss did not improve from 18.37123
196/196 - 52s - loss: 18.4950 - MinusLogProbMetric: 18.4950 - val_loss: 18.4845 - val_MinusLogProbMetric: 18.4845 - lr: 2.7778e-05 - 52s/epoch - 266ms/step
Epoch 375/1000
2023-09-26 22:37:40.433 
Epoch 375/1000 
	 loss: 18.4442, MinusLogProbMetric: 18.4442, val_loss: 18.5097, val_MinusLogProbMetric: 18.5097

Epoch 375: val_loss did not improve from 18.37123
196/196 - 54s - loss: 18.4442 - MinusLogProbMetric: 18.4442 - val_loss: 18.5097 - val_MinusLogProbMetric: 18.5097 - lr: 2.7778e-05 - 54s/epoch - 275ms/step
Epoch 376/1000
2023-09-26 22:38:42.842 
Epoch 376/1000 
	 loss: 18.4746, MinusLogProbMetric: 18.4746, val_loss: 18.5541, val_MinusLogProbMetric: 18.5541

Epoch 376: val_loss did not improve from 18.37123
196/196 - 62s - loss: 18.4746 - MinusLogProbMetric: 18.4746 - val_loss: 18.5541 - val_MinusLogProbMetric: 18.5541 - lr: 2.7778e-05 - 62s/epoch - 318ms/step
Epoch 377/1000
2023-09-26 22:39:46.104 
Epoch 377/1000 
	 loss: 18.4575, MinusLogProbMetric: 18.4575, val_loss: 18.4328, val_MinusLogProbMetric: 18.4328

Epoch 377: val_loss did not improve from 18.37123
196/196 - 63s - loss: 18.4575 - MinusLogProbMetric: 18.4575 - val_loss: 18.4328 - val_MinusLogProbMetric: 18.4328 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 378/1000
2023-09-26 22:40:50.151 
Epoch 378/1000 
	 loss: 18.4434, MinusLogProbMetric: 18.4434, val_loss: 18.3614, val_MinusLogProbMetric: 18.3614

Epoch 378: val_loss improved from 18.37123 to 18.36143, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.4434 - MinusLogProbMetric: 18.4434 - val_loss: 18.3614 - val_MinusLogProbMetric: 18.3614 - lr: 2.7778e-05 - 65s/epoch - 331ms/step
Epoch 379/1000
2023-09-26 22:41:55.053 
Epoch 379/1000 
	 loss: 18.4833, MinusLogProbMetric: 18.4833, val_loss: 18.4731, val_MinusLogProbMetric: 18.4731

Epoch 379: val_loss did not improve from 18.36143
196/196 - 64s - loss: 18.4833 - MinusLogProbMetric: 18.4833 - val_loss: 18.4731 - val_MinusLogProbMetric: 18.4731 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 380/1000
2023-09-26 22:42:59.503 
Epoch 380/1000 
	 loss: 18.4207, MinusLogProbMetric: 18.4207, val_loss: 18.4004, val_MinusLogProbMetric: 18.4004

Epoch 380: val_loss did not improve from 18.36143
196/196 - 64s - loss: 18.4207 - MinusLogProbMetric: 18.4207 - val_loss: 18.4004 - val_MinusLogProbMetric: 18.4004 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 381/1000
2023-09-26 22:44:03.751 
Epoch 381/1000 
	 loss: 18.4205, MinusLogProbMetric: 18.4205, val_loss: 18.4250, val_MinusLogProbMetric: 18.4250

Epoch 381: val_loss did not improve from 18.36143
196/196 - 64s - loss: 18.4205 - MinusLogProbMetric: 18.4205 - val_loss: 18.4250 - val_MinusLogProbMetric: 18.4250 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 382/1000
2023-09-26 22:45:07.647 
Epoch 382/1000 
	 loss: 18.4679, MinusLogProbMetric: 18.4679, val_loss: 18.6653, val_MinusLogProbMetric: 18.6653

Epoch 382: val_loss did not improve from 18.36143
196/196 - 64s - loss: 18.4679 - MinusLogProbMetric: 18.4679 - val_loss: 18.6653 - val_MinusLogProbMetric: 18.6653 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 383/1000
2023-09-26 22:46:11.859 
Epoch 383/1000 
	 loss: 18.4761, MinusLogProbMetric: 18.4761, val_loss: 18.6447, val_MinusLogProbMetric: 18.6447

Epoch 383: val_loss did not improve from 18.36143
196/196 - 64s - loss: 18.4761 - MinusLogProbMetric: 18.4761 - val_loss: 18.6447 - val_MinusLogProbMetric: 18.6447 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 384/1000
2023-09-26 22:47:15.532 
Epoch 384/1000 
	 loss: 18.4339, MinusLogProbMetric: 18.4339, val_loss: 18.4141, val_MinusLogProbMetric: 18.4141

Epoch 384: val_loss did not improve from 18.36143
196/196 - 64s - loss: 18.4339 - MinusLogProbMetric: 18.4339 - val_loss: 18.4141 - val_MinusLogProbMetric: 18.4141 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 385/1000
2023-09-26 22:48:19.515 
Epoch 385/1000 
	 loss: 18.4712, MinusLogProbMetric: 18.4712, val_loss: 18.4513, val_MinusLogProbMetric: 18.4513

Epoch 385: val_loss did not improve from 18.36143
196/196 - 64s - loss: 18.4712 - MinusLogProbMetric: 18.4712 - val_loss: 18.4513 - val_MinusLogProbMetric: 18.4513 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 386/1000
2023-09-26 22:49:24.018 
Epoch 386/1000 
	 loss: 18.4456, MinusLogProbMetric: 18.4456, val_loss: 18.5555, val_MinusLogProbMetric: 18.5555

Epoch 386: val_loss did not improve from 18.36143
196/196 - 64s - loss: 18.4456 - MinusLogProbMetric: 18.4456 - val_loss: 18.5555 - val_MinusLogProbMetric: 18.5555 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 387/1000
2023-09-26 22:50:27.652 
Epoch 387/1000 
	 loss: 18.4223, MinusLogProbMetric: 18.4223, val_loss: 18.3324, val_MinusLogProbMetric: 18.3324

Epoch 387: val_loss improved from 18.36143 to 18.33242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.4223 - MinusLogProbMetric: 18.4223 - val_loss: 18.3324 - val_MinusLogProbMetric: 18.3324 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 388/1000
2023-09-26 22:51:32.198 
Epoch 388/1000 
	 loss: 18.4114, MinusLogProbMetric: 18.4114, val_loss: 18.4132, val_MinusLogProbMetric: 18.4132

Epoch 388: val_loss did not improve from 18.33242
196/196 - 64s - loss: 18.4114 - MinusLogProbMetric: 18.4114 - val_loss: 18.4132 - val_MinusLogProbMetric: 18.4132 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 389/1000
2023-09-26 22:52:35.838 
Epoch 389/1000 
	 loss: 18.4239, MinusLogProbMetric: 18.4239, val_loss: 18.3810, val_MinusLogProbMetric: 18.3810

Epoch 389: val_loss did not improve from 18.33242
196/196 - 64s - loss: 18.4239 - MinusLogProbMetric: 18.4239 - val_loss: 18.3810 - val_MinusLogProbMetric: 18.3810 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 390/1000
2023-09-26 22:53:39.292 
Epoch 390/1000 
	 loss: 18.4009, MinusLogProbMetric: 18.4009, val_loss: 18.6459, val_MinusLogProbMetric: 18.6459

Epoch 390: val_loss did not improve from 18.33242
196/196 - 63s - loss: 18.4009 - MinusLogProbMetric: 18.4009 - val_loss: 18.6459 - val_MinusLogProbMetric: 18.6459 - lr: 2.7778e-05 - 63s/epoch - 324ms/step
Epoch 391/1000
2023-09-26 22:54:43.273 
Epoch 391/1000 
	 loss: 18.4732, MinusLogProbMetric: 18.4732, val_loss: 18.4387, val_MinusLogProbMetric: 18.4387

Epoch 391: val_loss did not improve from 18.33242
196/196 - 64s - loss: 18.4732 - MinusLogProbMetric: 18.4732 - val_loss: 18.4387 - val_MinusLogProbMetric: 18.4387 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 392/1000
2023-09-26 22:55:46.785 
Epoch 392/1000 
	 loss: 18.3848, MinusLogProbMetric: 18.3848, val_loss: 18.4440, val_MinusLogProbMetric: 18.4440

Epoch 392: val_loss did not improve from 18.33242
196/196 - 64s - loss: 18.3848 - MinusLogProbMetric: 18.3848 - val_loss: 18.4440 - val_MinusLogProbMetric: 18.4440 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 393/1000
2023-09-26 22:56:50.355 
Epoch 393/1000 
	 loss: 18.3934, MinusLogProbMetric: 18.3934, val_loss: 18.3682, val_MinusLogProbMetric: 18.3682

Epoch 393: val_loss did not improve from 18.33242
196/196 - 64s - loss: 18.3934 - MinusLogProbMetric: 18.3934 - val_loss: 18.3682 - val_MinusLogProbMetric: 18.3682 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 394/1000
2023-09-26 22:57:54.671 
Epoch 394/1000 
	 loss: 18.4111, MinusLogProbMetric: 18.4111, val_loss: 18.3882, val_MinusLogProbMetric: 18.3882

Epoch 394: val_loss did not improve from 18.33242
196/196 - 64s - loss: 18.4111 - MinusLogProbMetric: 18.4111 - val_loss: 18.3882 - val_MinusLogProbMetric: 18.3882 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 395/1000
2023-09-26 22:58:59.067 
Epoch 395/1000 
	 loss: 18.3940, MinusLogProbMetric: 18.3940, val_loss: 18.3930, val_MinusLogProbMetric: 18.3930

Epoch 395: val_loss did not improve from 18.33242
196/196 - 64s - loss: 18.3940 - MinusLogProbMetric: 18.3940 - val_loss: 18.3930 - val_MinusLogProbMetric: 18.3930 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 396/1000
2023-09-26 23:00:03.278 
Epoch 396/1000 
	 loss: 18.4102, MinusLogProbMetric: 18.4102, val_loss: 18.3214, val_MinusLogProbMetric: 18.3214

Epoch 396: val_loss improved from 18.33242 to 18.32141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.4102 - MinusLogProbMetric: 18.4102 - val_loss: 18.3214 - val_MinusLogProbMetric: 18.3214 - lr: 2.7778e-05 - 65s/epoch - 332ms/step
Epoch 397/1000
2023-09-26 23:01:07.983 
Epoch 397/1000 
	 loss: 18.3702, MinusLogProbMetric: 18.3702, val_loss: 18.4666, val_MinusLogProbMetric: 18.4666

Epoch 397: val_loss did not improve from 18.32141
196/196 - 64s - loss: 18.3702 - MinusLogProbMetric: 18.3702 - val_loss: 18.4666 - val_MinusLogProbMetric: 18.4666 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 398/1000
2023-09-26 23:02:11.588 
Epoch 398/1000 
	 loss: 18.4061, MinusLogProbMetric: 18.4061, val_loss: 18.3260, val_MinusLogProbMetric: 18.3260

Epoch 398: val_loss did not improve from 18.32141
196/196 - 64s - loss: 18.4061 - MinusLogProbMetric: 18.4061 - val_loss: 18.3260 - val_MinusLogProbMetric: 18.3260 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 399/1000
2023-09-26 23:03:15.278 
Epoch 399/1000 
	 loss: 18.4288, MinusLogProbMetric: 18.4288, val_loss: 18.5547, val_MinusLogProbMetric: 18.5547

Epoch 399: val_loss did not improve from 18.32141
196/196 - 64s - loss: 18.4288 - MinusLogProbMetric: 18.4288 - val_loss: 18.5547 - val_MinusLogProbMetric: 18.5547 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 400/1000
2023-09-26 23:04:18.870 
Epoch 400/1000 
	 loss: 18.4596, MinusLogProbMetric: 18.4596, val_loss: 18.6048, val_MinusLogProbMetric: 18.6048

Epoch 400: val_loss did not improve from 18.32141
196/196 - 64s - loss: 18.4596 - MinusLogProbMetric: 18.4596 - val_loss: 18.6048 - val_MinusLogProbMetric: 18.6048 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 401/1000
2023-09-26 23:05:22.837 
Epoch 401/1000 
	 loss: 18.3915, MinusLogProbMetric: 18.3915, val_loss: 18.3282, val_MinusLogProbMetric: 18.3282

Epoch 401: val_loss did not improve from 18.32141
196/196 - 64s - loss: 18.3915 - MinusLogProbMetric: 18.3915 - val_loss: 18.3282 - val_MinusLogProbMetric: 18.3282 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 402/1000
2023-09-26 23:06:26.490 
Epoch 402/1000 
	 loss: 18.4139, MinusLogProbMetric: 18.4139, val_loss: 18.6375, val_MinusLogProbMetric: 18.6375

Epoch 402: val_loss did not improve from 18.32141
196/196 - 64s - loss: 18.4139 - MinusLogProbMetric: 18.4139 - val_loss: 18.6375 - val_MinusLogProbMetric: 18.6375 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 403/1000
2023-09-26 23:07:30.446 
Epoch 403/1000 
	 loss: 18.4236, MinusLogProbMetric: 18.4236, val_loss: 18.7056, val_MinusLogProbMetric: 18.7056

Epoch 403: val_loss did not improve from 18.32141
196/196 - 64s - loss: 18.4236 - MinusLogProbMetric: 18.4236 - val_loss: 18.7056 - val_MinusLogProbMetric: 18.7056 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 404/1000
2023-09-26 23:08:34.346 
Epoch 404/1000 
	 loss: 18.4012, MinusLogProbMetric: 18.4012, val_loss: 18.4422, val_MinusLogProbMetric: 18.4422

Epoch 404: val_loss did not improve from 18.32141
196/196 - 64s - loss: 18.4012 - MinusLogProbMetric: 18.4012 - val_loss: 18.4422 - val_MinusLogProbMetric: 18.4422 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 405/1000
2023-09-26 23:09:38.512 
Epoch 405/1000 
	 loss: 18.4109, MinusLogProbMetric: 18.4109, val_loss: 18.4291, val_MinusLogProbMetric: 18.4291

Epoch 405: val_loss did not improve from 18.32141
196/196 - 64s - loss: 18.4109 - MinusLogProbMetric: 18.4109 - val_loss: 18.4291 - val_MinusLogProbMetric: 18.4291 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 406/1000
2023-09-26 23:10:43.072 
Epoch 406/1000 
	 loss: 18.3828, MinusLogProbMetric: 18.3828, val_loss: 18.4009, val_MinusLogProbMetric: 18.4009

Epoch 406: val_loss did not improve from 18.32141
196/196 - 65s - loss: 18.3828 - MinusLogProbMetric: 18.3828 - val_loss: 18.4009 - val_MinusLogProbMetric: 18.4009 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 407/1000
2023-09-26 23:11:46.796 
Epoch 407/1000 
	 loss: 18.4232, MinusLogProbMetric: 18.4232, val_loss: 18.4794, val_MinusLogProbMetric: 18.4794

Epoch 407: val_loss did not improve from 18.32141
196/196 - 64s - loss: 18.4232 - MinusLogProbMetric: 18.4232 - val_loss: 18.4794 - val_MinusLogProbMetric: 18.4794 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 408/1000
2023-09-26 23:12:50.100 
Epoch 408/1000 
	 loss: 18.4085, MinusLogProbMetric: 18.4085, val_loss: 18.2927, val_MinusLogProbMetric: 18.2927

Epoch 408: val_loss improved from 18.32141 to 18.29268, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 64s - loss: 18.4085 - MinusLogProbMetric: 18.4085 - val_loss: 18.2927 - val_MinusLogProbMetric: 18.2927 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 409/1000
2023-09-26 23:13:54.678 
Epoch 409/1000 
	 loss: 18.3844, MinusLogProbMetric: 18.3844, val_loss: 18.7780, val_MinusLogProbMetric: 18.7780

Epoch 409: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.3844 - MinusLogProbMetric: 18.3844 - val_loss: 18.7780 - val_MinusLogProbMetric: 18.7780 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 410/1000
2023-09-26 23:14:58.301 
Epoch 410/1000 
	 loss: 18.3555, MinusLogProbMetric: 18.3555, val_loss: 18.3343, val_MinusLogProbMetric: 18.3343

Epoch 410: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.3555 - MinusLogProbMetric: 18.3555 - val_loss: 18.3343 - val_MinusLogProbMetric: 18.3343 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 411/1000
2023-09-26 23:16:02.144 
Epoch 411/1000 
	 loss: 18.3626, MinusLogProbMetric: 18.3626, val_loss: 18.3182, val_MinusLogProbMetric: 18.3182

Epoch 411: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.3626 - MinusLogProbMetric: 18.3626 - val_loss: 18.3182 - val_MinusLogProbMetric: 18.3182 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 412/1000
2023-09-26 23:17:06.233 
Epoch 412/1000 
	 loss: 18.4114, MinusLogProbMetric: 18.4114, val_loss: 18.5432, val_MinusLogProbMetric: 18.5432

Epoch 412: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.4114 - MinusLogProbMetric: 18.4114 - val_loss: 18.5432 - val_MinusLogProbMetric: 18.5432 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 413/1000
2023-09-26 23:18:10.291 
Epoch 413/1000 
	 loss: 18.5633, MinusLogProbMetric: 18.5633, val_loss: 21.0135, val_MinusLogProbMetric: 21.0135

Epoch 413: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.5633 - MinusLogProbMetric: 18.5633 - val_loss: 21.0135 - val_MinusLogProbMetric: 21.0135 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 414/1000
2023-09-26 23:19:14.751 
Epoch 414/1000 
	 loss: 18.5614, MinusLogProbMetric: 18.5614, val_loss: 18.6345, val_MinusLogProbMetric: 18.6345

Epoch 414: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.5614 - MinusLogProbMetric: 18.5614 - val_loss: 18.6345 - val_MinusLogProbMetric: 18.6345 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 415/1000
2023-09-26 23:20:18.722 
Epoch 415/1000 
	 loss: 18.3856, MinusLogProbMetric: 18.3856, val_loss: 18.5730, val_MinusLogProbMetric: 18.5730

Epoch 415: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.3856 - MinusLogProbMetric: 18.3856 - val_loss: 18.5730 - val_MinusLogProbMetric: 18.5730 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 416/1000
2023-09-26 23:21:22.994 
Epoch 416/1000 
	 loss: 18.3929, MinusLogProbMetric: 18.3929, val_loss: 18.4744, val_MinusLogProbMetric: 18.4744

Epoch 416: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.3929 - MinusLogProbMetric: 18.3929 - val_loss: 18.4744 - val_MinusLogProbMetric: 18.4744 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 417/1000
2023-09-26 23:22:26.681 
Epoch 417/1000 
	 loss: 18.4184, MinusLogProbMetric: 18.4184, val_loss: 18.4513, val_MinusLogProbMetric: 18.4513

Epoch 417: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.4184 - MinusLogProbMetric: 18.4184 - val_loss: 18.4513 - val_MinusLogProbMetric: 18.4513 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 418/1000
2023-09-26 23:23:30.006 
Epoch 418/1000 
	 loss: 18.4243, MinusLogProbMetric: 18.4243, val_loss: 18.3849, val_MinusLogProbMetric: 18.3849

Epoch 418: val_loss did not improve from 18.29268
196/196 - 63s - loss: 18.4243 - MinusLogProbMetric: 18.4243 - val_loss: 18.3849 - val_MinusLogProbMetric: 18.3849 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 419/1000
2023-09-26 23:24:33.612 
Epoch 419/1000 
	 loss: 18.3651, MinusLogProbMetric: 18.3651, val_loss: 18.3666, val_MinusLogProbMetric: 18.3666

Epoch 419: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.3651 - MinusLogProbMetric: 18.3651 - val_loss: 18.3666 - val_MinusLogProbMetric: 18.3666 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 420/1000
2023-09-26 23:25:37.295 
Epoch 420/1000 
	 loss: 18.3317, MinusLogProbMetric: 18.3317, val_loss: 18.3922, val_MinusLogProbMetric: 18.3922

Epoch 420: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.3317 - MinusLogProbMetric: 18.3317 - val_loss: 18.3922 - val_MinusLogProbMetric: 18.3922 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 421/1000
2023-09-26 23:26:41.001 
Epoch 421/1000 
	 loss: 18.3395, MinusLogProbMetric: 18.3395, val_loss: 18.5523, val_MinusLogProbMetric: 18.5523

Epoch 421: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.3395 - MinusLogProbMetric: 18.3395 - val_loss: 18.5523 - val_MinusLogProbMetric: 18.5523 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 422/1000
2023-09-26 23:27:44.634 
Epoch 422/1000 
	 loss: 18.4044, MinusLogProbMetric: 18.4044, val_loss: 18.3371, val_MinusLogProbMetric: 18.3371

Epoch 422: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.4044 - MinusLogProbMetric: 18.4044 - val_loss: 18.3371 - val_MinusLogProbMetric: 18.3371 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 423/1000
2023-09-26 23:28:48.533 
Epoch 423/1000 
	 loss: 18.3839, MinusLogProbMetric: 18.3839, val_loss: 18.4505, val_MinusLogProbMetric: 18.4505

Epoch 423: val_loss did not improve from 18.29268
196/196 - 64s - loss: 18.3839 - MinusLogProbMetric: 18.3839 - val_loss: 18.4505 - val_MinusLogProbMetric: 18.4505 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 424/1000
2023-09-26 23:29:52.383 
Epoch 424/1000 
	 loss: 18.3439, MinusLogProbMetric: 18.3439, val_loss: 18.2585, val_MinusLogProbMetric: 18.2585

Epoch 424: val_loss improved from 18.29268 to 18.25849, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.3439 - MinusLogProbMetric: 18.3439 - val_loss: 18.2585 - val_MinusLogProbMetric: 18.2585 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 425/1000
2023-09-26 23:30:56.863 
Epoch 425/1000 
	 loss: 18.3569, MinusLogProbMetric: 18.3569, val_loss: 18.3622, val_MinusLogProbMetric: 18.3622

Epoch 425: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3569 - MinusLogProbMetric: 18.3569 - val_loss: 18.3622 - val_MinusLogProbMetric: 18.3622 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 426/1000
2023-09-26 23:32:00.572 
Epoch 426/1000 
	 loss: 18.3699, MinusLogProbMetric: 18.3699, val_loss: 18.6171, val_MinusLogProbMetric: 18.6171

Epoch 426: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3699 - MinusLogProbMetric: 18.3699 - val_loss: 18.6171 - val_MinusLogProbMetric: 18.6171 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 427/1000
2023-09-26 23:33:04.320 
Epoch 427/1000 
	 loss: 18.3417, MinusLogProbMetric: 18.3417, val_loss: 18.2935, val_MinusLogProbMetric: 18.2935

Epoch 427: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3417 - MinusLogProbMetric: 18.3417 - val_loss: 18.2935 - val_MinusLogProbMetric: 18.2935 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 428/1000
2023-09-26 23:34:08.045 
Epoch 428/1000 
	 loss: 18.4204, MinusLogProbMetric: 18.4204, val_loss: 18.2964, val_MinusLogProbMetric: 18.2964

Epoch 428: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.4204 - MinusLogProbMetric: 18.4204 - val_loss: 18.2964 - val_MinusLogProbMetric: 18.2964 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 429/1000
2023-09-26 23:35:11.629 
Epoch 429/1000 
	 loss: 18.4036, MinusLogProbMetric: 18.4036, val_loss: 18.4376, val_MinusLogProbMetric: 18.4376

Epoch 429: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.4036 - MinusLogProbMetric: 18.4036 - val_loss: 18.4376 - val_MinusLogProbMetric: 18.4376 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 430/1000
2023-09-26 23:36:15.109 
Epoch 430/1000 
	 loss: 18.3714, MinusLogProbMetric: 18.3714, val_loss: 18.5649, val_MinusLogProbMetric: 18.5649

Epoch 430: val_loss did not improve from 18.25849
196/196 - 63s - loss: 18.3714 - MinusLogProbMetric: 18.3714 - val_loss: 18.5649 - val_MinusLogProbMetric: 18.5649 - lr: 2.7778e-05 - 63s/epoch - 324ms/step
Epoch 431/1000
2023-09-26 23:37:18.778 
Epoch 431/1000 
	 loss: 18.3689, MinusLogProbMetric: 18.3689, val_loss: 18.4161, val_MinusLogProbMetric: 18.4161

Epoch 431: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3689 - MinusLogProbMetric: 18.3689 - val_loss: 18.4161 - val_MinusLogProbMetric: 18.4161 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 432/1000
2023-09-26 23:38:22.427 
Epoch 432/1000 
	 loss: 18.4155, MinusLogProbMetric: 18.4155, val_loss: 18.3158, val_MinusLogProbMetric: 18.3158

Epoch 432: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.4155 - MinusLogProbMetric: 18.4155 - val_loss: 18.3158 - val_MinusLogProbMetric: 18.3158 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 433/1000
2023-09-26 23:39:26.471 
Epoch 433/1000 
	 loss: 18.3365, MinusLogProbMetric: 18.3365, val_loss: 18.3640, val_MinusLogProbMetric: 18.3640

Epoch 433: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3365 - MinusLogProbMetric: 18.3365 - val_loss: 18.3640 - val_MinusLogProbMetric: 18.3640 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 434/1000
2023-09-26 23:40:30.706 
Epoch 434/1000 
	 loss: 18.3501, MinusLogProbMetric: 18.3501, val_loss: 18.5569, val_MinusLogProbMetric: 18.5569

Epoch 434: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3501 - MinusLogProbMetric: 18.3501 - val_loss: 18.5569 - val_MinusLogProbMetric: 18.5569 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 435/1000
2023-09-26 23:41:34.165 
Epoch 435/1000 
	 loss: 18.3779, MinusLogProbMetric: 18.3779, val_loss: 18.4441, val_MinusLogProbMetric: 18.4441

Epoch 435: val_loss did not improve from 18.25849
196/196 - 63s - loss: 18.3779 - MinusLogProbMetric: 18.3779 - val_loss: 18.4441 - val_MinusLogProbMetric: 18.4441 - lr: 2.7778e-05 - 63s/epoch - 324ms/step
Epoch 436/1000
2023-09-26 23:42:38.410 
Epoch 436/1000 
	 loss: 18.3401, MinusLogProbMetric: 18.3401, val_loss: 18.3004, val_MinusLogProbMetric: 18.3004

Epoch 436: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3401 - MinusLogProbMetric: 18.3401 - val_loss: 18.3004 - val_MinusLogProbMetric: 18.3004 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 437/1000
2023-09-26 23:43:42.456 
Epoch 437/1000 
	 loss: 18.3584, MinusLogProbMetric: 18.3584, val_loss: 18.3772, val_MinusLogProbMetric: 18.3772

Epoch 437: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3584 - MinusLogProbMetric: 18.3584 - val_loss: 18.3772 - val_MinusLogProbMetric: 18.3772 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 438/1000
2023-09-26 23:44:46.578 
Epoch 438/1000 
	 loss: 18.3215, MinusLogProbMetric: 18.3215, val_loss: 18.4187, val_MinusLogProbMetric: 18.4187

Epoch 438: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3215 - MinusLogProbMetric: 18.3215 - val_loss: 18.4187 - val_MinusLogProbMetric: 18.4187 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 439/1000
2023-09-26 23:45:50.439 
Epoch 439/1000 
	 loss: 18.3842, MinusLogProbMetric: 18.3842, val_loss: 18.3255, val_MinusLogProbMetric: 18.3255

Epoch 439: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3842 - MinusLogProbMetric: 18.3842 - val_loss: 18.3255 - val_MinusLogProbMetric: 18.3255 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 440/1000
2023-09-26 23:46:54.586 
Epoch 440/1000 
	 loss: 18.3523, MinusLogProbMetric: 18.3523, val_loss: 18.3726, val_MinusLogProbMetric: 18.3726

Epoch 440: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3523 - MinusLogProbMetric: 18.3523 - val_loss: 18.3726 - val_MinusLogProbMetric: 18.3726 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 441/1000
2023-09-26 23:47:58.664 
Epoch 441/1000 
	 loss: 18.3070, MinusLogProbMetric: 18.3070, val_loss: 18.3814, val_MinusLogProbMetric: 18.3814

Epoch 441: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3070 - MinusLogProbMetric: 18.3070 - val_loss: 18.3814 - val_MinusLogProbMetric: 18.3814 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 442/1000
2023-09-26 23:49:02.290 
Epoch 442/1000 
	 loss: 18.3784, MinusLogProbMetric: 18.3784, val_loss: 18.3015, val_MinusLogProbMetric: 18.3015

Epoch 442: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3784 - MinusLogProbMetric: 18.3784 - val_loss: 18.3015 - val_MinusLogProbMetric: 18.3015 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 443/1000
2023-09-26 23:50:05.851 
Epoch 443/1000 
	 loss: 18.3073, MinusLogProbMetric: 18.3073, val_loss: 18.4267, val_MinusLogProbMetric: 18.4267

Epoch 443: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3073 - MinusLogProbMetric: 18.3073 - val_loss: 18.4267 - val_MinusLogProbMetric: 18.4267 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 444/1000
2023-09-26 23:51:10.095 
Epoch 444/1000 
	 loss: 18.4029, MinusLogProbMetric: 18.4029, val_loss: 18.3026, val_MinusLogProbMetric: 18.3026

Epoch 444: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.4029 - MinusLogProbMetric: 18.4029 - val_loss: 18.3026 - val_MinusLogProbMetric: 18.3026 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 445/1000
2023-09-26 23:52:13.965 
Epoch 445/1000 
	 loss: 18.3273, MinusLogProbMetric: 18.3273, val_loss: 18.2668, val_MinusLogProbMetric: 18.2668

Epoch 445: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3273 - MinusLogProbMetric: 18.3273 - val_loss: 18.2668 - val_MinusLogProbMetric: 18.2668 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 446/1000
2023-09-26 23:53:18.246 
Epoch 446/1000 
	 loss: 18.3552, MinusLogProbMetric: 18.3552, val_loss: 18.6620, val_MinusLogProbMetric: 18.6620

Epoch 446: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3552 - MinusLogProbMetric: 18.3552 - val_loss: 18.6620 - val_MinusLogProbMetric: 18.6620 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 447/1000
2023-09-26 23:54:22.500 
Epoch 447/1000 
	 loss: 18.3199, MinusLogProbMetric: 18.3199, val_loss: 18.6806, val_MinusLogProbMetric: 18.6806

Epoch 447: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3199 - MinusLogProbMetric: 18.3199 - val_loss: 18.6806 - val_MinusLogProbMetric: 18.6806 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 448/1000
2023-09-26 23:55:25.433 
Epoch 448/1000 
	 loss: 18.3337, MinusLogProbMetric: 18.3337, val_loss: 18.2920, val_MinusLogProbMetric: 18.2920

Epoch 448: val_loss did not improve from 18.25849
196/196 - 63s - loss: 18.3337 - MinusLogProbMetric: 18.3337 - val_loss: 18.2920 - val_MinusLogProbMetric: 18.2920 - lr: 2.7778e-05 - 63s/epoch - 321ms/step
Epoch 449/1000
2023-09-26 23:56:29.212 
Epoch 449/1000 
	 loss: 18.3405, MinusLogProbMetric: 18.3405, val_loss: 18.3770, val_MinusLogProbMetric: 18.3770

Epoch 449: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3405 - MinusLogProbMetric: 18.3405 - val_loss: 18.3770 - val_MinusLogProbMetric: 18.3770 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 450/1000
2023-09-26 23:57:33.138 
Epoch 450/1000 
	 loss: 18.3395, MinusLogProbMetric: 18.3395, val_loss: 18.4424, val_MinusLogProbMetric: 18.4424

Epoch 450: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3395 - MinusLogProbMetric: 18.3395 - val_loss: 18.4424 - val_MinusLogProbMetric: 18.4424 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 451/1000
2023-09-26 23:58:36.965 
Epoch 451/1000 
	 loss: 18.3647, MinusLogProbMetric: 18.3647, val_loss: 18.5546, val_MinusLogProbMetric: 18.5546

Epoch 451: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3647 - MinusLogProbMetric: 18.3647 - val_loss: 18.5546 - val_MinusLogProbMetric: 18.5546 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 452/1000
2023-09-26 23:59:40.778 
Epoch 452/1000 
	 loss: 18.4698, MinusLogProbMetric: 18.4698, val_loss: 18.4200, val_MinusLogProbMetric: 18.4200

Epoch 452: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.4698 - MinusLogProbMetric: 18.4698 - val_loss: 18.4200 - val_MinusLogProbMetric: 18.4200 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 453/1000
2023-09-27 00:00:44.820 
Epoch 453/1000 
	 loss: 18.3270, MinusLogProbMetric: 18.3270, val_loss: 18.2807, val_MinusLogProbMetric: 18.2807

Epoch 453: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3270 - MinusLogProbMetric: 18.3270 - val_loss: 18.2807 - val_MinusLogProbMetric: 18.2807 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 454/1000
2023-09-27 00:01:48.686 
Epoch 454/1000 
	 loss: 18.3217, MinusLogProbMetric: 18.3217, val_loss: 18.3078, val_MinusLogProbMetric: 18.3078

Epoch 454: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3217 - MinusLogProbMetric: 18.3217 - val_loss: 18.3078 - val_MinusLogProbMetric: 18.3078 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 455/1000
2023-09-27 00:02:52.928 
Epoch 455/1000 
	 loss: 18.3537, MinusLogProbMetric: 18.3537, val_loss: 18.3049, val_MinusLogProbMetric: 18.3049

Epoch 455: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3537 - MinusLogProbMetric: 18.3537 - val_loss: 18.3049 - val_MinusLogProbMetric: 18.3049 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 456/1000
2023-09-27 00:03:56.903 
Epoch 456/1000 
	 loss: 18.2960, MinusLogProbMetric: 18.2960, val_loss: 18.4386, val_MinusLogProbMetric: 18.4386

Epoch 456: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.2960 - MinusLogProbMetric: 18.2960 - val_loss: 18.4386 - val_MinusLogProbMetric: 18.4386 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 457/1000
2023-09-27 00:05:00.697 
Epoch 457/1000 
	 loss: 18.3290, MinusLogProbMetric: 18.3290, val_loss: 18.2793, val_MinusLogProbMetric: 18.2793

Epoch 457: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3290 - MinusLogProbMetric: 18.3290 - val_loss: 18.2793 - val_MinusLogProbMetric: 18.2793 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 458/1000
2023-09-27 00:06:03.986 
Epoch 458/1000 
	 loss: 18.3123, MinusLogProbMetric: 18.3123, val_loss: 18.4924, val_MinusLogProbMetric: 18.4924

Epoch 458: val_loss did not improve from 18.25849
196/196 - 63s - loss: 18.3123 - MinusLogProbMetric: 18.3123 - val_loss: 18.4924 - val_MinusLogProbMetric: 18.4924 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 459/1000
2023-09-27 00:07:08.520 
Epoch 459/1000 
	 loss: 18.3324, MinusLogProbMetric: 18.3324, val_loss: 18.6768, val_MinusLogProbMetric: 18.6768

Epoch 459: val_loss did not improve from 18.25849
196/196 - 65s - loss: 18.3324 - MinusLogProbMetric: 18.3324 - val_loss: 18.6768 - val_MinusLogProbMetric: 18.6768 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 460/1000
2023-09-27 00:08:11.878 
Epoch 460/1000 
	 loss: 18.4635, MinusLogProbMetric: 18.4635, val_loss: 18.4274, val_MinusLogProbMetric: 18.4274

Epoch 460: val_loss did not improve from 18.25849
196/196 - 63s - loss: 18.4635 - MinusLogProbMetric: 18.4635 - val_loss: 18.4274 - val_MinusLogProbMetric: 18.4274 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 461/1000
2023-09-27 00:09:15.576 
Epoch 461/1000 
	 loss: 18.3327, MinusLogProbMetric: 18.3327, val_loss: 18.4585, val_MinusLogProbMetric: 18.4585

Epoch 461: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.3327 - MinusLogProbMetric: 18.3327 - val_loss: 18.4585 - val_MinusLogProbMetric: 18.4585 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 462/1000
2023-09-27 00:10:19.346 
Epoch 462/1000 
	 loss: 18.2891, MinusLogProbMetric: 18.2891, val_loss: 18.5397, val_MinusLogProbMetric: 18.5397

Epoch 462: val_loss did not improve from 18.25849
196/196 - 64s - loss: 18.2891 - MinusLogProbMetric: 18.2891 - val_loss: 18.5397 - val_MinusLogProbMetric: 18.5397 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 463/1000
2023-09-27 00:11:23.208 
Epoch 463/1000 
	 loss: 18.3335, MinusLogProbMetric: 18.3335, val_loss: 18.2288, val_MinusLogProbMetric: 18.2288

Epoch 463: val_loss improved from 18.25849 to 18.22877, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.3335 - MinusLogProbMetric: 18.3335 - val_loss: 18.2288 - val_MinusLogProbMetric: 18.2288 - lr: 2.7778e-05 - 65s/epoch - 331ms/step
Epoch 464/1000
2023-09-27 00:12:27.954 
Epoch 464/1000 
	 loss: 18.2849, MinusLogProbMetric: 18.2849, val_loss: 18.3444, val_MinusLogProbMetric: 18.3444

Epoch 464: val_loss did not improve from 18.22877
196/196 - 64s - loss: 18.2849 - MinusLogProbMetric: 18.2849 - val_loss: 18.3444 - val_MinusLogProbMetric: 18.3444 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 465/1000
2023-09-27 00:13:31.216 
Epoch 465/1000 
	 loss: 18.3040, MinusLogProbMetric: 18.3040, val_loss: 18.2337, val_MinusLogProbMetric: 18.2337

Epoch 465: val_loss did not improve from 18.22877
196/196 - 63s - loss: 18.3040 - MinusLogProbMetric: 18.3040 - val_loss: 18.2337 - val_MinusLogProbMetric: 18.2337 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 466/1000
2023-09-27 00:14:34.616 
Epoch 466/1000 
	 loss: 18.4070, MinusLogProbMetric: 18.4070, val_loss: 18.6865, val_MinusLogProbMetric: 18.6865

Epoch 466: val_loss did not improve from 18.22877
196/196 - 63s - loss: 18.4070 - MinusLogProbMetric: 18.4070 - val_loss: 18.6865 - val_MinusLogProbMetric: 18.6865 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 467/1000
2023-09-27 00:15:38.716 
Epoch 467/1000 
	 loss: 18.3894, MinusLogProbMetric: 18.3894, val_loss: 18.3740, val_MinusLogProbMetric: 18.3740

Epoch 467: val_loss did not improve from 18.22877
196/196 - 64s - loss: 18.3894 - MinusLogProbMetric: 18.3894 - val_loss: 18.3740 - val_MinusLogProbMetric: 18.3740 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 468/1000
2023-09-27 00:16:42.059 
Epoch 468/1000 
	 loss: 18.2879, MinusLogProbMetric: 18.2879, val_loss: 18.2011, val_MinusLogProbMetric: 18.2011

Epoch 468: val_loss improved from 18.22877 to 18.20114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 64s - loss: 18.2879 - MinusLogProbMetric: 18.2879 - val_loss: 18.2011 - val_MinusLogProbMetric: 18.2011 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 469/1000
2023-09-27 00:17:47.409 
Epoch 469/1000 
	 loss: 18.2902, MinusLogProbMetric: 18.2902, val_loss: 18.3860, val_MinusLogProbMetric: 18.3860

Epoch 469: val_loss did not improve from 18.20114
196/196 - 64s - loss: 18.2902 - MinusLogProbMetric: 18.2902 - val_loss: 18.3860 - val_MinusLogProbMetric: 18.3860 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 470/1000
2023-09-27 00:18:50.993 
Epoch 470/1000 
	 loss: 18.2507, MinusLogProbMetric: 18.2507, val_loss: 18.3915, val_MinusLogProbMetric: 18.3915

Epoch 470: val_loss did not improve from 18.20114
196/196 - 64s - loss: 18.2507 - MinusLogProbMetric: 18.2507 - val_loss: 18.3915 - val_MinusLogProbMetric: 18.3915 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 471/1000
2023-09-27 00:19:54.958 
Epoch 471/1000 
	 loss: 18.2975, MinusLogProbMetric: 18.2975, val_loss: 18.2690, val_MinusLogProbMetric: 18.2690

Epoch 471: val_loss did not improve from 18.20114
196/196 - 64s - loss: 18.2975 - MinusLogProbMetric: 18.2975 - val_loss: 18.2690 - val_MinusLogProbMetric: 18.2690 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 472/1000
2023-09-27 00:20:58.775 
Epoch 472/1000 
	 loss: 18.3600, MinusLogProbMetric: 18.3600, val_loss: 18.7773, val_MinusLogProbMetric: 18.7773

Epoch 472: val_loss did not improve from 18.20114
196/196 - 64s - loss: 18.3600 - MinusLogProbMetric: 18.3600 - val_loss: 18.7773 - val_MinusLogProbMetric: 18.7773 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 473/1000
2023-09-27 00:22:02.516 
Epoch 473/1000 
	 loss: 18.3361, MinusLogProbMetric: 18.3361, val_loss: 18.4026, val_MinusLogProbMetric: 18.4026

Epoch 473: val_loss did not improve from 18.20114
196/196 - 64s - loss: 18.3361 - MinusLogProbMetric: 18.3361 - val_loss: 18.4026 - val_MinusLogProbMetric: 18.4026 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 474/1000
2023-09-27 00:23:06.172 
Epoch 474/1000 
	 loss: 18.3313, MinusLogProbMetric: 18.3313, val_loss: 18.6271, val_MinusLogProbMetric: 18.6271

Epoch 474: val_loss did not improve from 18.20114
196/196 - 64s - loss: 18.3313 - MinusLogProbMetric: 18.3313 - val_loss: 18.6271 - val_MinusLogProbMetric: 18.6271 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 475/1000
2023-09-27 00:24:07.834 
Epoch 475/1000 
	 loss: 18.3442, MinusLogProbMetric: 18.3442, val_loss: 18.4794, val_MinusLogProbMetric: 18.4794

Epoch 475: val_loss did not improve from 18.20114
196/196 - 62s - loss: 18.3442 - MinusLogProbMetric: 18.3442 - val_loss: 18.4794 - val_MinusLogProbMetric: 18.4794 - lr: 2.7778e-05 - 62s/epoch - 315ms/step
Epoch 476/1000
2023-09-27 00:25:05.589 
Epoch 476/1000 
	 loss: 18.3232, MinusLogProbMetric: 18.3232, val_loss: 18.5956, val_MinusLogProbMetric: 18.5956

Epoch 476: val_loss did not improve from 18.20114
196/196 - 58s - loss: 18.3232 - MinusLogProbMetric: 18.3232 - val_loss: 18.5956 - val_MinusLogProbMetric: 18.5956 - lr: 2.7778e-05 - 58s/epoch - 295ms/step
Epoch 477/1000
2023-09-27 00:26:05.348 
Epoch 477/1000 
	 loss: 18.3209, MinusLogProbMetric: 18.3209, val_loss: 18.2443, val_MinusLogProbMetric: 18.2443

Epoch 477: val_loss did not improve from 18.20114
196/196 - 60s - loss: 18.3209 - MinusLogProbMetric: 18.3209 - val_loss: 18.2443 - val_MinusLogProbMetric: 18.2443 - lr: 2.7778e-05 - 60s/epoch - 305ms/step
Epoch 478/1000
2023-09-27 00:27:08.668 
Epoch 478/1000 
	 loss: 18.2901, MinusLogProbMetric: 18.2901, val_loss: 18.5565, val_MinusLogProbMetric: 18.5565

Epoch 478: val_loss did not improve from 18.20114
196/196 - 63s - loss: 18.2901 - MinusLogProbMetric: 18.2901 - val_loss: 18.5565 - val_MinusLogProbMetric: 18.5565 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 479/1000
2023-09-27 00:28:12.248 
Epoch 479/1000 
	 loss: 18.3275, MinusLogProbMetric: 18.3275, val_loss: 18.2411, val_MinusLogProbMetric: 18.2411

Epoch 479: val_loss did not improve from 18.20114
196/196 - 64s - loss: 18.3275 - MinusLogProbMetric: 18.3275 - val_loss: 18.2411 - val_MinusLogProbMetric: 18.2411 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 480/1000
2023-09-27 00:29:15.828 
Epoch 480/1000 
	 loss: 18.2943, MinusLogProbMetric: 18.2943, val_loss: 18.3242, val_MinusLogProbMetric: 18.3242

Epoch 480: val_loss did not improve from 18.20114
196/196 - 64s - loss: 18.2943 - MinusLogProbMetric: 18.2943 - val_loss: 18.3242 - val_MinusLogProbMetric: 18.3242 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 481/1000
2023-09-27 00:30:19.117 
Epoch 481/1000 
	 loss: 18.3709, MinusLogProbMetric: 18.3709, val_loss: 18.3882, val_MinusLogProbMetric: 18.3882

Epoch 481: val_loss did not improve from 18.20114
196/196 - 63s - loss: 18.3709 - MinusLogProbMetric: 18.3709 - val_loss: 18.3882 - val_MinusLogProbMetric: 18.3882 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 482/1000
2023-09-27 00:31:22.074 
Epoch 482/1000 
	 loss: 18.3277, MinusLogProbMetric: 18.3277, val_loss: 18.4289, val_MinusLogProbMetric: 18.4289

Epoch 482: val_loss did not improve from 18.20114
196/196 - 63s - loss: 18.3277 - MinusLogProbMetric: 18.3277 - val_loss: 18.4289 - val_MinusLogProbMetric: 18.4289 - lr: 2.7778e-05 - 63s/epoch - 321ms/step
Epoch 483/1000
2023-09-27 00:32:25.666 
Epoch 483/1000 
	 loss: 18.2957, MinusLogProbMetric: 18.2957, val_loss: 18.5115, val_MinusLogProbMetric: 18.5115

Epoch 483: val_loss did not improve from 18.20114
196/196 - 64s - loss: 18.2957 - MinusLogProbMetric: 18.2957 - val_loss: 18.5115 - val_MinusLogProbMetric: 18.5115 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 484/1000
2023-09-27 00:33:29.049 
Epoch 484/1000 
	 loss: 18.3042, MinusLogProbMetric: 18.3042, val_loss: 18.3185, val_MinusLogProbMetric: 18.3185

Epoch 484: val_loss did not improve from 18.20114
196/196 - 63s - loss: 18.3042 - MinusLogProbMetric: 18.3042 - val_loss: 18.3185 - val_MinusLogProbMetric: 18.3185 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 485/1000
2023-09-27 00:34:32.450 
Epoch 485/1000 
	 loss: 18.3396, MinusLogProbMetric: 18.3396, val_loss: 18.2836, val_MinusLogProbMetric: 18.2836

Epoch 485: val_loss did not improve from 18.20114
196/196 - 63s - loss: 18.3396 - MinusLogProbMetric: 18.3396 - val_loss: 18.2836 - val_MinusLogProbMetric: 18.2836 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 486/1000
2023-09-27 00:35:35.740 
Epoch 486/1000 
	 loss: 18.2812, MinusLogProbMetric: 18.2812, val_loss: 18.5481, val_MinusLogProbMetric: 18.5481

Epoch 486: val_loss did not improve from 18.20114
196/196 - 63s - loss: 18.2812 - MinusLogProbMetric: 18.2812 - val_loss: 18.5481 - val_MinusLogProbMetric: 18.5481 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 487/1000
2023-09-27 00:36:39.214 
Epoch 487/1000 
	 loss: 18.3073, MinusLogProbMetric: 18.3073, val_loss: 18.2016, val_MinusLogProbMetric: 18.2016

Epoch 487: val_loss did not improve from 18.20114
196/196 - 63s - loss: 18.3073 - MinusLogProbMetric: 18.3073 - val_loss: 18.2016 - val_MinusLogProbMetric: 18.2016 - lr: 2.7778e-05 - 63s/epoch - 324ms/step
Epoch 488/1000
2023-09-27 00:37:42.375 
Epoch 488/1000 
	 loss: 18.3168, MinusLogProbMetric: 18.3168, val_loss: 18.4275, val_MinusLogProbMetric: 18.4275

Epoch 488: val_loss did not improve from 18.20114
196/196 - 63s - loss: 18.3168 - MinusLogProbMetric: 18.3168 - val_loss: 18.4275 - val_MinusLogProbMetric: 18.4275 - lr: 2.7778e-05 - 63s/epoch - 322ms/step
Epoch 489/1000
2023-09-27 00:38:45.384 
Epoch 489/1000 
	 loss: 18.2818, MinusLogProbMetric: 18.2818, val_loss: 18.4743, val_MinusLogProbMetric: 18.4743

Epoch 489: val_loss did not improve from 18.20114
196/196 - 63s - loss: 18.2818 - MinusLogProbMetric: 18.2818 - val_loss: 18.4743 - val_MinusLogProbMetric: 18.4743 - lr: 2.7778e-05 - 63s/epoch - 321ms/step
Epoch 490/1000
2023-09-27 00:39:49.205 
Epoch 490/1000 
	 loss: 18.3222, MinusLogProbMetric: 18.3222, val_loss: 18.1717, val_MinusLogProbMetric: 18.1717

Epoch 490: val_loss improved from 18.20114 to 18.17165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 65s - loss: 18.3222 - MinusLogProbMetric: 18.3222 - val_loss: 18.1717 - val_MinusLogProbMetric: 18.1717 - lr: 2.7778e-05 - 65s/epoch - 332ms/step
Epoch 491/1000
2023-09-27 00:40:53.861 
Epoch 491/1000 
	 loss: 18.2975, MinusLogProbMetric: 18.2975, val_loss: 18.3237, val_MinusLogProbMetric: 18.3237

Epoch 491: val_loss did not improve from 18.17165
196/196 - 63s - loss: 18.2975 - MinusLogProbMetric: 18.2975 - val_loss: 18.3237 - val_MinusLogProbMetric: 18.3237 - lr: 2.7778e-05 - 63s/epoch - 324ms/step
Epoch 492/1000
2023-09-27 00:41:57.189 
Epoch 492/1000 
	 loss: 18.2599, MinusLogProbMetric: 18.2599, val_loss: 18.3118, val_MinusLogProbMetric: 18.3118

Epoch 492: val_loss did not improve from 18.17165
196/196 - 63s - loss: 18.2599 - MinusLogProbMetric: 18.2599 - val_loss: 18.3118 - val_MinusLogProbMetric: 18.3118 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 493/1000
2023-09-27 00:43:00.063 
Epoch 493/1000 
	 loss: 18.3052, MinusLogProbMetric: 18.3052, val_loss: 18.2470, val_MinusLogProbMetric: 18.2470

Epoch 493: val_loss did not improve from 18.17165
196/196 - 63s - loss: 18.3052 - MinusLogProbMetric: 18.3052 - val_loss: 18.2470 - val_MinusLogProbMetric: 18.2470 - lr: 2.7778e-05 - 63s/epoch - 321ms/step
Epoch 494/1000
2023-09-27 00:44:03.287 
Epoch 494/1000 
	 loss: 18.3457, MinusLogProbMetric: 18.3457, val_loss: 18.1630, val_MinusLogProbMetric: 18.1630

Epoch 494: val_loss improved from 18.17165 to 18.16298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_285/weights/best_weights.h5
196/196 - 64s - loss: 18.3457 - MinusLogProbMetric: 18.3457 - val_loss: 18.1630 - val_MinusLogProbMetric: 18.1630 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 495/1000
2023-09-27 00:45:07.776 
Epoch 495/1000 
	 loss: 18.3324, MinusLogProbMetric: 18.3324, val_loss: 18.3372, val_MinusLogProbMetric: 18.3372

Epoch 495: val_loss did not improve from 18.16298
196/196 - 63s - loss: 18.3324 - MinusLogProbMetric: 18.3324 - val_loss: 18.3372 - val_MinusLogProbMetric: 18.3372 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 496/1000
2023-09-27 00:46:11.333 
Epoch 496/1000 
	 loss: 21.3668, MinusLogProbMetric: 21.3668, val_loss: 24.3230, val_MinusLogProbMetric: 24.3230

Epoch 496: val_loss did not improve from 18.16298
196/196 - 64s - loss: 21.3668 - MinusLogProbMetric: 21.3668 - val_loss: 24.3230 - val_MinusLogProbMetric: 24.3230 - lr: 2.7778e-05 - 64s/epoch - 324ms/step
Epoch 497/1000
2023-09-27 00:47:14.819 
Epoch 497/1000 
	 loss: 20.5443, MinusLogProbMetric: 20.5443, val_loss: 19.9755, val_MinusLogProbMetric: 19.9755

Epoch 497: val_loss did not improve from 18.16298
196/196 - 63s - loss: 20.5443 - MinusLogProbMetric: 20.5443 - val_loss: 19.9755 - val_MinusLogProbMetric: 19.9755 - lr: 2.7778e-05 - 63s/epoch - 324ms/step
Epoch 498/1000
2023-09-27 00:48:18.122 
Epoch 498/1000 
	 loss: 19.7581, MinusLogProbMetric: 19.7581, val_loss: 19.7519, val_MinusLogProbMetric: 19.7519

Epoch 498: val_loss did not improve from 18.16298
196/196 - 63s - loss: 19.7581 - MinusLogProbMetric: 19.7581 - val_loss: 19.7519 - val_MinusLogProbMetric: 19.7519 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 499/1000
2023-09-27 00:49:21.508 
Epoch 499/1000 
	 loss: 19.5118, MinusLogProbMetric: 19.5118, val_loss: 19.5242, val_MinusLogProbMetric: 19.5242

Epoch 499: val_loss did not improve from 18.16298
196/196 - 63s - loss: 19.5118 - MinusLogProbMetric: 19.5118 - val_loss: 19.5242 - val_MinusLogProbMetric: 19.5242 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 500/1000
2023-09-27 00:50:25.167 
Epoch 500/1000 
	 loss: 19.1986, MinusLogProbMetric: 19.1986, val_loss: 19.0349, val_MinusLogProbMetric: 19.0349

Epoch 500: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.1986 - MinusLogProbMetric: 19.1986 - val_loss: 19.0349 - val_MinusLogProbMetric: 19.0349 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 501/1000
2023-09-27 00:51:28.775 
Epoch 501/1000 
	 loss: 19.0866, MinusLogProbMetric: 19.0866, val_loss: 23.2826, val_MinusLogProbMetric: 23.2826

Epoch 501: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.0866 - MinusLogProbMetric: 19.0866 - val_loss: 23.2826 - val_MinusLogProbMetric: 23.2826 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 502/1000
2023-09-27 00:52:32.210 
Epoch 502/1000 
	 loss: 18.8772, MinusLogProbMetric: 18.8772, val_loss: 18.3727, val_MinusLogProbMetric: 18.3727

Epoch 502: val_loss did not improve from 18.16298
196/196 - 63s - loss: 18.8772 - MinusLogProbMetric: 18.8772 - val_loss: 18.3727 - val_MinusLogProbMetric: 18.3727 - lr: 2.7778e-05 - 63s/epoch - 324ms/step
Epoch 503/1000
2023-09-27 00:53:36.078 
Epoch 503/1000 
	 loss: 18.3529, MinusLogProbMetric: 18.3529, val_loss: 19.7566, val_MinusLogProbMetric: 19.7566

Epoch 503: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.3529 - MinusLogProbMetric: 18.3529 - val_loss: 19.7566 - val_MinusLogProbMetric: 19.7566 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 504/1000
2023-09-27 00:54:39.778 
Epoch 504/1000 
	 loss: 18.5024, MinusLogProbMetric: 18.5024, val_loss: 18.4856, val_MinusLogProbMetric: 18.4856

Epoch 504: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.5024 - MinusLogProbMetric: 18.5024 - val_loss: 18.4856 - val_MinusLogProbMetric: 18.4856 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 505/1000
2023-09-27 00:55:43.707 
Epoch 505/1000 
	 loss: 18.4634, MinusLogProbMetric: 18.4634, val_loss: 18.3236, val_MinusLogProbMetric: 18.3236

Epoch 505: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.4634 - MinusLogProbMetric: 18.4634 - val_loss: 18.3236 - val_MinusLogProbMetric: 18.3236 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 506/1000
2023-09-27 00:56:47.552 
Epoch 506/1000 
	 loss: 18.9965, MinusLogProbMetric: 18.9965, val_loss: 19.0914, val_MinusLogProbMetric: 19.0914

Epoch 506: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.9965 - MinusLogProbMetric: 18.9965 - val_loss: 19.0914 - val_MinusLogProbMetric: 19.0914 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 507/1000
2023-09-27 00:57:51.250 
Epoch 507/1000 
	 loss: 18.3993, MinusLogProbMetric: 18.3993, val_loss: 18.3100, val_MinusLogProbMetric: 18.3100

Epoch 507: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.3993 - MinusLogProbMetric: 18.3993 - val_loss: 18.3100 - val_MinusLogProbMetric: 18.3100 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 508/1000
2023-09-27 00:58:54.653 
Epoch 508/1000 
	 loss: 18.3022, MinusLogProbMetric: 18.3022, val_loss: 18.3688, val_MinusLogProbMetric: 18.3688

Epoch 508: val_loss did not improve from 18.16298
196/196 - 63s - loss: 18.3022 - MinusLogProbMetric: 18.3022 - val_loss: 18.3688 - val_MinusLogProbMetric: 18.3688 - lr: 2.7778e-05 - 63s/epoch - 323ms/step
Epoch 509/1000
2023-09-27 00:59:59.015 
Epoch 509/1000 
	 loss: 18.3020, MinusLogProbMetric: 18.3020, val_loss: 18.2262, val_MinusLogProbMetric: 18.2262

Epoch 509: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.3020 - MinusLogProbMetric: 18.3020 - val_loss: 18.2262 - val_MinusLogProbMetric: 18.2262 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 510/1000
2023-09-27 01:01:03.303 
Epoch 510/1000 
	 loss: 18.3390, MinusLogProbMetric: 18.3390, val_loss: 18.3088, val_MinusLogProbMetric: 18.3088

Epoch 510: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.3390 - MinusLogProbMetric: 18.3390 - val_loss: 18.3088 - val_MinusLogProbMetric: 18.3088 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 511/1000
2023-09-27 01:02:07.429 
Epoch 511/1000 
	 loss: 18.2940, MinusLogProbMetric: 18.2940, val_loss: 18.4393, val_MinusLogProbMetric: 18.4393

Epoch 511: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.2940 - MinusLogProbMetric: 18.2940 - val_loss: 18.4393 - val_MinusLogProbMetric: 18.4393 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 512/1000
2023-09-27 01:03:11.674 
Epoch 512/1000 
	 loss: 18.2658, MinusLogProbMetric: 18.2658, val_loss: 18.1643, val_MinusLogProbMetric: 18.1643

Epoch 512: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.2658 - MinusLogProbMetric: 18.2658 - val_loss: 18.1643 - val_MinusLogProbMetric: 18.1643 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 513/1000
2023-09-27 01:04:15.331 
Epoch 513/1000 
	 loss: 18.2527, MinusLogProbMetric: 18.2527, val_loss: 18.3112, val_MinusLogProbMetric: 18.3112

Epoch 513: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.2527 - MinusLogProbMetric: 18.2527 - val_loss: 18.3112 - val_MinusLogProbMetric: 18.3112 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 514/1000
2023-09-27 01:05:19.699 
Epoch 514/1000 
	 loss: 18.2899, MinusLogProbMetric: 18.2899, val_loss: 18.2525, val_MinusLogProbMetric: 18.2525

Epoch 514: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.2899 - MinusLogProbMetric: 18.2899 - val_loss: 18.2525 - val_MinusLogProbMetric: 18.2525 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 515/1000
2023-09-27 01:06:22.898 
Epoch 515/1000 
	 loss: 18.3694, MinusLogProbMetric: 18.3694, val_loss: 18.5476, val_MinusLogProbMetric: 18.5476

Epoch 515: val_loss did not improve from 18.16298
196/196 - 63s - loss: 18.3694 - MinusLogProbMetric: 18.3694 - val_loss: 18.5476 - val_MinusLogProbMetric: 18.5476 - lr: 2.7778e-05 - 63s/epoch - 322ms/step
Epoch 516/1000
2023-09-27 01:07:26.812 
Epoch 516/1000 
	 loss: 18.4903, MinusLogProbMetric: 18.4903, val_loss: 18.5661, val_MinusLogProbMetric: 18.5661

Epoch 516: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.4903 - MinusLogProbMetric: 18.4903 - val_loss: 18.5661 - val_MinusLogProbMetric: 18.5661 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 517/1000
2023-09-27 01:08:30.972 
Epoch 517/1000 
	 loss: 18.4870, MinusLogProbMetric: 18.4870, val_loss: 18.4413, val_MinusLogProbMetric: 18.4413

Epoch 517: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.4870 - MinusLogProbMetric: 18.4870 - val_loss: 18.4413 - val_MinusLogProbMetric: 18.4413 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 518/1000
2023-09-27 01:09:34.689 
Epoch 518/1000 
	 loss: 18.5077, MinusLogProbMetric: 18.5077, val_loss: 18.5875, val_MinusLogProbMetric: 18.5875

Epoch 518: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.5077 - MinusLogProbMetric: 18.5077 - val_loss: 18.5875 - val_MinusLogProbMetric: 18.5875 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 519/1000
2023-09-27 01:10:38.464 
Epoch 519/1000 
	 loss: 18.4678, MinusLogProbMetric: 18.4678, val_loss: 18.5193, val_MinusLogProbMetric: 18.5193

Epoch 519: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.4678 - MinusLogProbMetric: 18.4678 - val_loss: 18.5193 - val_MinusLogProbMetric: 18.5193 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 520/1000
2023-09-27 01:11:42.607 
Epoch 520/1000 
	 loss: 18.4503, MinusLogProbMetric: 18.4503, val_loss: 18.5168, val_MinusLogProbMetric: 18.5168

Epoch 520: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.4503 - MinusLogProbMetric: 18.4503 - val_loss: 18.5168 - val_MinusLogProbMetric: 18.5168 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 521/1000
2023-09-27 01:12:46.963 
Epoch 521/1000 
	 loss: 18.4703, MinusLogProbMetric: 18.4703, val_loss: 18.4655, val_MinusLogProbMetric: 18.4655

Epoch 521: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.4703 - MinusLogProbMetric: 18.4703 - val_loss: 18.4655 - val_MinusLogProbMetric: 18.4655 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 522/1000
2023-09-27 01:13:51.333 
Epoch 522/1000 
	 loss: 18.4561, MinusLogProbMetric: 18.4561, val_loss: 18.4161, val_MinusLogProbMetric: 18.4161

Epoch 522: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.4561 - MinusLogProbMetric: 18.4561 - val_loss: 18.4161 - val_MinusLogProbMetric: 18.4161 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 523/1000
2023-09-27 01:14:55.427 
Epoch 523/1000 
	 loss: 18.4399, MinusLogProbMetric: 18.4399, val_loss: 18.4116, val_MinusLogProbMetric: 18.4116

Epoch 523: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.4399 - MinusLogProbMetric: 18.4399 - val_loss: 18.4116 - val_MinusLogProbMetric: 18.4116 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 524/1000
2023-09-27 01:15:59.948 
Epoch 524/1000 
	 loss: 18.4387, MinusLogProbMetric: 18.4387, val_loss: 18.4323, val_MinusLogProbMetric: 18.4323

Epoch 524: val_loss did not improve from 18.16298
196/196 - 65s - loss: 18.4387 - MinusLogProbMetric: 18.4387 - val_loss: 18.4323 - val_MinusLogProbMetric: 18.4323 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 525/1000
2023-09-27 01:17:04.571 
Epoch 525/1000 
	 loss: 18.7713, MinusLogProbMetric: 18.7713, val_loss: 18.4575, val_MinusLogProbMetric: 18.4575

Epoch 525: val_loss did not improve from 18.16298
196/196 - 65s - loss: 18.7713 - MinusLogProbMetric: 18.7713 - val_loss: 18.4575 - val_MinusLogProbMetric: 18.4575 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 526/1000
2023-09-27 01:18:09.210 
Epoch 526/1000 
	 loss: 18.4474, MinusLogProbMetric: 18.4474, val_loss: 18.5737, val_MinusLogProbMetric: 18.5737

Epoch 526: val_loss did not improve from 18.16298
196/196 - 65s - loss: 18.4474 - MinusLogProbMetric: 18.4474 - val_loss: 18.5737 - val_MinusLogProbMetric: 18.5737 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 527/1000
2023-09-27 01:19:13.313 
Epoch 527/1000 
	 loss: 18.4084, MinusLogProbMetric: 18.4084, val_loss: 18.4354, val_MinusLogProbMetric: 18.4354

Epoch 527: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.4084 - MinusLogProbMetric: 18.4084 - val_loss: 18.4354 - val_MinusLogProbMetric: 18.4354 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 528/1000
2023-09-27 01:20:17.334 
Epoch 528/1000 
	 loss: 18.4074, MinusLogProbMetric: 18.4074, val_loss: 18.6323, val_MinusLogProbMetric: 18.6323

Epoch 528: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.4074 - MinusLogProbMetric: 18.4074 - val_loss: 18.6323 - val_MinusLogProbMetric: 18.6323 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 529/1000
2023-09-27 01:21:21.315 
Epoch 529/1000 
	 loss: 18.4215, MinusLogProbMetric: 18.4215, val_loss: 18.3928, val_MinusLogProbMetric: 18.3928

Epoch 529: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.4215 - MinusLogProbMetric: 18.4215 - val_loss: 18.3928 - val_MinusLogProbMetric: 18.3928 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 530/1000
2023-09-27 01:22:25.272 
Epoch 530/1000 
	 loss: 18.4185, MinusLogProbMetric: 18.4185, val_loss: 18.4900, val_MinusLogProbMetric: 18.4900

Epoch 530: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.4185 - MinusLogProbMetric: 18.4185 - val_loss: 18.4900 - val_MinusLogProbMetric: 18.4900 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 531/1000
2023-09-27 01:23:29.616 
Epoch 531/1000 
	 loss: 18.4003, MinusLogProbMetric: 18.4003, val_loss: 18.4025, val_MinusLogProbMetric: 18.4025

Epoch 531: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.4003 - MinusLogProbMetric: 18.4003 - val_loss: 18.4025 - val_MinusLogProbMetric: 18.4025 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 532/1000
2023-09-27 01:24:33.383 
Epoch 532/1000 
	 loss: 18.3955, MinusLogProbMetric: 18.3955, val_loss: 18.4183, val_MinusLogProbMetric: 18.4183

Epoch 532: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.3955 - MinusLogProbMetric: 18.3955 - val_loss: 18.4183 - val_MinusLogProbMetric: 18.4183 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 533/1000
2023-09-27 01:25:37.613 
Epoch 533/1000 
	 loss: 19.7551, MinusLogProbMetric: 19.7551, val_loss: 18.6739, val_MinusLogProbMetric: 18.6739

Epoch 533: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.7551 - MinusLogProbMetric: 19.7551 - val_loss: 18.6739 - val_MinusLogProbMetric: 18.6739 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 534/1000
2023-09-27 01:26:41.851 
Epoch 534/1000 
	 loss: 18.3981, MinusLogProbMetric: 18.3981, val_loss: 18.3470, val_MinusLogProbMetric: 18.3470

Epoch 534: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.3981 - MinusLogProbMetric: 18.3981 - val_loss: 18.3470 - val_MinusLogProbMetric: 18.3470 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 535/1000
2023-09-27 01:27:46.441 
Epoch 535/1000 
	 loss: 18.3903, MinusLogProbMetric: 18.3903, val_loss: 18.4314, val_MinusLogProbMetric: 18.4314

Epoch 535: val_loss did not improve from 18.16298
196/196 - 65s - loss: 18.3903 - MinusLogProbMetric: 18.3903 - val_loss: 18.4314 - val_MinusLogProbMetric: 18.4314 - lr: 2.7778e-05 - 65s/epoch - 330ms/step
Epoch 536/1000
2023-09-27 01:28:50.236 
Epoch 536/1000 
	 loss: 18.3704, MinusLogProbMetric: 18.3704, val_loss: 18.4525, val_MinusLogProbMetric: 18.4525

Epoch 536: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.3704 - MinusLogProbMetric: 18.3704 - val_loss: 18.4525 - val_MinusLogProbMetric: 18.4525 - lr: 2.7778e-05 - 64s/epoch - 325ms/step
Epoch 537/1000
2023-09-27 01:29:54.735 
Epoch 537/1000 
	 loss: 18.3728, MinusLogProbMetric: 18.3728, val_loss: 18.3590, val_MinusLogProbMetric: 18.3590

Epoch 537: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.3728 - MinusLogProbMetric: 18.3728 - val_loss: 18.3590 - val_MinusLogProbMetric: 18.3590 - lr: 2.7778e-05 - 64s/epoch - 329ms/step
Epoch 538/1000
2023-09-27 01:30:59.295 
Epoch 538/1000 
	 loss: 34.2027, MinusLogProbMetric: 34.2027, val_loss: 33.4560, val_MinusLogProbMetric: 33.4560

Epoch 538: val_loss did not improve from 18.16298
196/196 - 65s - loss: 34.2027 - MinusLogProbMetric: 34.2027 - val_loss: 33.4560 - val_MinusLogProbMetric: 33.4560 - lr: 2.7778e-05 - 65s/epoch - 329ms/step
Epoch 539/1000
2023-09-27 01:32:03.259 
Epoch 539/1000 
	 loss: 24.2665, MinusLogProbMetric: 24.2665, val_loss: 20.6414, val_MinusLogProbMetric: 20.6414

Epoch 539: val_loss did not improve from 18.16298
196/196 - 64s - loss: 24.2665 - MinusLogProbMetric: 24.2665 - val_loss: 20.6414 - val_MinusLogProbMetric: 20.6414 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 540/1000
2023-09-27 01:33:07.402 
Epoch 540/1000 
	 loss: 20.4825, MinusLogProbMetric: 20.4825, val_loss: 20.3558, val_MinusLogProbMetric: 20.3558

Epoch 540: val_loss did not improve from 18.16298
196/196 - 64s - loss: 20.4825 - MinusLogProbMetric: 20.4825 - val_loss: 20.3558 - val_MinusLogProbMetric: 20.3558 - lr: 2.7778e-05 - 64s/epoch - 327ms/step
Epoch 541/1000
2023-09-27 01:34:11.207 
Epoch 541/1000 
	 loss: 20.1677, MinusLogProbMetric: 20.1677, val_loss: 20.1111, val_MinusLogProbMetric: 20.1111

Epoch 541: val_loss did not improve from 18.16298
196/196 - 64s - loss: 20.1677 - MinusLogProbMetric: 20.1677 - val_loss: 20.1111 - val_MinusLogProbMetric: 20.1111 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 542/1000
2023-09-27 01:35:15.516 
Epoch 542/1000 
	 loss: 19.9906, MinusLogProbMetric: 19.9906, val_loss: 20.1771, val_MinusLogProbMetric: 20.1771

Epoch 542: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.9906 - MinusLogProbMetric: 19.9906 - val_loss: 20.1771 - val_MinusLogProbMetric: 20.1771 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 543/1000
2023-09-27 01:36:19.363 
Epoch 543/1000 
	 loss: 19.9287, MinusLogProbMetric: 19.9287, val_loss: 19.7425, val_MinusLogProbMetric: 19.7425

Epoch 543: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.9287 - MinusLogProbMetric: 19.9287 - val_loss: 19.7425 - val_MinusLogProbMetric: 19.7425 - lr: 2.7778e-05 - 64s/epoch - 326ms/step
Epoch 544/1000
2023-09-27 01:37:23.612 
Epoch 544/1000 
	 loss: 21.0996, MinusLogProbMetric: 21.0996, val_loss: 20.5666, val_MinusLogProbMetric: 20.5666

Epoch 544: val_loss did not improve from 18.16298
196/196 - 64s - loss: 21.0996 - MinusLogProbMetric: 21.0996 - val_loss: 20.5666 - val_MinusLogProbMetric: 20.5666 - lr: 2.7778e-05 - 64s/epoch - 328ms/step
Epoch 545/1000
2023-09-27 01:38:27.715 
Epoch 545/1000 
	 loss: 19.7328, MinusLogProbMetric: 19.7328, val_loss: 19.6500, val_MinusLogProbMetric: 19.6500

Epoch 545: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.7328 - MinusLogProbMetric: 19.7328 - val_loss: 19.6500 - val_MinusLogProbMetric: 19.6500 - lr: 1.3889e-05 - 64s/epoch - 327ms/step
Epoch 546/1000
2023-09-27 01:39:31.787 
Epoch 546/1000 
	 loss: 19.5904, MinusLogProbMetric: 19.5904, val_loss: 19.5647, val_MinusLogProbMetric: 19.5647

Epoch 546: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.5904 - MinusLogProbMetric: 19.5904 - val_loss: 19.5647 - val_MinusLogProbMetric: 19.5647 - lr: 1.3889e-05 - 64s/epoch - 327ms/step
Epoch 547/1000
2023-09-27 01:40:35.531 
Epoch 547/1000 
	 loss: 19.5411, MinusLogProbMetric: 19.5411, val_loss: 19.5055, val_MinusLogProbMetric: 19.5055

Epoch 547: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.5411 - MinusLogProbMetric: 19.5411 - val_loss: 19.5055 - val_MinusLogProbMetric: 19.5055 - lr: 1.3889e-05 - 64s/epoch - 325ms/step
Epoch 548/1000
2023-09-27 01:41:39.037 
Epoch 548/1000 
	 loss: 19.4768, MinusLogProbMetric: 19.4768, val_loss: 19.4497, val_MinusLogProbMetric: 19.4497

Epoch 548: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.4768 - MinusLogProbMetric: 19.4768 - val_loss: 19.4497 - val_MinusLogProbMetric: 19.4497 - lr: 1.3889e-05 - 64s/epoch - 324ms/step
Epoch 549/1000
2023-09-27 01:42:42.727 
Epoch 549/1000 
	 loss: 19.4420, MinusLogProbMetric: 19.4420, val_loss: 19.4963, val_MinusLogProbMetric: 19.4963

Epoch 549: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.4420 - MinusLogProbMetric: 19.4420 - val_loss: 19.4963 - val_MinusLogProbMetric: 19.4963 - lr: 1.3889e-05 - 64s/epoch - 325ms/step
Epoch 550/1000
2023-09-27 01:43:46.521 
Epoch 550/1000 
	 loss: 19.4249, MinusLogProbMetric: 19.4249, val_loss: 19.3813, val_MinusLogProbMetric: 19.3813

Epoch 550: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.4249 - MinusLogProbMetric: 19.4249 - val_loss: 19.3813 - val_MinusLogProbMetric: 19.3813 - lr: 1.3889e-05 - 64s/epoch - 325ms/step
Epoch 551/1000
2023-09-27 01:44:51.066 
Epoch 551/1000 
	 loss: 19.3808, MinusLogProbMetric: 19.3808, val_loss: 19.3779, val_MinusLogProbMetric: 19.3779

Epoch 551: val_loss did not improve from 18.16298
196/196 - 65s - loss: 19.3808 - MinusLogProbMetric: 19.3808 - val_loss: 19.3779 - val_MinusLogProbMetric: 19.3779 - lr: 1.3889e-05 - 65s/epoch - 329ms/step
Epoch 552/1000
2023-09-27 01:45:55.306 
Epoch 552/1000 
	 loss: 19.3478, MinusLogProbMetric: 19.3478, val_loss: 19.3155, val_MinusLogProbMetric: 19.3155

Epoch 552: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.3478 - MinusLogProbMetric: 19.3478 - val_loss: 19.3155 - val_MinusLogProbMetric: 19.3155 - lr: 1.3889e-05 - 64s/epoch - 328ms/step
Epoch 553/1000
2023-09-27 01:46:59.682 
Epoch 553/1000 
	 loss: 21.4029, MinusLogProbMetric: 21.4029, val_loss: 20.7356, val_MinusLogProbMetric: 20.7356

Epoch 553: val_loss did not improve from 18.16298
196/196 - 64s - loss: 21.4029 - MinusLogProbMetric: 21.4029 - val_loss: 20.7356 - val_MinusLogProbMetric: 20.7356 - lr: 1.3889e-05 - 64s/epoch - 328ms/step
Epoch 554/1000
2023-09-27 01:48:04.067 
Epoch 554/1000 
	 loss: 19.3878, MinusLogProbMetric: 19.3878, val_loss: 19.4160, val_MinusLogProbMetric: 19.4160

Epoch 554: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.3878 - MinusLogProbMetric: 19.3878 - val_loss: 19.4160 - val_MinusLogProbMetric: 19.4160 - lr: 1.3889e-05 - 64s/epoch - 328ms/step
Epoch 555/1000
2023-09-27 01:49:07.910 
Epoch 555/1000 
	 loss: 19.3191, MinusLogProbMetric: 19.3191, val_loss: 19.2712, val_MinusLogProbMetric: 19.2712

Epoch 555: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.3191 - MinusLogProbMetric: 19.3191 - val_loss: 19.2712 - val_MinusLogProbMetric: 19.2712 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 556/1000
2023-09-27 01:50:11.871 
Epoch 556/1000 
	 loss: 19.2761, MinusLogProbMetric: 19.2761, val_loss: 19.2310, val_MinusLogProbMetric: 19.2310

Epoch 556: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.2761 - MinusLogProbMetric: 19.2761 - val_loss: 19.2310 - val_MinusLogProbMetric: 19.2310 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 557/1000
2023-09-27 01:51:15.818 
Epoch 557/1000 
	 loss: 19.2371, MinusLogProbMetric: 19.2371, val_loss: 19.2914, val_MinusLogProbMetric: 19.2914

Epoch 557: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.2371 - MinusLogProbMetric: 19.2371 - val_loss: 19.2914 - val_MinusLogProbMetric: 19.2914 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 558/1000
2023-09-27 01:52:19.299 
Epoch 558/1000 
	 loss: 21.3373, MinusLogProbMetric: 21.3373, val_loss: 19.4105, val_MinusLogProbMetric: 19.4105

Epoch 558: val_loss did not improve from 18.16298
196/196 - 63s - loss: 21.3373 - MinusLogProbMetric: 21.3373 - val_loss: 19.4105 - val_MinusLogProbMetric: 19.4105 - lr: 1.3889e-05 - 63s/epoch - 324ms/step
Epoch 559/1000
2023-09-27 01:53:23.360 
Epoch 559/1000 
	 loss: 19.2766, MinusLogProbMetric: 19.2766, val_loss: 19.3227, val_MinusLogProbMetric: 19.3227

Epoch 559: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.2766 - MinusLogProbMetric: 19.2766 - val_loss: 19.3227 - val_MinusLogProbMetric: 19.3227 - lr: 1.3889e-05 - 64s/epoch - 327ms/step
Epoch 560/1000
2023-09-27 01:54:27.057 
Epoch 560/1000 
	 loss: 19.2285, MinusLogProbMetric: 19.2285, val_loss: 19.3104, val_MinusLogProbMetric: 19.3104

Epoch 560: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.2285 - MinusLogProbMetric: 19.2285 - val_loss: 19.3104 - val_MinusLogProbMetric: 19.3104 - lr: 1.3889e-05 - 64s/epoch - 325ms/step
Epoch 561/1000
2023-09-27 01:55:31.155 
Epoch 561/1000 
	 loss: 19.2585, MinusLogProbMetric: 19.2585, val_loss: 19.5255, val_MinusLogProbMetric: 19.5255

Epoch 561: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.2585 - MinusLogProbMetric: 19.2585 - val_loss: 19.5255 - val_MinusLogProbMetric: 19.5255 - lr: 1.3889e-05 - 64s/epoch - 327ms/step
Epoch 562/1000
2023-09-27 01:56:35.102 
Epoch 562/1000 
	 loss: 19.4135, MinusLogProbMetric: 19.4135, val_loss: 19.2454, val_MinusLogProbMetric: 19.2454

Epoch 562: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.4135 - MinusLogProbMetric: 19.4135 - val_loss: 19.2454 - val_MinusLogProbMetric: 19.2454 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 563/1000
2023-09-27 01:57:39.021 
Epoch 563/1000 
	 loss: 19.4954, MinusLogProbMetric: 19.4954, val_loss: 19.2102, val_MinusLogProbMetric: 19.2102

Epoch 563: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.4954 - MinusLogProbMetric: 19.4954 - val_loss: 19.2102 - val_MinusLogProbMetric: 19.2102 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 564/1000
2023-09-27 01:58:43.174 
Epoch 564/1000 
	 loss: 19.1555, MinusLogProbMetric: 19.1555, val_loss: 19.1688, val_MinusLogProbMetric: 19.1688

Epoch 564: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.1555 - MinusLogProbMetric: 19.1555 - val_loss: 19.1688 - val_MinusLogProbMetric: 19.1688 - lr: 1.3889e-05 - 64s/epoch - 327ms/step
Epoch 565/1000
2023-09-27 01:59:47.406 
Epoch 565/1000 
	 loss: 19.1735, MinusLogProbMetric: 19.1735, val_loss: 19.3787, val_MinusLogProbMetric: 19.3787

Epoch 565: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.1735 - MinusLogProbMetric: 19.1735 - val_loss: 19.3787 - val_MinusLogProbMetric: 19.3787 - lr: 1.3889e-05 - 64s/epoch - 328ms/step
Epoch 566/1000
2023-09-27 02:00:50.938 
Epoch 566/1000 
	 loss: 19.5418, MinusLogProbMetric: 19.5418, val_loss: 22.2088, val_MinusLogProbMetric: 22.2088

Epoch 566: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.5418 - MinusLogProbMetric: 19.5418 - val_loss: 22.2088 - val_MinusLogProbMetric: 22.2088 - lr: 1.3889e-05 - 64s/epoch - 324ms/step
Epoch 567/1000
2023-09-27 02:01:54.927 
Epoch 567/1000 
	 loss: 19.8404, MinusLogProbMetric: 19.8404, val_loss: 19.1166, val_MinusLogProbMetric: 19.1166

Epoch 567: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.8404 - MinusLogProbMetric: 19.8404 - val_loss: 19.1166 - val_MinusLogProbMetric: 19.1166 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 568/1000
2023-09-27 02:02:58.949 
Epoch 568/1000 
	 loss: 19.1239, MinusLogProbMetric: 19.1239, val_loss: 19.1055, val_MinusLogProbMetric: 19.1055

Epoch 568: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.1239 - MinusLogProbMetric: 19.1239 - val_loss: 19.1055 - val_MinusLogProbMetric: 19.1055 - lr: 1.3889e-05 - 64s/epoch - 327ms/step
Epoch 569/1000
2023-09-27 02:04:03.494 
Epoch 569/1000 
	 loss: 19.0830, MinusLogProbMetric: 19.0830, val_loss: 19.1199, val_MinusLogProbMetric: 19.1199

Epoch 569: val_loss did not improve from 18.16298
196/196 - 65s - loss: 19.0830 - MinusLogProbMetric: 19.0830 - val_loss: 19.1199 - val_MinusLogProbMetric: 19.1199 - lr: 1.3889e-05 - 65s/epoch - 329ms/step
Epoch 570/1000
2023-09-27 02:05:07.583 
Epoch 570/1000 
	 loss: 19.1626, MinusLogProbMetric: 19.1626, val_loss: 19.0630, val_MinusLogProbMetric: 19.0630

Epoch 570: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.1626 - MinusLogProbMetric: 19.1626 - val_loss: 19.0630 - val_MinusLogProbMetric: 19.0630 - lr: 1.3889e-05 - 64s/epoch - 327ms/step
Epoch 571/1000
2023-09-27 02:06:11.562 
Epoch 571/1000 
	 loss: 19.0773, MinusLogProbMetric: 19.0773, val_loss: 19.0797, val_MinusLogProbMetric: 19.0797

Epoch 571: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.0773 - MinusLogProbMetric: 19.0773 - val_loss: 19.0797 - val_MinusLogProbMetric: 19.0797 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 572/1000
2023-09-27 02:07:15.440 
Epoch 572/1000 
	 loss: 19.0492, MinusLogProbMetric: 19.0492, val_loss: 19.6867, val_MinusLogProbMetric: 19.6867

Epoch 572: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.0492 - MinusLogProbMetric: 19.0492 - val_loss: 19.6867 - val_MinusLogProbMetric: 19.6867 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 573/1000
2023-09-27 02:08:19.688 
Epoch 573/1000 
	 loss: 19.2095, MinusLogProbMetric: 19.2095, val_loss: 19.0359, val_MinusLogProbMetric: 19.0359

Epoch 573: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.2095 - MinusLogProbMetric: 19.2095 - val_loss: 19.0359 - val_MinusLogProbMetric: 19.0359 - lr: 1.3889e-05 - 64s/epoch - 328ms/step
Epoch 574/1000
2023-09-27 02:09:23.895 
Epoch 574/1000 
	 loss: 19.0430, MinusLogProbMetric: 19.0430, val_loss: 19.0921, val_MinusLogProbMetric: 19.0921

Epoch 574: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.0430 - MinusLogProbMetric: 19.0430 - val_loss: 19.0921 - val_MinusLogProbMetric: 19.0921 - lr: 1.3889e-05 - 64s/epoch - 328ms/step
Epoch 575/1000
2023-09-27 02:10:27.616 
Epoch 575/1000 
	 loss: 19.6469, MinusLogProbMetric: 19.6469, val_loss: 22.4251, val_MinusLogProbMetric: 22.4251

Epoch 575: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.6469 - MinusLogProbMetric: 19.6469 - val_loss: 22.4251 - val_MinusLogProbMetric: 22.4251 - lr: 1.3889e-05 - 64s/epoch - 325ms/step
Epoch 576/1000
2023-09-27 02:11:31.474 
Epoch 576/1000 
	 loss: 20.4045, MinusLogProbMetric: 20.4045, val_loss: 19.0766, val_MinusLogProbMetric: 19.0766

Epoch 576: val_loss did not improve from 18.16298
196/196 - 64s - loss: 20.4045 - MinusLogProbMetric: 20.4045 - val_loss: 19.0766 - val_MinusLogProbMetric: 19.0766 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 577/1000
2023-09-27 02:12:35.339 
Epoch 577/1000 
	 loss: 19.0203, MinusLogProbMetric: 19.0203, val_loss: 19.0796, val_MinusLogProbMetric: 19.0796

Epoch 577: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.0203 - MinusLogProbMetric: 19.0203 - val_loss: 19.0796 - val_MinusLogProbMetric: 19.0796 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 578/1000
2023-09-27 02:13:38.875 
Epoch 578/1000 
	 loss: 19.0037, MinusLogProbMetric: 19.0037, val_loss: 19.0669, val_MinusLogProbMetric: 19.0669

Epoch 578: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.0037 - MinusLogProbMetric: 19.0037 - val_loss: 19.0669 - val_MinusLogProbMetric: 19.0669 - lr: 1.3889e-05 - 64s/epoch - 324ms/step
Epoch 579/1000
2023-09-27 02:14:43.205 
Epoch 579/1000 
	 loss: 18.9953, MinusLogProbMetric: 18.9953, val_loss: 19.0025, val_MinusLogProbMetric: 19.0025

Epoch 579: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.9953 - MinusLogProbMetric: 18.9953 - val_loss: 19.0025 - val_MinusLogProbMetric: 19.0025 - lr: 1.3889e-05 - 64s/epoch - 328ms/step
Epoch 580/1000
2023-09-27 02:15:46.856 
Epoch 580/1000 
	 loss: 18.9867, MinusLogProbMetric: 18.9867, val_loss: 19.1034, val_MinusLogProbMetric: 19.1034

Epoch 580: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.9867 - MinusLogProbMetric: 18.9867 - val_loss: 19.1034 - val_MinusLogProbMetric: 19.1034 - lr: 1.3889e-05 - 64s/epoch - 325ms/step
Epoch 581/1000
2023-09-27 02:16:50.350 
Epoch 581/1000 
	 loss: 19.3274, MinusLogProbMetric: 19.3274, val_loss: 19.0941, val_MinusLogProbMetric: 19.0941

Epoch 581: val_loss did not improve from 18.16298
196/196 - 63s - loss: 19.3274 - MinusLogProbMetric: 19.3274 - val_loss: 19.0941 - val_MinusLogProbMetric: 19.0941 - lr: 1.3889e-05 - 63s/epoch - 324ms/step
Epoch 582/1000
2023-09-27 02:17:54.419 
Epoch 582/1000 
	 loss: 18.9702, MinusLogProbMetric: 18.9702, val_loss: 19.0020, val_MinusLogProbMetric: 19.0020

Epoch 582: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.9702 - MinusLogProbMetric: 18.9702 - val_loss: 19.0020 - val_MinusLogProbMetric: 19.0020 - lr: 1.3889e-05 - 64s/epoch - 327ms/step
Epoch 583/1000
2023-09-27 02:18:58.607 
Epoch 583/1000 
	 loss: 19.0075, MinusLogProbMetric: 19.0075, val_loss: 18.9786, val_MinusLogProbMetric: 18.9786

Epoch 583: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.0075 - MinusLogProbMetric: 19.0075 - val_loss: 18.9786 - val_MinusLogProbMetric: 18.9786 - lr: 1.3889e-05 - 64s/epoch - 327ms/step
Epoch 584/1000
2023-09-27 02:20:02.563 
Epoch 584/1000 
	 loss: 18.9746, MinusLogProbMetric: 18.9746, val_loss: 18.9656, val_MinusLogProbMetric: 18.9656

Epoch 584: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.9746 - MinusLogProbMetric: 18.9746 - val_loss: 18.9656 - val_MinusLogProbMetric: 18.9656 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 585/1000
2023-09-27 02:21:06.432 
Epoch 585/1000 
	 loss: 19.1545, MinusLogProbMetric: 19.1545, val_loss: 18.9121, val_MinusLogProbMetric: 18.9121

Epoch 585: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.1545 - MinusLogProbMetric: 19.1545 - val_loss: 18.9121 - val_MinusLogProbMetric: 18.9121 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 586/1000
2023-09-27 02:22:10.613 
Epoch 586/1000 
	 loss: 19.8204, MinusLogProbMetric: 19.8204, val_loss: 19.0058, val_MinusLogProbMetric: 19.0058

Epoch 586: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.8204 - MinusLogProbMetric: 19.8204 - val_loss: 19.0058 - val_MinusLogProbMetric: 19.0058 - lr: 1.3889e-05 - 64s/epoch - 327ms/step
Epoch 587/1000
2023-09-27 02:23:14.644 
Epoch 587/1000 
	 loss: 18.9344, MinusLogProbMetric: 18.9344, val_loss: 18.9491, val_MinusLogProbMetric: 18.9491

Epoch 587: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.9344 - MinusLogProbMetric: 18.9344 - val_loss: 18.9491 - val_MinusLogProbMetric: 18.9491 - lr: 1.3889e-05 - 64s/epoch - 327ms/step
Epoch 588/1000
2023-09-27 02:24:18.837 
Epoch 588/1000 
	 loss: 18.9558, MinusLogProbMetric: 18.9558, val_loss: 19.0121, val_MinusLogProbMetric: 19.0121

Epoch 588: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.9558 - MinusLogProbMetric: 18.9558 - val_loss: 19.0121 - val_MinusLogProbMetric: 19.0121 - lr: 1.3889e-05 - 64s/epoch - 328ms/step
Epoch 589/1000
2023-09-27 02:25:22.563 
Epoch 589/1000 
	 loss: 18.8995, MinusLogProbMetric: 18.8995, val_loss: 18.9756, val_MinusLogProbMetric: 18.9756

Epoch 589: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.8995 - MinusLogProbMetric: 18.8995 - val_loss: 18.9756 - val_MinusLogProbMetric: 18.9756 - lr: 1.3889e-05 - 64s/epoch - 325ms/step
Epoch 590/1000
2023-09-27 02:26:26.933 
Epoch 590/1000 
	 loss: 19.0504, MinusLogProbMetric: 19.0504, val_loss: 18.9019, val_MinusLogProbMetric: 18.9019

Epoch 590: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.0504 - MinusLogProbMetric: 19.0504 - val_loss: 18.9019 - val_MinusLogProbMetric: 18.9019 - lr: 1.3889e-05 - 64s/epoch - 328ms/step
Epoch 591/1000
2023-09-27 02:27:30.690 
Epoch 591/1000 
	 loss: 18.8840, MinusLogProbMetric: 18.8840, val_loss: 18.9283, val_MinusLogProbMetric: 18.9283

Epoch 591: val_loss did not improve from 18.16298
196/196 - 64s - loss: 18.8840 - MinusLogProbMetric: 18.8840 - val_loss: 18.9283 - val_MinusLogProbMetric: 18.9283 - lr: 1.3889e-05 - 64s/epoch - 325ms/step
Epoch 592/1000
2023-09-27 02:28:34.544 
Epoch 592/1000 
	 loss: 19.0690, MinusLogProbMetric: 19.0690, val_loss: 18.9646, val_MinusLogProbMetric: 18.9646

Epoch 592: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.0690 - MinusLogProbMetric: 19.0690 - val_loss: 18.9646 - val_MinusLogProbMetric: 18.9646 - lr: 1.3889e-05 - 64s/epoch - 326ms/step
Epoch 593/1000
2023-09-27 02:29:39.030 
Epoch 593/1000 
	 loss: 19.0994, MinusLogProbMetric: 19.0994, val_loss: 18.8233, val_MinusLogProbMetric: 18.8233

Epoch 593: val_loss did not improve from 18.16298
196/196 - 64s - loss: 19.0994 - MinusLogProbMetric: 19.0994 - val_loss: 18.8233 - val_MinusLogProbMetric: 18.8233 - lr: 1.3889e-05 - 64s/epoch - 329ms/step
Epoch 594/1000
2023-09-27 02:30:42.871 
Epoch 594/1000 
	 loss: 18.8467, MinusLogProbMetric: 18.8467, val_loss: 18.9101, val_MinusLogProbMetric: 18.9101

Epoch 594: val_loss did not improve from 18.16298
Restoring model weights from the end of the best epoch: 494.
196/196 - 64s - loss: 18.8467 - MinusLogProbMetric: 18.8467 - val_loss: 18.9101 - val_MinusLogProbMetric: 18.9101 - lr: 1.3889e-05 - 64s/epoch - 329ms/step
Epoch 594: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
LR metric calculation completed in 25.830408672976773 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 13.789035087043885 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 10.888461839989759 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 13.862181007978506 seconds.
Training succeeded with seed 541.
Model trained in 38062.34 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 69.08 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 69.58 s.
===========
Run 285/720 done in 47328.25 s.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

===========
Generating train data for run 298.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_298/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_298/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_298/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_298
self.data_kwargs: {'seed': 869}
self.x_data: [[ 1.7417877   3.5044646   9.194398   ...  7.377848    2.8874114
   1.7757134 ]
 [ 3.8693519   4.134066    8.323213   ...  7.5259514   3.079576
   1.8540689 ]
 [ 2.7888105   3.5250566   6.8470335  ...  7.0512147   2.9893377
   1.7142482 ]
 ...
 [ 3.8914979   5.948059   -0.25578696 ...  1.7629418   6.8860593
   1.464444  ]
 [ 3.0775785   3.9001913   8.887795   ...  7.2041593   3.5200696
   1.5176823 ]
 [ 1.0419946   3.6002321   7.1163735  ...  7.3799496   3.218665
   1.6207042 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_38"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_39 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  1152560   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,152,560
Trainable params: 1,152,560
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f95f6d5da80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f908c1b93f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f908c1b93f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f95f735d090>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f906478dc30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f906478e1a0>, <keras.callbacks.ModelCheckpoint object at 0x7f906478e260>, <keras.callbacks.EarlyStopping object at 0x7f906478e4d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f906478e500>, <keras.callbacks.TerminateOnNaN object at 0x7f906478e140>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_298/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 298/720 with hyperparameters:
timestamp = 2023-09-27 02:31:57.573394
ndims = 32
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1152560
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.7417877   3.5044646   9.194398    1.0621872   8.910313    1.2448134
  9.752992    4.795541   10.214517    5.8200784   7.3571367   0.42017213
  3.0764782   1.1773593   3.1572356   1.3086243   2.6951017   5.712275
  0.4469142   6.9349284   5.5393224   1.8138231   6.1143165   1.1455625
  7.183009    9.098161    3.4021838   5.9704523   0.79392874  7.377848
  2.8874114   1.7757134 ]
Epoch 1/1000
2023-09-27 02:33:27.788 
Epoch 1/1000 
	 loss: 80.6535, MinusLogProbMetric: 80.6535, val_loss: 29.5820, val_MinusLogProbMetric: 29.5820

Epoch 1: val_loss improved from inf to 29.58203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 91s - loss: 80.6535 - MinusLogProbMetric: 80.6535 - val_loss: 29.5820 - val_MinusLogProbMetric: 29.5820 - lr: 0.0010 - 91s/epoch - 463ms/step
Epoch 2/1000
2023-09-27 02:34:03.098 
Epoch 2/1000 
	 loss: 27.3266, MinusLogProbMetric: 27.3266, val_loss: 25.6650, val_MinusLogProbMetric: 25.6650

Epoch 2: val_loss improved from 29.58203 to 25.66504, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 27.3266 - MinusLogProbMetric: 27.3266 - val_loss: 25.6650 - val_MinusLogProbMetric: 25.6650 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 3/1000
2023-09-27 02:34:37.962 
Epoch 3/1000 
	 loss: 24.2046, MinusLogProbMetric: 24.2046, val_loss: 23.3360, val_MinusLogProbMetric: 23.3360

Epoch 3: val_loss improved from 25.66504 to 23.33601, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 24.2046 - MinusLogProbMetric: 24.2046 - val_loss: 23.3360 - val_MinusLogProbMetric: 23.3360 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 4/1000
2023-09-27 02:35:12.794 
Epoch 4/1000 
	 loss: 23.2561, MinusLogProbMetric: 23.2561, val_loss: 21.9332, val_MinusLogProbMetric: 21.9332

Epoch 4: val_loss improved from 23.33601 to 21.93317, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 23.2561 - MinusLogProbMetric: 23.2561 - val_loss: 21.9332 - val_MinusLogProbMetric: 21.9332 - lr: 0.0010 - 35s/epoch - 179ms/step
Epoch 5/1000
2023-09-27 02:35:47.865 
Epoch 5/1000 
	 loss: 22.3142, MinusLogProbMetric: 22.3142, val_loss: 21.8470, val_MinusLogProbMetric: 21.8470

Epoch 5: val_loss improved from 21.93317 to 21.84695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 22.3142 - MinusLogProbMetric: 22.3142 - val_loss: 21.8470 - val_MinusLogProbMetric: 21.8470 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 6/1000
2023-09-27 02:36:22.346 
Epoch 6/1000 
	 loss: 21.5827, MinusLogProbMetric: 21.5827, val_loss: 25.6183, val_MinusLogProbMetric: 25.6183

Epoch 6: val_loss did not improve from 21.84695
196/196 - 34s - loss: 21.5827 - MinusLogProbMetric: 21.5827 - val_loss: 25.6183 - val_MinusLogProbMetric: 25.6183 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 7/1000
2023-09-27 02:36:56.298 
Epoch 7/1000 
	 loss: 21.2665, MinusLogProbMetric: 21.2665, val_loss: 20.9698, val_MinusLogProbMetric: 20.9698

Epoch 7: val_loss improved from 21.84695 to 20.96975, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 34s - loss: 21.2665 - MinusLogProbMetric: 21.2665 - val_loss: 20.9698 - val_MinusLogProbMetric: 20.9698 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 8/1000
2023-09-27 02:37:30.800 
Epoch 8/1000 
	 loss: 20.7235, MinusLogProbMetric: 20.7235, val_loss: 22.3560, val_MinusLogProbMetric: 22.3560

Epoch 8: val_loss did not improve from 20.96975
196/196 - 34s - loss: 20.7235 - MinusLogProbMetric: 20.7235 - val_loss: 22.3560 - val_MinusLogProbMetric: 22.3560 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 9/1000
2023-09-27 02:38:04.933 
Epoch 9/1000 
	 loss: 20.3145, MinusLogProbMetric: 20.3145, val_loss: 20.0878, val_MinusLogProbMetric: 20.0878

Epoch 9: val_loss improved from 20.96975 to 20.08783, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 20.3145 - MinusLogProbMetric: 20.3145 - val_loss: 20.0878 - val_MinusLogProbMetric: 20.0878 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 10/1000
2023-09-27 02:38:39.617 
Epoch 10/1000 
	 loss: 20.1268, MinusLogProbMetric: 20.1268, val_loss: 19.5575, val_MinusLogProbMetric: 19.5575

Epoch 10: val_loss improved from 20.08783 to 19.55752, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 20.1268 - MinusLogProbMetric: 20.1268 - val_loss: 19.5575 - val_MinusLogProbMetric: 19.5575 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 11/1000
2023-09-27 02:39:13.964 
Epoch 11/1000 
	 loss: 20.2970, MinusLogProbMetric: 20.2970, val_loss: 22.0913, val_MinusLogProbMetric: 22.0913

Epoch 11: val_loss did not improve from 19.55752
196/196 - 34s - loss: 20.2970 - MinusLogProbMetric: 20.2970 - val_loss: 22.0913 - val_MinusLogProbMetric: 22.0913 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 12/1000
2023-09-27 02:39:48.023 
Epoch 12/1000 
	 loss: 19.8319, MinusLogProbMetric: 19.8319, val_loss: 20.3824, val_MinusLogProbMetric: 20.3824

Epoch 12: val_loss did not improve from 19.55752
196/196 - 34s - loss: 19.8319 - MinusLogProbMetric: 19.8319 - val_loss: 20.3824 - val_MinusLogProbMetric: 20.3824 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 13/1000
2023-09-27 02:40:21.858 
Epoch 13/1000 
	 loss: 19.7647, MinusLogProbMetric: 19.7647, val_loss: 20.3989, val_MinusLogProbMetric: 20.3989

Epoch 13: val_loss did not improve from 19.55752
196/196 - 34s - loss: 19.7647 - MinusLogProbMetric: 19.7647 - val_loss: 20.3989 - val_MinusLogProbMetric: 20.3989 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 14/1000
2023-09-27 02:40:55.795 
Epoch 14/1000 
	 loss: 19.5745, MinusLogProbMetric: 19.5745, val_loss: 19.6550, val_MinusLogProbMetric: 19.6550

Epoch 14: val_loss did not improve from 19.55752
196/196 - 34s - loss: 19.5745 - MinusLogProbMetric: 19.5745 - val_loss: 19.6550 - val_MinusLogProbMetric: 19.6550 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 15/1000
2023-09-27 02:41:29.620 
Epoch 15/1000 
	 loss: 19.4321, MinusLogProbMetric: 19.4321, val_loss: 19.5023, val_MinusLogProbMetric: 19.5023

Epoch 15: val_loss improved from 19.55752 to 19.50230, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 34s - loss: 19.4321 - MinusLogProbMetric: 19.4321 - val_loss: 19.5023 - val_MinusLogProbMetric: 19.5023 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 16/1000
2023-09-27 02:42:04.572 
Epoch 16/1000 
	 loss: 19.4863, MinusLogProbMetric: 19.4863, val_loss: 19.4410, val_MinusLogProbMetric: 19.4410

Epoch 16: val_loss improved from 19.50230 to 19.44099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 19.4863 - MinusLogProbMetric: 19.4863 - val_loss: 19.4410 - val_MinusLogProbMetric: 19.4410 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 17/1000
2023-09-27 02:42:39.106 
Epoch 17/1000 
	 loss: 19.2886, MinusLogProbMetric: 19.2886, val_loss: 19.0464, val_MinusLogProbMetric: 19.0464

Epoch 17: val_loss improved from 19.44099 to 19.04638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 19.2886 - MinusLogProbMetric: 19.2886 - val_loss: 19.0464 - val_MinusLogProbMetric: 19.0464 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 18/1000
2023-09-27 02:43:13.733 
Epoch 18/1000 
	 loss: 19.2297, MinusLogProbMetric: 19.2297, val_loss: 18.8778, val_MinusLogProbMetric: 18.8778

Epoch 18: val_loss improved from 19.04638 to 18.87779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 19.2297 - MinusLogProbMetric: 19.2297 - val_loss: 18.8778 - val_MinusLogProbMetric: 18.8778 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 19/1000
2023-09-27 02:43:48.320 
Epoch 19/1000 
	 loss: 19.0015, MinusLogProbMetric: 19.0015, val_loss: 20.1753, val_MinusLogProbMetric: 20.1753

Epoch 19: val_loss did not improve from 18.87779
196/196 - 34s - loss: 19.0015 - MinusLogProbMetric: 19.0015 - val_loss: 20.1753 - val_MinusLogProbMetric: 20.1753 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 20/1000
2023-09-27 02:44:22.476 
Epoch 20/1000 
	 loss: 19.1328, MinusLogProbMetric: 19.1328, val_loss: 19.8211, val_MinusLogProbMetric: 19.8211

Epoch 20: val_loss did not improve from 18.87779
196/196 - 34s - loss: 19.1328 - MinusLogProbMetric: 19.1328 - val_loss: 19.8211 - val_MinusLogProbMetric: 19.8211 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 21/1000
2023-09-27 02:44:56.236 
Epoch 21/1000 
	 loss: 19.0061, MinusLogProbMetric: 19.0061, val_loss: 19.1350, val_MinusLogProbMetric: 19.1350

Epoch 21: val_loss did not improve from 18.87779
196/196 - 34s - loss: 19.0061 - MinusLogProbMetric: 19.0061 - val_loss: 19.1350 - val_MinusLogProbMetric: 19.1350 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 22/1000
2023-09-27 02:45:30.199 
Epoch 22/1000 
	 loss: 18.9800, MinusLogProbMetric: 18.9800, val_loss: 19.2201, val_MinusLogProbMetric: 19.2201

Epoch 22: val_loss did not improve from 18.87779
196/196 - 34s - loss: 18.9800 - MinusLogProbMetric: 18.9800 - val_loss: 19.2201 - val_MinusLogProbMetric: 19.2201 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 23/1000
2023-09-27 02:46:04.250 
Epoch 23/1000 
	 loss: 19.0023, MinusLogProbMetric: 19.0023, val_loss: 19.4716, val_MinusLogProbMetric: 19.4716

Epoch 23: val_loss did not improve from 18.87779
196/196 - 34s - loss: 19.0023 - MinusLogProbMetric: 19.0023 - val_loss: 19.4716 - val_MinusLogProbMetric: 19.4716 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 24/1000
2023-09-27 02:46:38.132 
Epoch 24/1000 
	 loss: 18.7155, MinusLogProbMetric: 18.7155, val_loss: 18.6778, val_MinusLogProbMetric: 18.6778

Epoch 24: val_loss improved from 18.87779 to 18.67784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 34s - loss: 18.7155 - MinusLogProbMetric: 18.7155 - val_loss: 18.6778 - val_MinusLogProbMetric: 18.6778 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 25/1000
2023-09-27 02:47:12.611 
Epoch 25/1000 
	 loss: 18.6809, MinusLogProbMetric: 18.6809, val_loss: 18.8923, val_MinusLogProbMetric: 18.8923

Epoch 25: val_loss did not improve from 18.67784
196/196 - 34s - loss: 18.6809 - MinusLogProbMetric: 18.6809 - val_loss: 18.8923 - val_MinusLogProbMetric: 18.8923 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 26/1000
2023-09-27 02:47:46.791 
Epoch 26/1000 
	 loss: 18.7572, MinusLogProbMetric: 18.7572, val_loss: 18.6120, val_MinusLogProbMetric: 18.6120

Epoch 26: val_loss improved from 18.67784 to 18.61204, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 18.7572 - MinusLogProbMetric: 18.7572 - val_loss: 18.6120 - val_MinusLogProbMetric: 18.6120 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 27/1000
2023-09-27 02:48:21.230 
Epoch 27/1000 
	 loss: 18.6850, MinusLogProbMetric: 18.6850, val_loss: 18.6282, val_MinusLogProbMetric: 18.6282

Epoch 27: val_loss did not improve from 18.61204
196/196 - 34s - loss: 18.6850 - MinusLogProbMetric: 18.6850 - val_loss: 18.6282 - val_MinusLogProbMetric: 18.6282 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 28/1000
2023-09-27 02:48:55.183 
Epoch 28/1000 
	 loss: 18.5506, MinusLogProbMetric: 18.5506, val_loss: 19.0824, val_MinusLogProbMetric: 19.0824

Epoch 28: val_loss did not improve from 18.61204
196/196 - 34s - loss: 18.5506 - MinusLogProbMetric: 18.5506 - val_loss: 19.0824 - val_MinusLogProbMetric: 19.0824 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 29/1000
2023-09-27 02:49:29.049 
Epoch 29/1000 
	 loss: 18.4824, MinusLogProbMetric: 18.4824, val_loss: 19.0541, val_MinusLogProbMetric: 19.0541

Epoch 29: val_loss did not improve from 18.61204
196/196 - 34s - loss: 18.4824 - MinusLogProbMetric: 18.4824 - val_loss: 19.0541 - val_MinusLogProbMetric: 19.0541 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 30/1000
2023-09-27 02:50:02.968 
Epoch 30/1000 
	 loss: 18.4241, MinusLogProbMetric: 18.4241, val_loss: 18.3487, val_MinusLogProbMetric: 18.3487

Epoch 30: val_loss improved from 18.61204 to 18.34873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 34s - loss: 18.4241 - MinusLogProbMetric: 18.4241 - val_loss: 18.3487 - val_MinusLogProbMetric: 18.3487 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 31/1000
2023-09-27 02:50:37.360 
Epoch 31/1000 
	 loss: 18.5424, MinusLogProbMetric: 18.5424, val_loss: 19.2929, val_MinusLogProbMetric: 19.2929

Epoch 31: val_loss did not improve from 18.34873
196/196 - 34s - loss: 18.5424 - MinusLogProbMetric: 18.5424 - val_loss: 19.2929 - val_MinusLogProbMetric: 19.2929 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 32/1000
2023-09-27 02:51:11.134 
Epoch 32/1000 
	 loss: 18.3328, MinusLogProbMetric: 18.3328, val_loss: 18.5425, val_MinusLogProbMetric: 18.5425

Epoch 32: val_loss did not improve from 18.34873
196/196 - 34s - loss: 18.3328 - MinusLogProbMetric: 18.3328 - val_loss: 18.5425 - val_MinusLogProbMetric: 18.5425 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 33/1000
2023-09-27 02:51:45.155 
Epoch 33/1000 
	 loss: 18.4198, MinusLogProbMetric: 18.4198, val_loss: 18.2799, val_MinusLogProbMetric: 18.2799

Epoch 33: val_loss improved from 18.34873 to 18.27988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 18.4198 - MinusLogProbMetric: 18.4198 - val_loss: 18.2799 - val_MinusLogProbMetric: 18.2799 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 34/1000
2023-09-27 02:52:19.449 
Epoch 34/1000 
	 loss: 18.2132, MinusLogProbMetric: 18.2132, val_loss: 18.2125, val_MinusLogProbMetric: 18.2125

Epoch 34: val_loss improved from 18.27988 to 18.21253, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 34s - loss: 18.2132 - MinusLogProbMetric: 18.2132 - val_loss: 18.2125 - val_MinusLogProbMetric: 18.2125 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 35/1000
2023-09-27 02:52:53.977 
Epoch 35/1000 
	 loss: 18.1891, MinusLogProbMetric: 18.1891, val_loss: 18.9013, val_MinusLogProbMetric: 18.9013

Epoch 35: val_loss did not improve from 18.21253
196/196 - 34s - loss: 18.1891 - MinusLogProbMetric: 18.1891 - val_loss: 18.9013 - val_MinusLogProbMetric: 18.9013 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 36/1000
2023-09-27 02:53:28.083 
Epoch 36/1000 
	 loss: 18.2523, MinusLogProbMetric: 18.2523, val_loss: 19.6922, val_MinusLogProbMetric: 19.6922

Epoch 36: val_loss did not improve from 18.21253
196/196 - 34s - loss: 18.2523 - MinusLogProbMetric: 18.2523 - val_loss: 19.6922 - val_MinusLogProbMetric: 19.6922 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 37/1000
2023-09-27 02:54:02.045 
Epoch 37/1000 
	 loss: 18.2410, MinusLogProbMetric: 18.2410, val_loss: 18.1128, val_MinusLogProbMetric: 18.1128

Epoch 37: val_loss improved from 18.21253 to 18.11283, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 18.2410 - MinusLogProbMetric: 18.2410 - val_loss: 18.1128 - val_MinusLogProbMetric: 18.1128 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 38/1000
2023-09-27 02:54:36.755 
Epoch 38/1000 
	 loss: 18.1383, MinusLogProbMetric: 18.1383, val_loss: 18.0475, val_MinusLogProbMetric: 18.0475

Epoch 38: val_loss improved from 18.11283 to 18.04748, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 18.1383 - MinusLogProbMetric: 18.1383 - val_loss: 18.0475 - val_MinusLogProbMetric: 18.0475 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 39/1000
2023-09-27 02:55:11.296 
Epoch 39/1000 
	 loss: 18.1544, MinusLogProbMetric: 18.1544, val_loss: 18.5742, val_MinusLogProbMetric: 18.5742

Epoch 39: val_loss did not improve from 18.04748
196/196 - 34s - loss: 18.1544 - MinusLogProbMetric: 18.1544 - val_loss: 18.5742 - val_MinusLogProbMetric: 18.5742 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 40/1000
2023-09-27 02:55:45.643 
Epoch 40/1000 
	 loss: 18.0720, MinusLogProbMetric: 18.0720, val_loss: 18.1189, val_MinusLogProbMetric: 18.1189

Epoch 40: val_loss did not improve from 18.04748
196/196 - 34s - loss: 18.0720 - MinusLogProbMetric: 18.0720 - val_loss: 18.1189 - val_MinusLogProbMetric: 18.1189 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 41/1000
2023-09-27 02:56:19.625 
Epoch 41/1000 
	 loss: 18.0429, MinusLogProbMetric: 18.0429, val_loss: 18.0097, val_MinusLogProbMetric: 18.0097

Epoch 41: val_loss improved from 18.04748 to 18.00966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 18.0429 - MinusLogProbMetric: 18.0429 - val_loss: 18.0097 - val_MinusLogProbMetric: 18.0097 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 42/1000
2023-09-27 02:56:53.933 
Epoch 42/1000 
	 loss: 18.1780, MinusLogProbMetric: 18.1780, val_loss: 18.0378, val_MinusLogProbMetric: 18.0378

Epoch 42: val_loss did not improve from 18.00966
196/196 - 34s - loss: 18.1780 - MinusLogProbMetric: 18.1780 - val_loss: 18.0378 - val_MinusLogProbMetric: 18.0378 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 43/1000
2023-09-27 02:57:28.052 
Epoch 43/1000 
	 loss: 18.0363, MinusLogProbMetric: 18.0363, val_loss: 18.4602, val_MinusLogProbMetric: 18.4602

Epoch 43: val_loss did not improve from 18.00966
196/196 - 34s - loss: 18.0363 - MinusLogProbMetric: 18.0363 - val_loss: 18.4602 - val_MinusLogProbMetric: 18.4602 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 44/1000
2023-09-27 02:58:02.168 
Epoch 44/1000 
	 loss: 17.9049, MinusLogProbMetric: 17.9049, val_loss: 18.0486, val_MinusLogProbMetric: 18.0486

Epoch 44: val_loss did not improve from 18.00966
196/196 - 34s - loss: 17.9049 - MinusLogProbMetric: 17.9049 - val_loss: 18.0486 - val_MinusLogProbMetric: 18.0486 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 45/1000
2023-09-27 02:58:36.150 
Epoch 45/1000 
	 loss: 18.0060, MinusLogProbMetric: 18.0060, val_loss: 18.1314, val_MinusLogProbMetric: 18.1314

Epoch 45: val_loss did not improve from 18.00966
196/196 - 34s - loss: 18.0060 - MinusLogProbMetric: 18.0060 - val_loss: 18.1314 - val_MinusLogProbMetric: 18.1314 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 46/1000
2023-09-27 02:59:10.392 
Epoch 46/1000 
	 loss: 17.9571, MinusLogProbMetric: 17.9571, val_loss: 18.0351, val_MinusLogProbMetric: 18.0351

Epoch 46: val_loss did not improve from 18.00966
196/196 - 34s - loss: 17.9571 - MinusLogProbMetric: 17.9571 - val_loss: 18.0351 - val_MinusLogProbMetric: 18.0351 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 47/1000
2023-09-27 02:59:44.819 
Epoch 47/1000 
	 loss: 17.8861, MinusLogProbMetric: 17.8861, val_loss: 18.0209, val_MinusLogProbMetric: 18.0209

Epoch 47: val_loss did not improve from 18.00966
196/196 - 34s - loss: 17.8861 - MinusLogProbMetric: 17.8861 - val_loss: 18.0209 - val_MinusLogProbMetric: 18.0209 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 48/1000
2023-09-27 03:00:18.785 
Epoch 48/1000 
	 loss: 17.8950, MinusLogProbMetric: 17.8950, val_loss: 17.8174, val_MinusLogProbMetric: 17.8174

Epoch 48: val_loss improved from 18.00966 to 17.81736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 34s - loss: 17.8950 - MinusLogProbMetric: 17.8950 - val_loss: 17.8174 - val_MinusLogProbMetric: 17.8174 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 49/1000
2023-09-27 03:00:53.111 
Epoch 49/1000 
	 loss: 17.8704, MinusLogProbMetric: 17.8704, val_loss: 18.3213, val_MinusLogProbMetric: 18.3213

Epoch 49: val_loss did not improve from 17.81736
196/196 - 34s - loss: 17.8704 - MinusLogProbMetric: 17.8704 - val_loss: 18.3213 - val_MinusLogProbMetric: 18.3213 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 50/1000
2023-09-27 03:01:27.121 
Epoch 50/1000 
	 loss: 18.0410, MinusLogProbMetric: 18.0410, val_loss: 19.7640, val_MinusLogProbMetric: 19.7640

Epoch 50: val_loss did not improve from 17.81736
196/196 - 34s - loss: 18.0410 - MinusLogProbMetric: 18.0410 - val_loss: 19.7640 - val_MinusLogProbMetric: 19.7640 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 51/1000
2023-09-27 03:02:01.040 
Epoch 51/1000 
	 loss: 17.8903, MinusLogProbMetric: 17.8903, val_loss: 17.8843, val_MinusLogProbMetric: 17.8843

Epoch 51: val_loss did not improve from 17.81736
196/196 - 34s - loss: 17.8903 - MinusLogProbMetric: 17.8903 - val_loss: 17.8843 - val_MinusLogProbMetric: 17.8843 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 52/1000
2023-09-27 03:02:34.879 
Epoch 52/1000 
	 loss: 17.8074, MinusLogProbMetric: 17.8074, val_loss: 17.9634, val_MinusLogProbMetric: 17.9634

Epoch 52: val_loss did not improve from 17.81736
196/196 - 34s - loss: 17.8074 - MinusLogProbMetric: 17.8074 - val_loss: 17.9634 - val_MinusLogProbMetric: 17.9634 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 53/1000
2023-09-27 03:03:08.591 
Epoch 53/1000 
	 loss: 17.9914, MinusLogProbMetric: 17.9914, val_loss: 18.4889, val_MinusLogProbMetric: 18.4889

Epoch 53: val_loss did not improve from 17.81736
196/196 - 34s - loss: 17.9914 - MinusLogProbMetric: 17.9914 - val_loss: 18.4889 - val_MinusLogProbMetric: 18.4889 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 54/1000
2023-09-27 03:03:42.562 
Epoch 54/1000 
	 loss: 17.7379, MinusLogProbMetric: 17.7379, val_loss: 18.3244, val_MinusLogProbMetric: 18.3244

Epoch 54: val_loss did not improve from 17.81736
196/196 - 34s - loss: 17.7379 - MinusLogProbMetric: 17.7379 - val_loss: 18.3244 - val_MinusLogProbMetric: 18.3244 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 55/1000
2023-09-27 03:04:16.294 
Epoch 55/1000 
	 loss: 17.7541, MinusLogProbMetric: 17.7541, val_loss: 18.1512, val_MinusLogProbMetric: 18.1512

Epoch 55: val_loss did not improve from 17.81736
196/196 - 34s - loss: 17.7541 - MinusLogProbMetric: 17.7541 - val_loss: 18.1512 - val_MinusLogProbMetric: 18.1512 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 56/1000
2023-09-27 03:04:50.567 
Epoch 56/1000 
	 loss: 17.7325, MinusLogProbMetric: 17.7325, val_loss: 17.8482, val_MinusLogProbMetric: 17.8482

Epoch 56: val_loss did not improve from 17.81736
196/196 - 34s - loss: 17.7325 - MinusLogProbMetric: 17.7325 - val_loss: 17.8482 - val_MinusLogProbMetric: 17.8482 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 57/1000
2023-09-27 03:05:24.684 
Epoch 57/1000 
	 loss: 17.6733, MinusLogProbMetric: 17.6733, val_loss: 18.0590, val_MinusLogProbMetric: 18.0590

Epoch 57: val_loss did not improve from 17.81736
196/196 - 34s - loss: 17.6733 - MinusLogProbMetric: 17.6733 - val_loss: 18.0590 - val_MinusLogProbMetric: 18.0590 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 58/1000
2023-09-27 03:05:58.758 
Epoch 58/1000 
	 loss: 17.6935, MinusLogProbMetric: 17.6935, val_loss: 18.6849, val_MinusLogProbMetric: 18.6849

Epoch 58: val_loss did not improve from 17.81736
196/196 - 34s - loss: 17.6935 - MinusLogProbMetric: 17.6935 - val_loss: 18.6849 - val_MinusLogProbMetric: 18.6849 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 59/1000
2023-09-27 03:06:32.759 
Epoch 59/1000 
	 loss: 17.7838, MinusLogProbMetric: 17.7838, val_loss: 18.0401, val_MinusLogProbMetric: 18.0401

Epoch 59: val_loss did not improve from 17.81736
196/196 - 34s - loss: 17.7838 - MinusLogProbMetric: 17.7838 - val_loss: 18.0401 - val_MinusLogProbMetric: 18.0401 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 60/1000
2023-09-27 03:07:07.042 
Epoch 60/1000 
	 loss: 17.6634, MinusLogProbMetric: 17.6634, val_loss: 18.5019, val_MinusLogProbMetric: 18.5019

Epoch 60: val_loss did not improve from 17.81736
196/196 - 34s - loss: 17.6634 - MinusLogProbMetric: 17.6634 - val_loss: 18.5019 - val_MinusLogProbMetric: 18.5019 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 61/1000
2023-09-27 03:07:41.226 
Epoch 61/1000 
	 loss: 17.6407, MinusLogProbMetric: 17.6407, val_loss: 17.8724, val_MinusLogProbMetric: 17.8724

Epoch 61: val_loss did not improve from 17.81736
196/196 - 34s - loss: 17.6407 - MinusLogProbMetric: 17.6407 - val_loss: 17.8724 - val_MinusLogProbMetric: 17.8724 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 62/1000
2023-09-27 03:08:15.344 
Epoch 62/1000 
	 loss: 17.6506, MinusLogProbMetric: 17.6506, val_loss: 17.8201, val_MinusLogProbMetric: 17.8201

Epoch 62: val_loss did not improve from 17.81736
196/196 - 34s - loss: 17.6506 - MinusLogProbMetric: 17.6506 - val_loss: 17.8201 - val_MinusLogProbMetric: 17.8201 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 63/1000
2023-09-27 03:08:49.228 
Epoch 63/1000 
	 loss: 17.7936, MinusLogProbMetric: 17.7936, val_loss: 17.7630, val_MinusLogProbMetric: 17.7630

Epoch 63: val_loss improved from 17.81736 to 17.76296, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 34s - loss: 17.7936 - MinusLogProbMetric: 17.7936 - val_loss: 17.7630 - val_MinusLogProbMetric: 17.7630 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 64/1000
2023-09-27 03:09:23.683 
Epoch 64/1000 
	 loss: 17.5322, MinusLogProbMetric: 17.5322, val_loss: 17.8144, val_MinusLogProbMetric: 17.8144

Epoch 64: val_loss did not improve from 17.76296
196/196 - 34s - loss: 17.5322 - MinusLogProbMetric: 17.5322 - val_loss: 17.8144 - val_MinusLogProbMetric: 17.8144 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 65/1000
2023-09-27 03:09:57.478 
Epoch 65/1000 
	 loss: 17.5716, MinusLogProbMetric: 17.5716, val_loss: 17.8042, val_MinusLogProbMetric: 17.8042

Epoch 65: val_loss did not improve from 17.76296
196/196 - 34s - loss: 17.5716 - MinusLogProbMetric: 17.5716 - val_loss: 17.8042 - val_MinusLogProbMetric: 17.8042 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 66/1000
2023-09-27 03:10:31.496 
Epoch 66/1000 
	 loss: 17.5997, MinusLogProbMetric: 17.5997, val_loss: 17.9215, val_MinusLogProbMetric: 17.9215

Epoch 66: val_loss did not improve from 17.76296
196/196 - 34s - loss: 17.5997 - MinusLogProbMetric: 17.5997 - val_loss: 17.9215 - val_MinusLogProbMetric: 17.9215 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 67/1000
2023-09-27 03:11:06.013 
Epoch 67/1000 
	 loss: 17.5250, MinusLogProbMetric: 17.5250, val_loss: 17.9981, val_MinusLogProbMetric: 17.9981

Epoch 67: val_loss did not improve from 17.76296
196/196 - 35s - loss: 17.5250 - MinusLogProbMetric: 17.5250 - val_loss: 17.9981 - val_MinusLogProbMetric: 17.9981 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 68/1000
2023-09-27 03:11:39.980 
Epoch 68/1000 
	 loss: 17.5173, MinusLogProbMetric: 17.5173, val_loss: 17.8044, val_MinusLogProbMetric: 17.8044

Epoch 68: val_loss did not improve from 17.76296
196/196 - 34s - loss: 17.5173 - MinusLogProbMetric: 17.5173 - val_loss: 17.8044 - val_MinusLogProbMetric: 17.8044 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 69/1000
2023-09-27 03:12:14.221 
Epoch 69/1000 
	 loss: 17.5232, MinusLogProbMetric: 17.5232, val_loss: 17.6962, val_MinusLogProbMetric: 17.6962

Epoch 69: val_loss improved from 17.76296 to 17.69620, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 17.5232 - MinusLogProbMetric: 17.5232 - val_loss: 17.6962 - val_MinusLogProbMetric: 17.6962 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 70/1000
2023-09-27 03:12:48.791 
Epoch 70/1000 
	 loss: 17.5092, MinusLogProbMetric: 17.5092, val_loss: 18.4841, val_MinusLogProbMetric: 18.4841

Epoch 70: val_loss did not improve from 17.69620
196/196 - 34s - loss: 17.5092 - MinusLogProbMetric: 17.5092 - val_loss: 18.4841 - val_MinusLogProbMetric: 18.4841 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 71/1000
2023-09-27 03:13:22.832 
Epoch 71/1000 
	 loss: 17.4937, MinusLogProbMetric: 17.4937, val_loss: 17.4958, val_MinusLogProbMetric: 17.4958

Epoch 71: val_loss improved from 17.69620 to 17.49585, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 17.4937 - MinusLogProbMetric: 17.4937 - val_loss: 17.4958 - val_MinusLogProbMetric: 17.4958 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 72/1000
2023-09-27 03:13:57.487 
Epoch 72/1000 
	 loss: 17.6007, MinusLogProbMetric: 17.6007, val_loss: 17.9471, val_MinusLogProbMetric: 17.9471

Epoch 72: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.6007 - MinusLogProbMetric: 17.6007 - val_loss: 17.9471 - val_MinusLogProbMetric: 17.9471 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 73/1000
2023-09-27 03:14:31.510 
Epoch 73/1000 
	 loss: 17.4789, MinusLogProbMetric: 17.4789, val_loss: 17.9129, val_MinusLogProbMetric: 17.9129

Epoch 73: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.4789 - MinusLogProbMetric: 17.4789 - val_loss: 17.9129 - val_MinusLogProbMetric: 17.9129 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 74/1000
2023-09-27 03:15:05.657 
Epoch 74/1000 
	 loss: 17.5193, MinusLogProbMetric: 17.5193, val_loss: 17.7837, val_MinusLogProbMetric: 17.7837

Epoch 74: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.5193 - MinusLogProbMetric: 17.5193 - val_loss: 17.7837 - val_MinusLogProbMetric: 17.7837 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 75/1000
2023-09-27 03:15:39.710 
Epoch 75/1000 
	 loss: 17.4430, MinusLogProbMetric: 17.4430, val_loss: 17.6980, val_MinusLogProbMetric: 17.6980

Epoch 75: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.4430 - MinusLogProbMetric: 17.4430 - val_loss: 17.6980 - val_MinusLogProbMetric: 17.6980 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 76/1000
2023-09-27 03:16:13.883 
Epoch 76/1000 
	 loss: 17.5014, MinusLogProbMetric: 17.5014, val_loss: 17.9694, val_MinusLogProbMetric: 17.9694

Epoch 76: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.5014 - MinusLogProbMetric: 17.5014 - val_loss: 17.9694 - val_MinusLogProbMetric: 17.9694 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 77/1000
2023-09-27 03:16:48.009 
Epoch 77/1000 
	 loss: 17.4730, MinusLogProbMetric: 17.4730, val_loss: 17.5931, val_MinusLogProbMetric: 17.5931

Epoch 77: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.4730 - MinusLogProbMetric: 17.4730 - val_loss: 17.5931 - val_MinusLogProbMetric: 17.5931 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 78/1000
2023-09-27 03:17:22.136 
Epoch 78/1000 
	 loss: 17.4435, MinusLogProbMetric: 17.4435, val_loss: 17.7353, val_MinusLogProbMetric: 17.7353

Epoch 78: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.4435 - MinusLogProbMetric: 17.4435 - val_loss: 17.7353 - val_MinusLogProbMetric: 17.7353 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 79/1000
2023-09-27 03:17:56.169 
Epoch 79/1000 
	 loss: 17.5337, MinusLogProbMetric: 17.5337, val_loss: 17.5321, val_MinusLogProbMetric: 17.5321

Epoch 79: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.5337 - MinusLogProbMetric: 17.5337 - val_loss: 17.5321 - val_MinusLogProbMetric: 17.5321 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 80/1000
2023-09-27 03:18:30.228 
Epoch 80/1000 
	 loss: 17.4642, MinusLogProbMetric: 17.4642, val_loss: 17.9436, val_MinusLogProbMetric: 17.9436

Epoch 80: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.4642 - MinusLogProbMetric: 17.4642 - val_loss: 17.9436 - val_MinusLogProbMetric: 17.9436 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 81/1000
2023-09-27 03:19:04.554 
Epoch 81/1000 
	 loss: 17.3777, MinusLogProbMetric: 17.3777, val_loss: 17.7817, val_MinusLogProbMetric: 17.7817

Epoch 81: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.3777 - MinusLogProbMetric: 17.3777 - val_loss: 17.7817 - val_MinusLogProbMetric: 17.7817 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 82/1000
2023-09-27 03:19:38.689 
Epoch 82/1000 
	 loss: 17.4292, MinusLogProbMetric: 17.4292, val_loss: 17.7936, val_MinusLogProbMetric: 17.7936

Epoch 82: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.4292 - MinusLogProbMetric: 17.4292 - val_loss: 17.7936 - val_MinusLogProbMetric: 17.7936 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 83/1000
2023-09-27 03:20:12.566 
Epoch 83/1000 
	 loss: 17.4759, MinusLogProbMetric: 17.4759, val_loss: 17.6793, val_MinusLogProbMetric: 17.6793

Epoch 83: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.4759 - MinusLogProbMetric: 17.4759 - val_loss: 17.6793 - val_MinusLogProbMetric: 17.6793 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 84/1000
2023-09-27 03:20:46.473 
Epoch 84/1000 
	 loss: 17.4580, MinusLogProbMetric: 17.4580, val_loss: 17.6026, val_MinusLogProbMetric: 17.6026

Epoch 84: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.4580 - MinusLogProbMetric: 17.4580 - val_loss: 17.6026 - val_MinusLogProbMetric: 17.6026 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 85/1000
2023-09-27 03:21:20.690 
Epoch 85/1000 
	 loss: 17.3168, MinusLogProbMetric: 17.3168, val_loss: 18.0733, val_MinusLogProbMetric: 18.0733

Epoch 85: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.3168 - MinusLogProbMetric: 17.3168 - val_loss: 18.0733 - val_MinusLogProbMetric: 18.0733 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 86/1000
2023-09-27 03:21:54.813 
Epoch 86/1000 
	 loss: 17.3572, MinusLogProbMetric: 17.3572, val_loss: 17.6346, val_MinusLogProbMetric: 17.6346

Epoch 86: val_loss did not improve from 17.49585
196/196 - 34s - loss: 17.3572 - MinusLogProbMetric: 17.3572 - val_loss: 17.6346 - val_MinusLogProbMetric: 17.6346 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 87/1000
2023-09-27 03:22:29.000 
Epoch 87/1000 
	 loss: 17.2955, MinusLogProbMetric: 17.2955, val_loss: 17.4787, val_MinusLogProbMetric: 17.4787

Epoch 87: val_loss improved from 17.49585 to 17.47865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 17.2955 - MinusLogProbMetric: 17.2955 - val_loss: 17.4787 - val_MinusLogProbMetric: 17.4787 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 88/1000
2023-09-27 03:23:03.477 
Epoch 88/1000 
	 loss: 17.3532, MinusLogProbMetric: 17.3532, val_loss: 17.5834, val_MinusLogProbMetric: 17.5834

Epoch 88: val_loss did not improve from 17.47865
196/196 - 34s - loss: 17.3532 - MinusLogProbMetric: 17.3532 - val_loss: 17.5834 - val_MinusLogProbMetric: 17.5834 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 89/1000
2023-09-27 03:23:37.426 
Epoch 89/1000 
	 loss: 17.2964, MinusLogProbMetric: 17.2964, val_loss: 17.8347, val_MinusLogProbMetric: 17.8347

Epoch 89: val_loss did not improve from 17.47865
196/196 - 34s - loss: 17.2964 - MinusLogProbMetric: 17.2964 - val_loss: 17.8347 - val_MinusLogProbMetric: 17.8347 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 90/1000
2023-09-27 03:24:11.876 
Epoch 90/1000 
	 loss: 17.3489, MinusLogProbMetric: 17.3489, val_loss: 18.4245, val_MinusLogProbMetric: 18.4245

Epoch 90: val_loss did not improve from 17.47865
196/196 - 34s - loss: 17.3489 - MinusLogProbMetric: 17.3489 - val_loss: 18.4245 - val_MinusLogProbMetric: 18.4245 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 91/1000
2023-09-27 03:24:45.825 
Epoch 91/1000 
	 loss: 17.2761, MinusLogProbMetric: 17.2761, val_loss: 17.5730, val_MinusLogProbMetric: 17.5730

Epoch 91: val_loss did not improve from 17.47865
196/196 - 34s - loss: 17.2761 - MinusLogProbMetric: 17.2761 - val_loss: 17.5730 - val_MinusLogProbMetric: 17.5730 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 92/1000
2023-09-27 03:25:19.924 
Epoch 92/1000 
	 loss: 17.2538, MinusLogProbMetric: 17.2538, val_loss: 18.3582, val_MinusLogProbMetric: 18.3582

Epoch 92: val_loss did not improve from 17.47865
196/196 - 34s - loss: 17.2538 - MinusLogProbMetric: 17.2538 - val_loss: 18.3582 - val_MinusLogProbMetric: 18.3582 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 93/1000
2023-09-27 03:25:54.267 
Epoch 93/1000 
	 loss: 17.2368, MinusLogProbMetric: 17.2368, val_loss: 17.7351, val_MinusLogProbMetric: 17.7351

Epoch 93: val_loss did not improve from 17.47865
196/196 - 34s - loss: 17.2368 - MinusLogProbMetric: 17.2368 - val_loss: 17.7351 - val_MinusLogProbMetric: 17.7351 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 94/1000
2023-09-27 03:26:28.425 
Epoch 94/1000 
	 loss: 17.2288, MinusLogProbMetric: 17.2288, val_loss: 17.6527, val_MinusLogProbMetric: 17.6527

Epoch 94: val_loss did not improve from 17.47865
196/196 - 34s - loss: 17.2288 - MinusLogProbMetric: 17.2288 - val_loss: 17.6527 - val_MinusLogProbMetric: 17.6527 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 95/1000
2023-09-27 03:27:02.354 
Epoch 95/1000 
	 loss: 17.2259, MinusLogProbMetric: 17.2259, val_loss: 17.5110, val_MinusLogProbMetric: 17.5110

Epoch 95: val_loss did not improve from 17.47865
196/196 - 34s - loss: 17.2259 - MinusLogProbMetric: 17.2259 - val_loss: 17.5110 - val_MinusLogProbMetric: 17.5110 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 96/1000
2023-09-27 03:27:36.421 
Epoch 96/1000 
	 loss: 17.2456, MinusLogProbMetric: 17.2456, val_loss: 17.6429, val_MinusLogProbMetric: 17.6429

Epoch 96: val_loss did not improve from 17.47865
196/196 - 34s - loss: 17.2456 - MinusLogProbMetric: 17.2456 - val_loss: 17.6429 - val_MinusLogProbMetric: 17.6429 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 97/1000
2023-09-27 03:28:10.589 
Epoch 97/1000 
	 loss: 17.3525, MinusLogProbMetric: 17.3525, val_loss: 17.5532, val_MinusLogProbMetric: 17.5532

Epoch 97: val_loss did not improve from 17.47865
196/196 - 34s - loss: 17.3525 - MinusLogProbMetric: 17.3525 - val_loss: 17.5532 - val_MinusLogProbMetric: 17.5532 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 98/1000
2023-09-27 03:28:44.794 
Epoch 98/1000 
	 loss: 17.2034, MinusLogProbMetric: 17.2034, val_loss: 19.5055, val_MinusLogProbMetric: 19.5055

Epoch 98: val_loss did not improve from 17.47865
196/196 - 34s - loss: 17.2034 - MinusLogProbMetric: 17.2034 - val_loss: 19.5055 - val_MinusLogProbMetric: 19.5055 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 99/1000
2023-09-27 03:29:19.112 
Epoch 99/1000 
	 loss: 17.1565, MinusLogProbMetric: 17.1565, val_loss: 17.5486, val_MinusLogProbMetric: 17.5486

Epoch 99: val_loss did not improve from 17.47865
196/196 - 34s - loss: 17.1565 - MinusLogProbMetric: 17.1565 - val_loss: 17.5486 - val_MinusLogProbMetric: 17.5486 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 100/1000
2023-09-27 03:29:53.303 
Epoch 100/1000 
	 loss: 17.2808, MinusLogProbMetric: 17.2808, val_loss: 18.0478, val_MinusLogProbMetric: 18.0478

Epoch 100: val_loss did not improve from 17.47865
196/196 - 34s - loss: 17.2808 - MinusLogProbMetric: 17.2808 - val_loss: 18.0478 - val_MinusLogProbMetric: 18.0478 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 101/1000
2023-09-27 03:30:27.400 
Epoch 101/1000 
	 loss: 17.1588, MinusLogProbMetric: 17.1588, val_loss: 17.4190, val_MinusLogProbMetric: 17.4190

Epoch 101: val_loss improved from 17.47865 to 17.41902, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 17.1588 - MinusLogProbMetric: 17.1588 - val_loss: 17.4190 - val_MinusLogProbMetric: 17.4190 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 102/1000
2023-09-27 03:31:02.150 
Epoch 102/1000 
	 loss: 17.1828, MinusLogProbMetric: 17.1828, val_loss: 18.7942, val_MinusLogProbMetric: 18.7942

Epoch 102: val_loss did not improve from 17.41902
196/196 - 34s - loss: 17.1828 - MinusLogProbMetric: 17.1828 - val_loss: 18.7942 - val_MinusLogProbMetric: 18.7942 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 103/1000
2023-09-27 03:31:36.369 
Epoch 103/1000 
	 loss: 17.1847, MinusLogProbMetric: 17.1847, val_loss: 17.5997, val_MinusLogProbMetric: 17.5997

Epoch 103: val_loss did not improve from 17.41902
196/196 - 34s - loss: 17.1847 - MinusLogProbMetric: 17.1847 - val_loss: 17.5997 - val_MinusLogProbMetric: 17.5997 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 104/1000
2023-09-27 03:32:10.600 
Epoch 104/1000 
	 loss: 17.1885, MinusLogProbMetric: 17.1885, val_loss: 17.6386, val_MinusLogProbMetric: 17.6386

Epoch 104: val_loss did not improve from 17.41902
196/196 - 34s - loss: 17.1885 - MinusLogProbMetric: 17.1885 - val_loss: 17.6386 - val_MinusLogProbMetric: 17.6386 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 105/1000
2023-09-27 03:32:45.171 
Epoch 105/1000 
	 loss: 17.1956, MinusLogProbMetric: 17.1956, val_loss: 17.8502, val_MinusLogProbMetric: 17.8502

Epoch 105: val_loss did not improve from 17.41902
196/196 - 35s - loss: 17.1956 - MinusLogProbMetric: 17.1956 - val_loss: 17.8502 - val_MinusLogProbMetric: 17.8502 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 106/1000
2023-09-27 03:33:18.893 
Epoch 106/1000 
	 loss: 17.1364, MinusLogProbMetric: 17.1364, val_loss: 17.5497, val_MinusLogProbMetric: 17.5497

Epoch 106: val_loss did not improve from 17.41902
196/196 - 34s - loss: 17.1364 - MinusLogProbMetric: 17.1364 - val_loss: 17.5497 - val_MinusLogProbMetric: 17.5497 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 107/1000
2023-09-27 03:33:53.347 
Epoch 107/1000 
	 loss: 17.1058, MinusLogProbMetric: 17.1058, val_loss: 17.4136, val_MinusLogProbMetric: 17.4136

Epoch 107: val_loss improved from 17.41902 to 17.41361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 17.1058 - MinusLogProbMetric: 17.1058 - val_loss: 17.4136 - val_MinusLogProbMetric: 17.4136 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 108/1000
2023-09-27 03:34:28.294 
Epoch 108/1000 
	 loss: 17.1696, MinusLogProbMetric: 17.1696, val_loss: 17.6405, val_MinusLogProbMetric: 17.6405

Epoch 108: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.1696 - MinusLogProbMetric: 17.1696 - val_loss: 17.6405 - val_MinusLogProbMetric: 17.6405 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 109/1000
2023-09-27 03:35:02.785 
Epoch 109/1000 
	 loss: 17.1790, MinusLogProbMetric: 17.1790, val_loss: 17.4184, val_MinusLogProbMetric: 17.4184

Epoch 109: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.1790 - MinusLogProbMetric: 17.1790 - val_loss: 17.4184 - val_MinusLogProbMetric: 17.4184 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 110/1000
2023-09-27 03:35:37.082 
Epoch 110/1000 
	 loss: 17.2083, MinusLogProbMetric: 17.2083, val_loss: 17.5158, val_MinusLogProbMetric: 17.5158

Epoch 110: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.2083 - MinusLogProbMetric: 17.2083 - val_loss: 17.5158 - val_MinusLogProbMetric: 17.5158 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 111/1000
2023-09-27 03:36:11.187 
Epoch 111/1000 
	 loss: 17.1183, MinusLogProbMetric: 17.1183, val_loss: 17.9101, val_MinusLogProbMetric: 17.9101

Epoch 111: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.1183 - MinusLogProbMetric: 17.1183 - val_loss: 17.9101 - val_MinusLogProbMetric: 17.9101 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 112/1000
2023-09-27 03:36:45.078 
Epoch 112/1000 
	 loss: 17.0719, MinusLogProbMetric: 17.0719, val_loss: 17.6390, val_MinusLogProbMetric: 17.6390

Epoch 112: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0719 - MinusLogProbMetric: 17.0719 - val_loss: 17.6390 - val_MinusLogProbMetric: 17.6390 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 113/1000
2023-09-27 03:37:19.215 
Epoch 113/1000 
	 loss: 17.1432, MinusLogProbMetric: 17.1432, val_loss: 17.5919, val_MinusLogProbMetric: 17.5919

Epoch 113: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.1432 - MinusLogProbMetric: 17.1432 - val_loss: 17.5919 - val_MinusLogProbMetric: 17.5919 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 114/1000
2023-09-27 03:37:53.492 
Epoch 114/1000 
	 loss: 17.0734, MinusLogProbMetric: 17.0734, val_loss: 17.6883, val_MinusLogProbMetric: 17.6883

Epoch 114: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0734 - MinusLogProbMetric: 17.0734 - val_loss: 17.6883 - val_MinusLogProbMetric: 17.6883 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 115/1000
2023-09-27 03:38:27.844 
Epoch 115/1000 
	 loss: 17.0769, MinusLogProbMetric: 17.0769, val_loss: 17.5309, val_MinusLogProbMetric: 17.5309

Epoch 115: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0769 - MinusLogProbMetric: 17.0769 - val_loss: 17.5309 - val_MinusLogProbMetric: 17.5309 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 116/1000
2023-09-27 03:39:01.884 
Epoch 116/1000 
	 loss: 17.0549, MinusLogProbMetric: 17.0549, val_loss: 17.6597, val_MinusLogProbMetric: 17.6597

Epoch 116: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0549 - MinusLogProbMetric: 17.0549 - val_loss: 17.6597 - val_MinusLogProbMetric: 17.6597 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 117/1000
2023-09-27 03:39:36.359 
Epoch 117/1000 
	 loss: 17.0162, MinusLogProbMetric: 17.0162, val_loss: 17.5117, val_MinusLogProbMetric: 17.5117

Epoch 117: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0162 - MinusLogProbMetric: 17.0162 - val_loss: 17.5117 - val_MinusLogProbMetric: 17.5117 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 118/1000
2023-09-27 03:40:10.463 
Epoch 118/1000 
	 loss: 17.0547, MinusLogProbMetric: 17.0547, val_loss: 18.1196, val_MinusLogProbMetric: 18.1196

Epoch 118: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0547 - MinusLogProbMetric: 17.0547 - val_loss: 18.1196 - val_MinusLogProbMetric: 18.1196 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 119/1000
2023-09-27 03:40:44.888 
Epoch 119/1000 
	 loss: 17.0545, MinusLogProbMetric: 17.0545, val_loss: 17.5773, val_MinusLogProbMetric: 17.5773

Epoch 119: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0545 - MinusLogProbMetric: 17.0545 - val_loss: 17.5773 - val_MinusLogProbMetric: 17.5773 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 120/1000
2023-09-27 03:41:18.982 
Epoch 120/1000 
	 loss: 17.0305, MinusLogProbMetric: 17.0305, val_loss: 17.6652, val_MinusLogProbMetric: 17.6652

Epoch 120: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0305 - MinusLogProbMetric: 17.0305 - val_loss: 17.6652 - val_MinusLogProbMetric: 17.6652 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 121/1000
2023-09-27 03:41:53.311 
Epoch 121/1000 
	 loss: 17.0614, MinusLogProbMetric: 17.0614, val_loss: 17.4623, val_MinusLogProbMetric: 17.4623

Epoch 121: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0614 - MinusLogProbMetric: 17.0614 - val_loss: 17.4623 - val_MinusLogProbMetric: 17.4623 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 122/1000
2023-09-27 03:42:27.164 
Epoch 122/1000 
	 loss: 17.0336, MinusLogProbMetric: 17.0336, val_loss: 17.6702, val_MinusLogProbMetric: 17.6702

Epoch 122: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0336 - MinusLogProbMetric: 17.0336 - val_loss: 17.6702 - val_MinusLogProbMetric: 17.6702 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 123/1000
2023-09-27 03:43:01.606 
Epoch 123/1000 
	 loss: 17.0407, MinusLogProbMetric: 17.0407, val_loss: 17.4645, val_MinusLogProbMetric: 17.4645

Epoch 123: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0407 - MinusLogProbMetric: 17.0407 - val_loss: 17.4645 - val_MinusLogProbMetric: 17.4645 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 124/1000
2023-09-27 03:43:35.870 
Epoch 124/1000 
	 loss: 16.9828, MinusLogProbMetric: 16.9828, val_loss: 17.5210, val_MinusLogProbMetric: 17.5210

Epoch 124: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9828 - MinusLogProbMetric: 16.9828 - val_loss: 17.5210 - val_MinusLogProbMetric: 17.5210 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 125/1000
2023-09-27 03:44:09.913 
Epoch 125/1000 
	 loss: 17.0380, MinusLogProbMetric: 17.0380, val_loss: 17.6371, val_MinusLogProbMetric: 17.6371

Epoch 125: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0380 - MinusLogProbMetric: 17.0380 - val_loss: 17.6371 - val_MinusLogProbMetric: 17.6371 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 126/1000
2023-09-27 03:44:44.274 
Epoch 126/1000 
	 loss: 16.9884, MinusLogProbMetric: 16.9884, val_loss: 17.4267, val_MinusLogProbMetric: 17.4267

Epoch 126: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9884 - MinusLogProbMetric: 16.9884 - val_loss: 17.4267 - val_MinusLogProbMetric: 17.4267 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 127/1000
2023-09-27 03:45:18.307 
Epoch 127/1000 
	 loss: 16.9758, MinusLogProbMetric: 16.9758, val_loss: 17.6071, val_MinusLogProbMetric: 17.6071

Epoch 127: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9758 - MinusLogProbMetric: 16.9758 - val_loss: 17.6071 - val_MinusLogProbMetric: 17.6071 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 128/1000
2023-09-27 03:45:52.873 
Epoch 128/1000 
	 loss: 16.9955, MinusLogProbMetric: 16.9955, val_loss: 17.8242, val_MinusLogProbMetric: 17.8242

Epoch 128: val_loss did not improve from 17.41361
196/196 - 35s - loss: 16.9955 - MinusLogProbMetric: 16.9955 - val_loss: 17.8242 - val_MinusLogProbMetric: 17.8242 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 129/1000
2023-09-27 03:46:27.122 
Epoch 129/1000 
	 loss: 16.9808, MinusLogProbMetric: 16.9808, val_loss: 17.4508, val_MinusLogProbMetric: 17.4508

Epoch 129: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9808 - MinusLogProbMetric: 16.9808 - val_loss: 17.4508 - val_MinusLogProbMetric: 17.4508 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 130/1000
2023-09-27 03:47:01.434 
Epoch 130/1000 
	 loss: 17.0481, MinusLogProbMetric: 17.0481, val_loss: 17.5409, val_MinusLogProbMetric: 17.5409

Epoch 130: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0481 - MinusLogProbMetric: 17.0481 - val_loss: 17.5409 - val_MinusLogProbMetric: 17.5409 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 131/1000
2023-09-27 03:47:35.452 
Epoch 131/1000 
	 loss: 16.9528, MinusLogProbMetric: 16.9528, val_loss: 17.6835, val_MinusLogProbMetric: 17.6835

Epoch 131: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9528 - MinusLogProbMetric: 16.9528 - val_loss: 17.6835 - val_MinusLogProbMetric: 17.6835 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 132/1000
2023-09-27 03:48:09.857 
Epoch 132/1000 
	 loss: 16.9066, MinusLogProbMetric: 16.9066, val_loss: 18.8085, val_MinusLogProbMetric: 18.8085

Epoch 132: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9066 - MinusLogProbMetric: 16.9066 - val_loss: 18.8085 - val_MinusLogProbMetric: 18.8085 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 133/1000
2023-09-27 03:48:44.088 
Epoch 133/1000 
	 loss: 16.9465, MinusLogProbMetric: 16.9465, val_loss: 17.4318, val_MinusLogProbMetric: 17.4318

Epoch 133: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9465 - MinusLogProbMetric: 16.9465 - val_loss: 17.4318 - val_MinusLogProbMetric: 17.4318 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 134/1000
2023-09-27 03:49:18.553 
Epoch 134/1000 
	 loss: 16.9810, MinusLogProbMetric: 16.9810, val_loss: 17.4336, val_MinusLogProbMetric: 17.4336

Epoch 134: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9810 - MinusLogProbMetric: 16.9810 - val_loss: 17.4336 - val_MinusLogProbMetric: 17.4336 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 135/1000
2023-09-27 03:49:52.678 
Epoch 135/1000 
	 loss: 17.0027, MinusLogProbMetric: 17.0027, val_loss: 18.0982, val_MinusLogProbMetric: 18.0982

Epoch 135: val_loss did not improve from 17.41361
196/196 - 34s - loss: 17.0027 - MinusLogProbMetric: 17.0027 - val_loss: 18.0982 - val_MinusLogProbMetric: 18.0982 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 136/1000
2023-09-27 03:50:26.904 
Epoch 136/1000 
	 loss: 16.9341, MinusLogProbMetric: 16.9341, val_loss: 17.4593, val_MinusLogProbMetric: 17.4593

Epoch 136: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9341 - MinusLogProbMetric: 16.9341 - val_loss: 17.4593 - val_MinusLogProbMetric: 17.4593 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 137/1000
2023-09-27 03:51:01.356 
Epoch 137/1000 
	 loss: 16.9421, MinusLogProbMetric: 16.9421, val_loss: 17.5513, val_MinusLogProbMetric: 17.5513

Epoch 137: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9421 - MinusLogProbMetric: 16.9421 - val_loss: 17.5513 - val_MinusLogProbMetric: 17.5513 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 138/1000
2023-09-27 03:51:35.757 
Epoch 138/1000 
	 loss: 16.9175, MinusLogProbMetric: 16.9175, val_loss: 17.5417, val_MinusLogProbMetric: 17.5417

Epoch 138: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9175 - MinusLogProbMetric: 16.9175 - val_loss: 17.5417 - val_MinusLogProbMetric: 17.5417 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 139/1000
2023-09-27 03:52:10.070 
Epoch 139/1000 
	 loss: 16.9212, MinusLogProbMetric: 16.9212, val_loss: 17.4493, val_MinusLogProbMetric: 17.4493

Epoch 139: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9212 - MinusLogProbMetric: 16.9212 - val_loss: 17.4493 - val_MinusLogProbMetric: 17.4493 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 140/1000
2023-09-27 03:52:44.144 
Epoch 140/1000 
	 loss: 16.9526, MinusLogProbMetric: 16.9526, val_loss: 17.6104, val_MinusLogProbMetric: 17.6104

Epoch 140: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9526 - MinusLogProbMetric: 16.9526 - val_loss: 17.6104 - val_MinusLogProbMetric: 17.6104 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 141/1000
2023-09-27 03:53:18.337 
Epoch 141/1000 
	 loss: 16.9327, MinusLogProbMetric: 16.9327, val_loss: 17.4585, val_MinusLogProbMetric: 17.4585

Epoch 141: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9327 - MinusLogProbMetric: 16.9327 - val_loss: 17.4585 - val_MinusLogProbMetric: 17.4585 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 142/1000
2023-09-27 03:53:52.547 
Epoch 142/1000 
	 loss: 16.8960, MinusLogProbMetric: 16.8960, val_loss: 18.2172, val_MinusLogProbMetric: 18.2172

Epoch 142: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.8960 - MinusLogProbMetric: 16.8960 - val_loss: 18.2172 - val_MinusLogProbMetric: 18.2172 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 143/1000
2023-09-27 03:54:26.791 
Epoch 143/1000 
	 loss: 16.9481, MinusLogProbMetric: 16.9481, val_loss: 17.5441, val_MinusLogProbMetric: 17.5441

Epoch 143: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.9481 - MinusLogProbMetric: 16.9481 - val_loss: 17.5441 - val_MinusLogProbMetric: 17.5441 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 144/1000
2023-09-27 03:55:00.968 
Epoch 144/1000 
	 loss: 16.8581, MinusLogProbMetric: 16.8581, val_loss: 17.4356, val_MinusLogProbMetric: 17.4356

Epoch 144: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.8581 - MinusLogProbMetric: 16.8581 - val_loss: 17.4356 - val_MinusLogProbMetric: 17.4356 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 145/1000
2023-09-27 03:55:35.325 
Epoch 145/1000 
	 loss: 16.8908, MinusLogProbMetric: 16.8908, val_loss: 17.6602, val_MinusLogProbMetric: 17.6602

Epoch 145: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.8908 - MinusLogProbMetric: 16.8908 - val_loss: 17.6602 - val_MinusLogProbMetric: 17.6602 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 146/1000
2023-09-27 03:56:09.559 
Epoch 146/1000 
	 loss: 16.8855, MinusLogProbMetric: 16.8855, val_loss: 17.6155, val_MinusLogProbMetric: 17.6155

Epoch 146: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.8855 - MinusLogProbMetric: 16.8855 - val_loss: 17.6155 - val_MinusLogProbMetric: 17.6155 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 147/1000
2023-09-27 03:56:43.628 
Epoch 147/1000 
	 loss: 16.8376, MinusLogProbMetric: 16.8376, val_loss: 17.5523, val_MinusLogProbMetric: 17.5523

Epoch 147: val_loss did not improve from 17.41361
196/196 - 34s - loss: 16.8376 - MinusLogProbMetric: 16.8376 - val_loss: 17.5523 - val_MinusLogProbMetric: 17.5523 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 148/1000
2023-09-27 03:57:17.991 
Epoch 148/1000 
	 loss: 16.8422, MinusLogProbMetric: 16.8422, val_loss: 17.4126, val_MinusLogProbMetric: 17.4126

Epoch 148: val_loss improved from 17.41361 to 17.41256, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 16.8422 - MinusLogProbMetric: 16.8422 - val_loss: 17.4126 - val_MinusLogProbMetric: 17.4126 - lr: 0.0010 - 35s/epoch - 178ms/step
Epoch 149/1000
2023-09-27 03:57:52.996 
Epoch 149/1000 
	 loss: 16.9185, MinusLogProbMetric: 16.9185, val_loss: 17.7601, val_MinusLogProbMetric: 17.7601

Epoch 149: val_loss did not improve from 17.41256
196/196 - 35s - loss: 16.9185 - MinusLogProbMetric: 16.9185 - val_loss: 17.7601 - val_MinusLogProbMetric: 17.7601 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 150/1000
2023-09-27 03:58:27.081 
Epoch 150/1000 
	 loss: 16.8338, MinusLogProbMetric: 16.8338, val_loss: 17.5384, val_MinusLogProbMetric: 17.5384

Epoch 150: val_loss did not improve from 17.41256
196/196 - 34s - loss: 16.8338 - MinusLogProbMetric: 16.8338 - val_loss: 17.5384 - val_MinusLogProbMetric: 17.5384 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 151/1000
2023-09-27 03:59:01.495 
Epoch 151/1000 
	 loss: 16.8269, MinusLogProbMetric: 16.8269, val_loss: 17.4710, val_MinusLogProbMetric: 17.4710

Epoch 151: val_loss did not improve from 17.41256
196/196 - 34s - loss: 16.8269 - MinusLogProbMetric: 16.8269 - val_loss: 17.4710 - val_MinusLogProbMetric: 17.4710 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 152/1000
2023-09-27 03:59:35.478 
Epoch 152/1000 
	 loss: 16.8243, MinusLogProbMetric: 16.8243, val_loss: 17.8004, val_MinusLogProbMetric: 17.8004

Epoch 152: val_loss did not improve from 17.41256
196/196 - 34s - loss: 16.8243 - MinusLogProbMetric: 16.8243 - val_loss: 17.8004 - val_MinusLogProbMetric: 17.8004 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 153/1000
2023-09-27 04:00:09.579 
Epoch 153/1000 
	 loss: 16.8330, MinusLogProbMetric: 16.8330, val_loss: 17.8861, val_MinusLogProbMetric: 17.8861

Epoch 153: val_loss did not improve from 17.41256
196/196 - 34s - loss: 16.8330 - MinusLogProbMetric: 16.8330 - val_loss: 17.8861 - val_MinusLogProbMetric: 17.8861 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 154/1000
2023-09-27 04:00:43.799 
Epoch 154/1000 
	 loss: 16.8403, MinusLogProbMetric: 16.8403, val_loss: 17.5564, val_MinusLogProbMetric: 17.5564

Epoch 154: val_loss did not improve from 17.41256
196/196 - 34s - loss: 16.8403 - MinusLogProbMetric: 16.8403 - val_loss: 17.5564 - val_MinusLogProbMetric: 17.5564 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 155/1000
2023-09-27 04:01:17.809 
Epoch 155/1000 
	 loss: 16.8145, MinusLogProbMetric: 16.8145, val_loss: 17.5711, val_MinusLogProbMetric: 17.5711

Epoch 155: val_loss did not improve from 17.41256
196/196 - 34s - loss: 16.8145 - MinusLogProbMetric: 16.8145 - val_loss: 17.5711 - val_MinusLogProbMetric: 17.5711 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 156/1000
2023-09-27 04:01:51.811 
Epoch 156/1000 
	 loss: 16.7937, MinusLogProbMetric: 16.7937, val_loss: 17.8594, val_MinusLogProbMetric: 17.8594

Epoch 156: val_loss did not improve from 17.41256
196/196 - 34s - loss: 16.7937 - MinusLogProbMetric: 16.7937 - val_loss: 17.8594 - val_MinusLogProbMetric: 17.8594 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 157/1000
2023-09-27 04:02:25.983 
Epoch 157/1000 
	 loss: 16.8346, MinusLogProbMetric: 16.8346, val_loss: 17.4222, val_MinusLogProbMetric: 17.4222

Epoch 157: val_loss did not improve from 17.41256
196/196 - 34s - loss: 16.8346 - MinusLogProbMetric: 16.8346 - val_loss: 17.4222 - val_MinusLogProbMetric: 17.4222 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 158/1000
2023-09-27 04:03:00.333 
Epoch 158/1000 
	 loss: 16.8037, MinusLogProbMetric: 16.8037, val_loss: 17.4822, val_MinusLogProbMetric: 17.4822

Epoch 158: val_loss did not improve from 17.41256
196/196 - 34s - loss: 16.8037 - MinusLogProbMetric: 16.8037 - val_loss: 17.4822 - val_MinusLogProbMetric: 17.4822 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 159/1000
2023-09-27 04:03:34.636 
Epoch 159/1000 
	 loss: 16.8802, MinusLogProbMetric: 16.8802, val_loss: 17.4097, val_MinusLogProbMetric: 17.4097

Epoch 159: val_loss improved from 17.41256 to 17.40968, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 16.8802 - MinusLogProbMetric: 16.8802 - val_loss: 17.4097 - val_MinusLogProbMetric: 17.4097 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 160/1000
2023-09-27 04:04:09.372 
Epoch 160/1000 
	 loss: 16.8197, MinusLogProbMetric: 16.8197, val_loss: 17.4032, val_MinusLogProbMetric: 17.4032

Epoch 160: val_loss improved from 17.40968 to 17.40320, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 16.8197 - MinusLogProbMetric: 16.8197 - val_loss: 17.4032 - val_MinusLogProbMetric: 17.4032 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 161/1000
2023-09-27 04:04:44.055 
Epoch 161/1000 
	 loss: 16.7828, MinusLogProbMetric: 16.7828, val_loss: 17.4032, val_MinusLogProbMetric: 17.4032

Epoch 161: val_loss improved from 17.40320 to 17.40318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 16.7828 - MinusLogProbMetric: 16.7828 - val_loss: 17.4032 - val_MinusLogProbMetric: 17.4032 - lr: 0.0010 - 35s/epoch - 177ms/step
Epoch 162/1000
2023-09-27 04:05:18.603 
Epoch 162/1000 
	 loss: 16.7579, MinusLogProbMetric: 16.7579, val_loss: 17.6673, val_MinusLogProbMetric: 17.6673

Epoch 162: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7579 - MinusLogProbMetric: 16.7579 - val_loss: 17.6673 - val_MinusLogProbMetric: 17.6673 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 163/1000
2023-09-27 04:05:52.821 
Epoch 163/1000 
	 loss: 16.7885, MinusLogProbMetric: 16.7885, val_loss: 17.4877, val_MinusLogProbMetric: 17.4877

Epoch 163: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7885 - MinusLogProbMetric: 16.7885 - val_loss: 17.4877 - val_MinusLogProbMetric: 17.4877 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 164/1000
2023-09-27 04:06:27.243 
Epoch 164/1000 
	 loss: 16.7739, MinusLogProbMetric: 16.7739, val_loss: 18.0096, val_MinusLogProbMetric: 18.0096

Epoch 164: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7739 - MinusLogProbMetric: 16.7739 - val_loss: 18.0096 - val_MinusLogProbMetric: 18.0096 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 165/1000
2023-09-27 04:07:01.499 
Epoch 165/1000 
	 loss: 16.7490, MinusLogProbMetric: 16.7490, val_loss: 17.6131, val_MinusLogProbMetric: 17.6131

Epoch 165: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7490 - MinusLogProbMetric: 16.7490 - val_loss: 17.6131 - val_MinusLogProbMetric: 17.6131 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 166/1000
2023-09-27 04:07:35.433 
Epoch 166/1000 
	 loss: 16.8377, MinusLogProbMetric: 16.8377, val_loss: 17.7423, val_MinusLogProbMetric: 17.7423

Epoch 166: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.8377 - MinusLogProbMetric: 16.8377 - val_loss: 17.7423 - val_MinusLogProbMetric: 17.7423 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 167/1000
2023-09-27 04:08:09.777 
Epoch 167/1000 
	 loss: 16.7743, MinusLogProbMetric: 16.7743, val_loss: 17.4262, val_MinusLogProbMetric: 17.4262

Epoch 167: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7743 - MinusLogProbMetric: 16.7743 - val_loss: 17.4262 - val_MinusLogProbMetric: 17.4262 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 168/1000
2023-09-27 04:08:43.947 
Epoch 168/1000 
	 loss: 16.7559, MinusLogProbMetric: 16.7559, val_loss: 17.8328, val_MinusLogProbMetric: 17.8328

Epoch 168: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7559 - MinusLogProbMetric: 16.7559 - val_loss: 17.8328 - val_MinusLogProbMetric: 17.8328 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 169/1000
2023-09-27 04:09:17.905 
Epoch 169/1000 
	 loss: 16.7714, MinusLogProbMetric: 16.7714, val_loss: 17.5314, val_MinusLogProbMetric: 17.5314

Epoch 169: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7714 - MinusLogProbMetric: 16.7714 - val_loss: 17.5314 - val_MinusLogProbMetric: 17.5314 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 170/1000
2023-09-27 04:09:52.344 
Epoch 170/1000 
	 loss: 16.7237, MinusLogProbMetric: 16.7237, val_loss: 17.4911, val_MinusLogProbMetric: 17.4911

Epoch 170: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7237 - MinusLogProbMetric: 16.7237 - val_loss: 17.4911 - val_MinusLogProbMetric: 17.4911 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 171/1000
2023-09-27 04:10:26.280 
Epoch 171/1000 
	 loss: 16.7436, MinusLogProbMetric: 16.7436, val_loss: 17.4976, val_MinusLogProbMetric: 17.4976

Epoch 171: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7436 - MinusLogProbMetric: 16.7436 - val_loss: 17.4976 - val_MinusLogProbMetric: 17.4976 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 172/1000
2023-09-27 04:11:00.517 
Epoch 172/1000 
	 loss: 16.7322, MinusLogProbMetric: 16.7322, val_loss: 17.6236, val_MinusLogProbMetric: 17.6236

Epoch 172: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7322 - MinusLogProbMetric: 16.7322 - val_loss: 17.6236 - val_MinusLogProbMetric: 17.6236 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 173/1000
2023-09-27 04:11:34.223 
Epoch 173/1000 
	 loss: 16.7452, MinusLogProbMetric: 16.7452, val_loss: 17.9524, val_MinusLogProbMetric: 17.9524

Epoch 173: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7452 - MinusLogProbMetric: 16.7452 - val_loss: 17.9524 - val_MinusLogProbMetric: 17.9524 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 174/1000
2023-09-27 04:12:08.547 
Epoch 174/1000 
	 loss: 16.7179, MinusLogProbMetric: 16.7179, val_loss: 17.7134, val_MinusLogProbMetric: 17.7134

Epoch 174: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7179 - MinusLogProbMetric: 16.7179 - val_loss: 17.7134 - val_MinusLogProbMetric: 17.7134 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 175/1000
2023-09-27 04:12:42.543 
Epoch 175/1000 
	 loss: 16.6974, MinusLogProbMetric: 16.6974, val_loss: 17.7908, val_MinusLogProbMetric: 17.7908

Epoch 175: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6974 - MinusLogProbMetric: 16.6974 - val_loss: 17.7908 - val_MinusLogProbMetric: 17.7908 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 176/1000
2023-09-27 04:13:16.804 
Epoch 176/1000 
	 loss: 16.7495, MinusLogProbMetric: 16.7495, val_loss: 17.6021, val_MinusLogProbMetric: 17.6021

Epoch 176: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7495 - MinusLogProbMetric: 16.7495 - val_loss: 17.6021 - val_MinusLogProbMetric: 17.6021 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 177/1000
2023-09-27 04:13:50.846 
Epoch 177/1000 
	 loss: 16.6986, MinusLogProbMetric: 16.6986, val_loss: 17.5795, val_MinusLogProbMetric: 17.5795

Epoch 177: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6986 - MinusLogProbMetric: 16.6986 - val_loss: 17.5795 - val_MinusLogProbMetric: 17.5795 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 178/1000
2023-09-27 04:14:25.216 
Epoch 178/1000 
	 loss: 16.7045, MinusLogProbMetric: 16.7045, val_loss: 17.5877, val_MinusLogProbMetric: 17.5877

Epoch 178: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7045 - MinusLogProbMetric: 16.7045 - val_loss: 17.5877 - val_MinusLogProbMetric: 17.5877 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 179/1000
2023-09-27 04:14:59.614 
Epoch 179/1000 
	 loss: 16.6959, MinusLogProbMetric: 16.6959, val_loss: 17.6653, val_MinusLogProbMetric: 17.6653

Epoch 179: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6959 - MinusLogProbMetric: 16.6959 - val_loss: 17.6653 - val_MinusLogProbMetric: 17.6653 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 180/1000
2023-09-27 04:15:33.524 
Epoch 180/1000 
	 loss: 16.6801, MinusLogProbMetric: 16.6801, val_loss: 17.8326, val_MinusLogProbMetric: 17.8326

Epoch 180: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6801 - MinusLogProbMetric: 16.6801 - val_loss: 17.8326 - val_MinusLogProbMetric: 17.8326 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 181/1000
2023-09-27 04:16:07.843 
Epoch 181/1000 
	 loss: 16.7168, MinusLogProbMetric: 16.7168, val_loss: 17.9317, val_MinusLogProbMetric: 17.9317

Epoch 181: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7168 - MinusLogProbMetric: 16.7168 - val_loss: 17.9317 - val_MinusLogProbMetric: 17.9317 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 182/1000
2023-09-27 04:16:41.766 
Epoch 182/1000 
	 loss: 16.7044, MinusLogProbMetric: 16.7044, val_loss: 17.7535, val_MinusLogProbMetric: 17.7535

Epoch 182: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7044 - MinusLogProbMetric: 16.7044 - val_loss: 17.7535 - val_MinusLogProbMetric: 17.7535 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 183/1000
2023-09-27 04:17:16.275 
Epoch 183/1000 
	 loss: 16.6948, MinusLogProbMetric: 16.6948, val_loss: 17.7095, val_MinusLogProbMetric: 17.7095

Epoch 183: val_loss did not improve from 17.40318
196/196 - 35s - loss: 16.6948 - MinusLogProbMetric: 16.6948 - val_loss: 17.7095 - val_MinusLogProbMetric: 17.7095 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 184/1000
2023-09-27 04:17:50.419 
Epoch 184/1000 
	 loss: 16.6504, MinusLogProbMetric: 16.6504, val_loss: 17.4662, val_MinusLogProbMetric: 17.4662

Epoch 184: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6504 - MinusLogProbMetric: 16.6504 - val_loss: 17.4662 - val_MinusLogProbMetric: 17.4662 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 185/1000
2023-09-27 04:18:24.628 
Epoch 185/1000 
	 loss: 16.6683, MinusLogProbMetric: 16.6683, val_loss: 17.5698, val_MinusLogProbMetric: 17.5698

Epoch 185: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6683 - MinusLogProbMetric: 16.6683 - val_loss: 17.5698 - val_MinusLogProbMetric: 17.5698 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 186/1000
2023-09-27 04:18:58.755 
Epoch 186/1000 
	 loss: 16.6687, MinusLogProbMetric: 16.6687, val_loss: 17.8314, val_MinusLogProbMetric: 17.8314

Epoch 186: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6687 - MinusLogProbMetric: 16.6687 - val_loss: 17.8314 - val_MinusLogProbMetric: 17.8314 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 187/1000
2023-09-27 04:19:33.129 
Epoch 187/1000 
	 loss: 16.6795, MinusLogProbMetric: 16.6795, val_loss: 17.6857, val_MinusLogProbMetric: 17.6857

Epoch 187: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6795 - MinusLogProbMetric: 16.6795 - val_loss: 17.6857 - val_MinusLogProbMetric: 17.6857 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 188/1000
2023-09-27 04:20:07.420 
Epoch 188/1000 
	 loss: 16.6406, MinusLogProbMetric: 16.6406, val_loss: 17.4593, val_MinusLogProbMetric: 17.4593

Epoch 188: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6406 - MinusLogProbMetric: 16.6406 - val_loss: 17.4593 - val_MinusLogProbMetric: 17.4593 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 189/1000
2023-09-27 04:20:41.739 
Epoch 189/1000 
	 loss: 16.6761, MinusLogProbMetric: 16.6761, val_loss: 17.5601, val_MinusLogProbMetric: 17.5601

Epoch 189: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6761 - MinusLogProbMetric: 16.6761 - val_loss: 17.5601 - val_MinusLogProbMetric: 17.5601 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 190/1000
2023-09-27 04:21:16.043 
Epoch 190/1000 
	 loss: 16.7038, MinusLogProbMetric: 16.7038, val_loss: 17.6832, val_MinusLogProbMetric: 17.6832

Epoch 190: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.7038 - MinusLogProbMetric: 16.7038 - val_loss: 17.6832 - val_MinusLogProbMetric: 17.6832 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 191/1000
2023-09-27 04:21:50.301 
Epoch 191/1000 
	 loss: 16.6160, MinusLogProbMetric: 16.6160, val_loss: 17.4382, val_MinusLogProbMetric: 17.4382

Epoch 191: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6160 - MinusLogProbMetric: 16.6160 - val_loss: 17.4382 - val_MinusLogProbMetric: 17.4382 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 192/1000
2023-09-27 04:22:24.816 
Epoch 192/1000 
	 loss: 16.6371, MinusLogProbMetric: 16.6371, val_loss: 18.0263, val_MinusLogProbMetric: 18.0263

Epoch 192: val_loss did not improve from 17.40318
196/196 - 35s - loss: 16.6371 - MinusLogProbMetric: 16.6371 - val_loss: 18.0263 - val_MinusLogProbMetric: 18.0263 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 193/1000
2023-09-27 04:22:59.025 
Epoch 193/1000 
	 loss: 16.6226, MinusLogProbMetric: 16.6226, val_loss: 17.5535, val_MinusLogProbMetric: 17.5535

Epoch 193: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6226 - MinusLogProbMetric: 16.6226 - val_loss: 17.5535 - val_MinusLogProbMetric: 17.5535 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 194/1000
2023-09-27 04:23:32.972 
Epoch 194/1000 
	 loss: 16.6463, MinusLogProbMetric: 16.6463, val_loss: 17.4539, val_MinusLogProbMetric: 17.4539

Epoch 194: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6463 - MinusLogProbMetric: 16.6463 - val_loss: 17.4539 - val_MinusLogProbMetric: 17.4539 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 195/1000
2023-09-27 04:24:06.961 
Epoch 195/1000 
	 loss: 16.6001, MinusLogProbMetric: 16.6001, val_loss: 17.8537, val_MinusLogProbMetric: 17.8537

Epoch 195: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6001 - MinusLogProbMetric: 16.6001 - val_loss: 17.8537 - val_MinusLogProbMetric: 17.8537 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 196/1000
2023-09-27 04:24:40.882 
Epoch 196/1000 
	 loss: 16.6649, MinusLogProbMetric: 16.6649, val_loss: 17.6925, val_MinusLogProbMetric: 17.6925

Epoch 196: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6649 - MinusLogProbMetric: 16.6649 - val_loss: 17.6925 - val_MinusLogProbMetric: 17.6925 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 197/1000
2023-09-27 04:25:15.071 
Epoch 197/1000 
	 loss: 16.6015, MinusLogProbMetric: 16.6015, val_loss: 17.5440, val_MinusLogProbMetric: 17.5440

Epoch 197: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6015 - MinusLogProbMetric: 16.6015 - val_loss: 17.5440 - val_MinusLogProbMetric: 17.5440 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 198/1000
2023-09-27 04:25:49.319 
Epoch 198/1000 
	 loss: 16.6248, MinusLogProbMetric: 16.6248, val_loss: 17.6492, val_MinusLogProbMetric: 17.6492

Epoch 198: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6248 - MinusLogProbMetric: 16.6248 - val_loss: 17.6492 - val_MinusLogProbMetric: 17.6492 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 199/1000
2023-09-27 04:26:23.145 
Epoch 199/1000 
	 loss: 16.6042, MinusLogProbMetric: 16.6042, val_loss: 17.5400, val_MinusLogProbMetric: 17.5400

Epoch 199: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6042 - MinusLogProbMetric: 16.6042 - val_loss: 17.5400 - val_MinusLogProbMetric: 17.5400 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 200/1000
2023-09-27 04:26:57.138 
Epoch 200/1000 
	 loss: 16.5947, MinusLogProbMetric: 16.5947, val_loss: 17.5965, val_MinusLogProbMetric: 17.5965

Epoch 200: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.5947 - MinusLogProbMetric: 16.5947 - val_loss: 17.5965 - val_MinusLogProbMetric: 17.5965 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 201/1000
2023-09-27 04:27:31.034 
Epoch 201/1000 
	 loss: 16.5879, MinusLogProbMetric: 16.5879, val_loss: 17.6334, val_MinusLogProbMetric: 17.6334

Epoch 201: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.5879 - MinusLogProbMetric: 16.5879 - val_loss: 17.6334 - val_MinusLogProbMetric: 17.6334 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 202/1000
2023-09-27 04:28:05.252 
Epoch 202/1000 
	 loss: 16.5641, MinusLogProbMetric: 16.5641, val_loss: 17.6485, val_MinusLogProbMetric: 17.6485

Epoch 202: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.5641 - MinusLogProbMetric: 16.5641 - val_loss: 17.6485 - val_MinusLogProbMetric: 17.6485 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 203/1000
2023-09-27 04:28:39.388 
Epoch 203/1000 
	 loss: 16.6189, MinusLogProbMetric: 16.6189, val_loss: 17.5233, val_MinusLogProbMetric: 17.5233

Epoch 203: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.6189 - MinusLogProbMetric: 16.6189 - val_loss: 17.5233 - val_MinusLogProbMetric: 17.5233 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 204/1000
2023-09-27 04:29:13.323 
Epoch 204/1000 
	 loss: 16.5779, MinusLogProbMetric: 16.5779, val_loss: 17.9572, val_MinusLogProbMetric: 17.9572

Epoch 204: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.5779 - MinusLogProbMetric: 16.5779 - val_loss: 17.9572 - val_MinusLogProbMetric: 17.9572 - lr: 0.0010 - 34s/epoch - 173ms/step
Epoch 205/1000
2023-09-27 04:29:47.681 
Epoch 205/1000 
	 loss: 16.5937, MinusLogProbMetric: 16.5937, val_loss: 17.6712, val_MinusLogProbMetric: 17.6712

Epoch 205: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.5937 - MinusLogProbMetric: 16.5937 - val_loss: 17.6712 - val_MinusLogProbMetric: 17.6712 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 206/1000
2023-09-27 04:30:21.869 
Epoch 206/1000 
	 loss: 16.5761, MinusLogProbMetric: 16.5761, val_loss: 17.5802, val_MinusLogProbMetric: 17.5802

Epoch 206: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.5761 - MinusLogProbMetric: 16.5761 - val_loss: 17.5802 - val_MinusLogProbMetric: 17.5802 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 207/1000
2023-09-27 04:30:56.241 
Epoch 207/1000 
	 loss: 16.5462, MinusLogProbMetric: 16.5462, val_loss: 17.4831, val_MinusLogProbMetric: 17.4831

Epoch 207: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.5462 - MinusLogProbMetric: 16.5462 - val_loss: 17.4831 - val_MinusLogProbMetric: 17.4831 - lr: 0.0010 - 34s/epoch - 175ms/step
Epoch 208/1000
2023-09-27 04:31:30.381 
Epoch 208/1000 
	 loss: 16.5612, MinusLogProbMetric: 16.5612, val_loss: 17.4965, val_MinusLogProbMetric: 17.4965

Epoch 208: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.5612 - MinusLogProbMetric: 16.5612 - val_loss: 17.4965 - val_MinusLogProbMetric: 17.4965 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 209/1000
2023-09-27 04:32:04.427 
Epoch 209/1000 
	 loss: 16.5331, MinusLogProbMetric: 16.5331, val_loss: 17.6653, val_MinusLogProbMetric: 17.6653

Epoch 209: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.5331 - MinusLogProbMetric: 16.5331 - val_loss: 17.6653 - val_MinusLogProbMetric: 17.6653 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 210/1000
2023-09-27 04:32:38.485 
Epoch 210/1000 
	 loss: 16.5415, MinusLogProbMetric: 16.5415, val_loss: 17.8268, val_MinusLogProbMetric: 17.8268

Epoch 210: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.5415 - MinusLogProbMetric: 16.5415 - val_loss: 17.8268 - val_MinusLogProbMetric: 17.8268 - lr: 0.0010 - 34s/epoch - 174ms/step
Epoch 211/1000
2023-09-27 04:33:12.606 
Epoch 211/1000 
	 loss: 16.2630, MinusLogProbMetric: 16.2630, val_loss: 17.4235, val_MinusLogProbMetric: 17.4235

Epoch 211: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.2630 - MinusLogProbMetric: 16.2630 - val_loss: 17.4235 - val_MinusLogProbMetric: 17.4235 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 212/1000
2023-09-27 04:33:46.967 
Epoch 212/1000 
	 loss: 16.2517, MinusLogProbMetric: 16.2517, val_loss: 17.5159, val_MinusLogProbMetric: 17.5159

Epoch 212: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.2517 - MinusLogProbMetric: 16.2517 - val_loss: 17.5159 - val_MinusLogProbMetric: 17.5159 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 213/1000
2023-09-27 04:34:20.955 
Epoch 213/1000 
	 loss: 16.2416, MinusLogProbMetric: 16.2416, val_loss: 17.6198, val_MinusLogProbMetric: 17.6198

Epoch 213: val_loss did not improve from 17.40318
196/196 - 34s - loss: 16.2416 - MinusLogProbMetric: 16.2416 - val_loss: 17.6198 - val_MinusLogProbMetric: 17.6198 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 214/1000
2023-09-27 04:34:55.184 
Epoch 214/1000 
	 loss: 16.2582, MinusLogProbMetric: 16.2582, val_loss: 17.3758, val_MinusLogProbMetric: 17.3758

Epoch 214: val_loss improved from 17.40318 to 17.37576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_298/weights/best_weights.h5
196/196 - 35s - loss: 16.2582 - MinusLogProbMetric: 16.2582 - val_loss: 17.3758 - val_MinusLogProbMetric: 17.3758 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 215/1000
2023-09-27 04:35:29.889 
Epoch 215/1000 
	 loss: 16.2528, MinusLogProbMetric: 16.2528, val_loss: 17.4119, val_MinusLogProbMetric: 17.4119

Epoch 215: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2528 - MinusLogProbMetric: 16.2528 - val_loss: 17.4119 - val_MinusLogProbMetric: 17.4119 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 216/1000
2023-09-27 04:36:04.644 
Epoch 216/1000 
	 loss: 16.2379, MinusLogProbMetric: 16.2379, val_loss: 17.5002, val_MinusLogProbMetric: 17.5002

Epoch 216: val_loss did not improve from 17.37576
196/196 - 35s - loss: 16.2379 - MinusLogProbMetric: 16.2379 - val_loss: 17.5002 - val_MinusLogProbMetric: 17.5002 - lr: 5.0000e-04 - 35s/epoch - 177ms/step
Epoch 217/1000
2023-09-27 04:36:38.646 
Epoch 217/1000 
	 loss: 16.2210, MinusLogProbMetric: 16.2210, val_loss: 17.4614, val_MinusLogProbMetric: 17.4614

Epoch 217: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2210 - MinusLogProbMetric: 16.2210 - val_loss: 17.4614 - val_MinusLogProbMetric: 17.4614 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 218/1000
2023-09-27 04:37:12.867 
Epoch 218/1000 
	 loss: 16.2471, MinusLogProbMetric: 16.2471, val_loss: 17.4382, val_MinusLogProbMetric: 17.4382

Epoch 218: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2471 - MinusLogProbMetric: 16.2471 - val_loss: 17.4382 - val_MinusLogProbMetric: 17.4382 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 219/1000
2023-09-27 04:37:46.762 
Epoch 219/1000 
	 loss: 16.2699, MinusLogProbMetric: 16.2699, val_loss: 17.4969, val_MinusLogProbMetric: 17.4969

Epoch 219: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2699 - MinusLogProbMetric: 16.2699 - val_loss: 17.4969 - val_MinusLogProbMetric: 17.4969 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 220/1000
2023-09-27 04:38:21.100 
Epoch 220/1000 
	 loss: 16.2519, MinusLogProbMetric: 16.2519, val_loss: 17.4041, val_MinusLogProbMetric: 17.4041

Epoch 220: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2519 - MinusLogProbMetric: 16.2519 - val_loss: 17.4041 - val_MinusLogProbMetric: 17.4041 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 221/1000
2023-09-27 04:38:55.166 
Epoch 221/1000 
	 loss: 16.2245, MinusLogProbMetric: 16.2245, val_loss: 17.4784, val_MinusLogProbMetric: 17.4784

Epoch 221: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2245 - MinusLogProbMetric: 16.2245 - val_loss: 17.4784 - val_MinusLogProbMetric: 17.4784 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 222/1000
2023-09-27 04:39:29.541 
Epoch 222/1000 
	 loss: 16.2246, MinusLogProbMetric: 16.2246, val_loss: 17.4027, val_MinusLogProbMetric: 17.4027

Epoch 222: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2246 - MinusLogProbMetric: 16.2246 - val_loss: 17.4027 - val_MinusLogProbMetric: 17.4027 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 223/1000
2023-09-27 04:40:03.634 
Epoch 223/1000 
	 loss: 16.2405, MinusLogProbMetric: 16.2405, val_loss: 17.5587, val_MinusLogProbMetric: 17.5587

Epoch 223: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2405 - MinusLogProbMetric: 16.2405 - val_loss: 17.5587 - val_MinusLogProbMetric: 17.5587 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 224/1000
2023-09-27 04:40:38.071 
Epoch 224/1000 
	 loss: 16.2294, MinusLogProbMetric: 16.2294, val_loss: 17.3902, val_MinusLogProbMetric: 17.3902

Epoch 224: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2294 - MinusLogProbMetric: 16.2294 - val_loss: 17.3902 - val_MinusLogProbMetric: 17.3902 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 225/1000
2023-09-27 04:41:12.432 
Epoch 225/1000 
	 loss: 16.2392, MinusLogProbMetric: 16.2392, val_loss: 17.5090, val_MinusLogProbMetric: 17.5090

Epoch 225: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2392 - MinusLogProbMetric: 16.2392 - val_loss: 17.5090 - val_MinusLogProbMetric: 17.5090 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 226/1000
2023-09-27 04:41:46.557 
Epoch 226/1000 
	 loss: 16.2110, MinusLogProbMetric: 16.2110, val_loss: 17.4942, val_MinusLogProbMetric: 17.4942

Epoch 226: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2110 - MinusLogProbMetric: 16.2110 - val_loss: 17.4942 - val_MinusLogProbMetric: 17.4942 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 227/1000
2023-09-27 04:42:21.088 
Epoch 227/1000 
	 loss: 16.2114, MinusLogProbMetric: 16.2114, val_loss: 17.4577, val_MinusLogProbMetric: 17.4577

Epoch 227: val_loss did not improve from 17.37576
196/196 - 35s - loss: 16.2114 - MinusLogProbMetric: 16.2114 - val_loss: 17.4577 - val_MinusLogProbMetric: 17.4577 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 228/1000
2023-09-27 04:42:55.150 
Epoch 228/1000 
	 loss: 16.2242, MinusLogProbMetric: 16.2242, val_loss: 17.3848, val_MinusLogProbMetric: 17.3848

Epoch 228: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2242 - MinusLogProbMetric: 16.2242 - val_loss: 17.3848 - val_MinusLogProbMetric: 17.3848 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 229/1000
2023-09-27 04:43:29.502 
Epoch 229/1000 
	 loss: 16.2090, MinusLogProbMetric: 16.2090, val_loss: 17.7059, val_MinusLogProbMetric: 17.7059

Epoch 229: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2090 - MinusLogProbMetric: 16.2090 - val_loss: 17.7059 - val_MinusLogProbMetric: 17.7059 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 230/1000
2023-09-27 04:44:03.597 
Epoch 230/1000 
	 loss: 16.2100, MinusLogProbMetric: 16.2100, val_loss: 17.4310, val_MinusLogProbMetric: 17.4310

Epoch 230: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2100 - MinusLogProbMetric: 16.2100 - val_loss: 17.4310 - val_MinusLogProbMetric: 17.4310 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 231/1000
2023-09-27 04:44:37.849 
Epoch 231/1000 
	 loss: 16.2109, MinusLogProbMetric: 16.2109, val_loss: 17.5185, val_MinusLogProbMetric: 17.5185

Epoch 231: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2109 - MinusLogProbMetric: 16.2109 - val_loss: 17.5185 - val_MinusLogProbMetric: 17.5185 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 232/1000
2023-09-27 04:45:11.906 
Epoch 232/1000 
	 loss: 16.2005, MinusLogProbMetric: 16.2005, val_loss: 17.4641, val_MinusLogProbMetric: 17.4641

Epoch 232: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2005 - MinusLogProbMetric: 16.2005 - val_loss: 17.4641 - val_MinusLogProbMetric: 17.4641 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 233/1000
2023-09-27 04:45:45.918 
Epoch 233/1000 
	 loss: 16.2243, MinusLogProbMetric: 16.2243, val_loss: 17.5012, val_MinusLogProbMetric: 17.5012

Epoch 233: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2243 - MinusLogProbMetric: 16.2243 - val_loss: 17.5012 - val_MinusLogProbMetric: 17.5012 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 234/1000
2023-09-27 04:46:20.239 
Epoch 234/1000 
	 loss: 16.2002, MinusLogProbMetric: 16.2002, val_loss: 17.4205, val_MinusLogProbMetric: 17.4205

Epoch 234: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2002 - MinusLogProbMetric: 16.2002 - val_loss: 17.4205 - val_MinusLogProbMetric: 17.4205 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 235/1000
2023-09-27 04:46:54.229 
Epoch 235/1000 
	 loss: 16.1804, MinusLogProbMetric: 16.1804, val_loss: 17.6761, val_MinusLogProbMetric: 17.6761

Epoch 235: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1804 - MinusLogProbMetric: 16.1804 - val_loss: 17.6761 - val_MinusLogProbMetric: 17.6761 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 236/1000
2023-09-27 04:47:28.646 
Epoch 236/1000 
	 loss: 16.1905, MinusLogProbMetric: 16.1905, val_loss: 17.4888, val_MinusLogProbMetric: 17.4888

Epoch 236: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1905 - MinusLogProbMetric: 16.1905 - val_loss: 17.4888 - val_MinusLogProbMetric: 17.4888 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 237/1000
2023-09-27 04:48:03.080 
Epoch 237/1000 
	 loss: 16.2009, MinusLogProbMetric: 16.2009, val_loss: 17.4910, val_MinusLogProbMetric: 17.4910

Epoch 237: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2009 - MinusLogProbMetric: 16.2009 - val_loss: 17.4910 - val_MinusLogProbMetric: 17.4910 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 238/1000
2023-09-27 04:48:37.471 
Epoch 238/1000 
	 loss: 16.1753, MinusLogProbMetric: 16.1753, val_loss: 17.5847, val_MinusLogProbMetric: 17.5847

Epoch 238: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1753 - MinusLogProbMetric: 16.1753 - val_loss: 17.5847 - val_MinusLogProbMetric: 17.5847 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 239/1000
2023-09-27 04:49:11.712 
Epoch 239/1000 
	 loss: 16.2488, MinusLogProbMetric: 16.2488, val_loss: 17.4917, val_MinusLogProbMetric: 17.4917

Epoch 239: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2488 - MinusLogProbMetric: 16.2488 - val_loss: 17.4917 - val_MinusLogProbMetric: 17.4917 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 240/1000
2023-09-27 04:49:46.162 
Epoch 240/1000 
	 loss: 16.2117, MinusLogProbMetric: 16.2117, val_loss: 17.4754, val_MinusLogProbMetric: 17.4754

Epoch 240: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2117 - MinusLogProbMetric: 16.2117 - val_loss: 17.4754 - val_MinusLogProbMetric: 17.4754 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 241/1000
2023-09-27 04:50:20.397 
Epoch 241/1000 
	 loss: 16.1867, MinusLogProbMetric: 16.1867, val_loss: 17.4837, val_MinusLogProbMetric: 17.4837

Epoch 241: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1867 - MinusLogProbMetric: 16.1867 - val_loss: 17.4837 - val_MinusLogProbMetric: 17.4837 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 242/1000
2023-09-27 04:50:54.845 
Epoch 242/1000 
	 loss: 16.1850, MinusLogProbMetric: 16.1850, val_loss: 17.4456, val_MinusLogProbMetric: 17.4456

Epoch 242: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1850 - MinusLogProbMetric: 16.1850 - val_loss: 17.4456 - val_MinusLogProbMetric: 17.4456 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 243/1000
2023-09-27 04:51:28.972 
Epoch 243/1000 
	 loss: 16.1843, MinusLogProbMetric: 16.1843, val_loss: 17.4229, val_MinusLogProbMetric: 17.4229

Epoch 243: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1843 - MinusLogProbMetric: 16.1843 - val_loss: 17.4229 - val_MinusLogProbMetric: 17.4229 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 244/1000
2023-09-27 04:52:03.445 
Epoch 244/1000 
	 loss: 16.2009, MinusLogProbMetric: 16.2009, val_loss: 17.4832, val_MinusLogProbMetric: 17.4832

Epoch 244: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2009 - MinusLogProbMetric: 16.2009 - val_loss: 17.4832 - val_MinusLogProbMetric: 17.4832 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 245/1000
2023-09-27 04:52:37.648 
Epoch 245/1000 
	 loss: 16.1458, MinusLogProbMetric: 16.1458, val_loss: 17.7384, val_MinusLogProbMetric: 17.7384

Epoch 245: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1458 - MinusLogProbMetric: 16.1458 - val_loss: 17.7384 - val_MinusLogProbMetric: 17.7384 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 246/1000
2023-09-27 04:53:11.612 
Epoch 246/1000 
	 loss: 16.1677, MinusLogProbMetric: 16.1677, val_loss: 17.5196, val_MinusLogProbMetric: 17.5196

Epoch 246: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1677 - MinusLogProbMetric: 16.1677 - val_loss: 17.5196 - val_MinusLogProbMetric: 17.5196 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 247/1000
2023-09-27 04:53:45.863 
Epoch 247/1000 
	 loss: 16.2045, MinusLogProbMetric: 16.2045, val_loss: 17.5042, val_MinusLogProbMetric: 17.5042

Epoch 247: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.2045 - MinusLogProbMetric: 16.2045 - val_loss: 17.5042 - val_MinusLogProbMetric: 17.5042 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 248/1000
2023-09-27 04:54:20.410 
Epoch 248/1000 
	 loss: 16.1756, MinusLogProbMetric: 16.1756, val_loss: 17.4987, val_MinusLogProbMetric: 17.4987

Epoch 248: val_loss did not improve from 17.37576
196/196 - 35s - loss: 16.1756 - MinusLogProbMetric: 16.1756 - val_loss: 17.4987 - val_MinusLogProbMetric: 17.4987 - lr: 5.0000e-04 - 35s/epoch - 176ms/step
Epoch 249/1000
2023-09-27 04:54:54.598 
Epoch 249/1000 
	 loss: 16.1333, MinusLogProbMetric: 16.1333, val_loss: 17.6500, val_MinusLogProbMetric: 17.6500

Epoch 249: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1333 - MinusLogProbMetric: 16.1333 - val_loss: 17.6500 - val_MinusLogProbMetric: 17.6500 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 250/1000
2023-09-27 04:55:28.669 
Epoch 250/1000 
	 loss: 16.1600, MinusLogProbMetric: 16.1600, val_loss: 17.5326, val_MinusLogProbMetric: 17.5326

Epoch 250: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1600 - MinusLogProbMetric: 16.1600 - val_loss: 17.5326 - val_MinusLogProbMetric: 17.5326 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 251/1000
2023-09-27 04:56:03.081 
Epoch 251/1000 
	 loss: 16.1664, MinusLogProbMetric: 16.1664, val_loss: 17.4842, val_MinusLogProbMetric: 17.4842

Epoch 251: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1664 - MinusLogProbMetric: 16.1664 - val_loss: 17.4842 - val_MinusLogProbMetric: 17.4842 - lr: 5.0000e-04 - 34s/epoch - 176ms/step
Epoch 252/1000
2023-09-27 04:56:36.927 
Epoch 252/1000 
	 loss: 16.1534, MinusLogProbMetric: 16.1534, val_loss: 17.5311, val_MinusLogProbMetric: 17.5311

Epoch 252: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1534 - MinusLogProbMetric: 16.1534 - val_loss: 17.5311 - val_MinusLogProbMetric: 17.5311 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 253/1000
2023-09-27 04:57:10.821 
Epoch 253/1000 
	 loss: 16.1737, MinusLogProbMetric: 16.1737, val_loss: 17.7552, val_MinusLogProbMetric: 17.7552

Epoch 253: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1737 - MinusLogProbMetric: 16.1737 - val_loss: 17.7552 - val_MinusLogProbMetric: 17.7552 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 254/1000
2023-09-27 04:57:44.204 
Epoch 254/1000 
	 loss: 16.1600, MinusLogProbMetric: 16.1600, val_loss: 17.6615, val_MinusLogProbMetric: 17.6615

Epoch 254: val_loss did not improve from 17.37576
196/196 - 33s - loss: 16.1600 - MinusLogProbMetric: 16.1600 - val_loss: 17.6615 - val_MinusLogProbMetric: 17.6615 - lr: 5.0000e-04 - 33s/epoch - 170ms/step
Epoch 255/1000
2023-09-27 04:58:14.874 
Epoch 255/1000 
	 loss: 16.1540, MinusLogProbMetric: 16.1540, val_loss: 18.0601, val_MinusLogProbMetric: 18.0601

Epoch 255: val_loss did not improve from 17.37576
196/196 - 31s - loss: 16.1540 - MinusLogProbMetric: 16.1540 - val_loss: 18.0601 - val_MinusLogProbMetric: 18.0601 - lr: 5.0000e-04 - 31s/epoch - 156ms/step
Epoch 256/1000
2023-09-27 04:58:47.183 
Epoch 256/1000 
	 loss: 16.1632, MinusLogProbMetric: 16.1632, val_loss: 17.5677, val_MinusLogProbMetric: 17.5677

Epoch 256: val_loss did not improve from 17.37576
196/196 - 32s - loss: 16.1632 - MinusLogProbMetric: 16.1632 - val_loss: 17.5677 - val_MinusLogProbMetric: 17.5677 - lr: 5.0000e-04 - 32s/epoch - 165ms/step
Epoch 257/1000
2023-09-27 04:59:21.290 
Epoch 257/1000 
	 loss: 16.1213, MinusLogProbMetric: 16.1213, val_loss: 17.5004, val_MinusLogProbMetric: 17.5004

Epoch 257: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1213 - MinusLogProbMetric: 16.1213 - val_loss: 17.5004 - val_MinusLogProbMetric: 17.5004 - lr: 5.0000e-04 - 34s/epoch - 174ms/step
Epoch 258/1000
2023-09-27 04:59:55.178 
Epoch 258/1000 
	 loss: 16.1699, MinusLogProbMetric: 16.1699, val_loss: 17.5636, val_MinusLogProbMetric: 17.5636

Epoch 258: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1699 - MinusLogProbMetric: 16.1699 - val_loss: 17.5636 - val_MinusLogProbMetric: 17.5636 - lr: 5.0000e-04 - 34s/epoch - 173ms/step
Epoch 259/1000
2023-09-27 05:00:29.387 
Epoch 259/1000 
	 loss: 16.1577, MinusLogProbMetric: 16.1577, val_loss: 17.5005, val_MinusLogProbMetric: 17.5005

Epoch 259: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1577 - MinusLogProbMetric: 16.1577 - val_loss: 17.5005 - val_MinusLogProbMetric: 17.5005 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 260/1000
2023-09-27 05:01:03.766 
Epoch 260/1000 
	 loss: 16.1344, MinusLogProbMetric: 16.1344, val_loss: 17.5250, val_MinusLogProbMetric: 17.5250

Epoch 260: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1344 - MinusLogProbMetric: 16.1344 - val_loss: 17.5250 - val_MinusLogProbMetric: 17.5250 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 261/1000
2023-09-27 05:01:38.148 
Epoch 261/1000 
	 loss: 16.1432, MinusLogProbMetric: 16.1432, val_loss: 17.7824, val_MinusLogProbMetric: 17.7824

Epoch 261: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1432 - MinusLogProbMetric: 16.1432 - val_loss: 17.7824 - val_MinusLogProbMetric: 17.7824 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 262/1000
2023-09-27 05:02:12.480 
Epoch 262/1000 
	 loss: 16.1627, MinusLogProbMetric: 16.1627, val_loss: 17.5690, val_MinusLogProbMetric: 17.5690

Epoch 262: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1627 - MinusLogProbMetric: 16.1627 - val_loss: 17.5690 - val_MinusLogProbMetric: 17.5690 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 263/1000
2023-09-27 05:02:46.813 
Epoch 263/1000 
	 loss: 16.1380, MinusLogProbMetric: 16.1380, val_loss: 17.7140, val_MinusLogProbMetric: 17.7140

Epoch 263: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1380 - MinusLogProbMetric: 16.1380 - val_loss: 17.7140 - val_MinusLogProbMetric: 17.7140 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 264/1000
2023-09-27 05:03:21.049 
Epoch 264/1000 
	 loss: 16.1449, MinusLogProbMetric: 16.1449, val_loss: 17.6258, val_MinusLogProbMetric: 17.6258

Epoch 264: val_loss did not improve from 17.37576
196/196 - 34s - loss: 16.1449 - MinusLogProbMetric: 16.1449 - val_loss: 17.6258 - val_MinusLogProbMetric: 17.6258 - lr: 5.0000e-04 - 34s/epoch - 175ms/step
Epoch 265/1000
2023-09-27 05:03:55.269 
Epoch 265/1000 
	 loss: 15.9903, MinusLogProbMetric: 15.9903, val_loss: 17.4720, val_MinusLogProbMetric: 17.4720

Epoch 265: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9903 - MinusLogProbMetric: 15.9903 - val_loss: 17.4720 - val_MinusLogProbMetric: 17.4720 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 266/1000
2023-09-27 05:04:29.511 
Epoch 266/1000 
	 loss: 15.9825, MinusLogProbMetric: 15.9825, val_loss: 17.5085, val_MinusLogProbMetric: 17.5085

Epoch 266: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9825 - MinusLogProbMetric: 15.9825 - val_loss: 17.5085 - val_MinusLogProbMetric: 17.5085 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 267/1000
2023-09-27 05:05:03.506 
Epoch 267/1000 
	 loss: 15.9888, MinusLogProbMetric: 15.9888, val_loss: 17.5256, val_MinusLogProbMetric: 17.5256

Epoch 267: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9888 - MinusLogProbMetric: 15.9888 - val_loss: 17.5256 - val_MinusLogProbMetric: 17.5256 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 268/1000
2023-09-27 05:05:37.992 
Epoch 268/1000 
	 loss: 15.9755, MinusLogProbMetric: 15.9755, val_loss: 17.4903, val_MinusLogProbMetric: 17.4903

Epoch 268: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9755 - MinusLogProbMetric: 15.9755 - val_loss: 17.4903 - val_MinusLogProbMetric: 17.4903 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 269/1000
2023-09-27 05:06:11.858 
Epoch 269/1000 
	 loss: 15.9815, MinusLogProbMetric: 15.9815, val_loss: 17.5002, val_MinusLogProbMetric: 17.5002

Epoch 269: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9815 - MinusLogProbMetric: 15.9815 - val_loss: 17.5002 - val_MinusLogProbMetric: 17.5002 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 270/1000
2023-09-27 05:06:46.174 
Epoch 270/1000 
	 loss: 15.9728, MinusLogProbMetric: 15.9728, val_loss: 17.6489, val_MinusLogProbMetric: 17.6489

Epoch 270: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9728 - MinusLogProbMetric: 15.9728 - val_loss: 17.6489 - val_MinusLogProbMetric: 17.6489 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 271/1000
2023-09-27 05:07:20.177 
Epoch 271/1000 
	 loss: 15.9862, MinusLogProbMetric: 15.9862, val_loss: 17.5386, val_MinusLogProbMetric: 17.5386

Epoch 271: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9862 - MinusLogProbMetric: 15.9862 - val_loss: 17.5386 - val_MinusLogProbMetric: 17.5386 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 272/1000
2023-09-27 05:07:54.610 
Epoch 272/1000 
	 loss: 15.9691, MinusLogProbMetric: 15.9691, val_loss: 17.4863, val_MinusLogProbMetric: 17.4863

Epoch 272: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9691 - MinusLogProbMetric: 15.9691 - val_loss: 17.4863 - val_MinusLogProbMetric: 17.4863 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 273/1000
2023-09-27 05:08:28.717 
Epoch 273/1000 
	 loss: 15.9635, MinusLogProbMetric: 15.9635, val_loss: 17.5209, val_MinusLogProbMetric: 17.5209

Epoch 273: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9635 - MinusLogProbMetric: 15.9635 - val_loss: 17.5209 - val_MinusLogProbMetric: 17.5209 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 274/1000
2023-09-27 05:09:02.850 
Epoch 274/1000 
	 loss: 15.9675, MinusLogProbMetric: 15.9675, val_loss: 17.5702, val_MinusLogProbMetric: 17.5702

Epoch 274: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9675 - MinusLogProbMetric: 15.9675 - val_loss: 17.5702 - val_MinusLogProbMetric: 17.5702 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 275/1000
2023-09-27 05:09:37.362 
Epoch 275/1000 
	 loss: 15.9787, MinusLogProbMetric: 15.9787, val_loss: 17.8956, val_MinusLogProbMetric: 17.8956

Epoch 275: val_loss did not improve from 17.37576
196/196 - 35s - loss: 15.9787 - MinusLogProbMetric: 15.9787 - val_loss: 17.8956 - val_MinusLogProbMetric: 17.8956 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 276/1000
2023-09-27 05:10:12.011 
Epoch 276/1000 
	 loss: 15.9664, MinusLogProbMetric: 15.9664, val_loss: 17.5152, val_MinusLogProbMetric: 17.5152

Epoch 276: val_loss did not improve from 17.37576
196/196 - 35s - loss: 15.9664 - MinusLogProbMetric: 15.9664 - val_loss: 17.5152 - val_MinusLogProbMetric: 17.5152 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 277/1000
2023-09-27 05:10:45.841 
Epoch 277/1000 
	 loss: 15.9750, MinusLogProbMetric: 15.9750, val_loss: 17.5030, val_MinusLogProbMetric: 17.5030

Epoch 277: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9750 - MinusLogProbMetric: 15.9750 - val_loss: 17.5030 - val_MinusLogProbMetric: 17.5030 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 278/1000
2023-09-27 05:11:20.124 
Epoch 278/1000 
	 loss: 15.9483, MinusLogProbMetric: 15.9483, val_loss: 17.5466, val_MinusLogProbMetric: 17.5466

Epoch 278: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9483 - MinusLogProbMetric: 15.9483 - val_loss: 17.5466 - val_MinusLogProbMetric: 17.5466 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 279/1000
2023-09-27 05:11:54.355 
Epoch 279/1000 
	 loss: 15.9661, MinusLogProbMetric: 15.9661, val_loss: 17.5148, val_MinusLogProbMetric: 17.5148

Epoch 279: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9661 - MinusLogProbMetric: 15.9661 - val_loss: 17.5148 - val_MinusLogProbMetric: 17.5148 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 280/1000
2023-09-27 05:12:28.247 
Epoch 280/1000 
	 loss: 15.9687, MinusLogProbMetric: 15.9687, val_loss: 17.5281, val_MinusLogProbMetric: 17.5281

Epoch 280: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9687 - MinusLogProbMetric: 15.9687 - val_loss: 17.5281 - val_MinusLogProbMetric: 17.5281 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 281/1000
2023-09-27 05:13:02.229 
Epoch 281/1000 
	 loss: 15.9495, MinusLogProbMetric: 15.9495, val_loss: 17.7154, val_MinusLogProbMetric: 17.7154

Epoch 281: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9495 - MinusLogProbMetric: 15.9495 - val_loss: 17.7154 - val_MinusLogProbMetric: 17.7154 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 282/1000
2023-09-27 05:13:36.116 
Epoch 282/1000 
	 loss: 15.9519, MinusLogProbMetric: 15.9519, val_loss: 17.6107, val_MinusLogProbMetric: 17.6107

Epoch 282: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9519 - MinusLogProbMetric: 15.9519 - val_loss: 17.6107 - val_MinusLogProbMetric: 17.6107 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 283/1000
2023-09-27 05:14:10.087 
Epoch 283/1000 
	 loss: 15.9742, MinusLogProbMetric: 15.9742, val_loss: 17.5347, val_MinusLogProbMetric: 17.5347

Epoch 283: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9742 - MinusLogProbMetric: 15.9742 - val_loss: 17.5347 - val_MinusLogProbMetric: 17.5347 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 284/1000
2023-09-27 05:14:44.076 
Epoch 284/1000 
	 loss: 15.9426, MinusLogProbMetric: 15.9426, val_loss: 17.5897, val_MinusLogProbMetric: 17.5897

Epoch 284: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9426 - MinusLogProbMetric: 15.9426 - val_loss: 17.5897 - val_MinusLogProbMetric: 17.5897 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 285/1000
2023-09-27 05:15:17.852 
Epoch 285/1000 
	 loss: 15.9557, MinusLogProbMetric: 15.9557, val_loss: 17.5333, val_MinusLogProbMetric: 17.5333

Epoch 285: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9557 - MinusLogProbMetric: 15.9557 - val_loss: 17.5333 - val_MinusLogProbMetric: 17.5333 - lr: 2.5000e-04 - 34s/epoch - 172ms/step
Epoch 286/1000
2023-09-27 05:15:51.798 
Epoch 286/1000 
	 loss: 15.9562, MinusLogProbMetric: 15.9562, val_loss: 17.6003, val_MinusLogProbMetric: 17.6003

Epoch 286: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9562 - MinusLogProbMetric: 15.9562 - val_loss: 17.6003 - val_MinusLogProbMetric: 17.6003 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 287/1000
2023-09-27 05:16:26.071 
Epoch 287/1000 
	 loss: 15.9554, MinusLogProbMetric: 15.9554, val_loss: 17.7618, val_MinusLogProbMetric: 17.7618

Epoch 287: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9554 - MinusLogProbMetric: 15.9554 - val_loss: 17.7618 - val_MinusLogProbMetric: 17.7618 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 288/1000
2023-09-27 05:17:00.155 
Epoch 288/1000 
	 loss: 15.9455, MinusLogProbMetric: 15.9455, val_loss: 17.4971, val_MinusLogProbMetric: 17.4971

Epoch 288: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9455 - MinusLogProbMetric: 15.9455 - val_loss: 17.4971 - val_MinusLogProbMetric: 17.4971 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 289/1000
2023-09-27 05:17:34.266 
Epoch 289/1000 
	 loss: 15.9317, MinusLogProbMetric: 15.9317, val_loss: 17.7344, val_MinusLogProbMetric: 17.7344

Epoch 289: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9317 - MinusLogProbMetric: 15.9317 - val_loss: 17.7344 - val_MinusLogProbMetric: 17.7344 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 290/1000
2023-09-27 05:18:08.614 
Epoch 290/1000 
	 loss: 15.9479, MinusLogProbMetric: 15.9479, val_loss: 17.6420, val_MinusLogProbMetric: 17.6420

Epoch 290: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9479 - MinusLogProbMetric: 15.9479 - val_loss: 17.6420 - val_MinusLogProbMetric: 17.6420 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 291/1000
2023-09-27 05:18:42.964 
Epoch 291/1000 
	 loss: 15.9508, MinusLogProbMetric: 15.9508, val_loss: 17.5126, val_MinusLogProbMetric: 17.5126

Epoch 291: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9508 - MinusLogProbMetric: 15.9508 - val_loss: 17.5126 - val_MinusLogProbMetric: 17.5126 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 292/1000
2023-09-27 05:19:16.934 
Epoch 292/1000 
	 loss: 15.9372, MinusLogProbMetric: 15.9372, val_loss: 17.5304, val_MinusLogProbMetric: 17.5304

Epoch 292: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9372 - MinusLogProbMetric: 15.9372 - val_loss: 17.5304 - val_MinusLogProbMetric: 17.5304 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 293/1000
2023-09-27 05:19:51.065 
Epoch 293/1000 
	 loss: 15.9430, MinusLogProbMetric: 15.9430, val_loss: 17.5123, val_MinusLogProbMetric: 17.5123

Epoch 293: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9430 - MinusLogProbMetric: 15.9430 - val_loss: 17.5123 - val_MinusLogProbMetric: 17.5123 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 294/1000
2023-09-27 05:20:24.919 
Epoch 294/1000 
	 loss: 15.9520, MinusLogProbMetric: 15.9520, val_loss: 17.6146, val_MinusLogProbMetric: 17.6146

Epoch 294: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9520 - MinusLogProbMetric: 15.9520 - val_loss: 17.6146 - val_MinusLogProbMetric: 17.6146 - lr: 2.5000e-04 - 34s/epoch - 173ms/step
Epoch 295/1000
2023-09-27 05:20:59.202 
Epoch 295/1000 
	 loss: 15.9301, MinusLogProbMetric: 15.9301, val_loss: 17.7327, val_MinusLogProbMetric: 17.7327

Epoch 295: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9301 - MinusLogProbMetric: 15.9301 - val_loss: 17.7327 - val_MinusLogProbMetric: 17.7327 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 296/1000
2023-09-27 05:21:31.007 
Epoch 296/1000 
	 loss: 15.9427, MinusLogProbMetric: 15.9427, val_loss: 17.8602, val_MinusLogProbMetric: 17.8602

Epoch 296: val_loss did not improve from 17.37576
196/196 - 32s - loss: 15.9427 - MinusLogProbMetric: 15.9427 - val_loss: 17.8602 - val_MinusLogProbMetric: 17.8602 - lr: 2.5000e-04 - 32s/epoch - 162ms/step
Epoch 297/1000
2023-09-27 05:22:02.939 
Epoch 297/1000 
	 loss: 15.9296, MinusLogProbMetric: 15.9296, val_loss: 17.5459, val_MinusLogProbMetric: 17.5459

Epoch 297: val_loss did not improve from 17.37576
196/196 - 32s - loss: 15.9296 - MinusLogProbMetric: 15.9296 - val_loss: 17.5459 - val_MinusLogProbMetric: 17.5459 - lr: 2.5000e-04 - 32s/epoch - 163ms/step
Epoch 298/1000
2023-09-27 05:22:34.447 
Epoch 298/1000 
	 loss: 15.9235, MinusLogProbMetric: 15.9235, val_loss: 17.5488, val_MinusLogProbMetric: 17.5488

Epoch 298: val_loss did not improve from 17.37576
196/196 - 32s - loss: 15.9235 - MinusLogProbMetric: 15.9235 - val_loss: 17.5488 - val_MinusLogProbMetric: 17.5488 - lr: 2.5000e-04 - 32s/epoch - 161ms/step
Epoch 299/1000
2023-09-27 05:23:03.064 
Epoch 299/1000 
	 loss: 15.9371, MinusLogProbMetric: 15.9371, val_loss: 17.5162, val_MinusLogProbMetric: 17.5162

Epoch 299: val_loss did not improve from 17.37576
196/196 - 29s - loss: 15.9371 - MinusLogProbMetric: 15.9371 - val_loss: 17.5162 - val_MinusLogProbMetric: 17.5162 - lr: 2.5000e-04 - 29s/epoch - 146ms/step
Epoch 300/1000
2023-09-27 05:23:35.243 
Epoch 300/1000 
	 loss: 15.9384, MinusLogProbMetric: 15.9384, val_loss: 17.6009, val_MinusLogProbMetric: 17.6009

Epoch 300: val_loss did not improve from 17.37576
196/196 - 32s - loss: 15.9384 - MinusLogProbMetric: 15.9384 - val_loss: 17.6009 - val_MinusLogProbMetric: 17.6009 - lr: 2.5000e-04 - 32s/epoch - 164ms/step
Epoch 301/1000
2023-09-27 05:24:09.708 
Epoch 301/1000 
	 loss: 15.9351, MinusLogProbMetric: 15.9351, val_loss: 17.5561, val_MinusLogProbMetric: 17.5561

Epoch 301: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9351 - MinusLogProbMetric: 15.9351 - val_loss: 17.5561 - val_MinusLogProbMetric: 17.5561 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 302/1000
2023-09-27 05:24:44.121 
Epoch 302/1000 
	 loss: 15.9170, MinusLogProbMetric: 15.9170, val_loss: 17.6334, val_MinusLogProbMetric: 17.6334

Epoch 302: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9170 - MinusLogProbMetric: 15.9170 - val_loss: 17.6334 - val_MinusLogProbMetric: 17.6334 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 303/1000
2023-09-27 05:25:18.675 
Epoch 303/1000 
	 loss: 15.9309, MinusLogProbMetric: 15.9309, val_loss: 17.5548, val_MinusLogProbMetric: 17.5548

Epoch 303: val_loss did not improve from 17.37576
196/196 - 35s - loss: 15.9309 - MinusLogProbMetric: 15.9309 - val_loss: 17.5548 - val_MinusLogProbMetric: 17.5548 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 304/1000
2023-09-27 05:25:53.318 
Epoch 304/1000 
	 loss: 15.9347, MinusLogProbMetric: 15.9347, val_loss: 17.5689, val_MinusLogProbMetric: 17.5689

Epoch 304: val_loss did not improve from 17.37576
196/196 - 35s - loss: 15.9347 - MinusLogProbMetric: 15.9347 - val_loss: 17.5689 - val_MinusLogProbMetric: 17.5689 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 305/1000
2023-09-27 05:26:27.752 
Epoch 305/1000 
	 loss: 15.9117, MinusLogProbMetric: 15.9117, val_loss: 17.6311, val_MinusLogProbMetric: 17.6311

Epoch 305: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9117 - MinusLogProbMetric: 15.9117 - val_loss: 17.6311 - val_MinusLogProbMetric: 17.6311 - lr: 2.5000e-04 - 34s/epoch - 176ms/step
Epoch 306/1000
2023-09-27 05:27:02.284 
Epoch 306/1000 
	 loss: 15.9234, MinusLogProbMetric: 15.9234, val_loss: 17.5528, val_MinusLogProbMetric: 17.5528

Epoch 306: val_loss did not improve from 17.37576
196/196 - 35s - loss: 15.9234 - MinusLogProbMetric: 15.9234 - val_loss: 17.5528 - val_MinusLogProbMetric: 17.5528 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 307/1000
2023-09-27 05:27:36.895 
Epoch 307/1000 
	 loss: 15.9411, MinusLogProbMetric: 15.9411, val_loss: 17.5358, val_MinusLogProbMetric: 17.5358

Epoch 307: val_loss did not improve from 17.37576
196/196 - 35s - loss: 15.9411 - MinusLogProbMetric: 15.9411 - val_loss: 17.5358 - val_MinusLogProbMetric: 17.5358 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 308/1000
2023-09-27 05:28:11.098 
Epoch 308/1000 
	 loss: 15.9125, MinusLogProbMetric: 15.9125, val_loss: 17.5788, val_MinusLogProbMetric: 17.5788

Epoch 308: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9125 - MinusLogProbMetric: 15.9125 - val_loss: 17.5788 - val_MinusLogProbMetric: 17.5788 - lr: 2.5000e-04 - 34s/epoch - 174ms/step
Epoch 309/1000
2023-09-27 05:28:45.766 
Epoch 309/1000 
	 loss: 15.9652, MinusLogProbMetric: 15.9652, val_loss: 17.6329, val_MinusLogProbMetric: 17.6329

Epoch 309: val_loss did not improve from 17.37576
196/196 - 35s - loss: 15.9652 - MinusLogProbMetric: 15.9652 - val_loss: 17.6329 - val_MinusLogProbMetric: 17.6329 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 310/1000
2023-09-27 05:29:20.297 
Epoch 310/1000 
	 loss: 15.9163, MinusLogProbMetric: 15.9163, val_loss: 17.5778, val_MinusLogProbMetric: 17.5778

Epoch 310: val_loss did not improve from 17.37576
196/196 - 35s - loss: 15.9163 - MinusLogProbMetric: 15.9163 - val_loss: 17.5778 - val_MinusLogProbMetric: 17.5778 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 311/1000
2023-09-27 05:29:54.671 
Epoch 311/1000 
	 loss: 15.9184, MinusLogProbMetric: 15.9184, val_loss: 17.6950, val_MinusLogProbMetric: 17.6950

Epoch 311: val_loss did not improve from 17.37576
196/196 - 34s - loss: 15.9184 - MinusLogProbMetric: 15.9184 - val_loss: 17.6950 - val_MinusLogProbMetric: 17.6950 - lr: 2.5000e-04 - 34s/epoch - 175ms/step
Epoch 312/1000
2023-09-27 05:30:29.178 
Epoch 312/1000 
	 loss: 15.9139, MinusLogProbMetric: 15.9139, val_loss: 17.5721, val_MinusLogProbMetric: 17.5721

Epoch 312: val_loss did not improve from 17.37576
196/196 - 35s - loss: 15.9139 - MinusLogProbMetric: 15.9139 - val_loss: 17.5721 - val_MinusLogProbMetric: 17.5721 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 313/1000
2023-09-27 05:31:03.722 
Epoch 313/1000 
	 loss: 15.9135, MinusLogProbMetric: 15.9135, val_loss: 17.5832, val_MinusLogProbMetric: 17.5832

Epoch 313: val_loss did not improve from 17.37576
196/196 - 35s - loss: 15.9135 - MinusLogProbMetric: 15.9135 - val_loss: 17.5832 - val_MinusLogProbMetric: 17.5832 - lr: 2.5000e-04 - 35s/epoch - 176ms/step
Epoch 314/1000
2023-09-27 05:31:38.009 
Epoch 314/1000 
	 loss: 15.9214, MinusLogProbMetric: 15.9214, val_loss: 17.6393, val_MinusLogProbMetric: 17.6393

Epoch 314: val_loss did not improve from 17.37576
Restoring model weights from the end of the best epoch: 214.
196/196 - 35s - loss: 15.9214 - MinusLogProbMetric: 15.9214 - val_loss: 17.6393 - val_MinusLogProbMetric: 17.6393 - lr: 2.5000e-04 - 35s/epoch - 177ms/step
Epoch 314: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 14.092889361956622 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 8.665382167964708 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 5.716826838965062 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 7.175608300953172 seconds.
Training succeeded with seed 869.
Model trained in 10780.87 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 36.81 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 37.15 s.
===========
Run 298/720 done in 10822.42 s.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

===========
Generating train data for run 301.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_301/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_301/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_301/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_301
self.data_kwargs: {'seed': 869}
self.x_data: [[ 1.7417877   3.5044646   9.194398   ...  7.377848    2.8874114
   1.7757134 ]
 [ 3.8693519   4.134066    8.323213   ...  7.5259514   3.079576
   1.8540689 ]
 [ 2.7888105   3.5250566   6.8470335  ...  7.0512147   2.9893377
   1.7142482 ]
 ...
 [ 3.8914979   5.948059   -0.25578696 ...  1.7629418   6.8860593
   1.464444  ]
 [ 3.0775785   3.9001913   8.887795   ...  7.2041593   3.5200696
   1.5176823 ]
 [ 1.0419946   3.6002321   7.1163735  ...  7.3799496   3.218665
   1.6207042 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_49"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_50 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f9590b32230>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f95dd5a9bd0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f95dd5a9bd0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8f2898b6d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8f289db8b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8f289dbe20>, <keras.callbacks.ModelCheckpoint object at 0x7f8f289dbee0>, <keras.callbacks.EarlyStopping object at 0x7f8f289dbfa0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8f289dbdc0>, <keras.callbacks.TerminateOnNaN object at 0x7f8f289dbdf0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_301/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 301/720 with hyperparameters:
timestamp = 2023-09-27 05:32:24.458320
ndims = 32
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 1.7417877   3.5044646   9.194398    1.0621872   8.910313    1.2448134
  9.752992    4.795541   10.214517    5.8200784   7.3571367   0.42017213
  3.0764782   1.1773593   3.1572356   1.3086243   2.6951017   5.712275
  0.4469142   6.9349284   5.5393224   1.8138231   6.1143165   1.1455625
  7.183009    9.098161    3.4021838   5.9704523   0.79392874  7.377848
  2.8874114   1.7757134 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 49: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-27 05:34:50.163 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1746.9250, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 145s - loss: nan - MinusLogProbMetric: 1746.9250 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 145s/epoch - 742ms/step
The loss history contains NaN values.
Training failed: trying again with seed 235536 and lr 0.0003333333333333333.
===========
Generating train data for run 301.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_301/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 869}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_301/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_301/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_301
self.data_kwargs: {'seed': 869}
self.x_data: [[ 1.7417877   3.5044646   9.194398   ...  7.377848    2.8874114
   1.7757134 ]
 [ 3.8693519   4.134066    8.323213   ...  7.5259514   3.079576
   1.8540689 ]
 [ 2.7888105   3.5250566   6.8470335  ...  7.0512147   2.9893377
   1.7142482 ]
 ...
 [ 3.8914979   5.948059   -0.25578696 ...  1.7629418   6.8860593
   1.464444  ]
 [ 3.0775785   3.9001913   8.887795   ...  7.2041593   3.5200696
   1.5176823 ]
 [ 1.0419946   3.6002321   7.1163735  ...  7.3799496   3.218665
   1.6207042 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_60"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_61 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  826720    
 yer)                                                            
                                                                 
=================================================================
Total params: 826,720
Trainable params: 826,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f9620b6faf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8fc434f820>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8fc434f820>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f90faf91330>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f90403a5660>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f90403a54b0>, <keras.callbacks.ModelCheckpoint object at 0x7f90403a4e80>, <keras.callbacks.EarlyStopping object at 0x7f90403a47c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f90403a4a00>, <keras.callbacks.TerminateOnNaN object at 0x7f90403a50f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.7724807 ,  3.5952213 ,  8.442487  , ...,  7.3644695 ,
         3.6588435 ,  2.0248146 ],
       [ 6.6242166 ,  7.1665483 ,  6.4277024 , ...,  3.1194153 ,
         2.6248946 ,  8.037665  ],
       [ 4.5211086 ,  5.2663136 , -0.6501303 , ...,  0.17722613,
         6.462096  ,  1.3170475 ],
       ...,
       [ 5.2085795 ,  5.348097  , -0.4658583 , ..., -0.03725696,
         5.9864964 ,  1.2388526 ],
       [ 3.9188156 ,  5.4859467 , -0.29968688, ...,  0.4149692 ,
         6.0867124 ,  1.4174792 ],
       [ 2.283803  ,  2.9701724 ,  8.729265  , ...,  6.0229187 ,
         3.2842712 ,  2.1182036 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_301/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 301/720 with hyperparameters:
timestamp = 2023-09-27 05:35:00.216339
ndims = 32
seed_train = 869
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 826720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 1.7417877   3.5044646   9.194398    1.0621872   8.910313    1.2448134
  9.752992    4.795541   10.214517    5.8200784   7.3571367   0.42017213
  3.0764782   1.1773593   3.1572356   1.3086243   2.6951017   5.712275
  0.4469142   6.9349284   5.5393224   1.8138231   6.1143165   1.1455625
  7.183009    9.098161    3.4021838   5.9704523   0.79392874  7.377848
  2.8874114   1.7757134 ]
Epoch 1/1000
2023-09-27 05:38:08.338 
Epoch 1/1000 
	 loss: 1125.6693, MinusLogProbMetric: 1125.6693, val_loss: 503.5368, val_MinusLogProbMetric: 503.5368

Epoch 1: val_loss improved from inf to 503.53677, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 189s - loss: 1125.6693 - MinusLogProbMetric: 1125.6693 - val_loss: 503.5368 - val_MinusLogProbMetric: 503.5368 - lr: 3.3333e-04 - 189s/epoch - 963ms/step
Epoch 2/1000
2023-09-27 05:39:13.902 
Epoch 2/1000 
	 loss: 274.9879, MinusLogProbMetric: 274.9879, val_loss: 192.1048, val_MinusLogProbMetric: 192.1048

Epoch 2: val_loss improved from 503.53677 to 192.10477, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 274.9879 - MinusLogProbMetric: 274.9879 - val_loss: 192.1048 - val_MinusLogProbMetric: 192.1048 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 3/1000
2023-09-27 05:40:19.682 
Epoch 3/1000 
	 loss: 169.1005, MinusLogProbMetric: 169.1005, val_loss: 149.1309, val_MinusLogProbMetric: 149.1309

Epoch 3: val_loss improved from 192.10477 to 149.13092, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 169.1005 - MinusLogProbMetric: 169.1005 - val_loss: 149.1309 - val_MinusLogProbMetric: 149.1309 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 4/1000
2023-09-27 05:41:25.220 
Epoch 4/1000 
	 loss: 130.7531, MinusLogProbMetric: 130.7531, val_loss: 118.0061, val_MinusLogProbMetric: 118.0061

Epoch 4: val_loss improved from 149.13092 to 118.00614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 130.7531 - MinusLogProbMetric: 130.7531 - val_loss: 118.0061 - val_MinusLogProbMetric: 118.0061 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 5/1000
2023-09-27 05:42:30.781 
Epoch 5/1000 
	 loss: 105.3692, MinusLogProbMetric: 105.3692, val_loss: 97.6638, val_MinusLogProbMetric: 97.6638

Epoch 5: val_loss improved from 118.00614 to 97.66376, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 105.3692 - MinusLogProbMetric: 105.3692 - val_loss: 97.6638 - val_MinusLogProbMetric: 97.6638 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 6/1000
2023-09-27 05:43:36.716 
Epoch 6/1000 
	 loss: 101.8108, MinusLogProbMetric: 101.8108, val_loss: 95.0434, val_MinusLogProbMetric: 95.0434

Epoch 6: val_loss improved from 97.66376 to 95.04340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 101.8108 - MinusLogProbMetric: 101.8108 - val_loss: 95.0434 - val_MinusLogProbMetric: 95.0434 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 7/1000
2023-09-27 05:44:42.074 
Epoch 7/1000 
	 loss: 86.2674, MinusLogProbMetric: 86.2674, val_loss: 77.7928, val_MinusLogProbMetric: 77.7928

Epoch 7: val_loss improved from 95.04340 to 77.79276, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 86.2674 - MinusLogProbMetric: 86.2674 - val_loss: 77.7928 - val_MinusLogProbMetric: 77.7928 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 8/1000
2023-09-27 05:45:47.381 
Epoch 8/1000 
	 loss: 69.8232, MinusLogProbMetric: 69.8232, val_loss: 75.9834, val_MinusLogProbMetric: 75.9834

Epoch 8: val_loss improved from 77.79276 to 75.98335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 69.8232 - MinusLogProbMetric: 69.8232 - val_loss: 75.9834 - val_MinusLogProbMetric: 75.9834 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 9/1000
2023-09-27 05:46:52.458 
Epoch 9/1000 
	 loss: 64.6783, MinusLogProbMetric: 64.6783, val_loss: 60.9146, val_MinusLogProbMetric: 60.9146

Epoch 9: val_loss improved from 75.98335 to 60.91465, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 64.6783 - MinusLogProbMetric: 64.6783 - val_loss: 60.9146 - val_MinusLogProbMetric: 60.9146 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 10/1000
2023-09-27 05:47:57.992 
Epoch 10/1000 
	 loss: 60.7543, MinusLogProbMetric: 60.7543, val_loss: 56.3462, val_MinusLogProbMetric: 56.3462

Epoch 10: val_loss improved from 60.91465 to 56.34619, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 60.7543 - MinusLogProbMetric: 60.7543 - val_loss: 56.3462 - val_MinusLogProbMetric: 56.3462 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 11/1000
2023-09-27 05:49:03.358 
Epoch 11/1000 
	 loss: 62.2418, MinusLogProbMetric: 62.2418, val_loss: 68.5783, val_MinusLogProbMetric: 68.5783

Epoch 11: val_loss did not improve from 56.34619
196/196 - 64s - loss: 62.2418 - MinusLogProbMetric: 62.2418 - val_loss: 68.5783 - val_MinusLogProbMetric: 68.5783 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 12/1000
2023-09-27 05:50:07.840 
Epoch 12/1000 
	 loss: 53.5632, MinusLogProbMetric: 53.5632, val_loss: 49.8849, val_MinusLogProbMetric: 49.8849

Epoch 12: val_loss improved from 56.34619 to 49.88486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 53.5632 - MinusLogProbMetric: 53.5632 - val_loss: 49.8849 - val_MinusLogProbMetric: 49.8849 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 13/1000
2023-09-27 05:51:12.956 
Epoch 13/1000 
	 loss: 48.2789, MinusLogProbMetric: 48.2789, val_loss: 48.7085, val_MinusLogProbMetric: 48.7085

Epoch 13: val_loss improved from 49.88486 to 48.70853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 48.2789 - MinusLogProbMetric: 48.2789 - val_loss: 48.7085 - val_MinusLogProbMetric: 48.7085 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 14/1000
2023-09-27 05:52:18.106 
Epoch 14/1000 
	 loss: 46.4727, MinusLogProbMetric: 46.4727, val_loss: 45.5866, val_MinusLogProbMetric: 45.5866

Epoch 14: val_loss improved from 48.70853 to 45.58661, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 46.4727 - MinusLogProbMetric: 46.4727 - val_loss: 45.5866 - val_MinusLogProbMetric: 45.5866 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 15/1000
2023-09-27 05:53:23.304 
Epoch 15/1000 
	 loss: 43.7776, MinusLogProbMetric: 43.7776, val_loss: 43.0236, val_MinusLogProbMetric: 43.0236

Epoch 15: val_loss improved from 45.58661 to 43.02357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 43.7776 - MinusLogProbMetric: 43.7776 - val_loss: 43.0236 - val_MinusLogProbMetric: 43.0236 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 16/1000
2023-09-27 05:54:29.005 
Epoch 16/1000 
	 loss: 42.5122, MinusLogProbMetric: 42.5122, val_loss: 41.4752, val_MinusLogProbMetric: 41.4752

Epoch 16: val_loss improved from 43.02357 to 41.47517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 42.5122 - MinusLogProbMetric: 42.5122 - val_loss: 41.4752 - val_MinusLogProbMetric: 41.4752 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 17/1000
2023-09-27 05:55:35.037 
Epoch 17/1000 
	 loss: 40.8288, MinusLogProbMetric: 40.8288, val_loss: 40.1229, val_MinusLogProbMetric: 40.1229

Epoch 17: val_loss improved from 41.47517 to 40.12288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 40.8288 - MinusLogProbMetric: 40.8288 - val_loss: 40.1229 - val_MinusLogProbMetric: 40.1229 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 18/1000
2023-09-27 05:56:40.550 
Epoch 18/1000 
	 loss: 39.5455, MinusLogProbMetric: 39.5455, val_loss: 39.1299, val_MinusLogProbMetric: 39.1299

Epoch 18: val_loss improved from 40.12288 to 39.12992, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 39.5455 - MinusLogProbMetric: 39.5455 - val_loss: 39.1299 - val_MinusLogProbMetric: 39.1299 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 19/1000
2023-09-27 05:57:46.235 
Epoch 19/1000 
	 loss: 38.3549, MinusLogProbMetric: 38.3549, val_loss: 36.6419, val_MinusLogProbMetric: 36.6419

Epoch 19: val_loss improved from 39.12992 to 36.64190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 38.3549 - MinusLogProbMetric: 38.3549 - val_loss: 36.6419 - val_MinusLogProbMetric: 36.6419 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 20/1000
2023-09-27 05:58:51.033 
Epoch 20/1000 
	 loss: 37.6166, MinusLogProbMetric: 37.6166, val_loss: 35.5161, val_MinusLogProbMetric: 35.5161

Epoch 20: val_loss improved from 36.64190 to 35.51612, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 37.6166 - MinusLogProbMetric: 37.6166 - val_loss: 35.5161 - val_MinusLogProbMetric: 35.5161 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 21/1000
2023-09-27 05:59:56.308 
Epoch 21/1000 
	 loss: 35.3657, MinusLogProbMetric: 35.3657, val_loss: 37.8907, val_MinusLogProbMetric: 37.8907

Epoch 21: val_loss did not improve from 35.51612
196/196 - 64s - loss: 35.3657 - MinusLogProbMetric: 35.3657 - val_loss: 37.8907 - val_MinusLogProbMetric: 37.8907 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 22/1000
2023-09-27 06:01:00.323 
Epoch 22/1000 
	 loss: 34.8716, MinusLogProbMetric: 34.8716, val_loss: 38.2448, val_MinusLogProbMetric: 38.2448

Epoch 22: val_loss did not improve from 35.51612
196/196 - 64s - loss: 34.8716 - MinusLogProbMetric: 34.8716 - val_loss: 38.2448 - val_MinusLogProbMetric: 38.2448 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 23/1000
2023-09-27 06:02:04.770 
Epoch 23/1000 
	 loss: 34.7379, MinusLogProbMetric: 34.7379, val_loss: 33.0554, val_MinusLogProbMetric: 33.0554

Epoch 23: val_loss improved from 35.51612 to 33.05536, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 34.7379 - MinusLogProbMetric: 34.7379 - val_loss: 33.0554 - val_MinusLogProbMetric: 33.0554 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 24/1000
2023-09-27 06:03:10.664 
Epoch 24/1000 
	 loss: 33.0556, MinusLogProbMetric: 33.0556, val_loss: 33.2272, val_MinusLogProbMetric: 33.2272

Epoch 24: val_loss did not improve from 33.05536
196/196 - 65s - loss: 33.0556 - MinusLogProbMetric: 33.0556 - val_loss: 33.2272 - val_MinusLogProbMetric: 33.2272 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 25/1000
2023-09-27 06:04:15.490 
Epoch 25/1000 
	 loss: 32.3897, MinusLogProbMetric: 32.3897, val_loss: 31.5933, val_MinusLogProbMetric: 31.5933

Epoch 25: val_loss improved from 33.05536 to 31.59328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 32.3897 - MinusLogProbMetric: 32.3897 - val_loss: 31.5933 - val_MinusLogProbMetric: 31.5933 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 26/1000
2023-09-27 06:05:20.750 
Epoch 26/1000 
	 loss: 32.1107, MinusLogProbMetric: 32.1107, val_loss: 32.7263, val_MinusLogProbMetric: 32.7263

Epoch 26: val_loss did not improve from 31.59328
196/196 - 64s - loss: 32.1107 - MinusLogProbMetric: 32.1107 - val_loss: 32.7263 - val_MinusLogProbMetric: 32.7263 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 27/1000
2023-09-27 06:06:25.117 
Epoch 27/1000 
	 loss: 31.0103, MinusLogProbMetric: 31.0103, val_loss: 31.6107, val_MinusLogProbMetric: 31.6107

Epoch 27: val_loss did not improve from 31.59328
196/196 - 64s - loss: 31.0103 - MinusLogProbMetric: 31.0103 - val_loss: 31.6107 - val_MinusLogProbMetric: 31.6107 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 28/1000
2023-09-27 06:07:29.448 
Epoch 28/1000 
	 loss: 30.7744, MinusLogProbMetric: 30.7744, val_loss: 29.9663, val_MinusLogProbMetric: 29.9663

Epoch 28: val_loss improved from 31.59328 to 29.96626, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 30.7744 - MinusLogProbMetric: 30.7744 - val_loss: 29.9663 - val_MinusLogProbMetric: 29.9663 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 29/1000
2023-09-27 06:08:35.208 
Epoch 29/1000 
	 loss: 30.3224, MinusLogProbMetric: 30.3224, val_loss: 29.9395, val_MinusLogProbMetric: 29.9395

Epoch 29: val_loss improved from 29.96626 to 29.93951, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 30.3224 - MinusLogProbMetric: 30.3224 - val_loss: 29.9395 - val_MinusLogProbMetric: 29.9395 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 30/1000
2023-09-27 06:09:40.518 
Epoch 30/1000 
	 loss: 29.7356, MinusLogProbMetric: 29.7356, val_loss: 29.9640, val_MinusLogProbMetric: 29.9640

Epoch 30: val_loss did not improve from 29.93951
196/196 - 64s - loss: 29.7356 - MinusLogProbMetric: 29.7356 - val_loss: 29.9640 - val_MinusLogProbMetric: 29.9640 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 31/1000
2023-09-27 06:10:45.023 
Epoch 31/1000 
	 loss: 29.2288, MinusLogProbMetric: 29.2288, val_loss: 29.4930, val_MinusLogProbMetric: 29.4930

Epoch 31: val_loss improved from 29.93951 to 29.49304, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 29.2288 - MinusLogProbMetric: 29.2288 - val_loss: 29.4930 - val_MinusLogProbMetric: 29.4930 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 32/1000
2023-09-27 06:11:50.184 
Epoch 32/1000 
	 loss: 29.2473, MinusLogProbMetric: 29.2473, val_loss: 29.2572, val_MinusLogProbMetric: 29.2572

Epoch 32: val_loss improved from 29.49304 to 29.25722, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 29.2473 - MinusLogProbMetric: 29.2473 - val_loss: 29.2572 - val_MinusLogProbMetric: 29.2572 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 33/1000
2023-09-27 06:12:55.043 
Epoch 33/1000 
	 loss: 28.6490, MinusLogProbMetric: 28.6490, val_loss: 28.2336, val_MinusLogProbMetric: 28.2336

Epoch 33: val_loss improved from 29.25722 to 28.23361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 28.6490 - MinusLogProbMetric: 28.6490 - val_loss: 28.2336 - val_MinusLogProbMetric: 28.2336 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 34/1000
2023-09-27 06:14:00.682 
Epoch 34/1000 
	 loss: 28.3080, MinusLogProbMetric: 28.3080, val_loss: 27.8182, val_MinusLogProbMetric: 27.8182

Epoch 34: val_loss improved from 28.23361 to 27.81819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 28.3080 - MinusLogProbMetric: 28.3080 - val_loss: 27.8182 - val_MinusLogProbMetric: 27.8182 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 35/1000
2023-09-27 06:15:06.320 
Epoch 35/1000 
	 loss: 27.8701, MinusLogProbMetric: 27.8701, val_loss: 28.2338, val_MinusLogProbMetric: 28.2338

Epoch 35: val_loss did not improve from 27.81819
196/196 - 65s - loss: 27.8701 - MinusLogProbMetric: 27.8701 - val_loss: 28.2338 - val_MinusLogProbMetric: 28.2338 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 36/1000
2023-09-27 06:16:10.666 
Epoch 36/1000 
	 loss: 27.6002, MinusLogProbMetric: 27.6002, val_loss: 28.2742, val_MinusLogProbMetric: 28.2742

Epoch 36: val_loss did not improve from 27.81819
196/196 - 64s - loss: 27.6002 - MinusLogProbMetric: 27.6002 - val_loss: 28.2742 - val_MinusLogProbMetric: 28.2742 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 37/1000
2023-09-27 06:17:14.816 
Epoch 37/1000 
	 loss: 27.5064, MinusLogProbMetric: 27.5064, val_loss: 28.1915, val_MinusLogProbMetric: 28.1915

Epoch 37: val_loss did not improve from 27.81819
196/196 - 64s - loss: 27.5064 - MinusLogProbMetric: 27.5064 - val_loss: 28.1915 - val_MinusLogProbMetric: 28.1915 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 38/1000
2023-09-27 06:18:19.429 
Epoch 38/1000 
	 loss: 27.0576, MinusLogProbMetric: 27.0576, val_loss: 26.8513, val_MinusLogProbMetric: 26.8513

Epoch 38: val_loss improved from 27.81819 to 26.85128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 27.0576 - MinusLogProbMetric: 27.0576 - val_loss: 26.8513 - val_MinusLogProbMetric: 26.8513 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 39/1000
2023-09-27 06:19:25.024 
Epoch 39/1000 
	 loss: 26.8841, MinusLogProbMetric: 26.8841, val_loss: 27.4073, val_MinusLogProbMetric: 27.4073

Epoch 39: val_loss did not improve from 26.85128
196/196 - 65s - loss: 26.8841 - MinusLogProbMetric: 26.8841 - val_loss: 27.4073 - val_MinusLogProbMetric: 27.4073 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 40/1000
2023-09-27 06:20:29.553 
Epoch 40/1000 
	 loss: 26.7822, MinusLogProbMetric: 26.7822, val_loss: 26.3894, val_MinusLogProbMetric: 26.3894

Epoch 40: val_loss improved from 26.85128 to 26.38942, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 26.7822 - MinusLogProbMetric: 26.7822 - val_loss: 26.3894 - val_MinusLogProbMetric: 26.3894 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 41/1000
2023-09-27 06:21:34.498 
Epoch 41/1000 
	 loss: 26.3444, MinusLogProbMetric: 26.3444, val_loss: 26.8781, val_MinusLogProbMetric: 26.8781

Epoch 41: val_loss did not improve from 26.38942
196/196 - 64s - loss: 26.3444 - MinusLogProbMetric: 26.3444 - val_loss: 26.8781 - val_MinusLogProbMetric: 26.8781 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 42/1000
2023-09-27 06:22:39.471 
Epoch 42/1000 
	 loss: 26.1874, MinusLogProbMetric: 26.1874, val_loss: 26.1428, val_MinusLogProbMetric: 26.1428

Epoch 42: val_loss improved from 26.38942 to 26.14277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 26.1874 - MinusLogProbMetric: 26.1874 - val_loss: 26.1428 - val_MinusLogProbMetric: 26.1428 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 43/1000
2023-09-27 06:23:44.701 
Epoch 43/1000 
	 loss: 26.1491, MinusLogProbMetric: 26.1491, val_loss: 26.8117, val_MinusLogProbMetric: 26.8117

Epoch 43: val_loss did not improve from 26.14277
196/196 - 64s - loss: 26.1491 - MinusLogProbMetric: 26.1491 - val_loss: 26.8117 - val_MinusLogProbMetric: 26.8117 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 44/1000
2023-09-27 06:24:49.091 
Epoch 44/1000 
	 loss: 25.8313, MinusLogProbMetric: 25.8313, val_loss: 26.7683, val_MinusLogProbMetric: 26.7683

Epoch 44: val_loss did not improve from 26.14277
196/196 - 64s - loss: 25.8313 - MinusLogProbMetric: 25.8313 - val_loss: 26.7683 - val_MinusLogProbMetric: 26.7683 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 45/1000
2023-09-27 06:25:53.527 
Epoch 45/1000 
	 loss: 25.5080, MinusLogProbMetric: 25.5080, val_loss: 25.1103, val_MinusLogProbMetric: 25.1103

Epoch 45: val_loss improved from 26.14277 to 25.11026, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 25.5080 - MinusLogProbMetric: 25.5080 - val_loss: 25.1103 - val_MinusLogProbMetric: 25.1103 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 46/1000
2023-09-27 06:26:59.102 
Epoch 46/1000 
	 loss: 25.1436, MinusLogProbMetric: 25.1436, val_loss: 24.8505, val_MinusLogProbMetric: 24.8505

Epoch 46: val_loss improved from 25.11026 to 24.85054, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 25.1436 - MinusLogProbMetric: 25.1436 - val_loss: 24.8505 - val_MinusLogProbMetric: 24.8505 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 47/1000
2023-09-27 06:28:04.451 
Epoch 47/1000 
	 loss: 25.0706, MinusLogProbMetric: 25.0706, val_loss: 24.9241, val_MinusLogProbMetric: 24.9241

Epoch 47: val_loss did not improve from 24.85054
196/196 - 64s - loss: 25.0706 - MinusLogProbMetric: 25.0706 - val_loss: 24.9241 - val_MinusLogProbMetric: 24.9241 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 48/1000
2023-09-27 06:29:08.828 
Epoch 48/1000 
	 loss: 24.8006, MinusLogProbMetric: 24.8006, val_loss: 25.1483, val_MinusLogProbMetric: 25.1483

Epoch 48: val_loss did not improve from 24.85054
196/196 - 64s - loss: 24.8006 - MinusLogProbMetric: 24.8006 - val_loss: 25.1483 - val_MinusLogProbMetric: 25.1483 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 49/1000
2023-09-27 06:30:13.571 
Epoch 49/1000 
	 loss: 24.6655, MinusLogProbMetric: 24.6655, val_loss: 24.6391, val_MinusLogProbMetric: 24.6391

Epoch 49: val_loss improved from 24.85054 to 24.63911, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 24.6655 - MinusLogProbMetric: 24.6655 - val_loss: 24.6391 - val_MinusLogProbMetric: 24.6391 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 50/1000
2023-09-27 06:31:19.283 
Epoch 50/1000 
	 loss: 24.4924, MinusLogProbMetric: 24.4924, val_loss: 24.5376, val_MinusLogProbMetric: 24.5376

Epoch 50: val_loss improved from 24.63911 to 24.53762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 24.4924 - MinusLogProbMetric: 24.4924 - val_loss: 24.5376 - val_MinusLogProbMetric: 24.5376 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 51/1000
2023-09-27 06:32:24.076 
Epoch 51/1000 
	 loss: 24.3420, MinusLogProbMetric: 24.3420, val_loss: 24.4784, val_MinusLogProbMetric: 24.4784

Epoch 51: val_loss improved from 24.53762 to 24.47839, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 24.3420 - MinusLogProbMetric: 24.3420 - val_loss: 24.4784 - val_MinusLogProbMetric: 24.4784 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 52/1000
2023-09-27 06:33:29.418 
Epoch 52/1000 
	 loss: 24.1631, MinusLogProbMetric: 24.1631, val_loss: 24.6977, val_MinusLogProbMetric: 24.6977

Epoch 52: val_loss did not improve from 24.47839
196/196 - 64s - loss: 24.1631 - MinusLogProbMetric: 24.1631 - val_loss: 24.6977 - val_MinusLogProbMetric: 24.6977 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 53/1000
2023-09-27 06:34:33.705 
Epoch 53/1000 
	 loss: 24.0465, MinusLogProbMetric: 24.0465, val_loss: 24.0631, val_MinusLogProbMetric: 24.0631

Epoch 53: val_loss improved from 24.47839 to 24.06313, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 24.0465 - MinusLogProbMetric: 24.0465 - val_loss: 24.0631 - val_MinusLogProbMetric: 24.0631 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 54/1000
2023-09-27 06:35:38.673 
Epoch 54/1000 
	 loss: 24.1520, MinusLogProbMetric: 24.1520, val_loss: 27.2382, val_MinusLogProbMetric: 27.2382

Epoch 54: val_loss did not improve from 24.06313
196/196 - 64s - loss: 24.1520 - MinusLogProbMetric: 24.1520 - val_loss: 27.2382 - val_MinusLogProbMetric: 27.2382 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 55/1000
2023-09-27 06:36:43.149 
Epoch 55/1000 
	 loss: 23.8959, MinusLogProbMetric: 23.8959, val_loss: 24.7052, val_MinusLogProbMetric: 24.7052

Epoch 55: val_loss did not improve from 24.06313
196/196 - 64s - loss: 23.8959 - MinusLogProbMetric: 23.8959 - val_loss: 24.7052 - val_MinusLogProbMetric: 24.7052 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 56/1000
2023-09-27 06:37:47.639 
Epoch 56/1000 
	 loss: 23.6189, MinusLogProbMetric: 23.6189, val_loss: 23.5527, val_MinusLogProbMetric: 23.5527

Epoch 56: val_loss improved from 24.06313 to 23.55268, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 23.6189 - MinusLogProbMetric: 23.6189 - val_loss: 23.5527 - val_MinusLogProbMetric: 23.5527 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 57/1000
2023-09-27 06:38:52.925 
Epoch 57/1000 
	 loss: 23.5814, MinusLogProbMetric: 23.5814, val_loss: 23.5905, val_MinusLogProbMetric: 23.5905

Epoch 57: val_loss did not improve from 23.55268
196/196 - 64s - loss: 23.5814 - MinusLogProbMetric: 23.5814 - val_loss: 23.5905 - val_MinusLogProbMetric: 23.5905 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 58/1000
2023-09-27 06:39:57.900 
Epoch 58/1000 
	 loss: 23.4273, MinusLogProbMetric: 23.4273, val_loss: 23.2311, val_MinusLogProbMetric: 23.2311

Epoch 58: val_loss improved from 23.55268 to 23.23113, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 23.4273 - MinusLogProbMetric: 23.4273 - val_loss: 23.2311 - val_MinusLogProbMetric: 23.2311 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 59/1000
2023-09-27 06:41:03.842 
Epoch 59/1000 
	 loss: 23.4361, MinusLogProbMetric: 23.4361, val_loss: 23.6803, val_MinusLogProbMetric: 23.6803

Epoch 59: val_loss did not improve from 23.23113
196/196 - 65s - loss: 23.4361 - MinusLogProbMetric: 23.4361 - val_loss: 23.6803 - val_MinusLogProbMetric: 23.6803 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 60/1000
2023-09-27 06:42:08.512 
Epoch 60/1000 
	 loss: 23.3547, MinusLogProbMetric: 23.3547, val_loss: 24.0905, val_MinusLogProbMetric: 24.0905

Epoch 60: val_loss did not improve from 23.23113
196/196 - 65s - loss: 23.3547 - MinusLogProbMetric: 23.3547 - val_loss: 24.0905 - val_MinusLogProbMetric: 24.0905 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 61/1000
2023-09-27 06:43:13.259 
Epoch 61/1000 
	 loss: 23.1950, MinusLogProbMetric: 23.1950, val_loss: 22.9106, val_MinusLogProbMetric: 22.9106

Epoch 61: val_loss improved from 23.23113 to 22.91062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 23.1950 - MinusLogProbMetric: 23.1950 - val_loss: 22.9106 - val_MinusLogProbMetric: 22.9106 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 62/1000
2023-09-27 06:44:18.722 
Epoch 62/1000 
	 loss: 23.0210, MinusLogProbMetric: 23.0210, val_loss: 23.4320, val_MinusLogProbMetric: 23.4320

Epoch 62: val_loss did not improve from 22.91062
196/196 - 65s - loss: 23.0210 - MinusLogProbMetric: 23.0210 - val_loss: 23.4320 - val_MinusLogProbMetric: 23.4320 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 63/1000
2023-09-27 06:45:23.387 
Epoch 63/1000 
	 loss: 22.9440, MinusLogProbMetric: 22.9440, val_loss: 22.9894, val_MinusLogProbMetric: 22.9894

Epoch 63: val_loss did not improve from 22.91062
196/196 - 65s - loss: 22.9440 - MinusLogProbMetric: 22.9440 - val_loss: 22.9894 - val_MinusLogProbMetric: 22.9894 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 64/1000
2023-09-27 06:46:28.060 
Epoch 64/1000 
	 loss: 23.2902, MinusLogProbMetric: 23.2902, val_loss: 23.9257, val_MinusLogProbMetric: 23.9257

Epoch 64: val_loss did not improve from 22.91062
196/196 - 65s - loss: 23.2902 - MinusLogProbMetric: 23.2902 - val_loss: 23.9257 - val_MinusLogProbMetric: 23.9257 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 65/1000
2023-09-27 06:47:32.147 
Epoch 65/1000 
	 loss: 22.9133, MinusLogProbMetric: 22.9133, val_loss: 22.6085, val_MinusLogProbMetric: 22.6085

Epoch 65: val_loss improved from 22.91062 to 22.60854, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 22.9133 - MinusLogProbMetric: 22.9133 - val_loss: 22.6085 - val_MinusLogProbMetric: 22.6085 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 66/1000
2023-09-27 06:48:38.162 
Epoch 66/1000 
	 loss: 22.8217, MinusLogProbMetric: 22.8217, val_loss: 22.7159, val_MinusLogProbMetric: 22.7159

Epoch 66: val_loss did not improve from 22.60854
196/196 - 65s - loss: 22.8217 - MinusLogProbMetric: 22.8217 - val_loss: 22.7159 - val_MinusLogProbMetric: 22.7159 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 67/1000
2023-09-27 06:49:43.070 
Epoch 67/1000 
	 loss: 22.7479, MinusLogProbMetric: 22.7479, val_loss: 23.3039, val_MinusLogProbMetric: 23.3039

Epoch 67: val_loss did not improve from 22.60854
196/196 - 65s - loss: 22.7479 - MinusLogProbMetric: 22.7479 - val_loss: 23.3039 - val_MinusLogProbMetric: 23.3039 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 68/1000
2023-09-27 06:50:47.928 
Epoch 68/1000 
	 loss: 22.5632, MinusLogProbMetric: 22.5632, val_loss: 22.4920, val_MinusLogProbMetric: 22.4920

Epoch 68: val_loss improved from 22.60854 to 22.49203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 22.5632 - MinusLogProbMetric: 22.5632 - val_loss: 22.4920 - val_MinusLogProbMetric: 22.4920 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 69/1000
2023-09-27 06:51:53.828 
Epoch 69/1000 
	 loss: 22.5047, MinusLogProbMetric: 22.5047, val_loss: 23.1008, val_MinusLogProbMetric: 23.1008

Epoch 69: val_loss did not improve from 22.49203
196/196 - 65s - loss: 22.5047 - MinusLogProbMetric: 22.5047 - val_loss: 23.1008 - val_MinusLogProbMetric: 23.1008 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 70/1000
2023-09-27 06:52:58.308 
Epoch 70/1000 
	 loss: 22.4722, MinusLogProbMetric: 22.4722, val_loss: 22.4209, val_MinusLogProbMetric: 22.4209

Epoch 70: val_loss improved from 22.49203 to 22.42089, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 22.4722 - MinusLogProbMetric: 22.4722 - val_loss: 22.4209 - val_MinusLogProbMetric: 22.4209 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 71/1000
2023-09-27 06:54:03.684 
Epoch 71/1000 
	 loss: 22.4311, MinusLogProbMetric: 22.4311, val_loss: 22.7413, val_MinusLogProbMetric: 22.7413

Epoch 71: val_loss did not improve from 22.42089
196/196 - 64s - loss: 22.4311 - MinusLogProbMetric: 22.4311 - val_loss: 22.7413 - val_MinusLogProbMetric: 22.7413 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 72/1000
2023-09-27 06:55:08.309 
Epoch 72/1000 
	 loss: 22.2717, MinusLogProbMetric: 22.2717, val_loss: 22.7539, val_MinusLogProbMetric: 22.7539

Epoch 72: val_loss did not improve from 22.42089
196/196 - 65s - loss: 22.2717 - MinusLogProbMetric: 22.2717 - val_loss: 22.7539 - val_MinusLogProbMetric: 22.7539 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 73/1000
2023-09-27 06:56:12.440 
Epoch 73/1000 
	 loss: 22.3010, MinusLogProbMetric: 22.3010, val_loss: 22.7423, val_MinusLogProbMetric: 22.7423

Epoch 73: val_loss did not improve from 22.42089
196/196 - 64s - loss: 22.3010 - MinusLogProbMetric: 22.3010 - val_loss: 22.7423 - val_MinusLogProbMetric: 22.7423 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 74/1000
2023-09-27 06:57:16.642 
Epoch 74/1000 
	 loss: 22.2659, MinusLogProbMetric: 22.2659, val_loss: 22.4912, val_MinusLogProbMetric: 22.4912

Epoch 74: val_loss did not improve from 22.42089
196/196 - 64s - loss: 22.2659 - MinusLogProbMetric: 22.2659 - val_loss: 22.4912 - val_MinusLogProbMetric: 22.4912 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 75/1000
2023-09-27 06:58:20.497 
Epoch 75/1000 
	 loss: 22.2790, MinusLogProbMetric: 22.2790, val_loss: 22.3642, val_MinusLogProbMetric: 22.3642

Epoch 75: val_loss improved from 22.42089 to 22.36416, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 22.2790 - MinusLogProbMetric: 22.2790 - val_loss: 22.3642 - val_MinusLogProbMetric: 22.3642 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 76/1000
2023-09-27 06:59:26.125 
Epoch 76/1000 
	 loss: 22.0504, MinusLogProbMetric: 22.0504, val_loss: 22.8368, val_MinusLogProbMetric: 22.8368

Epoch 76: val_loss did not improve from 22.36416
196/196 - 65s - loss: 22.0504 - MinusLogProbMetric: 22.0504 - val_loss: 22.8368 - val_MinusLogProbMetric: 22.8368 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 77/1000
2023-09-27 07:00:30.559 
Epoch 77/1000 
	 loss: 22.0819, MinusLogProbMetric: 22.0819, val_loss: 23.5076, val_MinusLogProbMetric: 23.5076

Epoch 77: val_loss did not improve from 22.36416
196/196 - 64s - loss: 22.0819 - MinusLogProbMetric: 22.0819 - val_loss: 23.5076 - val_MinusLogProbMetric: 23.5076 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 78/1000
2023-09-27 07:01:35.177 
Epoch 78/1000 
	 loss: 22.0281, MinusLogProbMetric: 22.0281, val_loss: 22.1193, val_MinusLogProbMetric: 22.1193

Epoch 78: val_loss improved from 22.36416 to 22.11928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 22.0281 - MinusLogProbMetric: 22.0281 - val_loss: 22.1193 - val_MinusLogProbMetric: 22.1193 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 79/1000
2023-09-27 07:02:40.663 
Epoch 79/1000 
	 loss: 21.9185, MinusLogProbMetric: 21.9185, val_loss: 22.2184, val_MinusLogProbMetric: 22.2184

Epoch 79: val_loss did not improve from 22.11928
196/196 - 65s - loss: 21.9185 - MinusLogProbMetric: 21.9185 - val_loss: 22.2184 - val_MinusLogProbMetric: 22.2184 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 80/1000
2023-09-27 07:03:45.641 
Epoch 80/1000 
	 loss: 21.9591, MinusLogProbMetric: 21.9591, val_loss: 22.0058, val_MinusLogProbMetric: 22.0058

Epoch 80: val_loss improved from 22.11928 to 22.00576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 21.9591 - MinusLogProbMetric: 21.9591 - val_loss: 22.0058 - val_MinusLogProbMetric: 22.0058 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 81/1000
2023-09-27 07:04:51.562 
Epoch 81/1000 
	 loss: 21.8504, MinusLogProbMetric: 21.8504, val_loss: 22.8695, val_MinusLogProbMetric: 22.8695

Epoch 81: val_loss did not improve from 22.00576
196/196 - 65s - loss: 21.8504 - MinusLogProbMetric: 21.8504 - val_loss: 22.8695 - val_MinusLogProbMetric: 22.8695 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 82/1000
2023-09-27 07:05:56.315 
Epoch 82/1000 
	 loss: 21.8291, MinusLogProbMetric: 21.8291, val_loss: 21.9448, val_MinusLogProbMetric: 21.9448

Epoch 82: val_loss improved from 22.00576 to 21.94476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 21.8291 - MinusLogProbMetric: 21.8291 - val_loss: 21.9448 - val_MinusLogProbMetric: 21.9448 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 83/1000
2023-09-27 07:07:01.079 
Epoch 83/1000 
	 loss: 21.7439, MinusLogProbMetric: 21.7439, val_loss: 22.4992, val_MinusLogProbMetric: 22.4992

Epoch 83: val_loss did not improve from 21.94476
196/196 - 64s - loss: 21.7439 - MinusLogProbMetric: 21.7439 - val_loss: 22.4992 - val_MinusLogProbMetric: 22.4992 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 84/1000
2023-09-27 07:08:05.382 
Epoch 84/1000 
	 loss: 21.8152, MinusLogProbMetric: 21.8152, val_loss: 21.5411, val_MinusLogProbMetric: 21.5411

Epoch 84: val_loss improved from 21.94476 to 21.54106, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 21.8152 - MinusLogProbMetric: 21.8152 - val_loss: 21.5411 - val_MinusLogProbMetric: 21.5411 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 85/1000
2023-09-27 07:09:11.158 
Epoch 85/1000 
	 loss: 21.5563, MinusLogProbMetric: 21.5563, val_loss: 22.3529, val_MinusLogProbMetric: 22.3529

Epoch 85: val_loss did not improve from 21.54106
196/196 - 65s - loss: 21.5563 - MinusLogProbMetric: 21.5563 - val_loss: 22.3529 - val_MinusLogProbMetric: 22.3529 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 86/1000
2023-09-27 07:10:15.457 
Epoch 86/1000 
	 loss: 21.6485, MinusLogProbMetric: 21.6485, val_loss: 21.7388, val_MinusLogProbMetric: 21.7388

Epoch 86: val_loss did not improve from 21.54106
196/196 - 64s - loss: 21.6485 - MinusLogProbMetric: 21.6485 - val_loss: 21.7388 - val_MinusLogProbMetric: 21.7388 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 87/1000
2023-09-27 07:11:20.086 
Epoch 87/1000 
	 loss: 21.5596, MinusLogProbMetric: 21.5596, val_loss: 22.3634, val_MinusLogProbMetric: 22.3634

Epoch 87: val_loss did not improve from 21.54106
196/196 - 65s - loss: 21.5596 - MinusLogProbMetric: 21.5596 - val_loss: 22.3634 - val_MinusLogProbMetric: 22.3634 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 88/1000
2023-09-27 07:12:24.661 
Epoch 88/1000 
	 loss: 21.5540, MinusLogProbMetric: 21.5540, val_loss: 21.3106, val_MinusLogProbMetric: 21.3106

Epoch 88: val_loss improved from 21.54106 to 21.31065, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 21.5540 - MinusLogProbMetric: 21.5540 - val_loss: 21.3106 - val_MinusLogProbMetric: 21.3106 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 89/1000
2023-09-27 07:13:30.115 
Epoch 89/1000 
	 loss: 21.4115, MinusLogProbMetric: 21.4115, val_loss: 22.2603, val_MinusLogProbMetric: 22.2603

Epoch 89: val_loss did not improve from 21.31065
196/196 - 65s - loss: 21.4115 - MinusLogProbMetric: 21.4115 - val_loss: 22.2603 - val_MinusLogProbMetric: 22.2603 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 90/1000
2023-09-27 07:14:34.753 
Epoch 90/1000 
	 loss: 21.4782, MinusLogProbMetric: 21.4782, val_loss: 22.3088, val_MinusLogProbMetric: 22.3088

Epoch 90: val_loss did not improve from 21.31065
196/196 - 65s - loss: 21.4782 - MinusLogProbMetric: 21.4782 - val_loss: 22.3088 - val_MinusLogProbMetric: 22.3088 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 91/1000
2023-09-27 07:15:40.053 
Epoch 91/1000 
	 loss: 21.4418, MinusLogProbMetric: 21.4418, val_loss: 22.1248, val_MinusLogProbMetric: 22.1248

Epoch 91: val_loss did not improve from 21.31065
196/196 - 65s - loss: 21.4418 - MinusLogProbMetric: 21.4418 - val_loss: 22.1248 - val_MinusLogProbMetric: 22.1248 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 92/1000
2023-09-27 07:16:45.005 
Epoch 92/1000 
	 loss: 21.3939, MinusLogProbMetric: 21.3939, val_loss: 22.8819, val_MinusLogProbMetric: 22.8819

Epoch 92: val_loss did not improve from 21.31065
196/196 - 65s - loss: 21.3939 - MinusLogProbMetric: 21.3939 - val_loss: 22.8819 - val_MinusLogProbMetric: 22.8819 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 93/1000
2023-09-27 07:17:46.670 
Epoch 93/1000 
	 loss: 21.3027, MinusLogProbMetric: 21.3027, val_loss: 21.1527, val_MinusLogProbMetric: 21.1527

Epoch 93: val_loss improved from 21.31065 to 21.15265, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 63s - loss: 21.3027 - MinusLogProbMetric: 21.3027 - val_loss: 21.1527 - val_MinusLogProbMetric: 21.1527 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 94/1000
2023-09-27 07:18:39.807 
Epoch 94/1000 
	 loss: 24.3212, MinusLogProbMetric: 24.3212, val_loss: 33.7615, val_MinusLogProbMetric: 33.7615

Epoch 94: val_loss did not improve from 21.15265
196/196 - 52s - loss: 24.3212 - MinusLogProbMetric: 24.3212 - val_loss: 33.7615 - val_MinusLogProbMetric: 33.7615 - lr: 3.3333e-04 - 52s/epoch - 267ms/step
Epoch 95/1000
2023-09-27 07:19:37.193 
Epoch 95/1000 
	 loss: 24.5039, MinusLogProbMetric: 24.5039, val_loss: 22.1761, val_MinusLogProbMetric: 22.1761

Epoch 95: val_loss did not improve from 21.15265
196/196 - 57s - loss: 24.5039 - MinusLogProbMetric: 24.5039 - val_loss: 22.1761 - val_MinusLogProbMetric: 22.1761 - lr: 3.3333e-04 - 57s/epoch - 293ms/step
Epoch 96/1000
2023-09-27 07:20:39.509 
Epoch 96/1000 
	 loss: 21.7616, MinusLogProbMetric: 21.7616, val_loss: 21.8661, val_MinusLogProbMetric: 21.8661

Epoch 96: val_loss did not improve from 21.15265
196/196 - 62s - loss: 21.7616 - MinusLogProbMetric: 21.7616 - val_loss: 21.8661 - val_MinusLogProbMetric: 21.8661 - lr: 3.3333e-04 - 62s/epoch - 318ms/step
Epoch 97/1000
2023-09-27 07:21:31.238 
Epoch 97/1000 
	 loss: 21.7161, MinusLogProbMetric: 21.7161, val_loss: 21.5764, val_MinusLogProbMetric: 21.5764

Epoch 97: val_loss did not improve from 21.15265
196/196 - 52s - loss: 21.7161 - MinusLogProbMetric: 21.7161 - val_loss: 21.5764 - val_MinusLogProbMetric: 21.5764 - lr: 3.3333e-04 - 52s/epoch - 264ms/step
Epoch 98/1000
2023-09-27 07:22:31.056 
Epoch 98/1000 
	 loss: 21.6787, MinusLogProbMetric: 21.6787, val_loss: 21.4630, val_MinusLogProbMetric: 21.4630

Epoch 98: val_loss did not improve from 21.15265
196/196 - 60s - loss: 21.6787 - MinusLogProbMetric: 21.6787 - val_loss: 21.4630 - val_MinusLogProbMetric: 21.4630 - lr: 3.3333e-04 - 60s/epoch - 305ms/step
Epoch 99/1000
2023-09-27 07:23:34.873 
Epoch 99/1000 
	 loss: 21.5267, MinusLogProbMetric: 21.5267, val_loss: 21.8513, val_MinusLogProbMetric: 21.8513

Epoch 99: val_loss did not improve from 21.15265
196/196 - 64s - loss: 21.5267 - MinusLogProbMetric: 21.5267 - val_loss: 21.8513 - val_MinusLogProbMetric: 21.8513 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 100/1000
2023-09-27 07:24:39.940 
Epoch 100/1000 
	 loss: 21.4236, MinusLogProbMetric: 21.4236, val_loss: 21.9024, val_MinusLogProbMetric: 21.9024

Epoch 100: val_loss did not improve from 21.15265
196/196 - 65s - loss: 21.4236 - MinusLogProbMetric: 21.4236 - val_loss: 21.9024 - val_MinusLogProbMetric: 21.9024 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 101/1000
2023-09-27 07:25:44.366 
Epoch 101/1000 
	 loss: 21.4854, MinusLogProbMetric: 21.4854, val_loss: 21.2728, val_MinusLogProbMetric: 21.2728

Epoch 101: val_loss did not improve from 21.15265
196/196 - 64s - loss: 21.4854 - MinusLogProbMetric: 21.4854 - val_loss: 21.2728 - val_MinusLogProbMetric: 21.2728 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 102/1000
2023-09-27 07:26:49.327 
Epoch 102/1000 
	 loss: 21.1858, MinusLogProbMetric: 21.1858, val_loss: 21.1848, val_MinusLogProbMetric: 21.1848

Epoch 102: val_loss did not improve from 21.15265
196/196 - 65s - loss: 21.1858 - MinusLogProbMetric: 21.1858 - val_loss: 21.1848 - val_MinusLogProbMetric: 21.1848 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 103/1000
2023-09-27 07:27:53.857 
Epoch 103/1000 
	 loss: 21.2529, MinusLogProbMetric: 21.2529, val_loss: 21.1292, val_MinusLogProbMetric: 21.1292

Epoch 103: val_loss improved from 21.15265 to 21.12921, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 21.2529 - MinusLogProbMetric: 21.2529 - val_loss: 21.1292 - val_MinusLogProbMetric: 21.1292 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 104/1000
2023-09-27 07:28:58.961 
Epoch 104/1000 
	 loss: 21.0380, MinusLogProbMetric: 21.0380, val_loss: 21.7533, val_MinusLogProbMetric: 21.7533

Epoch 104: val_loss did not improve from 21.12921
196/196 - 64s - loss: 21.0380 - MinusLogProbMetric: 21.0380 - val_loss: 21.7533 - val_MinusLogProbMetric: 21.7533 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 105/1000
2023-09-27 07:30:03.509 
Epoch 105/1000 
	 loss: 20.9334, MinusLogProbMetric: 20.9334, val_loss: 20.6874, val_MinusLogProbMetric: 20.6874

Epoch 105: val_loss improved from 21.12921 to 20.68741, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 20.9334 - MinusLogProbMetric: 20.9334 - val_loss: 20.6874 - val_MinusLogProbMetric: 20.6874 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 106/1000
2023-09-27 07:31:08.666 
Epoch 106/1000 
	 loss: 20.9452, MinusLogProbMetric: 20.9452, val_loss: 21.0455, val_MinusLogProbMetric: 21.0455

Epoch 106: val_loss did not improve from 20.68741
196/196 - 64s - loss: 20.9452 - MinusLogProbMetric: 20.9452 - val_loss: 21.0455 - val_MinusLogProbMetric: 21.0455 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 107/1000
2023-09-27 07:32:13.414 
Epoch 107/1000 
	 loss: 20.8844, MinusLogProbMetric: 20.8844, val_loss: 21.4306, val_MinusLogProbMetric: 21.4306

Epoch 107: val_loss did not improve from 20.68741
196/196 - 65s - loss: 20.8844 - MinusLogProbMetric: 20.8844 - val_loss: 21.4306 - val_MinusLogProbMetric: 21.4306 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 108/1000
2023-09-27 07:33:17.030 
Epoch 108/1000 
	 loss: 20.7947, MinusLogProbMetric: 20.7947, val_loss: 20.8638, val_MinusLogProbMetric: 20.8638

Epoch 108: val_loss did not improve from 20.68741
196/196 - 64s - loss: 20.7947 - MinusLogProbMetric: 20.7947 - val_loss: 20.8638 - val_MinusLogProbMetric: 20.8638 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 109/1000
2023-09-27 07:34:21.407 
Epoch 109/1000 
	 loss: 20.7137, MinusLogProbMetric: 20.7137, val_loss: 20.8962, val_MinusLogProbMetric: 20.8962

Epoch 109: val_loss did not improve from 20.68741
196/196 - 64s - loss: 20.7137 - MinusLogProbMetric: 20.7137 - val_loss: 20.8962 - val_MinusLogProbMetric: 20.8962 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 110/1000
2023-09-27 07:35:25.883 
Epoch 110/1000 
	 loss: 20.6625, MinusLogProbMetric: 20.6625, val_loss: 20.6035, val_MinusLogProbMetric: 20.6035

Epoch 110: val_loss improved from 20.68741 to 20.60354, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 20.6625 - MinusLogProbMetric: 20.6625 - val_loss: 20.6035 - val_MinusLogProbMetric: 20.6035 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 111/1000
2023-09-27 07:36:30.859 
Epoch 111/1000 
	 loss: 20.8574, MinusLogProbMetric: 20.8574, val_loss: 20.7422, val_MinusLogProbMetric: 20.7422

Epoch 111: val_loss did not improve from 20.60354
196/196 - 64s - loss: 20.8574 - MinusLogProbMetric: 20.8574 - val_loss: 20.7422 - val_MinusLogProbMetric: 20.7422 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 112/1000
2023-09-27 07:37:35.025 
Epoch 112/1000 
	 loss: 20.7227, MinusLogProbMetric: 20.7227, val_loss: 21.0858, val_MinusLogProbMetric: 21.0858

Epoch 112: val_loss did not improve from 20.60354
196/196 - 64s - loss: 20.7227 - MinusLogProbMetric: 20.7227 - val_loss: 21.0858 - val_MinusLogProbMetric: 21.0858 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 113/1000
2023-09-27 07:38:39.244 
Epoch 113/1000 
	 loss: 20.6750, MinusLogProbMetric: 20.6750, val_loss: 20.7414, val_MinusLogProbMetric: 20.7414

Epoch 113: val_loss did not improve from 20.60354
196/196 - 64s - loss: 20.6750 - MinusLogProbMetric: 20.6750 - val_loss: 20.7414 - val_MinusLogProbMetric: 20.7414 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 114/1000
2023-09-27 07:39:43.829 
Epoch 114/1000 
	 loss: 20.6963, MinusLogProbMetric: 20.6963, val_loss: 21.0700, val_MinusLogProbMetric: 21.0700

Epoch 114: val_loss did not improve from 20.60354
196/196 - 65s - loss: 20.6963 - MinusLogProbMetric: 20.6963 - val_loss: 21.0700 - val_MinusLogProbMetric: 21.0700 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 115/1000
2023-09-27 07:40:48.215 
Epoch 115/1000 
	 loss: 20.6183, MinusLogProbMetric: 20.6183, val_loss: 21.0747, val_MinusLogProbMetric: 21.0747

Epoch 115: val_loss did not improve from 20.60354
196/196 - 64s - loss: 20.6183 - MinusLogProbMetric: 20.6183 - val_loss: 21.0747 - val_MinusLogProbMetric: 21.0747 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 116/1000
2023-09-27 07:41:52.735 
Epoch 116/1000 
	 loss: 20.7553, MinusLogProbMetric: 20.7553, val_loss: 20.4023, val_MinusLogProbMetric: 20.4023

Epoch 116: val_loss improved from 20.60354 to 20.40227, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 20.7553 - MinusLogProbMetric: 20.7553 - val_loss: 20.4023 - val_MinusLogProbMetric: 20.4023 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 117/1000
2023-09-27 07:42:58.067 
Epoch 117/1000 
	 loss: 24.8034, MinusLogProbMetric: 24.8034, val_loss: 22.1899, val_MinusLogProbMetric: 22.1899

Epoch 117: val_loss did not improve from 20.40227
196/196 - 64s - loss: 24.8034 - MinusLogProbMetric: 24.8034 - val_loss: 22.1899 - val_MinusLogProbMetric: 22.1899 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 118/1000
2023-09-27 07:44:02.291 
Epoch 118/1000 
	 loss: 21.4817, MinusLogProbMetric: 21.4817, val_loss: 22.2545, val_MinusLogProbMetric: 22.2545

Epoch 118: val_loss did not improve from 20.40227
196/196 - 64s - loss: 21.4817 - MinusLogProbMetric: 21.4817 - val_loss: 22.2545 - val_MinusLogProbMetric: 22.2545 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 119/1000
2023-09-27 07:45:06.736 
Epoch 119/1000 
	 loss: 21.0192, MinusLogProbMetric: 21.0192, val_loss: 20.8751, val_MinusLogProbMetric: 20.8751

Epoch 119: val_loss did not improve from 20.40227
196/196 - 64s - loss: 21.0192 - MinusLogProbMetric: 21.0192 - val_loss: 20.8751 - val_MinusLogProbMetric: 20.8751 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 120/1000
2023-09-27 07:46:11.345 
Epoch 120/1000 
	 loss: 20.7862, MinusLogProbMetric: 20.7862, val_loss: 20.3412, val_MinusLogProbMetric: 20.3412

Epoch 120: val_loss improved from 20.40227 to 20.34116, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 20.7862 - MinusLogProbMetric: 20.7862 - val_loss: 20.3412 - val_MinusLogProbMetric: 20.3412 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 121/1000
2023-09-27 07:47:16.548 
Epoch 121/1000 
	 loss: 20.6680, MinusLogProbMetric: 20.6680, val_loss: 21.2494, val_MinusLogProbMetric: 21.2494

Epoch 121: val_loss did not improve from 20.34116
196/196 - 64s - loss: 20.6680 - MinusLogProbMetric: 20.6680 - val_loss: 21.2494 - val_MinusLogProbMetric: 21.2494 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 122/1000
2023-09-27 07:48:20.849 
Epoch 122/1000 
	 loss: 20.5073, MinusLogProbMetric: 20.5073, val_loss: 20.3776, val_MinusLogProbMetric: 20.3776

Epoch 122: val_loss did not improve from 20.34116
196/196 - 64s - loss: 20.5073 - MinusLogProbMetric: 20.5073 - val_loss: 20.3776 - val_MinusLogProbMetric: 20.3776 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 123/1000
2023-09-27 07:49:25.116 
Epoch 123/1000 
	 loss: 20.4333, MinusLogProbMetric: 20.4333, val_loss: 21.1474, val_MinusLogProbMetric: 21.1474

Epoch 123: val_loss did not improve from 20.34116
196/196 - 64s - loss: 20.4333 - MinusLogProbMetric: 20.4333 - val_loss: 21.1474 - val_MinusLogProbMetric: 21.1474 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 124/1000
2023-09-27 07:50:29.186 
Epoch 124/1000 
	 loss: 20.4183, MinusLogProbMetric: 20.4183, val_loss: 20.4792, val_MinusLogProbMetric: 20.4792

Epoch 124: val_loss did not improve from 20.34116
196/196 - 64s - loss: 20.4183 - MinusLogProbMetric: 20.4183 - val_loss: 20.4792 - val_MinusLogProbMetric: 20.4792 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 125/1000
2023-09-27 07:51:33.737 
Epoch 125/1000 
	 loss: 20.4501, MinusLogProbMetric: 20.4501, val_loss: 20.3069, val_MinusLogProbMetric: 20.3069

Epoch 125: val_loss improved from 20.34116 to 20.30694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 20.4501 - MinusLogProbMetric: 20.4501 - val_loss: 20.3069 - val_MinusLogProbMetric: 20.3069 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 126/1000
2023-09-27 07:52:39.189 
Epoch 126/1000 
	 loss: 20.3944, MinusLogProbMetric: 20.3944, val_loss: 20.3146, val_MinusLogProbMetric: 20.3146

Epoch 126: val_loss did not improve from 20.30694
196/196 - 64s - loss: 20.3944 - MinusLogProbMetric: 20.3944 - val_loss: 20.3146 - val_MinusLogProbMetric: 20.3146 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 127/1000
2023-09-27 07:53:43.169 
Epoch 127/1000 
	 loss: 20.3644, MinusLogProbMetric: 20.3644, val_loss: 20.4334, val_MinusLogProbMetric: 20.4334

Epoch 127: val_loss did not improve from 20.30694
196/196 - 64s - loss: 20.3644 - MinusLogProbMetric: 20.3644 - val_loss: 20.4334 - val_MinusLogProbMetric: 20.4334 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 128/1000
2023-09-27 07:54:47.920 
Epoch 128/1000 
	 loss: 20.3230, MinusLogProbMetric: 20.3230, val_loss: 20.5544, val_MinusLogProbMetric: 20.5544

Epoch 128: val_loss did not improve from 20.30694
196/196 - 65s - loss: 20.3230 - MinusLogProbMetric: 20.3230 - val_loss: 20.5544 - val_MinusLogProbMetric: 20.5544 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 129/1000
2023-09-27 07:55:52.262 
Epoch 129/1000 
	 loss: 20.3419, MinusLogProbMetric: 20.3419, val_loss: 20.6892, val_MinusLogProbMetric: 20.6892

Epoch 129: val_loss did not improve from 20.30694
196/196 - 64s - loss: 20.3419 - MinusLogProbMetric: 20.3419 - val_loss: 20.6892 - val_MinusLogProbMetric: 20.6892 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 130/1000
2023-09-27 07:56:56.880 
Epoch 130/1000 
	 loss: 20.2203, MinusLogProbMetric: 20.2203, val_loss: 20.7218, val_MinusLogProbMetric: 20.7218

Epoch 130: val_loss did not improve from 20.30694
196/196 - 65s - loss: 20.2203 - MinusLogProbMetric: 20.2203 - val_loss: 20.7218 - val_MinusLogProbMetric: 20.7218 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 131/1000
2023-09-27 07:58:01.814 
Epoch 131/1000 
	 loss: 20.1200, MinusLogProbMetric: 20.1200, val_loss: 20.1525, val_MinusLogProbMetric: 20.1525

Epoch 131: val_loss improved from 20.30694 to 20.15250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 20.1200 - MinusLogProbMetric: 20.1200 - val_loss: 20.1525 - val_MinusLogProbMetric: 20.1525 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 132/1000
2023-09-27 07:59:07.118 
Epoch 132/1000 
	 loss: 20.2876, MinusLogProbMetric: 20.2876, val_loss: 20.7951, val_MinusLogProbMetric: 20.7951

Epoch 132: val_loss did not improve from 20.15250
196/196 - 64s - loss: 20.2876 - MinusLogProbMetric: 20.2876 - val_loss: 20.7951 - val_MinusLogProbMetric: 20.7951 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 133/1000
2023-09-27 08:00:11.532 
Epoch 133/1000 
	 loss: 20.2442, MinusLogProbMetric: 20.2442, val_loss: 20.3470, val_MinusLogProbMetric: 20.3470

Epoch 133: val_loss did not improve from 20.15250
196/196 - 64s - loss: 20.2442 - MinusLogProbMetric: 20.2442 - val_loss: 20.3470 - val_MinusLogProbMetric: 20.3470 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 134/1000
2023-09-27 08:01:15.519 
Epoch 134/1000 
	 loss: 20.2079, MinusLogProbMetric: 20.2079, val_loss: 20.2087, val_MinusLogProbMetric: 20.2087

Epoch 134: val_loss did not improve from 20.15250
196/196 - 64s - loss: 20.2079 - MinusLogProbMetric: 20.2079 - val_loss: 20.2087 - val_MinusLogProbMetric: 20.2087 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 135/1000
2023-09-27 08:02:20.112 
Epoch 135/1000 
	 loss: 20.1167, MinusLogProbMetric: 20.1167, val_loss: 20.1570, val_MinusLogProbMetric: 20.1570

Epoch 135: val_loss did not improve from 20.15250
196/196 - 65s - loss: 20.1167 - MinusLogProbMetric: 20.1167 - val_loss: 20.1570 - val_MinusLogProbMetric: 20.1570 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 136/1000
2023-09-27 08:03:24.644 
Epoch 136/1000 
	 loss: 20.1060, MinusLogProbMetric: 20.1060, val_loss: 20.0327, val_MinusLogProbMetric: 20.0327

Epoch 136: val_loss improved from 20.15250 to 20.03274, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 20.1060 - MinusLogProbMetric: 20.1060 - val_loss: 20.0327 - val_MinusLogProbMetric: 20.0327 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 137/1000
2023-09-27 08:04:29.690 
Epoch 137/1000 
	 loss: 20.0383, MinusLogProbMetric: 20.0383, val_loss: 20.4856, val_MinusLogProbMetric: 20.4856

Epoch 137: val_loss did not improve from 20.03274
196/196 - 64s - loss: 20.0383 - MinusLogProbMetric: 20.0383 - val_loss: 20.4856 - val_MinusLogProbMetric: 20.4856 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 138/1000
2023-09-27 08:05:33.839 
Epoch 138/1000 
	 loss: 20.1477, MinusLogProbMetric: 20.1477, val_loss: 20.4898, val_MinusLogProbMetric: 20.4898

Epoch 138: val_loss did not improve from 20.03274
196/196 - 64s - loss: 20.1477 - MinusLogProbMetric: 20.1477 - val_loss: 20.4898 - val_MinusLogProbMetric: 20.4898 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 139/1000
2023-09-27 08:06:38.279 
Epoch 139/1000 
	 loss: 20.0095, MinusLogProbMetric: 20.0095, val_loss: 19.9696, val_MinusLogProbMetric: 19.9696

Epoch 139: val_loss improved from 20.03274 to 19.96956, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 20.0095 - MinusLogProbMetric: 20.0095 - val_loss: 19.9696 - val_MinusLogProbMetric: 19.9696 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 140/1000
2023-09-27 08:07:43.795 
Epoch 140/1000 
	 loss: 20.0359, MinusLogProbMetric: 20.0359, val_loss: 20.3926, val_MinusLogProbMetric: 20.3926

Epoch 140: val_loss did not improve from 19.96956
196/196 - 64s - loss: 20.0359 - MinusLogProbMetric: 20.0359 - val_loss: 20.3926 - val_MinusLogProbMetric: 20.3926 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 141/1000
2023-09-27 08:08:48.054 
Epoch 141/1000 
	 loss: 19.9718, MinusLogProbMetric: 19.9718, val_loss: 20.4153, val_MinusLogProbMetric: 20.4153

Epoch 141: val_loss did not improve from 19.96956
196/196 - 64s - loss: 19.9718 - MinusLogProbMetric: 19.9718 - val_loss: 20.4153 - val_MinusLogProbMetric: 20.4153 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 142/1000
2023-09-27 08:09:52.758 
Epoch 142/1000 
	 loss: 19.9732, MinusLogProbMetric: 19.9732, val_loss: 20.1409, val_MinusLogProbMetric: 20.1409

Epoch 142: val_loss did not improve from 19.96956
196/196 - 65s - loss: 19.9732 - MinusLogProbMetric: 19.9732 - val_loss: 20.1409 - val_MinusLogProbMetric: 20.1409 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 143/1000
2023-09-27 08:10:57.039 
Epoch 143/1000 
	 loss: 20.0252, MinusLogProbMetric: 20.0252, val_loss: 20.5238, val_MinusLogProbMetric: 20.5238

Epoch 143: val_loss did not improve from 19.96956
196/196 - 64s - loss: 20.0252 - MinusLogProbMetric: 20.0252 - val_loss: 20.5238 - val_MinusLogProbMetric: 20.5238 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 144/1000
2023-09-27 08:12:01.283 
Epoch 144/1000 
	 loss: 20.0119, MinusLogProbMetric: 20.0119, val_loss: 19.9327, val_MinusLogProbMetric: 19.9327

Epoch 144: val_loss improved from 19.96956 to 19.93266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 20.0119 - MinusLogProbMetric: 20.0119 - val_loss: 19.9327 - val_MinusLogProbMetric: 19.9327 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 145/1000
2023-09-27 08:13:06.622 
Epoch 145/1000 
	 loss: 19.9177, MinusLogProbMetric: 19.9177, val_loss: 20.3676, val_MinusLogProbMetric: 20.3676

Epoch 145: val_loss did not improve from 19.93266
196/196 - 64s - loss: 19.9177 - MinusLogProbMetric: 19.9177 - val_loss: 20.3676 - val_MinusLogProbMetric: 20.3676 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 146/1000
2023-09-27 08:14:11.278 
Epoch 146/1000 
	 loss: 19.9552, MinusLogProbMetric: 19.9552, val_loss: 20.0631, val_MinusLogProbMetric: 20.0631

Epoch 146: val_loss did not improve from 19.93266
196/196 - 65s - loss: 19.9552 - MinusLogProbMetric: 19.9552 - val_loss: 20.0631 - val_MinusLogProbMetric: 20.0631 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 147/1000
2023-09-27 08:15:15.575 
Epoch 147/1000 
	 loss: 19.8889, MinusLogProbMetric: 19.8889, val_loss: 19.9839, val_MinusLogProbMetric: 19.9839

Epoch 147: val_loss did not improve from 19.93266
196/196 - 64s - loss: 19.8889 - MinusLogProbMetric: 19.8889 - val_loss: 19.9839 - val_MinusLogProbMetric: 19.9839 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 148/1000
2023-09-27 08:16:20.419 
Epoch 148/1000 
	 loss: 19.7735, MinusLogProbMetric: 19.7735, val_loss: 20.0447, val_MinusLogProbMetric: 20.0447

Epoch 148: val_loss did not improve from 19.93266
196/196 - 65s - loss: 19.7735 - MinusLogProbMetric: 19.7735 - val_loss: 20.0447 - val_MinusLogProbMetric: 20.0447 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 149/1000
2023-09-27 08:17:25.137 
Epoch 149/1000 
	 loss: 19.8387, MinusLogProbMetric: 19.8387, val_loss: 20.3946, val_MinusLogProbMetric: 20.3946

Epoch 149: val_loss did not improve from 19.93266
196/196 - 65s - loss: 19.8387 - MinusLogProbMetric: 19.8387 - val_loss: 20.3946 - val_MinusLogProbMetric: 20.3946 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 150/1000
2023-09-27 08:18:29.886 
Epoch 150/1000 
	 loss: 19.8562, MinusLogProbMetric: 19.8562, val_loss: 20.7511, val_MinusLogProbMetric: 20.7511

Epoch 150: val_loss did not improve from 19.93266
196/196 - 65s - loss: 19.8562 - MinusLogProbMetric: 19.8562 - val_loss: 20.7511 - val_MinusLogProbMetric: 20.7511 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 151/1000
2023-09-27 08:19:34.504 
Epoch 151/1000 
	 loss: 19.8125, MinusLogProbMetric: 19.8125, val_loss: 19.9716, val_MinusLogProbMetric: 19.9716

Epoch 151: val_loss did not improve from 19.93266
196/196 - 65s - loss: 19.8125 - MinusLogProbMetric: 19.8125 - val_loss: 19.9716 - val_MinusLogProbMetric: 19.9716 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 152/1000
2023-09-27 08:20:38.893 
Epoch 152/1000 
	 loss: 19.8229, MinusLogProbMetric: 19.8229, val_loss: 19.8383, val_MinusLogProbMetric: 19.8383

Epoch 152: val_loss improved from 19.93266 to 19.83830, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 19.8229 - MinusLogProbMetric: 19.8229 - val_loss: 19.8383 - val_MinusLogProbMetric: 19.8383 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 153/1000
2023-09-27 08:21:44.300 
Epoch 153/1000 
	 loss: 19.8940, MinusLogProbMetric: 19.8940, val_loss: 19.8877, val_MinusLogProbMetric: 19.8877

Epoch 153: val_loss did not improve from 19.83830
196/196 - 64s - loss: 19.8940 - MinusLogProbMetric: 19.8940 - val_loss: 19.8877 - val_MinusLogProbMetric: 19.8877 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 154/1000
2023-09-27 08:22:48.795 
Epoch 154/1000 
	 loss: 19.8370, MinusLogProbMetric: 19.8370, val_loss: 19.9491, val_MinusLogProbMetric: 19.9491

Epoch 154: val_loss did not improve from 19.83830
196/196 - 64s - loss: 19.8370 - MinusLogProbMetric: 19.8370 - val_loss: 19.9491 - val_MinusLogProbMetric: 19.9491 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 155/1000
2023-09-27 08:23:53.750 
Epoch 155/1000 
	 loss: 19.7432, MinusLogProbMetric: 19.7432, val_loss: 20.0802, val_MinusLogProbMetric: 20.0802

Epoch 155: val_loss did not improve from 19.83830
196/196 - 65s - loss: 19.7432 - MinusLogProbMetric: 19.7432 - val_loss: 20.0802 - val_MinusLogProbMetric: 20.0802 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 156/1000
2023-09-27 08:24:58.151 
Epoch 156/1000 
	 loss: 19.7238, MinusLogProbMetric: 19.7238, val_loss: 19.9543, val_MinusLogProbMetric: 19.9543

Epoch 156: val_loss did not improve from 19.83830
196/196 - 64s - loss: 19.7238 - MinusLogProbMetric: 19.7238 - val_loss: 19.9543 - val_MinusLogProbMetric: 19.9543 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 157/1000
2023-09-27 08:26:02.634 
Epoch 157/1000 
	 loss: 19.7811, MinusLogProbMetric: 19.7811, val_loss: 20.1162, val_MinusLogProbMetric: 20.1162

Epoch 157: val_loss did not improve from 19.83830
196/196 - 64s - loss: 19.7811 - MinusLogProbMetric: 19.7811 - val_loss: 20.1162 - val_MinusLogProbMetric: 20.1162 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 158/1000
2023-09-27 08:27:07.113 
Epoch 158/1000 
	 loss: 19.7229, MinusLogProbMetric: 19.7229, val_loss: 20.0266, val_MinusLogProbMetric: 20.0266

Epoch 158: val_loss did not improve from 19.83830
196/196 - 64s - loss: 19.7229 - MinusLogProbMetric: 19.7229 - val_loss: 20.0266 - val_MinusLogProbMetric: 20.0266 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 159/1000
2023-09-27 08:28:11.089 
Epoch 159/1000 
	 loss: 19.7175, MinusLogProbMetric: 19.7175, val_loss: 19.9109, val_MinusLogProbMetric: 19.9109

Epoch 159: val_loss did not improve from 19.83830
196/196 - 64s - loss: 19.7175 - MinusLogProbMetric: 19.7175 - val_loss: 19.9109 - val_MinusLogProbMetric: 19.9109 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 160/1000
2023-09-27 08:29:15.158 
Epoch 160/1000 
	 loss: 19.7745, MinusLogProbMetric: 19.7745, val_loss: 20.1132, val_MinusLogProbMetric: 20.1132

Epoch 160: val_loss did not improve from 19.83830
196/196 - 64s - loss: 19.7745 - MinusLogProbMetric: 19.7745 - val_loss: 20.1132 - val_MinusLogProbMetric: 20.1132 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 161/1000
2023-09-27 08:30:19.461 
Epoch 161/1000 
	 loss: 19.7738, MinusLogProbMetric: 19.7738, val_loss: 20.5734, val_MinusLogProbMetric: 20.5734

Epoch 161: val_loss did not improve from 19.83830
196/196 - 64s - loss: 19.7738 - MinusLogProbMetric: 19.7738 - val_loss: 20.5734 - val_MinusLogProbMetric: 20.5734 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 162/1000
2023-09-27 08:31:22.906 
Epoch 162/1000 
	 loss: 19.5990, MinusLogProbMetric: 19.5990, val_loss: 20.7095, val_MinusLogProbMetric: 20.7095

Epoch 162: val_loss did not improve from 19.83830
196/196 - 63s - loss: 19.5990 - MinusLogProbMetric: 19.5990 - val_loss: 20.7095 - val_MinusLogProbMetric: 20.7095 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 163/1000
2023-09-27 08:32:27.035 
Epoch 163/1000 
	 loss: 19.6487, MinusLogProbMetric: 19.6487, val_loss: 20.5191, val_MinusLogProbMetric: 20.5191

Epoch 163: val_loss did not improve from 19.83830
196/196 - 64s - loss: 19.6487 - MinusLogProbMetric: 19.6487 - val_loss: 20.5191 - val_MinusLogProbMetric: 20.5191 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 164/1000
2023-09-27 08:33:31.093 
Epoch 164/1000 
	 loss: 19.6342, MinusLogProbMetric: 19.6342, val_loss: 19.5070, val_MinusLogProbMetric: 19.5070

Epoch 164: val_loss improved from 19.83830 to 19.50704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 19.6342 - MinusLogProbMetric: 19.6342 - val_loss: 19.5070 - val_MinusLogProbMetric: 19.5070 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 165/1000
2023-09-27 08:34:36.583 
Epoch 165/1000 
	 loss: 19.7032, MinusLogProbMetric: 19.7032, val_loss: 19.8539, val_MinusLogProbMetric: 19.8539

Epoch 165: val_loss did not improve from 19.50704
196/196 - 65s - loss: 19.7032 - MinusLogProbMetric: 19.7032 - val_loss: 19.8539 - val_MinusLogProbMetric: 19.8539 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 166/1000
2023-09-27 08:35:40.680 
Epoch 166/1000 
	 loss: 19.6723, MinusLogProbMetric: 19.6723, val_loss: 20.6472, val_MinusLogProbMetric: 20.6472

Epoch 166: val_loss did not improve from 19.50704
196/196 - 64s - loss: 19.6723 - MinusLogProbMetric: 19.6723 - val_loss: 20.6472 - val_MinusLogProbMetric: 20.6472 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 167/1000
2023-09-27 08:36:45.045 
Epoch 167/1000 
	 loss: 19.6145, MinusLogProbMetric: 19.6145, val_loss: 20.2194, val_MinusLogProbMetric: 20.2194

Epoch 167: val_loss did not improve from 19.50704
196/196 - 64s - loss: 19.6145 - MinusLogProbMetric: 19.6145 - val_loss: 20.2194 - val_MinusLogProbMetric: 20.2194 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 168/1000
2023-09-27 08:37:49.176 
Epoch 168/1000 
	 loss: 19.6723, MinusLogProbMetric: 19.6723, val_loss: 20.4361, val_MinusLogProbMetric: 20.4361

Epoch 168: val_loss did not improve from 19.50704
196/196 - 64s - loss: 19.6723 - MinusLogProbMetric: 19.6723 - val_loss: 20.4361 - val_MinusLogProbMetric: 20.4361 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 169/1000
2023-09-27 08:38:53.062 
Epoch 169/1000 
	 loss: 19.5288, MinusLogProbMetric: 19.5288, val_loss: 19.9560, val_MinusLogProbMetric: 19.9560

Epoch 169: val_loss did not improve from 19.50704
196/196 - 64s - loss: 19.5288 - MinusLogProbMetric: 19.5288 - val_loss: 19.9560 - val_MinusLogProbMetric: 19.9560 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 170/1000
2023-09-27 08:39:57.729 
Epoch 170/1000 
	 loss: 19.6729, MinusLogProbMetric: 19.6729, val_loss: 20.1091, val_MinusLogProbMetric: 20.1091

Epoch 170: val_loss did not improve from 19.50704
196/196 - 65s - loss: 19.6729 - MinusLogProbMetric: 19.6729 - val_loss: 20.1091 - val_MinusLogProbMetric: 20.1091 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 171/1000
2023-09-27 08:41:02.332 
Epoch 171/1000 
	 loss: 19.6234, MinusLogProbMetric: 19.6234, val_loss: 19.7133, val_MinusLogProbMetric: 19.7133

Epoch 171: val_loss did not improve from 19.50704
196/196 - 65s - loss: 19.6234 - MinusLogProbMetric: 19.6234 - val_loss: 19.7133 - val_MinusLogProbMetric: 19.7133 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 172/1000
2023-09-27 08:42:06.784 
Epoch 172/1000 
	 loss: 20.3294, MinusLogProbMetric: 20.3294, val_loss: 20.9224, val_MinusLogProbMetric: 20.9224

Epoch 172: val_loss did not improve from 19.50704
196/196 - 64s - loss: 20.3294 - MinusLogProbMetric: 20.3294 - val_loss: 20.9224 - val_MinusLogProbMetric: 20.9224 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 173/1000
2023-09-27 08:43:11.483 
Epoch 173/1000 
	 loss: 19.6051, MinusLogProbMetric: 19.6051, val_loss: 19.6073, val_MinusLogProbMetric: 19.6073

Epoch 173: val_loss did not improve from 19.50704
196/196 - 65s - loss: 19.6051 - MinusLogProbMetric: 19.6051 - val_loss: 19.6073 - val_MinusLogProbMetric: 19.6073 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 174/1000
2023-09-27 08:44:16.076 
Epoch 174/1000 
	 loss: 19.5291, MinusLogProbMetric: 19.5291, val_loss: 20.8817, val_MinusLogProbMetric: 20.8817

Epoch 174: val_loss did not improve from 19.50704
196/196 - 65s - loss: 19.5291 - MinusLogProbMetric: 19.5291 - val_loss: 20.8817 - val_MinusLogProbMetric: 20.8817 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 175/1000
2023-09-27 08:45:20.733 
Epoch 175/1000 
	 loss: 19.5099, MinusLogProbMetric: 19.5099, val_loss: 19.9253, val_MinusLogProbMetric: 19.9253

Epoch 175: val_loss did not improve from 19.50704
196/196 - 65s - loss: 19.5099 - MinusLogProbMetric: 19.5099 - val_loss: 19.9253 - val_MinusLogProbMetric: 19.9253 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 176/1000
2023-09-27 08:46:25.518 
Epoch 176/1000 
	 loss: 19.5136, MinusLogProbMetric: 19.5136, val_loss: 19.5583, val_MinusLogProbMetric: 19.5583

Epoch 176: val_loss did not improve from 19.50704
196/196 - 65s - loss: 19.5136 - MinusLogProbMetric: 19.5136 - val_loss: 19.5583 - val_MinusLogProbMetric: 19.5583 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 177/1000
2023-09-27 08:47:29.624 
Epoch 177/1000 
	 loss: 19.4607, MinusLogProbMetric: 19.4607, val_loss: 19.8404, val_MinusLogProbMetric: 19.8404

Epoch 177: val_loss did not improve from 19.50704
196/196 - 64s - loss: 19.4607 - MinusLogProbMetric: 19.4607 - val_loss: 19.8404 - val_MinusLogProbMetric: 19.8404 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 178/1000
2023-09-27 08:48:34.333 
Epoch 178/1000 
	 loss: 19.5201, MinusLogProbMetric: 19.5201, val_loss: 19.6026, val_MinusLogProbMetric: 19.6026

Epoch 178: val_loss did not improve from 19.50704
196/196 - 65s - loss: 19.5201 - MinusLogProbMetric: 19.5201 - val_loss: 19.6026 - val_MinusLogProbMetric: 19.6026 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 179/1000
2023-09-27 08:49:38.513 
Epoch 179/1000 
	 loss: 19.4425, MinusLogProbMetric: 19.4425, val_loss: 19.5619, val_MinusLogProbMetric: 19.5619

Epoch 179: val_loss did not improve from 19.50704
196/196 - 64s - loss: 19.4425 - MinusLogProbMetric: 19.4425 - val_loss: 19.5619 - val_MinusLogProbMetric: 19.5619 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 180/1000
2023-09-27 08:50:43.521 
Epoch 180/1000 
	 loss: 19.4664, MinusLogProbMetric: 19.4664, val_loss: 20.1945, val_MinusLogProbMetric: 20.1945

Epoch 180: val_loss did not improve from 19.50704
196/196 - 65s - loss: 19.4664 - MinusLogProbMetric: 19.4664 - val_loss: 20.1945 - val_MinusLogProbMetric: 20.1945 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 181/1000
2023-09-27 08:51:48.240 
Epoch 181/1000 
	 loss: 19.5779, MinusLogProbMetric: 19.5779, val_loss: 20.1138, val_MinusLogProbMetric: 20.1138

Epoch 181: val_loss did not improve from 19.50704
196/196 - 65s - loss: 19.5779 - MinusLogProbMetric: 19.5779 - val_loss: 20.1138 - val_MinusLogProbMetric: 20.1138 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 182/1000
2023-09-27 08:52:52.528 
Epoch 182/1000 
	 loss: 20.9056, MinusLogProbMetric: 20.9056, val_loss: 27.9835, val_MinusLogProbMetric: 27.9835

Epoch 182: val_loss did not improve from 19.50704
196/196 - 64s - loss: 20.9056 - MinusLogProbMetric: 20.9056 - val_loss: 27.9835 - val_MinusLogProbMetric: 27.9835 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 183/1000
2023-09-27 08:53:56.938 
Epoch 183/1000 
	 loss: 20.3082, MinusLogProbMetric: 20.3082, val_loss: 19.6121, val_MinusLogProbMetric: 19.6121

Epoch 183: val_loss did not improve from 19.50704
196/196 - 64s - loss: 20.3082 - MinusLogProbMetric: 20.3082 - val_loss: 19.6121 - val_MinusLogProbMetric: 19.6121 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 184/1000
2023-09-27 08:55:01.589 
Epoch 184/1000 
	 loss: 19.4956, MinusLogProbMetric: 19.4956, val_loss: 19.3093, val_MinusLogProbMetric: 19.3093

Epoch 184: val_loss improved from 19.50704 to 19.30932, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 19.4956 - MinusLogProbMetric: 19.4956 - val_loss: 19.3093 - val_MinusLogProbMetric: 19.3093 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 185/1000
2023-09-27 08:56:05.251 
Epoch 185/1000 
	 loss: 19.4456, MinusLogProbMetric: 19.4456, val_loss: 19.6231, val_MinusLogProbMetric: 19.6231

Epoch 185: val_loss did not improve from 19.30932
196/196 - 63s - loss: 19.4456 - MinusLogProbMetric: 19.4456 - val_loss: 19.6231 - val_MinusLogProbMetric: 19.6231 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 186/1000
2023-09-27 08:57:03.161 
Epoch 186/1000 
	 loss: 20.7608, MinusLogProbMetric: 20.7608, val_loss: 20.1003, val_MinusLogProbMetric: 20.1003

Epoch 186: val_loss did not improve from 19.30932
196/196 - 58s - loss: 20.7608 - MinusLogProbMetric: 20.7608 - val_loss: 20.1003 - val_MinusLogProbMetric: 20.1003 - lr: 3.3333e-04 - 58s/epoch - 295ms/step
Epoch 187/1000
2023-09-27 08:58:03.085 
Epoch 187/1000 
	 loss: 19.5562, MinusLogProbMetric: 19.5562, val_loss: 19.4075, val_MinusLogProbMetric: 19.4075

Epoch 187: val_loss did not improve from 19.30932
196/196 - 60s - loss: 19.5562 - MinusLogProbMetric: 19.5562 - val_loss: 19.4075 - val_MinusLogProbMetric: 19.4075 - lr: 3.3333e-04 - 60s/epoch - 306ms/step
Epoch 188/1000
2023-09-27 08:59:04.703 
Epoch 188/1000 
	 loss: 19.4061, MinusLogProbMetric: 19.4061, val_loss: 19.5056, val_MinusLogProbMetric: 19.5056

Epoch 188: val_loss did not improve from 19.30932
196/196 - 62s - loss: 19.4061 - MinusLogProbMetric: 19.4061 - val_loss: 19.5056 - val_MinusLogProbMetric: 19.5056 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 189/1000
2023-09-27 09:00:08.848 
Epoch 189/1000 
	 loss: 19.4028, MinusLogProbMetric: 19.4028, val_loss: 19.8285, val_MinusLogProbMetric: 19.8285

Epoch 189: val_loss did not improve from 19.30932
196/196 - 64s - loss: 19.4028 - MinusLogProbMetric: 19.4028 - val_loss: 19.8285 - val_MinusLogProbMetric: 19.8285 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 190/1000
2023-09-27 09:01:07.153 
Epoch 190/1000 
	 loss: 19.3542, MinusLogProbMetric: 19.3542, val_loss: 19.4823, val_MinusLogProbMetric: 19.4823

Epoch 190: val_loss did not improve from 19.30932
196/196 - 58s - loss: 19.3542 - MinusLogProbMetric: 19.3542 - val_loss: 19.4823 - val_MinusLogProbMetric: 19.4823 - lr: 3.3333e-04 - 58s/epoch - 297ms/step
Epoch 191/1000
2023-09-27 09:02:10.895 
Epoch 191/1000 
	 loss: 19.3181, MinusLogProbMetric: 19.3181, val_loss: 19.1565, val_MinusLogProbMetric: 19.1565

Epoch 191: val_loss improved from 19.30932 to 19.15654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 19.3181 - MinusLogProbMetric: 19.3181 - val_loss: 19.1565 - val_MinusLogProbMetric: 19.1565 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 192/1000
2023-09-27 09:03:13.144 
Epoch 192/1000 
	 loss: 19.2889, MinusLogProbMetric: 19.2889, val_loss: 19.3587, val_MinusLogProbMetric: 19.3587

Epoch 192: val_loss did not improve from 19.15654
196/196 - 61s - loss: 19.2889 - MinusLogProbMetric: 19.2889 - val_loss: 19.3587 - val_MinusLogProbMetric: 19.3587 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 193/1000
2023-09-27 09:04:17.403 
Epoch 193/1000 
	 loss: 19.3618, MinusLogProbMetric: 19.3618, val_loss: 20.2469, val_MinusLogProbMetric: 20.2469

Epoch 193: val_loss did not improve from 19.15654
196/196 - 64s - loss: 19.3618 - MinusLogProbMetric: 19.3618 - val_loss: 20.2469 - val_MinusLogProbMetric: 20.2469 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 194/1000
2023-09-27 09:05:21.410 
Epoch 194/1000 
	 loss: 19.4070, MinusLogProbMetric: 19.4070, val_loss: 19.4977, val_MinusLogProbMetric: 19.4977

Epoch 194: val_loss did not improve from 19.15654
196/196 - 64s - loss: 19.4070 - MinusLogProbMetric: 19.4070 - val_loss: 19.4977 - val_MinusLogProbMetric: 19.4977 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 195/1000
2023-09-27 09:06:25.348 
Epoch 195/1000 
	 loss: 19.2926, MinusLogProbMetric: 19.2926, val_loss: 19.1787, val_MinusLogProbMetric: 19.1787

Epoch 195: val_loss did not improve from 19.15654
196/196 - 64s - loss: 19.2926 - MinusLogProbMetric: 19.2926 - val_loss: 19.1787 - val_MinusLogProbMetric: 19.1787 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 196/1000
2023-09-27 09:07:29.521 
Epoch 196/1000 
	 loss: 19.3116, MinusLogProbMetric: 19.3116, val_loss: 19.3572, val_MinusLogProbMetric: 19.3572

Epoch 196: val_loss did not improve from 19.15654
196/196 - 64s - loss: 19.3116 - MinusLogProbMetric: 19.3116 - val_loss: 19.3572 - val_MinusLogProbMetric: 19.3572 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 197/1000
2023-09-27 09:08:33.609 
Epoch 197/1000 
	 loss: 19.2500, MinusLogProbMetric: 19.2500, val_loss: 19.7003, val_MinusLogProbMetric: 19.7003

Epoch 197: val_loss did not improve from 19.15654
196/196 - 64s - loss: 19.2500 - MinusLogProbMetric: 19.2500 - val_loss: 19.7003 - val_MinusLogProbMetric: 19.7003 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 198/1000
2023-09-27 09:09:37.907 
Epoch 198/1000 
	 loss: 20.6513, MinusLogProbMetric: 20.6513, val_loss: 19.1772, val_MinusLogProbMetric: 19.1772

Epoch 198: val_loss did not improve from 19.15654
196/196 - 64s - loss: 20.6513 - MinusLogProbMetric: 20.6513 - val_loss: 19.1772 - val_MinusLogProbMetric: 19.1772 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 199/1000
2023-09-27 09:10:42.137 
Epoch 199/1000 
	 loss: 19.2732, MinusLogProbMetric: 19.2732, val_loss: 19.3587, val_MinusLogProbMetric: 19.3587

Epoch 199: val_loss did not improve from 19.15654
196/196 - 64s - loss: 19.2732 - MinusLogProbMetric: 19.2732 - val_loss: 19.3587 - val_MinusLogProbMetric: 19.3587 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 200/1000
2023-09-27 09:11:46.648 
Epoch 200/1000 
	 loss: 19.2338, MinusLogProbMetric: 19.2338, val_loss: 19.3164, val_MinusLogProbMetric: 19.3164

Epoch 200: val_loss did not improve from 19.15654
196/196 - 65s - loss: 19.2338 - MinusLogProbMetric: 19.2338 - val_loss: 19.3164 - val_MinusLogProbMetric: 19.3164 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 201/1000
2023-09-27 09:12:51.013 
Epoch 201/1000 
	 loss: 19.3081, MinusLogProbMetric: 19.3081, val_loss: 19.7189, val_MinusLogProbMetric: 19.7189

Epoch 201: val_loss did not improve from 19.15654
196/196 - 64s - loss: 19.3081 - MinusLogProbMetric: 19.3081 - val_loss: 19.7189 - val_MinusLogProbMetric: 19.7189 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 202/1000
2023-09-27 09:13:54.715 
Epoch 202/1000 
	 loss: 19.2250, MinusLogProbMetric: 19.2250, val_loss: 19.4381, val_MinusLogProbMetric: 19.4381

Epoch 202: val_loss did not improve from 19.15654
196/196 - 64s - loss: 19.2250 - MinusLogProbMetric: 19.2250 - val_loss: 19.4381 - val_MinusLogProbMetric: 19.4381 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 203/1000
2023-09-27 09:14:58.963 
Epoch 203/1000 
	 loss: 19.1139, MinusLogProbMetric: 19.1139, val_loss: 19.6565, val_MinusLogProbMetric: 19.6565

Epoch 203: val_loss did not improve from 19.15654
196/196 - 64s - loss: 19.1139 - MinusLogProbMetric: 19.1139 - val_loss: 19.6565 - val_MinusLogProbMetric: 19.6565 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 204/1000
2023-09-27 09:16:03.649 
Epoch 204/1000 
	 loss: 19.2592, MinusLogProbMetric: 19.2592, val_loss: 19.5422, val_MinusLogProbMetric: 19.5422

Epoch 204: val_loss did not improve from 19.15654
196/196 - 65s - loss: 19.2592 - MinusLogProbMetric: 19.2592 - val_loss: 19.5422 - val_MinusLogProbMetric: 19.5422 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 205/1000
2023-09-27 09:17:07.337 
Epoch 205/1000 
	 loss: 19.1269, MinusLogProbMetric: 19.1269, val_loss: 19.7280, val_MinusLogProbMetric: 19.7280

Epoch 205: val_loss did not improve from 19.15654
196/196 - 64s - loss: 19.1269 - MinusLogProbMetric: 19.1269 - val_loss: 19.7280 - val_MinusLogProbMetric: 19.7280 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 206/1000
2023-09-27 09:18:08.311 
Epoch 206/1000 
	 loss: 19.2478, MinusLogProbMetric: 19.2478, val_loss: 19.4707, val_MinusLogProbMetric: 19.4707

Epoch 206: val_loss did not improve from 19.15654
196/196 - 61s - loss: 19.2478 - MinusLogProbMetric: 19.2478 - val_loss: 19.4707 - val_MinusLogProbMetric: 19.4707 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 207/1000
2023-09-27 09:19:07.350 
Epoch 207/1000 
	 loss: 19.2294, MinusLogProbMetric: 19.2294, val_loss: 19.8973, val_MinusLogProbMetric: 19.8973

Epoch 207: val_loss did not improve from 19.15654
196/196 - 59s - loss: 19.2294 - MinusLogProbMetric: 19.2294 - val_loss: 19.8973 - val_MinusLogProbMetric: 19.8973 - lr: 3.3333e-04 - 59s/epoch - 301ms/step
Epoch 208/1000
2023-09-27 09:20:11.118 
Epoch 208/1000 
	 loss: 19.3600, MinusLogProbMetric: 19.3600, val_loss: 20.0643, val_MinusLogProbMetric: 20.0643

Epoch 208: val_loss did not improve from 19.15654
196/196 - 64s - loss: 19.3600 - MinusLogProbMetric: 19.3600 - val_loss: 20.0643 - val_MinusLogProbMetric: 20.0643 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 209/1000
2023-09-27 09:21:15.302 
Epoch 209/1000 
	 loss: 19.1737, MinusLogProbMetric: 19.1737, val_loss: 19.1784, val_MinusLogProbMetric: 19.1784

Epoch 209: val_loss did not improve from 19.15654
196/196 - 64s - loss: 19.1737 - MinusLogProbMetric: 19.1737 - val_loss: 19.1784 - val_MinusLogProbMetric: 19.1784 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 210/1000
2023-09-27 09:22:16.012 
Epoch 210/1000 
	 loss: 19.1689, MinusLogProbMetric: 19.1689, val_loss: 19.3788, val_MinusLogProbMetric: 19.3788

Epoch 210: val_loss did not improve from 19.15654
196/196 - 61s - loss: 19.1689 - MinusLogProbMetric: 19.1689 - val_loss: 19.3788 - val_MinusLogProbMetric: 19.3788 - lr: 3.3333e-04 - 61s/epoch - 310ms/step
Epoch 211/1000
2023-09-27 09:23:15.043 
Epoch 211/1000 
	 loss: 19.0819, MinusLogProbMetric: 19.0819, val_loss: 19.4536, val_MinusLogProbMetric: 19.4536

Epoch 211: val_loss did not improve from 19.15654
196/196 - 59s - loss: 19.0819 - MinusLogProbMetric: 19.0819 - val_loss: 19.4536 - val_MinusLogProbMetric: 19.4536 - lr: 3.3333e-04 - 59s/epoch - 301ms/step
Epoch 212/1000
2023-09-27 09:24:10.368 
Epoch 212/1000 
	 loss: 19.0929, MinusLogProbMetric: 19.0929, val_loss: 19.2569, val_MinusLogProbMetric: 19.2569

Epoch 212: val_loss did not improve from 19.15654
196/196 - 55s - loss: 19.0929 - MinusLogProbMetric: 19.0929 - val_loss: 19.2569 - val_MinusLogProbMetric: 19.2569 - lr: 3.3333e-04 - 55s/epoch - 282ms/step
Epoch 213/1000
2023-09-27 09:25:10.689 
Epoch 213/1000 
	 loss: 19.1931, MinusLogProbMetric: 19.1931, val_loss: 19.3651, val_MinusLogProbMetric: 19.3651

Epoch 213: val_loss did not improve from 19.15654
196/196 - 60s - loss: 19.1931 - MinusLogProbMetric: 19.1931 - val_loss: 19.3651 - val_MinusLogProbMetric: 19.3651 - lr: 3.3333e-04 - 60s/epoch - 308ms/step
Epoch 214/1000
2023-09-27 09:26:09.693 
Epoch 214/1000 
	 loss: 19.1508, MinusLogProbMetric: 19.1508, val_loss: 19.5558, val_MinusLogProbMetric: 19.5558

Epoch 214: val_loss did not improve from 19.15654
196/196 - 59s - loss: 19.1508 - MinusLogProbMetric: 19.1508 - val_loss: 19.5558 - val_MinusLogProbMetric: 19.5558 - lr: 3.3333e-04 - 59s/epoch - 301ms/step
Epoch 215/1000
2023-09-27 09:27:05.797 
Epoch 215/1000 
	 loss: 19.1252, MinusLogProbMetric: 19.1252, val_loss: 19.1682, val_MinusLogProbMetric: 19.1682

Epoch 215: val_loss did not improve from 19.15654
196/196 - 56s - loss: 19.1252 - MinusLogProbMetric: 19.1252 - val_loss: 19.1682 - val_MinusLogProbMetric: 19.1682 - lr: 3.3333e-04 - 56s/epoch - 286ms/step
Epoch 216/1000
2023-09-27 09:28:04.565 
Epoch 216/1000 
	 loss: 19.0987, MinusLogProbMetric: 19.0987, val_loss: 19.5701, val_MinusLogProbMetric: 19.5701

Epoch 216: val_loss did not improve from 19.15654
196/196 - 59s - loss: 19.0987 - MinusLogProbMetric: 19.0987 - val_loss: 19.5701 - val_MinusLogProbMetric: 19.5701 - lr: 3.3333e-04 - 59s/epoch - 300ms/step
Epoch 217/1000
2023-09-27 09:29:08.607 
Epoch 217/1000 
	 loss: 19.1433, MinusLogProbMetric: 19.1433, val_loss: 19.4523, val_MinusLogProbMetric: 19.4523

Epoch 217: val_loss did not improve from 19.15654
196/196 - 64s - loss: 19.1433 - MinusLogProbMetric: 19.1433 - val_loss: 19.4523 - val_MinusLogProbMetric: 19.4523 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 218/1000
2023-09-27 09:30:11.830 
Epoch 218/1000 
	 loss: 19.0482, MinusLogProbMetric: 19.0482, val_loss: 19.7110, val_MinusLogProbMetric: 19.7110

Epoch 218: val_loss did not improve from 19.15654
196/196 - 63s - loss: 19.0482 - MinusLogProbMetric: 19.0482 - val_loss: 19.7110 - val_MinusLogProbMetric: 19.7110 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 219/1000
2023-09-27 09:31:15.841 
Epoch 219/1000 
	 loss: 19.2281, MinusLogProbMetric: 19.2281, val_loss: 19.1364, val_MinusLogProbMetric: 19.1364

Epoch 219: val_loss improved from 19.15654 to 19.13643, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 19.2281 - MinusLogProbMetric: 19.2281 - val_loss: 19.1364 - val_MinusLogProbMetric: 19.1364 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 220/1000
2023-09-27 09:32:20.744 
Epoch 220/1000 
	 loss: 19.0477, MinusLogProbMetric: 19.0477, val_loss: 20.0258, val_MinusLogProbMetric: 20.0258

Epoch 220: val_loss did not improve from 19.13643
196/196 - 64s - loss: 19.0477 - MinusLogProbMetric: 19.0477 - val_loss: 20.0258 - val_MinusLogProbMetric: 20.0258 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 221/1000
2023-09-27 09:33:24.827 
Epoch 221/1000 
	 loss: 19.1079, MinusLogProbMetric: 19.1079, val_loss: 19.0777, val_MinusLogProbMetric: 19.0777

Epoch 221: val_loss improved from 19.13643 to 19.07772, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 19.1079 - MinusLogProbMetric: 19.1079 - val_loss: 19.0777 - val_MinusLogProbMetric: 19.0777 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 222/1000
2023-09-27 09:34:29.460 
Epoch 222/1000 
	 loss: 19.0302, MinusLogProbMetric: 19.0302, val_loss: 19.0360, val_MinusLogProbMetric: 19.0360

Epoch 222: val_loss improved from 19.07772 to 19.03597, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 64s - loss: 19.0302 - MinusLogProbMetric: 19.0302 - val_loss: 19.0360 - val_MinusLogProbMetric: 19.0360 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 223/1000
2023-09-27 09:35:33.979 
Epoch 223/1000 
	 loss: 19.6614, MinusLogProbMetric: 19.6614, val_loss: 19.4033, val_MinusLogProbMetric: 19.4033

Epoch 223: val_loss did not improve from 19.03597
196/196 - 64s - loss: 19.6614 - MinusLogProbMetric: 19.6614 - val_loss: 19.4033 - val_MinusLogProbMetric: 19.4033 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 224/1000
2023-09-27 09:36:37.654 
Epoch 224/1000 
	 loss: 19.1517, MinusLogProbMetric: 19.1517, val_loss: 19.1033, val_MinusLogProbMetric: 19.1033

Epoch 224: val_loss did not improve from 19.03597
196/196 - 64s - loss: 19.1517 - MinusLogProbMetric: 19.1517 - val_loss: 19.1033 - val_MinusLogProbMetric: 19.1033 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 225/1000
2023-09-27 09:37:41.566 
Epoch 225/1000 
	 loss: 19.0830, MinusLogProbMetric: 19.0830, val_loss: 19.1349, val_MinusLogProbMetric: 19.1349

Epoch 225: val_loss did not improve from 19.03597
196/196 - 64s - loss: 19.0830 - MinusLogProbMetric: 19.0830 - val_loss: 19.1349 - val_MinusLogProbMetric: 19.1349 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 226/1000
2023-09-27 09:38:45.284 
Epoch 226/1000 
	 loss: 18.9708, MinusLogProbMetric: 18.9708, val_loss: 20.2388, val_MinusLogProbMetric: 20.2388

Epoch 226: val_loss did not improve from 19.03597
196/196 - 64s - loss: 18.9708 - MinusLogProbMetric: 18.9708 - val_loss: 20.2388 - val_MinusLogProbMetric: 20.2388 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 227/1000
2023-09-27 09:39:48.966 
Epoch 227/1000 
	 loss: 19.0637, MinusLogProbMetric: 19.0637, val_loss: 19.6717, val_MinusLogProbMetric: 19.6717

Epoch 227: val_loss did not improve from 19.03597
196/196 - 64s - loss: 19.0637 - MinusLogProbMetric: 19.0637 - val_loss: 19.6717 - val_MinusLogProbMetric: 19.6717 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 228/1000
2023-09-27 09:40:53.403 
Epoch 228/1000 
	 loss: 19.0160, MinusLogProbMetric: 19.0160, val_loss: 19.3952, val_MinusLogProbMetric: 19.3952

Epoch 228: val_loss did not improve from 19.03597
196/196 - 64s - loss: 19.0160 - MinusLogProbMetric: 19.0160 - val_loss: 19.3952 - val_MinusLogProbMetric: 19.3952 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 229/1000
2023-09-27 09:41:57.197 
Epoch 229/1000 
	 loss: 19.0824, MinusLogProbMetric: 19.0824, val_loss: 19.0667, val_MinusLogProbMetric: 19.0667

Epoch 229: val_loss did not improve from 19.03597
196/196 - 64s - loss: 19.0824 - MinusLogProbMetric: 19.0824 - val_loss: 19.0667 - val_MinusLogProbMetric: 19.0667 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 230/1000
2023-09-27 09:43:01.496 
Epoch 230/1000 
	 loss: 18.9501, MinusLogProbMetric: 18.9501, val_loss: 19.0974, val_MinusLogProbMetric: 19.0974

Epoch 230: val_loss did not improve from 19.03597
196/196 - 64s - loss: 18.9501 - MinusLogProbMetric: 18.9501 - val_loss: 19.0974 - val_MinusLogProbMetric: 19.0974 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 231/1000
2023-09-27 09:44:05.109 
Epoch 231/1000 
	 loss: 19.0289, MinusLogProbMetric: 19.0289, val_loss: 20.7500, val_MinusLogProbMetric: 20.7500

Epoch 231: val_loss did not improve from 19.03597
196/196 - 64s - loss: 19.0289 - MinusLogProbMetric: 19.0289 - val_loss: 20.7500 - val_MinusLogProbMetric: 20.7500 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 232/1000
2023-09-27 09:45:08.786 
Epoch 232/1000 
	 loss: 19.0898, MinusLogProbMetric: 19.0898, val_loss: 18.8735, val_MinusLogProbMetric: 18.8735

Epoch 232: val_loss improved from 19.03597 to 18.87349, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 64s - loss: 19.0898 - MinusLogProbMetric: 19.0898 - val_loss: 18.8735 - val_MinusLogProbMetric: 18.8735 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 233/1000
2023-09-27 09:46:13.684 
Epoch 233/1000 
	 loss: 18.9070, MinusLogProbMetric: 18.9070, val_loss: 18.9768, val_MinusLogProbMetric: 18.9768

Epoch 233: val_loss did not improve from 18.87349
196/196 - 64s - loss: 18.9070 - MinusLogProbMetric: 18.9070 - val_loss: 18.9768 - val_MinusLogProbMetric: 18.9768 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 234/1000
2023-09-27 09:47:17.349 
Epoch 234/1000 
	 loss: 18.9902, MinusLogProbMetric: 18.9902, val_loss: 19.1554, val_MinusLogProbMetric: 19.1554

Epoch 234: val_loss did not improve from 18.87349
196/196 - 64s - loss: 18.9902 - MinusLogProbMetric: 18.9902 - val_loss: 19.1554 - val_MinusLogProbMetric: 19.1554 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 235/1000
2023-09-27 09:48:21.432 
Epoch 235/1000 
	 loss: 18.9856, MinusLogProbMetric: 18.9856, val_loss: 18.9682, val_MinusLogProbMetric: 18.9682

Epoch 235: val_loss did not improve from 18.87349
196/196 - 64s - loss: 18.9856 - MinusLogProbMetric: 18.9856 - val_loss: 18.9682 - val_MinusLogProbMetric: 18.9682 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 236/1000
2023-09-27 09:49:25.368 
Epoch 236/1000 
	 loss: 19.1671, MinusLogProbMetric: 19.1671, val_loss: 20.3277, val_MinusLogProbMetric: 20.3277

Epoch 236: val_loss did not improve from 18.87349
196/196 - 64s - loss: 19.1671 - MinusLogProbMetric: 19.1671 - val_loss: 20.3277 - val_MinusLogProbMetric: 20.3277 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 237/1000
2023-09-27 09:50:28.898 
Epoch 237/1000 
	 loss: 19.0058, MinusLogProbMetric: 19.0058, val_loss: 18.8637, val_MinusLogProbMetric: 18.8637

Epoch 237: val_loss improved from 18.87349 to 18.86368, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 19.0058 - MinusLogProbMetric: 19.0058 - val_loss: 18.8637 - val_MinusLogProbMetric: 18.8637 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 238/1000
2023-09-27 09:51:33.644 
Epoch 238/1000 
	 loss: 18.9193, MinusLogProbMetric: 18.9193, val_loss: 19.3053, val_MinusLogProbMetric: 19.3053

Epoch 238: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.9193 - MinusLogProbMetric: 18.9193 - val_loss: 19.3053 - val_MinusLogProbMetric: 19.3053 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 239/1000
2023-09-27 09:52:37.977 
Epoch 239/1000 
	 loss: 19.0082, MinusLogProbMetric: 19.0082, val_loss: 18.8926, val_MinusLogProbMetric: 18.8926

Epoch 239: val_loss did not improve from 18.86368
196/196 - 64s - loss: 19.0082 - MinusLogProbMetric: 19.0082 - val_loss: 18.8926 - val_MinusLogProbMetric: 18.8926 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 240/1000
2023-09-27 09:53:41.922 
Epoch 240/1000 
	 loss: 18.9576, MinusLogProbMetric: 18.9576, val_loss: 19.1658, val_MinusLogProbMetric: 19.1658

Epoch 240: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.9576 - MinusLogProbMetric: 18.9576 - val_loss: 19.1658 - val_MinusLogProbMetric: 19.1658 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 241/1000
2023-09-27 09:54:46.291 
Epoch 241/1000 
	 loss: 18.9777, MinusLogProbMetric: 18.9777, val_loss: 19.3657, val_MinusLogProbMetric: 19.3657

Epoch 241: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.9777 - MinusLogProbMetric: 18.9777 - val_loss: 19.3657 - val_MinusLogProbMetric: 19.3657 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 242/1000
2023-09-27 09:55:50.892 
Epoch 242/1000 
	 loss: 18.9085, MinusLogProbMetric: 18.9085, val_loss: 19.2583, val_MinusLogProbMetric: 19.2583

Epoch 242: val_loss did not improve from 18.86368
196/196 - 65s - loss: 18.9085 - MinusLogProbMetric: 18.9085 - val_loss: 19.2583 - val_MinusLogProbMetric: 19.2583 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 243/1000
2023-09-27 09:56:54.928 
Epoch 243/1000 
	 loss: 18.9196, MinusLogProbMetric: 18.9196, val_loss: 18.8678, val_MinusLogProbMetric: 18.8678

Epoch 243: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.9196 - MinusLogProbMetric: 18.9196 - val_loss: 18.8678 - val_MinusLogProbMetric: 18.8678 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 244/1000
2023-09-27 09:57:58.698 
Epoch 244/1000 
	 loss: 18.9556, MinusLogProbMetric: 18.9556, val_loss: 18.9969, val_MinusLogProbMetric: 18.9969

Epoch 244: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.9556 - MinusLogProbMetric: 18.9556 - val_loss: 18.9969 - val_MinusLogProbMetric: 18.9969 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 245/1000
2023-09-27 09:59:02.917 
Epoch 245/1000 
	 loss: 18.9153, MinusLogProbMetric: 18.9153, val_loss: 18.8939, val_MinusLogProbMetric: 18.8939

Epoch 245: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.9153 - MinusLogProbMetric: 18.9153 - val_loss: 18.8939 - val_MinusLogProbMetric: 18.8939 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 246/1000
2023-09-27 10:00:06.947 
Epoch 246/1000 
	 loss: 18.9362, MinusLogProbMetric: 18.9362, val_loss: 19.6191, val_MinusLogProbMetric: 19.6191

Epoch 246: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.9362 - MinusLogProbMetric: 18.9362 - val_loss: 19.6191 - val_MinusLogProbMetric: 19.6191 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 247/1000
2023-09-27 10:01:11.370 
Epoch 247/1000 
	 loss: 18.8849, MinusLogProbMetric: 18.8849, val_loss: 19.6521, val_MinusLogProbMetric: 19.6521

Epoch 247: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.8849 - MinusLogProbMetric: 18.8849 - val_loss: 19.6521 - val_MinusLogProbMetric: 19.6521 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 248/1000
2023-09-27 10:02:15.352 
Epoch 248/1000 
	 loss: 18.9049, MinusLogProbMetric: 18.9049, val_loss: 19.2964, val_MinusLogProbMetric: 19.2964

Epoch 248: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.9049 - MinusLogProbMetric: 18.9049 - val_loss: 19.2964 - val_MinusLogProbMetric: 19.2964 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 249/1000
2023-09-27 10:03:19.449 
Epoch 249/1000 
	 loss: 18.8909, MinusLogProbMetric: 18.8909, val_loss: 19.7420, val_MinusLogProbMetric: 19.7420

Epoch 249: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.8909 - MinusLogProbMetric: 18.8909 - val_loss: 19.7420 - val_MinusLogProbMetric: 19.7420 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 250/1000
2023-09-27 10:04:23.380 
Epoch 250/1000 
	 loss: 18.9235, MinusLogProbMetric: 18.9235, val_loss: 18.9007, val_MinusLogProbMetric: 18.9007

Epoch 250: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.9235 - MinusLogProbMetric: 18.9235 - val_loss: 18.9007 - val_MinusLogProbMetric: 18.9007 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 251/1000
2023-09-27 10:05:27.190 
Epoch 251/1000 
	 loss: 18.8697, MinusLogProbMetric: 18.8697, val_loss: 19.6905, val_MinusLogProbMetric: 19.6905

Epoch 251: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.8697 - MinusLogProbMetric: 18.8697 - val_loss: 19.6905 - val_MinusLogProbMetric: 19.6905 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 252/1000
2023-09-27 10:06:28.603 
Epoch 252/1000 
	 loss: 18.8023, MinusLogProbMetric: 18.8023, val_loss: 19.7695, val_MinusLogProbMetric: 19.7695

Epoch 252: val_loss did not improve from 18.86368
196/196 - 61s - loss: 18.8023 - MinusLogProbMetric: 18.8023 - val_loss: 19.7695 - val_MinusLogProbMetric: 19.7695 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 253/1000
2023-09-27 10:07:27.950 
Epoch 253/1000 
	 loss: 18.9110, MinusLogProbMetric: 18.9110, val_loss: 19.1450, val_MinusLogProbMetric: 19.1450

Epoch 253: val_loss did not improve from 18.86368
196/196 - 59s - loss: 18.9110 - MinusLogProbMetric: 18.9110 - val_loss: 19.1450 - val_MinusLogProbMetric: 19.1450 - lr: 3.3333e-04 - 59s/epoch - 303ms/step
Epoch 254/1000
2023-09-27 10:08:28.166 
Epoch 254/1000 
	 loss: 18.9178, MinusLogProbMetric: 18.9178, val_loss: 18.8877, val_MinusLogProbMetric: 18.8877

Epoch 254: val_loss did not improve from 18.86368
196/196 - 60s - loss: 18.9178 - MinusLogProbMetric: 18.9178 - val_loss: 18.8877 - val_MinusLogProbMetric: 18.8877 - lr: 3.3333e-04 - 60s/epoch - 307ms/step
Epoch 255/1000
2023-09-27 10:09:32.793 
Epoch 255/1000 
	 loss: 18.7873, MinusLogProbMetric: 18.7873, val_loss: 19.9088, val_MinusLogProbMetric: 19.9088

Epoch 255: val_loss did not improve from 18.86368
196/196 - 65s - loss: 18.7873 - MinusLogProbMetric: 18.7873 - val_loss: 19.9088 - val_MinusLogProbMetric: 19.9088 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 256/1000
2023-09-27 10:10:36.595 
Epoch 256/1000 
	 loss: 18.8494, MinusLogProbMetric: 18.8494, val_loss: 19.4266, val_MinusLogProbMetric: 19.4266

Epoch 256: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.8494 - MinusLogProbMetric: 18.8494 - val_loss: 19.4266 - val_MinusLogProbMetric: 19.4266 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 257/1000
2023-09-27 10:11:40.546 
Epoch 257/1000 
	 loss: 19.1840, MinusLogProbMetric: 19.1840, val_loss: 18.9097, val_MinusLogProbMetric: 18.9097

Epoch 257: val_loss did not improve from 18.86368
196/196 - 64s - loss: 19.1840 - MinusLogProbMetric: 19.1840 - val_loss: 18.9097 - val_MinusLogProbMetric: 18.9097 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 258/1000
2023-09-27 10:12:44.328 
Epoch 258/1000 
	 loss: 18.8705, MinusLogProbMetric: 18.8705, val_loss: 18.9347, val_MinusLogProbMetric: 18.9347

Epoch 258: val_loss did not improve from 18.86368
196/196 - 64s - loss: 18.8705 - MinusLogProbMetric: 18.8705 - val_loss: 18.9347 - val_MinusLogProbMetric: 18.9347 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 259/1000
2023-09-27 10:13:48.198 
Epoch 259/1000 
	 loss: 18.8255, MinusLogProbMetric: 18.8255, val_loss: 18.8356, val_MinusLogProbMetric: 18.8356

Epoch 259: val_loss improved from 18.86368 to 18.83558, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 18.8255 - MinusLogProbMetric: 18.8255 - val_loss: 18.8356 - val_MinusLogProbMetric: 18.8356 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 260/1000
2023-09-27 10:14:53.383 
Epoch 260/1000 
	 loss: 18.8766, MinusLogProbMetric: 18.8766, val_loss: 19.0460, val_MinusLogProbMetric: 19.0460

Epoch 260: val_loss did not improve from 18.83558
196/196 - 64s - loss: 18.8766 - MinusLogProbMetric: 18.8766 - val_loss: 19.0460 - val_MinusLogProbMetric: 19.0460 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 261/1000
2023-09-27 10:15:57.818 
Epoch 261/1000 
	 loss: 18.7667, MinusLogProbMetric: 18.7667, val_loss: 19.9416, val_MinusLogProbMetric: 19.9416

Epoch 261: val_loss did not improve from 18.83558
196/196 - 64s - loss: 18.7667 - MinusLogProbMetric: 18.7667 - val_loss: 19.9416 - val_MinusLogProbMetric: 19.9416 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 262/1000
2023-09-27 10:17:01.529 
Epoch 262/1000 
	 loss: 18.7944, MinusLogProbMetric: 18.7944, val_loss: 18.8861, val_MinusLogProbMetric: 18.8861

Epoch 262: val_loss did not improve from 18.83558
196/196 - 64s - loss: 18.7944 - MinusLogProbMetric: 18.7944 - val_loss: 18.8861 - val_MinusLogProbMetric: 18.8861 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 263/1000
2023-09-27 10:18:05.862 
Epoch 263/1000 
	 loss: 18.7340, MinusLogProbMetric: 18.7340, val_loss: 19.0215, val_MinusLogProbMetric: 19.0215

Epoch 263: val_loss did not improve from 18.83558
196/196 - 64s - loss: 18.7340 - MinusLogProbMetric: 18.7340 - val_loss: 19.0215 - val_MinusLogProbMetric: 19.0215 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 264/1000
2023-09-27 10:19:10.409 
Epoch 264/1000 
	 loss: 18.8287, MinusLogProbMetric: 18.8287, val_loss: 18.8044, val_MinusLogProbMetric: 18.8044

Epoch 264: val_loss improved from 18.83558 to 18.80442, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 18.8287 - MinusLogProbMetric: 18.8287 - val_loss: 18.8044 - val_MinusLogProbMetric: 18.8044 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 265/1000
2023-09-27 10:20:15.233 
Epoch 265/1000 
	 loss: 18.8214, MinusLogProbMetric: 18.8214, val_loss: 19.2415, val_MinusLogProbMetric: 19.2415

Epoch 265: val_loss did not improve from 18.80442
196/196 - 64s - loss: 18.8214 - MinusLogProbMetric: 18.8214 - val_loss: 19.2415 - val_MinusLogProbMetric: 19.2415 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 266/1000
2023-09-27 10:21:18.963 
Epoch 266/1000 
	 loss: 18.7921, MinusLogProbMetric: 18.7921, val_loss: 19.2710, val_MinusLogProbMetric: 19.2710

Epoch 266: val_loss did not improve from 18.80442
196/196 - 64s - loss: 18.7921 - MinusLogProbMetric: 18.7921 - val_loss: 19.2710 - val_MinusLogProbMetric: 19.2710 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 267/1000
2023-09-27 10:22:23.213 
Epoch 267/1000 
	 loss: 18.8085, MinusLogProbMetric: 18.8085, val_loss: 19.2663, val_MinusLogProbMetric: 19.2663

Epoch 267: val_loss did not improve from 18.80442
196/196 - 64s - loss: 18.8085 - MinusLogProbMetric: 18.8085 - val_loss: 19.2663 - val_MinusLogProbMetric: 19.2663 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 268/1000
2023-09-27 10:23:26.697 
Epoch 268/1000 
	 loss: 18.7714, MinusLogProbMetric: 18.7714, val_loss: 18.8899, val_MinusLogProbMetric: 18.8899

Epoch 268: val_loss did not improve from 18.80442
196/196 - 63s - loss: 18.7714 - MinusLogProbMetric: 18.7714 - val_loss: 18.8899 - val_MinusLogProbMetric: 18.8899 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 269/1000
2023-09-27 10:24:30.713 
Epoch 269/1000 
	 loss: 18.7198, MinusLogProbMetric: 18.7198, val_loss: 18.7183, val_MinusLogProbMetric: 18.7183

Epoch 269: val_loss improved from 18.80442 to 18.71831, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 18.7198 - MinusLogProbMetric: 18.7198 - val_loss: 18.7183 - val_MinusLogProbMetric: 18.7183 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 270/1000
2023-09-27 10:25:35.781 
Epoch 270/1000 
	 loss: 18.7899, MinusLogProbMetric: 18.7899, val_loss: 19.0222, val_MinusLogProbMetric: 19.0222

Epoch 270: val_loss did not improve from 18.71831
196/196 - 64s - loss: 18.7899 - MinusLogProbMetric: 18.7899 - val_loss: 19.0222 - val_MinusLogProbMetric: 19.0222 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 271/1000
2023-09-27 10:26:39.627 
Epoch 271/1000 
	 loss: 18.7190, MinusLogProbMetric: 18.7190, val_loss: 19.2207, val_MinusLogProbMetric: 19.2207

Epoch 271: val_loss did not improve from 18.71831
196/196 - 64s - loss: 18.7190 - MinusLogProbMetric: 18.7190 - val_loss: 19.2207 - val_MinusLogProbMetric: 19.2207 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 272/1000
2023-09-27 10:27:43.263 
Epoch 272/1000 
	 loss: 18.9983, MinusLogProbMetric: 18.9983, val_loss: 20.0704, val_MinusLogProbMetric: 20.0704

Epoch 272: val_loss did not improve from 18.71831
196/196 - 64s - loss: 18.9983 - MinusLogProbMetric: 18.9983 - val_loss: 20.0704 - val_MinusLogProbMetric: 20.0704 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 273/1000
2023-09-27 10:28:47.811 
Epoch 273/1000 
	 loss: 18.7588, MinusLogProbMetric: 18.7588, val_loss: 19.1562, val_MinusLogProbMetric: 19.1562

Epoch 273: val_loss did not improve from 18.71831
196/196 - 65s - loss: 18.7588 - MinusLogProbMetric: 18.7588 - val_loss: 19.1562 - val_MinusLogProbMetric: 19.1562 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 274/1000
2023-09-27 10:29:52.141 
Epoch 274/1000 
	 loss: 18.7273, MinusLogProbMetric: 18.7273, val_loss: 18.5417, val_MinusLogProbMetric: 18.5417

Epoch 274: val_loss improved from 18.71831 to 18.54168, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 18.7273 - MinusLogProbMetric: 18.7273 - val_loss: 18.5417 - val_MinusLogProbMetric: 18.5417 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 275/1000
2023-09-27 10:30:57.106 
Epoch 275/1000 
	 loss: 18.6571, MinusLogProbMetric: 18.6571, val_loss: 18.8936, val_MinusLogProbMetric: 18.8936

Epoch 275: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.6571 - MinusLogProbMetric: 18.6571 - val_loss: 18.8936 - val_MinusLogProbMetric: 18.8936 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 276/1000
2023-09-27 10:32:00.416 
Epoch 276/1000 
	 loss: 18.7185, MinusLogProbMetric: 18.7185, val_loss: 18.8055, val_MinusLogProbMetric: 18.8055

Epoch 276: val_loss did not improve from 18.54168
196/196 - 63s - loss: 18.7185 - MinusLogProbMetric: 18.7185 - val_loss: 18.8055 - val_MinusLogProbMetric: 18.8055 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 277/1000
2023-09-27 10:33:04.575 
Epoch 277/1000 
	 loss: 18.7623, MinusLogProbMetric: 18.7623, val_loss: 18.9885, val_MinusLogProbMetric: 18.9885

Epoch 277: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.7623 - MinusLogProbMetric: 18.7623 - val_loss: 18.9885 - val_MinusLogProbMetric: 18.9885 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 278/1000
2023-09-27 10:34:08.347 
Epoch 278/1000 
	 loss: 18.7513, MinusLogProbMetric: 18.7513, val_loss: 18.9180, val_MinusLogProbMetric: 18.9180

Epoch 278: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.7513 - MinusLogProbMetric: 18.7513 - val_loss: 18.9180 - val_MinusLogProbMetric: 18.9180 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 279/1000
2023-09-27 10:35:12.665 
Epoch 279/1000 
	 loss: 18.6531, MinusLogProbMetric: 18.6531, val_loss: 18.6695, val_MinusLogProbMetric: 18.6695

Epoch 279: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.6531 - MinusLogProbMetric: 18.6531 - val_loss: 18.6695 - val_MinusLogProbMetric: 18.6695 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 280/1000
2023-09-27 10:36:16.493 
Epoch 280/1000 
	 loss: 18.7110, MinusLogProbMetric: 18.7110, val_loss: 19.1636, val_MinusLogProbMetric: 19.1636

Epoch 280: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.7110 - MinusLogProbMetric: 18.7110 - val_loss: 19.1636 - val_MinusLogProbMetric: 19.1636 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 281/1000
2023-09-27 10:37:20.580 
Epoch 281/1000 
	 loss: 18.7193, MinusLogProbMetric: 18.7193, val_loss: 18.5810, val_MinusLogProbMetric: 18.5810

Epoch 281: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.7193 - MinusLogProbMetric: 18.7193 - val_loss: 18.5810 - val_MinusLogProbMetric: 18.5810 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 282/1000
2023-09-27 10:38:24.526 
Epoch 282/1000 
	 loss: 18.7348, MinusLogProbMetric: 18.7348, val_loss: 18.6489, val_MinusLogProbMetric: 18.6489

Epoch 282: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.7348 - MinusLogProbMetric: 18.7348 - val_loss: 18.6489 - val_MinusLogProbMetric: 18.6489 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 283/1000
2023-09-27 10:39:28.659 
Epoch 283/1000 
	 loss: 18.6482, MinusLogProbMetric: 18.6482, val_loss: 18.7060, val_MinusLogProbMetric: 18.7060

Epoch 283: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.6482 - MinusLogProbMetric: 18.6482 - val_loss: 18.7060 - val_MinusLogProbMetric: 18.7060 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 284/1000
2023-09-27 10:40:32.897 
Epoch 284/1000 
	 loss: 18.6573, MinusLogProbMetric: 18.6573, val_loss: 18.8415, val_MinusLogProbMetric: 18.8415

Epoch 284: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.6573 - MinusLogProbMetric: 18.6573 - val_loss: 18.8415 - val_MinusLogProbMetric: 18.8415 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 285/1000
2023-09-27 10:41:36.882 
Epoch 285/1000 
	 loss: 18.6927, MinusLogProbMetric: 18.6927, val_loss: 18.9989, val_MinusLogProbMetric: 18.9989

Epoch 285: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.6927 - MinusLogProbMetric: 18.6927 - val_loss: 18.9989 - val_MinusLogProbMetric: 18.9989 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 286/1000
2023-09-27 10:42:41.130 
Epoch 286/1000 
	 loss: 18.6527, MinusLogProbMetric: 18.6527, val_loss: 18.8486, val_MinusLogProbMetric: 18.8486

Epoch 286: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.6527 - MinusLogProbMetric: 18.6527 - val_loss: 18.8486 - val_MinusLogProbMetric: 18.8486 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 287/1000
2023-09-27 10:43:45.079 
Epoch 287/1000 
	 loss: 18.7651, MinusLogProbMetric: 18.7651, val_loss: 18.6707, val_MinusLogProbMetric: 18.6707

Epoch 287: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.7651 - MinusLogProbMetric: 18.7651 - val_loss: 18.6707 - val_MinusLogProbMetric: 18.6707 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 288/1000
2023-09-27 10:44:49.133 
Epoch 288/1000 
	 loss: 18.6433, MinusLogProbMetric: 18.6433, val_loss: 18.8017, val_MinusLogProbMetric: 18.8017

Epoch 288: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.6433 - MinusLogProbMetric: 18.6433 - val_loss: 18.8017 - val_MinusLogProbMetric: 18.8017 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 289/1000
2023-09-27 10:45:53.308 
Epoch 289/1000 
	 loss: 18.6958, MinusLogProbMetric: 18.6958, val_loss: 18.8756, val_MinusLogProbMetric: 18.8756

Epoch 289: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.6958 - MinusLogProbMetric: 18.6958 - val_loss: 18.8756 - val_MinusLogProbMetric: 18.8756 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 290/1000
2023-09-27 10:46:57.354 
Epoch 290/1000 
	 loss: 18.6277, MinusLogProbMetric: 18.6277, val_loss: 18.6211, val_MinusLogProbMetric: 18.6211

Epoch 290: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.6277 - MinusLogProbMetric: 18.6277 - val_loss: 18.6211 - val_MinusLogProbMetric: 18.6211 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 291/1000
2023-09-27 10:48:01.244 
Epoch 291/1000 
	 loss: 18.6378, MinusLogProbMetric: 18.6378, val_loss: 19.6816, val_MinusLogProbMetric: 19.6816

Epoch 291: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.6378 - MinusLogProbMetric: 18.6378 - val_loss: 19.6816 - val_MinusLogProbMetric: 19.6816 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 292/1000
2023-09-27 10:49:05.252 
Epoch 292/1000 
	 loss: 18.7013, MinusLogProbMetric: 18.7013, val_loss: 19.0901, val_MinusLogProbMetric: 19.0901

Epoch 292: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.7013 - MinusLogProbMetric: 18.7013 - val_loss: 19.0901 - val_MinusLogProbMetric: 19.0901 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 293/1000
2023-09-27 10:50:09.125 
Epoch 293/1000 
	 loss: 18.6920, MinusLogProbMetric: 18.6920, val_loss: 18.5784, val_MinusLogProbMetric: 18.5784

Epoch 293: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.6920 - MinusLogProbMetric: 18.6920 - val_loss: 18.5784 - val_MinusLogProbMetric: 18.5784 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 294/1000
2023-09-27 10:51:13.211 
Epoch 294/1000 
	 loss: 18.6512, MinusLogProbMetric: 18.6512, val_loss: 18.9436, val_MinusLogProbMetric: 18.9436

Epoch 294: val_loss did not improve from 18.54168
196/196 - 64s - loss: 18.6512 - MinusLogProbMetric: 18.6512 - val_loss: 18.9436 - val_MinusLogProbMetric: 18.9436 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 295/1000
2023-09-27 10:52:17.356 
Epoch 295/1000 
	 loss: 18.7415, MinusLogProbMetric: 18.7415, val_loss: 18.5136, val_MinusLogProbMetric: 18.5136

Epoch 295: val_loss improved from 18.54168 to 18.51364, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 18.7415 - MinusLogProbMetric: 18.7415 - val_loss: 18.5136 - val_MinusLogProbMetric: 18.5136 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 296/1000
2023-09-27 10:53:22.176 
Epoch 296/1000 
	 loss: 18.6729, MinusLogProbMetric: 18.6729, val_loss: 18.7258, val_MinusLogProbMetric: 18.7258

Epoch 296: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.6729 - MinusLogProbMetric: 18.6729 - val_loss: 18.7258 - val_MinusLogProbMetric: 18.7258 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 297/1000
2023-09-27 10:54:26.347 
Epoch 297/1000 
	 loss: 18.6140, MinusLogProbMetric: 18.6140, val_loss: 18.8525, val_MinusLogProbMetric: 18.8525

Epoch 297: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.6140 - MinusLogProbMetric: 18.6140 - val_loss: 18.8525 - val_MinusLogProbMetric: 18.8525 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 298/1000
2023-09-27 10:55:30.407 
Epoch 298/1000 
	 loss: 18.6344, MinusLogProbMetric: 18.6344, val_loss: 19.4797, val_MinusLogProbMetric: 19.4797

Epoch 298: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.6344 - MinusLogProbMetric: 18.6344 - val_loss: 19.4797 - val_MinusLogProbMetric: 19.4797 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 299/1000
2023-09-27 10:56:34.377 
Epoch 299/1000 
	 loss: 18.5905, MinusLogProbMetric: 18.5905, val_loss: 18.5415, val_MinusLogProbMetric: 18.5415

Epoch 299: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.5905 - MinusLogProbMetric: 18.5905 - val_loss: 18.5415 - val_MinusLogProbMetric: 18.5415 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 300/1000
2023-09-27 10:57:38.079 
Epoch 300/1000 
	 loss: 18.7324, MinusLogProbMetric: 18.7324, val_loss: 18.8125, val_MinusLogProbMetric: 18.8125

Epoch 300: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.7324 - MinusLogProbMetric: 18.7324 - val_loss: 18.8125 - val_MinusLogProbMetric: 18.8125 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 301/1000
2023-09-27 10:58:41.900 
Epoch 301/1000 
	 loss: 18.6524, MinusLogProbMetric: 18.6524, val_loss: 18.5820, val_MinusLogProbMetric: 18.5820

Epoch 301: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.6524 - MinusLogProbMetric: 18.6524 - val_loss: 18.5820 - val_MinusLogProbMetric: 18.5820 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 302/1000
2023-09-27 10:59:46.256 
Epoch 302/1000 
	 loss: 18.5566, MinusLogProbMetric: 18.5566, val_loss: 18.9984, val_MinusLogProbMetric: 18.9984

Epoch 302: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.5566 - MinusLogProbMetric: 18.5566 - val_loss: 18.9984 - val_MinusLogProbMetric: 18.9984 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 303/1000
2023-09-27 11:00:50.444 
Epoch 303/1000 
	 loss: 18.6372, MinusLogProbMetric: 18.6372, val_loss: 18.7155, val_MinusLogProbMetric: 18.7155

Epoch 303: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.6372 - MinusLogProbMetric: 18.6372 - val_loss: 18.7155 - val_MinusLogProbMetric: 18.7155 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 304/1000
2023-09-27 11:01:54.694 
Epoch 304/1000 
	 loss: 18.5802, MinusLogProbMetric: 18.5802, val_loss: 18.7491, val_MinusLogProbMetric: 18.7491

Epoch 304: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.5802 - MinusLogProbMetric: 18.5802 - val_loss: 18.7491 - val_MinusLogProbMetric: 18.7491 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 305/1000
2023-09-27 11:02:58.943 
Epoch 305/1000 
	 loss: 18.5845, MinusLogProbMetric: 18.5845, val_loss: 19.0380, val_MinusLogProbMetric: 19.0380

Epoch 305: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.5845 - MinusLogProbMetric: 18.5845 - val_loss: 19.0380 - val_MinusLogProbMetric: 19.0380 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 306/1000
2023-09-27 11:04:03.403 
Epoch 306/1000 
	 loss: 18.5878, MinusLogProbMetric: 18.5878, val_loss: 18.6164, val_MinusLogProbMetric: 18.6164

Epoch 306: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.5878 - MinusLogProbMetric: 18.5878 - val_loss: 18.6164 - val_MinusLogProbMetric: 18.6164 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 307/1000
2023-09-27 11:05:07.700 
Epoch 307/1000 
	 loss: 18.6198, MinusLogProbMetric: 18.6198, val_loss: 19.2697, val_MinusLogProbMetric: 19.2697

Epoch 307: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.6198 - MinusLogProbMetric: 18.6198 - val_loss: 19.2697 - val_MinusLogProbMetric: 19.2697 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 308/1000
2023-09-27 11:06:11.461 
Epoch 308/1000 
	 loss: 18.5554, MinusLogProbMetric: 18.5554, val_loss: 18.7428, val_MinusLogProbMetric: 18.7428

Epoch 308: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.5554 - MinusLogProbMetric: 18.5554 - val_loss: 18.7428 - val_MinusLogProbMetric: 18.7428 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 309/1000
2023-09-27 11:07:15.239 
Epoch 309/1000 
	 loss: 18.6511, MinusLogProbMetric: 18.6511, val_loss: 19.3837, val_MinusLogProbMetric: 19.3837

Epoch 309: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.6511 - MinusLogProbMetric: 18.6511 - val_loss: 19.3837 - val_MinusLogProbMetric: 19.3837 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 310/1000
2023-09-27 11:08:18.818 
Epoch 310/1000 
	 loss: 18.5775, MinusLogProbMetric: 18.5775, val_loss: 18.6677, val_MinusLogProbMetric: 18.6677

Epoch 310: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.5775 - MinusLogProbMetric: 18.5775 - val_loss: 18.6677 - val_MinusLogProbMetric: 18.6677 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 311/1000
2023-09-27 11:09:23.106 
Epoch 311/1000 
	 loss: 18.6077, MinusLogProbMetric: 18.6077, val_loss: 18.6757, val_MinusLogProbMetric: 18.6757

Epoch 311: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.6077 - MinusLogProbMetric: 18.6077 - val_loss: 18.6757 - val_MinusLogProbMetric: 18.6757 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 312/1000
2023-09-27 11:10:26.469 
Epoch 312/1000 
	 loss: 18.5864, MinusLogProbMetric: 18.5864, val_loss: 18.7228, val_MinusLogProbMetric: 18.7228

Epoch 312: val_loss did not improve from 18.51364
196/196 - 63s - loss: 18.5864 - MinusLogProbMetric: 18.5864 - val_loss: 18.7228 - val_MinusLogProbMetric: 18.7228 - lr: 3.3333e-04 - 63s/epoch - 323ms/step
Epoch 313/1000
2023-09-27 11:11:28.009 
Epoch 313/1000 
	 loss: 18.5058, MinusLogProbMetric: 18.5058, val_loss: 18.6282, val_MinusLogProbMetric: 18.6282

Epoch 313: val_loss did not improve from 18.51364
196/196 - 62s - loss: 18.5058 - MinusLogProbMetric: 18.5058 - val_loss: 18.6282 - val_MinusLogProbMetric: 18.6282 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 314/1000
2023-09-27 11:12:31.745 
Epoch 314/1000 
	 loss: 18.5382, MinusLogProbMetric: 18.5382, val_loss: 18.9743, val_MinusLogProbMetric: 18.9743

Epoch 314: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.5382 - MinusLogProbMetric: 18.5382 - val_loss: 18.9743 - val_MinusLogProbMetric: 18.9743 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 315/1000
2023-09-27 11:13:35.721 
Epoch 315/1000 
	 loss: 18.5023, MinusLogProbMetric: 18.5023, val_loss: 18.7834, val_MinusLogProbMetric: 18.7834

Epoch 315: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.5023 - MinusLogProbMetric: 18.5023 - val_loss: 18.7834 - val_MinusLogProbMetric: 18.7834 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 316/1000
2023-09-27 11:14:39.808 
Epoch 316/1000 
	 loss: 18.5729, MinusLogProbMetric: 18.5729, val_loss: 18.8789, val_MinusLogProbMetric: 18.8789

Epoch 316: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.5729 - MinusLogProbMetric: 18.5729 - val_loss: 18.8789 - val_MinusLogProbMetric: 18.8789 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 317/1000
2023-09-27 11:15:43.961 
Epoch 317/1000 
	 loss: 18.5114, MinusLogProbMetric: 18.5114, val_loss: 18.5850, val_MinusLogProbMetric: 18.5850

Epoch 317: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.5114 - MinusLogProbMetric: 18.5114 - val_loss: 18.5850 - val_MinusLogProbMetric: 18.5850 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 318/1000
2023-09-27 11:16:47.929 
Epoch 318/1000 
	 loss: 18.6222, MinusLogProbMetric: 18.6222, val_loss: 18.5687, val_MinusLogProbMetric: 18.5687

Epoch 318: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.6222 - MinusLogProbMetric: 18.6222 - val_loss: 18.5687 - val_MinusLogProbMetric: 18.5687 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 319/1000
2023-09-27 11:17:52.044 
Epoch 319/1000 
	 loss: 18.5136, MinusLogProbMetric: 18.5136, val_loss: 18.5324, val_MinusLogProbMetric: 18.5324

Epoch 319: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.5136 - MinusLogProbMetric: 18.5136 - val_loss: 18.5324 - val_MinusLogProbMetric: 18.5324 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 320/1000
2023-09-27 11:18:56.372 
Epoch 320/1000 
	 loss: 18.4582, MinusLogProbMetric: 18.4582, val_loss: 18.7986, val_MinusLogProbMetric: 18.7986

Epoch 320: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.4582 - MinusLogProbMetric: 18.4582 - val_loss: 18.7986 - val_MinusLogProbMetric: 18.7986 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 321/1000
2023-09-27 11:20:01.613 
Epoch 321/1000 
	 loss: 18.5371, MinusLogProbMetric: 18.5371, val_loss: 18.5331, val_MinusLogProbMetric: 18.5331

Epoch 321: val_loss did not improve from 18.51364
196/196 - 65s - loss: 18.5371 - MinusLogProbMetric: 18.5371 - val_loss: 18.5331 - val_MinusLogProbMetric: 18.5331 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 322/1000
2023-09-27 11:21:05.875 
Epoch 322/1000 
	 loss: 18.5796, MinusLogProbMetric: 18.5796, val_loss: 19.3720, val_MinusLogProbMetric: 19.3720

Epoch 322: val_loss did not improve from 18.51364
196/196 - 64s - loss: 18.5796 - MinusLogProbMetric: 18.5796 - val_loss: 19.3720 - val_MinusLogProbMetric: 19.3720 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 323/1000
2023-09-27 11:22:10.004 
Epoch 323/1000 
	 loss: 18.4708, MinusLogProbMetric: 18.4708, val_loss: 18.5121, val_MinusLogProbMetric: 18.5121

Epoch 323: val_loss improved from 18.51364 to 18.51207, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 18.4708 - MinusLogProbMetric: 18.4708 - val_loss: 18.5121 - val_MinusLogProbMetric: 18.5121 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 324/1000
2023-09-27 11:23:15.337 
Epoch 324/1000 
	 loss: 18.5171, MinusLogProbMetric: 18.5171, val_loss: 18.6201, val_MinusLogProbMetric: 18.6201

Epoch 324: val_loss did not improve from 18.51207
196/196 - 64s - loss: 18.5171 - MinusLogProbMetric: 18.5171 - val_loss: 18.6201 - val_MinusLogProbMetric: 18.6201 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 325/1000
2023-09-27 11:24:19.270 
Epoch 325/1000 
	 loss: 18.5332, MinusLogProbMetric: 18.5332, val_loss: 18.6674, val_MinusLogProbMetric: 18.6674

Epoch 325: val_loss did not improve from 18.51207
196/196 - 64s - loss: 18.5332 - MinusLogProbMetric: 18.5332 - val_loss: 18.6674 - val_MinusLogProbMetric: 18.6674 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 326/1000
2023-09-27 11:25:23.037 
Epoch 326/1000 
	 loss: 18.4947, MinusLogProbMetric: 18.4947, val_loss: 18.5020, val_MinusLogProbMetric: 18.5020

Epoch 326: val_loss improved from 18.51207 to 18.50203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 18.4947 - MinusLogProbMetric: 18.4947 - val_loss: 18.5020 - val_MinusLogProbMetric: 18.5020 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 327/1000
2023-09-27 11:26:27.955 
Epoch 327/1000 
	 loss: 18.5371, MinusLogProbMetric: 18.5371, val_loss: 18.7860, val_MinusLogProbMetric: 18.7860

Epoch 327: val_loss did not improve from 18.50203
196/196 - 64s - loss: 18.5371 - MinusLogProbMetric: 18.5371 - val_loss: 18.7860 - val_MinusLogProbMetric: 18.7860 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 328/1000
2023-09-27 11:27:32.318 
Epoch 328/1000 
	 loss: 18.5342, MinusLogProbMetric: 18.5342, val_loss: 19.1573, val_MinusLogProbMetric: 19.1573

Epoch 328: val_loss did not improve from 18.50203
196/196 - 64s - loss: 18.5342 - MinusLogProbMetric: 18.5342 - val_loss: 19.1573 - val_MinusLogProbMetric: 19.1573 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 329/1000
2023-09-27 11:28:36.513 
Epoch 329/1000 
	 loss: 18.4546, MinusLogProbMetric: 18.4546, val_loss: 18.6707, val_MinusLogProbMetric: 18.6707

Epoch 329: val_loss did not improve from 18.50203
196/196 - 64s - loss: 18.4546 - MinusLogProbMetric: 18.4546 - val_loss: 18.6707 - val_MinusLogProbMetric: 18.6707 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 330/1000
2023-09-27 11:29:40.700 
Epoch 330/1000 
	 loss: 18.5241, MinusLogProbMetric: 18.5241, val_loss: 18.5163, val_MinusLogProbMetric: 18.5163

Epoch 330: val_loss did not improve from 18.50203
196/196 - 64s - loss: 18.5241 - MinusLogProbMetric: 18.5241 - val_loss: 18.5163 - val_MinusLogProbMetric: 18.5163 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 331/1000
2023-09-27 11:30:44.490 
Epoch 331/1000 
	 loss: 18.4956, MinusLogProbMetric: 18.4956, val_loss: 18.4209, val_MinusLogProbMetric: 18.4209

Epoch 331: val_loss improved from 18.50203 to 18.42090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 18.4956 - MinusLogProbMetric: 18.4956 - val_loss: 18.4209 - val_MinusLogProbMetric: 18.4209 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 332/1000
2023-09-27 11:31:49.584 
Epoch 332/1000 
	 loss: 18.4730, MinusLogProbMetric: 18.4730, val_loss: 21.0102, val_MinusLogProbMetric: 21.0102

Epoch 332: val_loss did not improve from 18.42090
196/196 - 64s - loss: 18.4730 - MinusLogProbMetric: 18.4730 - val_loss: 21.0102 - val_MinusLogProbMetric: 21.0102 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 333/1000
2023-09-27 11:32:54.153 
Epoch 333/1000 
	 loss: 18.4516, MinusLogProbMetric: 18.4516, val_loss: 18.7761, val_MinusLogProbMetric: 18.7761

Epoch 333: val_loss did not improve from 18.42090
196/196 - 65s - loss: 18.4516 - MinusLogProbMetric: 18.4516 - val_loss: 18.7761 - val_MinusLogProbMetric: 18.7761 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 334/1000
2023-09-27 11:33:57.959 
Epoch 334/1000 
	 loss: 18.4763, MinusLogProbMetric: 18.4763, val_loss: 19.0000, val_MinusLogProbMetric: 19.0000

Epoch 334: val_loss did not improve from 18.42090
196/196 - 64s - loss: 18.4763 - MinusLogProbMetric: 18.4763 - val_loss: 19.0000 - val_MinusLogProbMetric: 19.0000 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 335/1000
2023-09-27 11:35:02.037 
Epoch 335/1000 
	 loss: 18.5013, MinusLogProbMetric: 18.5013, val_loss: 19.0118, val_MinusLogProbMetric: 19.0118

Epoch 335: val_loss did not improve from 18.42090
196/196 - 64s - loss: 18.5013 - MinusLogProbMetric: 18.5013 - val_loss: 19.0118 - val_MinusLogProbMetric: 19.0118 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 336/1000
2023-09-27 11:36:06.503 
Epoch 336/1000 
	 loss: 18.4852, MinusLogProbMetric: 18.4852, val_loss: 18.3833, val_MinusLogProbMetric: 18.3833

Epoch 336: val_loss improved from 18.42090 to 18.38327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 18.4852 - MinusLogProbMetric: 18.4852 - val_loss: 18.3833 - val_MinusLogProbMetric: 18.3833 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 337/1000
2023-09-27 11:37:11.301 
Epoch 337/1000 
	 loss: 18.4521, MinusLogProbMetric: 18.4521, val_loss: 18.4942, val_MinusLogProbMetric: 18.4942

Epoch 337: val_loss did not improve from 18.38327
196/196 - 64s - loss: 18.4521 - MinusLogProbMetric: 18.4521 - val_loss: 18.4942 - val_MinusLogProbMetric: 18.4942 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 338/1000
2023-09-27 11:38:15.336 
Epoch 338/1000 
	 loss: 18.4261, MinusLogProbMetric: 18.4261, val_loss: 18.4961, val_MinusLogProbMetric: 18.4961

Epoch 338: val_loss did not improve from 18.38327
196/196 - 64s - loss: 18.4261 - MinusLogProbMetric: 18.4261 - val_loss: 18.4961 - val_MinusLogProbMetric: 18.4961 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 339/1000
2023-09-27 11:39:19.516 
Epoch 339/1000 
	 loss: 18.4834, MinusLogProbMetric: 18.4834, val_loss: 18.7612, val_MinusLogProbMetric: 18.7612

Epoch 339: val_loss did not improve from 18.38327
196/196 - 64s - loss: 18.4834 - MinusLogProbMetric: 18.4834 - val_loss: 18.7612 - val_MinusLogProbMetric: 18.7612 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 340/1000
2023-09-27 11:40:23.793 
Epoch 340/1000 
	 loss: 18.4664, MinusLogProbMetric: 18.4664, val_loss: 18.7948, val_MinusLogProbMetric: 18.7948

Epoch 340: val_loss did not improve from 18.38327
196/196 - 64s - loss: 18.4664 - MinusLogProbMetric: 18.4664 - val_loss: 18.7948 - val_MinusLogProbMetric: 18.7948 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 341/1000
2023-09-27 11:41:28.669 
Epoch 341/1000 
	 loss: 18.5451, MinusLogProbMetric: 18.5451, val_loss: 18.5102, val_MinusLogProbMetric: 18.5102

Epoch 341: val_loss did not improve from 18.38327
196/196 - 65s - loss: 18.5451 - MinusLogProbMetric: 18.5451 - val_loss: 18.5102 - val_MinusLogProbMetric: 18.5102 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 342/1000
2023-09-27 11:42:32.653 
Epoch 342/1000 
	 loss: 18.4773, MinusLogProbMetric: 18.4773, val_loss: 18.4248, val_MinusLogProbMetric: 18.4248

Epoch 342: val_loss did not improve from 18.38327
196/196 - 64s - loss: 18.4773 - MinusLogProbMetric: 18.4773 - val_loss: 18.4248 - val_MinusLogProbMetric: 18.4248 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 343/1000
2023-09-27 11:43:36.453 
Epoch 343/1000 
	 loss: 18.4364, MinusLogProbMetric: 18.4364, val_loss: 18.4014, val_MinusLogProbMetric: 18.4014

Epoch 343: val_loss did not improve from 18.38327
196/196 - 64s - loss: 18.4364 - MinusLogProbMetric: 18.4364 - val_loss: 18.4014 - val_MinusLogProbMetric: 18.4014 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 344/1000
2023-09-27 11:44:40.251 
Epoch 344/1000 
	 loss: 18.5756, MinusLogProbMetric: 18.5756, val_loss: 18.8923, val_MinusLogProbMetric: 18.8923

Epoch 344: val_loss did not improve from 18.38327
196/196 - 64s - loss: 18.5756 - MinusLogProbMetric: 18.5756 - val_loss: 18.8923 - val_MinusLogProbMetric: 18.8923 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 345/1000
2023-09-27 11:45:43.880 
Epoch 345/1000 
	 loss: 18.4069, MinusLogProbMetric: 18.4069, val_loss: 18.7824, val_MinusLogProbMetric: 18.7824

Epoch 345: val_loss did not improve from 18.38327
196/196 - 64s - loss: 18.4069 - MinusLogProbMetric: 18.4069 - val_loss: 18.7824 - val_MinusLogProbMetric: 18.7824 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 346/1000
2023-09-27 11:46:48.144 
Epoch 346/1000 
	 loss: 18.4798, MinusLogProbMetric: 18.4798, val_loss: 20.3695, val_MinusLogProbMetric: 20.3695

Epoch 346: val_loss did not improve from 18.38327
196/196 - 64s - loss: 18.4798 - MinusLogProbMetric: 18.4798 - val_loss: 20.3695 - val_MinusLogProbMetric: 20.3695 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 347/1000
2023-09-27 11:47:52.257 
Epoch 347/1000 
	 loss: 18.4309, MinusLogProbMetric: 18.4309, val_loss: 18.6057, val_MinusLogProbMetric: 18.6057

Epoch 347: val_loss did not improve from 18.38327
196/196 - 64s - loss: 18.4309 - MinusLogProbMetric: 18.4309 - val_loss: 18.6057 - val_MinusLogProbMetric: 18.6057 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 348/1000
2023-09-27 11:48:56.986 
Epoch 348/1000 
	 loss: 18.4593, MinusLogProbMetric: 18.4593, val_loss: 18.7009, val_MinusLogProbMetric: 18.7009

Epoch 348: val_loss did not improve from 18.38327
196/196 - 65s - loss: 18.4593 - MinusLogProbMetric: 18.4593 - val_loss: 18.7009 - val_MinusLogProbMetric: 18.7009 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 349/1000
2023-09-27 11:50:01.583 
Epoch 349/1000 
	 loss: 18.3629, MinusLogProbMetric: 18.3629, val_loss: 18.2291, val_MinusLogProbMetric: 18.2291

Epoch 349: val_loss improved from 18.38327 to 18.22914, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 18.3629 - MinusLogProbMetric: 18.3629 - val_loss: 18.2291 - val_MinusLogProbMetric: 18.2291 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 350/1000
2023-09-27 11:51:06.651 
Epoch 350/1000 
	 loss: 18.4210, MinusLogProbMetric: 18.4210, val_loss: 19.5374, val_MinusLogProbMetric: 19.5374

Epoch 350: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.4210 - MinusLogProbMetric: 18.4210 - val_loss: 19.5374 - val_MinusLogProbMetric: 19.5374 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 351/1000
2023-09-27 11:52:11.529 
Epoch 351/1000 
	 loss: 18.3695, MinusLogProbMetric: 18.3695, val_loss: 18.3534, val_MinusLogProbMetric: 18.3534

Epoch 351: val_loss did not improve from 18.22914
196/196 - 65s - loss: 18.3695 - MinusLogProbMetric: 18.3695 - val_loss: 18.3534 - val_MinusLogProbMetric: 18.3534 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 352/1000
2023-09-27 11:53:15.807 
Epoch 352/1000 
	 loss: 18.4460, MinusLogProbMetric: 18.4460, val_loss: 18.6760, val_MinusLogProbMetric: 18.6760

Epoch 352: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.4460 - MinusLogProbMetric: 18.4460 - val_loss: 18.6760 - val_MinusLogProbMetric: 18.6760 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 353/1000
2023-09-27 11:54:20.419 
Epoch 353/1000 
	 loss: 18.4389, MinusLogProbMetric: 18.4389, val_loss: 18.3868, val_MinusLogProbMetric: 18.3868

Epoch 353: val_loss did not improve from 18.22914
196/196 - 65s - loss: 18.4389 - MinusLogProbMetric: 18.4389 - val_loss: 18.3868 - val_MinusLogProbMetric: 18.3868 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 354/1000
2023-09-27 11:55:24.830 
Epoch 354/1000 
	 loss: 18.4172, MinusLogProbMetric: 18.4172, val_loss: 18.7997, val_MinusLogProbMetric: 18.7997

Epoch 354: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.4172 - MinusLogProbMetric: 18.4172 - val_loss: 18.7997 - val_MinusLogProbMetric: 18.7997 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 355/1000
2023-09-27 11:56:29.394 
Epoch 355/1000 
	 loss: 18.3479, MinusLogProbMetric: 18.3479, val_loss: 18.9084, val_MinusLogProbMetric: 18.9084

Epoch 355: val_loss did not improve from 18.22914
196/196 - 65s - loss: 18.3479 - MinusLogProbMetric: 18.3479 - val_loss: 18.9084 - val_MinusLogProbMetric: 18.9084 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 356/1000
2023-09-27 11:57:33.289 
Epoch 356/1000 
	 loss: 18.4494, MinusLogProbMetric: 18.4494, val_loss: 18.3057, val_MinusLogProbMetric: 18.3057

Epoch 356: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.4494 - MinusLogProbMetric: 18.4494 - val_loss: 18.3057 - val_MinusLogProbMetric: 18.3057 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 357/1000
2023-09-27 11:58:37.852 
Epoch 357/1000 
	 loss: 18.4823, MinusLogProbMetric: 18.4823, val_loss: 18.3264, val_MinusLogProbMetric: 18.3264

Epoch 357: val_loss did not improve from 18.22914
196/196 - 65s - loss: 18.4823 - MinusLogProbMetric: 18.4823 - val_loss: 18.3264 - val_MinusLogProbMetric: 18.3264 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 358/1000
2023-09-27 11:59:42.856 
Epoch 358/1000 
	 loss: 18.3930, MinusLogProbMetric: 18.3930, val_loss: 18.5615, val_MinusLogProbMetric: 18.5615

Epoch 358: val_loss did not improve from 18.22914
196/196 - 65s - loss: 18.3930 - MinusLogProbMetric: 18.3930 - val_loss: 18.5615 - val_MinusLogProbMetric: 18.5615 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 359/1000
2023-09-27 12:00:46.874 
Epoch 359/1000 
	 loss: 18.3562, MinusLogProbMetric: 18.3562, val_loss: 18.2315, val_MinusLogProbMetric: 18.2315

Epoch 359: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.3562 - MinusLogProbMetric: 18.3562 - val_loss: 18.2315 - val_MinusLogProbMetric: 18.2315 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 360/1000
2023-09-27 12:01:51.409 
Epoch 360/1000 
	 loss: 18.3793, MinusLogProbMetric: 18.3793, val_loss: 18.4063, val_MinusLogProbMetric: 18.4063

Epoch 360: val_loss did not improve from 18.22914
196/196 - 65s - loss: 18.3793 - MinusLogProbMetric: 18.3793 - val_loss: 18.4063 - val_MinusLogProbMetric: 18.4063 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 361/1000
2023-09-27 12:02:56.131 
Epoch 361/1000 
	 loss: 18.4086, MinusLogProbMetric: 18.4086, val_loss: 18.5809, val_MinusLogProbMetric: 18.5809

Epoch 361: val_loss did not improve from 18.22914
196/196 - 65s - loss: 18.4086 - MinusLogProbMetric: 18.4086 - val_loss: 18.5809 - val_MinusLogProbMetric: 18.5809 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 362/1000
2023-09-27 12:04:00.172 
Epoch 362/1000 
	 loss: 18.4149, MinusLogProbMetric: 18.4149, val_loss: 19.2112, val_MinusLogProbMetric: 19.2112

Epoch 362: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.4149 - MinusLogProbMetric: 18.4149 - val_loss: 19.2112 - val_MinusLogProbMetric: 19.2112 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 363/1000
2023-09-27 12:05:04.256 
Epoch 363/1000 
	 loss: 18.4288, MinusLogProbMetric: 18.4288, val_loss: 18.4999, val_MinusLogProbMetric: 18.4999

Epoch 363: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.4288 - MinusLogProbMetric: 18.4288 - val_loss: 18.4999 - val_MinusLogProbMetric: 18.4999 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 364/1000
2023-09-27 12:06:08.713 
Epoch 364/1000 
	 loss: 18.3840, MinusLogProbMetric: 18.3840, val_loss: 19.2458, val_MinusLogProbMetric: 19.2458

Epoch 364: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.3840 - MinusLogProbMetric: 18.3840 - val_loss: 19.2458 - val_MinusLogProbMetric: 19.2458 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 365/1000
2023-09-27 12:07:13.745 
Epoch 365/1000 
	 loss: 18.4272, MinusLogProbMetric: 18.4272, val_loss: 18.4537, val_MinusLogProbMetric: 18.4537

Epoch 365: val_loss did not improve from 18.22914
196/196 - 65s - loss: 18.4272 - MinusLogProbMetric: 18.4272 - val_loss: 18.4537 - val_MinusLogProbMetric: 18.4537 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 366/1000
2023-09-27 12:08:17.905 
Epoch 366/1000 
	 loss: 18.3867, MinusLogProbMetric: 18.3867, val_loss: 18.8591, val_MinusLogProbMetric: 18.8591

Epoch 366: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.3867 - MinusLogProbMetric: 18.3867 - val_loss: 18.8591 - val_MinusLogProbMetric: 18.8591 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 367/1000
2023-09-27 12:09:21.807 
Epoch 367/1000 
	 loss: 18.4868, MinusLogProbMetric: 18.4868, val_loss: 19.3027, val_MinusLogProbMetric: 19.3027

Epoch 367: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.4868 - MinusLogProbMetric: 18.4868 - val_loss: 19.3027 - val_MinusLogProbMetric: 19.3027 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 368/1000
2023-09-27 12:10:26.029 
Epoch 368/1000 
	 loss: 18.3495, MinusLogProbMetric: 18.3495, val_loss: 18.4359, val_MinusLogProbMetric: 18.4359

Epoch 368: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.3495 - MinusLogProbMetric: 18.3495 - val_loss: 18.4359 - val_MinusLogProbMetric: 18.4359 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 369/1000
2023-09-27 12:11:30.242 
Epoch 369/1000 
	 loss: 18.2887, MinusLogProbMetric: 18.2887, val_loss: 18.7925, val_MinusLogProbMetric: 18.7925

Epoch 369: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.2887 - MinusLogProbMetric: 18.2887 - val_loss: 18.7925 - val_MinusLogProbMetric: 18.7925 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 370/1000
2023-09-27 12:12:34.638 
Epoch 370/1000 
	 loss: 18.3428, MinusLogProbMetric: 18.3428, val_loss: 19.8966, val_MinusLogProbMetric: 19.8966

Epoch 370: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.3428 - MinusLogProbMetric: 18.3428 - val_loss: 19.8966 - val_MinusLogProbMetric: 19.8966 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 371/1000
2023-09-27 12:13:38.686 
Epoch 371/1000 
	 loss: 18.3911, MinusLogProbMetric: 18.3911, val_loss: 18.5827, val_MinusLogProbMetric: 18.5827

Epoch 371: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.3911 - MinusLogProbMetric: 18.3911 - val_loss: 18.5827 - val_MinusLogProbMetric: 18.5827 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 372/1000
2023-09-27 12:14:42.875 
Epoch 372/1000 
	 loss: 18.4235, MinusLogProbMetric: 18.4235, val_loss: 18.9031, val_MinusLogProbMetric: 18.9031

Epoch 372: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.4235 - MinusLogProbMetric: 18.4235 - val_loss: 18.9031 - val_MinusLogProbMetric: 18.9031 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 373/1000
2023-09-27 12:15:47.052 
Epoch 373/1000 
	 loss: 18.4022, MinusLogProbMetric: 18.4022, val_loss: 18.9165, val_MinusLogProbMetric: 18.9165

Epoch 373: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.4022 - MinusLogProbMetric: 18.4022 - val_loss: 18.9165 - val_MinusLogProbMetric: 18.9165 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 374/1000
2023-09-27 12:16:51.124 
Epoch 374/1000 
	 loss: 18.4070, MinusLogProbMetric: 18.4070, val_loss: 18.5019, val_MinusLogProbMetric: 18.5019

Epoch 374: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.4070 - MinusLogProbMetric: 18.4070 - val_loss: 18.5019 - val_MinusLogProbMetric: 18.5019 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 375/1000
2023-09-27 12:17:55.097 
Epoch 375/1000 
	 loss: 18.3763, MinusLogProbMetric: 18.3763, val_loss: 18.3701, val_MinusLogProbMetric: 18.3701

Epoch 375: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.3763 - MinusLogProbMetric: 18.3763 - val_loss: 18.3701 - val_MinusLogProbMetric: 18.3701 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 376/1000
2023-09-27 12:18:59.271 
Epoch 376/1000 
	 loss: 18.3001, MinusLogProbMetric: 18.3001, val_loss: 18.7453, val_MinusLogProbMetric: 18.7453

Epoch 376: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.3001 - MinusLogProbMetric: 18.3001 - val_loss: 18.7453 - val_MinusLogProbMetric: 18.7453 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 377/1000
2023-09-27 12:20:03.754 
Epoch 377/1000 
	 loss: 18.3546, MinusLogProbMetric: 18.3546, val_loss: 18.5033, val_MinusLogProbMetric: 18.5033

Epoch 377: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.3546 - MinusLogProbMetric: 18.3546 - val_loss: 18.5033 - val_MinusLogProbMetric: 18.5033 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 378/1000
2023-09-27 12:21:07.651 
Epoch 378/1000 
	 loss: 18.3542, MinusLogProbMetric: 18.3542, val_loss: 18.4887, val_MinusLogProbMetric: 18.4887

Epoch 378: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.3542 - MinusLogProbMetric: 18.3542 - val_loss: 18.4887 - val_MinusLogProbMetric: 18.4887 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 379/1000
2023-09-27 12:22:11.807 
Epoch 379/1000 
	 loss: 18.2817, MinusLogProbMetric: 18.2817, val_loss: 18.4467, val_MinusLogProbMetric: 18.4467

Epoch 379: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.2817 - MinusLogProbMetric: 18.2817 - val_loss: 18.4467 - val_MinusLogProbMetric: 18.4467 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 380/1000
2023-09-27 12:23:15.410 
Epoch 380/1000 
	 loss: 18.3567, MinusLogProbMetric: 18.3567, val_loss: 18.4416, val_MinusLogProbMetric: 18.4416

Epoch 380: val_loss did not improve from 18.22914
196/196 - 64s - loss: 18.3567 - MinusLogProbMetric: 18.3567 - val_loss: 18.4416 - val_MinusLogProbMetric: 18.4416 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 381/1000
2023-09-27 12:24:19.567 
Epoch 381/1000 
	 loss: 18.3570, MinusLogProbMetric: 18.3570, val_loss: 18.1610, val_MinusLogProbMetric: 18.1610

Epoch 381: val_loss improved from 18.22914 to 18.16097, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 18.3570 - MinusLogProbMetric: 18.3570 - val_loss: 18.1610 - val_MinusLogProbMetric: 18.1610 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 382/1000
2023-09-27 12:25:25.200 
Epoch 382/1000 
	 loss: 18.3560, MinusLogProbMetric: 18.3560, val_loss: 18.2259, val_MinusLogProbMetric: 18.2259

Epoch 382: val_loss did not improve from 18.16097
196/196 - 65s - loss: 18.3560 - MinusLogProbMetric: 18.3560 - val_loss: 18.2259 - val_MinusLogProbMetric: 18.2259 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 383/1000
2023-09-27 12:26:29.082 
Epoch 383/1000 
	 loss: 18.2565, MinusLogProbMetric: 18.2565, val_loss: 18.8063, val_MinusLogProbMetric: 18.8063

Epoch 383: val_loss did not improve from 18.16097
196/196 - 64s - loss: 18.2565 - MinusLogProbMetric: 18.2565 - val_loss: 18.8063 - val_MinusLogProbMetric: 18.8063 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 384/1000
2023-09-27 12:27:33.415 
Epoch 384/1000 
	 loss: 18.3576, MinusLogProbMetric: 18.3576, val_loss: 18.3068, val_MinusLogProbMetric: 18.3068

Epoch 384: val_loss did not improve from 18.16097
196/196 - 64s - loss: 18.3576 - MinusLogProbMetric: 18.3576 - val_loss: 18.3068 - val_MinusLogProbMetric: 18.3068 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 385/1000
2023-09-27 12:28:37.472 
Epoch 385/1000 
	 loss: 18.3280, MinusLogProbMetric: 18.3280, val_loss: 18.5687, val_MinusLogProbMetric: 18.5687

Epoch 385: val_loss did not improve from 18.16097
196/196 - 64s - loss: 18.3280 - MinusLogProbMetric: 18.3280 - val_loss: 18.5687 - val_MinusLogProbMetric: 18.5687 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 386/1000
2023-09-27 12:29:41.278 
Epoch 386/1000 
	 loss: 18.2984, MinusLogProbMetric: 18.2984, val_loss: 18.5267, val_MinusLogProbMetric: 18.5267

Epoch 386: val_loss did not improve from 18.16097
196/196 - 64s - loss: 18.2984 - MinusLogProbMetric: 18.2984 - val_loss: 18.5267 - val_MinusLogProbMetric: 18.5267 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 387/1000
2023-09-27 12:30:45.500 
Epoch 387/1000 
	 loss: 18.3005, MinusLogProbMetric: 18.3005, val_loss: 18.7068, val_MinusLogProbMetric: 18.7068

Epoch 387: val_loss did not improve from 18.16097
196/196 - 64s - loss: 18.3005 - MinusLogProbMetric: 18.3005 - val_loss: 18.7068 - val_MinusLogProbMetric: 18.7068 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 388/1000
2023-09-27 12:31:50.149 
Epoch 388/1000 
	 loss: 18.2675, MinusLogProbMetric: 18.2675, val_loss: 18.4226, val_MinusLogProbMetric: 18.4226

Epoch 388: val_loss did not improve from 18.16097
196/196 - 65s - loss: 18.2675 - MinusLogProbMetric: 18.2675 - val_loss: 18.4226 - val_MinusLogProbMetric: 18.4226 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 389/1000
2023-09-27 12:32:54.137 
Epoch 389/1000 
	 loss: 18.2550, MinusLogProbMetric: 18.2550, val_loss: 18.1678, val_MinusLogProbMetric: 18.1678

Epoch 389: val_loss did not improve from 18.16097
196/196 - 64s - loss: 18.2550 - MinusLogProbMetric: 18.2550 - val_loss: 18.1678 - val_MinusLogProbMetric: 18.1678 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 390/1000
2023-09-27 12:33:58.217 
Epoch 390/1000 
	 loss: 18.4511, MinusLogProbMetric: 18.4511, val_loss: 18.6694, val_MinusLogProbMetric: 18.6694

Epoch 390: val_loss did not improve from 18.16097
196/196 - 64s - loss: 18.4511 - MinusLogProbMetric: 18.4511 - val_loss: 18.6694 - val_MinusLogProbMetric: 18.6694 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 391/1000
2023-09-27 12:35:02.416 
Epoch 391/1000 
	 loss: 18.4424, MinusLogProbMetric: 18.4424, val_loss: 18.3641, val_MinusLogProbMetric: 18.3641

Epoch 391: val_loss did not improve from 18.16097
196/196 - 64s - loss: 18.4424 - MinusLogProbMetric: 18.4424 - val_loss: 18.3641 - val_MinusLogProbMetric: 18.3641 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 392/1000
2023-09-27 12:36:06.961 
Epoch 392/1000 
	 loss: 18.2532, MinusLogProbMetric: 18.2532, val_loss: 18.9399, val_MinusLogProbMetric: 18.9399

Epoch 392: val_loss did not improve from 18.16097
196/196 - 65s - loss: 18.2532 - MinusLogProbMetric: 18.2532 - val_loss: 18.9399 - val_MinusLogProbMetric: 18.9399 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 393/1000
2023-09-27 12:37:10.913 
Epoch 393/1000 
	 loss: 18.3414, MinusLogProbMetric: 18.3414, val_loss: 18.5841, val_MinusLogProbMetric: 18.5841

Epoch 393: val_loss did not improve from 18.16097
196/196 - 64s - loss: 18.3414 - MinusLogProbMetric: 18.3414 - val_loss: 18.5841 - val_MinusLogProbMetric: 18.5841 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 394/1000
2023-09-27 12:38:14.668 
Epoch 394/1000 
	 loss: 18.2784, MinusLogProbMetric: 18.2784, val_loss: 18.5599, val_MinusLogProbMetric: 18.5599

Epoch 394: val_loss did not improve from 18.16097
196/196 - 64s - loss: 18.2784 - MinusLogProbMetric: 18.2784 - val_loss: 18.5599 - val_MinusLogProbMetric: 18.5599 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 395/1000
2023-09-27 12:39:19.031 
Epoch 395/1000 
	 loss: 18.3439, MinusLogProbMetric: 18.3439, val_loss: 18.2748, val_MinusLogProbMetric: 18.2748

Epoch 395: val_loss did not improve from 18.16097
196/196 - 64s - loss: 18.3439 - MinusLogProbMetric: 18.3439 - val_loss: 18.2748 - val_MinusLogProbMetric: 18.2748 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 396/1000
2023-09-27 12:40:22.839 
Epoch 396/1000 
	 loss: 18.3930, MinusLogProbMetric: 18.3930, val_loss: 18.2555, val_MinusLogProbMetric: 18.2555

Epoch 396: val_loss did not improve from 18.16097
196/196 - 64s - loss: 18.3930 - MinusLogProbMetric: 18.3930 - val_loss: 18.2555 - val_MinusLogProbMetric: 18.2555 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 397/1000
2023-09-27 12:41:27.343 
Epoch 397/1000 
	 loss: 18.2453, MinusLogProbMetric: 18.2453, val_loss: 18.3285, val_MinusLogProbMetric: 18.3285

Epoch 397: val_loss did not improve from 18.16097
196/196 - 65s - loss: 18.2453 - MinusLogProbMetric: 18.2453 - val_loss: 18.3285 - val_MinusLogProbMetric: 18.3285 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 398/1000
2023-09-27 12:42:31.369 
Epoch 398/1000 
	 loss: 18.3050, MinusLogProbMetric: 18.3050, val_loss: 18.0677, val_MinusLogProbMetric: 18.0677

Epoch 398: val_loss improved from 18.16097 to 18.06769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 18.3050 - MinusLogProbMetric: 18.3050 - val_loss: 18.0677 - val_MinusLogProbMetric: 18.0677 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 399/1000
2023-09-27 12:43:36.980 
Epoch 399/1000 
	 loss: 18.1975, MinusLogProbMetric: 18.1975, val_loss: 18.5373, val_MinusLogProbMetric: 18.5373

Epoch 399: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.1975 - MinusLogProbMetric: 18.1975 - val_loss: 18.5373 - val_MinusLogProbMetric: 18.5373 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 400/1000
2023-09-27 12:44:40.876 
Epoch 400/1000 
	 loss: 18.2629, MinusLogProbMetric: 18.2629, val_loss: 18.6102, val_MinusLogProbMetric: 18.6102

Epoch 400: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2629 - MinusLogProbMetric: 18.2629 - val_loss: 18.6102 - val_MinusLogProbMetric: 18.6102 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 401/1000
2023-09-27 12:45:44.651 
Epoch 401/1000 
	 loss: 18.3166, MinusLogProbMetric: 18.3166, val_loss: 18.3657, val_MinusLogProbMetric: 18.3657

Epoch 401: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.3166 - MinusLogProbMetric: 18.3166 - val_loss: 18.3657 - val_MinusLogProbMetric: 18.3657 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 402/1000
2023-09-27 12:46:49.195 
Epoch 402/1000 
	 loss: 18.3055, MinusLogProbMetric: 18.3055, val_loss: 18.1740, val_MinusLogProbMetric: 18.1740

Epoch 402: val_loss did not improve from 18.06769
196/196 - 65s - loss: 18.3055 - MinusLogProbMetric: 18.3055 - val_loss: 18.1740 - val_MinusLogProbMetric: 18.1740 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 403/1000
2023-09-27 12:47:53.284 
Epoch 403/1000 
	 loss: 18.2671, MinusLogProbMetric: 18.2671, val_loss: 18.6205, val_MinusLogProbMetric: 18.6205

Epoch 403: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2671 - MinusLogProbMetric: 18.2671 - val_loss: 18.6205 - val_MinusLogProbMetric: 18.6205 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 404/1000
2023-09-27 12:48:56.885 
Epoch 404/1000 
	 loss: 18.2064, MinusLogProbMetric: 18.2064, val_loss: 18.2520, val_MinusLogProbMetric: 18.2520

Epoch 404: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2064 - MinusLogProbMetric: 18.2064 - val_loss: 18.2520 - val_MinusLogProbMetric: 18.2520 - lr: 3.3333e-04 - 64s/epoch - 324ms/step
Epoch 405/1000
2023-09-27 12:50:00.825 
Epoch 405/1000 
	 loss: 18.4578, MinusLogProbMetric: 18.4578, val_loss: 18.6883, val_MinusLogProbMetric: 18.6883

Epoch 405: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.4578 - MinusLogProbMetric: 18.4578 - val_loss: 18.6883 - val_MinusLogProbMetric: 18.6883 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 406/1000
2023-09-27 12:51:03.373 
Epoch 406/1000 
	 loss: 18.2528, MinusLogProbMetric: 18.2528, val_loss: 18.6329, val_MinusLogProbMetric: 18.6329

Epoch 406: val_loss did not improve from 18.06769
196/196 - 63s - loss: 18.2528 - MinusLogProbMetric: 18.2528 - val_loss: 18.6329 - val_MinusLogProbMetric: 18.6329 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 407/1000
2023-09-27 12:52:05.898 
Epoch 407/1000 
	 loss: 18.2529, MinusLogProbMetric: 18.2529, val_loss: 19.0694, val_MinusLogProbMetric: 19.0694

Epoch 407: val_loss did not improve from 18.06769
196/196 - 63s - loss: 18.2529 - MinusLogProbMetric: 18.2529 - val_loss: 19.0694 - val_MinusLogProbMetric: 19.0694 - lr: 3.3333e-04 - 63s/epoch - 319ms/step
Epoch 408/1000
2023-09-27 12:53:10.047 
Epoch 408/1000 
	 loss: 18.2584, MinusLogProbMetric: 18.2584, val_loss: 18.5735, val_MinusLogProbMetric: 18.5735

Epoch 408: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2584 - MinusLogProbMetric: 18.2584 - val_loss: 18.5735 - val_MinusLogProbMetric: 18.5735 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 409/1000
2023-09-27 12:54:13.918 
Epoch 409/1000 
	 loss: 18.2468, MinusLogProbMetric: 18.2468, val_loss: 18.4720, val_MinusLogProbMetric: 18.4720

Epoch 409: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2468 - MinusLogProbMetric: 18.2468 - val_loss: 18.4720 - val_MinusLogProbMetric: 18.4720 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 410/1000
2023-09-27 12:55:17.811 
Epoch 410/1000 
	 loss: 18.2823, MinusLogProbMetric: 18.2823, val_loss: 18.3424, val_MinusLogProbMetric: 18.3424

Epoch 410: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2823 - MinusLogProbMetric: 18.2823 - val_loss: 18.3424 - val_MinusLogProbMetric: 18.3424 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 411/1000
2023-09-27 12:56:21.922 
Epoch 411/1000 
	 loss: 18.3005, MinusLogProbMetric: 18.3005, val_loss: 18.5394, val_MinusLogProbMetric: 18.5394

Epoch 411: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.3005 - MinusLogProbMetric: 18.3005 - val_loss: 18.5394 - val_MinusLogProbMetric: 18.5394 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 412/1000
2023-09-27 12:57:25.742 
Epoch 412/1000 
	 loss: 18.2573, MinusLogProbMetric: 18.2573, val_loss: 18.1748, val_MinusLogProbMetric: 18.1748

Epoch 412: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2573 - MinusLogProbMetric: 18.2573 - val_loss: 18.1748 - val_MinusLogProbMetric: 18.1748 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 413/1000
2023-09-27 12:58:29.761 
Epoch 413/1000 
	 loss: 18.1904, MinusLogProbMetric: 18.1904, val_loss: 18.6007, val_MinusLogProbMetric: 18.6007

Epoch 413: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.1904 - MinusLogProbMetric: 18.1904 - val_loss: 18.6007 - val_MinusLogProbMetric: 18.6007 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 414/1000
2023-09-27 12:59:33.791 
Epoch 414/1000 
	 loss: 18.2381, MinusLogProbMetric: 18.2381, val_loss: 18.4251, val_MinusLogProbMetric: 18.4251

Epoch 414: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2381 - MinusLogProbMetric: 18.2381 - val_loss: 18.4251 - val_MinusLogProbMetric: 18.4251 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 415/1000
2023-09-27 13:00:38.076 
Epoch 415/1000 
	 loss: 18.3078, MinusLogProbMetric: 18.3078, val_loss: 18.4516, val_MinusLogProbMetric: 18.4516

Epoch 415: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.3078 - MinusLogProbMetric: 18.3078 - val_loss: 18.4516 - val_MinusLogProbMetric: 18.4516 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 416/1000
2023-09-27 13:01:41.795 
Epoch 416/1000 
	 loss: 18.2382, MinusLogProbMetric: 18.2382, val_loss: 18.7186, val_MinusLogProbMetric: 18.7186

Epoch 416: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2382 - MinusLogProbMetric: 18.2382 - val_loss: 18.7186 - val_MinusLogProbMetric: 18.7186 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 417/1000
2023-09-27 13:02:45.751 
Epoch 417/1000 
	 loss: 18.2164, MinusLogProbMetric: 18.2164, val_loss: 18.3740, val_MinusLogProbMetric: 18.3740

Epoch 417: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2164 - MinusLogProbMetric: 18.2164 - val_loss: 18.3740 - val_MinusLogProbMetric: 18.3740 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 418/1000
2023-09-27 13:03:49.779 
Epoch 418/1000 
	 loss: 18.2781, MinusLogProbMetric: 18.2781, val_loss: 18.4164, val_MinusLogProbMetric: 18.4164

Epoch 418: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2781 - MinusLogProbMetric: 18.2781 - val_loss: 18.4164 - val_MinusLogProbMetric: 18.4164 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 419/1000
2023-09-27 13:04:53.561 
Epoch 419/1000 
	 loss: 18.2185, MinusLogProbMetric: 18.2185, val_loss: 18.3132, val_MinusLogProbMetric: 18.3132

Epoch 419: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2185 - MinusLogProbMetric: 18.2185 - val_loss: 18.3132 - val_MinusLogProbMetric: 18.3132 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 420/1000
2023-09-27 13:05:57.216 
Epoch 420/1000 
	 loss: 18.2473, MinusLogProbMetric: 18.2473, val_loss: 18.1392, val_MinusLogProbMetric: 18.1392

Epoch 420: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2473 - MinusLogProbMetric: 18.2473 - val_loss: 18.1392 - val_MinusLogProbMetric: 18.1392 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 421/1000
2023-09-27 13:07:00.915 
Epoch 421/1000 
	 loss: 18.2365, MinusLogProbMetric: 18.2365, val_loss: 18.2502, val_MinusLogProbMetric: 18.2502

Epoch 421: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2365 - MinusLogProbMetric: 18.2365 - val_loss: 18.2502 - val_MinusLogProbMetric: 18.2502 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 422/1000
2023-09-27 13:08:05.459 
Epoch 422/1000 
	 loss: 18.2205, MinusLogProbMetric: 18.2205, val_loss: 18.6409, val_MinusLogProbMetric: 18.6409

Epoch 422: val_loss did not improve from 18.06769
196/196 - 65s - loss: 18.2205 - MinusLogProbMetric: 18.2205 - val_loss: 18.6409 - val_MinusLogProbMetric: 18.6409 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 423/1000
2023-09-27 13:09:09.534 
Epoch 423/1000 
	 loss: 18.2838, MinusLogProbMetric: 18.2838, val_loss: 18.3850, val_MinusLogProbMetric: 18.3850

Epoch 423: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2838 - MinusLogProbMetric: 18.2838 - val_loss: 18.3850 - val_MinusLogProbMetric: 18.3850 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 424/1000
2023-09-27 13:10:13.587 
Epoch 424/1000 
	 loss: 18.2449, MinusLogProbMetric: 18.2449, val_loss: 18.4344, val_MinusLogProbMetric: 18.4344

Epoch 424: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2449 - MinusLogProbMetric: 18.2449 - val_loss: 18.4344 - val_MinusLogProbMetric: 18.4344 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 425/1000
2023-09-27 13:11:17.342 
Epoch 425/1000 
	 loss: 18.1795, MinusLogProbMetric: 18.1795, val_loss: 18.4734, val_MinusLogProbMetric: 18.4734

Epoch 425: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.1795 - MinusLogProbMetric: 18.1795 - val_loss: 18.4734 - val_MinusLogProbMetric: 18.4734 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 426/1000
2023-09-27 13:12:21.576 
Epoch 426/1000 
	 loss: 18.2344, MinusLogProbMetric: 18.2344, val_loss: 18.0896, val_MinusLogProbMetric: 18.0896

Epoch 426: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2344 - MinusLogProbMetric: 18.2344 - val_loss: 18.0896 - val_MinusLogProbMetric: 18.0896 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 427/1000
2023-09-27 13:13:25.964 
Epoch 427/1000 
	 loss: 18.1789, MinusLogProbMetric: 18.1789, val_loss: 18.4817, val_MinusLogProbMetric: 18.4817

Epoch 427: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.1789 - MinusLogProbMetric: 18.1789 - val_loss: 18.4817 - val_MinusLogProbMetric: 18.4817 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 428/1000
2023-09-27 13:14:30.343 
Epoch 428/1000 
	 loss: 18.2988, MinusLogProbMetric: 18.2988, val_loss: 18.5246, val_MinusLogProbMetric: 18.5246

Epoch 428: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2988 - MinusLogProbMetric: 18.2988 - val_loss: 18.5246 - val_MinusLogProbMetric: 18.5246 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 429/1000
2023-09-27 13:15:34.125 
Epoch 429/1000 
	 loss: 18.2376, MinusLogProbMetric: 18.2376, val_loss: 18.4977, val_MinusLogProbMetric: 18.4977

Epoch 429: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2376 - MinusLogProbMetric: 18.2376 - val_loss: 18.4977 - val_MinusLogProbMetric: 18.4977 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 430/1000
2023-09-27 13:16:37.988 
Epoch 430/1000 
	 loss: 18.2317, MinusLogProbMetric: 18.2317, val_loss: 18.6635, val_MinusLogProbMetric: 18.6635

Epoch 430: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2317 - MinusLogProbMetric: 18.2317 - val_loss: 18.6635 - val_MinusLogProbMetric: 18.6635 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 431/1000
2023-09-27 13:17:42.258 
Epoch 431/1000 
	 loss: 18.2579, MinusLogProbMetric: 18.2579, val_loss: 18.1895, val_MinusLogProbMetric: 18.1895

Epoch 431: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2579 - MinusLogProbMetric: 18.2579 - val_loss: 18.1895 - val_MinusLogProbMetric: 18.1895 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 432/1000
2023-09-27 13:18:46.704 
Epoch 432/1000 
	 loss: 18.2800, MinusLogProbMetric: 18.2800, val_loss: 18.5356, val_MinusLogProbMetric: 18.5356

Epoch 432: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2800 - MinusLogProbMetric: 18.2800 - val_loss: 18.5356 - val_MinusLogProbMetric: 18.5356 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 433/1000
2023-09-27 13:19:51.148 
Epoch 433/1000 
	 loss: 18.2181, MinusLogProbMetric: 18.2181, val_loss: 18.1153, val_MinusLogProbMetric: 18.1153

Epoch 433: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2181 - MinusLogProbMetric: 18.2181 - val_loss: 18.1153 - val_MinusLogProbMetric: 18.1153 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 434/1000
2023-09-27 13:20:55.181 
Epoch 434/1000 
	 loss: 18.2223, MinusLogProbMetric: 18.2223, val_loss: 18.4424, val_MinusLogProbMetric: 18.4424

Epoch 434: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2223 - MinusLogProbMetric: 18.2223 - val_loss: 18.4424 - val_MinusLogProbMetric: 18.4424 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 435/1000
2023-09-27 13:21:59.016 
Epoch 435/1000 
	 loss: 20.5846, MinusLogProbMetric: 20.5846, val_loss: 18.8909, val_MinusLogProbMetric: 18.8909

Epoch 435: val_loss did not improve from 18.06769
196/196 - 64s - loss: 20.5846 - MinusLogProbMetric: 20.5846 - val_loss: 18.8909 - val_MinusLogProbMetric: 18.8909 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 436/1000
2023-09-27 13:23:03.116 
Epoch 436/1000 
	 loss: 18.4930, MinusLogProbMetric: 18.4930, val_loss: 18.3490, val_MinusLogProbMetric: 18.3490

Epoch 436: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.4930 - MinusLogProbMetric: 18.4930 - val_loss: 18.3490 - val_MinusLogProbMetric: 18.3490 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 437/1000
2023-09-27 13:24:07.290 
Epoch 437/1000 
	 loss: 18.3446, MinusLogProbMetric: 18.3446, val_loss: 18.3806, val_MinusLogProbMetric: 18.3806

Epoch 437: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.3446 - MinusLogProbMetric: 18.3446 - val_loss: 18.3806 - val_MinusLogProbMetric: 18.3806 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 438/1000
2023-09-27 13:25:11.768 
Epoch 438/1000 
	 loss: 18.2673, MinusLogProbMetric: 18.2673, val_loss: 18.9024, val_MinusLogProbMetric: 18.9024

Epoch 438: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2673 - MinusLogProbMetric: 18.2673 - val_loss: 18.9024 - val_MinusLogProbMetric: 18.9024 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 439/1000
2023-09-27 13:26:16.045 
Epoch 439/1000 
	 loss: 18.2433, MinusLogProbMetric: 18.2433, val_loss: 18.8141, val_MinusLogProbMetric: 18.8141

Epoch 439: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2433 - MinusLogProbMetric: 18.2433 - val_loss: 18.8141 - val_MinusLogProbMetric: 18.8141 - lr: 3.3333e-04 - 64s/epoch - 328ms/step
Epoch 440/1000
2023-09-27 13:27:19.543 
Epoch 440/1000 
	 loss: 18.1563, MinusLogProbMetric: 18.1563, val_loss: 18.4366, val_MinusLogProbMetric: 18.4366

Epoch 440: val_loss did not improve from 18.06769
196/196 - 63s - loss: 18.1563 - MinusLogProbMetric: 18.1563 - val_loss: 18.4366 - val_MinusLogProbMetric: 18.4366 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 441/1000
2023-09-27 13:28:23.299 
Epoch 441/1000 
	 loss: 18.1867, MinusLogProbMetric: 18.1867, val_loss: 18.3707, val_MinusLogProbMetric: 18.3707

Epoch 441: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.1867 - MinusLogProbMetric: 18.1867 - val_loss: 18.3707 - val_MinusLogProbMetric: 18.3707 - lr: 3.3333e-04 - 64s/epoch - 325ms/step
Epoch 442/1000
2023-09-27 13:29:27.429 
Epoch 442/1000 
	 loss: 18.1798, MinusLogProbMetric: 18.1798, val_loss: 18.4134, val_MinusLogProbMetric: 18.4134

Epoch 442: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.1798 - MinusLogProbMetric: 18.1798 - val_loss: 18.4134 - val_MinusLogProbMetric: 18.4134 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 443/1000
2023-09-27 13:30:31.915 
Epoch 443/1000 
	 loss: 18.2245, MinusLogProbMetric: 18.2245, val_loss: 18.4075, val_MinusLogProbMetric: 18.4075

Epoch 443: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2245 - MinusLogProbMetric: 18.2245 - val_loss: 18.4075 - val_MinusLogProbMetric: 18.4075 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 444/1000
2023-09-27 13:31:36.076 
Epoch 444/1000 
	 loss: 18.1640, MinusLogProbMetric: 18.1640, val_loss: 18.4994, val_MinusLogProbMetric: 18.4994

Epoch 444: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.1640 - MinusLogProbMetric: 18.1640 - val_loss: 18.4994 - val_MinusLogProbMetric: 18.4994 - lr: 3.3333e-04 - 64s/epoch - 327ms/step
Epoch 445/1000
2023-09-27 13:32:40.604 
Epoch 445/1000 
	 loss: 18.1858, MinusLogProbMetric: 18.1858, val_loss: 18.8759, val_MinusLogProbMetric: 18.8759

Epoch 445: val_loss did not improve from 18.06769
196/196 - 65s - loss: 18.1858 - MinusLogProbMetric: 18.1858 - val_loss: 18.8759 - val_MinusLogProbMetric: 18.8759 - lr: 3.3333e-04 - 65s/epoch - 329ms/step
Epoch 446/1000
2023-09-27 13:33:44.451 
Epoch 446/1000 
	 loss: 18.2156, MinusLogProbMetric: 18.2156, val_loss: 19.5468, val_MinusLogProbMetric: 19.5468

Epoch 446: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.2156 - MinusLogProbMetric: 18.2156 - val_loss: 19.5468 - val_MinusLogProbMetric: 19.5468 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 447/1000
2023-09-27 13:34:48.841 
Epoch 447/1000 
	 loss: 18.1895, MinusLogProbMetric: 18.1895, val_loss: 18.3375, val_MinusLogProbMetric: 18.3375

Epoch 447: val_loss did not improve from 18.06769
196/196 - 64s - loss: 18.1895 - MinusLogProbMetric: 18.1895 - val_loss: 18.3375 - val_MinusLogProbMetric: 18.3375 - lr: 3.3333e-04 - 64s/epoch - 329ms/step
Epoch 448/1000
2023-09-27 13:35:52.305 
Epoch 448/1000 
	 loss: 18.1725, MinusLogProbMetric: 18.1725, val_loss: 18.1292, val_MinusLogProbMetric: 18.1292

Epoch 448: val_loss did not improve from 18.06769
196/196 - 63s - loss: 18.1725 - MinusLogProbMetric: 18.1725 - val_loss: 18.1292 - val_MinusLogProbMetric: 18.1292 - lr: 3.3333e-04 - 63s/epoch - 324ms/step
Epoch 449/1000
2023-09-27 13:36:56.778 
Epoch 449/1000 
	 loss: 17.7000, MinusLogProbMetric: 17.7000, val_loss: 17.8659, val_MinusLogProbMetric: 17.8659

Epoch 449: val_loss improved from 18.06769 to 17.86589, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.7000 - MinusLogProbMetric: 17.7000 - val_loss: 17.8659 - val_MinusLogProbMetric: 17.8659 - lr: 1.6667e-04 - 65s/epoch - 334ms/step
Epoch 450/1000
2023-09-27 13:38:01.960 
Epoch 450/1000 
	 loss: 17.6825, MinusLogProbMetric: 17.6825, val_loss: 17.7585, val_MinusLogProbMetric: 17.7585

Epoch 450: val_loss improved from 17.86589 to 17.75853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.6825 - MinusLogProbMetric: 17.6825 - val_loss: 17.7585 - val_MinusLogProbMetric: 17.7585 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 451/1000
2023-09-27 13:39:07.050 
Epoch 451/1000 
	 loss: 17.6913, MinusLogProbMetric: 17.6913, val_loss: 17.9791, val_MinusLogProbMetric: 17.9791

Epoch 451: val_loss did not improve from 17.75853
196/196 - 64s - loss: 17.6913 - MinusLogProbMetric: 17.6913 - val_loss: 17.9791 - val_MinusLogProbMetric: 17.9791 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 452/1000
2023-09-27 13:40:10.374 
Epoch 452/1000 
	 loss: 17.6719, MinusLogProbMetric: 17.6719, val_loss: 17.7395, val_MinusLogProbMetric: 17.7395

Epoch 452: val_loss improved from 17.75853 to 17.73947, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 64s - loss: 17.6719 - MinusLogProbMetric: 17.6719 - val_loss: 17.7395 - val_MinusLogProbMetric: 17.7395 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 453/1000
2023-09-27 13:41:15.635 
Epoch 453/1000 
	 loss: 17.7196, MinusLogProbMetric: 17.7196, val_loss: 17.8698, val_MinusLogProbMetric: 17.8698

Epoch 453: val_loss did not improve from 17.73947
196/196 - 64s - loss: 17.7196 - MinusLogProbMetric: 17.7196 - val_loss: 17.8698 - val_MinusLogProbMetric: 17.8698 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 454/1000
2023-09-27 13:42:19.668 
Epoch 454/1000 
	 loss: 17.6693, MinusLogProbMetric: 17.6693, val_loss: 17.7965, val_MinusLogProbMetric: 17.7965

Epoch 454: val_loss did not improve from 17.73947
196/196 - 64s - loss: 17.6693 - MinusLogProbMetric: 17.6693 - val_loss: 17.7965 - val_MinusLogProbMetric: 17.7965 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 455/1000
2023-09-27 13:43:23.973 
Epoch 455/1000 
	 loss: 17.6777, MinusLogProbMetric: 17.6777, val_loss: 17.7976, val_MinusLogProbMetric: 17.7976

Epoch 455: val_loss did not improve from 17.73947
196/196 - 64s - loss: 17.6777 - MinusLogProbMetric: 17.6777 - val_loss: 17.7976 - val_MinusLogProbMetric: 17.7976 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 456/1000
2023-09-27 13:44:27.703 
Epoch 456/1000 
	 loss: 17.7281, MinusLogProbMetric: 17.7281, val_loss: 17.7404, val_MinusLogProbMetric: 17.7404

Epoch 456: val_loss did not improve from 17.73947
196/196 - 64s - loss: 17.7281 - MinusLogProbMetric: 17.7281 - val_loss: 17.7404 - val_MinusLogProbMetric: 17.7404 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 457/1000
2023-09-27 13:45:25.631 
Epoch 457/1000 
	 loss: 17.6852, MinusLogProbMetric: 17.6852, val_loss: 18.0084, val_MinusLogProbMetric: 18.0084

Epoch 457: val_loss did not improve from 17.73947
196/196 - 58s - loss: 17.6852 - MinusLogProbMetric: 17.6852 - val_loss: 18.0084 - val_MinusLogProbMetric: 18.0084 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 458/1000
2023-09-27 13:46:25.959 
Epoch 458/1000 
	 loss: 17.7152, MinusLogProbMetric: 17.7152, val_loss: 18.0502, val_MinusLogProbMetric: 18.0502

Epoch 458: val_loss did not improve from 17.73947
196/196 - 60s - loss: 17.7152 - MinusLogProbMetric: 17.7152 - val_loss: 18.0502 - val_MinusLogProbMetric: 18.0502 - lr: 1.6667e-04 - 60s/epoch - 308ms/step
Epoch 459/1000
2023-09-27 13:47:29.587 
Epoch 459/1000 
	 loss: 17.7201, MinusLogProbMetric: 17.7201, val_loss: 17.7614, val_MinusLogProbMetric: 17.7614

Epoch 459: val_loss did not improve from 17.73947
196/196 - 64s - loss: 17.7201 - MinusLogProbMetric: 17.7201 - val_loss: 17.7614 - val_MinusLogProbMetric: 17.7614 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 460/1000
2023-09-27 13:48:33.288 
Epoch 460/1000 
	 loss: 17.6890, MinusLogProbMetric: 17.6890, val_loss: 17.8481, val_MinusLogProbMetric: 17.8481

Epoch 460: val_loss did not improve from 17.73947
196/196 - 64s - loss: 17.6890 - MinusLogProbMetric: 17.6890 - val_loss: 17.8481 - val_MinusLogProbMetric: 17.8481 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 461/1000
2023-09-27 13:49:36.978 
Epoch 461/1000 
	 loss: 17.6572, MinusLogProbMetric: 17.6572, val_loss: 17.9566, val_MinusLogProbMetric: 17.9566

Epoch 461: val_loss did not improve from 17.73947
196/196 - 64s - loss: 17.6572 - MinusLogProbMetric: 17.6572 - val_loss: 17.9566 - val_MinusLogProbMetric: 17.9566 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 462/1000
2023-09-27 13:50:41.158 
Epoch 462/1000 
	 loss: 17.6810, MinusLogProbMetric: 17.6810, val_loss: 17.8567, val_MinusLogProbMetric: 17.8567

Epoch 462: val_loss did not improve from 17.73947
196/196 - 64s - loss: 17.6810 - MinusLogProbMetric: 17.6810 - val_loss: 17.8567 - val_MinusLogProbMetric: 17.8567 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 463/1000
2023-09-27 13:51:45.239 
Epoch 463/1000 
	 loss: 17.6889, MinusLogProbMetric: 17.6889, val_loss: 17.8344, val_MinusLogProbMetric: 17.8344

Epoch 463: val_loss did not improve from 17.73947
196/196 - 64s - loss: 17.6889 - MinusLogProbMetric: 17.6889 - val_loss: 17.8344 - val_MinusLogProbMetric: 17.8344 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 464/1000
2023-09-27 13:52:49.338 
Epoch 464/1000 
	 loss: 17.6620, MinusLogProbMetric: 17.6620, val_loss: 17.7921, val_MinusLogProbMetric: 17.7921

Epoch 464: val_loss did not improve from 17.73947
196/196 - 64s - loss: 17.6620 - MinusLogProbMetric: 17.6620 - val_loss: 17.7921 - val_MinusLogProbMetric: 17.7921 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 465/1000
2023-09-27 13:53:53.508 
Epoch 465/1000 
	 loss: 17.6876, MinusLogProbMetric: 17.6876, val_loss: 17.6718, val_MinusLogProbMetric: 17.6718

Epoch 465: val_loss improved from 17.73947 to 17.67183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.6876 - MinusLogProbMetric: 17.6876 - val_loss: 17.6718 - val_MinusLogProbMetric: 17.6718 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 466/1000
2023-09-27 13:54:58.458 
Epoch 466/1000 
	 loss: 17.6830, MinusLogProbMetric: 17.6830, val_loss: 17.7250, val_MinusLogProbMetric: 17.7250

Epoch 466: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6830 - MinusLogProbMetric: 17.6830 - val_loss: 17.7250 - val_MinusLogProbMetric: 17.7250 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 467/1000
2023-09-27 13:56:03.273 
Epoch 467/1000 
	 loss: 17.6642, MinusLogProbMetric: 17.6642, val_loss: 17.7723, val_MinusLogProbMetric: 17.7723

Epoch 467: val_loss did not improve from 17.67183
196/196 - 65s - loss: 17.6642 - MinusLogProbMetric: 17.6642 - val_loss: 17.7723 - val_MinusLogProbMetric: 17.7723 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 468/1000
2023-09-27 13:57:07.528 
Epoch 468/1000 
	 loss: 17.6760, MinusLogProbMetric: 17.6760, val_loss: 18.0088, val_MinusLogProbMetric: 18.0088

Epoch 468: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6760 - MinusLogProbMetric: 17.6760 - val_loss: 18.0088 - val_MinusLogProbMetric: 18.0088 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 469/1000
2023-09-27 13:58:11.725 
Epoch 469/1000 
	 loss: 17.7026, MinusLogProbMetric: 17.7026, val_loss: 17.7242, val_MinusLogProbMetric: 17.7242

Epoch 469: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.7026 - MinusLogProbMetric: 17.7026 - val_loss: 17.7242 - val_MinusLogProbMetric: 17.7242 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 470/1000
2023-09-27 13:59:16.016 
Epoch 470/1000 
	 loss: 17.6318, MinusLogProbMetric: 17.6318, val_loss: 17.8037, val_MinusLogProbMetric: 17.8037

Epoch 470: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6318 - MinusLogProbMetric: 17.6318 - val_loss: 17.8037 - val_MinusLogProbMetric: 17.8037 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 471/1000
2023-09-27 14:00:20.085 
Epoch 471/1000 
	 loss: 17.7099, MinusLogProbMetric: 17.7099, val_loss: 17.7266, val_MinusLogProbMetric: 17.7266

Epoch 471: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.7099 - MinusLogProbMetric: 17.7099 - val_loss: 17.7266 - val_MinusLogProbMetric: 17.7266 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 472/1000
2023-09-27 14:01:23.999 
Epoch 472/1000 
	 loss: 17.6466, MinusLogProbMetric: 17.6466, val_loss: 17.7142, val_MinusLogProbMetric: 17.7142

Epoch 472: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6466 - MinusLogProbMetric: 17.6466 - val_loss: 17.7142 - val_MinusLogProbMetric: 17.7142 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 473/1000
2023-09-27 14:02:28.152 
Epoch 473/1000 
	 loss: 17.6581, MinusLogProbMetric: 17.6581, val_loss: 17.7154, val_MinusLogProbMetric: 17.7154

Epoch 473: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6581 - MinusLogProbMetric: 17.6581 - val_loss: 17.7154 - val_MinusLogProbMetric: 17.7154 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 474/1000
2023-09-27 14:03:32.478 
Epoch 474/1000 
	 loss: 17.7467, MinusLogProbMetric: 17.7467, val_loss: 17.7889, val_MinusLogProbMetric: 17.7889

Epoch 474: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.7467 - MinusLogProbMetric: 17.7467 - val_loss: 17.7889 - val_MinusLogProbMetric: 17.7889 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 475/1000
2023-09-27 14:04:36.767 
Epoch 475/1000 
	 loss: 17.6752, MinusLogProbMetric: 17.6752, val_loss: 17.7311, val_MinusLogProbMetric: 17.7311

Epoch 475: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6752 - MinusLogProbMetric: 17.6752 - val_loss: 17.7311 - val_MinusLogProbMetric: 17.7311 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 476/1000
2023-09-27 14:05:41.054 
Epoch 476/1000 
	 loss: 17.6673, MinusLogProbMetric: 17.6673, val_loss: 17.8358, val_MinusLogProbMetric: 17.8358

Epoch 476: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6673 - MinusLogProbMetric: 17.6673 - val_loss: 17.8358 - val_MinusLogProbMetric: 17.8358 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 477/1000
2023-09-27 14:06:45.581 
Epoch 477/1000 
	 loss: 17.6854, MinusLogProbMetric: 17.6854, val_loss: 17.7386, val_MinusLogProbMetric: 17.7386

Epoch 477: val_loss did not improve from 17.67183
196/196 - 65s - loss: 17.6854 - MinusLogProbMetric: 17.6854 - val_loss: 17.7386 - val_MinusLogProbMetric: 17.7386 - lr: 1.6667e-04 - 65s/epoch - 329ms/step
Epoch 478/1000
2023-09-27 14:07:49.213 
Epoch 478/1000 
	 loss: 17.7730, MinusLogProbMetric: 17.7730, val_loss: 17.7515, val_MinusLogProbMetric: 17.7515

Epoch 478: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.7730 - MinusLogProbMetric: 17.7730 - val_loss: 17.7515 - val_MinusLogProbMetric: 17.7515 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 479/1000
2023-09-27 14:08:53.233 
Epoch 479/1000 
	 loss: 17.6936, MinusLogProbMetric: 17.6936, val_loss: 17.8772, val_MinusLogProbMetric: 17.8772

Epoch 479: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6936 - MinusLogProbMetric: 17.6936 - val_loss: 17.8772 - val_MinusLogProbMetric: 17.8772 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 480/1000
2023-09-27 14:09:57.279 
Epoch 480/1000 
	 loss: 17.6969, MinusLogProbMetric: 17.6969, val_loss: 17.9422, val_MinusLogProbMetric: 17.9422

Epoch 480: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6969 - MinusLogProbMetric: 17.6969 - val_loss: 17.9422 - val_MinusLogProbMetric: 17.9422 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 481/1000
2023-09-27 14:11:01.508 
Epoch 481/1000 
	 loss: 17.6473, MinusLogProbMetric: 17.6473, val_loss: 18.1674, val_MinusLogProbMetric: 18.1674

Epoch 481: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6473 - MinusLogProbMetric: 17.6473 - val_loss: 18.1674 - val_MinusLogProbMetric: 18.1674 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 482/1000
2023-09-27 14:12:05.270 
Epoch 482/1000 
	 loss: 17.6419, MinusLogProbMetric: 17.6419, val_loss: 17.8077, val_MinusLogProbMetric: 17.8077

Epoch 482: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6419 - MinusLogProbMetric: 17.6419 - val_loss: 17.8077 - val_MinusLogProbMetric: 17.8077 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 483/1000
2023-09-27 14:13:10.004 
Epoch 483/1000 
	 loss: 17.6288, MinusLogProbMetric: 17.6288, val_loss: 17.7837, val_MinusLogProbMetric: 17.7837

Epoch 483: val_loss did not improve from 17.67183
196/196 - 65s - loss: 17.6288 - MinusLogProbMetric: 17.6288 - val_loss: 17.7837 - val_MinusLogProbMetric: 17.7837 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 484/1000
2023-09-27 14:14:14.076 
Epoch 484/1000 
	 loss: 17.6736, MinusLogProbMetric: 17.6736, val_loss: 17.9596, val_MinusLogProbMetric: 17.9596

Epoch 484: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6736 - MinusLogProbMetric: 17.6736 - val_loss: 17.9596 - val_MinusLogProbMetric: 17.9596 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 485/1000
2023-09-27 14:15:18.127 
Epoch 485/1000 
	 loss: 17.6397, MinusLogProbMetric: 17.6397, val_loss: 18.0485, val_MinusLogProbMetric: 18.0485

Epoch 485: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6397 - MinusLogProbMetric: 17.6397 - val_loss: 18.0485 - val_MinusLogProbMetric: 18.0485 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 486/1000
2023-09-27 14:16:22.052 
Epoch 486/1000 
	 loss: 17.6790, MinusLogProbMetric: 17.6790, val_loss: 17.7744, val_MinusLogProbMetric: 17.7744

Epoch 486: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6790 - MinusLogProbMetric: 17.6790 - val_loss: 17.7744 - val_MinusLogProbMetric: 17.7744 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 487/1000
2023-09-27 14:17:25.766 
Epoch 487/1000 
	 loss: 17.6552, MinusLogProbMetric: 17.6552, val_loss: 17.7407, val_MinusLogProbMetric: 17.7407

Epoch 487: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6552 - MinusLogProbMetric: 17.6552 - val_loss: 17.7407 - val_MinusLogProbMetric: 17.7407 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 488/1000
2023-09-27 14:18:29.545 
Epoch 488/1000 
	 loss: 17.6561, MinusLogProbMetric: 17.6561, val_loss: 17.8338, val_MinusLogProbMetric: 17.8338

Epoch 488: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6561 - MinusLogProbMetric: 17.6561 - val_loss: 17.8338 - val_MinusLogProbMetric: 17.8338 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 489/1000
2023-09-27 14:19:33.570 
Epoch 489/1000 
	 loss: 17.6981, MinusLogProbMetric: 17.6981, val_loss: 17.7620, val_MinusLogProbMetric: 17.7620

Epoch 489: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6981 - MinusLogProbMetric: 17.6981 - val_loss: 17.7620 - val_MinusLogProbMetric: 17.7620 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 490/1000
2023-09-27 14:20:38.468 
Epoch 490/1000 
	 loss: 17.6156, MinusLogProbMetric: 17.6156, val_loss: 17.9273, val_MinusLogProbMetric: 17.9273

Epoch 490: val_loss did not improve from 17.67183
196/196 - 65s - loss: 17.6156 - MinusLogProbMetric: 17.6156 - val_loss: 17.9273 - val_MinusLogProbMetric: 17.9273 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 491/1000
2023-09-27 14:21:41.781 
Epoch 491/1000 
	 loss: 17.6705, MinusLogProbMetric: 17.6705, val_loss: 17.8042, val_MinusLogProbMetric: 17.8042

Epoch 491: val_loss did not improve from 17.67183
196/196 - 63s - loss: 17.6705 - MinusLogProbMetric: 17.6705 - val_loss: 17.8042 - val_MinusLogProbMetric: 17.8042 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 492/1000
2023-09-27 14:22:45.194 
Epoch 492/1000 
	 loss: 17.6728, MinusLogProbMetric: 17.6728, val_loss: 17.8525, val_MinusLogProbMetric: 17.8525

Epoch 492: val_loss did not improve from 17.67183
196/196 - 63s - loss: 17.6728 - MinusLogProbMetric: 17.6728 - val_loss: 17.8525 - val_MinusLogProbMetric: 17.8525 - lr: 1.6667e-04 - 63s/epoch - 324ms/step
Epoch 493/1000
2023-09-27 14:23:49.074 
Epoch 493/1000 
	 loss: 17.6357, MinusLogProbMetric: 17.6357, val_loss: 17.8717, val_MinusLogProbMetric: 17.8717

Epoch 493: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6357 - MinusLogProbMetric: 17.6357 - val_loss: 17.8717 - val_MinusLogProbMetric: 17.8717 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 494/1000
2023-09-27 14:24:52.998 
Epoch 494/1000 
	 loss: 17.6951, MinusLogProbMetric: 17.6951, val_loss: 17.7840, val_MinusLogProbMetric: 17.7840

Epoch 494: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6951 - MinusLogProbMetric: 17.6951 - val_loss: 17.7840 - val_MinusLogProbMetric: 17.7840 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 495/1000
2023-09-27 14:25:56.642 
Epoch 495/1000 
	 loss: 17.6370, MinusLogProbMetric: 17.6370, val_loss: 17.7797, val_MinusLogProbMetric: 17.7797

Epoch 495: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6370 - MinusLogProbMetric: 17.6370 - val_loss: 17.7797 - val_MinusLogProbMetric: 17.7797 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 496/1000
2023-09-27 14:27:01.275 
Epoch 496/1000 
	 loss: 17.7085, MinusLogProbMetric: 17.7085, val_loss: 17.8023, val_MinusLogProbMetric: 17.8023

Epoch 496: val_loss did not improve from 17.67183
196/196 - 65s - loss: 17.7085 - MinusLogProbMetric: 17.7085 - val_loss: 17.8023 - val_MinusLogProbMetric: 17.8023 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 497/1000
2023-09-27 14:28:05.702 
Epoch 497/1000 
	 loss: 17.6348, MinusLogProbMetric: 17.6348, val_loss: 17.7445, val_MinusLogProbMetric: 17.7445

Epoch 497: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6348 - MinusLogProbMetric: 17.6348 - val_loss: 17.7445 - val_MinusLogProbMetric: 17.7445 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 498/1000
2023-09-27 14:29:09.899 
Epoch 498/1000 
	 loss: 17.6050, MinusLogProbMetric: 17.6050, val_loss: 17.8146, val_MinusLogProbMetric: 17.8146

Epoch 498: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6050 - MinusLogProbMetric: 17.6050 - val_loss: 17.8146 - val_MinusLogProbMetric: 17.8146 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 499/1000
2023-09-27 14:30:14.143 
Epoch 499/1000 
	 loss: 17.6018, MinusLogProbMetric: 17.6018, val_loss: 17.8107, val_MinusLogProbMetric: 17.8107

Epoch 499: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6018 - MinusLogProbMetric: 17.6018 - val_loss: 17.8107 - val_MinusLogProbMetric: 17.8107 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 500/1000
2023-09-27 14:31:18.801 
Epoch 500/1000 
	 loss: 17.6858, MinusLogProbMetric: 17.6858, val_loss: 17.6874, val_MinusLogProbMetric: 17.6874

Epoch 500: val_loss did not improve from 17.67183
196/196 - 65s - loss: 17.6858 - MinusLogProbMetric: 17.6858 - val_loss: 17.6874 - val_MinusLogProbMetric: 17.6874 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 501/1000
2023-09-27 14:32:22.540 
Epoch 501/1000 
	 loss: 17.6628, MinusLogProbMetric: 17.6628, val_loss: 17.6960, val_MinusLogProbMetric: 17.6960

Epoch 501: val_loss did not improve from 17.67183
196/196 - 64s - loss: 17.6628 - MinusLogProbMetric: 17.6628 - val_loss: 17.6960 - val_MinusLogProbMetric: 17.6960 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 502/1000
2023-09-27 14:33:26.555 
Epoch 502/1000 
	 loss: 17.6253, MinusLogProbMetric: 17.6253, val_loss: 17.6695, val_MinusLogProbMetric: 17.6695

Epoch 502: val_loss improved from 17.67183 to 17.66953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.6253 - MinusLogProbMetric: 17.6253 - val_loss: 17.6695 - val_MinusLogProbMetric: 17.6695 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 503/1000
2023-09-27 14:34:31.520 
Epoch 503/1000 
	 loss: 17.6119, MinusLogProbMetric: 17.6119, val_loss: 17.6770, val_MinusLogProbMetric: 17.6770

Epoch 503: val_loss did not improve from 17.66953
196/196 - 64s - loss: 17.6119 - MinusLogProbMetric: 17.6119 - val_loss: 17.6770 - val_MinusLogProbMetric: 17.6770 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 504/1000
2023-09-27 14:35:35.193 
Epoch 504/1000 
	 loss: 17.6530, MinusLogProbMetric: 17.6530, val_loss: 17.9521, val_MinusLogProbMetric: 17.9521

Epoch 504: val_loss did not improve from 17.66953
196/196 - 64s - loss: 17.6530 - MinusLogProbMetric: 17.6530 - val_loss: 17.9521 - val_MinusLogProbMetric: 17.9521 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 505/1000
2023-09-27 14:36:38.827 
Epoch 505/1000 
	 loss: 18.2984, MinusLogProbMetric: 18.2984, val_loss: 18.3800, val_MinusLogProbMetric: 18.3800

Epoch 505: val_loss did not improve from 17.66953
196/196 - 64s - loss: 18.2984 - MinusLogProbMetric: 18.2984 - val_loss: 18.3800 - val_MinusLogProbMetric: 18.3800 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 506/1000
2023-09-27 14:37:42.717 
Epoch 506/1000 
	 loss: 17.8186, MinusLogProbMetric: 17.8186, val_loss: 17.9498, val_MinusLogProbMetric: 17.9498

Epoch 506: val_loss did not improve from 17.66953
196/196 - 64s - loss: 17.8186 - MinusLogProbMetric: 17.8186 - val_loss: 17.9498 - val_MinusLogProbMetric: 17.9498 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 507/1000
2023-09-27 14:38:46.768 
Epoch 507/1000 
	 loss: 17.7203, MinusLogProbMetric: 17.7203, val_loss: 17.7376, val_MinusLogProbMetric: 17.7376

Epoch 507: val_loss did not improve from 17.66953
196/196 - 64s - loss: 17.7203 - MinusLogProbMetric: 17.7203 - val_loss: 17.7376 - val_MinusLogProbMetric: 17.7376 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 508/1000
2023-09-27 14:39:50.801 
Epoch 508/1000 
	 loss: 17.6319, MinusLogProbMetric: 17.6319, val_loss: 17.7694, val_MinusLogProbMetric: 17.7694

Epoch 508: val_loss did not improve from 17.66953
196/196 - 64s - loss: 17.6319 - MinusLogProbMetric: 17.6319 - val_loss: 17.7694 - val_MinusLogProbMetric: 17.7694 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 509/1000
2023-09-27 14:40:55.001 
Epoch 509/1000 
	 loss: 17.6389, MinusLogProbMetric: 17.6389, val_loss: 17.7377, val_MinusLogProbMetric: 17.7377

Epoch 509: val_loss did not improve from 17.66953
196/196 - 64s - loss: 17.6389 - MinusLogProbMetric: 17.6389 - val_loss: 17.7377 - val_MinusLogProbMetric: 17.7377 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 510/1000
2023-09-27 14:41:59.241 
Epoch 510/1000 
	 loss: 17.6497, MinusLogProbMetric: 17.6497, val_loss: 17.7287, val_MinusLogProbMetric: 17.7287

Epoch 510: val_loss did not improve from 17.66953
196/196 - 64s - loss: 17.6497 - MinusLogProbMetric: 17.6497 - val_loss: 17.7287 - val_MinusLogProbMetric: 17.7287 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 511/1000
2023-09-27 14:43:02.867 
Epoch 511/1000 
	 loss: 17.6829, MinusLogProbMetric: 17.6829, val_loss: 17.6663, val_MinusLogProbMetric: 17.6663

Epoch 511: val_loss improved from 17.66953 to 17.66630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.6829 - MinusLogProbMetric: 17.6829 - val_loss: 17.6663 - val_MinusLogProbMetric: 17.6663 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 512/1000
2023-09-27 14:44:08.049 
Epoch 512/1000 
	 loss: 17.6447, MinusLogProbMetric: 17.6447, val_loss: 17.6267, val_MinusLogProbMetric: 17.6267

Epoch 512: val_loss improved from 17.66630 to 17.62667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.6447 - MinusLogProbMetric: 17.6447 - val_loss: 17.6267 - val_MinusLogProbMetric: 17.6267 - lr: 1.6667e-04 - 65s/epoch - 332ms/step
Epoch 513/1000
2023-09-27 14:45:13.298 
Epoch 513/1000 
	 loss: 17.6629, MinusLogProbMetric: 17.6629, val_loss: 17.6312, val_MinusLogProbMetric: 17.6312

Epoch 513: val_loss did not improve from 17.62667
196/196 - 64s - loss: 17.6629 - MinusLogProbMetric: 17.6629 - val_loss: 17.6312 - val_MinusLogProbMetric: 17.6312 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 514/1000
2023-09-27 14:46:17.441 
Epoch 514/1000 
	 loss: 17.6083, MinusLogProbMetric: 17.6083, val_loss: 17.6366, val_MinusLogProbMetric: 17.6366

Epoch 514: val_loss did not improve from 17.62667
196/196 - 64s - loss: 17.6083 - MinusLogProbMetric: 17.6083 - val_loss: 17.6366 - val_MinusLogProbMetric: 17.6366 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 515/1000
2023-09-27 14:47:21.472 
Epoch 515/1000 
	 loss: 17.6093, MinusLogProbMetric: 17.6093, val_loss: 17.6613, val_MinusLogProbMetric: 17.6613

Epoch 515: val_loss did not improve from 17.62667
196/196 - 64s - loss: 17.6093 - MinusLogProbMetric: 17.6093 - val_loss: 17.6613 - val_MinusLogProbMetric: 17.6613 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 516/1000
2023-09-27 14:48:25.170 
Epoch 516/1000 
	 loss: 17.6899, MinusLogProbMetric: 17.6899, val_loss: 17.7234, val_MinusLogProbMetric: 17.7234

Epoch 516: val_loss did not improve from 17.62667
196/196 - 64s - loss: 17.6899 - MinusLogProbMetric: 17.6899 - val_loss: 17.7234 - val_MinusLogProbMetric: 17.7234 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 517/1000
2023-09-27 14:49:28.810 
Epoch 517/1000 
	 loss: 17.6190, MinusLogProbMetric: 17.6190, val_loss: 17.7273, val_MinusLogProbMetric: 17.7273

Epoch 517: val_loss did not improve from 17.62667
196/196 - 64s - loss: 17.6190 - MinusLogProbMetric: 17.6190 - val_loss: 17.7273 - val_MinusLogProbMetric: 17.7273 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 518/1000
2023-09-27 14:50:32.656 
Epoch 518/1000 
	 loss: 17.6891, MinusLogProbMetric: 17.6891, val_loss: 18.2068, val_MinusLogProbMetric: 18.2068

Epoch 518: val_loss did not improve from 17.62667
196/196 - 64s - loss: 17.6891 - MinusLogProbMetric: 17.6891 - val_loss: 18.2068 - val_MinusLogProbMetric: 18.2068 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 519/1000
2023-09-27 14:51:36.976 
Epoch 519/1000 
	 loss: 17.6257, MinusLogProbMetric: 17.6257, val_loss: 17.9328, val_MinusLogProbMetric: 17.9328

Epoch 519: val_loss did not improve from 17.62667
196/196 - 64s - loss: 17.6257 - MinusLogProbMetric: 17.6257 - val_loss: 17.9328 - val_MinusLogProbMetric: 17.9328 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 520/1000
2023-09-27 14:52:41.355 
Epoch 520/1000 
	 loss: 17.6191, MinusLogProbMetric: 17.6191, val_loss: 17.8080, val_MinusLogProbMetric: 17.8080

Epoch 520: val_loss did not improve from 17.62667
196/196 - 64s - loss: 17.6191 - MinusLogProbMetric: 17.6191 - val_loss: 17.8080 - val_MinusLogProbMetric: 17.8080 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 521/1000
2023-09-27 14:53:45.224 
Epoch 521/1000 
	 loss: 17.6264, MinusLogProbMetric: 17.6264, val_loss: 18.0387, val_MinusLogProbMetric: 18.0387

Epoch 521: val_loss did not improve from 17.62667
196/196 - 64s - loss: 17.6264 - MinusLogProbMetric: 17.6264 - val_loss: 18.0387 - val_MinusLogProbMetric: 18.0387 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 522/1000
2023-09-27 14:54:48.937 
Epoch 522/1000 
	 loss: 17.6067, MinusLogProbMetric: 17.6067, val_loss: 17.6742, val_MinusLogProbMetric: 17.6742

Epoch 522: val_loss did not improve from 17.62667
196/196 - 64s - loss: 17.6067 - MinusLogProbMetric: 17.6067 - val_loss: 17.6742 - val_MinusLogProbMetric: 17.6742 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 523/1000
2023-09-27 14:55:53.208 
Epoch 523/1000 
	 loss: 17.5963, MinusLogProbMetric: 17.5963, val_loss: 17.9497, val_MinusLogProbMetric: 17.9497

Epoch 523: val_loss did not improve from 17.62667
196/196 - 64s - loss: 17.5963 - MinusLogProbMetric: 17.5963 - val_loss: 17.9497 - val_MinusLogProbMetric: 17.9497 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 524/1000
2023-09-27 14:56:56.909 
Epoch 524/1000 
	 loss: 17.7493, MinusLogProbMetric: 17.7493, val_loss: 18.1411, val_MinusLogProbMetric: 18.1411

Epoch 524: val_loss did not improve from 17.62667
196/196 - 64s - loss: 17.7493 - MinusLogProbMetric: 17.7493 - val_loss: 18.1411 - val_MinusLogProbMetric: 18.1411 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 525/1000
2023-09-27 14:58:00.627 
Epoch 525/1000 
	 loss: 17.6531, MinusLogProbMetric: 17.6531, val_loss: 17.6213, val_MinusLogProbMetric: 17.6213

Epoch 525: val_loss improved from 17.62667 to 17.62130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.6531 - MinusLogProbMetric: 17.6531 - val_loss: 17.6213 - val_MinusLogProbMetric: 17.6213 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 526/1000
2023-09-27 14:59:05.312 
Epoch 526/1000 
	 loss: 17.6348, MinusLogProbMetric: 17.6348, val_loss: 17.7485, val_MinusLogProbMetric: 17.7485

Epoch 526: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.6348 - MinusLogProbMetric: 17.6348 - val_loss: 17.7485 - val_MinusLogProbMetric: 17.7485 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 527/1000
2023-09-27 15:00:09.215 
Epoch 527/1000 
	 loss: 17.6557, MinusLogProbMetric: 17.6557, val_loss: 17.8336, val_MinusLogProbMetric: 17.8336

Epoch 527: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.6557 - MinusLogProbMetric: 17.6557 - val_loss: 17.8336 - val_MinusLogProbMetric: 17.8336 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 528/1000
2023-09-27 15:01:13.298 
Epoch 528/1000 
	 loss: 17.6842, MinusLogProbMetric: 17.6842, val_loss: 17.7851, val_MinusLogProbMetric: 17.7851

Epoch 528: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.6842 - MinusLogProbMetric: 17.6842 - val_loss: 17.7851 - val_MinusLogProbMetric: 17.7851 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 529/1000
2023-09-27 15:02:17.215 
Epoch 529/1000 
	 loss: 17.6234, MinusLogProbMetric: 17.6234, val_loss: 17.6988, val_MinusLogProbMetric: 17.6988

Epoch 529: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.6234 - MinusLogProbMetric: 17.6234 - val_loss: 17.6988 - val_MinusLogProbMetric: 17.6988 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 530/1000
2023-09-27 15:03:21.421 
Epoch 530/1000 
	 loss: 17.6112, MinusLogProbMetric: 17.6112, val_loss: 17.7443, val_MinusLogProbMetric: 17.7443

Epoch 530: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.6112 - MinusLogProbMetric: 17.6112 - val_loss: 17.7443 - val_MinusLogProbMetric: 17.7443 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 531/1000
2023-09-27 15:04:25.583 
Epoch 531/1000 
	 loss: 17.6510, MinusLogProbMetric: 17.6510, val_loss: 17.8859, val_MinusLogProbMetric: 17.8859

Epoch 531: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.6510 - MinusLogProbMetric: 17.6510 - val_loss: 17.8859 - val_MinusLogProbMetric: 17.8859 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 532/1000
2023-09-27 15:05:29.261 
Epoch 532/1000 
	 loss: 17.6192, MinusLogProbMetric: 17.6192, val_loss: 17.8077, val_MinusLogProbMetric: 17.8077

Epoch 532: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.6192 - MinusLogProbMetric: 17.6192 - val_loss: 17.8077 - val_MinusLogProbMetric: 17.8077 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 533/1000
2023-09-27 15:06:32.620 
Epoch 533/1000 
	 loss: 17.7579, MinusLogProbMetric: 17.7579, val_loss: 19.3532, val_MinusLogProbMetric: 19.3532

Epoch 533: val_loss did not improve from 17.62130
196/196 - 63s - loss: 17.7579 - MinusLogProbMetric: 17.7579 - val_loss: 19.3532 - val_MinusLogProbMetric: 19.3532 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 534/1000
2023-09-27 15:07:37.394 
Epoch 534/1000 
	 loss: 17.6473, MinusLogProbMetric: 17.6473, val_loss: 17.7972, val_MinusLogProbMetric: 17.7972

Epoch 534: val_loss did not improve from 17.62130
196/196 - 65s - loss: 17.6473 - MinusLogProbMetric: 17.6473 - val_loss: 17.7972 - val_MinusLogProbMetric: 17.7972 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 535/1000
2023-09-27 15:08:41.240 
Epoch 535/1000 
	 loss: 17.7340, MinusLogProbMetric: 17.7340, val_loss: 17.7027, val_MinusLogProbMetric: 17.7027

Epoch 535: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.7340 - MinusLogProbMetric: 17.7340 - val_loss: 17.7027 - val_MinusLogProbMetric: 17.7027 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 536/1000
2023-09-27 15:09:45.256 
Epoch 536/1000 
	 loss: 17.6055, MinusLogProbMetric: 17.6055, val_loss: 17.8253, val_MinusLogProbMetric: 17.8253

Epoch 536: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.6055 - MinusLogProbMetric: 17.6055 - val_loss: 17.8253 - val_MinusLogProbMetric: 17.8253 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 537/1000
2023-09-27 15:10:49.283 
Epoch 537/1000 
	 loss: 17.5763, MinusLogProbMetric: 17.5763, val_loss: 17.8181, val_MinusLogProbMetric: 17.8181

Epoch 537: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.5763 - MinusLogProbMetric: 17.5763 - val_loss: 17.8181 - val_MinusLogProbMetric: 17.8181 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 538/1000
2023-09-27 15:11:53.205 
Epoch 538/1000 
	 loss: 17.5883, MinusLogProbMetric: 17.5883, val_loss: 17.7945, val_MinusLogProbMetric: 17.7945

Epoch 538: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.5883 - MinusLogProbMetric: 17.5883 - val_loss: 17.7945 - val_MinusLogProbMetric: 17.7945 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 539/1000
2023-09-27 15:12:56.893 
Epoch 539/1000 
	 loss: 17.5878, MinusLogProbMetric: 17.5878, val_loss: 17.7407, val_MinusLogProbMetric: 17.7407

Epoch 539: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.5878 - MinusLogProbMetric: 17.5878 - val_loss: 17.7407 - val_MinusLogProbMetric: 17.7407 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 540/1000
2023-09-27 15:14:00.999 
Epoch 540/1000 
	 loss: 17.5962, MinusLogProbMetric: 17.5962, val_loss: 17.9316, val_MinusLogProbMetric: 17.9316

Epoch 540: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.5962 - MinusLogProbMetric: 17.5962 - val_loss: 17.9316 - val_MinusLogProbMetric: 17.9316 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 541/1000
2023-09-27 15:15:05.307 
Epoch 541/1000 
	 loss: 17.7184, MinusLogProbMetric: 17.7184, val_loss: 17.8165, val_MinusLogProbMetric: 17.8165

Epoch 541: val_loss did not improve from 17.62130
196/196 - 64s - loss: 17.7184 - MinusLogProbMetric: 17.7184 - val_loss: 17.8165 - val_MinusLogProbMetric: 17.8165 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 542/1000
2023-09-27 15:16:09.622 
Epoch 542/1000 
	 loss: 17.5922, MinusLogProbMetric: 17.5922, val_loss: 17.6162, val_MinusLogProbMetric: 17.6162

Epoch 542: val_loss improved from 17.62130 to 17.61616, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.5922 - MinusLogProbMetric: 17.5922 - val_loss: 17.6162 - val_MinusLogProbMetric: 17.6162 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 543/1000
2023-09-27 15:17:14.694 
Epoch 543/1000 
	 loss: 17.5853, MinusLogProbMetric: 17.5853, val_loss: 18.1582, val_MinusLogProbMetric: 18.1582

Epoch 543: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.5853 - MinusLogProbMetric: 17.5853 - val_loss: 18.1582 - val_MinusLogProbMetric: 18.1582 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 544/1000
2023-09-27 15:18:18.303 
Epoch 544/1000 
	 loss: 17.6164, MinusLogProbMetric: 17.6164, val_loss: 17.8158, val_MinusLogProbMetric: 17.8158

Epoch 544: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.6164 - MinusLogProbMetric: 17.6164 - val_loss: 17.8158 - val_MinusLogProbMetric: 17.8158 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 545/1000
2023-09-27 15:19:22.384 
Epoch 545/1000 
	 loss: 17.5983, MinusLogProbMetric: 17.5983, val_loss: 17.6320, val_MinusLogProbMetric: 17.6320

Epoch 545: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.5983 - MinusLogProbMetric: 17.5983 - val_loss: 17.6320 - val_MinusLogProbMetric: 17.6320 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 546/1000
2023-09-27 15:20:25.939 
Epoch 546/1000 
	 loss: 17.6331, MinusLogProbMetric: 17.6331, val_loss: 17.6748, val_MinusLogProbMetric: 17.6748

Epoch 546: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.6331 - MinusLogProbMetric: 17.6331 - val_loss: 17.6748 - val_MinusLogProbMetric: 17.6748 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 547/1000
2023-09-27 15:21:28.830 
Epoch 547/1000 
	 loss: 17.6225, MinusLogProbMetric: 17.6225, val_loss: 17.7238, val_MinusLogProbMetric: 17.7238

Epoch 547: val_loss did not improve from 17.61616
196/196 - 63s - loss: 17.6225 - MinusLogProbMetric: 17.6225 - val_loss: 17.7238 - val_MinusLogProbMetric: 17.7238 - lr: 1.6667e-04 - 63s/epoch - 321ms/step
Epoch 548/1000
2023-09-27 15:22:32.856 
Epoch 548/1000 
	 loss: 17.6119, MinusLogProbMetric: 17.6119, val_loss: 18.1030, val_MinusLogProbMetric: 18.1030

Epoch 548: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.6119 - MinusLogProbMetric: 17.6119 - val_loss: 18.1030 - val_MinusLogProbMetric: 18.1030 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 549/1000
2023-09-27 15:23:36.633 
Epoch 549/1000 
	 loss: 17.6482, MinusLogProbMetric: 17.6482, val_loss: 17.6834, val_MinusLogProbMetric: 17.6834

Epoch 549: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.6482 - MinusLogProbMetric: 17.6482 - val_loss: 17.6834 - val_MinusLogProbMetric: 17.6834 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 550/1000
2023-09-27 15:24:40.861 
Epoch 550/1000 
	 loss: 17.6247, MinusLogProbMetric: 17.6247, val_loss: 17.9692, val_MinusLogProbMetric: 17.9692

Epoch 550: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.6247 - MinusLogProbMetric: 17.6247 - val_loss: 17.9692 - val_MinusLogProbMetric: 17.9692 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 551/1000
2023-09-27 15:25:44.998 
Epoch 551/1000 
	 loss: 17.5797, MinusLogProbMetric: 17.5797, val_loss: 17.6664, val_MinusLogProbMetric: 17.6664

Epoch 551: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.5797 - MinusLogProbMetric: 17.5797 - val_loss: 17.6664 - val_MinusLogProbMetric: 17.6664 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 552/1000
2023-09-27 15:26:49.142 
Epoch 552/1000 
	 loss: 17.5825, MinusLogProbMetric: 17.5825, val_loss: 17.6424, val_MinusLogProbMetric: 17.6424

Epoch 552: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.5825 - MinusLogProbMetric: 17.5825 - val_loss: 17.6424 - val_MinusLogProbMetric: 17.6424 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 553/1000
2023-09-27 15:27:53.541 
Epoch 553/1000 
	 loss: 17.6288, MinusLogProbMetric: 17.6288, val_loss: 17.8457, val_MinusLogProbMetric: 17.8457

Epoch 553: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.6288 - MinusLogProbMetric: 17.6288 - val_loss: 17.8457 - val_MinusLogProbMetric: 17.8457 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 554/1000
2023-09-27 15:28:57.597 
Epoch 554/1000 
	 loss: 17.6319, MinusLogProbMetric: 17.6319, val_loss: 17.7825, val_MinusLogProbMetric: 17.7825

Epoch 554: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.6319 - MinusLogProbMetric: 17.6319 - val_loss: 17.7825 - val_MinusLogProbMetric: 17.7825 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 555/1000
2023-09-27 15:30:01.618 
Epoch 555/1000 
	 loss: 17.6219, MinusLogProbMetric: 17.6219, val_loss: 17.6858, val_MinusLogProbMetric: 17.6858

Epoch 555: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.6219 - MinusLogProbMetric: 17.6219 - val_loss: 17.6858 - val_MinusLogProbMetric: 17.6858 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 556/1000
2023-09-27 15:31:05.850 
Epoch 556/1000 
	 loss: 17.6624, MinusLogProbMetric: 17.6624, val_loss: 17.8147, val_MinusLogProbMetric: 17.8147

Epoch 556: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.6624 - MinusLogProbMetric: 17.6624 - val_loss: 17.8147 - val_MinusLogProbMetric: 17.8147 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 557/1000
2023-09-27 15:32:09.701 
Epoch 557/1000 
	 loss: 17.5869, MinusLogProbMetric: 17.5869, val_loss: 17.7217, val_MinusLogProbMetric: 17.7217

Epoch 557: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.5869 - MinusLogProbMetric: 17.5869 - val_loss: 17.7217 - val_MinusLogProbMetric: 17.7217 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 558/1000
2023-09-27 15:33:14.115 
Epoch 558/1000 
	 loss: 17.6282, MinusLogProbMetric: 17.6282, val_loss: 17.9144, val_MinusLogProbMetric: 17.9144

Epoch 558: val_loss did not improve from 17.61616
196/196 - 64s - loss: 17.6282 - MinusLogProbMetric: 17.6282 - val_loss: 17.9144 - val_MinusLogProbMetric: 17.9144 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 559/1000
2023-09-27 15:34:16.878 
Epoch 559/1000 
	 loss: 17.6095, MinusLogProbMetric: 17.6095, val_loss: 17.6309, val_MinusLogProbMetric: 17.6309

Epoch 559: val_loss did not improve from 17.61616
196/196 - 63s - loss: 17.6095 - MinusLogProbMetric: 17.6095 - val_loss: 17.6309 - val_MinusLogProbMetric: 17.6309 - lr: 1.6667e-04 - 63s/epoch - 320ms/step
Epoch 560/1000
2023-09-27 15:35:13.128 
Epoch 560/1000 
	 loss: 17.5447, MinusLogProbMetric: 17.5447, val_loss: 17.7490, val_MinusLogProbMetric: 17.7490

Epoch 560: val_loss did not improve from 17.61616
196/196 - 56s - loss: 17.5447 - MinusLogProbMetric: 17.5447 - val_loss: 17.7490 - val_MinusLogProbMetric: 17.7490 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 561/1000
2023-09-27 15:36:11.103 
Epoch 561/1000 
	 loss: 17.5829, MinusLogProbMetric: 17.5829, val_loss: 17.9887, val_MinusLogProbMetric: 17.9887

Epoch 561: val_loss did not improve from 17.61616
196/196 - 58s - loss: 17.5829 - MinusLogProbMetric: 17.5829 - val_loss: 17.9887 - val_MinusLogProbMetric: 17.9887 - lr: 1.6667e-04 - 58s/epoch - 296ms/step
Epoch 562/1000
2023-09-27 15:37:12.667 
Epoch 562/1000 
	 loss: 17.6038, MinusLogProbMetric: 17.6038, val_loss: 17.8310, val_MinusLogProbMetric: 17.8310

Epoch 562: val_loss did not improve from 17.61616
196/196 - 62s - loss: 17.6038 - MinusLogProbMetric: 17.6038 - val_loss: 17.8310 - val_MinusLogProbMetric: 17.8310 - lr: 1.6667e-04 - 62s/epoch - 314ms/step
Epoch 563/1000
2023-09-27 15:38:08.519 
Epoch 563/1000 
	 loss: 17.6163, MinusLogProbMetric: 17.6163, val_loss: 17.5905, val_MinusLogProbMetric: 17.5905

Epoch 563: val_loss improved from 17.61616 to 17.59046, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 57s - loss: 17.6163 - MinusLogProbMetric: 17.6163 - val_loss: 17.5905 - val_MinusLogProbMetric: 17.5905 - lr: 1.6667e-04 - 57s/epoch - 290ms/step
Epoch 564/1000
2023-09-27 15:39:12.841 
Epoch 564/1000 
	 loss: 17.6319, MinusLogProbMetric: 17.6319, val_loss: 17.7269, val_MinusLogProbMetric: 17.7269

Epoch 564: val_loss did not improve from 17.59046
196/196 - 63s - loss: 17.6319 - MinusLogProbMetric: 17.6319 - val_loss: 17.7269 - val_MinusLogProbMetric: 17.7269 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 565/1000
2023-09-27 15:40:16.177 
Epoch 565/1000 
	 loss: 17.5708, MinusLogProbMetric: 17.5708, val_loss: 17.6520, val_MinusLogProbMetric: 17.6520

Epoch 565: val_loss did not improve from 17.59046
196/196 - 63s - loss: 17.5708 - MinusLogProbMetric: 17.5708 - val_loss: 17.6520 - val_MinusLogProbMetric: 17.6520 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 566/1000
2023-09-27 15:41:19.934 
Epoch 566/1000 
	 loss: 17.5625, MinusLogProbMetric: 17.5625, val_loss: 17.7483, val_MinusLogProbMetric: 17.7483

Epoch 566: val_loss did not improve from 17.59046
196/196 - 64s - loss: 17.5625 - MinusLogProbMetric: 17.5625 - val_loss: 17.7483 - val_MinusLogProbMetric: 17.7483 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 567/1000
2023-09-27 15:42:24.109 
Epoch 567/1000 
	 loss: 17.5980, MinusLogProbMetric: 17.5980, val_loss: 17.7567, val_MinusLogProbMetric: 17.7567

Epoch 567: val_loss did not improve from 17.59046
196/196 - 64s - loss: 17.5980 - MinusLogProbMetric: 17.5980 - val_loss: 17.7567 - val_MinusLogProbMetric: 17.7567 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 568/1000
2023-09-27 15:43:28.497 
Epoch 568/1000 
	 loss: 17.5886, MinusLogProbMetric: 17.5886, val_loss: 17.7291, val_MinusLogProbMetric: 17.7291

Epoch 568: val_loss did not improve from 17.59046
196/196 - 64s - loss: 17.5886 - MinusLogProbMetric: 17.5886 - val_loss: 17.7291 - val_MinusLogProbMetric: 17.7291 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 569/1000
2023-09-27 15:44:32.888 
Epoch 569/1000 
	 loss: 17.5857, MinusLogProbMetric: 17.5857, val_loss: 17.8027, val_MinusLogProbMetric: 17.8027

Epoch 569: val_loss did not improve from 17.59046
196/196 - 64s - loss: 17.5857 - MinusLogProbMetric: 17.5857 - val_loss: 17.8027 - val_MinusLogProbMetric: 17.8027 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 570/1000
2023-09-27 15:45:37.312 
Epoch 570/1000 
	 loss: 17.6393, MinusLogProbMetric: 17.6393, val_loss: 17.7130, val_MinusLogProbMetric: 17.7130

Epoch 570: val_loss did not improve from 17.59046
196/196 - 64s - loss: 17.6393 - MinusLogProbMetric: 17.6393 - val_loss: 17.7130 - val_MinusLogProbMetric: 17.7130 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 571/1000
2023-09-27 15:46:41.187 
Epoch 571/1000 
	 loss: 17.5515, MinusLogProbMetric: 17.5515, val_loss: 17.6545, val_MinusLogProbMetric: 17.6545

Epoch 571: val_loss did not improve from 17.59046
196/196 - 64s - loss: 17.5515 - MinusLogProbMetric: 17.5515 - val_loss: 17.6545 - val_MinusLogProbMetric: 17.6545 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 572/1000
2023-09-27 15:47:45.646 
Epoch 572/1000 
	 loss: 17.5902, MinusLogProbMetric: 17.5902, val_loss: 17.6545, val_MinusLogProbMetric: 17.6545

Epoch 572: val_loss did not improve from 17.59046
196/196 - 64s - loss: 17.5902 - MinusLogProbMetric: 17.5902 - val_loss: 17.6545 - val_MinusLogProbMetric: 17.6545 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 573/1000
2023-09-27 15:48:42.799 
Epoch 573/1000 
	 loss: 17.5625, MinusLogProbMetric: 17.5625, val_loss: 17.6800, val_MinusLogProbMetric: 17.6800

Epoch 573: val_loss did not improve from 17.59046
196/196 - 57s - loss: 17.5625 - MinusLogProbMetric: 17.5625 - val_loss: 17.6800 - val_MinusLogProbMetric: 17.6800 - lr: 1.6667e-04 - 57s/epoch - 292ms/step
Epoch 574/1000
2023-09-27 15:49:40.930 
Epoch 574/1000 
	 loss: 17.5588, MinusLogProbMetric: 17.5588, val_loss: 17.5638, val_MinusLogProbMetric: 17.5638

Epoch 574: val_loss improved from 17.59046 to 17.56379, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 59s - loss: 17.5588 - MinusLogProbMetric: 17.5588 - val_loss: 17.5638 - val_MinusLogProbMetric: 17.5638 - lr: 1.6667e-04 - 59s/epoch - 301ms/step
Epoch 575/1000
2023-09-27 15:50:43.406 
Epoch 575/1000 
	 loss: 17.6609, MinusLogProbMetric: 17.6609, val_loss: 17.6195, val_MinusLogProbMetric: 17.6195

Epoch 575: val_loss did not improve from 17.56379
196/196 - 62s - loss: 17.6609 - MinusLogProbMetric: 17.6609 - val_loss: 17.6195 - val_MinusLogProbMetric: 17.6195 - lr: 1.6667e-04 - 62s/epoch - 314ms/step
Epoch 576/1000
2023-09-27 15:51:37.955 
Epoch 576/1000 
	 loss: 17.5992, MinusLogProbMetric: 17.5992, val_loss: 17.8452, val_MinusLogProbMetric: 17.8452

Epoch 576: val_loss did not improve from 17.56379
196/196 - 55s - loss: 17.5992 - MinusLogProbMetric: 17.5992 - val_loss: 17.8452 - val_MinusLogProbMetric: 17.8452 - lr: 1.6667e-04 - 55s/epoch - 278ms/step
Epoch 577/1000
2023-09-27 15:52:39.464 
Epoch 577/1000 
	 loss: 17.5892, MinusLogProbMetric: 17.5892, val_loss: 17.7516, val_MinusLogProbMetric: 17.7516

Epoch 577: val_loss did not improve from 17.56379
196/196 - 62s - loss: 17.5892 - MinusLogProbMetric: 17.5892 - val_loss: 17.7516 - val_MinusLogProbMetric: 17.7516 - lr: 1.6667e-04 - 62s/epoch - 314ms/step
Epoch 578/1000
2023-09-27 15:53:43.183 
Epoch 578/1000 
	 loss: 17.5717, MinusLogProbMetric: 17.5717, val_loss: 17.5724, val_MinusLogProbMetric: 17.5724

Epoch 578: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5717 - MinusLogProbMetric: 17.5717 - val_loss: 17.5724 - val_MinusLogProbMetric: 17.5724 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 579/1000
2023-09-27 15:54:44.657 
Epoch 579/1000 
	 loss: 17.5877, MinusLogProbMetric: 17.5877, val_loss: 17.7795, val_MinusLogProbMetric: 17.7795

Epoch 579: val_loss did not improve from 17.56379
196/196 - 61s - loss: 17.5877 - MinusLogProbMetric: 17.5877 - val_loss: 17.7795 - val_MinusLogProbMetric: 17.7795 - lr: 1.6667e-04 - 61s/epoch - 314ms/step
Epoch 580/1000
2023-09-27 15:55:46.104 
Epoch 580/1000 
	 loss: 17.5989, MinusLogProbMetric: 17.5989, val_loss: 18.0533, val_MinusLogProbMetric: 18.0533

Epoch 580: val_loss did not improve from 17.56379
196/196 - 61s - loss: 17.5989 - MinusLogProbMetric: 17.5989 - val_loss: 18.0533 - val_MinusLogProbMetric: 18.0533 - lr: 1.6667e-04 - 61s/epoch - 313ms/step
Epoch 581/1000
2023-09-27 15:56:48.672 
Epoch 581/1000 
	 loss: 17.6112, MinusLogProbMetric: 17.6112, val_loss: 17.8696, val_MinusLogProbMetric: 17.8696

Epoch 581: val_loss did not improve from 17.56379
196/196 - 63s - loss: 17.6112 - MinusLogProbMetric: 17.6112 - val_loss: 17.8696 - val_MinusLogProbMetric: 17.8696 - lr: 1.6667e-04 - 63s/epoch - 319ms/step
Epoch 582/1000
2023-09-27 15:57:52.904 
Epoch 582/1000 
	 loss: 17.6223, MinusLogProbMetric: 17.6223, val_loss: 17.8373, val_MinusLogProbMetric: 17.8373

Epoch 582: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.6223 - MinusLogProbMetric: 17.6223 - val_loss: 17.8373 - val_MinusLogProbMetric: 17.8373 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 583/1000
2023-09-27 15:58:57.183 
Epoch 583/1000 
	 loss: 17.5826, MinusLogProbMetric: 17.5826, val_loss: 17.6580, val_MinusLogProbMetric: 17.6580

Epoch 583: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5826 - MinusLogProbMetric: 17.5826 - val_loss: 17.6580 - val_MinusLogProbMetric: 17.6580 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 584/1000
2023-09-27 16:00:00.137 
Epoch 584/1000 
	 loss: 17.5480, MinusLogProbMetric: 17.5480, val_loss: 17.6916, val_MinusLogProbMetric: 17.6916

Epoch 584: val_loss did not improve from 17.56379
196/196 - 63s - loss: 17.5480 - MinusLogProbMetric: 17.5480 - val_loss: 17.6916 - val_MinusLogProbMetric: 17.6916 - lr: 1.6667e-04 - 63s/epoch - 321ms/step
Epoch 585/1000
2023-09-27 16:00:59.965 
Epoch 585/1000 
	 loss: 17.5349, MinusLogProbMetric: 17.5349, val_loss: 17.5947, val_MinusLogProbMetric: 17.5947

Epoch 585: val_loss did not improve from 17.56379
196/196 - 60s - loss: 17.5349 - MinusLogProbMetric: 17.5349 - val_loss: 17.5947 - val_MinusLogProbMetric: 17.5947 - lr: 1.6667e-04 - 60s/epoch - 305ms/step
Epoch 586/1000
2023-09-27 16:02:03.452 
Epoch 586/1000 
	 loss: 17.6442, MinusLogProbMetric: 17.6442, val_loss: 17.6364, val_MinusLogProbMetric: 17.6364

Epoch 586: val_loss did not improve from 17.56379
196/196 - 63s - loss: 17.6442 - MinusLogProbMetric: 17.6442 - val_loss: 17.6364 - val_MinusLogProbMetric: 17.6364 - lr: 1.6667e-04 - 63s/epoch - 324ms/step
Epoch 587/1000
2023-09-27 16:03:07.089 
Epoch 587/1000 
	 loss: 17.5827, MinusLogProbMetric: 17.5827, val_loss: 17.8834, val_MinusLogProbMetric: 17.8834

Epoch 587: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5827 - MinusLogProbMetric: 17.5827 - val_loss: 17.8834 - val_MinusLogProbMetric: 17.8834 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 588/1000
2023-09-27 16:04:11.009 
Epoch 588/1000 
	 loss: 17.5538, MinusLogProbMetric: 17.5538, val_loss: 17.6972, val_MinusLogProbMetric: 17.6972

Epoch 588: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5538 - MinusLogProbMetric: 17.5538 - val_loss: 17.6972 - val_MinusLogProbMetric: 17.6972 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 589/1000
2023-09-27 16:05:15.363 
Epoch 589/1000 
	 loss: 17.5612, MinusLogProbMetric: 17.5612, val_loss: 18.0441, val_MinusLogProbMetric: 18.0441

Epoch 589: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5612 - MinusLogProbMetric: 17.5612 - val_loss: 18.0441 - val_MinusLogProbMetric: 18.0441 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 590/1000
2023-09-27 16:06:19.936 
Epoch 590/1000 
	 loss: 17.5715, MinusLogProbMetric: 17.5715, val_loss: 17.9551, val_MinusLogProbMetric: 17.9551

Epoch 590: val_loss did not improve from 17.56379
196/196 - 65s - loss: 17.5715 - MinusLogProbMetric: 17.5715 - val_loss: 17.9551 - val_MinusLogProbMetric: 17.9551 - lr: 1.6667e-04 - 65s/epoch - 329ms/step
Epoch 591/1000
2023-09-27 16:07:22.495 
Epoch 591/1000 
	 loss: 17.6104, MinusLogProbMetric: 17.6104, val_loss: 17.7176, val_MinusLogProbMetric: 17.7176

Epoch 591: val_loss did not improve from 17.56379
196/196 - 63s - loss: 17.6104 - MinusLogProbMetric: 17.6104 - val_loss: 17.7176 - val_MinusLogProbMetric: 17.7176 - lr: 1.6667e-04 - 63s/epoch - 319ms/step
Epoch 592/1000
2023-09-27 16:08:22.879 
Epoch 592/1000 
	 loss: 17.5700, MinusLogProbMetric: 17.5700, val_loss: 17.8766, val_MinusLogProbMetric: 17.8766

Epoch 592: val_loss did not improve from 17.56379
196/196 - 60s - loss: 17.5700 - MinusLogProbMetric: 17.5700 - val_loss: 17.8766 - val_MinusLogProbMetric: 17.8766 - lr: 1.6667e-04 - 60s/epoch - 308ms/step
Epoch 593/1000
2023-09-27 16:09:23.868 
Epoch 593/1000 
	 loss: 17.5990, MinusLogProbMetric: 17.5990, val_loss: 17.7347, val_MinusLogProbMetric: 17.7347

Epoch 593: val_loss did not improve from 17.56379
196/196 - 61s - loss: 17.5990 - MinusLogProbMetric: 17.5990 - val_loss: 17.7347 - val_MinusLogProbMetric: 17.7347 - lr: 1.6667e-04 - 61s/epoch - 311ms/step
Epoch 594/1000
2023-09-27 16:10:25.796 
Epoch 594/1000 
	 loss: 17.5144, MinusLogProbMetric: 17.5144, val_loss: 17.6755, val_MinusLogProbMetric: 17.6755

Epoch 594: val_loss did not improve from 17.56379
196/196 - 62s - loss: 17.5144 - MinusLogProbMetric: 17.5144 - val_loss: 17.6755 - val_MinusLogProbMetric: 17.6755 - lr: 1.6667e-04 - 62s/epoch - 316ms/step
Epoch 595/1000
2023-09-27 16:11:27.242 
Epoch 595/1000 
	 loss: 17.5399, MinusLogProbMetric: 17.5399, val_loss: 17.6277, val_MinusLogProbMetric: 17.6277

Epoch 595: val_loss did not improve from 17.56379
196/196 - 61s - loss: 17.5399 - MinusLogProbMetric: 17.5399 - val_loss: 17.6277 - val_MinusLogProbMetric: 17.6277 - lr: 1.6667e-04 - 61s/epoch - 313ms/step
Epoch 596/1000
2023-09-27 16:12:30.003 
Epoch 596/1000 
	 loss: 17.5377, MinusLogProbMetric: 17.5377, val_loss: 17.7146, val_MinusLogProbMetric: 17.7146

Epoch 596: val_loss did not improve from 17.56379
196/196 - 63s - loss: 17.5377 - MinusLogProbMetric: 17.5377 - val_loss: 17.7146 - val_MinusLogProbMetric: 17.7146 - lr: 1.6667e-04 - 63s/epoch - 320ms/step
Epoch 597/1000
2023-09-27 16:13:31.932 
Epoch 597/1000 
	 loss: 17.5909, MinusLogProbMetric: 17.5909, val_loss: 17.6786, val_MinusLogProbMetric: 17.6786

Epoch 597: val_loss did not improve from 17.56379
196/196 - 62s - loss: 17.5909 - MinusLogProbMetric: 17.5909 - val_loss: 17.6786 - val_MinusLogProbMetric: 17.6786 - lr: 1.6667e-04 - 62s/epoch - 316ms/step
Epoch 598/1000
2023-09-27 16:14:32.914 
Epoch 598/1000 
	 loss: 17.5336, MinusLogProbMetric: 17.5336, val_loss: 17.6223, val_MinusLogProbMetric: 17.6223

Epoch 598: val_loss did not improve from 17.56379
196/196 - 61s - loss: 17.5336 - MinusLogProbMetric: 17.5336 - val_loss: 17.6223 - val_MinusLogProbMetric: 17.6223 - lr: 1.6667e-04 - 61s/epoch - 311ms/step
Epoch 599/1000
2023-09-27 16:15:34.261 
Epoch 599/1000 
	 loss: 17.5584, MinusLogProbMetric: 17.5584, val_loss: 17.9418, val_MinusLogProbMetric: 17.9418

Epoch 599: val_loss did not improve from 17.56379
196/196 - 61s - loss: 17.5584 - MinusLogProbMetric: 17.5584 - val_loss: 17.9418 - val_MinusLogProbMetric: 17.9418 - lr: 1.6667e-04 - 61s/epoch - 313ms/step
Epoch 600/1000
2023-09-27 16:16:38.144 
Epoch 600/1000 
	 loss: 17.5268, MinusLogProbMetric: 17.5268, val_loss: 17.7669, val_MinusLogProbMetric: 17.7669

Epoch 600: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5268 - MinusLogProbMetric: 17.5268 - val_loss: 17.7669 - val_MinusLogProbMetric: 17.7669 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 601/1000
2023-09-27 16:17:41.518 
Epoch 601/1000 
	 loss: 17.5456, MinusLogProbMetric: 17.5456, val_loss: 17.6332, val_MinusLogProbMetric: 17.6332

Epoch 601: val_loss did not improve from 17.56379
196/196 - 63s - loss: 17.5456 - MinusLogProbMetric: 17.5456 - val_loss: 17.6332 - val_MinusLogProbMetric: 17.6332 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 602/1000
2023-09-27 16:18:42.092 
Epoch 602/1000 
	 loss: 17.5317, MinusLogProbMetric: 17.5317, val_loss: 17.7809, val_MinusLogProbMetric: 17.7809

Epoch 602: val_loss did not improve from 17.56379
196/196 - 61s - loss: 17.5317 - MinusLogProbMetric: 17.5317 - val_loss: 17.7809 - val_MinusLogProbMetric: 17.7809 - lr: 1.6667e-04 - 61s/epoch - 309ms/step
Epoch 603/1000
2023-09-27 16:19:42.119 
Epoch 603/1000 
	 loss: 17.5959, MinusLogProbMetric: 17.5959, val_loss: 17.7871, val_MinusLogProbMetric: 17.7871

Epoch 603: val_loss did not improve from 17.56379
196/196 - 60s - loss: 17.5959 - MinusLogProbMetric: 17.5959 - val_loss: 17.7871 - val_MinusLogProbMetric: 17.7871 - lr: 1.6667e-04 - 60s/epoch - 306ms/step
Epoch 604/1000
2023-09-27 16:20:45.921 
Epoch 604/1000 
	 loss: 17.6080, MinusLogProbMetric: 17.6080, val_loss: 18.4683, val_MinusLogProbMetric: 18.4683

Epoch 604: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.6080 - MinusLogProbMetric: 17.6080 - val_loss: 18.4683 - val_MinusLogProbMetric: 18.4683 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 605/1000
2023-09-27 16:21:49.769 
Epoch 605/1000 
	 loss: 17.6232, MinusLogProbMetric: 17.6232, val_loss: 17.8402, val_MinusLogProbMetric: 17.8402

Epoch 605: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.6232 - MinusLogProbMetric: 17.6232 - val_loss: 17.8402 - val_MinusLogProbMetric: 17.8402 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 606/1000
2023-09-27 16:22:47.318 
Epoch 606/1000 
	 loss: 17.5564, MinusLogProbMetric: 17.5564, val_loss: 17.8025, val_MinusLogProbMetric: 17.8025

Epoch 606: val_loss did not improve from 17.56379
196/196 - 58s - loss: 17.5564 - MinusLogProbMetric: 17.5564 - val_loss: 17.8025 - val_MinusLogProbMetric: 17.8025 - lr: 1.6667e-04 - 58s/epoch - 294ms/step
Epoch 607/1000
2023-09-27 16:23:50.720 
Epoch 607/1000 
	 loss: 17.5154, MinusLogProbMetric: 17.5154, val_loss: 17.8000, val_MinusLogProbMetric: 17.8000

Epoch 607: val_loss did not improve from 17.56379
196/196 - 63s - loss: 17.5154 - MinusLogProbMetric: 17.5154 - val_loss: 17.8000 - val_MinusLogProbMetric: 17.8000 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 608/1000
2023-09-27 16:24:54.524 
Epoch 608/1000 
	 loss: 17.5550, MinusLogProbMetric: 17.5550, val_loss: 17.6282, val_MinusLogProbMetric: 17.6282

Epoch 608: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5550 - MinusLogProbMetric: 17.5550 - val_loss: 17.6282 - val_MinusLogProbMetric: 17.6282 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 609/1000
2023-09-27 16:25:58.469 
Epoch 609/1000 
	 loss: 17.5615, MinusLogProbMetric: 17.5615, val_loss: 17.7397, val_MinusLogProbMetric: 17.7397

Epoch 609: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5615 - MinusLogProbMetric: 17.5615 - val_loss: 17.7397 - val_MinusLogProbMetric: 17.7397 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 610/1000
2023-09-27 16:27:02.297 
Epoch 610/1000 
	 loss: 17.5858, MinusLogProbMetric: 17.5858, val_loss: 17.8082, val_MinusLogProbMetric: 17.8082

Epoch 610: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5858 - MinusLogProbMetric: 17.5858 - val_loss: 17.8082 - val_MinusLogProbMetric: 17.8082 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 611/1000
2023-09-27 16:28:06.194 
Epoch 611/1000 
	 loss: 17.5503, MinusLogProbMetric: 17.5503, val_loss: 17.6381, val_MinusLogProbMetric: 17.6381

Epoch 611: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5503 - MinusLogProbMetric: 17.5503 - val_loss: 17.6381 - val_MinusLogProbMetric: 17.6381 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 612/1000
2023-09-27 16:29:09.793 
Epoch 612/1000 
	 loss: 17.5959, MinusLogProbMetric: 17.5959, val_loss: 17.6668, val_MinusLogProbMetric: 17.6668

Epoch 612: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5959 - MinusLogProbMetric: 17.5959 - val_loss: 17.6668 - val_MinusLogProbMetric: 17.6668 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 613/1000
2023-09-27 16:30:13.424 
Epoch 613/1000 
	 loss: 17.5570, MinusLogProbMetric: 17.5570, val_loss: 17.6546, val_MinusLogProbMetric: 17.6546

Epoch 613: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5570 - MinusLogProbMetric: 17.5570 - val_loss: 17.6546 - val_MinusLogProbMetric: 17.6546 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 614/1000
2023-09-27 16:31:13.384 
Epoch 614/1000 
	 loss: 17.5146, MinusLogProbMetric: 17.5146, val_loss: 17.6505, val_MinusLogProbMetric: 17.6505

Epoch 614: val_loss did not improve from 17.56379
196/196 - 60s - loss: 17.5146 - MinusLogProbMetric: 17.5146 - val_loss: 17.6505 - val_MinusLogProbMetric: 17.6505 - lr: 1.6667e-04 - 60s/epoch - 306ms/step
Epoch 615/1000
2023-09-27 16:32:11.272 
Epoch 615/1000 
	 loss: 17.5921, MinusLogProbMetric: 17.5921, val_loss: 17.6720, val_MinusLogProbMetric: 17.6720

Epoch 615: val_loss did not improve from 17.56379
196/196 - 58s - loss: 17.5921 - MinusLogProbMetric: 17.5921 - val_loss: 17.6720 - val_MinusLogProbMetric: 17.6720 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 616/1000
2023-09-27 16:33:13.638 
Epoch 616/1000 
	 loss: 17.4947, MinusLogProbMetric: 17.4947, val_loss: 17.6860, val_MinusLogProbMetric: 17.6860

Epoch 616: val_loss did not improve from 17.56379
196/196 - 62s - loss: 17.4947 - MinusLogProbMetric: 17.4947 - val_loss: 17.6860 - val_MinusLogProbMetric: 17.6860 - lr: 1.6667e-04 - 62s/epoch - 318ms/step
Epoch 617/1000
2023-09-27 16:34:14.394 
Epoch 617/1000 
	 loss: 17.5435, MinusLogProbMetric: 17.5435, val_loss: 17.6385, val_MinusLogProbMetric: 17.6385

Epoch 617: val_loss did not improve from 17.56379
196/196 - 61s - loss: 17.5435 - MinusLogProbMetric: 17.5435 - val_loss: 17.6385 - val_MinusLogProbMetric: 17.6385 - lr: 1.6667e-04 - 61s/epoch - 310ms/step
Epoch 618/1000
2023-09-27 16:35:18.783 
Epoch 618/1000 
	 loss: 17.5358, MinusLogProbMetric: 17.5358, val_loss: 17.6783, val_MinusLogProbMetric: 17.6783

Epoch 618: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5358 - MinusLogProbMetric: 17.5358 - val_loss: 17.6783 - val_MinusLogProbMetric: 17.6783 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 619/1000
2023-09-27 16:36:22.530 
Epoch 619/1000 
	 loss: 17.5677, MinusLogProbMetric: 17.5677, val_loss: 17.6760, val_MinusLogProbMetric: 17.6760

Epoch 619: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5677 - MinusLogProbMetric: 17.5677 - val_loss: 17.6760 - val_MinusLogProbMetric: 17.6760 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 620/1000
2023-09-27 16:37:26.940 
Epoch 620/1000 
	 loss: 17.5380, MinusLogProbMetric: 17.5380, val_loss: 17.5758, val_MinusLogProbMetric: 17.5758

Epoch 620: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5380 - MinusLogProbMetric: 17.5380 - val_loss: 17.5758 - val_MinusLogProbMetric: 17.5758 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 621/1000
2023-09-27 16:38:30.933 
Epoch 621/1000 
	 loss: 17.6048, MinusLogProbMetric: 17.6048, val_loss: 17.6392, val_MinusLogProbMetric: 17.6392

Epoch 621: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.6048 - MinusLogProbMetric: 17.6048 - val_loss: 17.6392 - val_MinusLogProbMetric: 17.6392 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 622/1000
2023-09-27 16:39:34.320 
Epoch 622/1000 
	 loss: 17.5872, MinusLogProbMetric: 17.5872, val_loss: 18.0506, val_MinusLogProbMetric: 18.0506

Epoch 622: val_loss did not improve from 17.56379
196/196 - 63s - loss: 17.5872 - MinusLogProbMetric: 17.5872 - val_loss: 18.0506 - val_MinusLogProbMetric: 18.0506 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 623/1000
2023-09-27 16:40:38.102 
Epoch 623/1000 
	 loss: 17.5284, MinusLogProbMetric: 17.5284, val_loss: 17.6465, val_MinusLogProbMetric: 17.6465

Epoch 623: val_loss did not improve from 17.56379
196/196 - 64s - loss: 17.5284 - MinusLogProbMetric: 17.5284 - val_loss: 17.6465 - val_MinusLogProbMetric: 17.6465 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 624/1000
2023-09-27 16:41:42.816 
Epoch 624/1000 
	 loss: 17.5225, MinusLogProbMetric: 17.5225, val_loss: 17.5432, val_MinusLogProbMetric: 17.5432

Epoch 624: val_loss improved from 17.56379 to 17.54325, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 17.5225 - MinusLogProbMetric: 17.5225 - val_loss: 17.5432 - val_MinusLogProbMetric: 17.5432 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 625/1000
2023-09-27 16:42:47.892 
Epoch 625/1000 
	 loss: 17.5635, MinusLogProbMetric: 17.5635, val_loss: 17.5983, val_MinusLogProbMetric: 17.5983

Epoch 625: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5635 - MinusLogProbMetric: 17.5635 - val_loss: 17.5983 - val_MinusLogProbMetric: 17.5983 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 626/1000
2023-09-27 16:43:51.941 
Epoch 626/1000 
	 loss: 17.5515, MinusLogProbMetric: 17.5515, val_loss: 18.0132, val_MinusLogProbMetric: 18.0132

Epoch 626: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5515 - MinusLogProbMetric: 17.5515 - val_loss: 18.0132 - val_MinusLogProbMetric: 18.0132 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 627/1000
2023-09-27 16:44:56.343 
Epoch 627/1000 
	 loss: 17.6124, MinusLogProbMetric: 17.6124, val_loss: 17.5884, val_MinusLogProbMetric: 17.5884

Epoch 627: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.6124 - MinusLogProbMetric: 17.6124 - val_loss: 17.5884 - val_MinusLogProbMetric: 17.5884 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 628/1000
2023-09-27 16:46:00.625 
Epoch 628/1000 
	 loss: 17.5538, MinusLogProbMetric: 17.5538, val_loss: 18.0535, val_MinusLogProbMetric: 18.0535

Epoch 628: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5538 - MinusLogProbMetric: 17.5538 - val_loss: 18.0535 - val_MinusLogProbMetric: 18.0535 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 629/1000
2023-09-27 16:47:04.663 
Epoch 629/1000 
	 loss: 17.5284, MinusLogProbMetric: 17.5284, val_loss: 17.5869, val_MinusLogProbMetric: 17.5869

Epoch 629: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5284 - MinusLogProbMetric: 17.5284 - val_loss: 17.5869 - val_MinusLogProbMetric: 17.5869 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 630/1000
2023-09-27 16:48:08.678 
Epoch 630/1000 
	 loss: 17.5513, MinusLogProbMetric: 17.5513, val_loss: 17.6248, val_MinusLogProbMetric: 17.6248

Epoch 630: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5513 - MinusLogProbMetric: 17.5513 - val_loss: 17.6248 - val_MinusLogProbMetric: 17.6248 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 631/1000
2023-09-27 16:49:12.612 
Epoch 631/1000 
	 loss: 17.5464, MinusLogProbMetric: 17.5464, val_loss: 17.7019, val_MinusLogProbMetric: 17.7019

Epoch 631: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5464 - MinusLogProbMetric: 17.5464 - val_loss: 17.7019 - val_MinusLogProbMetric: 17.7019 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 632/1000
2023-09-27 16:50:16.884 
Epoch 632/1000 
	 loss: 17.5628, MinusLogProbMetric: 17.5628, val_loss: 18.0073, val_MinusLogProbMetric: 18.0073

Epoch 632: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5628 - MinusLogProbMetric: 17.5628 - val_loss: 18.0073 - val_MinusLogProbMetric: 18.0073 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 633/1000
2023-09-27 16:51:21.303 
Epoch 633/1000 
	 loss: 17.5948, MinusLogProbMetric: 17.5948, val_loss: 17.9816, val_MinusLogProbMetric: 17.9816

Epoch 633: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5948 - MinusLogProbMetric: 17.5948 - val_loss: 17.9816 - val_MinusLogProbMetric: 17.9816 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 634/1000
2023-09-27 16:52:25.611 
Epoch 634/1000 
	 loss: 17.5119, MinusLogProbMetric: 17.5119, val_loss: 17.6829, val_MinusLogProbMetric: 17.6829

Epoch 634: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5119 - MinusLogProbMetric: 17.5119 - val_loss: 17.6829 - val_MinusLogProbMetric: 17.6829 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 635/1000
2023-09-27 16:53:29.499 
Epoch 635/1000 
	 loss: 17.5649, MinusLogProbMetric: 17.5649, val_loss: 17.5801, val_MinusLogProbMetric: 17.5801

Epoch 635: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5649 - MinusLogProbMetric: 17.5649 - val_loss: 17.5801 - val_MinusLogProbMetric: 17.5801 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 636/1000
2023-09-27 16:54:33.545 
Epoch 636/1000 
	 loss: 17.5401, MinusLogProbMetric: 17.5401, val_loss: 17.6078, val_MinusLogProbMetric: 17.6078

Epoch 636: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5401 - MinusLogProbMetric: 17.5401 - val_loss: 17.6078 - val_MinusLogProbMetric: 17.6078 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 637/1000
2023-09-27 16:55:37.920 
Epoch 637/1000 
	 loss: 17.5486, MinusLogProbMetric: 17.5486, val_loss: 17.6458, val_MinusLogProbMetric: 17.6458

Epoch 637: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5486 - MinusLogProbMetric: 17.5486 - val_loss: 17.6458 - val_MinusLogProbMetric: 17.6458 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 638/1000
2023-09-27 16:56:41.958 
Epoch 638/1000 
	 loss: 17.5143, MinusLogProbMetric: 17.5143, val_loss: 17.6730, val_MinusLogProbMetric: 17.6730

Epoch 638: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5143 - MinusLogProbMetric: 17.5143 - val_loss: 17.6730 - val_MinusLogProbMetric: 17.6730 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 639/1000
2023-09-27 16:57:45.861 
Epoch 639/1000 
	 loss: 17.4829, MinusLogProbMetric: 17.4829, val_loss: 17.6596, val_MinusLogProbMetric: 17.6596

Epoch 639: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.4829 - MinusLogProbMetric: 17.4829 - val_loss: 17.6596 - val_MinusLogProbMetric: 17.6596 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 640/1000
2023-09-27 16:58:50.508 
Epoch 640/1000 
	 loss: 17.5141, MinusLogProbMetric: 17.5141, val_loss: 17.5973, val_MinusLogProbMetric: 17.5973

Epoch 640: val_loss did not improve from 17.54325
196/196 - 65s - loss: 17.5141 - MinusLogProbMetric: 17.5141 - val_loss: 17.5973 - val_MinusLogProbMetric: 17.5973 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 641/1000
2023-09-27 16:59:54.430 
Epoch 641/1000 
	 loss: 17.4986, MinusLogProbMetric: 17.4986, val_loss: 17.6214, val_MinusLogProbMetric: 17.6214

Epoch 641: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.4986 - MinusLogProbMetric: 17.4986 - val_loss: 17.6214 - val_MinusLogProbMetric: 17.6214 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 642/1000
2023-09-27 17:00:58.761 
Epoch 642/1000 
	 loss: 17.6617, MinusLogProbMetric: 17.6617, val_loss: 17.6481, val_MinusLogProbMetric: 17.6481

Epoch 642: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.6617 - MinusLogProbMetric: 17.6617 - val_loss: 17.6481 - val_MinusLogProbMetric: 17.6481 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 643/1000
2023-09-27 17:02:02.613 
Epoch 643/1000 
	 loss: 17.5071, MinusLogProbMetric: 17.5071, val_loss: 17.5717, val_MinusLogProbMetric: 17.5717

Epoch 643: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5071 - MinusLogProbMetric: 17.5071 - val_loss: 17.5717 - val_MinusLogProbMetric: 17.5717 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 644/1000
2023-09-27 17:03:06.810 
Epoch 644/1000 
	 loss: 17.5387, MinusLogProbMetric: 17.5387, val_loss: 17.7399, val_MinusLogProbMetric: 17.7399

Epoch 644: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5387 - MinusLogProbMetric: 17.5387 - val_loss: 17.7399 - val_MinusLogProbMetric: 17.7399 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 645/1000
2023-09-27 17:04:11.139 
Epoch 645/1000 
	 loss: 17.5185, MinusLogProbMetric: 17.5185, val_loss: 17.5932, val_MinusLogProbMetric: 17.5932

Epoch 645: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5185 - MinusLogProbMetric: 17.5185 - val_loss: 17.5932 - val_MinusLogProbMetric: 17.5932 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 646/1000
2023-09-27 17:05:14.830 
Epoch 646/1000 
	 loss: 17.5153, MinusLogProbMetric: 17.5153, val_loss: 17.5612, val_MinusLogProbMetric: 17.5612

Epoch 646: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5153 - MinusLogProbMetric: 17.5153 - val_loss: 17.5612 - val_MinusLogProbMetric: 17.5612 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 647/1000
2023-09-27 17:06:18.419 
Epoch 647/1000 
	 loss: 17.5227, MinusLogProbMetric: 17.5227, val_loss: 18.4173, val_MinusLogProbMetric: 18.4173

Epoch 647: val_loss did not improve from 17.54325
196/196 - 64s - loss: 17.5227 - MinusLogProbMetric: 17.5227 - val_loss: 18.4173 - val_MinusLogProbMetric: 18.4173 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 648/1000
2023-09-27 17:07:18.332 
Epoch 648/1000 
	 loss: 17.5187, MinusLogProbMetric: 17.5187, val_loss: 17.9997, val_MinusLogProbMetric: 17.9997

Epoch 648: val_loss did not improve from 17.54325
196/196 - 60s - loss: 17.5187 - MinusLogProbMetric: 17.5187 - val_loss: 17.9997 - val_MinusLogProbMetric: 17.9997 - lr: 1.6667e-04 - 60s/epoch - 306ms/step
Epoch 649/1000
2023-09-27 17:08:12.761 
Epoch 649/1000 
	 loss: 17.6339, MinusLogProbMetric: 17.6339, val_loss: 17.5320, val_MinusLogProbMetric: 17.5320

Epoch 649: val_loss improved from 17.54325 to 17.53201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 55s - loss: 17.6339 - MinusLogProbMetric: 17.6339 - val_loss: 17.5320 - val_MinusLogProbMetric: 17.5320 - lr: 1.6667e-04 - 55s/epoch - 282ms/step
Epoch 650/1000
2023-09-27 17:09:11.784 
Epoch 650/1000 
	 loss: 17.5487, MinusLogProbMetric: 17.5487, val_loss: 17.7607, val_MinusLogProbMetric: 17.7607

Epoch 650: val_loss did not improve from 17.53201
196/196 - 58s - loss: 17.5487 - MinusLogProbMetric: 17.5487 - val_loss: 17.7607 - val_MinusLogProbMetric: 17.7607 - lr: 1.6667e-04 - 58s/epoch - 297ms/step
Epoch 651/1000
2023-09-27 17:10:12.209 
Epoch 651/1000 
	 loss: 17.5056, MinusLogProbMetric: 17.5056, val_loss: 17.7411, val_MinusLogProbMetric: 17.7411

Epoch 651: val_loss did not improve from 17.53201
196/196 - 60s - loss: 17.5056 - MinusLogProbMetric: 17.5056 - val_loss: 17.7411 - val_MinusLogProbMetric: 17.7411 - lr: 1.6667e-04 - 60s/epoch - 308ms/step
Epoch 652/1000
2023-09-27 17:11:07.338 
Epoch 652/1000 
	 loss: 17.4814, MinusLogProbMetric: 17.4814, val_loss: 18.1135, val_MinusLogProbMetric: 18.1135

Epoch 652: val_loss did not improve from 17.53201
196/196 - 55s - loss: 17.4814 - MinusLogProbMetric: 17.4814 - val_loss: 18.1135 - val_MinusLogProbMetric: 18.1135 - lr: 1.6667e-04 - 55s/epoch - 281ms/step
Epoch 653/1000
2023-09-27 17:12:05.178 
Epoch 653/1000 
	 loss: 17.5405, MinusLogProbMetric: 17.5405, val_loss: 17.5380, val_MinusLogProbMetric: 17.5380

Epoch 653: val_loss did not improve from 17.53201
196/196 - 58s - loss: 17.5405 - MinusLogProbMetric: 17.5405 - val_loss: 17.5380 - val_MinusLogProbMetric: 17.5380 - lr: 1.6667e-04 - 58s/epoch - 295ms/step
Epoch 654/1000
2023-09-27 17:13:09.489 
Epoch 654/1000 
	 loss: 17.6226, MinusLogProbMetric: 17.6226, val_loss: 17.6719, val_MinusLogProbMetric: 17.6719

Epoch 654: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.6226 - MinusLogProbMetric: 17.6226 - val_loss: 17.6719 - val_MinusLogProbMetric: 17.6719 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 655/1000
2023-09-27 17:14:13.003 
Epoch 655/1000 
	 loss: 17.4860, MinusLogProbMetric: 17.4860, val_loss: 17.6696, val_MinusLogProbMetric: 17.6696

Epoch 655: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.4860 - MinusLogProbMetric: 17.4860 - val_loss: 17.6696 - val_MinusLogProbMetric: 17.6696 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 656/1000
2023-09-27 17:15:17.404 
Epoch 656/1000 
	 loss: 17.5251, MinusLogProbMetric: 17.5251, val_loss: 17.6719, val_MinusLogProbMetric: 17.6719

Epoch 656: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5251 - MinusLogProbMetric: 17.5251 - val_loss: 17.6719 - val_MinusLogProbMetric: 17.6719 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 657/1000
2023-09-27 17:16:21.585 
Epoch 657/1000 
	 loss: 17.5506, MinusLogProbMetric: 17.5506, val_loss: 17.5812, val_MinusLogProbMetric: 17.5812

Epoch 657: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5506 - MinusLogProbMetric: 17.5506 - val_loss: 17.5812 - val_MinusLogProbMetric: 17.5812 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 658/1000
2023-09-27 17:17:25.864 
Epoch 658/1000 
	 loss: 17.5236, MinusLogProbMetric: 17.5236, val_loss: 17.6537, val_MinusLogProbMetric: 17.6537

Epoch 658: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5236 - MinusLogProbMetric: 17.5236 - val_loss: 17.6537 - val_MinusLogProbMetric: 17.6537 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 659/1000
2023-09-27 17:18:29.623 
Epoch 659/1000 
	 loss: 17.5539, MinusLogProbMetric: 17.5539, val_loss: 17.7123, val_MinusLogProbMetric: 17.7123

Epoch 659: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5539 - MinusLogProbMetric: 17.5539 - val_loss: 17.7123 - val_MinusLogProbMetric: 17.7123 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 660/1000
2023-09-27 17:19:33.718 
Epoch 660/1000 
	 loss: 17.4943, MinusLogProbMetric: 17.4943, val_loss: 17.9290, val_MinusLogProbMetric: 17.9290

Epoch 660: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.4943 - MinusLogProbMetric: 17.4943 - val_loss: 17.9290 - val_MinusLogProbMetric: 17.9290 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 661/1000
2023-09-27 17:20:37.351 
Epoch 661/1000 
	 loss: 17.5287, MinusLogProbMetric: 17.5287, val_loss: 18.3562, val_MinusLogProbMetric: 18.3562

Epoch 661: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5287 - MinusLogProbMetric: 17.5287 - val_loss: 18.3562 - val_MinusLogProbMetric: 18.3562 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 662/1000
2023-09-27 17:21:41.943 
Epoch 662/1000 
	 loss: 17.5903, MinusLogProbMetric: 17.5903, val_loss: 17.5328, val_MinusLogProbMetric: 17.5328

Epoch 662: val_loss did not improve from 17.53201
196/196 - 65s - loss: 17.5903 - MinusLogProbMetric: 17.5903 - val_loss: 17.5328 - val_MinusLogProbMetric: 17.5328 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 663/1000
2023-09-27 17:22:46.454 
Epoch 663/1000 
	 loss: 17.5172, MinusLogProbMetric: 17.5172, val_loss: 17.8271, val_MinusLogProbMetric: 17.8271

Epoch 663: val_loss did not improve from 17.53201
196/196 - 65s - loss: 17.5172 - MinusLogProbMetric: 17.5172 - val_loss: 17.8271 - val_MinusLogProbMetric: 17.8271 - lr: 1.6667e-04 - 65s/epoch - 329ms/step
Epoch 664/1000
2023-09-27 17:23:50.688 
Epoch 664/1000 
	 loss: 17.5342, MinusLogProbMetric: 17.5342, val_loss: 17.6163, val_MinusLogProbMetric: 17.6163

Epoch 664: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5342 - MinusLogProbMetric: 17.5342 - val_loss: 17.6163 - val_MinusLogProbMetric: 17.6163 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 665/1000
2023-09-27 17:24:54.550 
Epoch 665/1000 
	 loss: 17.4973, MinusLogProbMetric: 17.4973, val_loss: 17.8612, val_MinusLogProbMetric: 17.8612

Epoch 665: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.4973 - MinusLogProbMetric: 17.4973 - val_loss: 17.8612 - val_MinusLogProbMetric: 17.8612 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 666/1000
2023-09-27 17:25:58.887 
Epoch 666/1000 
	 loss: 17.5257, MinusLogProbMetric: 17.5257, val_loss: 17.5640, val_MinusLogProbMetric: 17.5640

Epoch 666: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5257 - MinusLogProbMetric: 17.5257 - val_loss: 17.5640 - val_MinusLogProbMetric: 17.5640 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 667/1000
2023-09-27 17:27:03.553 
Epoch 667/1000 
	 loss: 17.4978, MinusLogProbMetric: 17.4978, val_loss: 17.6351, val_MinusLogProbMetric: 17.6351

Epoch 667: val_loss did not improve from 17.53201
196/196 - 65s - loss: 17.4978 - MinusLogProbMetric: 17.4978 - val_loss: 17.6351 - val_MinusLogProbMetric: 17.6351 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 668/1000
2023-09-27 17:28:07.971 
Epoch 668/1000 
	 loss: 17.5367, MinusLogProbMetric: 17.5367, val_loss: 17.5573, val_MinusLogProbMetric: 17.5573

Epoch 668: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5367 - MinusLogProbMetric: 17.5367 - val_loss: 17.5573 - val_MinusLogProbMetric: 17.5573 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 669/1000
2023-09-27 17:29:12.642 
Epoch 669/1000 
	 loss: 17.5421, MinusLogProbMetric: 17.5421, val_loss: 17.6882, val_MinusLogProbMetric: 17.6882

Epoch 669: val_loss did not improve from 17.53201
196/196 - 65s - loss: 17.5421 - MinusLogProbMetric: 17.5421 - val_loss: 17.6882 - val_MinusLogProbMetric: 17.6882 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 670/1000
2023-09-27 17:30:16.675 
Epoch 670/1000 
	 loss: 17.5176, MinusLogProbMetric: 17.5176, val_loss: 17.7271, val_MinusLogProbMetric: 17.7271

Epoch 670: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5176 - MinusLogProbMetric: 17.5176 - val_loss: 17.7271 - val_MinusLogProbMetric: 17.7271 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 671/1000
2023-09-27 17:31:20.906 
Epoch 671/1000 
	 loss: 17.4942, MinusLogProbMetric: 17.4942, val_loss: 17.7644, val_MinusLogProbMetric: 17.7644

Epoch 671: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.4942 - MinusLogProbMetric: 17.4942 - val_loss: 17.7644 - val_MinusLogProbMetric: 17.7644 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 672/1000
2023-09-27 17:32:24.967 
Epoch 672/1000 
	 loss: 17.5542, MinusLogProbMetric: 17.5542, val_loss: 17.6495, val_MinusLogProbMetric: 17.6495

Epoch 672: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5542 - MinusLogProbMetric: 17.5542 - val_loss: 17.6495 - val_MinusLogProbMetric: 17.6495 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 673/1000
2023-09-27 17:33:29.237 
Epoch 673/1000 
	 loss: 17.4859, MinusLogProbMetric: 17.4859, val_loss: 17.6445, val_MinusLogProbMetric: 17.6445

Epoch 673: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.4859 - MinusLogProbMetric: 17.4859 - val_loss: 17.6445 - val_MinusLogProbMetric: 17.6445 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 674/1000
2023-09-27 17:34:33.378 
Epoch 674/1000 
	 loss: 17.4884, MinusLogProbMetric: 17.4884, val_loss: 17.5598, val_MinusLogProbMetric: 17.5598

Epoch 674: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.4884 - MinusLogProbMetric: 17.4884 - val_loss: 17.5598 - val_MinusLogProbMetric: 17.5598 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 675/1000
2023-09-27 17:35:37.434 
Epoch 675/1000 
	 loss: 17.5115, MinusLogProbMetric: 17.5115, val_loss: 17.6965, val_MinusLogProbMetric: 17.6965

Epoch 675: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5115 - MinusLogProbMetric: 17.5115 - val_loss: 17.6965 - val_MinusLogProbMetric: 17.6965 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 676/1000
2023-09-27 17:36:41.709 
Epoch 676/1000 
	 loss: 17.5056, MinusLogProbMetric: 17.5056, val_loss: 17.5986, val_MinusLogProbMetric: 17.5986

Epoch 676: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5056 - MinusLogProbMetric: 17.5056 - val_loss: 17.5986 - val_MinusLogProbMetric: 17.5986 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 677/1000
2023-09-27 17:37:46.253 
Epoch 677/1000 
	 loss: 17.5158, MinusLogProbMetric: 17.5158, val_loss: 17.6679, val_MinusLogProbMetric: 17.6679

Epoch 677: val_loss did not improve from 17.53201
196/196 - 65s - loss: 17.5158 - MinusLogProbMetric: 17.5158 - val_loss: 17.6679 - val_MinusLogProbMetric: 17.6679 - lr: 1.6667e-04 - 65s/epoch - 329ms/step
Epoch 678/1000
2023-09-27 17:38:50.486 
Epoch 678/1000 
	 loss: 17.5490, MinusLogProbMetric: 17.5490, val_loss: 18.3309, val_MinusLogProbMetric: 18.3309

Epoch 678: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5490 - MinusLogProbMetric: 17.5490 - val_loss: 18.3309 - val_MinusLogProbMetric: 18.3309 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 679/1000
2023-09-27 17:39:54.464 
Epoch 679/1000 
	 loss: 17.5315, MinusLogProbMetric: 17.5315, val_loss: 17.7013, val_MinusLogProbMetric: 17.7013

Epoch 679: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5315 - MinusLogProbMetric: 17.5315 - val_loss: 17.7013 - val_MinusLogProbMetric: 17.7013 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 680/1000
2023-09-27 17:40:58.576 
Epoch 680/1000 
	 loss: 17.5374, MinusLogProbMetric: 17.5374, val_loss: 17.5973, val_MinusLogProbMetric: 17.5973

Epoch 680: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5374 - MinusLogProbMetric: 17.5374 - val_loss: 17.5973 - val_MinusLogProbMetric: 17.5973 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 681/1000
2023-09-27 17:42:03.176 
Epoch 681/1000 
	 loss: 17.4951, MinusLogProbMetric: 17.4951, val_loss: 17.5583, val_MinusLogProbMetric: 17.5583

Epoch 681: val_loss did not improve from 17.53201
196/196 - 65s - loss: 17.4951 - MinusLogProbMetric: 17.4951 - val_loss: 17.5583 - val_MinusLogProbMetric: 17.5583 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 682/1000
2023-09-27 17:43:07.511 
Epoch 682/1000 
	 loss: 17.5087, MinusLogProbMetric: 17.5087, val_loss: 17.7652, val_MinusLogProbMetric: 17.7652

Epoch 682: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5087 - MinusLogProbMetric: 17.5087 - val_loss: 17.7652 - val_MinusLogProbMetric: 17.7652 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 683/1000
2023-09-27 17:44:11.777 
Epoch 683/1000 
	 loss: 17.5553, MinusLogProbMetric: 17.5553, val_loss: 17.6965, val_MinusLogProbMetric: 17.6965

Epoch 683: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5553 - MinusLogProbMetric: 17.5553 - val_loss: 17.6965 - val_MinusLogProbMetric: 17.6965 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 684/1000
2023-09-27 17:45:16.378 
Epoch 684/1000 
	 loss: 17.4937, MinusLogProbMetric: 17.4937, val_loss: 17.6248, val_MinusLogProbMetric: 17.6248

Epoch 684: val_loss did not improve from 17.53201
196/196 - 65s - loss: 17.4937 - MinusLogProbMetric: 17.4937 - val_loss: 17.6248 - val_MinusLogProbMetric: 17.6248 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 685/1000
2023-09-27 17:46:20.805 
Epoch 685/1000 
	 loss: 17.5368, MinusLogProbMetric: 17.5368, val_loss: 17.5546, val_MinusLogProbMetric: 17.5546

Epoch 685: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5368 - MinusLogProbMetric: 17.5368 - val_loss: 17.5546 - val_MinusLogProbMetric: 17.5546 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 686/1000
2023-09-27 17:47:25.040 
Epoch 686/1000 
	 loss: 17.5146, MinusLogProbMetric: 17.5146, val_loss: 17.7054, val_MinusLogProbMetric: 17.7054

Epoch 686: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5146 - MinusLogProbMetric: 17.5146 - val_loss: 17.7054 - val_MinusLogProbMetric: 17.7054 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 687/1000
2023-09-27 17:48:28.969 
Epoch 687/1000 
	 loss: 17.4952, MinusLogProbMetric: 17.4952, val_loss: 17.5863, val_MinusLogProbMetric: 17.5863

Epoch 687: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.4952 - MinusLogProbMetric: 17.4952 - val_loss: 17.5863 - val_MinusLogProbMetric: 17.5863 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 688/1000
2023-09-27 17:49:32.834 
Epoch 688/1000 
	 loss: 17.5629, MinusLogProbMetric: 17.5629, val_loss: 18.0062, val_MinusLogProbMetric: 18.0062

Epoch 688: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5629 - MinusLogProbMetric: 17.5629 - val_loss: 18.0062 - val_MinusLogProbMetric: 18.0062 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 689/1000
2023-09-27 17:50:36.802 
Epoch 689/1000 
	 loss: 17.5193, MinusLogProbMetric: 17.5193, val_loss: 17.5815, val_MinusLogProbMetric: 17.5815

Epoch 689: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5193 - MinusLogProbMetric: 17.5193 - val_loss: 17.5815 - val_MinusLogProbMetric: 17.5815 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 690/1000
2023-09-27 17:51:41.012 
Epoch 690/1000 
	 loss: 17.5220, MinusLogProbMetric: 17.5220, val_loss: 17.6497, val_MinusLogProbMetric: 17.6497

Epoch 690: val_loss did not improve from 17.53201
196/196 - 64s - loss: 17.5220 - MinusLogProbMetric: 17.5220 - val_loss: 17.6497 - val_MinusLogProbMetric: 17.6497 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 691/1000
2023-09-27 17:52:44.793 
Epoch 691/1000 
	 loss: 17.4631, MinusLogProbMetric: 17.4631, val_loss: 17.5156, val_MinusLogProbMetric: 17.5156

Epoch 691: val_loss improved from 17.53201 to 17.51565, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.4631 - MinusLogProbMetric: 17.4631 - val_loss: 17.5156 - val_MinusLogProbMetric: 17.5156 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 692/1000
2023-09-27 17:53:49.664 
Epoch 692/1000 
	 loss: 17.6067, MinusLogProbMetric: 17.6067, val_loss: 17.6356, val_MinusLogProbMetric: 17.6356

Epoch 692: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.6067 - MinusLogProbMetric: 17.6067 - val_loss: 17.6356 - val_MinusLogProbMetric: 17.6356 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 693/1000
2023-09-27 17:54:54.156 
Epoch 693/1000 
	 loss: 17.4849, MinusLogProbMetric: 17.4849, val_loss: 17.5526, val_MinusLogProbMetric: 17.5526

Epoch 693: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.4849 - MinusLogProbMetric: 17.4849 - val_loss: 17.5526 - val_MinusLogProbMetric: 17.5526 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 694/1000
2023-09-27 17:55:57.864 
Epoch 694/1000 
	 loss: 17.5101, MinusLogProbMetric: 17.5101, val_loss: 17.5479, val_MinusLogProbMetric: 17.5479

Epoch 694: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.5101 - MinusLogProbMetric: 17.5101 - val_loss: 17.5479 - val_MinusLogProbMetric: 17.5479 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 695/1000
2023-09-27 17:57:01.713 
Epoch 695/1000 
	 loss: 17.4801, MinusLogProbMetric: 17.4801, val_loss: 17.8156, val_MinusLogProbMetric: 17.8156

Epoch 695: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.4801 - MinusLogProbMetric: 17.4801 - val_loss: 17.8156 - val_MinusLogProbMetric: 17.8156 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 696/1000
2023-09-27 17:58:05.478 
Epoch 696/1000 
	 loss: 17.5379, MinusLogProbMetric: 17.5379, val_loss: 17.8155, val_MinusLogProbMetric: 17.8155

Epoch 696: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.5379 - MinusLogProbMetric: 17.5379 - val_loss: 17.8155 - val_MinusLogProbMetric: 17.8155 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 697/1000
2023-09-27 17:59:09.731 
Epoch 697/1000 
	 loss: 17.5812, MinusLogProbMetric: 17.5812, val_loss: 17.6234, val_MinusLogProbMetric: 17.6234

Epoch 697: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.5812 - MinusLogProbMetric: 17.5812 - val_loss: 17.6234 - val_MinusLogProbMetric: 17.6234 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 698/1000
2023-09-27 18:00:13.841 
Epoch 698/1000 
	 loss: 17.4594, MinusLogProbMetric: 17.4594, val_loss: 18.3866, val_MinusLogProbMetric: 18.3866

Epoch 698: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.4594 - MinusLogProbMetric: 17.4594 - val_loss: 18.3866 - val_MinusLogProbMetric: 18.3866 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 699/1000
2023-09-27 18:01:17.859 
Epoch 699/1000 
	 loss: 17.5118, MinusLogProbMetric: 17.5118, val_loss: 18.0492, val_MinusLogProbMetric: 18.0492

Epoch 699: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.5118 - MinusLogProbMetric: 17.5118 - val_loss: 18.0492 - val_MinusLogProbMetric: 18.0492 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 700/1000
2023-09-27 18:02:21.484 
Epoch 700/1000 
	 loss: 17.4833, MinusLogProbMetric: 17.4833, val_loss: 17.5672, val_MinusLogProbMetric: 17.5672

Epoch 700: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.4833 - MinusLogProbMetric: 17.4833 - val_loss: 17.5672 - val_MinusLogProbMetric: 17.5672 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 701/1000
2023-09-27 18:03:25.489 
Epoch 701/1000 
	 loss: 17.4566, MinusLogProbMetric: 17.4566, val_loss: 17.6281, val_MinusLogProbMetric: 17.6281

Epoch 701: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.4566 - MinusLogProbMetric: 17.4566 - val_loss: 17.6281 - val_MinusLogProbMetric: 17.6281 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 702/1000
2023-09-27 18:04:30.436 
Epoch 702/1000 
	 loss: 17.5170, MinusLogProbMetric: 17.5170, val_loss: 17.6420, val_MinusLogProbMetric: 17.6420

Epoch 702: val_loss did not improve from 17.51565
196/196 - 65s - loss: 17.5170 - MinusLogProbMetric: 17.5170 - val_loss: 17.6420 - val_MinusLogProbMetric: 17.6420 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 703/1000
2023-09-27 18:05:33.983 
Epoch 703/1000 
	 loss: 17.4675, MinusLogProbMetric: 17.4675, val_loss: 18.5114, val_MinusLogProbMetric: 18.5114

Epoch 703: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.4675 - MinusLogProbMetric: 17.4675 - val_loss: 18.5114 - val_MinusLogProbMetric: 18.5114 - lr: 1.6667e-04 - 64s/epoch - 324ms/step
Epoch 704/1000
2023-09-27 18:06:38.312 
Epoch 704/1000 
	 loss: 17.5422, MinusLogProbMetric: 17.5422, val_loss: 19.5820, val_MinusLogProbMetric: 19.5820

Epoch 704: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.5422 - MinusLogProbMetric: 17.5422 - val_loss: 19.5820 - val_MinusLogProbMetric: 19.5820 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 705/1000
2023-09-27 18:07:42.400 
Epoch 705/1000 
	 loss: 17.5281, MinusLogProbMetric: 17.5281, val_loss: 17.7231, val_MinusLogProbMetric: 17.7231

Epoch 705: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.5281 - MinusLogProbMetric: 17.5281 - val_loss: 17.7231 - val_MinusLogProbMetric: 17.7231 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 706/1000
2023-09-27 18:08:46.524 
Epoch 706/1000 
	 loss: 17.4857, MinusLogProbMetric: 17.4857, val_loss: 17.5547, val_MinusLogProbMetric: 17.5547

Epoch 706: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.4857 - MinusLogProbMetric: 17.4857 - val_loss: 17.5547 - val_MinusLogProbMetric: 17.5547 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 707/1000
2023-09-27 18:09:51.160 
Epoch 707/1000 
	 loss: 17.4964, MinusLogProbMetric: 17.4964, val_loss: 17.6295, val_MinusLogProbMetric: 17.6295

Epoch 707: val_loss did not improve from 17.51565
196/196 - 65s - loss: 17.4964 - MinusLogProbMetric: 17.4964 - val_loss: 17.6295 - val_MinusLogProbMetric: 17.6295 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 708/1000
2023-09-27 18:10:55.190 
Epoch 708/1000 
	 loss: 17.5033, MinusLogProbMetric: 17.5033, val_loss: 17.6328, val_MinusLogProbMetric: 17.6328

Epoch 708: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.5033 - MinusLogProbMetric: 17.5033 - val_loss: 17.6328 - val_MinusLogProbMetric: 17.6328 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 709/1000
2023-09-27 18:11:59.311 
Epoch 709/1000 
	 loss: 17.4997, MinusLogProbMetric: 17.4997, val_loss: 17.7053, val_MinusLogProbMetric: 17.7053

Epoch 709: val_loss did not improve from 17.51565
196/196 - 64s - loss: 17.4997 - MinusLogProbMetric: 17.4997 - val_loss: 17.7053 - val_MinusLogProbMetric: 17.7053 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 710/1000
2023-09-27 18:13:02.695 
Epoch 710/1000 
	 loss: 17.5025, MinusLogProbMetric: 17.5025, val_loss: 17.6337, val_MinusLogProbMetric: 17.6337

Epoch 710: val_loss did not improve from 17.51565
196/196 - 63s - loss: 17.5025 - MinusLogProbMetric: 17.5025 - val_loss: 17.6337 - val_MinusLogProbMetric: 17.6337 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 711/1000
2023-09-27 18:14:06.894 
Epoch 711/1000 
	 loss: 17.4498, MinusLogProbMetric: 17.4498, val_loss: 17.4545, val_MinusLogProbMetric: 17.4545

Epoch 711: val_loss improved from 17.51565 to 17.45445, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.4498 - MinusLogProbMetric: 17.4498 - val_loss: 17.4545 - val_MinusLogProbMetric: 17.4545 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 712/1000
2023-09-27 18:15:12.180 
Epoch 712/1000 
	 loss: 17.5228, MinusLogProbMetric: 17.5228, val_loss: 17.7582, val_MinusLogProbMetric: 17.7582

Epoch 712: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.5228 - MinusLogProbMetric: 17.5228 - val_loss: 17.7582 - val_MinusLogProbMetric: 17.7582 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 713/1000
2023-09-27 18:16:16.644 
Epoch 713/1000 
	 loss: 17.5026, MinusLogProbMetric: 17.5026, val_loss: 17.5321, val_MinusLogProbMetric: 17.5321

Epoch 713: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.5026 - MinusLogProbMetric: 17.5026 - val_loss: 17.5321 - val_MinusLogProbMetric: 17.5321 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 714/1000
2023-09-27 18:17:20.689 
Epoch 714/1000 
	 loss: 17.4989, MinusLogProbMetric: 17.4989, val_loss: 17.8120, val_MinusLogProbMetric: 17.8120

Epoch 714: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4989 - MinusLogProbMetric: 17.4989 - val_loss: 17.8120 - val_MinusLogProbMetric: 17.8120 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 715/1000
2023-09-27 18:18:25.100 
Epoch 715/1000 
	 loss: 17.5608, MinusLogProbMetric: 17.5608, val_loss: 17.6716, val_MinusLogProbMetric: 17.6716

Epoch 715: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.5608 - MinusLogProbMetric: 17.5608 - val_loss: 17.6716 - val_MinusLogProbMetric: 17.6716 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 716/1000
2023-09-27 18:19:29.384 
Epoch 716/1000 
	 loss: 17.5108, MinusLogProbMetric: 17.5108, val_loss: 17.5154, val_MinusLogProbMetric: 17.5154

Epoch 716: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.5108 - MinusLogProbMetric: 17.5108 - val_loss: 17.5154 - val_MinusLogProbMetric: 17.5154 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 717/1000
2023-09-27 18:20:33.655 
Epoch 717/1000 
	 loss: 17.5303, MinusLogProbMetric: 17.5303, val_loss: 17.4845, val_MinusLogProbMetric: 17.4845

Epoch 717: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.5303 - MinusLogProbMetric: 17.5303 - val_loss: 17.4845 - val_MinusLogProbMetric: 17.4845 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 718/1000
2023-09-27 18:21:37.990 
Epoch 718/1000 
	 loss: 17.4374, MinusLogProbMetric: 17.4374, val_loss: 17.5720, val_MinusLogProbMetric: 17.5720

Epoch 718: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4374 - MinusLogProbMetric: 17.4374 - val_loss: 17.5720 - val_MinusLogProbMetric: 17.5720 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 719/1000
2023-09-27 18:22:42.487 
Epoch 719/1000 
	 loss: 17.5273, MinusLogProbMetric: 17.5273, val_loss: 17.6100, val_MinusLogProbMetric: 17.6100

Epoch 719: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.5273 - MinusLogProbMetric: 17.5273 - val_loss: 17.6100 - val_MinusLogProbMetric: 17.6100 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 720/1000
2023-09-27 18:23:46.803 
Epoch 720/1000 
	 loss: 17.5027, MinusLogProbMetric: 17.5027, val_loss: 18.0940, val_MinusLogProbMetric: 18.0940

Epoch 720: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.5027 - MinusLogProbMetric: 17.5027 - val_loss: 18.0940 - val_MinusLogProbMetric: 18.0940 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 721/1000
2023-09-27 18:24:51.079 
Epoch 721/1000 
	 loss: 17.5049, MinusLogProbMetric: 17.5049, val_loss: 17.6237, val_MinusLogProbMetric: 17.6237

Epoch 721: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.5049 - MinusLogProbMetric: 17.5049 - val_loss: 17.6237 - val_MinusLogProbMetric: 17.6237 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 722/1000
2023-09-27 18:25:54.449 
Epoch 722/1000 
	 loss: 17.4683, MinusLogProbMetric: 17.4683, val_loss: 17.6758, val_MinusLogProbMetric: 17.6758

Epoch 722: val_loss did not improve from 17.45445
196/196 - 63s - loss: 17.4683 - MinusLogProbMetric: 17.4683 - val_loss: 17.6758 - val_MinusLogProbMetric: 17.6758 - lr: 1.6667e-04 - 63s/epoch - 323ms/step
Epoch 723/1000
2023-09-27 18:26:58.353 
Epoch 723/1000 
	 loss: 17.5021, MinusLogProbMetric: 17.5021, val_loss: 18.3419, val_MinusLogProbMetric: 18.3419

Epoch 723: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.5021 - MinusLogProbMetric: 17.5021 - val_loss: 18.3419 - val_MinusLogProbMetric: 18.3419 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 724/1000
2023-09-27 18:28:02.115 
Epoch 724/1000 
	 loss: 17.5216, MinusLogProbMetric: 17.5216, val_loss: 17.7176, val_MinusLogProbMetric: 17.7176

Epoch 724: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.5216 - MinusLogProbMetric: 17.5216 - val_loss: 17.7176 - val_MinusLogProbMetric: 17.7176 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 725/1000
2023-09-27 18:29:06.365 
Epoch 725/1000 
	 loss: 17.9793, MinusLogProbMetric: 17.9793, val_loss: 17.6818, val_MinusLogProbMetric: 17.6818

Epoch 725: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.9793 - MinusLogProbMetric: 17.9793 - val_loss: 17.6818 - val_MinusLogProbMetric: 17.6818 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 726/1000
2023-09-27 18:30:10.191 
Epoch 726/1000 
	 loss: 17.4644, MinusLogProbMetric: 17.4644, val_loss: 17.7750, val_MinusLogProbMetric: 17.7750

Epoch 726: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4644 - MinusLogProbMetric: 17.4644 - val_loss: 17.7750 - val_MinusLogProbMetric: 17.7750 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 727/1000
2023-09-27 18:31:14.726 
Epoch 727/1000 
	 loss: 17.4463, MinusLogProbMetric: 17.4463, val_loss: 17.7486, val_MinusLogProbMetric: 17.7486

Epoch 727: val_loss did not improve from 17.45445
196/196 - 65s - loss: 17.4463 - MinusLogProbMetric: 17.4463 - val_loss: 17.7486 - val_MinusLogProbMetric: 17.7486 - lr: 1.6667e-04 - 65s/epoch - 329ms/step
Epoch 728/1000
2023-09-27 18:32:18.455 
Epoch 728/1000 
	 loss: 17.4845, MinusLogProbMetric: 17.4845, val_loss: 17.8578, val_MinusLogProbMetric: 17.8578

Epoch 728: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4845 - MinusLogProbMetric: 17.4845 - val_loss: 17.8578 - val_MinusLogProbMetric: 17.8578 - lr: 1.6667e-04 - 64s/epoch - 325ms/step
Epoch 729/1000
2023-09-27 18:33:21.659 
Epoch 729/1000 
	 loss: 17.4730, MinusLogProbMetric: 17.4730, val_loss: 17.6349, val_MinusLogProbMetric: 17.6349

Epoch 729: val_loss did not improve from 17.45445
196/196 - 63s - loss: 17.4730 - MinusLogProbMetric: 17.4730 - val_loss: 17.6349 - val_MinusLogProbMetric: 17.6349 - lr: 1.6667e-04 - 63s/epoch - 322ms/step
Epoch 730/1000
2023-09-27 18:34:20.167 
Epoch 730/1000 
	 loss: 17.4423, MinusLogProbMetric: 17.4423, val_loss: 18.0650, val_MinusLogProbMetric: 18.0650

Epoch 730: val_loss did not improve from 17.45445
196/196 - 59s - loss: 17.4423 - MinusLogProbMetric: 17.4423 - val_loss: 18.0650 - val_MinusLogProbMetric: 18.0650 - lr: 1.6667e-04 - 59s/epoch - 298ms/step
Epoch 731/1000
2023-09-27 18:35:13.281 
Epoch 731/1000 
	 loss: 17.4591, MinusLogProbMetric: 17.4591, val_loss: 17.7427, val_MinusLogProbMetric: 17.7427

Epoch 731: val_loss did not improve from 17.45445
196/196 - 53s - loss: 17.4591 - MinusLogProbMetric: 17.4591 - val_loss: 17.7427 - val_MinusLogProbMetric: 17.7427 - lr: 1.6667e-04 - 53s/epoch - 271ms/step
Epoch 732/1000
2023-09-27 18:36:04.545 
Epoch 732/1000 
	 loss: 17.4726, MinusLogProbMetric: 17.4726, val_loss: 17.8062, val_MinusLogProbMetric: 17.8062

Epoch 732: val_loss did not improve from 17.45445
196/196 - 51s - loss: 17.4726 - MinusLogProbMetric: 17.4726 - val_loss: 17.8062 - val_MinusLogProbMetric: 17.8062 - lr: 1.6667e-04 - 51s/epoch - 262ms/step
Epoch 733/1000
2023-09-27 18:37:04.473 
Epoch 733/1000 
	 loss: 17.4667, MinusLogProbMetric: 17.4667, val_loss: 17.6145, val_MinusLogProbMetric: 17.6145

Epoch 733: val_loss did not improve from 17.45445
196/196 - 60s - loss: 17.4667 - MinusLogProbMetric: 17.4667 - val_loss: 17.6145 - val_MinusLogProbMetric: 17.6145 - lr: 1.6667e-04 - 60s/epoch - 306ms/step
Epoch 734/1000
2023-09-27 18:37:59.757 
Epoch 734/1000 
	 loss: 17.5156, MinusLogProbMetric: 17.5156, val_loss: 17.5379, val_MinusLogProbMetric: 17.5379

Epoch 734: val_loss did not improve from 17.45445
196/196 - 55s - loss: 17.5156 - MinusLogProbMetric: 17.5156 - val_loss: 17.5379 - val_MinusLogProbMetric: 17.5379 - lr: 1.6667e-04 - 55s/epoch - 282ms/step
Epoch 735/1000
2023-09-27 18:38:50.593 
Epoch 735/1000 
	 loss: 17.4719, MinusLogProbMetric: 17.4719, val_loss: 17.6227, val_MinusLogProbMetric: 17.6227

Epoch 735: val_loss did not improve from 17.45445
196/196 - 51s - loss: 17.4719 - MinusLogProbMetric: 17.4719 - val_loss: 17.6227 - val_MinusLogProbMetric: 17.6227 - lr: 1.6667e-04 - 51s/epoch - 259ms/step
Epoch 736/1000
2023-09-27 18:39:46.896 
Epoch 736/1000 
	 loss: 17.4658, MinusLogProbMetric: 17.4658, val_loss: 17.6516, val_MinusLogProbMetric: 17.6516

Epoch 736: val_loss did not improve from 17.45445
196/196 - 56s - loss: 17.4658 - MinusLogProbMetric: 17.4658 - val_loss: 17.6516 - val_MinusLogProbMetric: 17.6516 - lr: 1.6667e-04 - 56s/epoch - 287ms/step
Epoch 737/1000
2023-09-27 18:40:50.737 
Epoch 737/1000 
	 loss: 17.4741, MinusLogProbMetric: 17.4741, val_loss: 17.5969, val_MinusLogProbMetric: 17.5969

Epoch 737: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4741 - MinusLogProbMetric: 17.4741 - val_loss: 17.5969 - val_MinusLogProbMetric: 17.5969 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 738/1000
2023-09-27 18:41:53.092 
Epoch 738/1000 
	 loss: 17.4878, MinusLogProbMetric: 17.4878, val_loss: 17.7846, val_MinusLogProbMetric: 17.7846

Epoch 738: val_loss did not improve from 17.45445
196/196 - 62s - loss: 17.4878 - MinusLogProbMetric: 17.4878 - val_loss: 17.7846 - val_MinusLogProbMetric: 17.7846 - lr: 1.6667e-04 - 62s/epoch - 318ms/step
Epoch 739/1000
2023-09-27 18:42:57.437 
Epoch 739/1000 
	 loss: 17.4607, MinusLogProbMetric: 17.4607, val_loss: 17.6565, val_MinusLogProbMetric: 17.6565

Epoch 739: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4607 - MinusLogProbMetric: 17.4607 - val_loss: 17.6565 - val_MinusLogProbMetric: 17.6565 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 740/1000
2023-09-27 18:44:02.045 
Epoch 740/1000 
	 loss: 17.4943, MinusLogProbMetric: 17.4943, val_loss: 18.2664, val_MinusLogProbMetric: 18.2664

Epoch 740: val_loss did not improve from 17.45445
196/196 - 65s - loss: 17.4943 - MinusLogProbMetric: 17.4943 - val_loss: 18.2664 - val_MinusLogProbMetric: 18.2664 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 741/1000
2023-09-27 18:45:06.632 
Epoch 741/1000 
	 loss: 17.5335, MinusLogProbMetric: 17.5335, val_loss: 17.6245, val_MinusLogProbMetric: 17.6245

Epoch 741: val_loss did not improve from 17.45445
196/196 - 65s - loss: 17.5335 - MinusLogProbMetric: 17.5335 - val_loss: 17.6245 - val_MinusLogProbMetric: 17.6245 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 742/1000
2023-09-27 18:46:10.840 
Epoch 742/1000 
	 loss: 17.4416, MinusLogProbMetric: 17.4416, val_loss: 17.7956, val_MinusLogProbMetric: 17.7956

Epoch 742: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4416 - MinusLogProbMetric: 17.4416 - val_loss: 17.7956 - val_MinusLogProbMetric: 17.7956 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 743/1000
2023-09-27 18:47:14.950 
Epoch 743/1000 
	 loss: 17.5105, MinusLogProbMetric: 17.5105, val_loss: 17.8470, val_MinusLogProbMetric: 17.8470

Epoch 743: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.5105 - MinusLogProbMetric: 17.5105 - val_loss: 17.8470 - val_MinusLogProbMetric: 17.8470 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 744/1000
2023-09-27 18:48:19.054 
Epoch 744/1000 
	 loss: 17.4737, MinusLogProbMetric: 17.4737, val_loss: 17.6881, val_MinusLogProbMetric: 17.6881

Epoch 744: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4737 - MinusLogProbMetric: 17.4737 - val_loss: 17.6881 - val_MinusLogProbMetric: 17.6881 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 745/1000
2023-09-27 18:49:23.480 
Epoch 745/1000 
	 loss: 17.4999, MinusLogProbMetric: 17.4999, val_loss: 17.6312, val_MinusLogProbMetric: 17.6312

Epoch 745: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4999 - MinusLogProbMetric: 17.4999 - val_loss: 17.6312 - val_MinusLogProbMetric: 17.6312 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 746/1000
2023-09-27 18:50:28.009 
Epoch 746/1000 
	 loss: 17.4712, MinusLogProbMetric: 17.4712, val_loss: 17.7338, val_MinusLogProbMetric: 17.7338

Epoch 746: val_loss did not improve from 17.45445
196/196 - 65s - loss: 17.4712 - MinusLogProbMetric: 17.4712 - val_loss: 17.7338 - val_MinusLogProbMetric: 17.7338 - lr: 1.6667e-04 - 65s/epoch - 329ms/step
Epoch 747/1000
2023-09-27 18:51:32.120 
Epoch 747/1000 
	 loss: 17.5031, MinusLogProbMetric: 17.5031, val_loss: 17.5409, val_MinusLogProbMetric: 17.5409

Epoch 747: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.5031 - MinusLogProbMetric: 17.5031 - val_loss: 17.5409 - val_MinusLogProbMetric: 17.5409 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 748/1000
2023-09-27 18:52:36.433 
Epoch 748/1000 
	 loss: 17.4553, MinusLogProbMetric: 17.4553, val_loss: 17.5735, val_MinusLogProbMetric: 17.5735

Epoch 748: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4553 - MinusLogProbMetric: 17.4553 - val_loss: 17.5735 - val_MinusLogProbMetric: 17.5735 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 749/1000
2023-09-27 18:53:40.904 
Epoch 749/1000 
	 loss: 17.4438, MinusLogProbMetric: 17.4438, val_loss: 17.9625, val_MinusLogProbMetric: 17.9625

Epoch 749: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4438 - MinusLogProbMetric: 17.4438 - val_loss: 17.9625 - val_MinusLogProbMetric: 17.9625 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 750/1000
2023-09-27 18:54:45.489 
Epoch 750/1000 
	 loss: 17.4823, MinusLogProbMetric: 17.4823, val_loss: 17.5709, val_MinusLogProbMetric: 17.5709

Epoch 750: val_loss did not improve from 17.45445
196/196 - 65s - loss: 17.4823 - MinusLogProbMetric: 17.4823 - val_loss: 17.5709 - val_MinusLogProbMetric: 17.5709 - lr: 1.6667e-04 - 65s/epoch - 329ms/step
Epoch 751/1000
2023-09-27 18:55:49.579 
Epoch 751/1000 
	 loss: 17.5118, MinusLogProbMetric: 17.5118, val_loss: 17.7116, val_MinusLogProbMetric: 17.7116

Epoch 751: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.5118 - MinusLogProbMetric: 17.5118 - val_loss: 17.7116 - val_MinusLogProbMetric: 17.7116 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 752/1000
2023-09-27 18:56:53.914 
Epoch 752/1000 
	 loss: 17.4540, MinusLogProbMetric: 17.4540, val_loss: 17.6397, val_MinusLogProbMetric: 17.6397

Epoch 752: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4540 - MinusLogProbMetric: 17.4540 - val_loss: 17.6397 - val_MinusLogProbMetric: 17.6397 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 753/1000
2023-09-27 18:57:58.298 
Epoch 753/1000 
	 loss: 17.4318, MinusLogProbMetric: 17.4318, val_loss: 17.6675, val_MinusLogProbMetric: 17.6675

Epoch 753: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4318 - MinusLogProbMetric: 17.4318 - val_loss: 17.6675 - val_MinusLogProbMetric: 17.6675 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 754/1000
2023-09-27 18:59:03.051 
Epoch 754/1000 
	 loss: 17.5001, MinusLogProbMetric: 17.5001, val_loss: 17.6409, val_MinusLogProbMetric: 17.6409

Epoch 754: val_loss did not improve from 17.45445
196/196 - 65s - loss: 17.5001 - MinusLogProbMetric: 17.5001 - val_loss: 17.6409 - val_MinusLogProbMetric: 17.6409 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 755/1000
2023-09-27 19:00:07.990 
Epoch 755/1000 
	 loss: 17.4247, MinusLogProbMetric: 17.4247, val_loss: 17.4556, val_MinusLogProbMetric: 17.4556

Epoch 755: val_loss did not improve from 17.45445
196/196 - 65s - loss: 17.4247 - MinusLogProbMetric: 17.4247 - val_loss: 17.4556 - val_MinusLogProbMetric: 17.4556 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 756/1000
2023-09-27 19:01:12.777 
Epoch 756/1000 
	 loss: 17.5427, MinusLogProbMetric: 17.5427, val_loss: 17.6195, val_MinusLogProbMetric: 17.6195

Epoch 756: val_loss did not improve from 17.45445
196/196 - 65s - loss: 17.5427 - MinusLogProbMetric: 17.5427 - val_loss: 17.6195 - val_MinusLogProbMetric: 17.6195 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 757/1000
2023-09-27 19:02:17.217 
Epoch 757/1000 
	 loss: 17.4176, MinusLogProbMetric: 17.4176, val_loss: 18.0825, val_MinusLogProbMetric: 18.0825

Epoch 757: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4176 - MinusLogProbMetric: 17.4176 - val_loss: 18.0825 - val_MinusLogProbMetric: 18.0825 - lr: 1.6667e-04 - 64s/epoch - 329ms/step
Epoch 758/1000
2023-09-27 19:03:21.373 
Epoch 758/1000 
	 loss: 17.4935, MinusLogProbMetric: 17.4935, val_loss: 17.6891, val_MinusLogProbMetric: 17.6891

Epoch 758: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4935 - MinusLogProbMetric: 17.4935 - val_loss: 17.6891 - val_MinusLogProbMetric: 17.6891 - lr: 1.6667e-04 - 64s/epoch - 327ms/step
Epoch 759/1000
2023-09-27 19:04:25.571 
Epoch 759/1000 
	 loss: 17.4640, MinusLogProbMetric: 17.4640, val_loss: 17.5729, val_MinusLogProbMetric: 17.5729

Epoch 759: val_loss did not improve from 17.45445
196/196 - 64s - loss: 17.4640 - MinusLogProbMetric: 17.4640 - val_loss: 17.5729 - val_MinusLogProbMetric: 17.5729 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 760/1000
2023-09-27 19:05:30.364 
Epoch 760/1000 
	 loss: 17.5091, MinusLogProbMetric: 17.5091, val_loss: 17.8017, val_MinusLogProbMetric: 17.8017

Epoch 760: val_loss did not improve from 17.45445
196/196 - 65s - loss: 17.5091 - MinusLogProbMetric: 17.5091 - val_loss: 17.8017 - val_MinusLogProbMetric: 17.8017 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 761/1000
2023-09-27 19:06:35.109 
Epoch 761/1000 
	 loss: 17.5046, MinusLogProbMetric: 17.5046, val_loss: 17.5768, val_MinusLogProbMetric: 17.5768

Epoch 761: val_loss did not improve from 17.45445
196/196 - 65s - loss: 17.5046 - MinusLogProbMetric: 17.5046 - val_loss: 17.5768 - val_MinusLogProbMetric: 17.5768 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 762/1000
2023-09-27 19:07:39.915 
Epoch 762/1000 
	 loss: 17.2554, MinusLogProbMetric: 17.2554, val_loss: 17.4704, val_MinusLogProbMetric: 17.4704

Epoch 762: val_loss did not improve from 17.45445
196/196 - 65s - loss: 17.2554 - MinusLogProbMetric: 17.2554 - val_loss: 17.4704 - val_MinusLogProbMetric: 17.4704 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 763/1000
2023-09-27 19:08:44.636 
Epoch 763/1000 
	 loss: 17.2486, MinusLogProbMetric: 17.2486, val_loss: 17.4637, val_MinusLogProbMetric: 17.4637

Epoch 763: val_loss did not improve from 17.45445
196/196 - 65s - loss: 17.2486 - MinusLogProbMetric: 17.2486 - val_loss: 17.4637 - val_MinusLogProbMetric: 17.4637 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 764/1000
2023-09-27 19:09:49.121 
Epoch 764/1000 
	 loss: 17.2556, MinusLogProbMetric: 17.2556, val_loss: 17.4142, val_MinusLogProbMetric: 17.4142

Epoch 764: val_loss improved from 17.45445 to 17.41418, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 17.2556 - MinusLogProbMetric: 17.2556 - val_loss: 17.4142 - val_MinusLogProbMetric: 17.4142 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 765/1000
2023-09-27 19:10:54.202 
Epoch 765/1000 
	 loss: 17.2520, MinusLogProbMetric: 17.2520, val_loss: 17.3733, val_MinusLogProbMetric: 17.3733

Epoch 765: val_loss improved from 17.41418 to 17.37332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.2520 - MinusLogProbMetric: 17.2520 - val_loss: 17.3733 - val_MinusLogProbMetric: 17.3733 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 766/1000
2023-09-27 19:11:59.624 
Epoch 766/1000 
	 loss: 17.2485, MinusLogProbMetric: 17.2485, val_loss: 17.4104, val_MinusLogProbMetric: 17.4104

Epoch 766: val_loss did not improve from 17.37332
196/196 - 65s - loss: 17.2485 - MinusLogProbMetric: 17.2485 - val_loss: 17.4104 - val_MinusLogProbMetric: 17.4104 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 767/1000
2023-09-27 19:13:04.425 
Epoch 767/1000 
	 loss: 17.2560, MinusLogProbMetric: 17.2560, val_loss: 17.4350, val_MinusLogProbMetric: 17.4350

Epoch 767: val_loss did not improve from 17.37332
196/196 - 65s - loss: 17.2560 - MinusLogProbMetric: 17.2560 - val_loss: 17.4350 - val_MinusLogProbMetric: 17.4350 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 768/1000
2023-09-27 19:14:09.057 
Epoch 768/1000 
	 loss: 17.2534, MinusLogProbMetric: 17.2534, val_loss: 17.4563, val_MinusLogProbMetric: 17.4563

Epoch 768: val_loss did not improve from 17.37332
196/196 - 65s - loss: 17.2534 - MinusLogProbMetric: 17.2534 - val_loss: 17.4563 - val_MinusLogProbMetric: 17.4563 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 769/1000
2023-09-27 19:15:13.923 
Epoch 769/1000 
	 loss: 17.2431, MinusLogProbMetric: 17.2431, val_loss: 17.3539, val_MinusLogProbMetric: 17.3539

Epoch 769: val_loss improved from 17.37332 to 17.35392, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 17.2431 - MinusLogProbMetric: 17.2431 - val_loss: 17.3539 - val_MinusLogProbMetric: 17.3539 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 770/1000
2023-09-27 19:16:19.357 
Epoch 770/1000 
	 loss: 17.2573, MinusLogProbMetric: 17.2573, val_loss: 17.4233, val_MinusLogProbMetric: 17.4233

Epoch 770: val_loss did not improve from 17.35392
196/196 - 65s - loss: 17.2573 - MinusLogProbMetric: 17.2573 - val_loss: 17.4233 - val_MinusLogProbMetric: 17.4233 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 771/1000
2023-09-27 19:17:24.026 
Epoch 771/1000 
	 loss: 17.2519, MinusLogProbMetric: 17.2519, val_loss: 17.3896, val_MinusLogProbMetric: 17.3896

Epoch 771: val_loss did not improve from 17.35392
196/196 - 65s - loss: 17.2519 - MinusLogProbMetric: 17.2519 - val_loss: 17.3896 - val_MinusLogProbMetric: 17.3896 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 772/1000
2023-09-27 19:18:28.055 
Epoch 772/1000 
	 loss: 17.2713, MinusLogProbMetric: 17.2713, val_loss: 17.3845, val_MinusLogProbMetric: 17.3845

Epoch 772: val_loss did not improve from 17.35392
196/196 - 64s - loss: 17.2713 - MinusLogProbMetric: 17.2713 - val_loss: 17.3845 - val_MinusLogProbMetric: 17.3845 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 773/1000
2023-09-27 19:19:32.597 
Epoch 773/1000 
	 loss: 17.2569, MinusLogProbMetric: 17.2569, val_loss: 17.4061, val_MinusLogProbMetric: 17.4061

Epoch 773: val_loss did not improve from 17.35392
196/196 - 65s - loss: 17.2569 - MinusLogProbMetric: 17.2569 - val_loss: 17.4061 - val_MinusLogProbMetric: 17.4061 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 774/1000
2023-09-27 19:20:37.295 
Epoch 774/1000 
	 loss: 17.2613, MinusLogProbMetric: 17.2613, val_loss: 17.4200, val_MinusLogProbMetric: 17.4200

Epoch 774: val_loss did not improve from 17.35392
196/196 - 65s - loss: 17.2613 - MinusLogProbMetric: 17.2613 - val_loss: 17.4200 - val_MinusLogProbMetric: 17.4200 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 775/1000
2023-09-27 19:21:41.511 
Epoch 775/1000 
	 loss: 17.2484, MinusLogProbMetric: 17.2484, val_loss: 17.3540, val_MinusLogProbMetric: 17.3540

Epoch 775: val_loss did not improve from 17.35392
196/196 - 64s - loss: 17.2484 - MinusLogProbMetric: 17.2484 - val_loss: 17.3540 - val_MinusLogProbMetric: 17.3540 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 776/1000
2023-09-27 19:22:45.771 
Epoch 776/1000 
	 loss: 17.2499, MinusLogProbMetric: 17.2499, val_loss: 17.4545, val_MinusLogProbMetric: 17.4545

Epoch 776: val_loss did not improve from 17.35392
196/196 - 64s - loss: 17.2499 - MinusLogProbMetric: 17.2499 - val_loss: 17.4545 - val_MinusLogProbMetric: 17.4545 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 777/1000
2023-09-27 19:23:49.949 
Epoch 777/1000 
	 loss: 17.2529, MinusLogProbMetric: 17.2529, val_loss: 17.4409, val_MinusLogProbMetric: 17.4409

Epoch 777: val_loss did not improve from 17.35392
196/196 - 64s - loss: 17.2529 - MinusLogProbMetric: 17.2529 - val_loss: 17.4409 - val_MinusLogProbMetric: 17.4409 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 778/1000
2023-09-27 19:24:54.511 
Epoch 778/1000 
	 loss: 17.2288, MinusLogProbMetric: 17.2288, val_loss: 17.4079, val_MinusLogProbMetric: 17.4079

Epoch 778: val_loss did not improve from 17.35392
196/196 - 65s - loss: 17.2288 - MinusLogProbMetric: 17.2288 - val_loss: 17.4079 - val_MinusLogProbMetric: 17.4079 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 779/1000
2023-09-27 19:25:57.509 
Epoch 779/1000 
	 loss: 17.2460, MinusLogProbMetric: 17.2460, val_loss: 17.4833, val_MinusLogProbMetric: 17.4833

Epoch 779: val_loss did not improve from 17.35392
196/196 - 63s - loss: 17.2460 - MinusLogProbMetric: 17.2460 - val_loss: 17.4833 - val_MinusLogProbMetric: 17.4833 - lr: 8.3333e-05 - 63s/epoch - 321ms/step
Epoch 780/1000
2023-09-27 19:26:55.905 
Epoch 780/1000 
	 loss: 17.2332, MinusLogProbMetric: 17.2332, val_loss: 17.3656, val_MinusLogProbMetric: 17.3656

Epoch 780: val_loss did not improve from 17.35392
196/196 - 58s - loss: 17.2332 - MinusLogProbMetric: 17.2332 - val_loss: 17.3656 - val_MinusLogProbMetric: 17.3656 - lr: 8.3333e-05 - 58s/epoch - 298ms/step
Epoch 781/1000
2023-09-27 19:27:51.717 
Epoch 781/1000 
	 loss: 17.2245, MinusLogProbMetric: 17.2245, val_loss: 17.3493, val_MinusLogProbMetric: 17.3493

Epoch 781: val_loss improved from 17.35392 to 17.34929, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 57s - loss: 17.2245 - MinusLogProbMetric: 17.2245 - val_loss: 17.3493 - val_MinusLogProbMetric: 17.3493 - lr: 8.3333e-05 - 57s/epoch - 289ms/step
Epoch 782/1000
2023-09-27 19:28:56.586 
Epoch 782/1000 
	 loss: 17.2476, MinusLogProbMetric: 17.2476, val_loss: 17.4248, val_MinusLogProbMetric: 17.4248

Epoch 782: val_loss did not improve from 17.34929
196/196 - 64s - loss: 17.2476 - MinusLogProbMetric: 17.2476 - val_loss: 17.4248 - val_MinusLogProbMetric: 17.4248 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 783/1000
2023-09-27 19:30:01.521 
Epoch 783/1000 
	 loss: 17.2462, MinusLogProbMetric: 17.2462, val_loss: 17.3661, val_MinusLogProbMetric: 17.3661

Epoch 783: val_loss did not improve from 17.34929
196/196 - 65s - loss: 17.2462 - MinusLogProbMetric: 17.2462 - val_loss: 17.3661 - val_MinusLogProbMetric: 17.3661 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 784/1000
2023-09-27 19:31:06.934 
Epoch 784/1000 
	 loss: 17.2530, MinusLogProbMetric: 17.2530, val_loss: 17.4860, val_MinusLogProbMetric: 17.4860

Epoch 784: val_loss did not improve from 17.34929
196/196 - 65s - loss: 17.2530 - MinusLogProbMetric: 17.2530 - val_loss: 17.4860 - val_MinusLogProbMetric: 17.4860 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 785/1000
2023-09-27 19:32:11.879 
Epoch 785/1000 
	 loss: 17.2334, MinusLogProbMetric: 17.2334, val_loss: 17.3507, val_MinusLogProbMetric: 17.3507

Epoch 785: val_loss did not improve from 17.34929
196/196 - 65s - loss: 17.2334 - MinusLogProbMetric: 17.2334 - val_loss: 17.3507 - val_MinusLogProbMetric: 17.3507 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 786/1000
2023-09-27 19:33:16.348 
Epoch 786/1000 
	 loss: 17.2407, MinusLogProbMetric: 17.2407, val_loss: 17.3493, val_MinusLogProbMetric: 17.3493

Epoch 786: val_loss improved from 17.34929 to 17.34928, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.2407 - MinusLogProbMetric: 17.2407 - val_loss: 17.3493 - val_MinusLogProbMetric: 17.3493 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 787/1000
2023-09-27 19:34:22.275 
Epoch 787/1000 
	 loss: 17.2392, MinusLogProbMetric: 17.2392, val_loss: 17.4333, val_MinusLogProbMetric: 17.4333

Epoch 787: val_loss did not improve from 17.34928
196/196 - 65s - loss: 17.2392 - MinusLogProbMetric: 17.2392 - val_loss: 17.4333 - val_MinusLogProbMetric: 17.4333 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 788/1000
2023-09-27 19:35:26.931 
Epoch 788/1000 
	 loss: 17.2435, MinusLogProbMetric: 17.2435, val_loss: 17.4633, val_MinusLogProbMetric: 17.4633

Epoch 788: val_loss did not improve from 17.34928
196/196 - 65s - loss: 17.2435 - MinusLogProbMetric: 17.2435 - val_loss: 17.4633 - val_MinusLogProbMetric: 17.4633 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 789/1000
2023-09-27 19:36:31.328 
Epoch 789/1000 
	 loss: 17.2403, MinusLogProbMetric: 17.2403, val_loss: 17.3741, val_MinusLogProbMetric: 17.3741

Epoch 789: val_loss did not improve from 17.34928
196/196 - 64s - loss: 17.2403 - MinusLogProbMetric: 17.2403 - val_loss: 17.3741 - val_MinusLogProbMetric: 17.3741 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 790/1000
2023-09-27 19:37:36.277 
Epoch 790/1000 
	 loss: 17.2586, MinusLogProbMetric: 17.2586, val_loss: 17.4721, val_MinusLogProbMetric: 17.4721

Epoch 790: val_loss did not improve from 17.34928
196/196 - 65s - loss: 17.2586 - MinusLogProbMetric: 17.2586 - val_loss: 17.4721 - val_MinusLogProbMetric: 17.4721 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 791/1000
2023-09-27 19:38:41.305 
Epoch 791/1000 
	 loss: 17.2463, MinusLogProbMetric: 17.2463, val_loss: 17.3951, val_MinusLogProbMetric: 17.3951

Epoch 791: val_loss did not improve from 17.34928
196/196 - 65s - loss: 17.2463 - MinusLogProbMetric: 17.2463 - val_loss: 17.3951 - val_MinusLogProbMetric: 17.3951 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 792/1000
2023-09-27 19:39:46.061 
Epoch 792/1000 
	 loss: 17.2490, MinusLogProbMetric: 17.2490, val_loss: 17.4735, val_MinusLogProbMetric: 17.4735

Epoch 792: val_loss did not improve from 17.34928
196/196 - 65s - loss: 17.2490 - MinusLogProbMetric: 17.2490 - val_loss: 17.4735 - val_MinusLogProbMetric: 17.4735 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 793/1000
2023-09-27 19:40:50.830 
Epoch 793/1000 
	 loss: 17.2424, MinusLogProbMetric: 17.2424, val_loss: 17.4261, val_MinusLogProbMetric: 17.4261

Epoch 793: val_loss did not improve from 17.34928
196/196 - 65s - loss: 17.2424 - MinusLogProbMetric: 17.2424 - val_loss: 17.4261 - val_MinusLogProbMetric: 17.4261 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 794/1000
2023-09-27 19:41:55.897 
Epoch 794/1000 
	 loss: 17.2427, MinusLogProbMetric: 17.2427, val_loss: 17.3669, val_MinusLogProbMetric: 17.3669

Epoch 794: val_loss did not improve from 17.34928
196/196 - 65s - loss: 17.2427 - MinusLogProbMetric: 17.2427 - val_loss: 17.3669 - val_MinusLogProbMetric: 17.3669 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 795/1000
2023-09-27 19:43:00.011 
Epoch 795/1000 
	 loss: 17.2391, MinusLogProbMetric: 17.2391, val_loss: 17.4013, val_MinusLogProbMetric: 17.4013

Epoch 795: val_loss did not improve from 17.34928
196/196 - 64s - loss: 17.2391 - MinusLogProbMetric: 17.2391 - val_loss: 17.4013 - val_MinusLogProbMetric: 17.4013 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 796/1000
2023-09-27 19:44:04.255 
Epoch 796/1000 
	 loss: 17.2476, MinusLogProbMetric: 17.2476, val_loss: 17.4400, val_MinusLogProbMetric: 17.4400

Epoch 796: val_loss did not improve from 17.34928
196/196 - 64s - loss: 17.2476 - MinusLogProbMetric: 17.2476 - val_loss: 17.4400 - val_MinusLogProbMetric: 17.4400 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 797/1000
2023-09-27 19:45:08.942 
Epoch 797/1000 
	 loss: 17.2483, MinusLogProbMetric: 17.2483, val_loss: 17.3995, val_MinusLogProbMetric: 17.3995

Epoch 797: val_loss did not improve from 17.34928
196/196 - 65s - loss: 17.2483 - MinusLogProbMetric: 17.2483 - val_loss: 17.3995 - val_MinusLogProbMetric: 17.3995 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 798/1000
2023-09-27 19:46:14.315 
Epoch 798/1000 
	 loss: 17.2619, MinusLogProbMetric: 17.2619, val_loss: 17.5111, val_MinusLogProbMetric: 17.5111

Epoch 798: val_loss did not improve from 17.34928
196/196 - 65s - loss: 17.2619 - MinusLogProbMetric: 17.2619 - val_loss: 17.5111 - val_MinusLogProbMetric: 17.5111 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 799/1000
2023-09-27 19:47:19.228 
Epoch 799/1000 
	 loss: 17.2653, MinusLogProbMetric: 17.2653, val_loss: 17.4037, val_MinusLogProbMetric: 17.4037

Epoch 799: val_loss did not improve from 17.34928
196/196 - 65s - loss: 17.2653 - MinusLogProbMetric: 17.2653 - val_loss: 17.4037 - val_MinusLogProbMetric: 17.4037 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 800/1000
2023-09-27 19:48:24.207 
Epoch 800/1000 
	 loss: 17.2619, MinusLogProbMetric: 17.2619, val_loss: 17.3692, val_MinusLogProbMetric: 17.3692

Epoch 800: val_loss did not improve from 17.34928
196/196 - 65s - loss: 17.2619 - MinusLogProbMetric: 17.2619 - val_loss: 17.3692 - val_MinusLogProbMetric: 17.3692 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 801/1000
2023-09-27 19:49:28.638 
Epoch 801/1000 
	 loss: 17.2304, MinusLogProbMetric: 17.2304, val_loss: 17.3329, val_MinusLogProbMetric: 17.3329

Epoch 801: val_loss improved from 17.34928 to 17.33285, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.2304 - MinusLogProbMetric: 17.2304 - val_loss: 17.3329 - val_MinusLogProbMetric: 17.3329 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 802/1000
2023-09-27 19:50:34.663 
Epoch 802/1000 
	 loss: 17.2252, MinusLogProbMetric: 17.2252, val_loss: 17.3760, val_MinusLogProbMetric: 17.3760

Epoch 802: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2252 - MinusLogProbMetric: 17.2252 - val_loss: 17.3760 - val_MinusLogProbMetric: 17.3760 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 803/1000
2023-09-27 19:51:39.632 
Epoch 803/1000 
	 loss: 17.2310, MinusLogProbMetric: 17.2310, val_loss: 17.4543, val_MinusLogProbMetric: 17.4543

Epoch 803: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2310 - MinusLogProbMetric: 17.2310 - val_loss: 17.4543 - val_MinusLogProbMetric: 17.4543 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 804/1000
2023-09-27 19:52:44.662 
Epoch 804/1000 
	 loss: 17.2489, MinusLogProbMetric: 17.2489, val_loss: 17.3483, val_MinusLogProbMetric: 17.3483

Epoch 804: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2489 - MinusLogProbMetric: 17.2489 - val_loss: 17.3483 - val_MinusLogProbMetric: 17.3483 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 805/1000
2023-09-27 19:53:49.707 
Epoch 805/1000 
	 loss: 17.2307, MinusLogProbMetric: 17.2307, val_loss: 17.4504, val_MinusLogProbMetric: 17.4504

Epoch 805: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2307 - MinusLogProbMetric: 17.2307 - val_loss: 17.4504 - val_MinusLogProbMetric: 17.4504 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 806/1000
2023-09-27 19:54:54.963 
Epoch 806/1000 
	 loss: 17.2403, MinusLogProbMetric: 17.2403, val_loss: 17.4026, val_MinusLogProbMetric: 17.4026

Epoch 806: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2403 - MinusLogProbMetric: 17.2403 - val_loss: 17.4026 - val_MinusLogProbMetric: 17.4026 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 807/1000
2023-09-27 19:55:59.949 
Epoch 807/1000 
	 loss: 17.2475, MinusLogProbMetric: 17.2475, val_loss: 17.3808, val_MinusLogProbMetric: 17.3808

Epoch 807: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2475 - MinusLogProbMetric: 17.2475 - val_loss: 17.3808 - val_MinusLogProbMetric: 17.3808 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 808/1000
2023-09-27 19:57:04.578 
Epoch 808/1000 
	 loss: 17.2419, MinusLogProbMetric: 17.2419, val_loss: 17.3397, val_MinusLogProbMetric: 17.3397

Epoch 808: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2419 - MinusLogProbMetric: 17.2419 - val_loss: 17.3397 - val_MinusLogProbMetric: 17.3397 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 809/1000
2023-09-27 19:58:09.650 
Epoch 809/1000 
	 loss: 17.2509, MinusLogProbMetric: 17.2509, val_loss: 17.4284, val_MinusLogProbMetric: 17.4284

Epoch 809: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2509 - MinusLogProbMetric: 17.2509 - val_loss: 17.4284 - val_MinusLogProbMetric: 17.4284 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 810/1000
2023-09-27 19:59:15.127 
Epoch 810/1000 
	 loss: 17.2440, MinusLogProbMetric: 17.2440, val_loss: 17.3534, val_MinusLogProbMetric: 17.3534

Epoch 810: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2440 - MinusLogProbMetric: 17.2440 - val_loss: 17.3534 - val_MinusLogProbMetric: 17.3534 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 811/1000
2023-09-27 20:00:19.620 
Epoch 811/1000 
	 loss: 17.2682, MinusLogProbMetric: 17.2682, val_loss: 17.3940, val_MinusLogProbMetric: 17.3940

Epoch 811: val_loss did not improve from 17.33285
196/196 - 64s - loss: 17.2682 - MinusLogProbMetric: 17.2682 - val_loss: 17.3940 - val_MinusLogProbMetric: 17.3940 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 812/1000
2023-09-27 20:01:24.847 
Epoch 812/1000 
	 loss: 17.2362, MinusLogProbMetric: 17.2362, val_loss: 17.4762, val_MinusLogProbMetric: 17.4762

Epoch 812: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2362 - MinusLogProbMetric: 17.2362 - val_loss: 17.4762 - val_MinusLogProbMetric: 17.4762 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 813/1000
2023-09-27 20:02:29.454 
Epoch 813/1000 
	 loss: 17.2401, MinusLogProbMetric: 17.2401, val_loss: 17.4459, val_MinusLogProbMetric: 17.4459

Epoch 813: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2401 - MinusLogProbMetric: 17.2401 - val_loss: 17.4459 - val_MinusLogProbMetric: 17.4459 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 814/1000
2023-09-27 20:03:34.340 
Epoch 814/1000 
	 loss: 17.2384, MinusLogProbMetric: 17.2384, val_loss: 17.3729, val_MinusLogProbMetric: 17.3729

Epoch 814: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2384 - MinusLogProbMetric: 17.2384 - val_loss: 17.3729 - val_MinusLogProbMetric: 17.3729 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 815/1000
2023-09-27 20:04:39.217 
Epoch 815/1000 
	 loss: 17.2454, MinusLogProbMetric: 17.2454, val_loss: 17.4355, val_MinusLogProbMetric: 17.4355

Epoch 815: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2454 - MinusLogProbMetric: 17.2454 - val_loss: 17.4355 - val_MinusLogProbMetric: 17.4355 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 816/1000
2023-09-27 20:05:44.321 
Epoch 816/1000 
	 loss: 17.2414, MinusLogProbMetric: 17.2414, val_loss: 17.3684, val_MinusLogProbMetric: 17.3684

Epoch 816: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2414 - MinusLogProbMetric: 17.2414 - val_loss: 17.3684 - val_MinusLogProbMetric: 17.3684 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 817/1000
2023-09-27 20:06:49.217 
Epoch 817/1000 
	 loss: 17.2299, MinusLogProbMetric: 17.2299, val_loss: 17.3963, val_MinusLogProbMetric: 17.3963

Epoch 817: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2299 - MinusLogProbMetric: 17.2299 - val_loss: 17.3963 - val_MinusLogProbMetric: 17.3963 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 818/1000
2023-09-27 20:07:53.882 
Epoch 818/1000 
	 loss: 17.2444, MinusLogProbMetric: 17.2444, val_loss: 17.4183, val_MinusLogProbMetric: 17.4183

Epoch 818: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2444 - MinusLogProbMetric: 17.2444 - val_loss: 17.4183 - val_MinusLogProbMetric: 17.4183 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 819/1000
2023-09-27 20:08:58.463 
Epoch 819/1000 
	 loss: 17.2734, MinusLogProbMetric: 17.2734, val_loss: 17.5162, val_MinusLogProbMetric: 17.5162

Epoch 819: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2734 - MinusLogProbMetric: 17.2734 - val_loss: 17.5162 - val_MinusLogProbMetric: 17.5162 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 820/1000
2023-09-27 20:10:03.340 
Epoch 820/1000 
	 loss: 17.2367, MinusLogProbMetric: 17.2367, val_loss: 17.3694, val_MinusLogProbMetric: 17.3694

Epoch 820: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2367 - MinusLogProbMetric: 17.2367 - val_loss: 17.3694 - val_MinusLogProbMetric: 17.3694 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 821/1000
2023-09-27 20:11:08.690 
Epoch 821/1000 
	 loss: 17.2254, MinusLogProbMetric: 17.2254, val_loss: 17.3651, val_MinusLogProbMetric: 17.3651

Epoch 821: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2254 - MinusLogProbMetric: 17.2254 - val_loss: 17.3651 - val_MinusLogProbMetric: 17.3651 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 822/1000
2023-09-27 20:12:13.832 
Epoch 822/1000 
	 loss: 17.2194, MinusLogProbMetric: 17.2194, val_loss: 17.3889, val_MinusLogProbMetric: 17.3889

Epoch 822: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2194 - MinusLogProbMetric: 17.2194 - val_loss: 17.3889 - val_MinusLogProbMetric: 17.3889 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 823/1000
2023-09-27 20:13:19.032 
Epoch 823/1000 
	 loss: 17.2456, MinusLogProbMetric: 17.2456, val_loss: 17.3439, val_MinusLogProbMetric: 17.3439

Epoch 823: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2456 - MinusLogProbMetric: 17.2456 - val_loss: 17.3439 - val_MinusLogProbMetric: 17.3439 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 824/1000
2023-09-27 20:14:24.343 
Epoch 824/1000 
	 loss: 17.2315, MinusLogProbMetric: 17.2315, val_loss: 17.4081, val_MinusLogProbMetric: 17.4081

Epoch 824: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2315 - MinusLogProbMetric: 17.2315 - val_loss: 17.4081 - val_MinusLogProbMetric: 17.4081 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 825/1000
2023-09-27 20:15:29.269 
Epoch 825/1000 
	 loss: 17.2718, MinusLogProbMetric: 17.2718, val_loss: 17.5088, val_MinusLogProbMetric: 17.5088

Epoch 825: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2718 - MinusLogProbMetric: 17.2718 - val_loss: 17.5088 - val_MinusLogProbMetric: 17.5088 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 826/1000
2023-09-27 20:16:34.417 
Epoch 826/1000 
	 loss: 17.2390, MinusLogProbMetric: 17.2390, val_loss: 17.4580, val_MinusLogProbMetric: 17.4580

Epoch 826: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2390 - MinusLogProbMetric: 17.2390 - val_loss: 17.4580 - val_MinusLogProbMetric: 17.4580 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 827/1000
2023-09-27 20:17:39.598 
Epoch 827/1000 
	 loss: 17.2426, MinusLogProbMetric: 17.2426, val_loss: 17.3919, val_MinusLogProbMetric: 17.3919

Epoch 827: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2426 - MinusLogProbMetric: 17.2426 - val_loss: 17.3919 - val_MinusLogProbMetric: 17.3919 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 828/1000
2023-09-27 20:18:44.032 
Epoch 828/1000 
	 loss: 17.2359, MinusLogProbMetric: 17.2359, val_loss: 17.4757, val_MinusLogProbMetric: 17.4757

Epoch 828: val_loss did not improve from 17.33285
196/196 - 64s - loss: 17.2359 - MinusLogProbMetric: 17.2359 - val_loss: 17.4757 - val_MinusLogProbMetric: 17.4757 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 829/1000
2023-09-27 20:19:49.311 
Epoch 829/1000 
	 loss: 17.2356, MinusLogProbMetric: 17.2356, val_loss: 17.3542, val_MinusLogProbMetric: 17.3542

Epoch 829: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2356 - MinusLogProbMetric: 17.2356 - val_loss: 17.3542 - val_MinusLogProbMetric: 17.3542 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 830/1000
2023-09-27 20:20:54.098 
Epoch 830/1000 
	 loss: 17.2232, MinusLogProbMetric: 17.2232, val_loss: 17.4404, val_MinusLogProbMetric: 17.4404

Epoch 830: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2232 - MinusLogProbMetric: 17.2232 - val_loss: 17.4404 - val_MinusLogProbMetric: 17.4404 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 831/1000
2023-09-27 20:21:58.952 
Epoch 831/1000 
	 loss: 17.2482, MinusLogProbMetric: 17.2482, val_loss: 17.4149, val_MinusLogProbMetric: 17.4149

Epoch 831: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2482 - MinusLogProbMetric: 17.2482 - val_loss: 17.4149 - val_MinusLogProbMetric: 17.4149 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 832/1000
2023-09-27 20:23:03.957 
Epoch 832/1000 
	 loss: 17.2237, MinusLogProbMetric: 17.2237, val_loss: 17.3991, val_MinusLogProbMetric: 17.3991

Epoch 832: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2237 - MinusLogProbMetric: 17.2237 - val_loss: 17.3991 - val_MinusLogProbMetric: 17.3991 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 833/1000
2023-09-27 20:24:08.926 
Epoch 833/1000 
	 loss: 17.2283, MinusLogProbMetric: 17.2283, val_loss: 17.3730, val_MinusLogProbMetric: 17.3730

Epoch 833: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2283 - MinusLogProbMetric: 17.2283 - val_loss: 17.3730 - val_MinusLogProbMetric: 17.3730 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 834/1000
2023-09-27 20:25:13.944 
Epoch 834/1000 
	 loss: 17.2427, MinusLogProbMetric: 17.2427, val_loss: 17.4554, val_MinusLogProbMetric: 17.4554

Epoch 834: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2427 - MinusLogProbMetric: 17.2427 - val_loss: 17.4554 - val_MinusLogProbMetric: 17.4554 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 835/1000
2023-09-27 20:26:19.483 
Epoch 835/1000 
	 loss: 17.2359, MinusLogProbMetric: 17.2359, val_loss: 17.3622, val_MinusLogProbMetric: 17.3622

Epoch 835: val_loss did not improve from 17.33285
196/196 - 66s - loss: 17.2359 - MinusLogProbMetric: 17.2359 - val_loss: 17.3622 - val_MinusLogProbMetric: 17.3622 - lr: 8.3333e-05 - 66s/epoch - 334ms/step
Epoch 836/1000
2023-09-27 20:27:24.147 
Epoch 836/1000 
	 loss: 17.2469, MinusLogProbMetric: 17.2469, val_loss: 17.4013, val_MinusLogProbMetric: 17.4013

Epoch 836: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2469 - MinusLogProbMetric: 17.2469 - val_loss: 17.4013 - val_MinusLogProbMetric: 17.4013 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 837/1000
2023-09-27 20:28:28.791 
Epoch 837/1000 
	 loss: 17.2585, MinusLogProbMetric: 17.2585, val_loss: 17.4915, val_MinusLogProbMetric: 17.4915

Epoch 837: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2585 - MinusLogProbMetric: 17.2585 - val_loss: 17.4915 - val_MinusLogProbMetric: 17.4915 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 838/1000
2023-09-27 20:29:33.650 
Epoch 838/1000 
	 loss: 17.2400, MinusLogProbMetric: 17.2400, val_loss: 17.4916, val_MinusLogProbMetric: 17.4916

Epoch 838: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2400 - MinusLogProbMetric: 17.2400 - val_loss: 17.4916 - val_MinusLogProbMetric: 17.4916 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 839/1000
2023-09-27 20:30:39.085 
Epoch 839/1000 
	 loss: 17.2270, MinusLogProbMetric: 17.2270, val_loss: 17.4562, val_MinusLogProbMetric: 17.4562

Epoch 839: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2270 - MinusLogProbMetric: 17.2270 - val_loss: 17.4562 - val_MinusLogProbMetric: 17.4562 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 840/1000
2023-09-27 20:31:43.830 
Epoch 840/1000 
	 loss: 17.2341, MinusLogProbMetric: 17.2341, val_loss: 17.4294, val_MinusLogProbMetric: 17.4294

Epoch 840: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2341 - MinusLogProbMetric: 17.2341 - val_loss: 17.4294 - val_MinusLogProbMetric: 17.4294 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 841/1000
2023-09-27 20:32:48.550 
Epoch 841/1000 
	 loss: 17.2470, MinusLogProbMetric: 17.2470, val_loss: 17.3455, val_MinusLogProbMetric: 17.3455

Epoch 841: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2470 - MinusLogProbMetric: 17.2470 - val_loss: 17.3455 - val_MinusLogProbMetric: 17.3455 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 842/1000
2023-09-27 20:33:53.683 
Epoch 842/1000 
	 loss: 17.2277, MinusLogProbMetric: 17.2277, val_loss: 17.4537, val_MinusLogProbMetric: 17.4537

Epoch 842: val_loss did not improve from 17.33285
196/196 - 65s - loss: 17.2277 - MinusLogProbMetric: 17.2277 - val_loss: 17.4537 - val_MinusLogProbMetric: 17.4537 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 843/1000
2023-09-27 20:34:58.507 
Epoch 843/1000 
	 loss: 17.2276, MinusLogProbMetric: 17.2276, val_loss: 17.3141, val_MinusLogProbMetric: 17.3141

Epoch 843: val_loss improved from 17.33285 to 17.31406, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 17.2276 - MinusLogProbMetric: 17.2276 - val_loss: 17.3141 - val_MinusLogProbMetric: 17.3141 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 844/1000
2023-09-27 20:36:04.724 
Epoch 844/1000 
	 loss: 17.2175, MinusLogProbMetric: 17.2175, val_loss: 17.3486, val_MinusLogProbMetric: 17.3486

Epoch 844: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2175 - MinusLogProbMetric: 17.2175 - val_loss: 17.3486 - val_MinusLogProbMetric: 17.3486 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 845/1000
2023-09-27 20:37:09.162 
Epoch 845/1000 
	 loss: 17.2270, MinusLogProbMetric: 17.2270, val_loss: 17.5555, val_MinusLogProbMetric: 17.5555

Epoch 845: val_loss did not improve from 17.31406
196/196 - 64s - loss: 17.2270 - MinusLogProbMetric: 17.2270 - val_loss: 17.5555 - val_MinusLogProbMetric: 17.5555 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 846/1000
2023-09-27 20:38:14.022 
Epoch 846/1000 
	 loss: 17.2439, MinusLogProbMetric: 17.2439, val_loss: 17.3610, val_MinusLogProbMetric: 17.3610

Epoch 846: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2439 - MinusLogProbMetric: 17.2439 - val_loss: 17.3610 - val_MinusLogProbMetric: 17.3610 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 847/1000
2023-09-27 20:39:19.003 
Epoch 847/1000 
	 loss: 17.2249, MinusLogProbMetric: 17.2249, val_loss: 17.5295, val_MinusLogProbMetric: 17.5295

Epoch 847: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2249 - MinusLogProbMetric: 17.2249 - val_loss: 17.5295 - val_MinusLogProbMetric: 17.5295 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 848/1000
2023-09-27 20:40:23.732 
Epoch 848/1000 
	 loss: 17.2273, MinusLogProbMetric: 17.2273, val_loss: 17.3597, val_MinusLogProbMetric: 17.3597

Epoch 848: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2273 - MinusLogProbMetric: 17.2273 - val_loss: 17.3597 - val_MinusLogProbMetric: 17.3597 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 849/1000
2023-09-27 20:41:28.634 
Epoch 849/1000 
	 loss: 17.2200, MinusLogProbMetric: 17.2200, val_loss: 17.3342, val_MinusLogProbMetric: 17.3342

Epoch 849: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2200 - MinusLogProbMetric: 17.2200 - val_loss: 17.3342 - val_MinusLogProbMetric: 17.3342 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 850/1000
2023-09-27 20:42:32.492 
Epoch 850/1000 
	 loss: 17.2256, MinusLogProbMetric: 17.2256, val_loss: 17.3759, val_MinusLogProbMetric: 17.3759

Epoch 850: val_loss did not improve from 17.31406
196/196 - 64s - loss: 17.2256 - MinusLogProbMetric: 17.2256 - val_loss: 17.3759 - val_MinusLogProbMetric: 17.3759 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 851/1000
2023-09-27 20:43:32.290 
Epoch 851/1000 
	 loss: 17.2218, MinusLogProbMetric: 17.2218, val_loss: 17.3737, val_MinusLogProbMetric: 17.3737

Epoch 851: val_loss did not improve from 17.31406
196/196 - 60s - loss: 17.2218 - MinusLogProbMetric: 17.2218 - val_loss: 17.3737 - val_MinusLogProbMetric: 17.3737 - lr: 8.3333e-05 - 60s/epoch - 305ms/step
Epoch 852/1000
2023-09-27 20:44:36.270 
Epoch 852/1000 
	 loss: 17.2352, MinusLogProbMetric: 17.2352, val_loss: 17.3480, val_MinusLogProbMetric: 17.3480

Epoch 852: val_loss did not improve from 17.31406
196/196 - 64s - loss: 17.2352 - MinusLogProbMetric: 17.2352 - val_loss: 17.3480 - val_MinusLogProbMetric: 17.3480 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 853/1000
2023-09-27 20:45:40.999 
Epoch 853/1000 
	 loss: 17.2062, MinusLogProbMetric: 17.2062, val_loss: 17.3989, val_MinusLogProbMetric: 17.3989

Epoch 853: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2062 - MinusLogProbMetric: 17.2062 - val_loss: 17.3989 - val_MinusLogProbMetric: 17.3989 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 854/1000
2023-09-27 20:46:45.718 
Epoch 854/1000 
	 loss: 17.2168, MinusLogProbMetric: 17.2168, val_loss: 17.3507, val_MinusLogProbMetric: 17.3507

Epoch 854: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2168 - MinusLogProbMetric: 17.2168 - val_loss: 17.3507 - val_MinusLogProbMetric: 17.3507 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 855/1000
2023-09-27 20:47:50.469 
Epoch 855/1000 
	 loss: 17.2161, MinusLogProbMetric: 17.2161, val_loss: 17.3918, val_MinusLogProbMetric: 17.3918

Epoch 855: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2161 - MinusLogProbMetric: 17.2161 - val_loss: 17.3918 - val_MinusLogProbMetric: 17.3918 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 856/1000
2023-09-27 20:48:55.494 
Epoch 856/1000 
	 loss: 17.2416, MinusLogProbMetric: 17.2416, val_loss: 17.3326, val_MinusLogProbMetric: 17.3326

Epoch 856: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2416 - MinusLogProbMetric: 17.2416 - val_loss: 17.3326 - val_MinusLogProbMetric: 17.3326 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 857/1000
2023-09-27 20:50:00.329 
Epoch 857/1000 
	 loss: 17.2277, MinusLogProbMetric: 17.2277, val_loss: 17.6398, val_MinusLogProbMetric: 17.6398

Epoch 857: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2277 - MinusLogProbMetric: 17.2277 - val_loss: 17.6398 - val_MinusLogProbMetric: 17.6398 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 858/1000
2023-09-27 20:51:04.718 
Epoch 858/1000 
	 loss: 17.2449, MinusLogProbMetric: 17.2449, val_loss: 17.3917, val_MinusLogProbMetric: 17.3917

Epoch 858: val_loss did not improve from 17.31406
196/196 - 64s - loss: 17.2449 - MinusLogProbMetric: 17.2449 - val_loss: 17.3917 - val_MinusLogProbMetric: 17.3917 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 859/1000
2023-09-27 20:52:09.036 
Epoch 859/1000 
	 loss: 17.2266, MinusLogProbMetric: 17.2266, val_loss: 17.3755, val_MinusLogProbMetric: 17.3755

Epoch 859: val_loss did not improve from 17.31406
196/196 - 64s - loss: 17.2266 - MinusLogProbMetric: 17.2266 - val_loss: 17.3755 - val_MinusLogProbMetric: 17.3755 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 860/1000
2023-09-27 20:53:13.831 
Epoch 860/1000 
	 loss: 17.2204, MinusLogProbMetric: 17.2204, val_loss: 17.4271, val_MinusLogProbMetric: 17.4271

Epoch 860: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2204 - MinusLogProbMetric: 17.2204 - val_loss: 17.4271 - val_MinusLogProbMetric: 17.4271 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 861/1000
2023-09-27 20:54:18.620 
Epoch 861/1000 
	 loss: 17.2326, MinusLogProbMetric: 17.2326, val_loss: 17.3547, val_MinusLogProbMetric: 17.3547

Epoch 861: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2326 - MinusLogProbMetric: 17.2326 - val_loss: 17.3547 - val_MinusLogProbMetric: 17.3547 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 862/1000
2023-09-27 20:55:23.436 
Epoch 862/1000 
	 loss: 17.2418, MinusLogProbMetric: 17.2418, val_loss: 17.3172, val_MinusLogProbMetric: 17.3172

Epoch 862: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2418 - MinusLogProbMetric: 17.2418 - val_loss: 17.3172 - val_MinusLogProbMetric: 17.3172 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 863/1000
2023-09-27 20:56:28.515 
Epoch 863/1000 
	 loss: 17.2389, MinusLogProbMetric: 17.2389, val_loss: 17.4414, val_MinusLogProbMetric: 17.4414

Epoch 863: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2389 - MinusLogProbMetric: 17.2389 - val_loss: 17.4414 - val_MinusLogProbMetric: 17.4414 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 864/1000
2023-09-27 20:57:33.235 
Epoch 864/1000 
	 loss: 17.2271, MinusLogProbMetric: 17.2271, val_loss: 17.4291, val_MinusLogProbMetric: 17.4291

Epoch 864: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2271 - MinusLogProbMetric: 17.2271 - val_loss: 17.4291 - val_MinusLogProbMetric: 17.4291 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 865/1000
2023-09-27 20:58:38.192 
Epoch 865/1000 
	 loss: 17.2425, MinusLogProbMetric: 17.2425, val_loss: 17.4697, val_MinusLogProbMetric: 17.4697

Epoch 865: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2425 - MinusLogProbMetric: 17.2425 - val_loss: 17.4697 - val_MinusLogProbMetric: 17.4697 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 866/1000
2023-09-27 20:59:43.573 
Epoch 866/1000 
	 loss: 17.2426, MinusLogProbMetric: 17.2426, val_loss: 17.3355, val_MinusLogProbMetric: 17.3355

Epoch 866: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2426 - MinusLogProbMetric: 17.2426 - val_loss: 17.3355 - val_MinusLogProbMetric: 17.3355 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 867/1000
2023-09-27 21:00:48.443 
Epoch 867/1000 
	 loss: 17.2283, MinusLogProbMetric: 17.2283, val_loss: 17.3574, val_MinusLogProbMetric: 17.3574

Epoch 867: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2283 - MinusLogProbMetric: 17.2283 - val_loss: 17.3574 - val_MinusLogProbMetric: 17.3574 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 868/1000
2023-09-27 21:01:53.368 
Epoch 868/1000 
	 loss: 17.2313, MinusLogProbMetric: 17.2313, val_loss: 17.4643, val_MinusLogProbMetric: 17.4643

Epoch 868: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2313 - MinusLogProbMetric: 17.2313 - val_loss: 17.4643 - val_MinusLogProbMetric: 17.4643 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 869/1000
2023-09-27 21:02:58.435 
Epoch 869/1000 
	 loss: 17.2482, MinusLogProbMetric: 17.2482, val_loss: 17.4864, val_MinusLogProbMetric: 17.4864

Epoch 869: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2482 - MinusLogProbMetric: 17.2482 - val_loss: 17.4864 - val_MinusLogProbMetric: 17.4864 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 870/1000
2023-09-27 21:04:03.754 
Epoch 870/1000 
	 loss: 17.2342, MinusLogProbMetric: 17.2342, val_loss: 17.3980, val_MinusLogProbMetric: 17.3980

Epoch 870: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2342 - MinusLogProbMetric: 17.2342 - val_loss: 17.3980 - val_MinusLogProbMetric: 17.3980 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 871/1000
2023-09-27 21:05:08.675 
Epoch 871/1000 
	 loss: 17.2137, MinusLogProbMetric: 17.2137, val_loss: 17.4192, val_MinusLogProbMetric: 17.4192

Epoch 871: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2137 - MinusLogProbMetric: 17.2137 - val_loss: 17.4192 - val_MinusLogProbMetric: 17.4192 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 872/1000
2023-09-27 21:06:12.884 
Epoch 872/1000 
	 loss: 17.2259, MinusLogProbMetric: 17.2259, val_loss: 17.3177, val_MinusLogProbMetric: 17.3177

Epoch 872: val_loss did not improve from 17.31406
196/196 - 64s - loss: 17.2259 - MinusLogProbMetric: 17.2259 - val_loss: 17.3177 - val_MinusLogProbMetric: 17.3177 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 873/1000
2023-09-27 21:07:17.364 
Epoch 873/1000 
	 loss: 17.2355, MinusLogProbMetric: 17.2355, val_loss: 17.4012, val_MinusLogProbMetric: 17.4012

Epoch 873: val_loss did not improve from 17.31406
196/196 - 64s - loss: 17.2355 - MinusLogProbMetric: 17.2355 - val_loss: 17.4012 - val_MinusLogProbMetric: 17.4012 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 874/1000
2023-09-27 21:08:21.679 
Epoch 874/1000 
	 loss: 17.2226, MinusLogProbMetric: 17.2226, val_loss: 17.4372, val_MinusLogProbMetric: 17.4372

Epoch 874: val_loss did not improve from 17.31406
196/196 - 64s - loss: 17.2226 - MinusLogProbMetric: 17.2226 - val_loss: 17.4372 - val_MinusLogProbMetric: 17.4372 - lr: 8.3333e-05 - 64s/epoch - 328ms/step
Epoch 875/1000
2023-09-27 21:09:26.343 
Epoch 875/1000 
	 loss: 17.2204, MinusLogProbMetric: 17.2204, val_loss: 17.5265, val_MinusLogProbMetric: 17.5265

Epoch 875: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2204 - MinusLogProbMetric: 17.2204 - val_loss: 17.5265 - val_MinusLogProbMetric: 17.5265 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 876/1000
2023-09-27 21:10:31.159 
Epoch 876/1000 
	 loss: 17.2538, MinusLogProbMetric: 17.2538, val_loss: 17.6594, val_MinusLogProbMetric: 17.6594

Epoch 876: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2538 - MinusLogProbMetric: 17.2538 - val_loss: 17.6594 - val_MinusLogProbMetric: 17.6594 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 877/1000
2023-09-27 21:11:36.200 
Epoch 877/1000 
	 loss: 17.2297, MinusLogProbMetric: 17.2297, val_loss: 17.3552, val_MinusLogProbMetric: 17.3552

Epoch 877: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2297 - MinusLogProbMetric: 17.2297 - val_loss: 17.3552 - val_MinusLogProbMetric: 17.3552 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 878/1000
2023-09-27 21:12:41.053 
Epoch 878/1000 
	 loss: 17.2404, MinusLogProbMetric: 17.2404, val_loss: 17.3280, val_MinusLogProbMetric: 17.3280

Epoch 878: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2404 - MinusLogProbMetric: 17.2404 - val_loss: 17.3280 - val_MinusLogProbMetric: 17.3280 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 879/1000
2023-09-27 21:13:45.867 
Epoch 879/1000 
	 loss: 17.2035, MinusLogProbMetric: 17.2035, val_loss: 17.3364, val_MinusLogProbMetric: 17.3364

Epoch 879: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2035 - MinusLogProbMetric: 17.2035 - val_loss: 17.3364 - val_MinusLogProbMetric: 17.3364 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 880/1000
2023-09-27 21:14:50.426 
Epoch 880/1000 
	 loss: 17.2177, MinusLogProbMetric: 17.2177, val_loss: 17.4353, val_MinusLogProbMetric: 17.4353

Epoch 880: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2177 - MinusLogProbMetric: 17.2177 - val_loss: 17.4353 - val_MinusLogProbMetric: 17.4353 - lr: 8.3333e-05 - 65s/epoch - 329ms/step
Epoch 881/1000
2023-09-27 21:15:55.116 
Epoch 881/1000 
	 loss: 17.2400, MinusLogProbMetric: 17.2400, val_loss: 17.3638, val_MinusLogProbMetric: 17.3638

Epoch 881: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2400 - MinusLogProbMetric: 17.2400 - val_loss: 17.3638 - val_MinusLogProbMetric: 17.3638 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 882/1000
2023-09-27 21:16:59.611 
Epoch 882/1000 
	 loss: 17.2206, MinusLogProbMetric: 17.2206, val_loss: 17.3320, val_MinusLogProbMetric: 17.3320

Epoch 882: val_loss did not improve from 17.31406
196/196 - 64s - loss: 17.2206 - MinusLogProbMetric: 17.2206 - val_loss: 17.3320 - val_MinusLogProbMetric: 17.3320 - lr: 8.3333e-05 - 64s/epoch - 329ms/step
Epoch 883/1000
2023-09-27 21:18:04.352 
Epoch 883/1000 
	 loss: 17.2154, MinusLogProbMetric: 17.2154, val_loss: 17.3789, val_MinusLogProbMetric: 17.3789

Epoch 883: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2154 - MinusLogProbMetric: 17.2154 - val_loss: 17.3789 - val_MinusLogProbMetric: 17.3789 - lr: 8.3333e-05 - 65s/epoch - 330ms/step
Epoch 884/1000
2023-09-27 21:19:09.135 
Epoch 884/1000 
	 loss: 17.2218, MinusLogProbMetric: 17.2218, val_loss: 17.3159, val_MinusLogProbMetric: 17.3159

Epoch 884: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2218 - MinusLogProbMetric: 17.2218 - val_loss: 17.3159 - val_MinusLogProbMetric: 17.3159 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 885/1000
2023-09-27 21:20:14.103 
Epoch 885/1000 
	 loss: 17.2065, MinusLogProbMetric: 17.2065, val_loss: 17.3456, val_MinusLogProbMetric: 17.3456

Epoch 885: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2065 - MinusLogProbMetric: 17.2065 - val_loss: 17.3456 - val_MinusLogProbMetric: 17.3456 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 886/1000
2023-09-27 21:21:19.720 
Epoch 886/1000 
	 loss: 17.2192, MinusLogProbMetric: 17.2192, val_loss: 17.3540, val_MinusLogProbMetric: 17.3540

Epoch 886: val_loss did not improve from 17.31406
196/196 - 66s - loss: 17.2192 - MinusLogProbMetric: 17.2192 - val_loss: 17.3540 - val_MinusLogProbMetric: 17.3540 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 887/1000
2023-09-27 21:22:24.846 
Epoch 887/1000 
	 loss: 17.2276, MinusLogProbMetric: 17.2276, val_loss: 17.3558, val_MinusLogProbMetric: 17.3558

Epoch 887: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2276 - MinusLogProbMetric: 17.2276 - val_loss: 17.3558 - val_MinusLogProbMetric: 17.3558 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 888/1000
2023-09-27 21:23:29.858 
Epoch 888/1000 
	 loss: 17.2280, MinusLogProbMetric: 17.2280, val_loss: 17.3790, val_MinusLogProbMetric: 17.3790

Epoch 888: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2280 - MinusLogProbMetric: 17.2280 - val_loss: 17.3790 - val_MinusLogProbMetric: 17.3790 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 889/1000
2023-09-27 21:24:34.862 
Epoch 889/1000 
	 loss: 17.2091, MinusLogProbMetric: 17.2091, val_loss: 17.5271, val_MinusLogProbMetric: 17.5271

Epoch 889: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2091 - MinusLogProbMetric: 17.2091 - val_loss: 17.5271 - val_MinusLogProbMetric: 17.5271 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 890/1000
2023-09-27 21:25:40.346 
Epoch 890/1000 
	 loss: 17.2391, MinusLogProbMetric: 17.2391, val_loss: 17.4083, val_MinusLogProbMetric: 17.4083

Epoch 890: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2391 - MinusLogProbMetric: 17.2391 - val_loss: 17.4083 - val_MinusLogProbMetric: 17.4083 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 891/1000
2023-09-27 21:26:45.196 
Epoch 891/1000 
	 loss: 17.2255, MinusLogProbMetric: 17.2255, val_loss: 17.5662, val_MinusLogProbMetric: 17.5662

Epoch 891: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2255 - MinusLogProbMetric: 17.2255 - val_loss: 17.5662 - val_MinusLogProbMetric: 17.5662 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 892/1000
2023-09-27 21:27:50.020 
Epoch 892/1000 
	 loss: 17.2270, MinusLogProbMetric: 17.2270, val_loss: 17.4028, val_MinusLogProbMetric: 17.4028

Epoch 892: val_loss did not improve from 17.31406
196/196 - 65s - loss: 17.2270 - MinusLogProbMetric: 17.2270 - val_loss: 17.4028 - val_MinusLogProbMetric: 17.4028 - lr: 8.3333e-05 - 65s/epoch - 331ms/step
Epoch 893/1000
2023-09-27 21:28:55.534 
Epoch 893/1000 
	 loss: 17.2242, MinusLogProbMetric: 17.2242, val_loss: 17.4546, val_MinusLogProbMetric: 17.4546

Epoch 893: val_loss did not improve from 17.31406
196/196 - 66s - loss: 17.2242 - MinusLogProbMetric: 17.2242 - val_loss: 17.4546 - val_MinusLogProbMetric: 17.4546 - lr: 8.3333e-05 - 66s/epoch - 334ms/step
Epoch 894/1000
2023-09-27 21:30:01.022 
Epoch 894/1000 
	 loss: 17.1483, MinusLogProbMetric: 17.1483, val_loss: 17.2892, val_MinusLogProbMetric: 17.2892

Epoch 894: val_loss improved from 17.31406 to 17.28919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 67s - loss: 17.1483 - MinusLogProbMetric: 17.1483 - val_loss: 17.2892 - val_MinusLogProbMetric: 17.2892 - lr: 4.1667e-05 - 67s/epoch - 340ms/step
Epoch 895/1000
2023-09-27 21:31:06.475 
Epoch 895/1000 
	 loss: 17.1394, MinusLogProbMetric: 17.1394, val_loss: 17.3006, val_MinusLogProbMetric: 17.3006

Epoch 895: val_loss did not improve from 17.28919
196/196 - 64s - loss: 17.1394 - MinusLogProbMetric: 17.1394 - val_loss: 17.3006 - val_MinusLogProbMetric: 17.3006 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 896/1000
2023-09-27 21:32:11.196 
Epoch 896/1000 
	 loss: 17.1396, MinusLogProbMetric: 17.1396, val_loss: 17.3129, val_MinusLogProbMetric: 17.3129

Epoch 896: val_loss did not improve from 17.28919
196/196 - 65s - loss: 17.1396 - MinusLogProbMetric: 17.1396 - val_loss: 17.3129 - val_MinusLogProbMetric: 17.3129 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 897/1000
2023-09-27 21:33:15.835 
Epoch 897/1000 
	 loss: 17.1374, MinusLogProbMetric: 17.1374, val_loss: 17.3060, val_MinusLogProbMetric: 17.3060

Epoch 897: val_loss did not improve from 17.28919
196/196 - 65s - loss: 17.1374 - MinusLogProbMetric: 17.1374 - val_loss: 17.3060 - val_MinusLogProbMetric: 17.3060 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 898/1000
2023-09-27 21:34:20.691 
Epoch 898/1000 
	 loss: 17.1356, MinusLogProbMetric: 17.1356, val_loss: 17.2801, val_MinusLogProbMetric: 17.2801

Epoch 898: val_loss improved from 17.28919 to 17.28010, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 17.1356 - MinusLogProbMetric: 17.1356 - val_loss: 17.2801 - val_MinusLogProbMetric: 17.2801 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 899/1000
2023-09-27 21:35:26.935 
Epoch 899/1000 
	 loss: 17.1459, MinusLogProbMetric: 17.1459, val_loss: 17.2744, val_MinusLogProbMetric: 17.2744

Epoch 899: val_loss improved from 17.28010 to 17.27444, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 17.1459 - MinusLogProbMetric: 17.1459 - val_loss: 17.2744 - val_MinusLogProbMetric: 17.2744 - lr: 4.1667e-05 - 66s/epoch - 338ms/step
Epoch 900/1000
2023-09-27 21:36:33.242 
Epoch 900/1000 
	 loss: 17.1333, MinusLogProbMetric: 17.1333, val_loss: 17.2828, val_MinusLogProbMetric: 17.2828

Epoch 900: val_loss did not improve from 17.27444
196/196 - 65s - loss: 17.1333 - MinusLogProbMetric: 17.1333 - val_loss: 17.2828 - val_MinusLogProbMetric: 17.2828 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 901/1000
2023-09-27 21:37:38.536 
Epoch 901/1000 
	 loss: 17.1378, MinusLogProbMetric: 17.1378, val_loss: 17.3251, val_MinusLogProbMetric: 17.3251

Epoch 901: val_loss did not improve from 17.27444
196/196 - 65s - loss: 17.1378 - MinusLogProbMetric: 17.1378 - val_loss: 17.3251 - val_MinusLogProbMetric: 17.3251 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 902/1000
2023-09-27 21:38:43.960 
Epoch 902/1000 
	 loss: 17.1551, MinusLogProbMetric: 17.1551, val_loss: 17.2974, val_MinusLogProbMetric: 17.2974

Epoch 902: val_loss did not improve from 17.27444
196/196 - 65s - loss: 17.1551 - MinusLogProbMetric: 17.1551 - val_loss: 17.2974 - val_MinusLogProbMetric: 17.2974 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 903/1000
2023-09-27 21:39:49.470 
Epoch 903/1000 
	 loss: 17.1495, MinusLogProbMetric: 17.1495, val_loss: 17.2977, val_MinusLogProbMetric: 17.2977

Epoch 903: val_loss did not improve from 17.27444
196/196 - 66s - loss: 17.1495 - MinusLogProbMetric: 17.1495 - val_loss: 17.2977 - val_MinusLogProbMetric: 17.2977 - lr: 4.1667e-05 - 66s/epoch - 334ms/step
Epoch 904/1000
2023-09-27 21:40:54.898 
Epoch 904/1000 
	 loss: 17.1361, MinusLogProbMetric: 17.1361, val_loss: 17.2890, val_MinusLogProbMetric: 17.2890

Epoch 904: val_loss did not improve from 17.27444
196/196 - 65s - loss: 17.1361 - MinusLogProbMetric: 17.1361 - val_loss: 17.2890 - val_MinusLogProbMetric: 17.2890 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 905/1000
2023-09-27 21:41:59.855 
Epoch 905/1000 
	 loss: 17.1371, MinusLogProbMetric: 17.1371, val_loss: 17.2948, val_MinusLogProbMetric: 17.2948

Epoch 905: val_loss did not improve from 17.27444
196/196 - 65s - loss: 17.1371 - MinusLogProbMetric: 17.1371 - val_loss: 17.2948 - val_MinusLogProbMetric: 17.2948 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 906/1000
2023-09-27 21:43:04.558 
Epoch 906/1000 
	 loss: 17.1325, MinusLogProbMetric: 17.1325, val_loss: 17.2781, val_MinusLogProbMetric: 17.2781

Epoch 906: val_loss did not improve from 17.27444
196/196 - 65s - loss: 17.1325 - MinusLogProbMetric: 17.1325 - val_loss: 17.2781 - val_MinusLogProbMetric: 17.2781 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 907/1000
2023-09-27 21:44:09.667 
Epoch 907/1000 
	 loss: 17.1346, MinusLogProbMetric: 17.1346, val_loss: 17.3365, val_MinusLogProbMetric: 17.3365

Epoch 907: val_loss did not improve from 17.27444
196/196 - 65s - loss: 17.1346 - MinusLogProbMetric: 17.1346 - val_loss: 17.3365 - val_MinusLogProbMetric: 17.3365 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 908/1000
2023-09-27 21:45:14.396 
Epoch 908/1000 
	 loss: 17.1327, MinusLogProbMetric: 17.1327, val_loss: 17.2837, val_MinusLogProbMetric: 17.2837

Epoch 908: val_loss did not improve from 17.27444
196/196 - 65s - loss: 17.1327 - MinusLogProbMetric: 17.1327 - val_loss: 17.2837 - val_MinusLogProbMetric: 17.2837 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 909/1000
2023-09-27 21:46:19.434 
Epoch 909/1000 
	 loss: 17.1417, MinusLogProbMetric: 17.1417, val_loss: 17.2880, val_MinusLogProbMetric: 17.2880

Epoch 909: val_loss did not improve from 17.27444
196/196 - 65s - loss: 17.1417 - MinusLogProbMetric: 17.1417 - val_loss: 17.2880 - val_MinusLogProbMetric: 17.2880 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 910/1000
2023-09-27 21:47:24.404 
Epoch 910/1000 
	 loss: 17.1331, MinusLogProbMetric: 17.1331, val_loss: 17.3134, val_MinusLogProbMetric: 17.3134

Epoch 910: val_loss did not improve from 17.27444
196/196 - 65s - loss: 17.1331 - MinusLogProbMetric: 17.1331 - val_loss: 17.3134 - val_MinusLogProbMetric: 17.3134 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 911/1000
2023-09-27 21:48:29.733 
Epoch 911/1000 
	 loss: 17.1288, MinusLogProbMetric: 17.1288, val_loss: 17.2940, val_MinusLogProbMetric: 17.2940

Epoch 911: val_loss did not improve from 17.27444
196/196 - 65s - loss: 17.1288 - MinusLogProbMetric: 17.1288 - val_loss: 17.2940 - val_MinusLogProbMetric: 17.2940 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 912/1000
2023-09-27 21:49:34.788 
Epoch 912/1000 
	 loss: 17.1282, MinusLogProbMetric: 17.1282, val_loss: 17.2944, val_MinusLogProbMetric: 17.2944

Epoch 912: val_loss did not improve from 17.27444
196/196 - 65s - loss: 17.1282 - MinusLogProbMetric: 17.1282 - val_loss: 17.2944 - val_MinusLogProbMetric: 17.2944 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 913/1000
2023-09-27 21:50:39.860 
Epoch 913/1000 
	 loss: 17.1334, MinusLogProbMetric: 17.1334, val_loss: 17.2698, val_MinusLogProbMetric: 17.2698

Epoch 913: val_loss improved from 17.27444 to 17.26985, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 17.1334 - MinusLogProbMetric: 17.1334 - val_loss: 17.2698 - val_MinusLogProbMetric: 17.2698 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 914/1000
2023-09-27 21:51:46.151 
Epoch 914/1000 
	 loss: 17.1359, MinusLogProbMetric: 17.1359, val_loss: 17.2766, val_MinusLogProbMetric: 17.2766

Epoch 914: val_loss did not improve from 17.26985
196/196 - 65s - loss: 17.1359 - MinusLogProbMetric: 17.1359 - val_loss: 17.2766 - val_MinusLogProbMetric: 17.2766 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 915/1000
2023-09-27 21:52:51.549 
Epoch 915/1000 
	 loss: 17.1284, MinusLogProbMetric: 17.1284, val_loss: 17.2904, val_MinusLogProbMetric: 17.2904

Epoch 915: val_loss did not improve from 17.26985
196/196 - 65s - loss: 17.1284 - MinusLogProbMetric: 17.1284 - val_loss: 17.2904 - val_MinusLogProbMetric: 17.2904 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 916/1000
2023-09-27 21:53:56.346 
Epoch 916/1000 
	 loss: 17.1316, MinusLogProbMetric: 17.1316, val_loss: 17.3072, val_MinusLogProbMetric: 17.3072

Epoch 916: val_loss did not improve from 17.26985
196/196 - 65s - loss: 17.1316 - MinusLogProbMetric: 17.1316 - val_loss: 17.3072 - val_MinusLogProbMetric: 17.3072 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 917/1000
2023-09-27 21:55:01.235 
Epoch 917/1000 
	 loss: 17.1320, MinusLogProbMetric: 17.1320, val_loss: 17.2732, val_MinusLogProbMetric: 17.2732

Epoch 917: val_loss did not improve from 17.26985
196/196 - 65s - loss: 17.1320 - MinusLogProbMetric: 17.1320 - val_loss: 17.2732 - val_MinusLogProbMetric: 17.2732 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 918/1000
2023-09-27 21:56:06.431 
Epoch 918/1000 
	 loss: 17.1335, MinusLogProbMetric: 17.1335, val_loss: 17.3251, val_MinusLogProbMetric: 17.3251

Epoch 918: val_loss did not improve from 17.26985
196/196 - 65s - loss: 17.1335 - MinusLogProbMetric: 17.1335 - val_loss: 17.3251 - val_MinusLogProbMetric: 17.3251 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 919/1000
2023-09-27 21:57:11.366 
Epoch 919/1000 
	 loss: 17.1327, MinusLogProbMetric: 17.1327, val_loss: 17.3432, val_MinusLogProbMetric: 17.3432

Epoch 919: val_loss did not improve from 17.26985
196/196 - 65s - loss: 17.1327 - MinusLogProbMetric: 17.1327 - val_loss: 17.3432 - val_MinusLogProbMetric: 17.3432 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 920/1000
2023-09-27 21:58:17.103 
Epoch 920/1000 
	 loss: 17.1410, MinusLogProbMetric: 17.1410, val_loss: 17.2728, val_MinusLogProbMetric: 17.2728

Epoch 920: val_loss did not improve from 17.26985
196/196 - 66s - loss: 17.1410 - MinusLogProbMetric: 17.1410 - val_loss: 17.2728 - val_MinusLogProbMetric: 17.2728 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 921/1000
2023-09-27 21:59:21.736 
Epoch 921/1000 
	 loss: 17.1379, MinusLogProbMetric: 17.1379, val_loss: 17.2666, val_MinusLogProbMetric: 17.2666

Epoch 921: val_loss improved from 17.26985 to 17.26657, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 17.1379 - MinusLogProbMetric: 17.1379 - val_loss: 17.2666 - val_MinusLogProbMetric: 17.2666 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 922/1000
2023-09-27 22:00:27.585 
Epoch 922/1000 
	 loss: 17.1267, MinusLogProbMetric: 17.1267, val_loss: 17.2732, val_MinusLogProbMetric: 17.2732

Epoch 922: val_loss did not improve from 17.26657
196/196 - 65s - loss: 17.1267 - MinusLogProbMetric: 17.1267 - val_loss: 17.2732 - val_MinusLogProbMetric: 17.2732 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 923/1000
2023-09-27 22:01:32.794 
Epoch 923/1000 
	 loss: 17.1333, MinusLogProbMetric: 17.1333, val_loss: 17.3266, val_MinusLogProbMetric: 17.3266

Epoch 923: val_loss did not improve from 17.26657
196/196 - 65s - loss: 17.1333 - MinusLogProbMetric: 17.1333 - val_loss: 17.3266 - val_MinusLogProbMetric: 17.3266 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 924/1000
2023-09-27 22:02:38.264 
Epoch 924/1000 
	 loss: 17.1392, MinusLogProbMetric: 17.1392, val_loss: 17.2953, val_MinusLogProbMetric: 17.2953

Epoch 924: val_loss did not improve from 17.26657
196/196 - 65s - loss: 17.1392 - MinusLogProbMetric: 17.1392 - val_loss: 17.2953 - val_MinusLogProbMetric: 17.2953 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 925/1000
2023-09-27 22:03:43.080 
Epoch 925/1000 
	 loss: 17.1389, MinusLogProbMetric: 17.1389, val_loss: 17.3164, val_MinusLogProbMetric: 17.3164

Epoch 925: val_loss did not improve from 17.26657
196/196 - 65s - loss: 17.1389 - MinusLogProbMetric: 17.1389 - val_loss: 17.3164 - val_MinusLogProbMetric: 17.3164 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 926/1000
2023-09-27 22:04:48.352 
Epoch 926/1000 
	 loss: 17.1297, MinusLogProbMetric: 17.1297, val_loss: 17.2667, val_MinusLogProbMetric: 17.2667

Epoch 926: val_loss did not improve from 17.26657
196/196 - 65s - loss: 17.1297 - MinusLogProbMetric: 17.1297 - val_loss: 17.2667 - val_MinusLogProbMetric: 17.2667 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 927/1000
2023-09-27 22:05:54.045 
Epoch 927/1000 
	 loss: 17.1281, MinusLogProbMetric: 17.1281, val_loss: 17.2868, val_MinusLogProbMetric: 17.2868

Epoch 927: val_loss did not improve from 17.26657
196/196 - 66s - loss: 17.1281 - MinusLogProbMetric: 17.1281 - val_loss: 17.2868 - val_MinusLogProbMetric: 17.2868 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 928/1000
2023-09-27 22:06:58.959 
Epoch 928/1000 
	 loss: 17.1359, MinusLogProbMetric: 17.1359, val_loss: 17.2945, val_MinusLogProbMetric: 17.2945

Epoch 928: val_loss did not improve from 17.26657
196/196 - 65s - loss: 17.1359 - MinusLogProbMetric: 17.1359 - val_loss: 17.2945 - val_MinusLogProbMetric: 17.2945 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 929/1000
2023-09-27 22:08:04.107 
Epoch 929/1000 
	 loss: 17.1362, MinusLogProbMetric: 17.1362, val_loss: 17.2742, val_MinusLogProbMetric: 17.2742

Epoch 929: val_loss did not improve from 17.26657
196/196 - 65s - loss: 17.1362 - MinusLogProbMetric: 17.1362 - val_loss: 17.2742 - val_MinusLogProbMetric: 17.2742 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 930/1000
2023-09-27 22:09:06.848 
Epoch 930/1000 
	 loss: 17.1296, MinusLogProbMetric: 17.1296, val_loss: 17.3044, val_MinusLogProbMetric: 17.3044

Epoch 930: val_loss did not improve from 17.26657
196/196 - 63s - loss: 17.1296 - MinusLogProbMetric: 17.1296 - val_loss: 17.3044 - val_MinusLogProbMetric: 17.3044 - lr: 4.1667e-05 - 63s/epoch - 320ms/step
Epoch 931/1000
2023-09-27 22:10:04.931 
Epoch 931/1000 
	 loss: 17.1383, MinusLogProbMetric: 17.1383, val_loss: 17.2628, val_MinusLogProbMetric: 17.2628

Epoch 931: val_loss improved from 17.26657 to 17.26280, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 59s - loss: 17.1383 - MinusLogProbMetric: 17.1383 - val_loss: 17.2628 - val_MinusLogProbMetric: 17.2628 - lr: 4.1667e-05 - 59s/epoch - 302ms/step
Epoch 932/1000
2023-09-27 22:11:08.931 
Epoch 932/1000 
	 loss: 17.1307, MinusLogProbMetric: 17.1307, val_loss: 17.2897, val_MinusLogProbMetric: 17.2897

Epoch 932: val_loss did not improve from 17.26280
196/196 - 63s - loss: 17.1307 - MinusLogProbMetric: 17.1307 - val_loss: 17.2897 - val_MinusLogProbMetric: 17.2897 - lr: 4.1667e-05 - 63s/epoch - 321ms/step
Epoch 933/1000
2023-09-27 22:12:13.824 
Epoch 933/1000 
	 loss: 17.1361, MinusLogProbMetric: 17.1361, val_loss: 17.3208, val_MinusLogProbMetric: 17.3208

Epoch 933: val_loss did not improve from 17.26280
196/196 - 65s - loss: 17.1361 - MinusLogProbMetric: 17.1361 - val_loss: 17.3208 - val_MinusLogProbMetric: 17.3208 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 934/1000
2023-09-27 22:13:18.554 
Epoch 934/1000 
	 loss: 17.1358, MinusLogProbMetric: 17.1358, val_loss: 17.2998, val_MinusLogProbMetric: 17.2998

Epoch 934: val_loss did not improve from 17.26280
196/196 - 65s - loss: 17.1358 - MinusLogProbMetric: 17.1358 - val_loss: 17.2998 - val_MinusLogProbMetric: 17.2998 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 935/1000
2023-09-27 22:14:23.033 
Epoch 935/1000 
	 loss: 17.1317, MinusLogProbMetric: 17.1317, val_loss: 17.2653, val_MinusLogProbMetric: 17.2653

Epoch 935: val_loss did not improve from 17.26280
196/196 - 64s - loss: 17.1317 - MinusLogProbMetric: 17.1317 - val_loss: 17.2653 - val_MinusLogProbMetric: 17.2653 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Epoch 936/1000
2023-09-27 22:15:28.035 
Epoch 936/1000 
	 loss: 17.1270, MinusLogProbMetric: 17.1270, val_loss: 17.2794, val_MinusLogProbMetric: 17.2794

Epoch 936: val_loss did not improve from 17.26280
196/196 - 65s - loss: 17.1270 - MinusLogProbMetric: 17.1270 - val_loss: 17.2794 - val_MinusLogProbMetric: 17.2794 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 937/1000
2023-09-27 22:16:32.671 
Epoch 937/1000 
	 loss: 17.1386, MinusLogProbMetric: 17.1386, val_loss: 17.2885, val_MinusLogProbMetric: 17.2885

Epoch 937: val_loss did not improve from 17.26280
196/196 - 65s - loss: 17.1386 - MinusLogProbMetric: 17.1386 - val_loss: 17.2885 - val_MinusLogProbMetric: 17.2885 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 938/1000
2023-09-27 22:17:37.894 
Epoch 938/1000 
	 loss: 17.1309, MinusLogProbMetric: 17.1309, val_loss: 17.2906, val_MinusLogProbMetric: 17.2906

Epoch 938: val_loss did not improve from 17.26280
196/196 - 65s - loss: 17.1309 - MinusLogProbMetric: 17.1309 - val_loss: 17.2906 - val_MinusLogProbMetric: 17.2906 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 939/1000
2023-09-27 22:18:42.261 
Epoch 939/1000 
	 loss: 17.1319, MinusLogProbMetric: 17.1319, val_loss: 17.2969, val_MinusLogProbMetric: 17.2969

Epoch 939: val_loss did not improve from 17.26280
196/196 - 64s - loss: 17.1319 - MinusLogProbMetric: 17.1319 - val_loss: 17.2969 - val_MinusLogProbMetric: 17.2969 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 940/1000
2023-09-27 22:19:46.499 
Epoch 940/1000 
	 loss: 17.1407, MinusLogProbMetric: 17.1407, val_loss: 17.3021, val_MinusLogProbMetric: 17.3021

Epoch 940: val_loss did not improve from 17.26280
196/196 - 64s - loss: 17.1407 - MinusLogProbMetric: 17.1407 - val_loss: 17.3021 - val_MinusLogProbMetric: 17.3021 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 941/1000
2023-09-27 22:20:51.294 
Epoch 941/1000 
	 loss: 17.1330, MinusLogProbMetric: 17.1330, val_loss: 17.3310, val_MinusLogProbMetric: 17.3310

Epoch 941: val_loss did not improve from 17.26280
196/196 - 65s - loss: 17.1330 - MinusLogProbMetric: 17.1330 - val_loss: 17.3310 - val_MinusLogProbMetric: 17.3310 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 942/1000
2023-09-27 22:21:56.044 
Epoch 942/1000 
	 loss: 17.1290, MinusLogProbMetric: 17.1290, val_loss: 17.3001, val_MinusLogProbMetric: 17.3001

Epoch 942: val_loss did not improve from 17.26280
196/196 - 65s - loss: 17.1290 - MinusLogProbMetric: 17.1290 - val_loss: 17.3001 - val_MinusLogProbMetric: 17.3001 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 943/1000
2023-09-27 22:23:01.026 
Epoch 943/1000 
	 loss: 17.1280, MinusLogProbMetric: 17.1280, val_loss: 17.2996, val_MinusLogProbMetric: 17.2996

Epoch 943: val_loss did not improve from 17.26280
196/196 - 65s - loss: 17.1280 - MinusLogProbMetric: 17.1280 - val_loss: 17.2996 - val_MinusLogProbMetric: 17.2996 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 944/1000
2023-09-27 22:24:05.808 
Epoch 944/1000 
	 loss: 17.1291, MinusLogProbMetric: 17.1291, val_loss: 17.2627, val_MinusLogProbMetric: 17.2627

Epoch 944: val_loss improved from 17.26280 to 17.26273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 65s - loss: 17.1291 - MinusLogProbMetric: 17.1291 - val_loss: 17.2627 - val_MinusLogProbMetric: 17.2627 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 945/1000
2023-09-27 22:25:10.721 
Epoch 945/1000 
	 loss: 17.1279, MinusLogProbMetric: 17.1279, val_loss: 17.3214, val_MinusLogProbMetric: 17.3214

Epoch 945: val_loss did not improve from 17.26273
196/196 - 64s - loss: 17.1279 - MinusLogProbMetric: 17.1279 - val_loss: 17.3214 - val_MinusLogProbMetric: 17.3214 - lr: 4.1667e-05 - 64s/epoch - 328ms/step
Epoch 946/1000
2023-09-27 22:26:15.621 
Epoch 946/1000 
	 loss: 17.1366, MinusLogProbMetric: 17.1366, val_loss: 17.3297, val_MinusLogProbMetric: 17.3297

Epoch 946: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1366 - MinusLogProbMetric: 17.1366 - val_loss: 17.3297 - val_MinusLogProbMetric: 17.3297 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 947/1000
2023-09-27 22:27:20.444 
Epoch 947/1000 
	 loss: 17.1329, MinusLogProbMetric: 17.1329, val_loss: 17.2871, val_MinusLogProbMetric: 17.2871

Epoch 947: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1329 - MinusLogProbMetric: 17.1329 - val_loss: 17.2871 - val_MinusLogProbMetric: 17.2871 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 948/1000
2023-09-27 22:28:25.129 
Epoch 948/1000 
	 loss: 17.1295, MinusLogProbMetric: 17.1295, val_loss: 17.3187, val_MinusLogProbMetric: 17.3187

Epoch 948: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1295 - MinusLogProbMetric: 17.1295 - val_loss: 17.3187 - val_MinusLogProbMetric: 17.3187 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 949/1000
2023-09-27 22:29:30.215 
Epoch 949/1000 
	 loss: 17.1280, MinusLogProbMetric: 17.1280, val_loss: 17.2638, val_MinusLogProbMetric: 17.2638

Epoch 949: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1280 - MinusLogProbMetric: 17.1280 - val_loss: 17.2638 - val_MinusLogProbMetric: 17.2638 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 950/1000
2023-09-27 22:30:34.817 
Epoch 950/1000 
	 loss: 17.1241, MinusLogProbMetric: 17.1241, val_loss: 17.3350, val_MinusLogProbMetric: 17.3350

Epoch 950: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1241 - MinusLogProbMetric: 17.1241 - val_loss: 17.3350 - val_MinusLogProbMetric: 17.3350 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 951/1000
2023-09-27 22:31:39.437 
Epoch 951/1000 
	 loss: 17.1280, MinusLogProbMetric: 17.1280, val_loss: 17.3071, val_MinusLogProbMetric: 17.3071

Epoch 951: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1280 - MinusLogProbMetric: 17.1280 - val_loss: 17.3071 - val_MinusLogProbMetric: 17.3071 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 952/1000
2023-09-27 22:32:43.952 
Epoch 952/1000 
	 loss: 17.1306, MinusLogProbMetric: 17.1306, val_loss: 17.2829, val_MinusLogProbMetric: 17.2829

Epoch 952: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1306 - MinusLogProbMetric: 17.1306 - val_loss: 17.2829 - val_MinusLogProbMetric: 17.2829 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 953/1000
2023-09-27 22:33:49.248 
Epoch 953/1000 
	 loss: 17.1287, MinusLogProbMetric: 17.1287, val_loss: 17.2750, val_MinusLogProbMetric: 17.2750

Epoch 953: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1287 - MinusLogProbMetric: 17.1287 - val_loss: 17.2750 - val_MinusLogProbMetric: 17.2750 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 954/1000
2023-09-27 22:34:54.188 
Epoch 954/1000 
	 loss: 17.1342, MinusLogProbMetric: 17.1342, val_loss: 17.2925, val_MinusLogProbMetric: 17.2925

Epoch 954: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1342 - MinusLogProbMetric: 17.1342 - val_loss: 17.2925 - val_MinusLogProbMetric: 17.2925 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 955/1000
2023-09-27 22:35:59.532 
Epoch 955/1000 
	 loss: 17.1293, MinusLogProbMetric: 17.1293, val_loss: 17.2653, val_MinusLogProbMetric: 17.2653

Epoch 955: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1293 - MinusLogProbMetric: 17.1293 - val_loss: 17.2653 - val_MinusLogProbMetric: 17.2653 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 956/1000
2023-09-27 22:37:05.052 
Epoch 956/1000 
	 loss: 17.1342, MinusLogProbMetric: 17.1342, val_loss: 17.3104, val_MinusLogProbMetric: 17.3104

Epoch 956: val_loss did not improve from 17.26273
196/196 - 66s - loss: 17.1342 - MinusLogProbMetric: 17.1342 - val_loss: 17.3104 - val_MinusLogProbMetric: 17.3104 - lr: 4.1667e-05 - 66s/epoch - 334ms/step
Epoch 957/1000
2023-09-27 22:38:10.100 
Epoch 957/1000 
	 loss: 17.1248, MinusLogProbMetric: 17.1248, val_loss: 17.2899, val_MinusLogProbMetric: 17.2899

Epoch 957: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1248 - MinusLogProbMetric: 17.1248 - val_loss: 17.2899 - val_MinusLogProbMetric: 17.2899 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 958/1000
2023-09-27 22:39:14.747 
Epoch 958/1000 
	 loss: 17.1265, MinusLogProbMetric: 17.1265, val_loss: 17.2763, val_MinusLogProbMetric: 17.2763

Epoch 958: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1265 - MinusLogProbMetric: 17.1265 - val_loss: 17.2763 - val_MinusLogProbMetric: 17.2763 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 959/1000
2023-09-27 22:40:19.389 
Epoch 959/1000 
	 loss: 17.1330, MinusLogProbMetric: 17.1330, val_loss: 17.2964, val_MinusLogProbMetric: 17.2964

Epoch 959: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1330 - MinusLogProbMetric: 17.1330 - val_loss: 17.2964 - val_MinusLogProbMetric: 17.2964 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 960/1000
2023-09-27 22:41:24.540 
Epoch 960/1000 
	 loss: 17.1288, MinusLogProbMetric: 17.1288, val_loss: 17.2927, val_MinusLogProbMetric: 17.2927

Epoch 960: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1288 - MinusLogProbMetric: 17.1288 - val_loss: 17.2927 - val_MinusLogProbMetric: 17.2927 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 961/1000
2023-09-27 22:42:29.427 
Epoch 961/1000 
	 loss: 17.1435, MinusLogProbMetric: 17.1435, val_loss: 17.3049, val_MinusLogProbMetric: 17.3049

Epoch 961: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1435 - MinusLogProbMetric: 17.1435 - val_loss: 17.3049 - val_MinusLogProbMetric: 17.3049 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 962/1000
2023-09-27 22:43:34.680 
Epoch 962/1000 
	 loss: 17.1253, MinusLogProbMetric: 17.1253, val_loss: 17.3542, val_MinusLogProbMetric: 17.3542

Epoch 962: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1253 - MinusLogProbMetric: 17.1253 - val_loss: 17.3542 - val_MinusLogProbMetric: 17.3542 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 963/1000
2023-09-27 22:44:39.754 
Epoch 963/1000 
	 loss: 17.1267, MinusLogProbMetric: 17.1267, val_loss: 17.2833, val_MinusLogProbMetric: 17.2833

Epoch 963: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1267 - MinusLogProbMetric: 17.1267 - val_loss: 17.2833 - val_MinusLogProbMetric: 17.2833 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 964/1000
2023-09-27 22:45:45.176 
Epoch 964/1000 
	 loss: 17.1239, MinusLogProbMetric: 17.1239, val_loss: 17.2889, val_MinusLogProbMetric: 17.2889

Epoch 964: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1239 - MinusLogProbMetric: 17.1239 - val_loss: 17.2889 - val_MinusLogProbMetric: 17.2889 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 965/1000
2023-09-27 22:46:49.926 
Epoch 965/1000 
	 loss: 17.1393, MinusLogProbMetric: 17.1393, val_loss: 17.2667, val_MinusLogProbMetric: 17.2667

Epoch 965: val_loss did not improve from 17.26273
196/196 - 65s - loss: 17.1393 - MinusLogProbMetric: 17.1393 - val_loss: 17.2667 - val_MinusLogProbMetric: 17.2667 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 966/1000
2023-09-27 22:47:54.901 
Epoch 966/1000 
	 loss: 17.1270, MinusLogProbMetric: 17.1270, val_loss: 17.2589, val_MinusLogProbMetric: 17.2589

Epoch 966: val_loss improved from 17.26273 to 17.25889, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 17.1270 - MinusLogProbMetric: 17.1270 - val_loss: 17.2589 - val_MinusLogProbMetric: 17.2589 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 967/1000
2023-09-27 22:49:01.002 
Epoch 967/1000 
	 loss: 17.1346, MinusLogProbMetric: 17.1346, val_loss: 17.2703, val_MinusLogProbMetric: 17.2703

Epoch 967: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1346 - MinusLogProbMetric: 17.1346 - val_loss: 17.2703 - val_MinusLogProbMetric: 17.2703 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 968/1000
2023-09-27 22:50:05.750 
Epoch 968/1000 
	 loss: 17.1310, MinusLogProbMetric: 17.1310, val_loss: 17.2632, val_MinusLogProbMetric: 17.2632

Epoch 968: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1310 - MinusLogProbMetric: 17.1310 - val_loss: 17.2632 - val_MinusLogProbMetric: 17.2632 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 969/1000
2023-09-27 22:51:10.549 
Epoch 969/1000 
	 loss: 17.1231, MinusLogProbMetric: 17.1231, val_loss: 17.3242, val_MinusLogProbMetric: 17.3242

Epoch 969: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1231 - MinusLogProbMetric: 17.1231 - val_loss: 17.3242 - val_MinusLogProbMetric: 17.3242 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 970/1000
2023-09-27 22:52:15.117 
Epoch 970/1000 
	 loss: 17.1253, MinusLogProbMetric: 17.1253, val_loss: 17.2788, val_MinusLogProbMetric: 17.2788

Epoch 970: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1253 - MinusLogProbMetric: 17.1253 - val_loss: 17.2788 - val_MinusLogProbMetric: 17.2788 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 971/1000
2023-09-27 22:53:20.353 
Epoch 971/1000 
	 loss: 17.1327, MinusLogProbMetric: 17.1327, val_loss: 17.2602, val_MinusLogProbMetric: 17.2602

Epoch 971: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1327 - MinusLogProbMetric: 17.1327 - val_loss: 17.2602 - val_MinusLogProbMetric: 17.2602 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 972/1000
2023-09-27 22:54:25.480 
Epoch 972/1000 
	 loss: 17.1299, MinusLogProbMetric: 17.1299, val_loss: 17.2779, val_MinusLogProbMetric: 17.2779

Epoch 972: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1299 - MinusLogProbMetric: 17.1299 - val_loss: 17.2779 - val_MinusLogProbMetric: 17.2779 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 973/1000
2023-09-27 22:55:30.821 
Epoch 973/1000 
	 loss: 17.1253, MinusLogProbMetric: 17.1253, val_loss: 17.3146, val_MinusLogProbMetric: 17.3146

Epoch 973: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1253 - MinusLogProbMetric: 17.1253 - val_loss: 17.3146 - val_MinusLogProbMetric: 17.3146 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 974/1000
2023-09-27 22:56:35.860 
Epoch 974/1000 
	 loss: 17.1264, MinusLogProbMetric: 17.1264, val_loss: 17.2938, val_MinusLogProbMetric: 17.2938

Epoch 974: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1264 - MinusLogProbMetric: 17.1264 - val_loss: 17.2938 - val_MinusLogProbMetric: 17.2938 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 975/1000
2023-09-27 22:57:40.727 
Epoch 975/1000 
	 loss: 17.1232, MinusLogProbMetric: 17.1232, val_loss: 17.2795, val_MinusLogProbMetric: 17.2795

Epoch 975: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1232 - MinusLogProbMetric: 17.1232 - val_loss: 17.2795 - val_MinusLogProbMetric: 17.2795 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 976/1000
2023-09-27 22:58:45.566 
Epoch 976/1000 
	 loss: 17.1331, MinusLogProbMetric: 17.1331, val_loss: 17.2699, val_MinusLogProbMetric: 17.2699

Epoch 976: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1331 - MinusLogProbMetric: 17.1331 - val_loss: 17.2699 - val_MinusLogProbMetric: 17.2699 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 977/1000
2023-09-27 22:59:50.864 
Epoch 977/1000 
	 loss: 17.1352, MinusLogProbMetric: 17.1352, val_loss: 17.2829, val_MinusLogProbMetric: 17.2829

Epoch 977: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1352 - MinusLogProbMetric: 17.1352 - val_loss: 17.2829 - val_MinusLogProbMetric: 17.2829 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 978/1000
2023-09-27 23:00:56.137 
Epoch 978/1000 
	 loss: 17.1263, MinusLogProbMetric: 17.1263, val_loss: 17.3845, val_MinusLogProbMetric: 17.3845

Epoch 978: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1263 - MinusLogProbMetric: 17.1263 - val_loss: 17.3845 - val_MinusLogProbMetric: 17.3845 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 979/1000
2023-09-27 23:02:01.516 
Epoch 979/1000 
	 loss: 17.1290, MinusLogProbMetric: 17.1290, val_loss: 17.2890, val_MinusLogProbMetric: 17.2890

Epoch 979: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1290 - MinusLogProbMetric: 17.1290 - val_loss: 17.2890 - val_MinusLogProbMetric: 17.2890 - lr: 4.1667e-05 - 65s/epoch - 334ms/step
Epoch 980/1000
2023-09-27 23:03:06.051 
Epoch 980/1000 
	 loss: 17.1307, MinusLogProbMetric: 17.1307, val_loss: 17.3359, val_MinusLogProbMetric: 17.3359

Epoch 980: val_loss did not improve from 17.25889
196/196 - 65s - loss: 17.1307 - MinusLogProbMetric: 17.1307 - val_loss: 17.3359 - val_MinusLogProbMetric: 17.3359 - lr: 4.1667e-05 - 65s/epoch - 329ms/step
Epoch 981/1000
2023-09-27 23:04:11.660 
Epoch 981/1000 
	 loss: 17.1233, MinusLogProbMetric: 17.1233, val_loss: 17.2502, val_MinusLogProbMetric: 17.2502

Epoch 981: val_loss improved from 17.25889 to 17.25016, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 17.1233 - MinusLogProbMetric: 17.1233 - val_loss: 17.2502 - val_MinusLogProbMetric: 17.2502 - lr: 4.1667e-05 - 66s/epoch - 339ms/step
Epoch 982/1000
2023-09-27 23:05:17.409 
Epoch 982/1000 
	 loss: 17.1197, MinusLogProbMetric: 17.1197, val_loss: 17.2738, val_MinusLogProbMetric: 17.2738

Epoch 982: val_loss did not improve from 17.25016
196/196 - 65s - loss: 17.1197 - MinusLogProbMetric: 17.1197 - val_loss: 17.2738 - val_MinusLogProbMetric: 17.2738 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 983/1000
2023-09-27 23:06:23.123 
Epoch 983/1000 
	 loss: 17.1203, MinusLogProbMetric: 17.1203, val_loss: 17.2939, val_MinusLogProbMetric: 17.2939

Epoch 983: val_loss did not improve from 17.25016
196/196 - 66s - loss: 17.1203 - MinusLogProbMetric: 17.1203 - val_loss: 17.2939 - val_MinusLogProbMetric: 17.2939 - lr: 4.1667e-05 - 66s/epoch - 335ms/step
Epoch 984/1000
2023-09-27 23:07:27.798 
Epoch 984/1000 
	 loss: 17.1323, MinusLogProbMetric: 17.1323, val_loss: 17.2932, val_MinusLogProbMetric: 17.2932

Epoch 984: val_loss did not improve from 17.25016
196/196 - 65s - loss: 17.1323 - MinusLogProbMetric: 17.1323 - val_loss: 17.2932 - val_MinusLogProbMetric: 17.2932 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 985/1000
2023-09-27 23:08:32.508 
Epoch 985/1000 
	 loss: 17.1229, MinusLogProbMetric: 17.1229, val_loss: 17.2900, val_MinusLogProbMetric: 17.2900

Epoch 985: val_loss did not improve from 17.25016
196/196 - 65s - loss: 17.1229 - MinusLogProbMetric: 17.1229 - val_loss: 17.2900 - val_MinusLogProbMetric: 17.2900 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 986/1000
2023-09-27 23:09:37.572 
Epoch 986/1000 
	 loss: 17.1303, MinusLogProbMetric: 17.1303, val_loss: 17.3249, val_MinusLogProbMetric: 17.3249

Epoch 986: val_loss did not improve from 17.25016
196/196 - 65s - loss: 17.1303 - MinusLogProbMetric: 17.1303 - val_loss: 17.3249 - val_MinusLogProbMetric: 17.3249 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 987/1000
2023-09-27 23:10:42.987 
Epoch 987/1000 
	 loss: 17.1321, MinusLogProbMetric: 17.1321, val_loss: 17.2475, val_MinusLogProbMetric: 17.2475

Epoch 987: val_loss improved from 17.25016 to 17.24747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_301/weights/best_weights.h5
196/196 - 66s - loss: 17.1321 - MinusLogProbMetric: 17.1321 - val_loss: 17.2475 - val_MinusLogProbMetric: 17.2475 - lr: 4.1667e-05 - 66s/epoch - 338ms/step
Epoch 988/1000
2023-09-27 23:11:49.680 
Epoch 988/1000 
	 loss: 17.1244, MinusLogProbMetric: 17.1244, val_loss: 17.3241, val_MinusLogProbMetric: 17.3241

Epoch 988: val_loss did not improve from 17.24747
196/196 - 66s - loss: 17.1244 - MinusLogProbMetric: 17.1244 - val_loss: 17.3241 - val_MinusLogProbMetric: 17.3241 - lr: 4.1667e-05 - 66s/epoch - 336ms/step
Epoch 989/1000
2023-09-27 23:12:54.596 
Epoch 989/1000 
	 loss: 17.1261, MinusLogProbMetric: 17.1261, val_loss: 17.3155, val_MinusLogProbMetric: 17.3155

Epoch 989: val_loss did not improve from 17.24747
196/196 - 65s - loss: 17.1261 - MinusLogProbMetric: 17.1261 - val_loss: 17.3155 - val_MinusLogProbMetric: 17.3155 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 990/1000
2023-09-27 23:13:59.399 
Epoch 990/1000 
	 loss: 17.1246, MinusLogProbMetric: 17.1246, val_loss: 17.2940, val_MinusLogProbMetric: 17.2940

Epoch 990: val_loss did not improve from 17.24747
196/196 - 65s - loss: 17.1246 - MinusLogProbMetric: 17.1246 - val_loss: 17.2940 - val_MinusLogProbMetric: 17.2940 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 991/1000
2023-09-27 23:15:04.537 
Epoch 991/1000 
	 loss: 17.1337, MinusLogProbMetric: 17.1337, val_loss: 17.3004, val_MinusLogProbMetric: 17.3004

Epoch 991: val_loss did not improve from 17.24747
196/196 - 65s - loss: 17.1337 - MinusLogProbMetric: 17.1337 - val_loss: 17.3004 - val_MinusLogProbMetric: 17.3004 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 992/1000
2023-09-27 23:16:09.827 
Epoch 992/1000 
	 loss: 17.1177, MinusLogProbMetric: 17.1177, val_loss: 17.2995, val_MinusLogProbMetric: 17.2995

Epoch 992: val_loss did not improve from 17.24747
196/196 - 65s - loss: 17.1177 - MinusLogProbMetric: 17.1177 - val_loss: 17.2995 - val_MinusLogProbMetric: 17.2995 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 993/1000
2023-09-27 23:17:14.456 
Epoch 993/1000 
	 loss: 17.1255, MinusLogProbMetric: 17.1255, val_loss: 17.2819, val_MinusLogProbMetric: 17.2819

Epoch 993: val_loss did not improve from 17.24747
196/196 - 65s - loss: 17.1255 - MinusLogProbMetric: 17.1255 - val_loss: 17.2819 - val_MinusLogProbMetric: 17.2819 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 994/1000
2023-09-27 23:18:19.375 
Epoch 994/1000 
	 loss: 17.1281, MinusLogProbMetric: 17.1281, val_loss: 17.2631, val_MinusLogProbMetric: 17.2631

Epoch 994: val_loss did not improve from 17.24747
196/196 - 65s - loss: 17.1281 - MinusLogProbMetric: 17.1281 - val_loss: 17.2631 - val_MinusLogProbMetric: 17.2631 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 995/1000
2023-09-27 23:19:24.022 
Epoch 995/1000 
	 loss: 17.1271, MinusLogProbMetric: 17.1271, val_loss: 17.2947, val_MinusLogProbMetric: 17.2947

Epoch 995: val_loss did not improve from 17.24747
196/196 - 65s - loss: 17.1271 - MinusLogProbMetric: 17.1271 - val_loss: 17.2947 - val_MinusLogProbMetric: 17.2947 - lr: 4.1667e-05 - 65s/epoch - 330ms/step
Epoch 996/1000
2023-09-27 23:20:29.170 
Epoch 996/1000 
	 loss: 17.1164, MinusLogProbMetric: 17.1164, val_loss: 17.2656, val_MinusLogProbMetric: 17.2656

Epoch 996: val_loss did not improve from 17.24747
196/196 - 65s - loss: 17.1164 - MinusLogProbMetric: 17.1164 - val_loss: 17.2656 - val_MinusLogProbMetric: 17.2656 - lr: 4.1667e-05 - 65s/epoch - 332ms/step
Epoch 997/1000
2023-09-27 23:21:34.363 
Epoch 997/1000 
	 loss: 17.1177, MinusLogProbMetric: 17.1177, val_loss: 17.2657, val_MinusLogProbMetric: 17.2657

Epoch 997: val_loss did not improve from 17.24747
196/196 - 65s - loss: 17.1177 - MinusLogProbMetric: 17.1177 - val_loss: 17.2657 - val_MinusLogProbMetric: 17.2657 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 998/1000
2023-09-27 23:22:39.209 
Epoch 998/1000 
	 loss: 17.1214, MinusLogProbMetric: 17.1214, val_loss: 17.2595, val_MinusLogProbMetric: 17.2595

Epoch 998: val_loss did not improve from 17.24747
196/196 - 65s - loss: 17.1214 - MinusLogProbMetric: 17.1214 - val_loss: 17.2595 - val_MinusLogProbMetric: 17.2595 - lr: 4.1667e-05 - 65s/epoch - 331ms/step
Epoch 999/1000
2023-09-27 23:23:44.395 
Epoch 999/1000 
	 loss: 17.1241, MinusLogProbMetric: 17.1241, val_loss: 17.2653, val_MinusLogProbMetric: 17.2653

Epoch 999: val_loss did not improve from 17.24747
196/196 - 65s - loss: 17.1241 - MinusLogProbMetric: 17.1241 - val_loss: 17.2653 - val_MinusLogProbMetric: 17.2653 - lr: 4.1667e-05 - 65s/epoch - 333ms/step
Epoch 1000/1000
2023-09-27 23:24:48.845 
Epoch 1000/1000 
	 loss: 17.1209, MinusLogProbMetric: 17.1209, val_loss: 17.3067, val_MinusLogProbMetric: 17.3067

Epoch 1000: val_loss did not improve from 17.24747
196/196 - 64s - loss: 17.1209 - MinusLogProbMetric: 17.1209 - val_loss: 17.3067 - val_MinusLogProbMetric: 17.3067 - lr: 4.1667e-05 - 64s/epoch - 329ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 25.091323425993323 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 12.879922762978822 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 10.497353239974473 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 13.53488470800221 seconds.
Training succeeded with seed 869.
Model trained in 64188.76 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 63.79 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 64.05 s.
===========
Run 301/720 done in 64417.47 s.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

===========
Generating train data for run 316.
===========
Train data generated in 0.15 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_316/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_316/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_316/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_316
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_66"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_67 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  1399280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,399,280
Trainable params: 1,399,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f8e8423fd30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f95dd20e770>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f95dd20e770>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8e7c5f4550>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f96225b9900>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f96225b9e70>, <keras.callbacks.ModelCheckpoint object at 0x7f96225b9f30>, <keras.callbacks.EarlyStopping object at 0x7f96225ba1a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f96225ba1d0>, <keras.callbacks.TerminateOnNaN object at 0x7f96225b9e10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_316/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 316/720 with hyperparameters:
timestamp = 2023-09-27 23:25:58.445280
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1399280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
2023-09-27 23:27:59.019 
Epoch 1/1000 
	 loss: 60.6684, MinusLogProbMetric: 60.6684, val_loss: 25.6067, val_MinusLogProbMetric: 25.6067

Epoch 1: val_loss improved from inf to 25.60668, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 121s - loss: 60.6684 - MinusLogProbMetric: 60.6684 - val_loss: 25.6067 - val_MinusLogProbMetric: 25.6067 - lr: 0.0010 - 121s/epoch - 618ms/step
Epoch 2/1000
2023-09-27 23:28:42.307 
Epoch 2/1000 
	 loss: 23.6364, MinusLogProbMetric: 23.6364, val_loss: 22.8829, val_MinusLogProbMetric: 22.8829

Epoch 2: val_loss improved from 25.60668 to 22.88287, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 23.6364 - MinusLogProbMetric: 23.6364 - val_loss: 22.8829 - val_MinusLogProbMetric: 22.8829 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 3/1000
2023-09-27 23:29:24.924 
Epoch 3/1000 
	 loss: 21.8038, MinusLogProbMetric: 21.8038, val_loss: 22.5240, val_MinusLogProbMetric: 22.5240

Epoch 3: val_loss improved from 22.88287 to 22.52399, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 21.8038 - MinusLogProbMetric: 21.8038 - val_loss: 22.5240 - val_MinusLogProbMetric: 22.5240 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 4/1000
2023-09-27 23:30:08.039 
Epoch 4/1000 
	 loss: 21.2780, MinusLogProbMetric: 21.2780, val_loss: 21.8044, val_MinusLogProbMetric: 21.8044

Epoch 4: val_loss improved from 22.52399 to 21.80436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 21.2780 - MinusLogProbMetric: 21.2780 - val_loss: 21.8044 - val_MinusLogProbMetric: 21.8044 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 5/1000
2023-09-27 23:30:51.110 
Epoch 5/1000 
	 loss: 20.6731, MinusLogProbMetric: 20.6731, val_loss: 20.2376, val_MinusLogProbMetric: 20.2376

Epoch 5: val_loss improved from 21.80436 to 20.23761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 20.6731 - MinusLogProbMetric: 20.6731 - val_loss: 20.2376 - val_MinusLogProbMetric: 20.2376 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 6/1000
2023-09-27 23:31:34.268 
Epoch 6/1000 
	 loss: 20.3579, MinusLogProbMetric: 20.3579, val_loss: 20.5979, val_MinusLogProbMetric: 20.5979

Epoch 6: val_loss did not improve from 20.23761
196/196 - 42s - loss: 20.3579 - MinusLogProbMetric: 20.3579 - val_loss: 20.5979 - val_MinusLogProbMetric: 20.5979 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 7/1000
2023-09-27 23:32:16.229 
Epoch 7/1000 
	 loss: 20.3625, MinusLogProbMetric: 20.3625, val_loss: 20.4371, val_MinusLogProbMetric: 20.4371

Epoch 7: val_loss did not improve from 20.23761
196/196 - 42s - loss: 20.3625 - MinusLogProbMetric: 20.3625 - val_loss: 20.4371 - val_MinusLogProbMetric: 20.4371 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 8/1000
2023-09-27 23:32:58.839 
Epoch 8/1000 
	 loss: 19.9143, MinusLogProbMetric: 19.9143, val_loss: 19.7392, val_MinusLogProbMetric: 19.7392

Epoch 8: val_loss improved from 20.23761 to 19.73923, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 19.9143 - MinusLogProbMetric: 19.9143 - val_loss: 19.7392 - val_MinusLogProbMetric: 19.7392 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 9/1000
2023-09-27 23:33:41.619 
Epoch 9/1000 
	 loss: 19.5888, MinusLogProbMetric: 19.5888, val_loss: 19.6075, val_MinusLogProbMetric: 19.6075

Epoch 9: val_loss improved from 19.73923 to 19.60747, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 19.5888 - MinusLogProbMetric: 19.5888 - val_loss: 19.6075 - val_MinusLogProbMetric: 19.6075 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 10/1000
2023-09-27 23:34:24.543 
Epoch 10/1000 
	 loss: 19.6769, MinusLogProbMetric: 19.6769, val_loss: 19.0532, val_MinusLogProbMetric: 19.0532

Epoch 10: val_loss improved from 19.60747 to 19.05324, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 19.6769 - MinusLogProbMetric: 19.6769 - val_loss: 19.0532 - val_MinusLogProbMetric: 19.0532 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 11/1000
2023-09-27 23:35:07.225 
Epoch 11/1000 
	 loss: 19.3790, MinusLogProbMetric: 19.3790, val_loss: 19.0152, val_MinusLogProbMetric: 19.0152

Epoch 11: val_loss improved from 19.05324 to 19.01519, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 19.3790 - MinusLogProbMetric: 19.3790 - val_loss: 19.0152 - val_MinusLogProbMetric: 19.0152 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 12/1000
2023-09-27 23:35:49.884 
Epoch 12/1000 
	 loss: 19.3176, MinusLogProbMetric: 19.3176, val_loss: 19.7573, val_MinusLogProbMetric: 19.7573

Epoch 12: val_loss did not improve from 19.01519
196/196 - 42s - loss: 19.3176 - MinusLogProbMetric: 19.3176 - val_loss: 19.7573 - val_MinusLogProbMetric: 19.7573 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 13/1000
2023-09-27 23:36:32.246 
Epoch 13/1000 
	 loss: 19.1756, MinusLogProbMetric: 19.1756, val_loss: 18.9974, val_MinusLogProbMetric: 18.9974

Epoch 13: val_loss improved from 19.01519 to 18.99740, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 19.1756 - MinusLogProbMetric: 19.1756 - val_loss: 18.9974 - val_MinusLogProbMetric: 18.9974 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 14/1000
2023-09-27 23:37:15.009 
Epoch 14/1000 
	 loss: 19.2162, MinusLogProbMetric: 19.2162, val_loss: 19.6323, val_MinusLogProbMetric: 19.6323

Epoch 14: val_loss did not improve from 18.99740
196/196 - 42s - loss: 19.2162 - MinusLogProbMetric: 19.2162 - val_loss: 19.6323 - val_MinusLogProbMetric: 19.6323 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 15/1000
2023-09-27 23:37:56.904 
Epoch 15/1000 
	 loss: 19.1013, MinusLogProbMetric: 19.1013, val_loss: 18.7471, val_MinusLogProbMetric: 18.7471

Epoch 15: val_loss improved from 18.99740 to 18.74712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 19.1013 - MinusLogProbMetric: 19.1013 - val_loss: 18.7471 - val_MinusLogProbMetric: 18.7471 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 16/1000
2023-09-27 23:38:39.573 
Epoch 16/1000 
	 loss: 19.2772, MinusLogProbMetric: 19.2772, val_loss: 19.4359, val_MinusLogProbMetric: 19.4359

Epoch 16: val_loss did not improve from 18.74712
196/196 - 42s - loss: 19.2772 - MinusLogProbMetric: 19.2772 - val_loss: 19.4359 - val_MinusLogProbMetric: 19.4359 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 17/1000
2023-09-27 23:39:21.578 
Epoch 17/1000 
	 loss: 18.8931, MinusLogProbMetric: 18.8931, val_loss: 18.9492, val_MinusLogProbMetric: 18.9492

Epoch 17: val_loss did not improve from 18.74712
196/196 - 42s - loss: 18.8931 - MinusLogProbMetric: 18.8931 - val_loss: 18.9492 - val_MinusLogProbMetric: 18.9492 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 18/1000
2023-09-27 23:40:03.706 
Epoch 18/1000 
	 loss: 18.9291, MinusLogProbMetric: 18.9291, val_loss: 18.8378, val_MinusLogProbMetric: 18.8378

Epoch 18: val_loss did not improve from 18.74712
196/196 - 42s - loss: 18.9291 - MinusLogProbMetric: 18.9291 - val_loss: 18.8378 - val_MinusLogProbMetric: 18.8378 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 19/1000
2023-09-27 23:40:46.103 
Epoch 19/1000 
	 loss: 18.7259, MinusLogProbMetric: 18.7259, val_loss: 18.4575, val_MinusLogProbMetric: 18.4575

Epoch 19: val_loss improved from 18.74712 to 18.45750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 18.7259 - MinusLogProbMetric: 18.7259 - val_loss: 18.4575 - val_MinusLogProbMetric: 18.4575 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 20/1000
2023-09-27 23:41:29.031 
Epoch 20/1000 
	 loss: 18.6620, MinusLogProbMetric: 18.6620, val_loss: 19.6049, val_MinusLogProbMetric: 19.6049

Epoch 20: val_loss did not improve from 18.45750
196/196 - 42s - loss: 18.6620 - MinusLogProbMetric: 18.6620 - val_loss: 19.6049 - val_MinusLogProbMetric: 19.6049 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 21/1000
2023-09-27 23:42:11.260 
Epoch 21/1000 
	 loss: 18.6321, MinusLogProbMetric: 18.6321, val_loss: 18.4929, val_MinusLogProbMetric: 18.4929

Epoch 21: val_loss did not improve from 18.45750
196/196 - 42s - loss: 18.6321 - MinusLogProbMetric: 18.6321 - val_loss: 18.4929 - val_MinusLogProbMetric: 18.4929 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 22/1000
2023-09-27 23:42:53.246 
Epoch 22/1000 
	 loss: 18.5560, MinusLogProbMetric: 18.5560, val_loss: 19.6677, val_MinusLogProbMetric: 19.6677

Epoch 22: val_loss did not improve from 18.45750
196/196 - 42s - loss: 18.5560 - MinusLogProbMetric: 18.5560 - val_loss: 19.6677 - val_MinusLogProbMetric: 19.6677 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 23/1000
2023-09-27 23:43:35.921 
Epoch 23/1000 
	 loss: 18.5826, MinusLogProbMetric: 18.5826, val_loss: 18.7089, val_MinusLogProbMetric: 18.7089

Epoch 23: val_loss did not improve from 18.45750
196/196 - 43s - loss: 18.5826 - MinusLogProbMetric: 18.5826 - val_loss: 18.7089 - val_MinusLogProbMetric: 18.7089 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 24/1000
2023-09-27 23:44:18.171 
Epoch 24/1000 
	 loss: 18.6252, MinusLogProbMetric: 18.6252, val_loss: 18.3796, val_MinusLogProbMetric: 18.3796

Epoch 24: val_loss improved from 18.45750 to 18.37962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 18.6252 - MinusLogProbMetric: 18.6252 - val_loss: 18.3796 - val_MinusLogProbMetric: 18.3796 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 25/1000
2023-09-27 23:45:01.268 
Epoch 25/1000 
	 loss: 18.4050, MinusLogProbMetric: 18.4050, val_loss: 18.7965, val_MinusLogProbMetric: 18.7965

Epoch 25: val_loss did not improve from 18.37962
196/196 - 42s - loss: 18.4050 - MinusLogProbMetric: 18.4050 - val_loss: 18.7965 - val_MinusLogProbMetric: 18.7965 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 26/1000
2023-09-27 23:45:43.418 
Epoch 26/1000 
	 loss: 18.5112, MinusLogProbMetric: 18.5112, val_loss: 18.2720, val_MinusLogProbMetric: 18.2720

Epoch 26: val_loss improved from 18.37962 to 18.27201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 18.5112 - MinusLogProbMetric: 18.5112 - val_loss: 18.2720 - val_MinusLogProbMetric: 18.2720 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 27/1000
2023-09-27 23:46:26.483 
Epoch 27/1000 
	 loss: 18.2404, MinusLogProbMetric: 18.2404, val_loss: 18.3125, val_MinusLogProbMetric: 18.3125

Epoch 27: val_loss did not improve from 18.27201
196/196 - 42s - loss: 18.2404 - MinusLogProbMetric: 18.2404 - val_loss: 18.3125 - val_MinusLogProbMetric: 18.3125 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 28/1000
2023-09-27 23:47:08.734 
Epoch 28/1000 
	 loss: 18.4169, MinusLogProbMetric: 18.4169, val_loss: 19.3199, val_MinusLogProbMetric: 19.3199

Epoch 28: val_loss did not improve from 18.27201
196/196 - 42s - loss: 18.4169 - MinusLogProbMetric: 18.4169 - val_loss: 19.3199 - val_MinusLogProbMetric: 19.3199 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 29/1000
2023-09-27 23:47:50.947 
Epoch 29/1000 
	 loss: 18.4055, MinusLogProbMetric: 18.4055, val_loss: 18.7102, val_MinusLogProbMetric: 18.7102

Epoch 29: val_loss did not improve from 18.27201
196/196 - 42s - loss: 18.4055 - MinusLogProbMetric: 18.4055 - val_loss: 18.7102 - val_MinusLogProbMetric: 18.7102 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 30/1000
2023-09-27 23:48:32.836 
Epoch 30/1000 
	 loss: 18.3514, MinusLogProbMetric: 18.3514, val_loss: 18.2101, val_MinusLogProbMetric: 18.2101

Epoch 30: val_loss improved from 18.27201 to 18.21006, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 18.3514 - MinusLogProbMetric: 18.3514 - val_loss: 18.2101 - val_MinusLogProbMetric: 18.2101 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 31/1000
2023-09-27 23:49:15.711 
Epoch 31/1000 
	 loss: 18.1434, MinusLogProbMetric: 18.1434, val_loss: 18.5669, val_MinusLogProbMetric: 18.5669

Epoch 31: val_loss did not improve from 18.21006
196/196 - 42s - loss: 18.1434 - MinusLogProbMetric: 18.1434 - val_loss: 18.5669 - val_MinusLogProbMetric: 18.5669 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 32/1000
2023-09-27 23:49:57.775 
Epoch 32/1000 
	 loss: 18.1079, MinusLogProbMetric: 18.1079, val_loss: 18.5394, val_MinusLogProbMetric: 18.5394

Epoch 32: val_loss did not improve from 18.21006
196/196 - 42s - loss: 18.1079 - MinusLogProbMetric: 18.1079 - val_loss: 18.5394 - val_MinusLogProbMetric: 18.5394 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 33/1000
2023-09-27 23:50:40.247 
Epoch 33/1000 
	 loss: 18.0595, MinusLogProbMetric: 18.0595, val_loss: 18.7508, val_MinusLogProbMetric: 18.7508

Epoch 33: val_loss did not improve from 18.21006
196/196 - 42s - loss: 18.0595 - MinusLogProbMetric: 18.0595 - val_loss: 18.7508 - val_MinusLogProbMetric: 18.7508 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 34/1000
2023-09-27 23:51:22.351 
Epoch 34/1000 
	 loss: 18.1162, MinusLogProbMetric: 18.1162, val_loss: 19.1494, val_MinusLogProbMetric: 19.1494

Epoch 34: val_loss did not improve from 18.21006
196/196 - 42s - loss: 18.1162 - MinusLogProbMetric: 18.1162 - val_loss: 19.1494 - val_MinusLogProbMetric: 19.1494 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 35/1000
2023-09-27 23:52:04.678 
Epoch 35/1000 
	 loss: 18.1924, MinusLogProbMetric: 18.1924, val_loss: 19.5983, val_MinusLogProbMetric: 19.5983

Epoch 35: val_loss did not improve from 18.21006
196/196 - 42s - loss: 18.1924 - MinusLogProbMetric: 18.1924 - val_loss: 19.5983 - val_MinusLogProbMetric: 19.5983 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 36/1000
2023-09-27 23:52:47.021 
Epoch 36/1000 
	 loss: 18.3346, MinusLogProbMetric: 18.3346, val_loss: 18.5449, val_MinusLogProbMetric: 18.5449

Epoch 36: val_loss did not improve from 18.21006
196/196 - 42s - loss: 18.3346 - MinusLogProbMetric: 18.3346 - val_loss: 18.5449 - val_MinusLogProbMetric: 18.5449 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 37/1000
2023-09-27 23:53:29.355 
Epoch 37/1000 
	 loss: 18.1229, MinusLogProbMetric: 18.1229, val_loss: 18.1629, val_MinusLogProbMetric: 18.1629

Epoch 37: val_loss improved from 18.21006 to 18.16294, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 18.1229 - MinusLogProbMetric: 18.1229 - val_loss: 18.1629 - val_MinusLogProbMetric: 18.1629 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 38/1000
2023-09-27 23:54:12.154 
Epoch 38/1000 
	 loss: 17.9931, MinusLogProbMetric: 17.9931, val_loss: 18.1302, val_MinusLogProbMetric: 18.1302

Epoch 38: val_loss improved from 18.16294 to 18.13018, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 17.9931 - MinusLogProbMetric: 17.9931 - val_loss: 18.1302 - val_MinusLogProbMetric: 18.1302 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 39/1000
2023-09-27 23:54:55.247 
Epoch 39/1000 
	 loss: 17.8924, MinusLogProbMetric: 17.8924, val_loss: 18.8940, val_MinusLogProbMetric: 18.8940

Epoch 39: val_loss did not improve from 18.13018
196/196 - 42s - loss: 17.8924 - MinusLogProbMetric: 17.8924 - val_loss: 18.8940 - val_MinusLogProbMetric: 18.8940 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 40/1000
2023-09-27 23:55:37.239 
Epoch 40/1000 
	 loss: 17.9987, MinusLogProbMetric: 17.9987, val_loss: 18.0028, val_MinusLogProbMetric: 18.0028

Epoch 40: val_loss improved from 18.13018 to 18.00277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 17.9987 - MinusLogProbMetric: 17.9987 - val_loss: 18.0028 - val_MinusLogProbMetric: 18.0028 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 41/1000
2023-09-27 23:56:20.439 
Epoch 41/1000 
	 loss: 17.8349, MinusLogProbMetric: 17.8349, val_loss: 18.2253, val_MinusLogProbMetric: 18.2253

Epoch 41: val_loss did not improve from 18.00277
196/196 - 43s - loss: 17.8349 - MinusLogProbMetric: 17.8349 - val_loss: 18.2253 - val_MinusLogProbMetric: 18.2253 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 42/1000
2023-09-27 23:57:02.632 
Epoch 42/1000 
	 loss: 17.8858, MinusLogProbMetric: 17.8858, val_loss: 17.9100, val_MinusLogProbMetric: 17.9100

Epoch 42: val_loss improved from 18.00277 to 17.91000, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 17.8858 - MinusLogProbMetric: 17.8858 - val_loss: 17.9100 - val_MinusLogProbMetric: 17.9100 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 43/1000
2023-09-27 23:57:45.306 
Epoch 43/1000 
	 loss: 17.9487, MinusLogProbMetric: 17.9487, val_loss: 18.0979, val_MinusLogProbMetric: 18.0979

Epoch 43: val_loss did not improve from 17.91000
196/196 - 42s - loss: 17.9487 - MinusLogProbMetric: 17.9487 - val_loss: 18.0979 - val_MinusLogProbMetric: 18.0979 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 44/1000
2023-09-27 23:58:27.109 
Epoch 44/1000 
	 loss: 17.8749, MinusLogProbMetric: 17.8749, val_loss: 18.1280, val_MinusLogProbMetric: 18.1280

Epoch 44: val_loss did not improve from 17.91000
196/196 - 42s - loss: 17.8749 - MinusLogProbMetric: 17.8749 - val_loss: 18.1280 - val_MinusLogProbMetric: 18.1280 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 45/1000
2023-09-27 23:59:09.378 
Epoch 45/1000 
	 loss: 17.7910, MinusLogProbMetric: 17.7910, val_loss: 18.3282, val_MinusLogProbMetric: 18.3282

Epoch 45: val_loss did not improve from 17.91000
196/196 - 42s - loss: 17.7910 - MinusLogProbMetric: 17.7910 - val_loss: 18.3282 - val_MinusLogProbMetric: 18.3282 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 46/1000
2023-09-27 23:59:51.688 
Epoch 46/1000 
	 loss: 17.9600, MinusLogProbMetric: 17.9600, val_loss: 18.1820, val_MinusLogProbMetric: 18.1820

Epoch 46: val_loss did not improve from 17.91000
196/196 - 42s - loss: 17.9600 - MinusLogProbMetric: 17.9600 - val_loss: 18.1820 - val_MinusLogProbMetric: 18.1820 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 47/1000
2023-09-28 00:00:33.991 
Epoch 47/1000 
	 loss: 17.8407, MinusLogProbMetric: 17.8407, val_loss: 18.2577, val_MinusLogProbMetric: 18.2577

Epoch 47: val_loss did not improve from 17.91000
196/196 - 42s - loss: 17.8407 - MinusLogProbMetric: 17.8407 - val_loss: 18.2577 - val_MinusLogProbMetric: 18.2577 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 48/1000
2023-09-28 00:01:16.400 
Epoch 48/1000 
	 loss: 17.7685, MinusLogProbMetric: 17.7685, val_loss: 18.0804, val_MinusLogProbMetric: 18.0804

Epoch 48: val_loss did not improve from 17.91000
196/196 - 42s - loss: 17.7685 - MinusLogProbMetric: 17.7685 - val_loss: 18.0804 - val_MinusLogProbMetric: 18.0804 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 49/1000
2023-09-28 00:01:58.620 
Epoch 49/1000 
	 loss: 17.7462, MinusLogProbMetric: 17.7462, val_loss: 17.7345, val_MinusLogProbMetric: 17.7345

Epoch 49: val_loss improved from 17.91000 to 17.73449, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 17.7462 - MinusLogProbMetric: 17.7462 - val_loss: 17.7345 - val_MinusLogProbMetric: 17.7345 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 50/1000
2023-09-28 00:02:41.265 
Epoch 50/1000 
	 loss: 17.6995, MinusLogProbMetric: 17.6995, val_loss: 17.9686, val_MinusLogProbMetric: 17.9686

Epoch 50: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.6995 - MinusLogProbMetric: 17.6995 - val_loss: 17.9686 - val_MinusLogProbMetric: 17.9686 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 51/1000
2023-09-28 00:03:23.702 
Epoch 51/1000 
	 loss: 17.7430, MinusLogProbMetric: 17.7430, val_loss: 17.8954, val_MinusLogProbMetric: 17.8954

Epoch 51: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.7430 - MinusLogProbMetric: 17.7430 - val_loss: 17.8954 - val_MinusLogProbMetric: 17.8954 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 52/1000
2023-09-28 00:04:06.091 
Epoch 52/1000 
	 loss: 17.7888, MinusLogProbMetric: 17.7888, val_loss: 18.3761, val_MinusLogProbMetric: 18.3761

Epoch 52: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.7888 - MinusLogProbMetric: 17.7888 - val_loss: 18.3761 - val_MinusLogProbMetric: 18.3761 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 53/1000
2023-09-28 00:04:48.283 
Epoch 53/1000 
	 loss: 17.7705, MinusLogProbMetric: 17.7705, val_loss: 21.1591, val_MinusLogProbMetric: 21.1591

Epoch 53: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.7705 - MinusLogProbMetric: 17.7705 - val_loss: 21.1591 - val_MinusLogProbMetric: 21.1591 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 54/1000
2023-09-28 00:05:30.673 
Epoch 54/1000 
	 loss: 17.8919, MinusLogProbMetric: 17.8919, val_loss: 19.7255, val_MinusLogProbMetric: 19.7255

Epoch 54: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.8919 - MinusLogProbMetric: 17.8919 - val_loss: 19.7255 - val_MinusLogProbMetric: 19.7255 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 55/1000
2023-09-28 00:06:12.516 
Epoch 55/1000 
	 loss: 17.9322, MinusLogProbMetric: 17.9322, val_loss: 17.8742, val_MinusLogProbMetric: 17.8742

Epoch 55: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.9322 - MinusLogProbMetric: 17.9322 - val_loss: 17.8742 - val_MinusLogProbMetric: 17.8742 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 56/1000
2023-09-28 00:06:54.868 
Epoch 56/1000 
	 loss: 17.7439, MinusLogProbMetric: 17.7439, val_loss: 17.8437, val_MinusLogProbMetric: 17.8437

Epoch 56: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.7439 - MinusLogProbMetric: 17.7439 - val_loss: 17.8437 - val_MinusLogProbMetric: 17.8437 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 57/1000
2023-09-28 00:07:36.942 
Epoch 57/1000 
	 loss: 17.7364, MinusLogProbMetric: 17.7364, val_loss: 18.1770, val_MinusLogProbMetric: 18.1770

Epoch 57: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.7364 - MinusLogProbMetric: 17.7364 - val_loss: 18.1770 - val_MinusLogProbMetric: 18.1770 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 58/1000
2023-09-28 00:08:18.968 
Epoch 58/1000 
	 loss: 17.6741, MinusLogProbMetric: 17.6741, val_loss: 17.8997, val_MinusLogProbMetric: 17.8997

Epoch 58: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.6741 - MinusLogProbMetric: 17.6741 - val_loss: 17.8997 - val_MinusLogProbMetric: 17.8997 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 59/1000
2023-09-28 00:09:01.422 
Epoch 59/1000 
	 loss: 17.7975, MinusLogProbMetric: 17.7975, val_loss: 18.6165, val_MinusLogProbMetric: 18.6165

Epoch 59: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.7975 - MinusLogProbMetric: 17.7975 - val_loss: 18.6165 - val_MinusLogProbMetric: 18.6165 - lr: 0.0010 - 42s/epoch - 217ms/step
Epoch 60/1000
2023-09-28 00:09:43.535 
Epoch 60/1000 
	 loss: 17.5446, MinusLogProbMetric: 17.5446, val_loss: 18.0702, val_MinusLogProbMetric: 18.0702

Epoch 60: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.5446 - MinusLogProbMetric: 17.5446 - val_loss: 18.0702 - val_MinusLogProbMetric: 18.0702 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 61/1000
2023-09-28 00:10:25.687 
Epoch 61/1000 
	 loss: 17.6074, MinusLogProbMetric: 17.6074, val_loss: 17.8776, val_MinusLogProbMetric: 17.8776

Epoch 61: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.6074 - MinusLogProbMetric: 17.6074 - val_loss: 17.8776 - val_MinusLogProbMetric: 17.8776 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 62/1000
2023-09-28 00:11:07.701 
Epoch 62/1000 
	 loss: 17.5518, MinusLogProbMetric: 17.5518, val_loss: 17.9639, val_MinusLogProbMetric: 17.9639

Epoch 62: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.5518 - MinusLogProbMetric: 17.5518 - val_loss: 17.9639 - val_MinusLogProbMetric: 17.9639 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 63/1000
2023-09-28 00:11:49.874 
Epoch 63/1000 
	 loss: 17.5505, MinusLogProbMetric: 17.5505, val_loss: 17.8931, val_MinusLogProbMetric: 17.8931

Epoch 63: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.5505 - MinusLogProbMetric: 17.5505 - val_loss: 17.8931 - val_MinusLogProbMetric: 17.8931 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 64/1000
2023-09-28 00:12:32.082 
Epoch 64/1000 
	 loss: 17.5545, MinusLogProbMetric: 17.5545, val_loss: 18.3297, val_MinusLogProbMetric: 18.3297

Epoch 64: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.5545 - MinusLogProbMetric: 17.5545 - val_loss: 18.3297 - val_MinusLogProbMetric: 18.3297 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 65/1000
2023-09-28 00:13:14.058 
Epoch 65/1000 
	 loss: 17.6048, MinusLogProbMetric: 17.6048, val_loss: 18.0823, val_MinusLogProbMetric: 18.0823

Epoch 65: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.6048 - MinusLogProbMetric: 17.6048 - val_loss: 18.0823 - val_MinusLogProbMetric: 18.0823 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 66/1000
2023-09-28 00:13:55.810 
Epoch 66/1000 
	 loss: 17.4867, MinusLogProbMetric: 17.4867, val_loss: 17.9450, val_MinusLogProbMetric: 17.9450

Epoch 66: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.4867 - MinusLogProbMetric: 17.4867 - val_loss: 17.9450 - val_MinusLogProbMetric: 17.9450 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 67/1000
2023-09-28 00:14:37.895 
Epoch 67/1000 
	 loss: 17.5471, MinusLogProbMetric: 17.5471, val_loss: 18.4391, val_MinusLogProbMetric: 18.4391

Epoch 67: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.5471 - MinusLogProbMetric: 17.5471 - val_loss: 18.4391 - val_MinusLogProbMetric: 18.4391 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 68/1000
2023-09-28 00:15:19.678 
Epoch 68/1000 
	 loss: 17.4119, MinusLogProbMetric: 17.4119, val_loss: 17.8787, val_MinusLogProbMetric: 17.8787

Epoch 68: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.4119 - MinusLogProbMetric: 17.4119 - val_loss: 17.8787 - val_MinusLogProbMetric: 17.8787 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 69/1000
2023-09-28 00:16:02.103 
Epoch 69/1000 
	 loss: 17.4644, MinusLogProbMetric: 17.4644, val_loss: 17.8506, val_MinusLogProbMetric: 17.8506

Epoch 69: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.4644 - MinusLogProbMetric: 17.4644 - val_loss: 17.8506 - val_MinusLogProbMetric: 17.8506 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 70/1000
2023-09-28 00:16:44.033 
Epoch 70/1000 
	 loss: 17.5553, MinusLogProbMetric: 17.5553, val_loss: 18.4756, val_MinusLogProbMetric: 18.4756

Epoch 70: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.5553 - MinusLogProbMetric: 17.5553 - val_loss: 18.4756 - val_MinusLogProbMetric: 18.4756 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 71/1000
2023-09-28 00:17:26.090 
Epoch 71/1000 
	 loss: 17.4016, MinusLogProbMetric: 17.4016, val_loss: 18.0433, val_MinusLogProbMetric: 18.0433

Epoch 71: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.4016 - MinusLogProbMetric: 17.4016 - val_loss: 18.0433 - val_MinusLogProbMetric: 18.0433 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 72/1000
2023-09-28 00:18:08.102 
Epoch 72/1000 
	 loss: 17.5586, MinusLogProbMetric: 17.5586, val_loss: 17.9744, val_MinusLogProbMetric: 17.9744

Epoch 72: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.5586 - MinusLogProbMetric: 17.5586 - val_loss: 17.9744 - val_MinusLogProbMetric: 17.9744 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 73/1000
2023-09-28 00:18:49.826 
Epoch 73/1000 
	 loss: 17.4185, MinusLogProbMetric: 17.4185, val_loss: 18.7706, val_MinusLogProbMetric: 18.7706

Epoch 73: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.4185 - MinusLogProbMetric: 17.4185 - val_loss: 18.7706 - val_MinusLogProbMetric: 18.7706 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 74/1000
2023-09-28 00:19:31.938 
Epoch 74/1000 
	 loss: 17.3523, MinusLogProbMetric: 17.3523, val_loss: 18.7051, val_MinusLogProbMetric: 18.7051

Epoch 74: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.3523 - MinusLogProbMetric: 17.3523 - val_loss: 18.7051 - val_MinusLogProbMetric: 18.7051 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 75/1000
2023-09-28 00:20:13.953 
Epoch 75/1000 
	 loss: 17.3791, MinusLogProbMetric: 17.3791, val_loss: 17.7825, val_MinusLogProbMetric: 17.7825

Epoch 75: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.3791 - MinusLogProbMetric: 17.3791 - val_loss: 17.7825 - val_MinusLogProbMetric: 17.7825 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 76/1000
2023-09-28 00:20:55.969 
Epoch 76/1000 
	 loss: 17.3354, MinusLogProbMetric: 17.3354, val_loss: 18.2350, val_MinusLogProbMetric: 18.2350

Epoch 76: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.3354 - MinusLogProbMetric: 17.3354 - val_loss: 18.2350 - val_MinusLogProbMetric: 18.2350 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 77/1000
2023-09-28 00:21:38.187 
Epoch 77/1000 
	 loss: 17.3781, MinusLogProbMetric: 17.3781, val_loss: 17.9089, val_MinusLogProbMetric: 17.9089

Epoch 77: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.3781 - MinusLogProbMetric: 17.3781 - val_loss: 17.9089 - val_MinusLogProbMetric: 17.9089 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 78/1000
2023-09-28 00:22:20.207 
Epoch 78/1000 
	 loss: 17.3746, MinusLogProbMetric: 17.3746, val_loss: 18.0524, val_MinusLogProbMetric: 18.0524

Epoch 78: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.3746 - MinusLogProbMetric: 17.3746 - val_loss: 18.0524 - val_MinusLogProbMetric: 18.0524 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 79/1000
2023-09-28 00:23:02.434 
Epoch 79/1000 
	 loss: 17.3281, MinusLogProbMetric: 17.3281, val_loss: 17.9113, val_MinusLogProbMetric: 17.9113

Epoch 79: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.3281 - MinusLogProbMetric: 17.3281 - val_loss: 17.9113 - val_MinusLogProbMetric: 17.9113 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 80/1000
2023-09-28 00:23:44.567 
Epoch 80/1000 
	 loss: 17.2153, MinusLogProbMetric: 17.2153, val_loss: 17.7514, val_MinusLogProbMetric: 17.7514

Epoch 80: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.2153 - MinusLogProbMetric: 17.2153 - val_loss: 17.7514 - val_MinusLogProbMetric: 17.7514 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 81/1000
2023-09-28 00:24:26.751 
Epoch 81/1000 
	 loss: 17.2908, MinusLogProbMetric: 17.2908, val_loss: 17.9951, val_MinusLogProbMetric: 17.9951

Epoch 81: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.2908 - MinusLogProbMetric: 17.2908 - val_loss: 17.9951 - val_MinusLogProbMetric: 17.9951 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 82/1000
2023-09-28 00:25:09.167 
Epoch 82/1000 
	 loss: 17.3440, MinusLogProbMetric: 17.3440, val_loss: 18.6863, val_MinusLogProbMetric: 18.6863

Epoch 82: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.3440 - MinusLogProbMetric: 17.3440 - val_loss: 18.6863 - val_MinusLogProbMetric: 18.6863 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 83/1000
2023-09-28 00:25:51.334 
Epoch 83/1000 
	 loss: 17.2379, MinusLogProbMetric: 17.2379, val_loss: 17.8305, val_MinusLogProbMetric: 17.8305

Epoch 83: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.2379 - MinusLogProbMetric: 17.2379 - val_loss: 17.8305 - val_MinusLogProbMetric: 17.8305 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 84/1000
2023-09-28 00:26:33.298 
Epoch 84/1000 
	 loss: 17.2937, MinusLogProbMetric: 17.2937, val_loss: 17.9079, val_MinusLogProbMetric: 17.9079

Epoch 84: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.2937 - MinusLogProbMetric: 17.2937 - val_loss: 17.9079 - val_MinusLogProbMetric: 17.9079 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 85/1000
2023-09-28 00:27:15.657 
Epoch 85/1000 
	 loss: 17.3640, MinusLogProbMetric: 17.3640, val_loss: 17.9558, val_MinusLogProbMetric: 17.9558

Epoch 85: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.3640 - MinusLogProbMetric: 17.3640 - val_loss: 17.9558 - val_MinusLogProbMetric: 17.9558 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 86/1000
2023-09-28 00:27:57.958 
Epoch 86/1000 
	 loss: 17.2759, MinusLogProbMetric: 17.2759, val_loss: 18.0284, val_MinusLogProbMetric: 18.0284

Epoch 86: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.2759 - MinusLogProbMetric: 17.2759 - val_loss: 18.0284 - val_MinusLogProbMetric: 18.0284 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 87/1000
2023-09-28 00:28:40.078 
Epoch 87/1000 
	 loss: 17.2194, MinusLogProbMetric: 17.2194, val_loss: 18.9685, val_MinusLogProbMetric: 18.9685

Epoch 87: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.2194 - MinusLogProbMetric: 17.2194 - val_loss: 18.9685 - val_MinusLogProbMetric: 18.9685 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 88/1000
2023-09-28 00:29:22.224 
Epoch 88/1000 
	 loss: 17.1729, MinusLogProbMetric: 17.1729, val_loss: 17.8241, val_MinusLogProbMetric: 17.8241

Epoch 88: val_loss did not improve from 17.73449
196/196 - 42s - loss: 17.1729 - MinusLogProbMetric: 17.1729 - val_loss: 17.8241 - val_MinusLogProbMetric: 17.8241 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 89/1000
2023-09-28 00:30:04.718 
Epoch 89/1000 
	 loss: 17.2477, MinusLogProbMetric: 17.2477, val_loss: 17.6841, val_MinusLogProbMetric: 17.6841

Epoch 89: val_loss improved from 17.73449 to 17.68413, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_316/weights/best_weights.h5
196/196 - 43s - loss: 17.2477 - MinusLogProbMetric: 17.2477 - val_loss: 17.6841 - val_MinusLogProbMetric: 17.6841 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 90/1000
2023-09-28 00:30:47.785 
Epoch 90/1000 
	 loss: 17.2527, MinusLogProbMetric: 17.2527, val_loss: 17.8480, val_MinusLogProbMetric: 17.8480

Epoch 90: val_loss did not improve from 17.68413
196/196 - 42s - loss: 17.2527 - MinusLogProbMetric: 17.2527 - val_loss: 17.8480 - val_MinusLogProbMetric: 17.8480 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 91/1000
2023-09-28 00:31:29.862 
Epoch 91/1000 
	 loss: 17.1957, MinusLogProbMetric: 17.1957, val_loss: 17.8869, val_MinusLogProbMetric: 17.8869

Epoch 91: val_loss did not improve from 17.68413
196/196 - 42s - loss: 17.1957 - MinusLogProbMetric: 17.1957 - val_loss: 17.8869 - val_MinusLogProbMetric: 17.8869 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 92/1000
2023-09-28 00:32:12.081 
Epoch 92/1000 
	 loss: 17.1149, MinusLogProbMetric: 17.1149, val_loss: 18.0136, val_MinusLogProbMetric: 18.0136

Epoch 92: val_loss did not improve from 17.68413
196/196 - 42s - loss: 17.1149 - MinusLogProbMetric: 17.1149 - val_loss: 18.0136 - val_MinusLogProbMetric: 18.0136 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 93/1000
2023-09-28 00:32:54.604 
Epoch 93/1000 
	 loss: 17.1159, MinusLogProbMetric: 17.1159, val_loss: 17.7475, val_MinusLogProbMetric: 17.7475

Epoch 93: val_loss did not improve from 17.68413
196/196 - 43s - loss: 17.1159 - MinusLogProbMetric: 17.1159 - val_loss: 17.7475 - val_MinusLogProbMetric: 17.7475 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 94/1000
2023-09-28 00:33:37.013 
Epoch 94/1000 
	 loss: 17.2255, MinusLogProbMetric: 17.2255, val_loss: 17.9042, val_MinusLogProbMetric: 17.9042

Epoch 94: val_loss did not improve from 17.68413
196/196 - 42s - loss: 17.2255 - MinusLogProbMetric: 17.2255 - val_loss: 17.9042 - val_MinusLogProbMetric: 17.9042 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 95/1000
2023-09-28 00:34:19.149 
Epoch 95/1000 
	 loss: 17.1427, MinusLogProbMetric: 17.1427, val_loss: 18.1453, val_MinusLogProbMetric: 18.1453

Epoch 95: val_loss did not improve from 17.68413
196/196 - 42s - loss: 17.1427 - MinusLogProbMetric: 17.1427 - val_loss: 18.1453 - val_MinusLogProbMetric: 18.1453 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 96/1000
2023-09-28 00:35:01.083 
Epoch 96/1000 
	 loss: 17.1449, MinusLogProbMetric: 17.1449, val_loss: 18.2884, val_MinusLogProbMetric: 18.2884

Epoch 96: val_loss did not improve from 17.68413
196/196 - 42s - loss: 17.1449 - MinusLogProbMetric: 17.1449 - val_loss: 18.2884 - val_MinusLogProbMetric: 18.2884 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 97/1000
2023-09-28 00:35:42.866 
Epoch 97/1000 
	 loss: 17.0839, MinusLogProbMetric: 17.0839, val_loss: 17.9238, val_MinusLogProbMetric: 17.9238

Epoch 97: val_loss did not improve from 17.68413
196/196 - 42s - loss: 17.0839 - MinusLogProbMetric: 17.0839 - val_loss: 17.9238 - val_MinusLogProbMetric: 17.9238 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 98/1000
2023-09-28 00:36:24.494 
Epoch 98/1000 
	 loss: 17.1132, MinusLogProbMetric: 17.1132, val_loss: 19.2726, val_MinusLogProbMetric: 19.2726

Epoch 98: val_loss did not improve from 17.68413
196/196 - 42s - loss: 17.1132 - MinusLogProbMetric: 17.1132 - val_loss: 19.2726 - val_MinusLogProbMetric: 19.2726 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 99/1000
2023-09-28 00:37:04.061 
Epoch 99/1000 
	 loss: 17.0776, MinusLogProbMetric: 17.0776, val_loss: 17.8965, val_MinusLogProbMetric: 17.8965

Epoch 99: val_loss did not improve from 17.68413
196/196 - 40s - loss: 17.0776 - MinusLogProbMetric: 17.0776 - val_loss: 17.8965 - val_MinusLogProbMetric: 17.8965 - lr: 0.0010 - 40s/epoch - 202ms/step
Epoch 100/1000
2023-09-28 00:37:39.648 
Epoch 100/1000 
	 loss: 17.0144, MinusLogProbMetric: 17.0144, val_loss: 17.8640, val_MinusLogProbMetric: 17.8640

Epoch 100: val_loss did not improve from 17.68413
196/196 - 36s - loss: 17.0144 - MinusLogProbMetric: 17.0144 - val_loss: 17.8640 - val_MinusLogProbMetric: 17.8640 - lr: 0.0010 - 36s/epoch - 182ms/step
Epoch 101/1000
2023-09-28 00:38:14.148 
Epoch 101/1000 
	 loss: 17.1411, MinusLogProbMetric: 17.1411, val_loss: 17.9971, val_MinusLogProbMetric: 17.9971

Epoch 101: val_loss did not improve from 17.68413
196/196 - 34s - loss: 17.1411 - MinusLogProbMetric: 17.1411 - val_loss: 17.9971 - val_MinusLogProbMetric: 17.9971 - lr: 0.0010 - 34s/epoch - 176ms/step
Epoch 102/1000
2023-09-28 00:38:52.028 
Epoch 102/1000 
	 loss: 16.9913, MinusLogProbMetric: 16.9913, val_loss: 18.5012, val_MinusLogProbMetric: 18.5012

Epoch 102: val_loss did not improve from 17.68413
196/196 - 38s - loss: 16.9913 - MinusLogProbMetric: 16.9913 - val_loss: 18.5012 - val_MinusLogProbMetric: 18.5012 - lr: 0.0010 - 38s/epoch - 193ms/step
Epoch 103/1000
2023-09-28 00:39:32.499 
Epoch 103/1000 
	 loss: 17.0063, MinusLogProbMetric: 17.0063, val_loss: 17.9172, val_MinusLogProbMetric: 17.9172

Epoch 103: val_loss did not improve from 17.68413
196/196 - 40s - loss: 17.0063 - MinusLogProbMetric: 17.0063 - val_loss: 17.9172 - val_MinusLogProbMetric: 17.9172 - lr: 0.0010 - 40s/epoch - 206ms/step
Epoch 104/1000
2023-09-28 00:40:06.242 
Epoch 104/1000 
	 loss: 17.0223, MinusLogProbMetric: 17.0223, val_loss: 18.3173, val_MinusLogProbMetric: 18.3173

Epoch 104: val_loss did not improve from 17.68413
196/196 - 34s - loss: 17.0223 - MinusLogProbMetric: 17.0223 - val_loss: 18.3173 - val_MinusLogProbMetric: 18.3173 - lr: 0.0010 - 34s/epoch - 172ms/step
Epoch 105/1000
2023-09-28 00:40:40.821 
Epoch 105/1000 
	 loss: 17.1353, MinusLogProbMetric: 17.1353, val_loss: 17.9908, val_MinusLogProbMetric: 17.9908

Epoch 105: val_loss did not improve from 17.68413
196/196 - 35s - loss: 17.1353 - MinusLogProbMetric: 17.1353 - val_loss: 17.9908 - val_MinusLogProbMetric: 17.9908 - lr: 0.0010 - 35s/epoch - 176ms/step
Epoch 106/1000
2023-09-28 00:41:18.975 
Epoch 106/1000 
	 loss: 16.9154, MinusLogProbMetric: 16.9154, val_loss: 18.0321, val_MinusLogProbMetric: 18.0321

Epoch 106: val_loss did not improve from 17.68413
196/196 - 38s - loss: 16.9154 - MinusLogProbMetric: 16.9154 - val_loss: 18.0321 - val_MinusLogProbMetric: 18.0321 - lr: 0.0010 - 38s/epoch - 195ms/step
Epoch 107/1000
2023-09-28 00:42:00.869 
Epoch 107/1000 
	 loss: 16.9427, MinusLogProbMetric: 16.9427, val_loss: 18.0187, val_MinusLogProbMetric: 18.0187

Epoch 107: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.9427 - MinusLogProbMetric: 16.9427 - val_loss: 18.0187 - val_MinusLogProbMetric: 18.0187 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 108/1000
2023-09-28 00:42:42.154 
Epoch 108/1000 
	 loss: 17.0310, MinusLogProbMetric: 17.0310, val_loss: 17.8790, val_MinusLogProbMetric: 17.8790

Epoch 108: val_loss did not improve from 17.68413
196/196 - 41s - loss: 17.0310 - MinusLogProbMetric: 17.0310 - val_loss: 17.8790 - val_MinusLogProbMetric: 17.8790 - lr: 0.0010 - 41s/epoch - 211ms/step
Epoch 109/1000
2023-09-28 00:43:24.159 
Epoch 109/1000 
	 loss: 16.8581, MinusLogProbMetric: 16.8581, val_loss: 17.8436, val_MinusLogProbMetric: 17.8436

Epoch 109: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.8581 - MinusLogProbMetric: 16.8581 - val_loss: 17.8436 - val_MinusLogProbMetric: 17.8436 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 110/1000
2023-09-28 00:44:06.035 
Epoch 110/1000 
	 loss: 17.0035, MinusLogProbMetric: 17.0035, val_loss: 18.2099, val_MinusLogProbMetric: 18.2099

Epoch 110: val_loss did not improve from 17.68413
196/196 - 42s - loss: 17.0035 - MinusLogProbMetric: 17.0035 - val_loss: 18.2099 - val_MinusLogProbMetric: 18.2099 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 111/1000
2023-09-28 00:44:47.882 
Epoch 111/1000 
	 loss: 16.8876, MinusLogProbMetric: 16.8876, val_loss: 19.3074, val_MinusLogProbMetric: 19.3074

Epoch 111: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.8876 - MinusLogProbMetric: 16.8876 - val_loss: 19.3074 - val_MinusLogProbMetric: 19.3074 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 112/1000
2023-09-28 00:45:29.561 
Epoch 112/1000 
	 loss: 16.9211, MinusLogProbMetric: 16.9211, val_loss: 17.9066, val_MinusLogProbMetric: 17.9066

Epoch 112: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.9211 - MinusLogProbMetric: 16.9211 - val_loss: 17.9066 - val_MinusLogProbMetric: 17.9066 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 113/1000
2023-09-28 00:46:11.602 
Epoch 113/1000 
	 loss: 16.8540, MinusLogProbMetric: 16.8540, val_loss: 18.5225, val_MinusLogProbMetric: 18.5225

Epoch 113: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.8540 - MinusLogProbMetric: 16.8540 - val_loss: 18.5225 - val_MinusLogProbMetric: 18.5225 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 114/1000
2023-09-28 00:46:53.357 
Epoch 114/1000 
	 loss: 16.9084, MinusLogProbMetric: 16.9084, val_loss: 17.9290, val_MinusLogProbMetric: 17.9290

Epoch 114: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.9084 - MinusLogProbMetric: 16.9084 - val_loss: 17.9290 - val_MinusLogProbMetric: 17.9290 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 115/1000
2023-09-28 00:47:35.385 
Epoch 115/1000 
	 loss: 16.9131, MinusLogProbMetric: 16.9131, val_loss: 17.9279, val_MinusLogProbMetric: 17.9279

Epoch 115: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.9131 - MinusLogProbMetric: 16.9131 - val_loss: 17.9279 - val_MinusLogProbMetric: 17.9279 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 116/1000
2023-09-28 00:48:17.258 
Epoch 116/1000 
	 loss: 16.8372, MinusLogProbMetric: 16.8372, val_loss: 17.9424, val_MinusLogProbMetric: 17.9424

Epoch 116: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.8372 - MinusLogProbMetric: 16.8372 - val_loss: 17.9424 - val_MinusLogProbMetric: 17.9424 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 117/1000
2023-09-28 00:48:59.053 
Epoch 117/1000 
	 loss: 16.7681, MinusLogProbMetric: 16.7681, val_loss: 17.8552, val_MinusLogProbMetric: 17.8552

Epoch 117: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.7681 - MinusLogProbMetric: 16.7681 - val_loss: 17.8552 - val_MinusLogProbMetric: 17.8552 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 118/1000
2023-09-28 00:49:41.315 
Epoch 118/1000 
	 loss: 16.7651, MinusLogProbMetric: 16.7651, val_loss: 18.0761, val_MinusLogProbMetric: 18.0761

Epoch 118: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.7651 - MinusLogProbMetric: 16.7651 - val_loss: 18.0761 - val_MinusLogProbMetric: 18.0761 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 119/1000
2023-09-28 00:50:23.473 
Epoch 119/1000 
	 loss: 16.8741, MinusLogProbMetric: 16.8741, val_loss: 18.0740, val_MinusLogProbMetric: 18.0740

Epoch 119: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.8741 - MinusLogProbMetric: 16.8741 - val_loss: 18.0740 - val_MinusLogProbMetric: 18.0740 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 120/1000
2023-09-28 00:51:05.322 
Epoch 120/1000 
	 loss: 16.7767, MinusLogProbMetric: 16.7767, val_loss: 17.9802, val_MinusLogProbMetric: 17.9802

Epoch 120: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.7767 - MinusLogProbMetric: 16.7767 - val_loss: 17.9802 - val_MinusLogProbMetric: 17.9802 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 121/1000
2023-09-28 00:51:47.097 
Epoch 121/1000 
	 loss: 16.7499, MinusLogProbMetric: 16.7499, val_loss: 18.0651, val_MinusLogProbMetric: 18.0651

Epoch 121: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.7499 - MinusLogProbMetric: 16.7499 - val_loss: 18.0651 - val_MinusLogProbMetric: 18.0651 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 122/1000
2023-09-28 00:52:29.046 
Epoch 122/1000 
	 loss: 16.7414, MinusLogProbMetric: 16.7414, val_loss: 17.9114, val_MinusLogProbMetric: 17.9114

Epoch 122: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.7414 - MinusLogProbMetric: 16.7414 - val_loss: 17.9114 - val_MinusLogProbMetric: 17.9114 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 123/1000
2023-09-28 00:53:10.971 
Epoch 123/1000 
	 loss: 16.6788, MinusLogProbMetric: 16.6788, val_loss: 18.4071, val_MinusLogProbMetric: 18.4071

Epoch 123: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.6788 - MinusLogProbMetric: 16.6788 - val_loss: 18.4071 - val_MinusLogProbMetric: 18.4071 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 124/1000
2023-09-28 00:53:52.901 
Epoch 124/1000 
	 loss: 16.7304, MinusLogProbMetric: 16.7304, val_loss: 17.8244, val_MinusLogProbMetric: 17.8244

Epoch 124: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.7304 - MinusLogProbMetric: 16.7304 - val_loss: 17.8244 - val_MinusLogProbMetric: 17.8244 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 125/1000
2023-09-28 00:54:34.784 
Epoch 125/1000 
	 loss: 16.8074, MinusLogProbMetric: 16.8074, val_loss: 18.1771, val_MinusLogProbMetric: 18.1771

Epoch 125: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.8074 - MinusLogProbMetric: 16.8074 - val_loss: 18.1771 - val_MinusLogProbMetric: 18.1771 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 126/1000
2023-09-28 00:55:17.143 
Epoch 126/1000 
	 loss: 16.7593, MinusLogProbMetric: 16.7593, val_loss: 18.0907, val_MinusLogProbMetric: 18.0907

Epoch 126: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.7593 - MinusLogProbMetric: 16.7593 - val_loss: 18.0907 - val_MinusLogProbMetric: 18.0907 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 127/1000
2023-09-28 00:55:59.370 
Epoch 127/1000 
	 loss: 16.6771, MinusLogProbMetric: 16.6771, val_loss: 19.0645, val_MinusLogProbMetric: 19.0645

Epoch 127: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.6771 - MinusLogProbMetric: 16.6771 - val_loss: 19.0645 - val_MinusLogProbMetric: 19.0645 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 128/1000
2023-09-28 00:56:41.268 
Epoch 128/1000 
	 loss: 16.5975, MinusLogProbMetric: 16.5975, val_loss: 17.9132, val_MinusLogProbMetric: 17.9132

Epoch 128: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.5975 - MinusLogProbMetric: 16.5975 - val_loss: 17.9132 - val_MinusLogProbMetric: 17.9132 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 129/1000
2023-09-28 00:57:23.032 
Epoch 129/1000 
	 loss: 16.6752, MinusLogProbMetric: 16.6752, val_loss: 18.1451, val_MinusLogProbMetric: 18.1451

Epoch 129: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.6752 - MinusLogProbMetric: 16.6752 - val_loss: 18.1451 - val_MinusLogProbMetric: 18.1451 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 130/1000
2023-09-28 00:58:05.282 
Epoch 130/1000 
	 loss: 16.6719, MinusLogProbMetric: 16.6719, val_loss: 18.2951, val_MinusLogProbMetric: 18.2951

Epoch 130: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.6719 - MinusLogProbMetric: 16.6719 - val_loss: 18.2951 - val_MinusLogProbMetric: 18.2951 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 131/1000
2023-09-28 00:58:47.166 
Epoch 131/1000 
	 loss: 16.7586, MinusLogProbMetric: 16.7586, val_loss: 18.1732, val_MinusLogProbMetric: 18.1732

Epoch 131: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.7586 - MinusLogProbMetric: 16.7586 - val_loss: 18.1732 - val_MinusLogProbMetric: 18.1732 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 132/1000
2023-09-28 00:59:29.279 
Epoch 132/1000 
	 loss: 16.6664, MinusLogProbMetric: 16.6664, val_loss: 18.2451, val_MinusLogProbMetric: 18.2451

Epoch 132: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.6664 - MinusLogProbMetric: 16.6664 - val_loss: 18.2451 - val_MinusLogProbMetric: 18.2451 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 133/1000
2023-09-28 01:00:11.423 
Epoch 133/1000 
	 loss: 16.6766, MinusLogProbMetric: 16.6766, val_loss: 18.1348, val_MinusLogProbMetric: 18.1348

Epoch 133: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.6766 - MinusLogProbMetric: 16.6766 - val_loss: 18.1348 - val_MinusLogProbMetric: 18.1348 - lr: 0.0010 - 42s/epoch - 215ms/step
Epoch 134/1000
2023-09-28 01:00:53.234 
Epoch 134/1000 
	 loss: 16.5584, MinusLogProbMetric: 16.5584, val_loss: 18.1856, val_MinusLogProbMetric: 18.1856

Epoch 134: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.5584 - MinusLogProbMetric: 16.5584 - val_loss: 18.1856 - val_MinusLogProbMetric: 18.1856 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 135/1000
2023-09-28 01:01:35.244 
Epoch 135/1000 
	 loss: 16.7353, MinusLogProbMetric: 16.7353, val_loss: 18.0789, val_MinusLogProbMetric: 18.0789

Epoch 135: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.7353 - MinusLogProbMetric: 16.7353 - val_loss: 18.0789 - val_MinusLogProbMetric: 18.0789 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 136/1000
2023-09-28 01:02:17.164 
Epoch 136/1000 
	 loss: 16.6282, MinusLogProbMetric: 16.6282, val_loss: 18.0618, val_MinusLogProbMetric: 18.0618

Epoch 136: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.6282 - MinusLogProbMetric: 16.6282 - val_loss: 18.0618 - val_MinusLogProbMetric: 18.0618 - lr: 0.0010 - 42s/epoch - 214ms/step
Epoch 137/1000
2023-09-28 01:02:58.975 
Epoch 137/1000 
	 loss: 16.6004, MinusLogProbMetric: 16.6004, val_loss: 18.2563, val_MinusLogProbMetric: 18.2563

Epoch 137: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.6004 - MinusLogProbMetric: 16.6004 - val_loss: 18.2563 - val_MinusLogProbMetric: 18.2563 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 138/1000
2023-09-28 01:03:40.648 
Epoch 138/1000 
	 loss: 16.6129, MinusLogProbMetric: 16.6129, val_loss: 17.9323, val_MinusLogProbMetric: 17.9323

Epoch 138: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.6129 - MinusLogProbMetric: 16.6129 - val_loss: 17.9323 - val_MinusLogProbMetric: 17.9323 - lr: 0.0010 - 42s/epoch - 213ms/step
Epoch 139/1000
2023-09-28 01:04:22.914 
Epoch 139/1000 
	 loss: 16.5565, MinusLogProbMetric: 16.5565, val_loss: 17.9251, val_MinusLogProbMetric: 17.9251

Epoch 139: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.5565 - MinusLogProbMetric: 16.5565 - val_loss: 17.9251 - val_MinusLogProbMetric: 17.9251 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 140/1000
2023-09-28 01:05:04.845 
Epoch 140/1000 
	 loss: 16.1015, MinusLogProbMetric: 16.1015, val_loss: 17.8864, val_MinusLogProbMetric: 17.8864

Epoch 140: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.1015 - MinusLogProbMetric: 16.1015 - val_loss: 17.8864 - val_MinusLogProbMetric: 17.8864 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 141/1000
2023-09-28 01:05:46.504 
Epoch 141/1000 
	 loss: 16.1320, MinusLogProbMetric: 16.1320, val_loss: 17.8729, val_MinusLogProbMetric: 17.8729

Epoch 141: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.1320 - MinusLogProbMetric: 16.1320 - val_loss: 17.8729 - val_MinusLogProbMetric: 17.8729 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 142/1000
2023-09-28 01:06:28.795 
Epoch 142/1000 
	 loss: 16.1476, MinusLogProbMetric: 16.1476, val_loss: 17.8552, val_MinusLogProbMetric: 17.8552

Epoch 142: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.1476 - MinusLogProbMetric: 16.1476 - val_loss: 17.8552 - val_MinusLogProbMetric: 17.8552 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 143/1000
2023-09-28 01:07:10.843 
Epoch 143/1000 
	 loss: 16.1547, MinusLogProbMetric: 16.1547, val_loss: 17.9673, val_MinusLogProbMetric: 17.9673

Epoch 143: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.1547 - MinusLogProbMetric: 16.1547 - val_loss: 17.9673 - val_MinusLogProbMetric: 17.9673 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 144/1000
2023-09-28 01:07:52.713 
Epoch 144/1000 
	 loss: 16.1264, MinusLogProbMetric: 16.1264, val_loss: 17.9410, val_MinusLogProbMetric: 17.9410

Epoch 144: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.1264 - MinusLogProbMetric: 16.1264 - val_loss: 17.9410 - val_MinusLogProbMetric: 17.9410 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 145/1000
2023-09-28 01:08:34.927 
Epoch 145/1000 
	 loss: 16.0349, MinusLogProbMetric: 16.0349, val_loss: 17.9301, val_MinusLogProbMetric: 17.9301

Epoch 145: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.0349 - MinusLogProbMetric: 16.0349 - val_loss: 17.9301 - val_MinusLogProbMetric: 17.9301 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 146/1000
2023-09-28 01:09:16.641 
Epoch 146/1000 
	 loss: 16.0832, MinusLogProbMetric: 16.0832, val_loss: 17.8949, val_MinusLogProbMetric: 17.8949

Epoch 146: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.0832 - MinusLogProbMetric: 16.0832 - val_loss: 17.8949 - val_MinusLogProbMetric: 17.8949 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 147/1000
2023-09-28 01:09:59.034 
Epoch 147/1000 
	 loss: 16.1236, MinusLogProbMetric: 16.1236, val_loss: 18.3710, val_MinusLogProbMetric: 18.3710

Epoch 147: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.1236 - MinusLogProbMetric: 16.1236 - val_loss: 18.3710 - val_MinusLogProbMetric: 18.3710 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 148/1000
2023-09-28 01:10:41.142 
Epoch 148/1000 
	 loss: 16.0309, MinusLogProbMetric: 16.0309, val_loss: 17.8242, val_MinusLogProbMetric: 17.8242

Epoch 148: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.0309 - MinusLogProbMetric: 16.0309 - val_loss: 17.8242 - val_MinusLogProbMetric: 17.8242 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 149/1000
2023-09-28 01:11:22.753 
Epoch 149/1000 
	 loss: 16.0543, MinusLogProbMetric: 16.0543, val_loss: 17.9676, val_MinusLogProbMetric: 17.9676

Epoch 149: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.0543 - MinusLogProbMetric: 16.0543 - val_loss: 17.9676 - val_MinusLogProbMetric: 17.9676 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 150/1000
2023-09-28 01:12:04.837 
Epoch 150/1000 
	 loss: 16.0098, MinusLogProbMetric: 16.0098, val_loss: 18.0893, val_MinusLogProbMetric: 18.0893

Epoch 150: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.0098 - MinusLogProbMetric: 16.0098 - val_loss: 18.0893 - val_MinusLogProbMetric: 18.0893 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 151/1000
2023-09-28 01:12:46.782 
Epoch 151/1000 
	 loss: 16.0431, MinusLogProbMetric: 16.0431, val_loss: 18.0828, val_MinusLogProbMetric: 18.0828

Epoch 151: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.0431 - MinusLogProbMetric: 16.0431 - val_loss: 18.0828 - val_MinusLogProbMetric: 18.0828 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 152/1000
2023-09-28 01:13:28.660 
Epoch 152/1000 
	 loss: 16.0049, MinusLogProbMetric: 16.0049, val_loss: 17.9546, val_MinusLogProbMetric: 17.9546

Epoch 152: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.0049 - MinusLogProbMetric: 16.0049 - val_loss: 17.9546 - val_MinusLogProbMetric: 17.9546 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 153/1000
2023-09-28 01:14:10.785 
Epoch 153/1000 
	 loss: 16.0095, MinusLogProbMetric: 16.0095, val_loss: 17.9919, val_MinusLogProbMetric: 17.9919

Epoch 153: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.0095 - MinusLogProbMetric: 16.0095 - val_loss: 17.9919 - val_MinusLogProbMetric: 17.9919 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 154/1000
2023-09-28 01:14:52.721 
Epoch 154/1000 
	 loss: 15.9627, MinusLogProbMetric: 15.9627, val_loss: 18.2935, val_MinusLogProbMetric: 18.2935

Epoch 154: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9627 - MinusLogProbMetric: 15.9627 - val_loss: 18.2935 - val_MinusLogProbMetric: 18.2935 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 155/1000
2023-09-28 01:15:34.680 
Epoch 155/1000 
	 loss: 16.0416, MinusLogProbMetric: 16.0416, val_loss: 17.9983, val_MinusLogProbMetric: 17.9983

Epoch 155: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.0416 - MinusLogProbMetric: 16.0416 - val_loss: 17.9983 - val_MinusLogProbMetric: 17.9983 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 156/1000
2023-09-28 01:16:16.922 
Epoch 156/1000 
	 loss: 15.9780, MinusLogProbMetric: 15.9780, val_loss: 18.5161, val_MinusLogProbMetric: 18.5161

Epoch 156: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9780 - MinusLogProbMetric: 15.9780 - val_loss: 18.5161 - val_MinusLogProbMetric: 18.5161 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 157/1000
2023-09-28 01:16:58.446 
Epoch 157/1000 
	 loss: 15.9579, MinusLogProbMetric: 15.9579, val_loss: 18.0173, val_MinusLogProbMetric: 18.0173

Epoch 157: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9579 - MinusLogProbMetric: 15.9579 - val_loss: 18.0173 - val_MinusLogProbMetric: 18.0173 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 158/1000
2023-09-28 01:17:40.493 
Epoch 158/1000 
	 loss: 16.0175, MinusLogProbMetric: 16.0175, val_loss: 17.9797, val_MinusLogProbMetric: 17.9797

Epoch 158: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.0175 - MinusLogProbMetric: 16.0175 - val_loss: 17.9797 - val_MinusLogProbMetric: 17.9797 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 159/1000
2023-09-28 01:18:22.494 
Epoch 159/1000 
	 loss: 15.9683, MinusLogProbMetric: 15.9683, val_loss: 18.1291, val_MinusLogProbMetric: 18.1291

Epoch 159: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9683 - MinusLogProbMetric: 15.9683 - val_loss: 18.1291 - val_MinusLogProbMetric: 18.1291 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 160/1000
2023-09-28 01:19:04.364 
Epoch 160/1000 
	 loss: 15.9489, MinusLogProbMetric: 15.9489, val_loss: 18.1477, val_MinusLogProbMetric: 18.1477

Epoch 160: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9489 - MinusLogProbMetric: 15.9489 - val_loss: 18.1477 - val_MinusLogProbMetric: 18.1477 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 161/1000
2023-09-28 01:19:46.061 
Epoch 161/1000 
	 loss: 15.9488, MinusLogProbMetric: 15.9488, val_loss: 18.6010, val_MinusLogProbMetric: 18.6010

Epoch 161: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9488 - MinusLogProbMetric: 15.9488 - val_loss: 18.6010 - val_MinusLogProbMetric: 18.6010 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 162/1000
2023-09-28 01:20:27.966 
Epoch 162/1000 
	 loss: 15.9657, MinusLogProbMetric: 15.9657, val_loss: 18.3800, val_MinusLogProbMetric: 18.3800

Epoch 162: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9657 - MinusLogProbMetric: 15.9657 - val_loss: 18.3800 - val_MinusLogProbMetric: 18.3800 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 163/1000
2023-09-28 01:21:10.012 
Epoch 163/1000 
	 loss: 15.9306, MinusLogProbMetric: 15.9306, val_loss: 18.3798, val_MinusLogProbMetric: 18.3798

Epoch 163: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9306 - MinusLogProbMetric: 15.9306 - val_loss: 18.3798 - val_MinusLogProbMetric: 18.3798 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 164/1000
2023-09-28 01:21:51.628 
Epoch 164/1000 
	 loss: 15.9098, MinusLogProbMetric: 15.9098, val_loss: 18.0446, val_MinusLogProbMetric: 18.0446

Epoch 164: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9098 - MinusLogProbMetric: 15.9098 - val_loss: 18.0446 - val_MinusLogProbMetric: 18.0446 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 165/1000
2023-09-28 01:22:33.358 
Epoch 165/1000 
	 loss: 15.9057, MinusLogProbMetric: 15.9057, val_loss: 18.1182, val_MinusLogProbMetric: 18.1182

Epoch 165: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9057 - MinusLogProbMetric: 15.9057 - val_loss: 18.1182 - val_MinusLogProbMetric: 18.1182 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 166/1000
2023-09-28 01:23:15.001 
Epoch 166/1000 
	 loss: 15.8892, MinusLogProbMetric: 15.8892, val_loss: 18.1802, val_MinusLogProbMetric: 18.1802

Epoch 166: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.8892 - MinusLogProbMetric: 15.8892 - val_loss: 18.1802 - val_MinusLogProbMetric: 18.1802 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 167/1000
2023-09-28 01:23:57.052 
Epoch 167/1000 
	 loss: 15.9435, MinusLogProbMetric: 15.9435, val_loss: 18.1852, val_MinusLogProbMetric: 18.1852

Epoch 167: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9435 - MinusLogProbMetric: 15.9435 - val_loss: 18.1852 - val_MinusLogProbMetric: 18.1852 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 168/1000
2023-09-28 01:24:39.228 
Epoch 168/1000 
	 loss: 15.9293, MinusLogProbMetric: 15.9293, val_loss: 18.1642, val_MinusLogProbMetric: 18.1642

Epoch 168: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9293 - MinusLogProbMetric: 15.9293 - val_loss: 18.1642 - val_MinusLogProbMetric: 18.1642 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 169/1000
2023-09-28 01:25:21.200 
Epoch 169/1000 
	 loss: 15.9075, MinusLogProbMetric: 15.9075, val_loss: 18.0122, val_MinusLogProbMetric: 18.0122

Epoch 169: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9075 - MinusLogProbMetric: 15.9075 - val_loss: 18.0122 - val_MinusLogProbMetric: 18.0122 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 170/1000
2023-09-28 01:26:03.159 
Epoch 170/1000 
	 loss: 15.9053, MinusLogProbMetric: 15.9053, val_loss: 18.0888, val_MinusLogProbMetric: 18.0888

Epoch 170: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9053 - MinusLogProbMetric: 15.9053 - val_loss: 18.0888 - val_MinusLogProbMetric: 18.0888 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 171/1000
2023-09-28 01:26:45.296 
Epoch 171/1000 
	 loss: 15.9228, MinusLogProbMetric: 15.9228, val_loss: 18.1228, val_MinusLogProbMetric: 18.1228

Epoch 171: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9228 - MinusLogProbMetric: 15.9228 - val_loss: 18.1228 - val_MinusLogProbMetric: 18.1228 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 172/1000
2023-09-28 01:27:27.244 
Epoch 172/1000 
	 loss: 15.8774, MinusLogProbMetric: 15.8774, val_loss: 19.2869, val_MinusLogProbMetric: 19.2869

Epoch 172: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.8774 - MinusLogProbMetric: 15.8774 - val_loss: 19.2869 - val_MinusLogProbMetric: 19.2869 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 173/1000
2023-09-28 01:28:09.465 
Epoch 173/1000 
	 loss: 16.0752, MinusLogProbMetric: 16.0752, val_loss: 18.6004, val_MinusLogProbMetric: 18.6004

Epoch 173: val_loss did not improve from 17.68413
196/196 - 42s - loss: 16.0752 - MinusLogProbMetric: 16.0752 - val_loss: 18.6004 - val_MinusLogProbMetric: 18.6004 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 174/1000
2023-09-28 01:28:51.073 
Epoch 174/1000 
	 loss: 15.9751, MinusLogProbMetric: 15.9751, val_loss: 18.1734, val_MinusLogProbMetric: 18.1734

Epoch 174: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9751 - MinusLogProbMetric: 15.9751 - val_loss: 18.1734 - val_MinusLogProbMetric: 18.1734 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 175/1000
2023-09-28 01:29:32.976 
Epoch 175/1000 
	 loss: 15.8653, MinusLogProbMetric: 15.8653, val_loss: 18.1608, val_MinusLogProbMetric: 18.1608

Epoch 175: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.8653 - MinusLogProbMetric: 15.8653 - val_loss: 18.1608 - val_MinusLogProbMetric: 18.1608 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 176/1000
2023-09-28 01:30:15.261 
Epoch 176/1000 
	 loss: 15.8558, MinusLogProbMetric: 15.8558, val_loss: 18.5348, val_MinusLogProbMetric: 18.5348

Epoch 176: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.8558 - MinusLogProbMetric: 15.8558 - val_loss: 18.5348 - val_MinusLogProbMetric: 18.5348 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 177/1000
2023-09-28 01:30:57.154 
Epoch 177/1000 
	 loss: 15.9391, MinusLogProbMetric: 15.9391, val_loss: 18.1704, val_MinusLogProbMetric: 18.1704

Epoch 177: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9391 - MinusLogProbMetric: 15.9391 - val_loss: 18.1704 - val_MinusLogProbMetric: 18.1704 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 178/1000
2023-09-28 01:31:39.035 
Epoch 178/1000 
	 loss: 15.8883, MinusLogProbMetric: 15.8883, val_loss: 18.2875, val_MinusLogProbMetric: 18.2875

Epoch 178: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.8883 - MinusLogProbMetric: 15.8883 - val_loss: 18.2875 - val_MinusLogProbMetric: 18.2875 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 179/1000
2023-09-28 01:32:20.995 
Epoch 179/1000 
	 loss: 15.8637, MinusLogProbMetric: 15.8637, val_loss: 18.5971, val_MinusLogProbMetric: 18.5971

Epoch 179: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.8637 - MinusLogProbMetric: 15.8637 - val_loss: 18.5971 - val_MinusLogProbMetric: 18.5971 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 180/1000
2023-09-28 01:33:03.044 
Epoch 180/1000 
	 loss: 15.9303, MinusLogProbMetric: 15.9303, val_loss: 18.2079, val_MinusLogProbMetric: 18.2079

Epoch 180: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.9303 - MinusLogProbMetric: 15.9303 - val_loss: 18.2079 - val_MinusLogProbMetric: 18.2079 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 181/1000
2023-09-28 01:33:45.012 
Epoch 181/1000 
	 loss: 15.8715, MinusLogProbMetric: 15.8715, val_loss: 18.2896, val_MinusLogProbMetric: 18.2896

Epoch 181: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.8715 - MinusLogProbMetric: 15.8715 - val_loss: 18.2896 - val_MinusLogProbMetric: 18.2896 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 182/1000
2023-09-28 01:34:26.631 
Epoch 182/1000 
	 loss: 15.8133, MinusLogProbMetric: 15.8133, val_loss: 18.1770, val_MinusLogProbMetric: 18.1770

Epoch 182: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.8133 - MinusLogProbMetric: 15.8133 - val_loss: 18.1770 - val_MinusLogProbMetric: 18.1770 - lr: 5.0000e-04 - 42s/epoch - 212ms/step
Epoch 183/1000
2023-09-28 01:35:08.323 
Epoch 183/1000 
	 loss: 15.8247, MinusLogProbMetric: 15.8247, val_loss: 18.1620, val_MinusLogProbMetric: 18.1620

Epoch 183: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.8247 - MinusLogProbMetric: 15.8247 - val_loss: 18.1620 - val_MinusLogProbMetric: 18.1620 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 184/1000
2023-09-28 01:35:50.127 
Epoch 184/1000 
	 loss: 15.8366, MinusLogProbMetric: 15.8366, val_loss: 18.3074, val_MinusLogProbMetric: 18.3074

Epoch 184: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.8366 - MinusLogProbMetric: 15.8366 - val_loss: 18.3074 - val_MinusLogProbMetric: 18.3074 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 185/1000
2023-09-28 01:36:32.504 
Epoch 185/1000 
	 loss: 15.8907, MinusLogProbMetric: 15.8907, val_loss: 18.6593, val_MinusLogProbMetric: 18.6593

Epoch 185: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.8907 - MinusLogProbMetric: 15.8907 - val_loss: 18.6593 - val_MinusLogProbMetric: 18.6593 - lr: 5.0000e-04 - 42s/epoch - 216ms/step
Epoch 186/1000
2023-09-28 01:37:14.250 
Epoch 186/1000 
	 loss: 15.8067, MinusLogProbMetric: 15.8067, val_loss: 18.3003, val_MinusLogProbMetric: 18.3003

Epoch 186: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.8067 - MinusLogProbMetric: 15.8067 - val_loss: 18.3003 - val_MinusLogProbMetric: 18.3003 - lr: 5.0000e-04 - 42s/epoch - 213ms/step
Epoch 187/1000
2023-09-28 01:37:56.260 
Epoch 187/1000 
	 loss: 15.8661, MinusLogProbMetric: 15.8661, val_loss: 18.2421, val_MinusLogProbMetric: 18.2421

Epoch 187: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.8661 - MinusLogProbMetric: 15.8661 - val_loss: 18.2421 - val_MinusLogProbMetric: 18.2421 - lr: 5.0000e-04 - 42s/epoch - 214ms/step
Epoch 188/1000
2023-09-28 01:38:38.408 
Epoch 188/1000 
	 loss: 15.7541, MinusLogProbMetric: 15.7541, val_loss: 18.8149, val_MinusLogProbMetric: 18.8149

Epoch 188: val_loss did not improve from 17.68413
196/196 - 42s - loss: 15.7541 - MinusLogProbMetric: 15.7541 - val_loss: 18.8149 - val_MinusLogProbMetric: 18.8149 - lr: 5.0000e-04 - 42s/epoch - 215ms/step
Epoch 189/1000
2023-09-28 01:39:20.479 
Epoch 189/1000 
	 loss: 15.7742, MinusLogProbMetric: 15.7742, val_loss: 18.5075, val_MinusLogProbMetric: 18.5075

Epoch 189: val_loss did not improve from 17.68413
Restoring model weights from the end of the best epoch: 89.
196/196 - 43s - loss: 15.7742 - MinusLogProbMetric: 15.7742 - val_loss: 18.5075 - val_MinusLogProbMetric: 18.5075 - lr: 5.0000e-04 - 43s/epoch - 217ms/step
Epoch 189: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 17.51040940801613 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 10.394436075992417 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 7.535918352019507 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 8.036910174007062 seconds.
Training succeeded with seed 933.
Model trained in 8002.57 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 44.83 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 45.07 s.
===========
Run 316/720 done in 8053.03 s.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

===========
Generating train data for run 318.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_318/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_318/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_318/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_318
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_77"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_78 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  2305120   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,305,120
Trainable params: 2,305,120
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f8f0595dd20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8f0511a8c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8f0511a8c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8f059e0b50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8f051e8dc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8f051e9330>, <keras.callbacks.ModelCheckpoint object at 0x7f8f051e93f0>, <keras.callbacks.EarlyStopping object at 0x7f8f051e9660>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8f051e9690>, <keras.callbacks.TerminateOnNaN object at 0x7f8f051e92d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_318/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 318/720 with hyperparameters:
timestamp = 2023-09-28 01:40:14.295661
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2305120
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 35: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 01:42:31.789 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 1337.5282, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 137s - loss: nan - MinusLogProbMetric: 1337.5282 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 137s/epoch - 701ms/step
The loss history contains NaN values.
Training failed: trying again with seed 402839 and lr 0.0003333333333333333.
===========
Generating train data for run 318.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 32)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_318/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 933}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_318/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[32], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_318/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_318
self.data_kwargs: {'seed': 933}
self.x_data: [[4.6118093  6.294248   0.6753535  ... 1.3966799  6.5528636  1.404321  ]
 [5.0913153  7.181254   5.302016   ... 4.927679   2.6276157  7.5562596 ]
 [2.3468595  3.6813745  8.449473   ... 7.5625257  2.3049273  1.7164713 ]
 ...
 [1.7774892  4.58926    7.003348   ... 7.297376   3.211844   1.3889377 ]
 [5.1396513  5.5684643  1.4578001  ... 0.69219434 6.9603205  1.3834548 ]
 [3.2499232  3.5331872  8.207085   ... 6.760554   3.3946218  2.0297966 ]]
self.y_data: []
self.ndims: 32
Model defined.
Model: "model_88"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_89 (InputLayer)       [(None, 32)]              0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  2305120   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,305,120
Trainable params: 2,305,120
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f8fbc71c4f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f91f85440a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f91f85440a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f962034c400>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f96202f0790>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f96202f0d00>, <keras.callbacks.ModelCheckpoint object at 0x7f96202f0dc0>, <keras.callbacks.EarlyStopping object at 0x7f96202f1030>, <keras.callbacks.ReduceLROnPlateau object at 0x7f96202f1060>, <keras.callbacks.TerminateOnNaN object at 0x7f96202f0ca0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[2.7429588 , 2.5700626 , 7.723717  , ..., 7.7501717 , 2.9186718 ,
        1.775845  ],
       [4.728784  , 6.065677  , 0.9645814 , ..., 1.2628946 , 6.7253027 ,
        1.4381479 ],
       [3.8513129 , 6.1744533 , 0.2601294 , ..., 1.1662469 , 6.572044  ,
        1.3500103 ],
       ...,
       [2.8332918 , 4.2526407 , 9.701975  , ..., 7.0738525 , 2.5952196 ,
        1.8190176 ],
       [3.118363  , 5.4312434 , 0.32481733, ..., 1.9350358 , 6.6219816 ,
        1.3230293 ],
       [1.792576  , 4.9344916 , 9.207544  , ..., 7.378074  , 2.857349  ,
        1.56005   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_318/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 318/720 with hyperparameters:
timestamp = 2023-09-28 01:42:40.616076
ndims = 32
seed_train = 933
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2305120
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 4.6118093   6.294248    0.6753535   5.561676    6.128757    6.145322
  9.785121    7.1052794   3.7474928   5.3494825   6.3791265   0.5788075
  5.683702    6.3188157   1.6447939   1.0492666   3.6812766   3.7191741
  5.81123     3.2186081  10.359394    0.68370855  2.036205    1.0944312
  6.489696    3.6441135   4.6169076   1.7655768   1.5199437   1.3966799
  6.5528636   1.404321  ]
Epoch 1/1000
2023-09-28 01:45:52.071 
Epoch 1/1000 
	 loss: 267.6651, MinusLogProbMetric: 267.6651, val_loss: 68.3770, val_MinusLogProbMetric: 68.3770

Epoch 1: val_loss improved from inf to 68.37699, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 192s - loss: 267.6651 - MinusLogProbMetric: 267.6651 - val_loss: 68.3770 - val_MinusLogProbMetric: 68.3770 - lr: 3.3333e-04 - 192s/epoch - 982ms/step
Epoch 2/1000
2023-09-28 01:46:59.745 
Epoch 2/1000 
	 loss: 58.0255, MinusLogProbMetric: 58.0255, val_loss: 47.2934, val_MinusLogProbMetric: 47.2934

Epoch 2: val_loss improved from 68.37699 to 47.29340, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 68s - loss: 58.0255 - MinusLogProbMetric: 58.0255 - val_loss: 47.2934 - val_MinusLogProbMetric: 47.2934 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 3/1000
2023-09-28 01:48:07.067 
Epoch 3/1000 
	 loss: 39.1776, MinusLogProbMetric: 39.1776, val_loss: 33.6970, val_MinusLogProbMetric: 33.6970

Epoch 3: val_loss improved from 47.29340 to 33.69703, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 39.1776 - MinusLogProbMetric: 39.1776 - val_loss: 33.6970 - val_MinusLogProbMetric: 33.6970 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 4/1000
2023-09-28 01:49:13.911 
Epoch 4/1000 
	 loss: 33.7582, MinusLogProbMetric: 33.7582, val_loss: 33.3617, val_MinusLogProbMetric: 33.3617

Epoch 4: val_loss improved from 33.69703 to 33.36169, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 33.7582 - MinusLogProbMetric: 33.7582 - val_loss: 33.3617 - val_MinusLogProbMetric: 33.3617 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 5/1000
2023-09-28 01:50:20.985 
Epoch 5/1000 
	 loss: 29.4393, MinusLogProbMetric: 29.4393, val_loss: 27.8850, val_MinusLogProbMetric: 27.8850

Epoch 5: val_loss improved from 33.36169 to 27.88503, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 29.4393 - MinusLogProbMetric: 29.4393 - val_loss: 27.8850 - val_MinusLogProbMetric: 27.8850 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 6/1000
2023-09-28 01:51:27.535 
Epoch 6/1000 
	 loss: 27.4560, MinusLogProbMetric: 27.4560, val_loss: 28.2621, val_MinusLogProbMetric: 28.2621

Epoch 6: val_loss did not improve from 27.88503
196/196 - 66s - loss: 27.4560 - MinusLogProbMetric: 27.4560 - val_loss: 28.2621 - val_MinusLogProbMetric: 28.2621 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 7/1000
2023-09-28 01:52:33.236 
Epoch 7/1000 
	 loss: 26.1126, MinusLogProbMetric: 26.1126, val_loss: 26.0081, val_MinusLogProbMetric: 26.0081

Epoch 7: val_loss improved from 27.88503 to 26.00806, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 26.1126 - MinusLogProbMetric: 26.1126 - val_loss: 26.0081 - val_MinusLogProbMetric: 26.0081 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 8/1000
2023-09-28 01:53:39.650 
Epoch 8/1000 
	 loss: 25.0911, MinusLogProbMetric: 25.0911, val_loss: 24.3064, val_MinusLogProbMetric: 24.3064

Epoch 8: val_loss improved from 26.00806 to 24.30644, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 66s - loss: 25.0911 - MinusLogProbMetric: 25.0911 - val_loss: 24.3064 - val_MinusLogProbMetric: 24.3064 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 9/1000
2023-09-28 01:54:46.357 
Epoch 9/1000 
	 loss: 24.6631, MinusLogProbMetric: 24.6631, val_loss: 24.4752, val_MinusLogProbMetric: 24.4752

Epoch 9: val_loss did not improve from 24.30644
196/196 - 66s - loss: 24.6631 - MinusLogProbMetric: 24.6631 - val_loss: 24.4752 - val_MinusLogProbMetric: 24.4752 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 10/1000
2023-09-28 01:55:52.516 
Epoch 10/1000 
	 loss: 23.8480, MinusLogProbMetric: 23.8480, val_loss: 23.8994, val_MinusLogProbMetric: 23.8994

Epoch 10: val_loss improved from 24.30644 to 23.89939, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 68s - loss: 23.8480 - MinusLogProbMetric: 23.8480 - val_loss: 23.8994 - val_MinusLogProbMetric: 23.8994 - lr: 3.3333e-04 - 68s/epoch - 345ms/step
Epoch 11/1000
2023-09-28 01:56:59.541 
Epoch 11/1000 
	 loss: 23.4569, MinusLogProbMetric: 23.4569, val_loss: 25.7380, val_MinusLogProbMetric: 25.7380

Epoch 11: val_loss did not improve from 23.89939
196/196 - 66s - loss: 23.4569 - MinusLogProbMetric: 23.4569 - val_loss: 25.7380 - val_MinusLogProbMetric: 25.7380 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 12/1000
2023-09-28 01:58:05.599 
Epoch 12/1000 
	 loss: 23.0594, MinusLogProbMetric: 23.0594, val_loss: 23.3864, val_MinusLogProbMetric: 23.3864

Epoch 12: val_loss improved from 23.89939 to 23.38636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 23.0594 - MinusLogProbMetric: 23.0594 - val_loss: 23.3864 - val_MinusLogProbMetric: 23.3864 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 13/1000
2023-09-28 01:59:12.418 
Epoch 13/1000 
	 loss: 22.8156, MinusLogProbMetric: 22.8156, val_loss: 22.9895, val_MinusLogProbMetric: 22.9895

Epoch 13: val_loss improved from 23.38636 to 22.98945, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 22.8156 - MinusLogProbMetric: 22.8156 - val_loss: 22.9895 - val_MinusLogProbMetric: 22.9895 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 14/1000
2023-09-28 02:00:18.525 
Epoch 14/1000 
	 loss: 22.3020, MinusLogProbMetric: 22.3020, val_loss: 21.8398, val_MinusLogProbMetric: 21.8398

Epoch 14: val_loss improved from 22.98945 to 21.83984, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 66s - loss: 22.3020 - MinusLogProbMetric: 22.3020 - val_loss: 21.8398 - val_MinusLogProbMetric: 21.8398 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 15/1000
2023-09-28 02:01:25.505 
Epoch 15/1000 
	 loss: 22.2216, MinusLogProbMetric: 22.2216, val_loss: 21.4669, val_MinusLogProbMetric: 21.4669

Epoch 15: val_loss improved from 21.83984 to 21.46686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 22.2216 - MinusLogProbMetric: 22.2216 - val_loss: 21.4669 - val_MinusLogProbMetric: 21.4669 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 16/1000
2023-09-28 02:02:32.177 
Epoch 16/1000 
	 loss: 21.9546, MinusLogProbMetric: 21.9546, val_loss: 21.5883, val_MinusLogProbMetric: 21.5883

Epoch 16: val_loss did not improve from 21.46686
196/196 - 66s - loss: 21.9546 - MinusLogProbMetric: 21.9546 - val_loss: 21.5883 - val_MinusLogProbMetric: 21.5883 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 17/1000
2023-09-28 02:03:38.538 
Epoch 17/1000 
	 loss: 21.8253, MinusLogProbMetric: 21.8253, val_loss: 23.5239, val_MinusLogProbMetric: 23.5239

Epoch 17: val_loss did not improve from 21.46686
196/196 - 66s - loss: 21.8253 - MinusLogProbMetric: 21.8253 - val_loss: 23.5239 - val_MinusLogProbMetric: 23.5239 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 18/1000
2023-09-28 02:04:44.322 
Epoch 18/1000 
	 loss: 21.6365, MinusLogProbMetric: 21.6365, val_loss: 22.2339, val_MinusLogProbMetric: 22.2339

Epoch 18: val_loss did not improve from 21.46686
196/196 - 66s - loss: 21.6365 - MinusLogProbMetric: 21.6365 - val_loss: 22.2339 - val_MinusLogProbMetric: 22.2339 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 19/1000
2023-09-28 02:05:49.995 
Epoch 19/1000 
	 loss: 21.4965, MinusLogProbMetric: 21.4965, val_loss: 21.1521, val_MinusLogProbMetric: 21.1521

Epoch 19: val_loss improved from 21.46686 to 21.15206, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 21.4965 - MinusLogProbMetric: 21.4965 - val_loss: 21.1521 - val_MinusLogProbMetric: 21.1521 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 20/1000
2023-09-28 02:06:56.912 
Epoch 20/1000 
	 loss: 21.4199, MinusLogProbMetric: 21.4199, val_loss: 22.1544, val_MinusLogProbMetric: 22.1544

Epoch 20: val_loss did not improve from 21.15206
196/196 - 66s - loss: 21.4199 - MinusLogProbMetric: 21.4199 - val_loss: 22.1544 - val_MinusLogProbMetric: 22.1544 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 21/1000
2023-09-28 02:08:02.404 
Epoch 21/1000 
	 loss: 21.2096, MinusLogProbMetric: 21.2096, val_loss: 21.4446, val_MinusLogProbMetric: 21.4446

Epoch 21: val_loss did not improve from 21.15206
196/196 - 65s - loss: 21.2096 - MinusLogProbMetric: 21.2096 - val_loss: 21.4446 - val_MinusLogProbMetric: 21.4446 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 22/1000
2023-09-28 02:09:08.147 
Epoch 22/1000 
	 loss: 21.0690, MinusLogProbMetric: 21.0690, val_loss: 21.1200, val_MinusLogProbMetric: 21.1200

Epoch 22: val_loss improved from 21.15206 to 21.12003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 21.0690 - MinusLogProbMetric: 21.0690 - val_loss: 21.1200 - val_MinusLogProbMetric: 21.1200 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 23/1000
2023-09-28 02:10:15.317 
Epoch 23/1000 
	 loss: 20.8396, MinusLogProbMetric: 20.8396, val_loss: 21.3574, val_MinusLogProbMetric: 21.3574

Epoch 23: val_loss did not improve from 21.12003
196/196 - 66s - loss: 20.8396 - MinusLogProbMetric: 20.8396 - val_loss: 21.3574 - val_MinusLogProbMetric: 21.3574 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 24/1000
2023-09-28 02:11:21.077 
Epoch 24/1000 
	 loss: 20.9358, MinusLogProbMetric: 20.9358, val_loss: 20.8412, val_MinusLogProbMetric: 20.8412

Epoch 24: val_loss improved from 21.12003 to 20.84119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 20.9358 - MinusLogProbMetric: 20.9358 - val_loss: 20.8412 - val_MinusLogProbMetric: 20.8412 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 25/1000
2023-09-28 02:12:28.399 
Epoch 25/1000 
	 loss: 20.8527, MinusLogProbMetric: 20.8527, val_loss: 20.9411, val_MinusLogProbMetric: 20.9411

Epoch 25: val_loss did not improve from 20.84119
196/196 - 66s - loss: 20.8527 - MinusLogProbMetric: 20.8527 - val_loss: 20.9411 - val_MinusLogProbMetric: 20.9411 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 26/1000
2023-09-28 02:13:34.451 
Epoch 26/1000 
	 loss: 20.7289, MinusLogProbMetric: 20.7289, val_loss: 21.4523, val_MinusLogProbMetric: 21.4523

Epoch 26: val_loss did not improve from 20.84119
196/196 - 66s - loss: 20.7289 - MinusLogProbMetric: 20.7289 - val_loss: 21.4523 - val_MinusLogProbMetric: 21.4523 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 27/1000
2023-09-28 02:14:40.034 
Epoch 27/1000 
	 loss: 20.6470, MinusLogProbMetric: 20.6470, val_loss: 20.7669, val_MinusLogProbMetric: 20.7669

Epoch 27: val_loss improved from 20.84119 to 20.76688, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 20.6470 - MinusLogProbMetric: 20.6470 - val_loss: 20.7669 - val_MinusLogProbMetric: 20.7669 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 28/1000
2023-09-28 02:15:47.339 
Epoch 28/1000 
	 loss: 20.5249, MinusLogProbMetric: 20.5249, val_loss: 21.2309, val_MinusLogProbMetric: 21.2309

Epoch 28: val_loss did not improve from 20.76688
196/196 - 66s - loss: 20.5249 - MinusLogProbMetric: 20.5249 - val_loss: 21.2309 - val_MinusLogProbMetric: 21.2309 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 29/1000
2023-09-28 02:16:53.044 
Epoch 29/1000 
	 loss: 20.4347, MinusLogProbMetric: 20.4347, val_loss: 20.7754, val_MinusLogProbMetric: 20.7754

Epoch 29: val_loss did not improve from 20.76688
196/196 - 66s - loss: 20.4347 - MinusLogProbMetric: 20.4347 - val_loss: 20.7754 - val_MinusLogProbMetric: 20.7754 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 30/1000
2023-09-28 02:17:58.395 
Epoch 30/1000 
	 loss: 20.5638, MinusLogProbMetric: 20.5638, val_loss: 21.3544, val_MinusLogProbMetric: 21.3544

Epoch 30: val_loss did not improve from 20.76688
196/196 - 65s - loss: 20.5638 - MinusLogProbMetric: 20.5638 - val_loss: 21.3544 - val_MinusLogProbMetric: 21.3544 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 31/1000
2023-09-28 02:19:04.037 
Epoch 31/1000 
	 loss: 20.3870, MinusLogProbMetric: 20.3870, val_loss: 20.1470, val_MinusLogProbMetric: 20.1470

Epoch 31: val_loss improved from 20.76688 to 20.14696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 20.3870 - MinusLogProbMetric: 20.3870 - val_loss: 20.1470 - val_MinusLogProbMetric: 20.1470 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 32/1000
2023-09-28 02:20:10.606 
Epoch 32/1000 
	 loss: 20.4328, MinusLogProbMetric: 20.4328, val_loss: 20.0294, val_MinusLogProbMetric: 20.0294

Epoch 32: val_loss improved from 20.14696 to 20.02943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 20.4328 - MinusLogProbMetric: 20.4328 - val_loss: 20.0294 - val_MinusLogProbMetric: 20.0294 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 33/1000
2023-09-28 02:21:17.210 
Epoch 33/1000 
	 loss: 20.3697, MinusLogProbMetric: 20.3697, val_loss: 21.7151, val_MinusLogProbMetric: 21.7151

Epoch 33: val_loss did not improve from 20.02943
196/196 - 65s - loss: 20.3697 - MinusLogProbMetric: 20.3697 - val_loss: 21.7151 - val_MinusLogProbMetric: 21.7151 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 34/1000
2023-09-28 02:22:22.858 
Epoch 34/1000 
	 loss: 20.2993, MinusLogProbMetric: 20.2993, val_loss: 23.3546, val_MinusLogProbMetric: 23.3546

Epoch 34: val_loss did not improve from 20.02943
196/196 - 66s - loss: 20.2993 - MinusLogProbMetric: 20.2993 - val_loss: 23.3546 - val_MinusLogProbMetric: 23.3546 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 35/1000
2023-09-28 02:23:28.397 
Epoch 35/1000 
	 loss: 20.1667, MinusLogProbMetric: 20.1667, val_loss: 19.8102, val_MinusLogProbMetric: 19.8102

Epoch 35: val_loss improved from 20.02943 to 19.81024, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 20.1667 - MinusLogProbMetric: 20.1667 - val_loss: 19.8102 - val_MinusLogProbMetric: 19.8102 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 36/1000
2023-09-28 02:24:34.954 
Epoch 36/1000 
	 loss: 20.1770, MinusLogProbMetric: 20.1770, val_loss: 20.1315, val_MinusLogProbMetric: 20.1315

Epoch 36: val_loss did not improve from 19.81024
196/196 - 66s - loss: 20.1770 - MinusLogProbMetric: 20.1770 - val_loss: 20.1315 - val_MinusLogProbMetric: 20.1315 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 37/1000
2023-09-28 02:25:40.422 
Epoch 37/1000 
	 loss: 20.0133, MinusLogProbMetric: 20.0133, val_loss: 19.7011, val_MinusLogProbMetric: 19.7011

Epoch 37: val_loss improved from 19.81024 to 19.70108, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 66s - loss: 20.0133 - MinusLogProbMetric: 20.0133 - val_loss: 19.7011 - val_MinusLogProbMetric: 19.7011 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 38/1000
2023-09-28 02:26:46.415 
Epoch 38/1000 
	 loss: 20.2161, MinusLogProbMetric: 20.2161, val_loss: 20.7424, val_MinusLogProbMetric: 20.7424

Epoch 38: val_loss did not improve from 19.70108
196/196 - 65s - loss: 20.2161 - MinusLogProbMetric: 20.2161 - val_loss: 20.7424 - val_MinusLogProbMetric: 20.7424 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 39/1000
2023-09-28 02:27:52.095 
Epoch 39/1000 
	 loss: 20.0015, MinusLogProbMetric: 20.0015, val_loss: 20.0283, val_MinusLogProbMetric: 20.0283

Epoch 39: val_loss did not improve from 19.70108
196/196 - 66s - loss: 20.0015 - MinusLogProbMetric: 20.0015 - val_loss: 20.0283 - val_MinusLogProbMetric: 20.0283 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 40/1000
2023-09-28 02:28:57.215 
Epoch 40/1000 
	 loss: 19.9060, MinusLogProbMetric: 19.9060, val_loss: 20.2935, val_MinusLogProbMetric: 20.2935

Epoch 40: val_loss did not improve from 19.70108
196/196 - 65s - loss: 19.9060 - MinusLogProbMetric: 19.9060 - val_loss: 20.2935 - val_MinusLogProbMetric: 20.2935 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 41/1000
2023-09-28 02:30:03.004 
Epoch 41/1000 
	 loss: 19.8208, MinusLogProbMetric: 19.8208, val_loss: 19.4147, val_MinusLogProbMetric: 19.4147

Epoch 41: val_loss improved from 19.70108 to 19.41466, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 19.8208 - MinusLogProbMetric: 19.8208 - val_loss: 19.4147 - val_MinusLogProbMetric: 19.4147 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 42/1000
2023-09-28 02:31:09.760 
Epoch 42/1000 
	 loss: 19.8514, MinusLogProbMetric: 19.8514, val_loss: 19.0206, val_MinusLogProbMetric: 19.0206

Epoch 42: val_loss improved from 19.41466 to 19.02063, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 19.8514 - MinusLogProbMetric: 19.8514 - val_loss: 19.0206 - val_MinusLogProbMetric: 19.0206 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 43/1000
2023-09-28 02:32:15.867 
Epoch 43/1000 
	 loss: 19.7064, MinusLogProbMetric: 19.7064, val_loss: 20.4262, val_MinusLogProbMetric: 20.4262

Epoch 43: val_loss did not improve from 19.02063
196/196 - 65s - loss: 19.7064 - MinusLogProbMetric: 19.7064 - val_loss: 20.4262 - val_MinusLogProbMetric: 20.4262 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 44/1000
2023-09-28 02:33:21.828 
Epoch 44/1000 
	 loss: 19.8370, MinusLogProbMetric: 19.8370, val_loss: 21.3111, val_MinusLogProbMetric: 21.3111

Epoch 44: val_loss did not improve from 19.02063
196/196 - 66s - loss: 19.8370 - MinusLogProbMetric: 19.8370 - val_loss: 21.3111 - val_MinusLogProbMetric: 21.3111 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 45/1000
2023-09-28 02:34:27.680 
Epoch 45/1000 
	 loss: 19.7524, MinusLogProbMetric: 19.7524, val_loss: 19.7243, val_MinusLogProbMetric: 19.7243

Epoch 45: val_loss did not improve from 19.02063
196/196 - 66s - loss: 19.7524 - MinusLogProbMetric: 19.7524 - val_loss: 19.7243 - val_MinusLogProbMetric: 19.7243 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 46/1000
2023-09-28 02:35:33.475 
Epoch 46/1000 
	 loss: 19.7554, MinusLogProbMetric: 19.7554, val_loss: 22.6438, val_MinusLogProbMetric: 22.6438

Epoch 46: val_loss did not improve from 19.02063
196/196 - 66s - loss: 19.7554 - MinusLogProbMetric: 19.7554 - val_loss: 22.6438 - val_MinusLogProbMetric: 22.6438 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 47/1000
2023-09-28 02:36:38.639 
Epoch 47/1000 
	 loss: 19.6108, MinusLogProbMetric: 19.6108, val_loss: 19.8132, val_MinusLogProbMetric: 19.8132

Epoch 47: val_loss did not improve from 19.02063
196/196 - 65s - loss: 19.6108 - MinusLogProbMetric: 19.6108 - val_loss: 19.8132 - val_MinusLogProbMetric: 19.8132 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 48/1000
2023-09-28 02:37:44.314 
Epoch 48/1000 
	 loss: 19.4961, MinusLogProbMetric: 19.4961, val_loss: 20.0075, val_MinusLogProbMetric: 20.0075

Epoch 48: val_loss did not improve from 19.02063
196/196 - 66s - loss: 19.4961 - MinusLogProbMetric: 19.4961 - val_loss: 20.0075 - val_MinusLogProbMetric: 20.0075 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 49/1000
2023-09-28 02:38:50.706 
Epoch 49/1000 
	 loss: 19.5643, MinusLogProbMetric: 19.5643, val_loss: 19.5685, val_MinusLogProbMetric: 19.5685

Epoch 49: val_loss did not improve from 19.02063
196/196 - 66s - loss: 19.5643 - MinusLogProbMetric: 19.5643 - val_loss: 19.5685 - val_MinusLogProbMetric: 19.5685 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 50/1000
2023-09-28 02:39:56.531 
Epoch 50/1000 
	 loss: 19.5604, MinusLogProbMetric: 19.5604, val_loss: 19.6589, val_MinusLogProbMetric: 19.6589

Epoch 50: val_loss did not improve from 19.02063
196/196 - 66s - loss: 19.5604 - MinusLogProbMetric: 19.5604 - val_loss: 19.6589 - val_MinusLogProbMetric: 19.6589 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 51/1000
2023-09-28 02:41:02.395 
Epoch 51/1000 
	 loss: 19.9731, MinusLogProbMetric: 19.9731, val_loss: 19.1867, val_MinusLogProbMetric: 19.1867

Epoch 51: val_loss did not improve from 19.02063
196/196 - 66s - loss: 19.9731 - MinusLogProbMetric: 19.9731 - val_loss: 19.1867 - val_MinusLogProbMetric: 19.1867 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 52/1000
2023-09-28 02:42:08.900 
Epoch 52/1000 
	 loss: 19.5410, MinusLogProbMetric: 19.5410, val_loss: 19.8593, val_MinusLogProbMetric: 19.8593

Epoch 52: val_loss did not improve from 19.02063
196/196 - 67s - loss: 19.5410 - MinusLogProbMetric: 19.5410 - val_loss: 19.8593 - val_MinusLogProbMetric: 19.8593 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 53/1000
2023-09-28 02:43:14.090 
Epoch 53/1000 
	 loss: 19.3766, MinusLogProbMetric: 19.3766, val_loss: 21.2274, val_MinusLogProbMetric: 21.2274

Epoch 53: val_loss did not improve from 19.02063
196/196 - 65s - loss: 19.3766 - MinusLogProbMetric: 19.3766 - val_loss: 21.2274 - val_MinusLogProbMetric: 21.2274 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 54/1000
2023-09-28 02:44:19.566 
Epoch 54/1000 
	 loss: 19.5548, MinusLogProbMetric: 19.5548, val_loss: 19.6850, val_MinusLogProbMetric: 19.6850

Epoch 54: val_loss did not improve from 19.02063
196/196 - 65s - loss: 19.5548 - MinusLogProbMetric: 19.5548 - val_loss: 19.6850 - val_MinusLogProbMetric: 19.6850 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 55/1000
2023-09-28 02:45:25.095 
Epoch 55/1000 
	 loss: 19.3844, MinusLogProbMetric: 19.3844, val_loss: 19.4142, val_MinusLogProbMetric: 19.4142

Epoch 55: val_loss did not improve from 19.02063
196/196 - 66s - loss: 19.3844 - MinusLogProbMetric: 19.3844 - val_loss: 19.4142 - val_MinusLogProbMetric: 19.4142 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 56/1000
2023-09-28 02:46:30.677 
Epoch 56/1000 
	 loss: 19.2055, MinusLogProbMetric: 19.2055, val_loss: 19.7527, val_MinusLogProbMetric: 19.7527

Epoch 56: val_loss did not improve from 19.02063
196/196 - 66s - loss: 19.2055 - MinusLogProbMetric: 19.2055 - val_loss: 19.7527 - val_MinusLogProbMetric: 19.7527 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 57/1000
2023-09-28 02:47:36.120 
Epoch 57/1000 
	 loss: 19.7324, MinusLogProbMetric: 19.7324, val_loss: 20.3852, val_MinusLogProbMetric: 20.3852

Epoch 57: val_loss did not improve from 19.02063
196/196 - 65s - loss: 19.7324 - MinusLogProbMetric: 19.7324 - val_loss: 20.3852 - val_MinusLogProbMetric: 20.3852 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 58/1000
2023-09-28 02:48:42.028 
Epoch 58/1000 
	 loss: 19.2657, MinusLogProbMetric: 19.2657, val_loss: 19.7986, val_MinusLogProbMetric: 19.7986

Epoch 58: val_loss did not improve from 19.02063
196/196 - 66s - loss: 19.2657 - MinusLogProbMetric: 19.2657 - val_loss: 19.7986 - val_MinusLogProbMetric: 19.7986 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 59/1000
2023-09-28 02:49:48.181 
Epoch 59/1000 
	 loss: 19.4041, MinusLogProbMetric: 19.4041, val_loss: 18.9483, val_MinusLogProbMetric: 18.9483

Epoch 59: val_loss improved from 19.02063 to 18.94826, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 19.4041 - MinusLogProbMetric: 19.4041 - val_loss: 18.9483 - val_MinusLogProbMetric: 18.9483 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 60/1000
2023-09-28 02:50:54.816 
Epoch 60/1000 
	 loss: 19.3963, MinusLogProbMetric: 19.3963, val_loss: 19.9752, val_MinusLogProbMetric: 19.9752

Epoch 60: val_loss did not improve from 18.94826
196/196 - 66s - loss: 19.3963 - MinusLogProbMetric: 19.3963 - val_loss: 19.9752 - val_MinusLogProbMetric: 19.9752 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 61/1000
2023-09-28 02:52:00.493 
Epoch 61/1000 
	 loss: 19.2332, MinusLogProbMetric: 19.2332, val_loss: 19.0059, val_MinusLogProbMetric: 19.0059

Epoch 61: val_loss did not improve from 18.94826
196/196 - 66s - loss: 19.2332 - MinusLogProbMetric: 19.2332 - val_loss: 19.0059 - val_MinusLogProbMetric: 19.0059 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 62/1000
2023-09-28 02:53:05.678 
Epoch 62/1000 
	 loss: 19.2903, MinusLogProbMetric: 19.2903, val_loss: 19.3440, val_MinusLogProbMetric: 19.3440

Epoch 62: val_loss did not improve from 18.94826
196/196 - 65s - loss: 19.2903 - MinusLogProbMetric: 19.2903 - val_loss: 19.3440 - val_MinusLogProbMetric: 19.3440 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 63/1000
2023-09-28 02:54:11.284 
Epoch 63/1000 
	 loss: 19.2013, MinusLogProbMetric: 19.2013, val_loss: 19.1793, val_MinusLogProbMetric: 19.1793

Epoch 63: val_loss did not improve from 18.94826
196/196 - 66s - loss: 19.2013 - MinusLogProbMetric: 19.2013 - val_loss: 19.1793 - val_MinusLogProbMetric: 19.1793 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 64/1000
2023-09-28 02:55:16.467 
Epoch 64/1000 
	 loss: 19.1554, MinusLogProbMetric: 19.1554, val_loss: 18.9802, val_MinusLogProbMetric: 18.9802

Epoch 64: val_loss did not improve from 18.94826
196/196 - 65s - loss: 19.1554 - MinusLogProbMetric: 19.1554 - val_loss: 18.9802 - val_MinusLogProbMetric: 18.9802 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 65/1000
2023-09-28 02:56:21.241 
Epoch 65/1000 
	 loss: 19.2424, MinusLogProbMetric: 19.2424, val_loss: 20.2622, val_MinusLogProbMetric: 20.2622

Epoch 65: val_loss did not improve from 18.94826
196/196 - 65s - loss: 19.2424 - MinusLogProbMetric: 19.2424 - val_loss: 20.2622 - val_MinusLogProbMetric: 20.2622 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 66/1000
2023-09-28 02:57:26.760 
Epoch 66/1000 
	 loss: 19.2670, MinusLogProbMetric: 19.2670, val_loss: 19.3119, val_MinusLogProbMetric: 19.3119

Epoch 66: val_loss did not improve from 18.94826
196/196 - 66s - loss: 19.2670 - MinusLogProbMetric: 19.2670 - val_loss: 19.3119 - val_MinusLogProbMetric: 19.3119 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 67/1000
2023-09-28 02:58:32.698 
Epoch 67/1000 
	 loss: 19.0304, MinusLogProbMetric: 19.0304, val_loss: 18.6675, val_MinusLogProbMetric: 18.6675

Epoch 67: val_loss improved from 18.94826 to 18.66750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 19.0304 - MinusLogProbMetric: 19.0304 - val_loss: 18.6675 - val_MinusLogProbMetric: 18.6675 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 68/1000
2023-09-28 02:59:39.518 
Epoch 68/1000 
	 loss: 19.0790, MinusLogProbMetric: 19.0790, val_loss: 18.5294, val_MinusLogProbMetric: 18.5294

Epoch 68: val_loss improved from 18.66750 to 18.52938, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 19.0790 - MinusLogProbMetric: 19.0790 - val_loss: 18.5294 - val_MinusLogProbMetric: 18.5294 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 69/1000
2023-09-28 03:00:46.116 
Epoch 69/1000 
	 loss: 19.1374, MinusLogProbMetric: 19.1374, val_loss: 19.2281, val_MinusLogProbMetric: 19.2281

Epoch 69: val_loss did not improve from 18.52938
196/196 - 66s - loss: 19.1374 - MinusLogProbMetric: 19.1374 - val_loss: 19.2281 - val_MinusLogProbMetric: 19.2281 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 70/1000
2023-09-28 03:01:51.227 
Epoch 70/1000 
	 loss: 18.9647, MinusLogProbMetric: 18.9647, val_loss: 19.1611, val_MinusLogProbMetric: 19.1611

Epoch 70: val_loss did not improve from 18.52938
196/196 - 65s - loss: 18.9647 - MinusLogProbMetric: 18.9647 - val_loss: 19.1611 - val_MinusLogProbMetric: 19.1611 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 71/1000
2023-09-28 03:02:56.536 
Epoch 71/1000 
	 loss: 19.1214, MinusLogProbMetric: 19.1214, val_loss: 19.4051, val_MinusLogProbMetric: 19.4051

Epoch 71: val_loss did not improve from 18.52938
196/196 - 65s - loss: 19.1214 - MinusLogProbMetric: 19.1214 - val_loss: 19.4051 - val_MinusLogProbMetric: 19.4051 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 72/1000
2023-09-28 03:04:02.165 
Epoch 72/1000 
	 loss: 18.8816, MinusLogProbMetric: 18.8816, val_loss: 19.2959, val_MinusLogProbMetric: 19.2959

Epoch 72: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.8816 - MinusLogProbMetric: 18.8816 - val_loss: 19.2959 - val_MinusLogProbMetric: 19.2959 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 73/1000
2023-09-28 03:05:07.882 
Epoch 73/1000 
	 loss: 19.0473, MinusLogProbMetric: 19.0473, val_loss: 19.2707, val_MinusLogProbMetric: 19.2707

Epoch 73: val_loss did not improve from 18.52938
196/196 - 66s - loss: 19.0473 - MinusLogProbMetric: 19.0473 - val_loss: 19.2707 - val_MinusLogProbMetric: 19.2707 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 74/1000
2023-09-28 03:06:13.369 
Epoch 74/1000 
	 loss: 18.8768, MinusLogProbMetric: 18.8768, val_loss: 19.6801, val_MinusLogProbMetric: 19.6801

Epoch 74: val_loss did not improve from 18.52938
196/196 - 65s - loss: 18.8768 - MinusLogProbMetric: 18.8768 - val_loss: 19.6801 - val_MinusLogProbMetric: 19.6801 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 75/1000
2023-09-28 03:07:19.763 
Epoch 75/1000 
	 loss: 19.1893, MinusLogProbMetric: 19.1893, val_loss: 19.0284, val_MinusLogProbMetric: 19.0284

Epoch 75: val_loss did not improve from 18.52938
196/196 - 66s - loss: 19.1893 - MinusLogProbMetric: 19.1893 - val_loss: 19.0284 - val_MinusLogProbMetric: 19.0284 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 76/1000
2023-09-28 03:08:25.564 
Epoch 76/1000 
	 loss: 18.9581, MinusLogProbMetric: 18.9581, val_loss: 19.3789, val_MinusLogProbMetric: 19.3789

Epoch 76: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.9581 - MinusLogProbMetric: 18.9581 - val_loss: 19.3789 - val_MinusLogProbMetric: 19.3789 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 77/1000
2023-09-28 03:09:31.315 
Epoch 77/1000 
	 loss: 18.8912, MinusLogProbMetric: 18.8912, val_loss: 18.9377, val_MinusLogProbMetric: 18.9377

Epoch 77: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.8912 - MinusLogProbMetric: 18.8912 - val_loss: 18.9377 - val_MinusLogProbMetric: 18.9377 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 78/1000
2023-09-28 03:10:36.694 
Epoch 78/1000 
	 loss: 18.8137, MinusLogProbMetric: 18.8137, val_loss: 19.6619, val_MinusLogProbMetric: 19.6619

Epoch 78: val_loss did not improve from 18.52938
196/196 - 65s - loss: 18.8137 - MinusLogProbMetric: 18.8137 - val_loss: 19.6619 - val_MinusLogProbMetric: 19.6619 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 79/1000
2023-09-28 03:11:42.687 
Epoch 79/1000 
	 loss: 18.9785, MinusLogProbMetric: 18.9785, val_loss: 19.0937, val_MinusLogProbMetric: 19.0937

Epoch 79: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.9785 - MinusLogProbMetric: 18.9785 - val_loss: 19.0937 - val_MinusLogProbMetric: 19.0937 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 80/1000
2023-09-28 03:12:48.704 
Epoch 80/1000 
	 loss: 18.8346, MinusLogProbMetric: 18.8346, val_loss: 18.9501, val_MinusLogProbMetric: 18.9501

Epoch 80: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.8346 - MinusLogProbMetric: 18.8346 - val_loss: 18.9501 - val_MinusLogProbMetric: 18.9501 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 81/1000
2023-09-28 03:13:54.316 
Epoch 81/1000 
	 loss: 18.9229, MinusLogProbMetric: 18.9229, val_loss: 18.7900, val_MinusLogProbMetric: 18.7900

Epoch 81: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.9229 - MinusLogProbMetric: 18.9229 - val_loss: 18.7900 - val_MinusLogProbMetric: 18.7900 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 82/1000
2023-09-28 03:15:00.202 
Epoch 82/1000 
	 loss: 18.8453, MinusLogProbMetric: 18.8453, val_loss: 18.8419, val_MinusLogProbMetric: 18.8419

Epoch 82: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.8453 - MinusLogProbMetric: 18.8453 - val_loss: 18.8419 - val_MinusLogProbMetric: 18.8419 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 83/1000
2023-09-28 03:16:06.089 
Epoch 83/1000 
	 loss: 18.8969, MinusLogProbMetric: 18.8969, val_loss: 18.8442, val_MinusLogProbMetric: 18.8442

Epoch 83: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.8969 - MinusLogProbMetric: 18.8969 - val_loss: 18.8442 - val_MinusLogProbMetric: 18.8442 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 84/1000
2023-09-28 03:17:11.873 
Epoch 84/1000 
	 loss: 18.8450, MinusLogProbMetric: 18.8450, val_loss: 18.8304, val_MinusLogProbMetric: 18.8304

Epoch 84: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.8450 - MinusLogProbMetric: 18.8450 - val_loss: 18.8304 - val_MinusLogProbMetric: 18.8304 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 85/1000
2023-09-28 03:18:17.588 
Epoch 85/1000 
	 loss: 18.9264, MinusLogProbMetric: 18.9264, val_loss: 18.8925, val_MinusLogProbMetric: 18.8925

Epoch 85: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.9264 - MinusLogProbMetric: 18.9264 - val_loss: 18.8925 - val_MinusLogProbMetric: 18.8925 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 86/1000
2023-09-28 03:19:23.106 
Epoch 86/1000 
	 loss: 18.8964, MinusLogProbMetric: 18.8964, val_loss: 18.9511, val_MinusLogProbMetric: 18.9511

Epoch 86: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.8964 - MinusLogProbMetric: 18.8964 - val_loss: 18.9511 - val_MinusLogProbMetric: 18.9511 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 87/1000
2023-09-28 03:20:28.878 
Epoch 87/1000 
	 loss: 18.8794, MinusLogProbMetric: 18.8794, val_loss: 21.5208, val_MinusLogProbMetric: 21.5208

Epoch 87: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.8794 - MinusLogProbMetric: 18.8794 - val_loss: 21.5208 - val_MinusLogProbMetric: 21.5208 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 88/1000
2023-09-28 03:21:35.280 
Epoch 88/1000 
	 loss: 18.8158, MinusLogProbMetric: 18.8158, val_loss: 19.1379, val_MinusLogProbMetric: 19.1379

Epoch 88: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.8158 - MinusLogProbMetric: 18.8158 - val_loss: 19.1379 - val_MinusLogProbMetric: 19.1379 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 89/1000
2023-09-28 03:22:41.358 
Epoch 89/1000 
	 loss: 18.8363, MinusLogProbMetric: 18.8363, val_loss: 19.3584, val_MinusLogProbMetric: 19.3584

Epoch 89: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.8363 - MinusLogProbMetric: 18.8363 - val_loss: 19.3584 - val_MinusLogProbMetric: 19.3584 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 90/1000
2023-09-28 03:23:47.288 
Epoch 90/1000 
	 loss: 18.7810, MinusLogProbMetric: 18.7810, val_loss: 18.9860, val_MinusLogProbMetric: 18.9860

Epoch 90: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.7810 - MinusLogProbMetric: 18.7810 - val_loss: 18.9860 - val_MinusLogProbMetric: 18.9860 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 91/1000
2023-09-28 03:24:53.140 
Epoch 91/1000 
	 loss: 18.7162, MinusLogProbMetric: 18.7162, val_loss: 21.2970, val_MinusLogProbMetric: 21.2970

Epoch 91: val_loss did not improve from 18.52938
196/196 - 66s - loss: 18.7162 - MinusLogProbMetric: 18.7162 - val_loss: 21.2970 - val_MinusLogProbMetric: 21.2970 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 92/1000
2023-09-28 03:25:58.812 
Epoch 92/1000 
	 loss: 18.7476, MinusLogProbMetric: 18.7476, val_loss: 18.3920, val_MinusLogProbMetric: 18.3920

Epoch 92: val_loss improved from 18.52938 to 18.39198, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 18.7476 - MinusLogProbMetric: 18.7476 - val_loss: 18.3920 - val_MinusLogProbMetric: 18.3920 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 93/1000
2023-09-28 03:27:05.582 
Epoch 93/1000 
	 loss: 18.7924, MinusLogProbMetric: 18.7924, val_loss: 18.7135, val_MinusLogProbMetric: 18.7135

Epoch 93: val_loss did not improve from 18.39198
196/196 - 66s - loss: 18.7924 - MinusLogProbMetric: 18.7924 - val_loss: 18.7135 - val_MinusLogProbMetric: 18.7135 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 94/1000
2023-09-28 03:28:10.436 
Epoch 94/1000 
	 loss: 19.0052, MinusLogProbMetric: 19.0052, val_loss: 18.9379, val_MinusLogProbMetric: 18.9379

Epoch 94: val_loss did not improve from 18.39198
196/196 - 65s - loss: 19.0052 - MinusLogProbMetric: 19.0052 - val_loss: 18.9379 - val_MinusLogProbMetric: 18.9379 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 95/1000
2023-09-28 03:29:16.218 
Epoch 95/1000 
	 loss: 18.7463, MinusLogProbMetric: 18.7463, val_loss: 18.6656, val_MinusLogProbMetric: 18.6656

Epoch 95: val_loss did not improve from 18.39198
196/196 - 66s - loss: 18.7463 - MinusLogProbMetric: 18.7463 - val_loss: 18.6656 - val_MinusLogProbMetric: 18.6656 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 96/1000
2023-09-28 03:30:21.675 
Epoch 96/1000 
	 loss: 18.8471, MinusLogProbMetric: 18.8471, val_loss: 19.8299, val_MinusLogProbMetric: 19.8299

Epoch 96: val_loss did not improve from 18.39198
196/196 - 65s - loss: 18.8471 - MinusLogProbMetric: 18.8471 - val_loss: 19.8299 - val_MinusLogProbMetric: 19.8299 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 97/1000
2023-09-28 03:31:27.678 
Epoch 97/1000 
	 loss: 18.6602, MinusLogProbMetric: 18.6602, val_loss: 20.5659, val_MinusLogProbMetric: 20.5659

Epoch 97: val_loss did not improve from 18.39198
196/196 - 66s - loss: 18.6602 - MinusLogProbMetric: 18.6602 - val_loss: 20.5659 - val_MinusLogProbMetric: 20.5659 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 98/1000
2023-09-28 03:32:32.833 
Epoch 98/1000 
	 loss: 18.7199, MinusLogProbMetric: 18.7199, val_loss: 18.6213, val_MinusLogProbMetric: 18.6213

Epoch 98: val_loss did not improve from 18.39198
196/196 - 65s - loss: 18.7199 - MinusLogProbMetric: 18.7199 - val_loss: 18.6213 - val_MinusLogProbMetric: 18.6213 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 99/1000
2023-09-28 03:33:38.498 
Epoch 99/1000 
	 loss: 18.5864, MinusLogProbMetric: 18.5864, val_loss: 18.5095, val_MinusLogProbMetric: 18.5095

Epoch 99: val_loss did not improve from 18.39198
196/196 - 66s - loss: 18.5864 - MinusLogProbMetric: 18.5864 - val_loss: 18.5095 - val_MinusLogProbMetric: 18.5095 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 100/1000
2023-09-28 03:34:44.383 
Epoch 100/1000 
	 loss: 18.5651, MinusLogProbMetric: 18.5651, val_loss: 18.7920, val_MinusLogProbMetric: 18.7920

Epoch 100: val_loss did not improve from 18.39198
196/196 - 66s - loss: 18.5651 - MinusLogProbMetric: 18.5651 - val_loss: 18.7920 - val_MinusLogProbMetric: 18.7920 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 101/1000
2023-09-28 03:35:49.929 
Epoch 101/1000 
	 loss: 18.5370, MinusLogProbMetric: 18.5370, val_loss: 19.1656, val_MinusLogProbMetric: 19.1656

Epoch 101: val_loss did not improve from 18.39198
196/196 - 66s - loss: 18.5370 - MinusLogProbMetric: 18.5370 - val_loss: 19.1656 - val_MinusLogProbMetric: 19.1656 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 102/1000
2023-09-28 03:36:55.287 
Epoch 102/1000 
	 loss: 18.5440, MinusLogProbMetric: 18.5440, val_loss: 20.7855, val_MinusLogProbMetric: 20.7855

Epoch 102: val_loss did not improve from 18.39198
196/196 - 65s - loss: 18.5440 - MinusLogProbMetric: 18.5440 - val_loss: 20.7855 - val_MinusLogProbMetric: 20.7855 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 103/1000
2023-09-28 03:38:00.680 
Epoch 103/1000 
	 loss: 18.5416, MinusLogProbMetric: 18.5416, val_loss: 18.6612, val_MinusLogProbMetric: 18.6612

Epoch 103: val_loss did not improve from 18.39198
196/196 - 65s - loss: 18.5416 - MinusLogProbMetric: 18.5416 - val_loss: 18.6612 - val_MinusLogProbMetric: 18.6612 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 104/1000
2023-09-28 03:39:06.686 
Epoch 104/1000 
	 loss: 18.5203, MinusLogProbMetric: 18.5203, val_loss: 19.1720, val_MinusLogProbMetric: 19.1720

Epoch 104: val_loss did not improve from 18.39198
196/196 - 66s - loss: 18.5203 - MinusLogProbMetric: 18.5203 - val_loss: 19.1720 - val_MinusLogProbMetric: 19.1720 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 105/1000
2023-09-28 03:40:12.353 
Epoch 105/1000 
	 loss: 18.5501, MinusLogProbMetric: 18.5501, val_loss: 19.6574, val_MinusLogProbMetric: 19.6574

Epoch 105: val_loss did not improve from 18.39198
196/196 - 66s - loss: 18.5501 - MinusLogProbMetric: 18.5501 - val_loss: 19.6574 - val_MinusLogProbMetric: 19.6574 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 106/1000
2023-09-28 03:41:17.815 
Epoch 106/1000 
	 loss: 18.6886, MinusLogProbMetric: 18.6886, val_loss: 19.0395, val_MinusLogProbMetric: 19.0395

Epoch 106: val_loss did not improve from 18.39198
196/196 - 65s - loss: 18.6886 - MinusLogProbMetric: 18.6886 - val_loss: 19.0395 - val_MinusLogProbMetric: 19.0395 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 107/1000
2023-09-28 03:42:23.706 
Epoch 107/1000 
	 loss: 18.7453, MinusLogProbMetric: 18.7453, val_loss: 19.1659, val_MinusLogProbMetric: 19.1659

Epoch 107: val_loss did not improve from 18.39198
196/196 - 66s - loss: 18.7453 - MinusLogProbMetric: 18.7453 - val_loss: 19.1659 - val_MinusLogProbMetric: 19.1659 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 108/1000
2023-09-28 03:43:29.414 
Epoch 108/1000 
	 loss: 18.5559, MinusLogProbMetric: 18.5559, val_loss: 19.1390, val_MinusLogProbMetric: 19.1390

Epoch 108: val_loss did not improve from 18.39198
196/196 - 66s - loss: 18.5559 - MinusLogProbMetric: 18.5559 - val_loss: 19.1390 - val_MinusLogProbMetric: 19.1390 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 109/1000
2023-09-28 03:44:35.413 
Epoch 109/1000 
	 loss: 18.4753, MinusLogProbMetric: 18.4753, val_loss: 18.7002, val_MinusLogProbMetric: 18.7002

Epoch 109: val_loss did not improve from 18.39198
196/196 - 66s - loss: 18.4753 - MinusLogProbMetric: 18.4753 - val_loss: 18.7002 - val_MinusLogProbMetric: 18.7002 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 110/1000
2023-09-28 03:45:40.926 
Epoch 110/1000 
	 loss: 18.4710, MinusLogProbMetric: 18.4710, val_loss: 18.3270, val_MinusLogProbMetric: 18.3270

Epoch 110: val_loss improved from 18.39198 to 18.32697, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 66s - loss: 18.4710 - MinusLogProbMetric: 18.4710 - val_loss: 18.3270 - val_MinusLogProbMetric: 18.3270 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 111/1000
2023-09-28 03:46:47.565 
Epoch 111/1000 
	 loss: 18.5789, MinusLogProbMetric: 18.5789, val_loss: 18.3545, val_MinusLogProbMetric: 18.3545

Epoch 111: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.5789 - MinusLogProbMetric: 18.5789 - val_loss: 18.3545 - val_MinusLogProbMetric: 18.3545 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 112/1000
2023-09-28 03:47:53.300 
Epoch 112/1000 
	 loss: 18.5958, MinusLogProbMetric: 18.5958, val_loss: 18.6302, val_MinusLogProbMetric: 18.6302

Epoch 112: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.5958 - MinusLogProbMetric: 18.5958 - val_loss: 18.6302 - val_MinusLogProbMetric: 18.6302 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 113/1000
2023-09-28 03:48:59.359 
Epoch 113/1000 
	 loss: 18.6659, MinusLogProbMetric: 18.6659, val_loss: 18.4799, val_MinusLogProbMetric: 18.4799

Epoch 113: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.6659 - MinusLogProbMetric: 18.6659 - val_loss: 18.4799 - val_MinusLogProbMetric: 18.4799 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 114/1000
2023-09-28 03:50:05.024 
Epoch 114/1000 
	 loss: 18.4907, MinusLogProbMetric: 18.4907, val_loss: 18.5793, val_MinusLogProbMetric: 18.5793

Epoch 114: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.4907 - MinusLogProbMetric: 18.4907 - val_loss: 18.5793 - val_MinusLogProbMetric: 18.5793 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 115/1000
2023-09-28 03:51:10.891 
Epoch 115/1000 
	 loss: 18.4516, MinusLogProbMetric: 18.4516, val_loss: 18.9196, val_MinusLogProbMetric: 18.9196

Epoch 115: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.4516 - MinusLogProbMetric: 18.4516 - val_loss: 18.9196 - val_MinusLogProbMetric: 18.9196 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 116/1000
2023-09-28 03:52:16.418 
Epoch 116/1000 
	 loss: 18.5096, MinusLogProbMetric: 18.5096, val_loss: 18.3282, val_MinusLogProbMetric: 18.3282

Epoch 116: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.5096 - MinusLogProbMetric: 18.5096 - val_loss: 18.3282 - val_MinusLogProbMetric: 18.3282 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 117/1000
2023-09-28 03:53:22.160 
Epoch 117/1000 
	 loss: 18.5107, MinusLogProbMetric: 18.5107, val_loss: 19.6680, val_MinusLogProbMetric: 19.6680

Epoch 117: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.5107 - MinusLogProbMetric: 18.5107 - val_loss: 19.6680 - val_MinusLogProbMetric: 19.6680 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 118/1000
2023-09-28 03:54:27.948 
Epoch 118/1000 
	 loss: 18.3994, MinusLogProbMetric: 18.3994, val_loss: 18.3949, val_MinusLogProbMetric: 18.3949

Epoch 118: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.3994 - MinusLogProbMetric: 18.3994 - val_loss: 18.3949 - val_MinusLogProbMetric: 18.3949 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 119/1000
2023-09-28 03:55:33.834 
Epoch 119/1000 
	 loss: 18.4096, MinusLogProbMetric: 18.4096, val_loss: 18.6589, val_MinusLogProbMetric: 18.6589

Epoch 119: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.4096 - MinusLogProbMetric: 18.4096 - val_loss: 18.6589 - val_MinusLogProbMetric: 18.6589 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 120/1000
2023-09-28 03:56:39.634 
Epoch 120/1000 
	 loss: 18.3404, MinusLogProbMetric: 18.3404, val_loss: 18.4628, val_MinusLogProbMetric: 18.4628

Epoch 120: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.3404 - MinusLogProbMetric: 18.3404 - val_loss: 18.4628 - val_MinusLogProbMetric: 18.4628 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 121/1000
2023-09-28 03:57:45.431 
Epoch 121/1000 
	 loss: 18.4707, MinusLogProbMetric: 18.4707, val_loss: 18.9576, val_MinusLogProbMetric: 18.9576

Epoch 121: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.4707 - MinusLogProbMetric: 18.4707 - val_loss: 18.9576 - val_MinusLogProbMetric: 18.9576 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 122/1000
2023-09-28 03:58:51.286 
Epoch 122/1000 
	 loss: 18.3194, MinusLogProbMetric: 18.3194, val_loss: 19.2384, val_MinusLogProbMetric: 19.2384

Epoch 122: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.3194 - MinusLogProbMetric: 18.3194 - val_loss: 19.2384 - val_MinusLogProbMetric: 19.2384 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 123/1000
2023-09-28 03:59:56.684 
Epoch 123/1000 
	 loss: 18.3759, MinusLogProbMetric: 18.3759, val_loss: 18.7260, val_MinusLogProbMetric: 18.7260

Epoch 123: val_loss did not improve from 18.32697
196/196 - 65s - loss: 18.3759 - MinusLogProbMetric: 18.3759 - val_loss: 18.7260 - val_MinusLogProbMetric: 18.7260 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 124/1000
2023-09-28 04:01:02.350 
Epoch 124/1000 
	 loss: 18.3251, MinusLogProbMetric: 18.3251, val_loss: 18.5069, val_MinusLogProbMetric: 18.5069

Epoch 124: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.3251 - MinusLogProbMetric: 18.3251 - val_loss: 18.5069 - val_MinusLogProbMetric: 18.5069 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 125/1000
2023-09-28 04:02:07.974 
Epoch 125/1000 
	 loss: 18.2833, MinusLogProbMetric: 18.2833, val_loss: 19.7664, val_MinusLogProbMetric: 19.7664

Epoch 125: val_loss did not improve from 18.32697
196/196 - 66s - loss: 18.2833 - MinusLogProbMetric: 18.2833 - val_loss: 19.7664 - val_MinusLogProbMetric: 19.7664 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 126/1000
2023-09-28 04:03:13.351 
Epoch 126/1000 
	 loss: 18.3642, MinusLogProbMetric: 18.3642, val_loss: 18.0640, val_MinusLogProbMetric: 18.0640

Epoch 126: val_loss improved from 18.32697 to 18.06396, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 66s - loss: 18.3642 - MinusLogProbMetric: 18.3642 - val_loss: 18.0640 - val_MinusLogProbMetric: 18.0640 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 127/1000
2023-09-28 04:04:19.868 
Epoch 127/1000 
	 loss: 18.2641, MinusLogProbMetric: 18.2641, val_loss: 18.4579, val_MinusLogProbMetric: 18.4579

Epoch 127: val_loss did not improve from 18.06396
196/196 - 66s - loss: 18.2641 - MinusLogProbMetric: 18.2641 - val_loss: 18.4579 - val_MinusLogProbMetric: 18.4579 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 128/1000
2023-09-28 04:05:25.793 
Epoch 128/1000 
	 loss: 18.3186, MinusLogProbMetric: 18.3186, val_loss: 18.4967, val_MinusLogProbMetric: 18.4967

Epoch 128: val_loss did not improve from 18.06396
196/196 - 66s - loss: 18.3186 - MinusLogProbMetric: 18.3186 - val_loss: 18.4967 - val_MinusLogProbMetric: 18.4967 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 129/1000
2023-09-28 04:06:31.616 
Epoch 129/1000 
	 loss: 18.2001, MinusLogProbMetric: 18.2001, val_loss: 18.3523, val_MinusLogProbMetric: 18.3523

Epoch 129: val_loss did not improve from 18.06396
196/196 - 66s - loss: 18.2001 - MinusLogProbMetric: 18.2001 - val_loss: 18.3523 - val_MinusLogProbMetric: 18.3523 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 130/1000
2023-09-28 04:07:37.108 
Epoch 130/1000 
	 loss: 18.3142, MinusLogProbMetric: 18.3142, val_loss: 18.1579, val_MinusLogProbMetric: 18.1579

Epoch 130: val_loss did not improve from 18.06396
196/196 - 65s - loss: 18.3142 - MinusLogProbMetric: 18.3142 - val_loss: 18.1579 - val_MinusLogProbMetric: 18.1579 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 131/1000
2023-09-28 04:08:42.682 
Epoch 131/1000 
	 loss: 18.2069, MinusLogProbMetric: 18.2069, val_loss: 18.8276, val_MinusLogProbMetric: 18.8276

Epoch 131: val_loss did not improve from 18.06396
196/196 - 66s - loss: 18.2069 - MinusLogProbMetric: 18.2069 - val_loss: 18.8276 - val_MinusLogProbMetric: 18.8276 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 132/1000
2023-09-28 04:09:48.630 
Epoch 132/1000 
	 loss: 18.4528, MinusLogProbMetric: 18.4528, val_loss: 19.1748, val_MinusLogProbMetric: 19.1748

Epoch 132: val_loss did not improve from 18.06396
196/196 - 66s - loss: 18.4528 - MinusLogProbMetric: 18.4528 - val_loss: 19.1748 - val_MinusLogProbMetric: 19.1748 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 133/1000
2023-09-28 04:10:54.896 
Epoch 133/1000 
	 loss: 18.2740, MinusLogProbMetric: 18.2740, val_loss: 19.4799, val_MinusLogProbMetric: 19.4799

Epoch 133: val_loss did not improve from 18.06396
196/196 - 66s - loss: 18.2740 - MinusLogProbMetric: 18.2740 - val_loss: 19.4799 - val_MinusLogProbMetric: 19.4799 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 134/1000
2023-09-28 04:12:00.803 
Epoch 134/1000 
	 loss: 18.2479, MinusLogProbMetric: 18.2479, val_loss: 18.0072, val_MinusLogProbMetric: 18.0072

Epoch 134: val_loss improved from 18.06396 to 18.00718, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 68s - loss: 18.2479 - MinusLogProbMetric: 18.2479 - val_loss: 18.0072 - val_MinusLogProbMetric: 18.0072 - lr: 3.3333e-04 - 68s/epoch - 349ms/step
Epoch 135/1000
2023-09-28 04:13:09.649 
Epoch 135/1000 
	 loss: 18.2711, MinusLogProbMetric: 18.2711, val_loss: 18.1745, val_MinusLogProbMetric: 18.1745

Epoch 135: val_loss did not improve from 18.00718
196/196 - 66s - loss: 18.2711 - MinusLogProbMetric: 18.2711 - val_loss: 18.1745 - val_MinusLogProbMetric: 18.1745 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 136/1000
2023-09-28 04:14:15.329 
Epoch 136/1000 
	 loss: 18.2512, MinusLogProbMetric: 18.2512, val_loss: 19.4338, val_MinusLogProbMetric: 19.4338

Epoch 136: val_loss did not improve from 18.00718
196/196 - 66s - loss: 18.2512 - MinusLogProbMetric: 18.2512 - val_loss: 19.4338 - val_MinusLogProbMetric: 19.4338 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 137/1000
2023-09-28 04:15:20.828 
Epoch 137/1000 
	 loss: 18.3353, MinusLogProbMetric: 18.3353, val_loss: 18.6745, val_MinusLogProbMetric: 18.6745

Epoch 137: val_loss did not improve from 18.00718
196/196 - 65s - loss: 18.3353 - MinusLogProbMetric: 18.3353 - val_loss: 18.6745 - val_MinusLogProbMetric: 18.6745 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 138/1000
2023-09-28 04:16:26.277 
Epoch 138/1000 
	 loss: 18.4400, MinusLogProbMetric: 18.4400, val_loss: 18.1318, val_MinusLogProbMetric: 18.1318

Epoch 138: val_loss did not improve from 18.00718
196/196 - 65s - loss: 18.4400 - MinusLogProbMetric: 18.4400 - val_loss: 18.1318 - val_MinusLogProbMetric: 18.1318 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 139/1000
2023-09-28 04:17:32.436 
Epoch 139/1000 
	 loss: 18.3420, MinusLogProbMetric: 18.3420, val_loss: 19.2656, val_MinusLogProbMetric: 19.2656

Epoch 139: val_loss did not improve from 18.00718
196/196 - 66s - loss: 18.3420 - MinusLogProbMetric: 18.3420 - val_loss: 19.2656 - val_MinusLogProbMetric: 19.2656 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 140/1000
2023-09-28 04:18:37.857 
Epoch 140/1000 
	 loss: 18.2901, MinusLogProbMetric: 18.2901, val_loss: 19.6685, val_MinusLogProbMetric: 19.6685

Epoch 140: val_loss did not improve from 18.00718
196/196 - 65s - loss: 18.2901 - MinusLogProbMetric: 18.2901 - val_loss: 19.6685 - val_MinusLogProbMetric: 19.6685 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 141/1000
2023-09-28 04:19:44.077 
Epoch 141/1000 
	 loss: 18.1472, MinusLogProbMetric: 18.1472, val_loss: 19.6091, val_MinusLogProbMetric: 19.6091

Epoch 141: val_loss did not improve from 18.00718
196/196 - 66s - loss: 18.1472 - MinusLogProbMetric: 18.1472 - val_loss: 19.6091 - val_MinusLogProbMetric: 19.6091 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 142/1000
2023-09-28 04:20:49.969 
Epoch 142/1000 
	 loss: 18.1750, MinusLogProbMetric: 18.1750, val_loss: 17.9016, val_MinusLogProbMetric: 17.9016

Epoch 142: val_loss improved from 18.00718 to 17.90165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 18.1750 - MinusLogProbMetric: 18.1750 - val_loss: 17.9016 - val_MinusLogProbMetric: 17.9016 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 143/1000
2023-09-28 04:21:56.979 
Epoch 143/1000 
	 loss: 18.3783, MinusLogProbMetric: 18.3783, val_loss: 18.9998, val_MinusLogProbMetric: 18.9998

Epoch 143: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.3783 - MinusLogProbMetric: 18.3783 - val_loss: 18.9998 - val_MinusLogProbMetric: 18.9998 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 144/1000
2023-09-28 04:23:02.972 
Epoch 144/1000 
	 loss: 18.1961, MinusLogProbMetric: 18.1961, val_loss: 18.2172, val_MinusLogProbMetric: 18.2172

Epoch 144: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.1961 - MinusLogProbMetric: 18.1961 - val_loss: 18.2172 - val_MinusLogProbMetric: 18.2172 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 145/1000
2023-09-28 04:24:08.661 
Epoch 145/1000 
	 loss: 18.0868, MinusLogProbMetric: 18.0868, val_loss: 17.9854, val_MinusLogProbMetric: 17.9854

Epoch 145: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.0868 - MinusLogProbMetric: 18.0868 - val_loss: 17.9854 - val_MinusLogProbMetric: 17.9854 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 146/1000
2023-09-28 04:25:14.671 
Epoch 146/1000 
	 loss: 18.1094, MinusLogProbMetric: 18.1094, val_loss: 18.3539, val_MinusLogProbMetric: 18.3539

Epoch 146: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.1094 - MinusLogProbMetric: 18.1094 - val_loss: 18.3539 - val_MinusLogProbMetric: 18.3539 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 147/1000
2023-09-28 04:26:20.776 
Epoch 147/1000 
	 loss: 18.0980, MinusLogProbMetric: 18.0980, val_loss: 18.7127, val_MinusLogProbMetric: 18.7127

Epoch 147: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.0980 - MinusLogProbMetric: 18.0980 - val_loss: 18.7127 - val_MinusLogProbMetric: 18.7127 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 148/1000
2023-09-28 04:27:26.520 
Epoch 148/1000 
	 loss: 18.0334, MinusLogProbMetric: 18.0334, val_loss: 18.7678, val_MinusLogProbMetric: 18.7678

Epoch 148: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.0334 - MinusLogProbMetric: 18.0334 - val_loss: 18.7678 - val_MinusLogProbMetric: 18.7678 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 149/1000
2023-09-28 04:28:32.727 
Epoch 149/1000 
	 loss: 18.1387, MinusLogProbMetric: 18.1387, val_loss: 19.1095, val_MinusLogProbMetric: 19.1095

Epoch 149: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.1387 - MinusLogProbMetric: 18.1387 - val_loss: 19.1095 - val_MinusLogProbMetric: 19.1095 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 150/1000
2023-09-28 04:29:38.600 
Epoch 150/1000 
	 loss: 18.2792, MinusLogProbMetric: 18.2792, val_loss: 18.3628, val_MinusLogProbMetric: 18.3628

Epoch 150: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.2792 - MinusLogProbMetric: 18.2792 - val_loss: 18.3628 - val_MinusLogProbMetric: 18.3628 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 151/1000
2023-09-28 04:30:44.370 
Epoch 151/1000 
	 loss: 18.1400, MinusLogProbMetric: 18.1400, val_loss: 18.0518, val_MinusLogProbMetric: 18.0518

Epoch 151: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.1400 - MinusLogProbMetric: 18.1400 - val_loss: 18.0518 - val_MinusLogProbMetric: 18.0518 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 152/1000
2023-09-28 04:31:50.082 
Epoch 152/1000 
	 loss: 18.1377, MinusLogProbMetric: 18.1377, val_loss: 18.5274, val_MinusLogProbMetric: 18.5274

Epoch 152: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.1377 - MinusLogProbMetric: 18.1377 - val_loss: 18.5274 - val_MinusLogProbMetric: 18.5274 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 153/1000
2023-09-28 04:32:56.631 
Epoch 153/1000 
	 loss: 18.1571, MinusLogProbMetric: 18.1571, val_loss: 18.1256, val_MinusLogProbMetric: 18.1256

Epoch 153: val_loss did not improve from 17.90165
196/196 - 67s - loss: 18.1571 - MinusLogProbMetric: 18.1571 - val_loss: 18.1256 - val_MinusLogProbMetric: 18.1256 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 154/1000
2023-09-28 04:34:02.955 
Epoch 154/1000 
	 loss: 18.0319, MinusLogProbMetric: 18.0319, val_loss: 18.0219, val_MinusLogProbMetric: 18.0219

Epoch 154: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.0319 - MinusLogProbMetric: 18.0319 - val_loss: 18.0219 - val_MinusLogProbMetric: 18.0219 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 155/1000
2023-09-28 04:35:08.486 
Epoch 155/1000 
	 loss: 18.1370, MinusLogProbMetric: 18.1370, val_loss: 18.2606, val_MinusLogProbMetric: 18.2606

Epoch 155: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.1370 - MinusLogProbMetric: 18.1370 - val_loss: 18.2606 - val_MinusLogProbMetric: 18.2606 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 156/1000
2023-09-28 04:36:14.856 
Epoch 156/1000 
	 loss: 18.2066, MinusLogProbMetric: 18.2066, val_loss: 18.5291, val_MinusLogProbMetric: 18.5291

Epoch 156: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.2066 - MinusLogProbMetric: 18.2066 - val_loss: 18.5291 - val_MinusLogProbMetric: 18.5291 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 157/1000
2023-09-28 04:37:20.684 
Epoch 157/1000 
	 loss: 18.0379, MinusLogProbMetric: 18.0379, val_loss: 19.2628, val_MinusLogProbMetric: 19.2628

Epoch 157: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.0379 - MinusLogProbMetric: 18.0379 - val_loss: 19.2628 - val_MinusLogProbMetric: 19.2628 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 158/1000
2023-09-28 04:38:26.510 
Epoch 158/1000 
	 loss: 18.1179, MinusLogProbMetric: 18.1179, val_loss: 18.6889, val_MinusLogProbMetric: 18.6889

Epoch 158: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.1179 - MinusLogProbMetric: 18.1179 - val_loss: 18.6889 - val_MinusLogProbMetric: 18.6889 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 159/1000
2023-09-28 04:39:32.353 
Epoch 159/1000 
	 loss: 18.1908, MinusLogProbMetric: 18.1908, val_loss: 18.3036, val_MinusLogProbMetric: 18.3036

Epoch 159: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.1908 - MinusLogProbMetric: 18.1908 - val_loss: 18.3036 - val_MinusLogProbMetric: 18.3036 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 160/1000
2023-09-28 04:40:38.620 
Epoch 160/1000 
	 loss: 18.0428, MinusLogProbMetric: 18.0428, val_loss: 18.1119, val_MinusLogProbMetric: 18.1119

Epoch 160: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.0428 - MinusLogProbMetric: 18.0428 - val_loss: 18.1119 - val_MinusLogProbMetric: 18.1119 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 161/1000
2023-09-28 04:41:44.530 
Epoch 161/1000 
	 loss: 18.0270, MinusLogProbMetric: 18.0270, val_loss: 18.4710, val_MinusLogProbMetric: 18.4710

Epoch 161: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.0270 - MinusLogProbMetric: 18.0270 - val_loss: 18.4710 - val_MinusLogProbMetric: 18.4710 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 162/1000
2023-09-28 04:42:49.980 
Epoch 162/1000 
	 loss: 18.0192, MinusLogProbMetric: 18.0192, val_loss: 17.9034, val_MinusLogProbMetric: 17.9034

Epoch 162: val_loss did not improve from 17.90165
196/196 - 65s - loss: 18.0192 - MinusLogProbMetric: 18.0192 - val_loss: 17.9034 - val_MinusLogProbMetric: 17.9034 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 163/1000
2023-09-28 04:43:55.857 
Epoch 163/1000 
	 loss: 17.9980, MinusLogProbMetric: 17.9980, val_loss: 18.0672, val_MinusLogProbMetric: 18.0672

Epoch 163: val_loss did not improve from 17.90165
196/196 - 66s - loss: 17.9980 - MinusLogProbMetric: 17.9980 - val_loss: 18.0672 - val_MinusLogProbMetric: 18.0672 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 164/1000
2023-09-28 04:45:01.997 
Epoch 164/1000 
	 loss: 18.1291, MinusLogProbMetric: 18.1291, val_loss: 18.1128, val_MinusLogProbMetric: 18.1128

Epoch 164: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.1291 - MinusLogProbMetric: 18.1291 - val_loss: 18.1128 - val_MinusLogProbMetric: 18.1128 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 165/1000
2023-09-28 04:46:07.941 
Epoch 165/1000 
	 loss: 17.9220, MinusLogProbMetric: 17.9220, val_loss: 18.2145, val_MinusLogProbMetric: 18.2145

Epoch 165: val_loss did not improve from 17.90165
196/196 - 66s - loss: 17.9220 - MinusLogProbMetric: 17.9220 - val_loss: 18.2145 - val_MinusLogProbMetric: 18.2145 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 166/1000
2023-09-28 04:47:14.043 
Epoch 166/1000 
	 loss: 17.9694, MinusLogProbMetric: 17.9694, val_loss: 18.4899, val_MinusLogProbMetric: 18.4899

Epoch 166: val_loss did not improve from 17.90165
196/196 - 66s - loss: 17.9694 - MinusLogProbMetric: 17.9694 - val_loss: 18.4899 - val_MinusLogProbMetric: 18.4899 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 167/1000
2023-09-28 04:48:19.987 
Epoch 167/1000 
	 loss: 17.9795, MinusLogProbMetric: 17.9795, val_loss: 19.4928, val_MinusLogProbMetric: 19.4928

Epoch 167: val_loss did not improve from 17.90165
196/196 - 66s - loss: 17.9795 - MinusLogProbMetric: 17.9795 - val_loss: 19.4928 - val_MinusLogProbMetric: 19.4928 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 168/1000
2023-09-28 04:49:25.813 
Epoch 168/1000 
	 loss: 18.0430, MinusLogProbMetric: 18.0430, val_loss: 18.2687, val_MinusLogProbMetric: 18.2687

Epoch 168: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.0430 - MinusLogProbMetric: 18.0430 - val_loss: 18.2687 - val_MinusLogProbMetric: 18.2687 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 169/1000
2023-09-28 04:50:31.186 
Epoch 169/1000 
	 loss: 17.9134, MinusLogProbMetric: 17.9134, val_loss: 18.5389, val_MinusLogProbMetric: 18.5389

Epoch 169: val_loss did not improve from 17.90165
196/196 - 65s - loss: 17.9134 - MinusLogProbMetric: 17.9134 - val_loss: 18.5389 - val_MinusLogProbMetric: 18.5389 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 170/1000
2023-09-28 04:51:37.361 
Epoch 170/1000 
	 loss: 18.0130, MinusLogProbMetric: 18.0130, val_loss: 18.0427, val_MinusLogProbMetric: 18.0427

Epoch 170: val_loss did not improve from 17.90165
196/196 - 66s - loss: 18.0130 - MinusLogProbMetric: 18.0130 - val_loss: 18.0427 - val_MinusLogProbMetric: 18.0427 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 171/1000
2023-09-28 04:52:43.747 
Epoch 171/1000 
	 loss: 17.9622, MinusLogProbMetric: 17.9622, val_loss: 17.8684, val_MinusLogProbMetric: 17.8684

Epoch 171: val_loss improved from 17.90165 to 17.86837, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 17.9622 - MinusLogProbMetric: 17.9622 - val_loss: 17.8684 - val_MinusLogProbMetric: 17.8684 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 172/1000
2023-09-28 04:53:50.881 
Epoch 172/1000 
	 loss: 18.0488, MinusLogProbMetric: 18.0488, val_loss: 18.3958, val_MinusLogProbMetric: 18.3958

Epoch 172: val_loss did not improve from 17.86837
196/196 - 66s - loss: 18.0488 - MinusLogProbMetric: 18.0488 - val_loss: 18.3958 - val_MinusLogProbMetric: 18.3958 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 173/1000
2023-09-28 04:54:56.529 
Epoch 173/1000 
	 loss: 17.9789, MinusLogProbMetric: 17.9789, val_loss: 18.8356, val_MinusLogProbMetric: 18.8356

Epoch 173: val_loss did not improve from 17.86837
196/196 - 66s - loss: 17.9789 - MinusLogProbMetric: 17.9789 - val_loss: 18.8356 - val_MinusLogProbMetric: 18.8356 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 174/1000
2023-09-28 04:56:02.163 
Epoch 174/1000 
	 loss: 18.0691, MinusLogProbMetric: 18.0691, val_loss: 18.1569, val_MinusLogProbMetric: 18.1569

Epoch 174: val_loss did not improve from 17.86837
196/196 - 66s - loss: 18.0691 - MinusLogProbMetric: 18.0691 - val_loss: 18.1569 - val_MinusLogProbMetric: 18.1569 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 175/1000
2023-09-28 04:57:07.962 
Epoch 175/1000 
	 loss: 18.0051, MinusLogProbMetric: 18.0051, val_loss: 18.0716, val_MinusLogProbMetric: 18.0716

Epoch 175: val_loss did not improve from 17.86837
196/196 - 66s - loss: 18.0051 - MinusLogProbMetric: 18.0051 - val_loss: 18.0716 - val_MinusLogProbMetric: 18.0716 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 176/1000
2023-09-28 04:58:13.854 
Epoch 176/1000 
	 loss: 17.9846, MinusLogProbMetric: 17.9846, val_loss: 18.2515, val_MinusLogProbMetric: 18.2515

Epoch 176: val_loss did not improve from 17.86837
196/196 - 66s - loss: 17.9846 - MinusLogProbMetric: 17.9846 - val_loss: 18.2515 - val_MinusLogProbMetric: 18.2515 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 177/1000
2023-09-28 04:59:19.372 
Epoch 177/1000 
	 loss: 17.8591, MinusLogProbMetric: 17.8591, val_loss: 18.0270, val_MinusLogProbMetric: 18.0270

Epoch 177: val_loss did not improve from 17.86837
196/196 - 66s - loss: 17.8591 - MinusLogProbMetric: 17.8591 - val_loss: 18.0270 - val_MinusLogProbMetric: 18.0270 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 178/1000
2023-09-28 05:00:25.585 
Epoch 178/1000 
	 loss: 17.9144, MinusLogProbMetric: 17.9144, val_loss: 18.1170, val_MinusLogProbMetric: 18.1170

Epoch 178: val_loss did not improve from 17.86837
196/196 - 66s - loss: 17.9144 - MinusLogProbMetric: 17.9144 - val_loss: 18.1170 - val_MinusLogProbMetric: 18.1170 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 179/1000
2023-09-28 05:01:31.430 
Epoch 179/1000 
	 loss: 17.9538, MinusLogProbMetric: 17.9538, val_loss: 17.9620, val_MinusLogProbMetric: 17.9620

Epoch 179: val_loss did not improve from 17.86837
196/196 - 66s - loss: 17.9538 - MinusLogProbMetric: 17.9538 - val_loss: 17.9620 - val_MinusLogProbMetric: 17.9620 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 180/1000
2023-09-28 05:02:37.038 
Epoch 180/1000 
	 loss: 17.9406, MinusLogProbMetric: 17.9406, val_loss: 18.6931, val_MinusLogProbMetric: 18.6931

Epoch 180: val_loss did not improve from 17.86837
196/196 - 66s - loss: 17.9406 - MinusLogProbMetric: 17.9406 - val_loss: 18.6931 - val_MinusLogProbMetric: 18.6931 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 181/1000
2023-09-28 05:03:42.915 
Epoch 181/1000 
	 loss: 17.9251, MinusLogProbMetric: 17.9251, val_loss: 17.9661, val_MinusLogProbMetric: 17.9661

Epoch 181: val_loss did not improve from 17.86837
196/196 - 66s - loss: 17.9251 - MinusLogProbMetric: 17.9251 - val_loss: 17.9661 - val_MinusLogProbMetric: 17.9661 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 182/1000
2023-09-28 05:04:48.839 
Epoch 182/1000 
	 loss: 18.2170, MinusLogProbMetric: 18.2170, val_loss: 18.2473, val_MinusLogProbMetric: 18.2473

Epoch 182: val_loss did not improve from 17.86837
196/196 - 66s - loss: 18.2170 - MinusLogProbMetric: 18.2170 - val_loss: 18.2473 - val_MinusLogProbMetric: 18.2473 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 183/1000
2023-09-28 05:05:54.800 
Epoch 183/1000 
	 loss: 17.9350, MinusLogProbMetric: 17.9350, val_loss: 17.8854, val_MinusLogProbMetric: 17.8854

Epoch 183: val_loss did not improve from 17.86837
196/196 - 66s - loss: 17.9350 - MinusLogProbMetric: 17.9350 - val_loss: 17.8854 - val_MinusLogProbMetric: 17.8854 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 184/1000
2023-09-28 05:07:01.569 
Epoch 184/1000 
	 loss: 18.0295, MinusLogProbMetric: 18.0295, val_loss: 18.4260, val_MinusLogProbMetric: 18.4260

Epoch 184: val_loss did not improve from 17.86837
196/196 - 67s - loss: 18.0295 - MinusLogProbMetric: 18.0295 - val_loss: 18.4260 - val_MinusLogProbMetric: 18.4260 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 185/1000
2023-09-28 05:08:07.794 
Epoch 185/1000 
	 loss: 17.9865, MinusLogProbMetric: 17.9865, val_loss: 18.4499, val_MinusLogProbMetric: 18.4499

Epoch 185: val_loss did not improve from 17.86837
196/196 - 66s - loss: 17.9865 - MinusLogProbMetric: 17.9865 - val_loss: 18.4499 - val_MinusLogProbMetric: 18.4499 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 186/1000
2023-09-28 05:09:14.071 
Epoch 186/1000 
	 loss: 17.8548, MinusLogProbMetric: 17.8548, val_loss: 18.0870, val_MinusLogProbMetric: 18.0870

Epoch 186: val_loss did not improve from 17.86837
196/196 - 66s - loss: 17.8548 - MinusLogProbMetric: 17.8548 - val_loss: 18.0870 - val_MinusLogProbMetric: 18.0870 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 187/1000
2023-09-28 05:10:20.018 
Epoch 187/1000 
	 loss: 17.8712, MinusLogProbMetric: 17.8712, val_loss: 18.1443, val_MinusLogProbMetric: 18.1443

Epoch 187: val_loss did not improve from 17.86837
196/196 - 66s - loss: 17.8712 - MinusLogProbMetric: 17.8712 - val_loss: 18.1443 - val_MinusLogProbMetric: 18.1443 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 188/1000
2023-09-28 05:11:25.991 
Epoch 188/1000 
	 loss: 17.9256, MinusLogProbMetric: 17.9256, val_loss: 18.0881, val_MinusLogProbMetric: 18.0881

Epoch 188: val_loss did not improve from 17.86837
196/196 - 66s - loss: 17.9256 - MinusLogProbMetric: 17.9256 - val_loss: 18.0881 - val_MinusLogProbMetric: 18.0881 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 189/1000
2023-09-28 05:12:31.627 
Epoch 189/1000 
	 loss: 17.8921, MinusLogProbMetric: 17.8921, val_loss: 17.9872, val_MinusLogProbMetric: 17.9872

Epoch 189: val_loss did not improve from 17.86837
196/196 - 66s - loss: 17.8921 - MinusLogProbMetric: 17.8921 - val_loss: 17.9872 - val_MinusLogProbMetric: 17.9872 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 190/1000
2023-09-28 05:13:37.885 
Epoch 190/1000 
	 loss: 17.9516, MinusLogProbMetric: 17.9516, val_loss: 17.8516, val_MinusLogProbMetric: 17.8516

Epoch 190: val_loss improved from 17.86837 to 17.85157, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 17.9516 - MinusLogProbMetric: 17.9516 - val_loss: 17.8516 - val_MinusLogProbMetric: 17.8516 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 191/1000
2023-09-28 05:14:45.438 
Epoch 191/1000 
	 loss: 17.8669, MinusLogProbMetric: 17.8669, val_loss: 18.3063, val_MinusLogProbMetric: 18.3063

Epoch 191: val_loss did not improve from 17.85157
196/196 - 67s - loss: 17.8669 - MinusLogProbMetric: 17.8669 - val_loss: 18.3063 - val_MinusLogProbMetric: 18.3063 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 192/1000
2023-09-28 05:15:51.258 
Epoch 192/1000 
	 loss: 17.8102, MinusLogProbMetric: 17.8102, val_loss: 18.8512, val_MinusLogProbMetric: 18.8512

Epoch 192: val_loss did not improve from 17.85157
196/196 - 66s - loss: 17.8102 - MinusLogProbMetric: 17.8102 - val_loss: 18.8512 - val_MinusLogProbMetric: 18.8512 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 193/1000
2023-09-28 05:16:56.973 
Epoch 193/1000 
	 loss: 17.9103, MinusLogProbMetric: 17.9103, val_loss: 18.2691, val_MinusLogProbMetric: 18.2691

Epoch 193: val_loss did not improve from 17.85157
196/196 - 66s - loss: 17.9103 - MinusLogProbMetric: 17.9103 - val_loss: 18.2691 - val_MinusLogProbMetric: 18.2691 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 194/1000
2023-09-28 05:18:02.596 
Epoch 194/1000 
	 loss: 17.8134, MinusLogProbMetric: 17.8134, val_loss: 18.9256, val_MinusLogProbMetric: 18.9256

Epoch 194: val_loss did not improve from 17.85157
196/196 - 66s - loss: 17.8134 - MinusLogProbMetric: 17.8134 - val_loss: 18.9256 - val_MinusLogProbMetric: 18.9256 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 195/1000
2023-09-28 05:19:08.547 
Epoch 195/1000 
	 loss: 17.9837, MinusLogProbMetric: 17.9837, val_loss: 17.9229, val_MinusLogProbMetric: 17.9229

Epoch 195: val_loss did not improve from 17.85157
196/196 - 66s - loss: 17.9837 - MinusLogProbMetric: 17.9837 - val_loss: 17.9229 - val_MinusLogProbMetric: 17.9229 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 196/1000
2023-09-28 05:20:13.778 
Epoch 196/1000 
	 loss: 17.8223, MinusLogProbMetric: 17.8223, val_loss: 18.0645, val_MinusLogProbMetric: 18.0645

Epoch 196: val_loss did not improve from 17.85157
196/196 - 65s - loss: 17.8223 - MinusLogProbMetric: 17.8223 - val_loss: 18.0645 - val_MinusLogProbMetric: 18.0645 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 197/1000
2023-09-28 05:21:19.967 
Epoch 197/1000 
	 loss: 17.8161, MinusLogProbMetric: 17.8161, val_loss: 17.7631, val_MinusLogProbMetric: 17.7631

Epoch 197: val_loss improved from 17.85157 to 17.76311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 17.8161 - MinusLogProbMetric: 17.8161 - val_loss: 17.7631 - val_MinusLogProbMetric: 17.7631 - lr: 3.3333e-04 - 67s/epoch - 343ms/step
Epoch 198/1000
2023-09-28 05:22:27.050 
Epoch 198/1000 
	 loss: 17.9442, MinusLogProbMetric: 17.9442, val_loss: 18.2074, val_MinusLogProbMetric: 18.2074

Epoch 198: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.9442 - MinusLogProbMetric: 17.9442 - val_loss: 18.2074 - val_MinusLogProbMetric: 18.2074 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 199/1000
2023-09-28 05:23:33.013 
Epoch 199/1000 
	 loss: 17.8003, MinusLogProbMetric: 17.8003, val_loss: 18.8623, val_MinusLogProbMetric: 18.8623

Epoch 199: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.8003 - MinusLogProbMetric: 17.8003 - val_loss: 18.8623 - val_MinusLogProbMetric: 18.8623 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 200/1000
2023-09-28 05:24:39.349 
Epoch 200/1000 
	 loss: 17.7525, MinusLogProbMetric: 17.7525, val_loss: 17.7873, val_MinusLogProbMetric: 17.7873

Epoch 200: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.7525 - MinusLogProbMetric: 17.7525 - val_loss: 17.7873 - val_MinusLogProbMetric: 17.7873 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 201/1000
2023-09-28 05:25:44.679 
Epoch 201/1000 
	 loss: 17.8303, MinusLogProbMetric: 17.8303, val_loss: 18.2296, val_MinusLogProbMetric: 18.2296

Epoch 201: val_loss did not improve from 17.76311
196/196 - 65s - loss: 17.8303 - MinusLogProbMetric: 17.8303 - val_loss: 18.2296 - val_MinusLogProbMetric: 18.2296 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 202/1000
2023-09-28 05:26:50.271 
Epoch 202/1000 
	 loss: 17.7883, MinusLogProbMetric: 17.7883, val_loss: 17.9117, val_MinusLogProbMetric: 17.9117

Epoch 202: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.7883 - MinusLogProbMetric: 17.7883 - val_loss: 17.9117 - val_MinusLogProbMetric: 17.9117 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 203/1000
2023-09-28 05:27:55.977 
Epoch 203/1000 
	 loss: 17.7873, MinusLogProbMetric: 17.7873, val_loss: 18.2389, val_MinusLogProbMetric: 18.2389

Epoch 203: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.7873 - MinusLogProbMetric: 17.7873 - val_loss: 18.2389 - val_MinusLogProbMetric: 18.2389 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 204/1000
2023-09-28 05:29:02.001 
Epoch 204/1000 
	 loss: 17.7944, MinusLogProbMetric: 17.7944, val_loss: 18.0294, val_MinusLogProbMetric: 18.0294

Epoch 204: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.7944 - MinusLogProbMetric: 17.7944 - val_loss: 18.0294 - val_MinusLogProbMetric: 18.0294 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 205/1000
2023-09-28 05:30:07.454 
Epoch 205/1000 
	 loss: 17.9298, MinusLogProbMetric: 17.9298, val_loss: 18.0848, val_MinusLogProbMetric: 18.0848

Epoch 205: val_loss did not improve from 17.76311
196/196 - 65s - loss: 17.9298 - MinusLogProbMetric: 17.9298 - val_loss: 18.0848 - val_MinusLogProbMetric: 18.0848 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 206/1000
2023-09-28 05:31:13.243 
Epoch 206/1000 
	 loss: 17.7970, MinusLogProbMetric: 17.7970, val_loss: 18.0329, val_MinusLogProbMetric: 18.0329

Epoch 206: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.7970 - MinusLogProbMetric: 17.7970 - val_loss: 18.0329 - val_MinusLogProbMetric: 18.0329 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 207/1000
2023-09-28 05:32:19.359 
Epoch 207/1000 
	 loss: 17.7313, MinusLogProbMetric: 17.7313, val_loss: 18.7457, val_MinusLogProbMetric: 18.7457

Epoch 207: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.7313 - MinusLogProbMetric: 17.7313 - val_loss: 18.7457 - val_MinusLogProbMetric: 18.7457 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 208/1000
2023-09-28 05:33:25.627 
Epoch 208/1000 
	 loss: 17.6974, MinusLogProbMetric: 17.6974, val_loss: 17.9128, val_MinusLogProbMetric: 17.9128

Epoch 208: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.6974 - MinusLogProbMetric: 17.6974 - val_loss: 17.9128 - val_MinusLogProbMetric: 17.9128 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 209/1000
2023-09-28 05:34:31.371 
Epoch 209/1000 
	 loss: 17.7892, MinusLogProbMetric: 17.7892, val_loss: 18.0669, val_MinusLogProbMetric: 18.0669

Epoch 209: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.7892 - MinusLogProbMetric: 17.7892 - val_loss: 18.0669 - val_MinusLogProbMetric: 18.0669 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 210/1000
2023-09-28 05:35:37.227 
Epoch 210/1000 
	 loss: 17.8053, MinusLogProbMetric: 17.8053, val_loss: 19.7475, val_MinusLogProbMetric: 19.7475

Epoch 210: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.8053 - MinusLogProbMetric: 17.8053 - val_loss: 19.7475 - val_MinusLogProbMetric: 19.7475 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 211/1000
2023-09-28 05:36:42.822 
Epoch 211/1000 
	 loss: 17.7965, MinusLogProbMetric: 17.7965, val_loss: 17.9241, val_MinusLogProbMetric: 17.9241

Epoch 211: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.7965 - MinusLogProbMetric: 17.7965 - val_loss: 17.9241 - val_MinusLogProbMetric: 17.9241 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 212/1000
2023-09-28 05:37:48.675 
Epoch 212/1000 
	 loss: 17.7556, MinusLogProbMetric: 17.7556, val_loss: 19.5865, val_MinusLogProbMetric: 19.5865

Epoch 212: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.7556 - MinusLogProbMetric: 17.7556 - val_loss: 19.5865 - val_MinusLogProbMetric: 19.5865 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 213/1000
2023-09-28 05:38:54.825 
Epoch 213/1000 
	 loss: 17.7543, MinusLogProbMetric: 17.7543, val_loss: 17.8328, val_MinusLogProbMetric: 17.8328

Epoch 213: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.7543 - MinusLogProbMetric: 17.7543 - val_loss: 17.8328 - val_MinusLogProbMetric: 17.8328 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 214/1000
2023-09-28 05:40:00.556 
Epoch 214/1000 
	 loss: 17.6655, MinusLogProbMetric: 17.6655, val_loss: 17.8249, val_MinusLogProbMetric: 17.8249

Epoch 214: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.6655 - MinusLogProbMetric: 17.6655 - val_loss: 17.8249 - val_MinusLogProbMetric: 17.8249 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 215/1000
2023-09-28 05:41:06.274 
Epoch 215/1000 
	 loss: 17.8933, MinusLogProbMetric: 17.8933, val_loss: 18.3512, val_MinusLogProbMetric: 18.3512

Epoch 215: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.8933 - MinusLogProbMetric: 17.8933 - val_loss: 18.3512 - val_MinusLogProbMetric: 18.3512 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 216/1000
2023-09-28 05:42:12.213 
Epoch 216/1000 
	 loss: 17.6950, MinusLogProbMetric: 17.6950, val_loss: 17.9884, val_MinusLogProbMetric: 17.9884

Epoch 216: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.6950 - MinusLogProbMetric: 17.6950 - val_loss: 17.9884 - val_MinusLogProbMetric: 17.9884 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 217/1000
2023-09-28 05:43:17.916 
Epoch 217/1000 
	 loss: 17.6882, MinusLogProbMetric: 17.6882, val_loss: 17.8008, val_MinusLogProbMetric: 17.8008

Epoch 217: val_loss did not improve from 17.76311
196/196 - 66s - loss: 17.6882 - MinusLogProbMetric: 17.6882 - val_loss: 17.8008 - val_MinusLogProbMetric: 17.8008 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 218/1000
2023-09-28 05:44:23.970 
Epoch 218/1000 
	 loss: 17.7835, MinusLogProbMetric: 17.7835, val_loss: 17.7508, val_MinusLogProbMetric: 17.7508

Epoch 218: val_loss improved from 17.76311 to 17.75077, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 17.7835 - MinusLogProbMetric: 17.7835 - val_loss: 17.7508 - val_MinusLogProbMetric: 17.7508 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 219/1000
2023-09-28 05:45:30.434 
Epoch 219/1000 
	 loss: 17.7226, MinusLogProbMetric: 17.7226, val_loss: 17.7767, val_MinusLogProbMetric: 17.7767

Epoch 219: val_loss did not improve from 17.75077
196/196 - 66s - loss: 17.7226 - MinusLogProbMetric: 17.7226 - val_loss: 17.7767 - val_MinusLogProbMetric: 17.7767 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 220/1000
2023-09-28 05:46:35.987 
Epoch 220/1000 
	 loss: 17.7283, MinusLogProbMetric: 17.7283, val_loss: 17.8465, val_MinusLogProbMetric: 17.8465

Epoch 220: val_loss did not improve from 17.75077
196/196 - 66s - loss: 17.7283 - MinusLogProbMetric: 17.7283 - val_loss: 17.8465 - val_MinusLogProbMetric: 17.8465 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 221/1000
2023-09-28 05:47:41.139 
Epoch 221/1000 
	 loss: 17.7452, MinusLogProbMetric: 17.7452, val_loss: 18.5013, val_MinusLogProbMetric: 18.5013

Epoch 221: val_loss did not improve from 17.75077
196/196 - 65s - loss: 17.7452 - MinusLogProbMetric: 17.7452 - val_loss: 18.5013 - val_MinusLogProbMetric: 18.5013 - lr: 3.3333e-04 - 65s/epoch - 332ms/step
Epoch 222/1000
2023-09-28 05:48:47.209 
Epoch 222/1000 
	 loss: 17.7262, MinusLogProbMetric: 17.7262, val_loss: 17.8952, val_MinusLogProbMetric: 17.8952

Epoch 222: val_loss did not improve from 17.75077
196/196 - 66s - loss: 17.7262 - MinusLogProbMetric: 17.7262 - val_loss: 17.8952 - val_MinusLogProbMetric: 17.8952 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 223/1000
2023-09-28 05:49:52.627 
Epoch 223/1000 
	 loss: 17.8014, MinusLogProbMetric: 17.8014, val_loss: 17.7997, val_MinusLogProbMetric: 17.7997

Epoch 223: val_loss did not improve from 17.75077
196/196 - 65s - loss: 17.8014 - MinusLogProbMetric: 17.8014 - val_loss: 17.7997 - val_MinusLogProbMetric: 17.7997 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 224/1000
2023-09-28 05:50:58.224 
Epoch 224/1000 
	 loss: 17.6204, MinusLogProbMetric: 17.6204, val_loss: 18.4349, val_MinusLogProbMetric: 18.4349

Epoch 224: val_loss did not improve from 17.75077
196/196 - 66s - loss: 17.6204 - MinusLogProbMetric: 17.6204 - val_loss: 18.4349 - val_MinusLogProbMetric: 18.4349 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 225/1000
2023-09-28 05:52:03.604 
Epoch 225/1000 
	 loss: 17.7226, MinusLogProbMetric: 17.7226, val_loss: 18.2489, val_MinusLogProbMetric: 18.2489

Epoch 225: val_loss did not improve from 17.75077
196/196 - 65s - loss: 17.7226 - MinusLogProbMetric: 17.7226 - val_loss: 18.2489 - val_MinusLogProbMetric: 18.2489 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 226/1000
2023-09-28 05:53:09.624 
Epoch 226/1000 
	 loss: 17.7487, MinusLogProbMetric: 17.7487, val_loss: 18.6646, val_MinusLogProbMetric: 18.6646

Epoch 226: val_loss did not improve from 17.75077
196/196 - 66s - loss: 17.7487 - MinusLogProbMetric: 17.7487 - val_loss: 18.6646 - val_MinusLogProbMetric: 18.6646 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 227/1000
2023-09-28 05:54:15.253 
Epoch 227/1000 
	 loss: 17.6784, MinusLogProbMetric: 17.6784, val_loss: 17.8940, val_MinusLogProbMetric: 17.8940

Epoch 227: val_loss did not improve from 17.75077
196/196 - 66s - loss: 17.6784 - MinusLogProbMetric: 17.6784 - val_loss: 17.8940 - val_MinusLogProbMetric: 17.8940 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 228/1000
2023-09-28 05:55:21.018 
Epoch 228/1000 
	 loss: 17.7457, MinusLogProbMetric: 17.7457, val_loss: 18.1606, val_MinusLogProbMetric: 18.1606

Epoch 228: val_loss did not improve from 17.75077
196/196 - 66s - loss: 17.7457 - MinusLogProbMetric: 17.7457 - val_loss: 18.1606 - val_MinusLogProbMetric: 18.1606 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 229/1000
2023-09-28 05:56:26.923 
Epoch 229/1000 
	 loss: 17.6961, MinusLogProbMetric: 17.6961, val_loss: 17.7845, val_MinusLogProbMetric: 17.7845

Epoch 229: val_loss did not improve from 17.75077
196/196 - 66s - loss: 17.6961 - MinusLogProbMetric: 17.6961 - val_loss: 17.7845 - val_MinusLogProbMetric: 17.7845 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 230/1000
2023-09-28 05:57:32.519 
Epoch 230/1000 
	 loss: 17.7075, MinusLogProbMetric: 17.7075, val_loss: 17.9682, val_MinusLogProbMetric: 17.9682

Epoch 230: val_loss did not improve from 17.75077
196/196 - 66s - loss: 17.7075 - MinusLogProbMetric: 17.7075 - val_loss: 17.9682 - val_MinusLogProbMetric: 17.9682 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 231/1000
2023-09-28 05:58:38.380 
Epoch 231/1000 
	 loss: 17.6417, MinusLogProbMetric: 17.6417, val_loss: 17.9183, val_MinusLogProbMetric: 17.9183

Epoch 231: val_loss did not improve from 17.75077
196/196 - 66s - loss: 17.6417 - MinusLogProbMetric: 17.6417 - val_loss: 17.9183 - val_MinusLogProbMetric: 17.9183 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 232/1000
2023-09-28 05:59:43.924 
Epoch 232/1000 
	 loss: 17.7564, MinusLogProbMetric: 17.7564, val_loss: 17.9554, val_MinusLogProbMetric: 17.9554

Epoch 232: val_loss did not improve from 17.75077
196/196 - 66s - loss: 17.7564 - MinusLogProbMetric: 17.7564 - val_loss: 17.9554 - val_MinusLogProbMetric: 17.9554 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 233/1000
2023-09-28 06:00:50.334 
Epoch 233/1000 
	 loss: 17.8342, MinusLogProbMetric: 17.8342, val_loss: 18.1969, val_MinusLogProbMetric: 18.1969

Epoch 233: val_loss did not improve from 17.75077
196/196 - 66s - loss: 17.8342 - MinusLogProbMetric: 17.8342 - val_loss: 18.1969 - val_MinusLogProbMetric: 18.1969 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 234/1000
2023-09-28 06:01:56.138 
Epoch 234/1000 
	 loss: 17.6443, MinusLogProbMetric: 17.6443, val_loss: 18.3448, val_MinusLogProbMetric: 18.3448

Epoch 234: val_loss did not improve from 17.75077
196/196 - 66s - loss: 17.6443 - MinusLogProbMetric: 17.6443 - val_loss: 18.3448 - val_MinusLogProbMetric: 18.3448 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 235/1000
2023-09-28 06:03:01.896 
Epoch 235/1000 
	 loss: 17.6584, MinusLogProbMetric: 17.6584, val_loss: 17.6882, val_MinusLogProbMetric: 17.6882

Epoch 235: val_loss improved from 17.75077 to 17.68821, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 17.6584 - MinusLogProbMetric: 17.6584 - val_loss: 17.6882 - val_MinusLogProbMetric: 17.6882 - lr: 3.3333e-04 - 67s/epoch - 340ms/step
Epoch 236/1000
2023-09-28 06:04:08.899 
Epoch 236/1000 
	 loss: 17.6587, MinusLogProbMetric: 17.6587, val_loss: 17.7522, val_MinusLogProbMetric: 17.7522

Epoch 236: val_loss did not improve from 17.68821
196/196 - 66s - loss: 17.6587 - MinusLogProbMetric: 17.6587 - val_loss: 17.7522 - val_MinusLogProbMetric: 17.7522 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 237/1000
2023-09-28 06:05:14.545 
Epoch 237/1000 
	 loss: 17.6053, MinusLogProbMetric: 17.6053, val_loss: 18.0571, val_MinusLogProbMetric: 18.0571

Epoch 237: val_loss did not improve from 17.68821
196/196 - 66s - loss: 17.6053 - MinusLogProbMetric: 17.6053 - val_loss: 18.0571 - val_MinusLogProbMetric: 18.0571 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 238/1000
2023-09-28 06:06:20.905 
Epoch 238/1000 
	 loss: 17.7016, MinusLogProbMetric: 17.7016, val_loss: 18.1554, val_MinusLogProbMetric: 18.1554

Epoch 238: val_loss did not improve from 17.68821
196/196 - 66s - loss: 17.7016 - MinusLogProbMetric: 17.7016 - val_loss: 18.1554 - val_MinusLogProbMetric: 18.1554 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 239/1000
2023-09-28 06:07:26.732 
Epoch 239/1000 
	 loss: 17.6258, MinusLogProbMetric: 17.6258, val_loss: 18.1457, val_MinusLogProbMetric: 18.1457

Epoch 239: val_loss did not improve from 17.68821
196/196 - 66s - loss: 17.6258 - MinusLogProbMetric: 17.6258 - val_loss: 18.1457 - val_MinusLogProbMetric: 18.1457 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 240/1000
2023-09-28 06:08:32.826 
Epoch 240/1000 
	 loss: 17.5896, MinusLogProbMetric: 17.5896, val_loss: 17.6699, val_MinusLogProbMetric: 17.6699

Epoch 240: val_loss improved from 17.68821 to 17.66989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 17.5896 - MinusLogProbMetric: 17.5896 - val_loss: 17.6699 - val_MinusLogProbMetric: 17.6699 - lr: 3.3333e-04 - 67s/epoch - 342ms/step
Epoch 241/1000
2023-09-28 06:09:39.728 
Epoch 241/1000 
	 loss: 17.8482, MinusLogProbMetric: 17.8482, val_loss: 18.1945, val_MinusLogProbMetric: 18.1945

Epoch 241: val_loss did not improve from 17.66989
196/196 - 66s - loss: 17.8482 - MinusLogProbMetric: 17.8482 - val_loss: 18.1945 - val_MinusLogProbMetric: 18.1945 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 242/1000
2023-09-28 06:10:45.775 
Epoch 242/1000 
	 loss: 17.6538, MinusLogProbMetric: 17.6538, val_loss: 17.8715, val_MinusLogProbMetric: 17.8715

Epoch 242: val_loss did not improve from 17.66989
196/196 - 66s - loss: 17.6538 - MinusLogProbMetric: 17.6538 - val_loss: 17.8715 - val_MinusLogProbMetric: 17.8715 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 243/1000
2023-09-28 06:11:51.689 
Epoch 243/1000 
	 loss: 17.6655, MinusLogProbMetric: 17.6655, val_loss: 18.0645, val_MinusLogProbMetric: 18.0645

Epoch 243: val_loss did not improve from 17.66989
196/196 - 66s - loss: 17.6655 - MinusLogProbMetric: 17.6655 - val_loss: 18.0645 - val_MinusLogProbMetric: 18.0645 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 244/1000
2023-09-28 06:12:57.336 
Epoch 244/1000 
	 loss: 17.6721, MinusLogProbMetric: 17.6721, val_loss: 17.8959, val_MinusLogProbMetric: 17.8959

Epoch 244: val_loss did not improve from 17.66989
196/196 - 66s - loss: 17.6721 - MinusLogProbMetric: 17.6721 - val_loss: 17.8959 - val_MinusLogProbMetric: 17.8959 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 245/1000
2023-09-28 06:14:03.381 
Epoch 245/1000 
	 loss: 17.6782, MinusLogProbMetric: 17.6782, val_loss: 17.8860, val_MinusLogProbMetric: 17.8860

Epoch 245: val_loss did not improve from 17.66989
196/196 - 66s - loss: 17.6782 - MinusLogProbMetric: 17.6782 - val_loss: 17.8860 - val_MinusLogProbMetric: 17.8860 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 246/1000
2023-09-28 06:15:09.618 
Epoch 246/1000 
	 loss: 17.5919, MinusLogProbMetric: 17.5919, val_loss: 20.7129, val_MinusLogProbMetric: 20.7129

Epoch 246: val_loss did not improve from 17.66989
196/196 - 66s - loss: 17.5919 - MinusLogProbMetric: 17.5919 - val_loss: 20.7129 - val_MinusLogProbMetric: 20.7129 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 247/1000
2023-09-28 06:16:15.760 
Epoch 247/1000 
	 loss: 17.6064, MinusLogProbMetric: 17.6064, val_loss: 17.8529, val_MinusLogProbMetric: 17.8529

Epoch 247: val_loss did not improve from 17.66989
196/196 - 66s - loss: 17.6064 - MinusLogProbMetric: 17.6064 - val_loss: 17.8529 - val_MinusLogProbMetric: 17.8529 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 248/1000
2023-09-28 06:17:21.696 
Epoch 248/1000 
	 loss: 17.6326, MinusLogProbMetric: 17.6326, val_loss: 18.4689, val_MinusLogProbMetric: 18.4689

Epoch 248: val_loss did not improve from 17.66989
196/196 - 66s - loss: 17.6326 - MinusLogProbMetric: 17.6326 - val_loss: 18.4689 - val_MinusLogProbMetric: 18.4689 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 249/1000
2023-09-28 06:18:27.451 
Epoch 249/1000 
	 loss: 17.7836, MinusLogProbMetric: 17.7836, val_loss: 17.6636, val_MinusLogProbMetric: 17.6636

Epoch 249: val_loss improved from 17.66989 to 17.66355, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 17.7836 - MinusLogProbMetric: 17.7836 - val_loss: 17.6636 - val_MinusLogProbMetric: 17.6636 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 250/1000
2023-09-28 06:19:34.236 
Epoch 250/1000 
	 loss: 17.5952, MinusLogProbMetric: 17.5952, val_loss: 18.9655, val_MinusLogProbMetric: 18.9655

Epoch 250: val_loss did not improve from 17.66355
196/196 - 66s - loss: 17.5952 - MinusLogProbMetric: 17.5952 - val_loss: 18.9655 - val_MinusLogProbMetric: 18.9655 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 251/1000
2023-09-28 06:20:40.493 
Epoch 251/1000 
	 loss: 17.5769, MinusLogProbMetric: 17.5769, val_loss: 17.5881, val_MinusLogProbMetric: 17.5881

Epoch 251: val_loss improved from 17.66355 to 17.58809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 17.5769 - MinusLogProbMetric: 17.5769 - val_loss: 17.5881 - val_MinusLogProbMetric: 17.5881 - lr: 3.3333e-04 - 67s/epoch - 344ms/step
Epoch 252/1000
2023-09-28 06:21:46.906 
Epoch 252/1000 
	 loss: 17.5390, MinusLogProbMetric: 17.5390, val_loss: 18.2798, val_MinusLogProbMetric: 18.2798

Epoch 252: val_loss did not improve from 17.58809
196/196 - 65s - loss: 17.5390 - MinusLogProbMetric: 17.5390 - val_loss: 18.2798 - val_MinusLogProbMetric: 18.2798 - lr: 3.3333e-04 - 65s/epoch - 333ms/step
Epoch 253/1000
2023-09-28 06:22:52.633 
Epoch 253/1000 
	 loss: 17.5544, MinusLogProbMetric: 17.5544, val_loss: 18.5526, val_MinusLogProbMetric: 18.5526

Epoch 253: val_loss did not improve from 17.58809
196/196 - 66s - loss: 17.5544 - MinusLogProbMetric: 17.5544 - val_loss: 18.5526 - val_MinusLogProbMetric: 18.5526 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 254/1000
2023-09-28 06:23:58.956 
Epoch 254/1000 
	 loss: 17.6542, MinusLogProbMetric: 17.6542, val_loss: 18.0178, val_MinusLogProbMetric: 18.0178

Epoch 254: val_loss did not improve from 17.58809
196/196 - 66s - loss: 17.6542 - MinusLogProbMetric: 17.6542 - val_loss: 18.0178 - val_MinusLogProbMetric: 18.0178 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 255/1000
2023-09-28 06:25:04.767 
Epoch 255/1000 
	 loss: 17.5618, MinusLogProbMetric: 17.5618, val_loss: 17.9576, val_MinusLogProbMetric: 17.9576

Epoch 255: val_loss did not improve from 17.58809
196/196 - 66s - loss: 17.5618 - MinusLogProbMetric: 17.5618 - val_loss: 17.9576 - val_MinusLogProbMetric: 17.9576 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 256/1000
2023-09-28 06:26:09.462 
Epoch 256/1000 
	 loss: 17.6439, MinusLogProbMetric: 17.6439, val_loss: 18.2470, val_MinusLogProbMetric: 18.2470

Epoch 256: val_loss did not improve from 17.58809
196/196 - 65s - loss: 17.6439 - MinusLogProbMetric: 17.6439 - val_loss: 18.2470 - val_MinusLogProbMetric: 18.2470 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 257/1000
2023-09-28 06:27:11.489 
Epoch 257/1000 
	 loss: 17.5616, MinusLogProbMetric: 17.5616, val_loss: 18.7835, val_MinusLogProbMetric: 18.7835

Epoch 257: val_loss did not improve from 17.58809
196/196 - 62s - loss: 17.5616 - MinusLogProbMetric: 17.5616 - val_loss: 18.7835 - val_MinusLogProbMetric: 18.7835 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 258/1000
2023-09-28 06:28:08.262 
Epoch 258/1000 
	 loss: 17.5270, MinusLogProbMetric: 17.5270, val_loss: 17.8368, val_MinusLogProbMetric: 17.8368

Epoch 258: val_loss did not improve from 17.58809
196/196 - 57s - loss: 17.5270 - MinusLogProbMetric: 17.5270 - val_loss: 17.8368 - val_MinusLogProbMetric: 17.8368 - lr: 3.3333e-04 - 57s/epoch - 290ms/step
Epoch 259/1000
2023-09-28 06:29:04.711 
Epoch 259/1000 
	 loss: 17.6058, MinusLogProbMetric: 17.6058, val_loss: 17.9413, val_MinusLogProbMetric: 17.9413

Epoch 259: val_loss did not improve from 17.58809
196/196 - 56s - loss: 17.6058 - MinusLogProbMetric: 17.6058 - val_loss: 17.9413 - val_MinusLogProbMetric: 17.9413 - lr: 3.3333e-04 - 56s/epoch - 288ms/step
Epoch 260/1000
2023-09-28 06:30:09.371 
Epoch 260/1000 
	 loss: 17.5537, MinusLogProbMetric: 17.5537, val_loss: 18.1936, val_MinusLogProbMetric: 18.1936

Epoch 260: val_loss did not improve from 17.58809
196/196 - 65s - loss: 17.5537 - MinusLogProbMetric: 17.5537 - val_loss: 18.1936 - val_MinusLogProbMetric: 18.1936 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 261/1000
2023-09-28 06:31:08.609 
Epoch 261/1000 
	 loss: 17.6301, MinusLogProbMetric: 17.6301, val_loss: 19.6110, val_MinusLogProbMetric: 19.6110

Epoch 261: val_loss did not improve from 17.58809
196/196 - 59s - loss: 17.6301 - MinusLogProbMetric: 17.6301 - val_loss: 19.6110 - val_MinusLogProbMetric: 19.6110 - lr: 3.3333e-04 - 59s/epoch - 302ms/step
Epoch 262/1000
2023-09-28 06:32:04.512 
Epoch 262/1000 
	 loss: 17.6427, MinusLogProbMetric: 17.6427, val_loss: 18.0796, val_MinusLogProbMetric: 18.0796

Epoch 262: val_loss did not improve from 17.58809
196/196 - 56s - loss: 17.6427 - MinusLogProbMetric: 17.6427 - val_loss: 18.0796 - val_MinusLogProbMetric: 18.0796 - lr: 3.3333e-04 - 56s/epoch - 285ms/step
Epoch 263/1000
2023-09-28 06:33:09.102 
Epoch 263/1000 
	 loss: 17.5270, MinusLogProbMetric: 17.5270, val_loss: 17.6977, val_MinusLogProbMetric: 17.6977

Epoch 263: val_loss did not improve from 17.58809
196/196 - 65s - loss: 17.5270 - MinusLogProbMetric: 17.5270 - val_loss: 17.6977 - val_MinusLogProbMetric: 17.6977 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 264/1000
2023-09-28 06:34:08.844 
Epoch 264/1000 
	 loss: 17.5706, MinusLogProbMetric: 17.5706, val_loss: 17.8070, val_MinusLogProbMetric: 17.8070

Epoch 264: val_loss did not improve from 17.58809
196/196 - 60s - loss: 17.5706 - MinusLogProbMetric: 17.5706 - val_loss: 17.8070 - val_MinusLogProbMetric: 17.8070 - lr: 3.3333e-04 - 60s/epoch - 305ms/step
Epoch 265/1000
2023-09-28 06:35:06.122 
Epoch 265/1000 
	 loss: 17.5163, MinusLogProbMetric: 17.5163, val_loss: 17.7191, val_MinusLogProbMetric: 17.7191

Epoch 265: val_loss did not improve from 17.58809
196/196 - 57s - loss: 17.5163 - MinusLogProbMetric: 17.5163 - val_loss: 17.7191 - val_MinusLogProbMetric: 17.7191 - lr: 3.3333e-04 - 57s/epoch - 292ms/step
Epoch 266/1000
2023-09-28 06:36:08.039 
Epoch 266/1000 
	 loss: 17.5102, MinusLogProbMetric: 17.5102, val_loss: 17.6915, val_MinusLogProbMetric: 17.6915

Epoch 266: val_loss did not improve from 17.58809
196/196 - 62s - loss: 17.5102 - MinusLogProbMetric: 17.5102 - val_loss: 17.6915 - val_MinusLogProbMetric: 17.6915 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 267/1000
2023-09-28 06:37:09.846 
Epoch 267/1000 
	 loss: 17.5967, MinusLogProbMetric: 17.5967, val_loss: 17.9350, val_MinusLogProbMetric: 17.9350

Epoch 267: val_loss did not improve from 17.58809
196/196 - 62s - loss: 17.5967 - MinusLogProbMetric: 17.5967 - val_loss: 17.9350 - val_MinusLogProbMetric: 17.9350 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 268/1000
2023-09-28 06:38:06.366 
Epoch 268/1000 
	 loss: 17.5411, MinusLogProbMetric: 17.5411, val_loss: 18.1272, val_MinusLogProbMetric: 18.1272

Epoch 268: val_loss did not improve from 17.58809
196/196 - 57s - loss: 17.5411 - MinusLogProbMetric: 17.5411 - val_loss: 18.1272 - val_MinusLogProbMetric: 18.1272 - lr: 3.3333e-04 - 57s/epoch - 288ms/step
Epoch 269/1000
2023-09-28 06:39:07.406 
Epoch 269/1000 
	 loss: 17.5131, MinusLogProbMetric: 17.5131, val_loss: 17.6752, val_MinusLogProbMetric: 17.6752

Epoch 269: val_loss did not improve from 17.58809
196/196 - 61s - loss: 17.5131 - MinusLogProbMetric: 17.5131 - val_loss: 17.6752 - val_MinusLogProbMetric: 17.6752 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 270/1000
2023-09-28 06:40:08.923 
Epoch 270/1000 
	 loss: 17.6345, MinusLogProbMetric: 17.6345, val_loss: 17.7105, val_MinusLogProbMetric: 17.7105

Epoch 270: val_loss did not improve from 17.58809
196/196 - 62s - loss: 17.6345 - MinusLogProbMetric: 17.6345 - val_loss: 17.7105 - val_MinusLogProbMetric: 17.7105 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 271/1000
2023-09-28 06:41:05.128 
Epoch 271/1000 
	 loss: 17.6056, MinusLogProbMetric: 17.6056, val_loss: 17.8975, val_MinusLogProbMetric: 17.8975

Epoch 271: val_loss did not improve from 17.58809
196/196 - 56s - loss: 17.6056 - MinusLogProbMetric: 17.6056 - val_loss: 17.8975 - val_MinusLogProbMetric: 17.8975 - lr: 3.3333e-04 - 56s/epoch - 287ms/step
Epoch 272/1000
2023-09-28 06:42:05.791 
Epoch 272/1000 
	 loss: 17.4586, MinusLogProbMetric: 17.4586, val_loss: 17.4955, val_MinusLogProbMetric: 17.4955

Epoch 272: val_loss improved from 17.58809 to 17.49551, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 62s - loss: 17.4586 - MinusLogProbMetric: 17.4586 - val_loss: 17.4955 - val_MinusLogProbMetric: 17.4955 - lr: 3.3333e-04 - 62s/epoch - 314ms/step
Epoch 273/1000
2023-09-28 06:43:08.129 
Epoch 273/1000 
	 loss: 17.4772, MinusLogProbMetric: 17.4772, val_loss: 17.8798, val_MinusLogProbMetric: 17.8798

Epoch 273: val_loss did not improve from 17.49551
196/196 - 61s - loss: 17.4772 - MinusLogProbMetric: 17.4772 - val_loss: 17.8798 - val_MinusLogProbMetric: 17.8798 - lr: 3.3333e-04 - 61s/epoch - 313ms/step
Epoch 274/1000
2023-09-28 06:44:03.961 
Epoch 274/1000 
	 loss: 17.5152, MinusLogProbMetric: 17.5152, val_loss: 17.8334, val_MinusLogProbMetric: 17.8334

Epoch 274: val_loss did not improve from 17.49551
196/196 - 56s - loss: 17.5152 - MinusLogProbMetric: 17.5152 - val_loss: 17.8334 - val_MinusLogProbMetric: 17.8334 - lr: 3.3333e-04 - 56s/epoch - 285ms/step
Epoch 275/1000
2023-09-28 06:45:03.593 
Epoch 275/1000 
	 loss: 17.5005, MinusLogProbMetric: 17.5005, val_loss: 18.7888, val_MinusLogProbMetric: 18.7888

Epoch 275: val_loss did not improve from 17.49551
196/196 - 60s - loss: 17.5005 - MinusLogProbMetric: 17.5005 - val_loss: 18.7888 - val_MinusLogProbMetric: 18.7888 - lr: 3.3333e-04 - 60s/epoch - 304ms/step
Epoch 276/1000
2023-09-28 06:46:07.461 
Epoch 276/1000 
	 loss: 17.5655, MinusLogProbMetric: 17.5655, val_loss: 17.9714, val_MinusLogProbMetric: 17.9714

Epoch 276: val_loss did not improve from 17.49551
196/196 - 64s - loss: 17.5655 - MinusLogProbMetric: 17.5655 - val_loss: 17.9714 - val_MinusLogProbMetric: 17.9714 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 277/1000
2023-09-28 06:47:03.237 
Epoch 277/1000 
	 loss: 17.4958, MinusLogProbMetric: 17.4958, val_loss: 17.7123, val_MinusLogProbMetric: 17.7123

Epoch 277: val_loss did not improve from 17.49551
196/196 - 56s - loss: 17.4958 - MinusLogProbMetric: 17.4958 - val_loss: 17.7123 - val_MinusLogProbMetric: 17.7123 - lr: 3.3333e-04 - 56s/epoch - 285ms/step
Epoch 278/1000
2023-09-28 06:47:58.756 
Epoch 278/1000 
	 loss: 17.4667, MinusLogProbMetric: 17.4667, val_loss: 17.6999, val_MinusLogProbMetric: 17.6999

Epoch 278: val_loss did not improve from 17.49551
196/196 - 56s - loss: 17.4667 - MinusLogProbMetric: 17.4667 - val_loss: 17.6999 - val_MinusLogProbMetric: 17.6999 - lr: 3.3333e-04 - 56s/epoch - 283ms/step
Epoch 279/1000
2023-09-28 06:49:01.601 
Epoch 279/1000 
	 loss: 17.4522, MinusLogProbMetric: 17.4522, val_loss: 18.0786, val_MinusLogProbMetric: 18.0786

Epoch 279: val_loss did not improve from 17.49551
196/196 - 63s - loss: 17.4522 - MinusLogProbMetric: 17.4522 - val_loss: 18.0786 - val_MinusLogProbMetric: 18.0786 - lr: 3.3333e-04 - 63s/epoch - 321ms/step
Epoch 280/1000
2023-09-28 06:49:58.623 
Epoch 280/1000 
	 loss: 17.5593, MinusLogProbMetric: 17.5593, val_loss: 17.8527, val_MinusLogProbMetric: 17.8527

Epoch 280: val_loss did not improve from 17.49551
196/196 - 57s - loss: 17.5593 - MinusLogProbMetric: 17.5593 - val_loss: 17.8527 - val_MinusLogProbMetric: 17.8527 - lr: 3.3333e-04 - 57s/epoch - 291ms/step
Epoch 281/1000
2023-09-28 06:50:53.957 
Epoch 281/1000 
	 loss: 17.5259, MinusLogProbMetric: 17.5259, val_loss: 18.2144, val_MinusLogProbMetric: 18.2144

Epoch 281: val_loss did not improve from 17.49551
196/196 - 55s - loss: 17.5259 - MinusLogProbMetric: 17.5259 - val_loss: 18.2144 - val_MinusLogProbMetric: 18.2144 - lr: 3.3333e-04 - 55s/epoch - 282ms/step
Epoch 282/1000
2023-09-28 06:51:55.000 
Epoch 282/1000 
	 loss: 17.4160, MinusLogProbMetric: 17.4160, val_loss: 17.9790, val_MinusLogProbMetric: 17.9790

Epoch 282: val_loss did not improve from 17.49551
196/196 - 61s - loss: 17.4160 - MinusLogProbMetric: 17.4160 - val_loss: 17.9790 - val_MinusLogProbMetric: 17.9790 - lr: 3.3333e-04 - 61s/epoch - 311ms/step
Epoch 283/1000
2023-09-28 06:52:57.026 
Epoch 283/1000 
	 loss: 17.4671, MinusLogProbMetric: 17.4671, val_loss: 17.6102, val_MinusLogProbMetric: 17.6102

Epoch 283: val_loss did not improve from 17.49551
196/196 - 62s - loss: 17.4671 - MinusLogProbMetric: 17.4671 - val_loss: 17.6102 - val_MinusLogProbMetric: 17.6102 - lr: 3.3333e-04 - 62s/epoch - 316ms/step
Epoch 284/1000
2023-09-28 06:53:52.077 
Epoch 284/1000 
	 loss: 17.4156, MinusLogProbMetric: 17.4156, val_loss: 18.1004, val_MinusLogProbMetric: 18.1004

Epoch 284: val_loss did not improve from 17.49551
196/196 - 55s - loss: 17.4156 - MinusLogProbMetric: 17.4156 - val_loss: 18.1004 - val_MinusLogProbMetric: 18.1004 - lr: 3.3333e-04 - 55s/epoch - 281ms/step
Epoch 285/1000
2023-09-28 06:54:48.293 
Epoch 285/1000 
	 loss: 17.5365, MinusLogProbMetric: 17.5365, val_loss: 18.0205, val_MinusLogProbMetric: 18.0205

Epoch 285: val_loss did not improve from 17.49551
196/196 - 56s - loss: 17.5365 - MinusLogProbMetric: 17.5365 - val_loss: 18.0205 - val_MinusLogProbMetric: 18.0205 - lr: 3.3333e-04 - 56s/epoch - 287ms/step
Epoch 286/1000
2023-09-28 06:55:52.961 
Epoch 286/1000 
	 loss: 17.6045, MinusLogProbMetric: 17.6045, val_loss: 17.9190, val_MinusLogProbMetric: 17.9190

Epoch 286: val_loss did not improve from 17.49551
196/196 - 65s - loss: 17.6045 - MinusLogProbMetric: 17.6045 - val_loss: 17.9190 - val_MinusLogProbMetric: 17.9190 - lr: 3.3333e-04 - 65s/epoch - 330ms/step
Epoch 287/1000
2023-09-28 06:56:50.159 
Epoch 287/1000 
	 loss: 17.4266, MinusLogProbMetric: 17.4266, val_loss: 18.5313, val_MinusLogProbMetric: 18.5313

Epoch 287: val_loss did not improve from 17.49551
196/196 - 57s - loss: 17.4266 - MinusLogProbMetric: 17.4266 - val_loss: 18.5313 - val_MinusLogProbMetric: 18.5313 - lr: 3.3333e-04 - 57s/epoch - 292ms/step
Epoch 288/1000
2023-09-28 06:57:46.946 
Epoch 288/1000 
	 loss: 17.4593, MinusLogProbMetric: 17.4593, val_loss: 18.0305, val_MinusLogProbMetric: 18.0305

Epoch 288: val_loss did not improve from 17.49551
196/196 - 57s - loss: 17.4593 - MinusLogProbMetric: 17.4593 - val_loss: 18.0305 - val_MinusLogProbMetric: 18.0305 - lr: 3.3333e-04 - 57s/epoch - 290ms/step
Epoch 289/1000
2023-09-28 06:58:49.966 
Epoch 289/1000 
	 loss: 17.4640, MinusLogProbMetric: 17.4640, val_loss: 18.5761, val_MinusLogProbMetric: 18.5761

Epoch 289: val_loss did not improve from 17.49551
196/196 - 63s - loss: 17.4640 - MinusLogProbMetric: 17.4640 - val_loss: 18.5761 - val_MinusLogProbMetric: 18.5761 - lr: 3.3333e-04 - 63s/epoch - 322ms/step
Epoch 290/1000
2023-09-28 06:59:49.248 
Epoch 290/1000 
	 loss: 17.3971, MinusLogProbMetric: 17.3971, val_loss: 18.1741, val_MinusLogProbMetric: 18.1741

Epoch 290: val_loss did not improve from 17.49551
196/196 - 59s - loss: 17.3971 - MinusLogProbMetric: 17.3971 - val_loss: 18.1741 - val_MinusLogProbMetric: 18.1741 - lr: 3.3333e-04 - 59s/epoch - 302ms/step
Epoch 291/1000
2023-09-28 07:00:45.125 
Epoch 291/1000 
	 loss: 17.5244, MinusLogProbMetric: 17.5244, val_loss: 18.5252, val_MinusLogProbMetric: 18.5252

Epoch 291: val_loss did not improve from 17.49551
196/196 - 56s - loss: 17.5244 - MinusLogProbMetric: 17.5244 - val_loss: 18.5252 - val_MinusLogProbMetric: 18.5252 - lr: 3.3333e-04 - 56s/epoch - 285ms/step
Epoch 292/1000
2023-09-28 07:01:46.853 
Epoch 292/1000 
	 loss: 17.4681, MinusLogProbMetric: 17.4681, val_loss: 18.5076, val_MinusLogProbMetric: 18.5076

Epoch 292: val_loss did not improve from 17.49551
196/196 - 62s - loss: 17.4681 - MinusLogProbMetric: 17.4681 - val_loss: 18.5076 - val_MinusLogProbMetric: 18.5076 - lr: 3.3333e-04 - 62s/epoch - 315ms/step
Epoch 293/1000
2023-09-28 07:02:51.705 
Epoch 293/1000 
	 loss: 17.3856, MinusLogProbMetric: 17.3856, val_loss: 18.3579, val_MinusLogProbMetric: 18.3579

Epoch 293: val_loss did not improve from 17.49551
196/196 - 65s - loss: 17.3856 - MinusLogProbMetric: 17.3856 - val_loss: 18.3579 - val_MinusLogProbMetric: 18.3579 - lr: 3.3333e-04 - 65s/epoch - 331ms/step
Epoch 294/1000
2023-09-28 07:03:51.338 
Epoch 294/1000 
	 loss: 17.3907, MinusLogProbMetric: 17.3907, val_loss: 18.7945, val_MinusLogProbMetric: 18.7945

Epoch 294: val_loss did not improve from 17.49551
196/196 - 60s - loss: 17.3907 - MinusLogProbMetric: 17.3907 - val_loss: 18.7945 - val_MinusLogProbMetric: 18.7945 - lr: 3.3333e-04 - 60s/epoch - 304ms/step
Epoch 295/1000
2023-09-28 07:04:56.880 
Epoch 295/1000 
	 loss: 17.4238, MinusLogProbMetric: 17.4238, val_loss: 18.2892, val_MinusLogProbMetric: 18.2892

Epoch 295: val_loss did not improve from 17.49551
196/196 - 66s - loss: 17.4238 - MinusLogProbMetric: 17.4238 - val_loss: 18.2892 - val_MinusLogProbMetric: 18.2892 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 296/1000
2023-09-28 07:06:02.420 
Epoch 296/1000 
	 loss: 17.5944, MinusLogProbMetric: 17.5944, val_loss: 17.9039, val_MinusLogProbMetric: 17.9039

Epoch 296: val_loss did not improve from 17.49551
196/196 - 66s - loss: 17.5944 - MinusLogProbMetric: 17.5944 - val_loss: 17.9039 - val_MinusLogProbMetric: 17.9039 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 297/1000
2023-09-28 07:07:08.242 
Epoch 297/1000 
	 loss: 17.4555, MinusLogProbMetric: 17.4555, val_loss: 17.6962, val_MinusLogProbMetric: 17.6962

Epoch 297: val_loss did not improve from 17.49551
196/196 - 66s - loss: 17.4555 - MinusLogProbMetric: 17.4555 - val_loss: 17.6962 - val_MinusLogProbMetric: 17.6962 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 298/1000
2023-09-28 07:08:14.064 
Epoch 298/1000 
	 loss: 17.3970, MinusLogProbMetric: 17.3970, val_loss: 17.7253, val_MinusLogProbMetric: 17.7253

Epoch 298: val_loss did not improve from 17.49551
196/196 - 66s - loss: 17.3970 - MinusLogProbMetric: 17.3970 - val_loss: 17.7253 - val_MinusLogProbMetric: 17.7253 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 299/1000
2023-09-28 07:09:17.945 
Epoch 299/1000 
	 loss: 17.4413, MinusLogProbMetric: 17.4413, val_loss: 17.6471, val_MinusLogProbMetric: 17.6471

Epoch 299: val_loss did not improve from 17.49551
196/196 - 64s - loss: 17.4413 - MinusLogProbMetric: 17.4413 - val_loss: 17.6471 - val_MinusLogProbMetric: 17.6471 - lr: 3.3333e-04 - 64s/epoch - 326ms/step
Epoch 300/1000
2023-09-28 07:10:17.906 
Epoch 300/1000 
	 loss: 17.4150, MinusLogProbMetric: 17.4150, val_loss: 17.9374, val_MinusLogProbMetric: 17.9374

Epoch 300: val_loss did not improve from 17.49551
196/196 - 60s - loss: 17.4150 - MinusLogProbMetric: 17.4150 - val_loss: 17.9374 - val_MinusLogProbMetric: 17.9374 - lr: 3.3333e-04 - 60s/epoch - 306ms/step
Epoch 301/1000
2023-09-28 07:11:23.422 
Epoch 301/1000 
	 loss: 17.4683, MinusLogProbMetric: 17.4683, val_loss: 18.0079, val_MinusLogProbMetric: 18.0079

Epoch 301: val_loss did not improve from 17.49551
196/196 - 66s - loss: 17.4683 - MinusLogProbMetric: 17.4683 - val_loss: 18.0079 - val_MinusLogProbMetric: 18.0079 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 302/1000
2023-09-28 07:12:28.995 
Epoch 302/1000 
	 loss: 17.3916, MinusLogProbMetric: 17.3916, val_loss: 18.9065, val_MinusLogProbMetric: 18.9065

Epoch 302: val_loss did not improve from 17.49551
196/196 - 66s - loss: 17.3916 - MinusLogProbMetric: 17.3916 - val_loss: 18.9065 - val_MinusLogProbMetric: 18.9065 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 303/1000
2023-09-28 07:13:34.838 
Epoch 303/1000 
	 loss: 17.4378, MinusLogProbMetric: 17.4378, val_loss: 17.4906, val_MinusLogProbMetric: 17.4906

Epoch 303: val_loss improved from 17.49551 to 17.49062, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 17.4378 - MinusLogProbMetric: 17.4378 - val_loss: 17.4906 - val_MinusLogProbMetric: 17.4906 - lr: 3.3333e-04 - 67s/epoch - 341ms/step
Epoch 304/1000
2023-09-28 07:14:42.015 
Epoch 304/1000 
	 loss: 17.3943, MinusLogProbMetric: 17.3943, val_loss: 17.6102, val_MinusLogProbMetric: 17.6102

Epoch 304: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3943 - MinusLogProbMetric: 17.3943 - val_loss: 17.6102 - val_MinusLogProbMetric: 17.6102 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 305/1000
2023-09-28 07:15:47.777 
Epoch 305/1000 
	 loss: 17.5236, MinusLogProbMetric: 17.5236, val_loss: 17.6541, val_MinusLogProbMetric: 17.6541

Epoch 305: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.5236 - MinusLogProbMetric: 17.5236 - val_loss: 17.6541 - val_MinusLogProbMetric: 17.6541 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 306/1000
2023-09-28 07:16:53.526 
Epoch 306/1000 
	 loss: 17.4405, MinusLogProbMetric: 17.4405, val_loss: 17.6786, val_MinusLogProbMetric: 17.6786

Epoch 306: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.4405 - MinusLogProbMetric: 17.4405 - val_loss: 17.6786 - val_MinusLogProbMetric: 17.6786 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 307/1000
2023-09-28 07:17:59.520 
Epoch 307/1000 
	 loss: 17.4255, MinusLogProbMetric: 17.4255, val_loss: 18.8761, val_MinusLogProbMetric: 18.8761

Epoch 307: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.4255 - MinusLogProbMetric: 17.4255 - val_loss: 18.8761 - val_MinusLogProbMetric: 18.8761 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 308/1000
2023-09-28 07:19:05.413 
Epoch 308/1000 
	 loss: 17.4105, MinusLogProbMetric: 17.4105, val_loss: 18.2228, val_MinusLogProbMetric: 18.2228

Epoch 308: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.4105 - MinusLogProbMetric: 17.4105 - val_loss: 18.2228 - val_MinusLogProbMetric: 18.2228 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 309/1000
2023-09-28 07:20:11.254 
Epoch 309/1000 
	 loss: 17.3672, MinusLogProbMetric: 17.3672, val_loss: 18.3837, val_MinusLogProbMetric: 18.3837

Epoch 309: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3672 - MinusLogProbMetric: 17.3672 - val_loss: 18.3837 - val_MinusLogProbMetric: 18.3837 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 310/1000
2023-09-28 07:21:17.298 
Epoch 310/1000 
	 loss: 17.4234, MinusLogProbMetric: 17.4234, val_loss: 18.5949, val_MinusLogProbMetric: 18.5949

Epoch 310: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.4234 - MinusLogProbMetric: 17.4234 - val_loss: 18.5949 - val_MinusLogProbMetric: 18.5949 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 311/1000
2023-09-28 07:22:22.899 
Epoch 311/1000 
	 loss: 17.4051, MinusLogProbMetric: 17.4051, val_loss: 17.9881, val_MinusLogProbMetric: 17.9881

Epoch 311: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.4051 - MinusLogProbMetric: 17.4051 - val_loss: 17.9881 - val_MinusLogProbMetric: 17.9881 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 312/1000
2023-09-28 07:23:29.213 
Epoch 312/1000 
	 loss: 17.5028, MinusLogProbMetric: 17.5028, val_loss: 17.8137, val_MinusLogProbMetric: 17.8137

Epoch 312: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.5028 - MinusLogProbMetric: 17.5028 - val_loss: 17.8137 - val_MinusLogProbMetric: 17.8137 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 313/1000
2023-09-28 07:24:35.183 
Epoch 313/1000 
	 loss: 17.2953, MinusLogProbMetric: 17.2953, val_loss: 18.2564, val_MinusLogProbMetric: 18.2564

Epoch 313: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.2953 - MinusLogProbMetric: 17.2953 - val_loss: 18.2564 - val_MinusLogProbMetric: 18.2564 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 314/1000
2023-09-28 07:25:40.936 
Epoch 314/1000 
	 loss: 17.3878, MinusLogProbMetric: 17.3878, val_loss: 17.9575, val_MinusLogProbMetric: 17.9575

Epoch 314: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3878 - MinusLogProbMetric: 17.3878 - val_loss: 17.9575 - val_MinusLogProbMetric: 17.9575 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 315/1000
2023-09-28 07:26:46.449 
Epoch 315/1000 
	 loss: 17.4327, MinusLogProbMetric: 17.4327, val_loss: 18.2211, val_MinusLogProbMetric: 18.2211

Epoch 315: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.4327 - MinusLogProbMetric: 17.4327 - val_loss: 18.2211 - val_MinusLogProbMetric: 18.2211 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 316/1000
2023-09-28 07:27:51.840 
Epoch 316/1000 
	 loss: 17.3540, MinusLogProbMetric: 17.3540, val_loss: 18.0596, val_MinusLogProbMetric: 18.0596

Epoch 316: val_loss did not improve from 17.49062
196/196 - 65s - loss: 17.3540 - MinusLogProbMetric: 17.3540 - val_loss: 18.0596 - val_MinusLogProbMetric: 18.0596 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 317/1000
2023-09-28 07:28:57.784 
Epoch 317/1000 
	 loss: 17.4103, MinusLogProbMetric: 17.4103, val_loss: 17.6603, val_MinusLogProbMetric: 17.6603

Epoch 317: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.4103 - MinusLogProbMetric: 17.4103 - val_loss: 17.6603 - val_MinusLogProbMetric: 17.6603 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 318/1000
2023-09-28 07:30:03.780 
Epoch 318/1000 
	 loss: 17.4284, MinusLogProbMetric: 17.4284, val_loss: 18.0544, val_MinusLogProbMetric: 18.0544

Epoch 318: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.4284 - MinusLogProbMetric: 17.4284 - val_loss: 18.0544 - val_MinusLogProbMetric: 18.0544 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 319/1000
2023-09-28 07:31:09.376 
Epoch 319/1000 
	 loss: 17.3857, MinusLogProbMetric: 17.3857, val_loss: 17.6118, val_MinusLogProbMetric: 17.6118

Epoch 319: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3857 - MinusLogProbMetric: 17.3857 - val_loss: 17.6118 - val_MinusLogProbMetric: 17.6118 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 320/1000
2023-09-28 07:32:15.514 
Epoch 320/1000 
	 loss: 17.3573, MinusLogProbMetric: 17.3573, val_loss: 17.7356, val_MinusLogProbMetric: 17.7356

Epoch 320: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3573 - MinusLogProbMetric: 17.3573 - val_loss: 17.7356 - val_MinusLogProbMetric: 17.7356 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 321/1000
2023-09-28 07:33:21.180 
Epoch 321/1000 
	 loss: 17.3274, MinusLogProbMetric: 17.3274, val_loss: 17.8732, val_MinusLogProbMetric: 17.8732

Epoch 321: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3274 - MinusLogProbMetric: 17.3274 - val_loss: 17.8732 - val_MinusLogProbMetric: 17.8732 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 322/1000
2023-09-28 07:34:27.035 
Epoch 322/1000 
	 loss: 17.3619, MinusLogProbMetric: 17.3619, val_loss: 17.5438, val_MinusLogProbMetric: 17.5438

Epoch 322: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3619 - MinusLogProbMetric: 17.3619 - val_loss: 17.5438 - val_MinusLogProbMetric: 17.5438 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 323/1000
2023-09-28 07:35:33.054 
Epoch 323/1000 
	 loss: 17.3452, MinusLogProbMetric: 17.3452, val_loss: 17.9973, val_MinusLogProbMetric: 17.9973

Epoch 323: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3452 - MinusLogProbMetric: 17.3452 - val_loss: 17.9973 - val_MinusLogProbMetric: 17.9973 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 324/1000
2023-09-28 07:36:38.921 
Epoch 324/1000 
	 loss: 17.3549, MinusLogProbMetric: 17.3549, val_loss: 17.6615, val_MinusLogProbMetric: 17.6615

Epoch 324: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3549 - MinusLogProbMetric: 17.3549 - val_loss: 17.6615 - val_MinusLogProbMetric: 17.6615 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 325/1000
2023-09-28 07:37:44.686 
Epoch 325/1000 
	 loss: 17.3583, MinusLogProbMetric: 17.3583, val_loss: 17.8564, val_MinusLogProbMetric: 17.8564

Epoch 325: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3583 - MinusLogProbMetric: 17.3583 - val_loss: 17.8564 - val_MinusLogProbMetric: 17.8564 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 326/1000
2023-09-28 07:38:50.202 
Epoch 326/1000 
	 loss: 17.3229, MinusLogProbMetric: 17.3229, val_loss: 18.0709, val_MinusLogProbMetric: 18.0709

Epoch 326: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3229 - MinusLogProbMetric: 17.3229 - val_loss: 18.0709 - val_MinusLogProbMetric: 18.0709 - lr: 3.3333e-04 - 66s/epoch - 334ms/step
Epoch 327/1000
2023-09-28 07:39:56.160 
Epoch 327/1000 
	 loss: 17.3592, MinusLogProbMetric: 17.3592, val_loss: 17.6071, val_MinusLogProbMetric: 17.6071

Epoch 327: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3592 - MinusLogProbMetric: 17.3592 - val_loss: 17.6071 - val_MinusLogProbMetric: 17.6071 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 328/1000
2023-09-28 07:41:02.218 
Epoch 328/1000 
	 loss: 17.3243, MinusLogProbMetric: 17.3243, val_loss: 17.5831, val_MinusLogProbMetric: 17.5831

Epoch 328: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3243 - MinusLogProbMetric: 17.3243 - val_loss: 17.5831 - val_MinusLogProbMetric: 17.5831 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 329/1000
2023-09-28 07:42:08.143 
Epoch 329/1000 
	 loss: 17.3473, MinusLogProbMetric: 17.3473, val_loss: 17.7997, val_MinusLogProbMetric: 17.7997

Epoch 329: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3473 - MinusLogProbMetric: 17.3473 - val_loss: 17.7997 - val_MinusLogProbMetric: 17.7997 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 330/1000
2023-09-28 07:43:14.049 
Epoch 330/1000 
	 loss: 17.3969, MinusLogProbMetric: 17.3969, val_loss: 17.4979, val_MinusLogProbMetric: 17.4979

Epoch 330: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3969 - MinusLogProbMetric: 17.3969 - val_loss: 17.4979 - val_MinusLogProbMetric: 17.4979 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 331/1000
2023-09-28 07:44:19.833 
Epoch 331/1000 
	 loss: 17.3280, MinusLogProbMetric: 17.3280, val_loss: 17.5729, val_MinusLogProbMetric: 17.5729

Epoch 331: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3280 - MinusLogProbMetric: 17.3280 - val_loss: 17.5729 - val_MinusLogProbMetric: 17.5729 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 332/1000
2023-09-28 07:45:25.518 
Epoch 332/1000 
	 loss: 17.3459, MinusLogProbMetric: 17.3459, val_loss: 17.7561, val_MinusLogProbMetric: 17.7561

Epoch 332: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3459 - MinusLogProbMetric: 17.3459 - val_loss: 17.7561 - val_MinusLogProbMetric: 17.7561 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 333/1000
2023-09-28 07:46:31.651 
Epoch 333/1000 
	 loss: 17.3342, MinusLogProbMetric: 17.3342, val_loss: 18.1607, val_MinusLogProbMetric: 18.1607

Epoch 333: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3342 - MinusLogProbMetric: 17.3342 - val_loss: 18.1607 - val_MinusLogProbMetric: 18.1607 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 334/1000
2023-09-28 07:47:37.660 
Epoch 334/1000 
	 loss: 17.4110, MinusLogProbMetric: 17.4110, val_loss: 17.9601, val_MinusLogProbMetric: 17.9601

Epoch 334: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.4110 - MinusLogProbMetric: 17.4110 - val_loss: 17.9601 - val_MinusLogProbMetric: 17.9601 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 335/1000
2023-09-28 07:48:44.138 
Epoch 335/1000 
	 loss: 17.2913, MinusLogProbMetric: 17.2913, val_loss: 18.5906, val_MinusLogProbMetric: 18.5906

Epoch 335: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.2913 - MinusLogProbMetric: 17.2913 - val_loss: 18.5906 - val_MinusLogProbMetric: 18.5906 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 336/1000
2023-09-28 07:49:50.630 
Epoch 336/1000 
	 loss: 17.3317, MinusLogProbMetric: 17.3317, val_loss: 17.9096, val_MinusLogProbMetric: 17.9096

Epoch 336: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3317 - MinusLogProbMetric: 17.3317 - val_loss: 17.9096 - val_MinusLogProbMetric: 17.9096 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 337/1000
2023-09-28 07:50:56.807 
Epoch 337/1000 
	 loss: 17.3401, MinusLogProbMetric: 17.3401, val_loss: 18.0549, val_MinusLogProbMetric: 18.0549

Epoch 337: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3401 - MinusLogProbMetric: 17.3401 - val_loss: 18.0549 - val_MinusLogProbMetric: 18.0549 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 338/1000
2023-09-28 07:52:03.040 
Epoch 338/1000 
	 loss: 17.3216, MinusLogProbMetric: 17.3216, val_loss: 17.7588, val_MinusLogProbMetric: 17.7588

Epoch 338: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3216 - MinusLogProbMetric: 17.3216 - val_loss: 17.7588 - val_MinusLogProbMetric: 17.7588 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 339/1000
2023-09-28 07:53:08.999 
Epoch 339/1000 
	 loss: 17.3570, MinusLogProbMetric: 17.3570, val_loss: 17.6614, val_MinusLogProbMetric: 17.6614

Epoch 339: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3570 - MinusLogProbMetric: 17.3570 - val_loss: 17.6614 - val_MinusLogProbMetric: 17.6614 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 340/1000
2023-09-28 07:54:15.028 
Epoch 340/1000 
	 loss: 17.3150, MinusLogProbMetric: 17.3150, val_loss: 18.1229, val_MinusLogProbMetric: 18.1229

Epoch 340: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3150 - MinusLogProbMetric: 17.3150 - val_loss: 18.1229 - val_MinusLogProbMetric: 18.1229 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 341/1000
2023-09-28 07:55:21.103 
Epoch 341/1000 
	 loss: 17.3358, MinusLogProbMetric: 17.3358, val_loss: 17.8695, val_MinusLogProbMetric: 17.8695

Epoch 341: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3358 - MinusLogProbMetric: 17.3358 - val_loss: 17.8695 - val_MinusLogProbMetric: 17.8695 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 342/1000
2023-09-28 07:56:27.519 
Epoch 342/1000 
	 loss: 17.2874, MinusLogProbMetric: 17.2874, val_loss: 17.8642, val_MinusLogProbMetric: 17.8642

Epoch 342: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.2874 - MinusLogProbMetric: 17.2874 - val_loss: 17.8642 - val_MinusLogProbMetric: 17.8642 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 343/1000
2023-09-28 07:57:33.900 
Epoch 343/1000 
	 loss: 17.3255, MinusLogProbMetric: 17.3255, val_loss: 17.7617, val_MinusLogProbMetric: 17.7617

Epoch 343: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3255 - MinusLogProbMetric: 17.3255 - val_loss: 17.7617 - val_MinusLogProbMetric: 17.7617 - lr: 3.3333e-04 - 66s/epoch - 339ms/step
Epoch 344/1000
2023-09-28 07:58:40.162 
Epoch 344/1000 
	 loss: 17.2879, MinusLogProbMetric: 17.2879, val_loss: 17.5979, val_MinusLogProbMetric: 17.5979

Epoch 344: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.2879 - MinusLogProbMetric: 17.2879 - val_loss: 17.5979 - val_MinusLogProbMetric: 17.5979 - lr: 3.3333e-04 - 66s/epoch - 338ms/step
Epoch 345/1000
2023-09-28 07:59:46.219 
Epoch 345/1000 
	 loss: 17.3567, MinusLogProbMetric: 17.3567, val_loss: 18.0672, val_MinusLogProbMetric: 18.0672

Epoch 345: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3567 - MinusLogProbMetric: 17.3567 - val_loss: 18.0672 - val_MinusLogProbMetric: 18.0672 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 346/1000
2023-09-28 08:00:52.357 
Epoch 346/1000 
	 loss: 17.3269, MinusLogProbMetric: 17.3269, val_loss: 17.6060, val_MinusLogProbMetric: 17.6060

Epoch 346: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3269 - MinusLogProbMetric: 17.3269 - val_loss: 17.6060 - val_MinusLogProbMetric: 17.6060 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 347/1000
2023-09-28 08:01:58.372 
Epoch 347/1000 
	 loss: 17.3061, MinusLogProbMetric: 17.3061, val_loss: 17.7721, val_MinusLogProbMetric: 17.7721

Epoch 347: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3061 - MinusLogProbMetric: 17.3061 - val_loss: 17.7721 - val_MinusLogProbMetric: 17.7721 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 348/1000
2023-09-28 08:03:04.312 
Epoch 348/1000 
	 loss: 17.2448, MinusLogProbMetric: 17.2448, val_loss: 17.6666, val_MinusLogProbMetric: 17.6666

Epoch 348: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.2448 - MinusLogProbMetric: 17.2448 - val_loss: 17.6666 - val_MinusLogProbMetric: 17.6666 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 349/1000
2023-09-28 08:04:10.195 
Epoch 349/1000 
	 loss: 17.3183, MinusLogProbMetric: 17.3183, val_loss: 18.5183, val_MinusLogProbMetric: 18.5183

Epoch 349: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3183 - MinusLogProbMetric: 17.3183 - val_loss: 18.5183 - val_MinusLogProbMetric: 18.5183 - lr: 3.3333e-04 - 66s/epoch - 336ms/step
Epoch 350/1000
2023-09-28 08:05:16.729 
Epoch 350/1000 
	 loss: 17.2789, MinusLogProbMetric: 17.2789, val_loss: 17.5260, val_MinusLogProbMetric: 17.5260

Epoch 350: val_loss did not improve from 17.49062
196/196 - 67s - loss: 17.2789 - MinusLogProbMetric: 17.2789 - val_loss: 17.5260 - val_MinusLogProbMetric: 17.5260 - lr: 3.3333e-04 - 67s/epoch - 339ms/step
Epoch 351/1000
2023-09-28 08:06:22.402 
Epoch 351/1000 
	 loss: 17.3092, MinusLogProbMetric: 17.3092, val_loss: 17.7368, val_MinusLogProbMetric: 17.7368

Epoch 351: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3092 - MinusLogProbMetric: 17.3092 - val_loss: 17.7368 - val_MinusLogProbMetric: 17.7368 - lr: 3.3333e-04 - 66s/epoch - 335ms/step
Epoch 352/1000
2023-09-28 08:07:28.507 
Epoch 352/1000 
	 loss: 17.3752, MinusLogProbMetric: 17.3752, val_loss: 17.6401, val_MinusLogProbMetric: 17.6401

Epoch 352: val_loss did not improve from 17.49062
196/196 - 66s - loss: 17.3752 - MinusLogProbMetric: 17.3752 - val_loss: 17.6401 - val_MinusLogProbMetric: 17.6401 - lr: 3.3333e-04 - 66s/epoch - 337ms/step
Epoch 353/1000
2023-09-28 08:08:33.998 
Epoch 353/1000 
	 loss: 17.2665, MinusLogProbMetric: 17.2665, val_loss: 17.7970, val_MinusLogProbMetric: 17.7970

Epoch 353: val_loss did not improve from 17.49062
196/196 - 65s - loss: 17.2665 - MinusLogProbMetric: 17.2665 - val_loss: 17.7970 - val_MinusLogProbMetric: 17.7970 - lr: 3.3333e-04 - 65s/epoch - 334ms/step
Epoch 354/1000
2023-09-28 08:09:40.093 
Epoch 354/1000 
	 loss: 16.8311, MinusLogProbMetric: 16.8311, val_loss: 17.2833, val_MinusLogProbMetric: 17.2833

Epoch 354: val_loss improved from 17.49062 to 17.28330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 16.8311 - MinusLogProbMetric: 16.8311 - val_loss: 17.2833 - val_MinusLogProbMetric: 17.2833 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 355/1000
2023-09-28 08:10:47.282 
Epoch 355/1000 
	 loss: 16.7795, MinusLogProbMetric: 16.7795, val_loss: 17.3299, val_MinusLogProbMetric: 17.3299

Epoch 355: val_loss did not improve from 17.28330
196/196 - 66s - loss: 16.7795 - MinusLogProbMetric: 16.7795 - val_loss: 17.3299 - val_MinusLogProbMetric: 17.3299 - lr: 1.6667e-04 - 66s/epoch - 338ms/step
Epoch 356/1000
2023-09-28 08:11:53.142 
Epoch 356/1000 
	 loss: 16.7828, MinusLogProbMetric: 16.7828, val_loss: 17.3344, val_MinusLogProbMetric: 17.3344

Epoch 356: val_loss did not improve from 17.28330
196/196 - 66s - loss: 16.7828 - MinusLogProbMetric: 16.7828 - val_loss: 17.3344 - val_MinusLogProbMetric: 17.3344 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 357/1000
2023-09-28 08:12:59.016 
Epoch 357/1000 
	 loss: 16.8115, MinusLogProbMetric: 16.8115, val_loss: 17.4536, val_MinusLogProbMetric: 17.4536

Epoch 357: val_loss did not improve from 17.28330
196/196 - 66s - loss: 16.8115 - MinusLogProbMetric: 16.8115 - val_loss: 17.4536 - val_MinusLogProbMetric: 17.4536 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 358/1000
2023-09-28 08:14:05.150 
Epoch 358/1000 
	 loss: 16.8008, MinusLogProbMetric: 16.8008, val_loss: 17.3940, val_MinusLogProbMetric: 17.3940

Epoch 358: val_loss did not improve from 17.28330
196/196 - 66s - loss: 16.8008 - MinusLogProbMetric: 16.8008 - val_loss: 17.3940 - val_MinusLogProbMetric: 17.3940 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 359/1000
2023-09-28 08:15:10.847 
Epoch 359/1000 
	 loss: 16.8246, MinusLogProbMetric: 16.8246, val_loss: 17.3224, val_MinusLogProbMetric: 17.3224

Epoch 359: val_loss did not improve from 17.28330
196/196 - 66s - loss: 16.8246 - MinusLogProbMetric: 16.8246 - val_loss: 17.3224 - val_MinusLogProbMetric: 17.3224 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 360/1000
2023-09-28 08:16:16.698 
Epoch 360/1000 
	 loss: 16.8120, MinusLogProbMetric: 16.8120, val_loss: 17.3606, val_MinusLogProbMetric: 17.3606

Epoch 360: val_loss did not improve from 17.28330
196/196 - 66s - loss: 16.8120 - MinusLogProbMetric: 16.8120 - val_loss: 17.3606 - val_MinusLogProbMetric: 17.3606 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 361/1000
2023-09-28 08:17:23.141 
Epoch 361/1000 
	 loss: 16.8195, MinusLogProbMetric: 16.8195, val_loss: 17.2785, val_MinusLogProbMetric: 17.2785

Epoch 361: val_loss improved from 17.28330 to 17.27853, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 16.8195 - MinusLogProbMetric: 16.8195 - val_loss: 17.2785 - val_MinusLogProbMetric: 17.2785 - lr: 1.6667e-04 - 67s/epoch - 344ms/step
Epoch 362/1000
2023-09-28 08:18:30.345 
Epoch 362/1000 
	 loss: 16.8355, MinusLogProbMetric: 16.8355, val_loss: 17.3763, val_MinusLogProbMetric: 17.3763

Epoch 362: val_loss did not improve from 17.27853
196/196 - 66s - loss: 16.8355 - MinusLogProbMetric: 16.8355 - val_loss: 17.3763 - val_MinusLogProbMetric: 17.3763 - lr: 1.6667e-04 - 66s/epoch - 338ms/step
Epoch 363/1000
2023-09-28 08:19:37.059 
Epoch 363/1000 
	 loss: 16.8117, MinusLogProbMetric: 16.8117, val_loss: 17.3227, val_MinusLogProbMetric: 17.3227

Epoch 363: val_loss did not improve from 17.27853
196/196 - 67s - loss: 16.8117 - MinusLogProbMetric: 16.8117 - val_loss: 17.3227 - val_MinusLogProbMetric: 17.3227 - lr: 1.6667e-04 - 67s/epoch - 340ms/step
Epoch 364/1000
2023-09-28 08:20:43.147 
Epoch 364/1000 
	 loss: 16.8673, MinusLogProbMetric: 16.8673, val_loss: 17.3849, val_MinusLogProbMetric: 17.3849

Epoch 364: val_loss did not improve from 17.27853
196/196 - 66s - loss: 16.8673 - MinusLogProbMetric: 16.8673 - val_loss: 17.3849 - val_MinusLogProbMetric: 17.3849 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 365/1000
2023-09-28 08:21:49.242 
Epoch 365/1000 
	 loss: 16.7957, MinusLogProbMetric: 16.7957, val_loss: 17.2490, val_MinusLogProbMetric: 17.2490

Epoch 365: val_loss improved from 17.27853 to 17.24904, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 16.7957 - MinusLogProbMetric: 16.7957 - val_loss: 17.2490 - val_MinusLogProbMetric: 17.2490 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 366/1000
2023-09-28 08:22:56.153 
Epoch 366/1000 
	 loss: 16.7934, MinusLogProbMetric: 16.7934, val_loss: 17.3647, val_MinusLogProbMetric: 17.3647

Epoch 366: val_loss did not improve from 17.24904
196/196 - 66s - loss: 16.7934 - MinusLogProbMetric: 16.7934 - val_loss: 17.3647 - val_MinusLogProbMetric: 17.3647 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 367/1000
2023-09-28 08:24:02.068 
Epoch 367/1000 
	 loss: 16.8458, MinusLogProbMetric: 16.8458, val_loss: 17.2371, val_MinusLogProbMetric: 17.2371

Epoch 367: val_loss improved from 17.24904 to 17.23710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 16.8458 - MinusLogProbMetric: 16.8458 - val_loss: 17.2371 - val_MinusLogProbMetric: 17.2371 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 368/1000
2023-09-28 08:25:09.281 
Epoch 368/1000 
	 loss: 16.8415, MinusLogProbMetric: 16.8415, val_loss: 17.2413, val_MinusLogProbMetric: 17.2413

Epoch 368: val_loss did not improve from 17.23710
196/196 - 66s - loss: 16.8415 - MinusLogProbMetric: 16.8415 - val_loss: 17.2413 - val_MinusLogProbMetric: 17.2413 - lr: 1.6667e-04 - 66s/epoch - 338ms/step
Epoch 369/1000
2023-09-28 08:26:14.868 
Epoch 369/1000 
	 loss: 16.7809, MinusLogProbMetric: 16.7809, val_loss: 17.3273, val_MinusLogProbMetric: 17.3273

Epoch 369: val_loss did not improve from 17.23710
196/196 - 66s - loss: 16.7809 - MinusLogProbMetric: 16.7809 - val_loss: 17.3273 - val_MinusLogProbMetric: 17.3273 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 370/1000
2023-09-28 08:27:21.327 
Epoch 370/1000 
	 loss: 16.7880, MinusLogProbMetric: 16.7880, val_loss: 17.4182, val_MinusLogProbMetric: 17.4182

Epoch 370: val_loss did not improve from 17.23710
196/196 - 66s - loss: 16.7880 - MinusLogProbMetric: 16.7880 - val_loss: 17.4182 - val_MinusLogProbMetric: 17.4182 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 371/1000
2023-09-28 08:28:27.428 
Epoch 371/1000 
	 loss: 16.7625, MinusLogProbMetric: 16.7625, val_loss: 17.2617, val_MinusLogProbMetric: 17.2617

Epoch 371: val_loss did not improve from 17.23710
196/196 - 66s - loss: 16.7625 - MinusLogProbMetric: 16.7625 - val_loss: 17.2617 - val_MinusLogProbMetric: 17.2617 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 372/1000
2023-09-28 08:29:33.360 
Epoch 372/1000 
	 loss: 16.7718, MinusLogProbMetric: 16.7718, val_loss: 17.2758, val_MinusLogProbMetric: 17.2758

Epoch 372: val_loss did not improve from 17.23710
196/196 - 66s - loss: 16.7718 - MinusLogProbMetric: 16.7718 - val_loss: 17.2758 - val_MinusLogProbMetric: 17.2758 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 373/1000
2023-09-28 08:30:39.372 
Epoch 373/1000 
	 loss: 16.7640, MinusLogProbMetric: 16.7640, val_loss: 17.2901, val_MinusLogProbMetric: 17.2901

Epoch 373: val_loss did not improve from 17.23710
196/196 - 66s - loss: 16.7640 - MinusLogProbMetric: 16.7640 - val_loss: 17.2901 - val_MinusLogProbMetric: 17.2901 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 374/1000
2023-09-28 08:31:45.813 
Epoch 374/1000 
	 loss: 16.7980, MinusLogProbMetric: 16.7980, val_loss: 17.3484, val_MinusLogProbMetric: 17.3484

Epoch 374: val_loss did not improve from 17.23710
196/196 - 66s - loss: 16.7980 - MinusLogProbMetric: 16.7980 - val_loss: 17.3484 - val_MinusLogProbMetric: 17.3484 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 375/1000
2023-09-28 08:32:51.980 
Epoch 375/1000 
	 loss: 16.7572, MinusLogProbMetric: 16.7572, val_loss: 17.2568, val_MinusLogProbMetric: 17.2568

Epoch 375: val_loss did not improve from 17.23710
196/196 - 66s - loss: 16.7572 - MinusLogProbMetric: 16.7572 - val_loss: 17.2568 - val_MinusLogProbMetric: 17.2568 - lr: 1.6667e-04 - 66s/epoch - 338ms/step
Epoch 376/1000
2023-09-28 08:33:58.072 
Epoch 376/1000 
	 loss: 16.7857, MinusLogProbMetric: 16.7857, val_loss: 17.2863, val_MinusLogProbMetric: 17.2863

Epoch 376: val_loss did not improve from 17.23710
196/196 - 66s - loss: 16.7857 - MinusLogProbMetric: 16.7857 - val_loss: 17.2863 - val_MinusLogProbMetric: 17.2863 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 377/1000
2023-09-28 08:35:04.062 
Epoch 377/1000 
	 loss: 16.7914, MinusLogProbMetric: 16.7914, val_loss: 17.2227, val_MinusLogProbMetric: 17.2227

Epoch 377: val_loss improved from 17.23710 to 17.22266, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 16.7914 - MinusLogProbMetric: 16.7914 - val_loss: 17.2227 - val_MinusLogProbMetric: 17.2227 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 378/1000
2023-09-28 08:36:10.800 
Epoch 378/1000 
	 loss: 16.8337, MinusLogProbMetric: 16.8337, val_loss: 17.2937, val_MinusLogProbMetric: 17.2937

Epoch 378: val_loss did not improve from 17.22266
196/196 - 66s - loss: 16.8337 - MinusLogProbMetric: 16.8337 - val_loss: 17.2937 - val_MinusLogProbMetric: 17.2937 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 379/1000
2023-09-28 08:37:16.624 
Epoch 379/1000 
	 loss: 16.7793, MinusLogProbMetric: 16.7793, val_loss: 17.3183, val_MinusLogProbMetric: 17.3183

Epoch 379: val_loss did not improve from 17.22266
196/196 - 66s - loss: 16.7793 - MinusLogProbMetric: 16.7793 - val_loss: 17.3183 - val_MinusLogProbMetric: 17.3183 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 380/1000
2023-09-28 08:38:22.848 
Epoch 380/1000 
	 loss: 16.8043, MinusLogProbMetric: 16.8043, val_loss: 17.5171, val_MinusLogProbMetric: 17.5171

Epoch 380: val_loss did not improve from 17.22266
196/196 - 66s - loss: 16.8043 - MinusLogProbMetric: 16.8043 - val_loss: 17.5171 - val_MinusLogProbMetric: 17.5171 - lr: 1.6667e-04 - 66s/epoch - 338ms/step
Epoch 381/1000
2023-09-28 08:39:28.932 
Epoch 381/1000 
	 loss: 16.7797, MinusLogProbMetric: 16.7797, val_loss: 17.2580, val_MinusLogProbMetric: 17.2580

Epoch 381: val_loss did not improve from 17.22266
196/196 - 66s - loss: 16.7797 - MinusLogProbMetric: 16.7797 - val_loss: 17.2580 - val_MinusLogProbMetric: 17.2580 - lr: 1.6667e-04 - 66s/epoch - 337ms/step
Epoch 382/1000
2023-09-28 08:40:34.821 
Epoch 382/1000 
	 loss: 16.8329, MinusLogProbMetric: 16.8329, val_loss: 17.2980, val_MinusLogProbMetric: 17.2980

Epoch 382: val_loss did not improve from 17.22266
196/196 - 66s - loss: 16.8329 - MinusLogProbMetric: 16.8329 - val_loss: 17.2980 - val_MinusLogProbMetric: 17.2980 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 383/1000
2023-09-28 08:41:40.466 
Epoch 383/1000 
	 loss: 16.7760, MinusLogProbMetric: 16.7760, val_loss: 17.4933, val_MinusLogProbMetric: 17.4933

Epoch 383: val_loss did not improve from 17.22266
196/196 - 66s - loss: 16.7760 - MinusLogProbMetric: 16.7760 - val_loss: 17.4933 - val_MinusLogProbMetric: 17.4933 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 384/1000
2023-09-28 08:42:46.871 
Epoch 384/1000 
	 loss: 16.7863, MinusLogProbMetric: 16.7863, val_loss: 17.3451, val_MinusLogProbMetric: 17.3451

Epoch 384: val_loss did not improve from 17.22266
196/196 - 66s - loss: 16.7863 - MinusLogProbMetric: 16.7863 - val_loss: 17.3451 - val_MinusLogProbMetric: 17.3451 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 385/1000
2023-09-28 08:43:52.605 
Epoch 385/1000 
	 loss: 16.7681, MinusLogProbMetric: 16.7681, val_loss: 17.2546, val_MinusLogProbMetric: 17.2546

Epoch 385: val_loss did not improve from 17.22266
196/196 - 66s - loss: 16.7681 - MinusLogProbMetric: 16.7681 - val_loss: 17.2546 - val_MinusLogProbMetric: 17.2546 - lr: 1.6667e-04 - 66s/epoch - 335ms/step
Epoch 386/1000
2023-09-28 08:44:57.913 
Epoch 386/1000 
	 loss: 16.7821, MinusLogProbMetric: 16.7821, val_loss: 17.2811, val_MinusLogProbMetric: 17.2811

Epoch 386: val_loss did not improve from 17.22266
196/196 - 65s - loss: 16.7821 - MinusLogProbMetric: 16.7821 - val_loss: 17.2811 - val_MinusLogProbMetric: 17.2811 - lr: 1.6667e-04 - 65s/epoch - 333ms/step
Epoch 387/1000
2023-09-28 08:46:02.638 
Epoch 387/1000 
	 loss: 16.7862, MinusLogProbMetric: 16.7862, val_loss: 17.4258, val_MinusLogProbMetric: 17.4258

Epoch 387: val_loss did not improve from 17.22266
196/196 - 65s - loss: 16.7862 - MinusLogProbMetric: 16.7862 - val_loss: 17.4258 - val_MinusLogProbMetric: 17.4258 - lr: 1.6667e-04 - 65s/epoch - 330ms/step
Epoch 388/1000
2023-09-28 08:47:06.883 
Epoch 388/1000 
	 loss: 16.8220, MinusLogProbMetric: 16.8220, val_loss: 17.3602, val_MinusLogProbMetric: 17.3602

Epoch 388: val_loss did not improve from 17.22266
196/196 - 64s - loss: 16.8220 - MinusLogProbMetric: 16.8220 - val_loss: 17.3602 - val_MinusLogProbMetric: 17.3602 - lr: 1.6667e-04 - 64s/epoch - 328ms/step
Epoch 389/1000
2023-09-28 08:48:10.792 
Epoch 389/1000 
	 loss: 16.8557, MinusLogProbMetric: 16.8557, val_loss: 17.4027, val_MinusLogProbMetric: 17.4027

Epoch 389: val_loss did not improve from 17.22266
196/196 - 64s - loss: 16.8557 - MinusLogProbMetric: 16.8557 - val_loss: 17.4027 - val_MinusLogProbMetric: 17.4027 - lr: 1.6667e-04 - 64s/epoch - 326ms/step
Epoch 390/1000
2023-09-28 08:49:08.286 
Epoch 390/1000 
	 loss: 16.8037, MinusLogProbMetric: 16.8037, val_loss: 17.3245, val_MinusLogProbMetric: 17.3245

Epoch 390: val_loss did not improve from 17.22266
196/196 - 57s - loss: 16.8037 - MinusLogProbMetric: 16.8037 - val_loss: 17.3245 - val_MinusLogProbMetric: 17.3245 - lr: 1.6667e-04 - 57s/epoch - 293ms/step
Epoch 391/1000
2023-09-28 08:50:12.022 
Epoch 391/1000 
	 loss: 16.8082, MinusLogProbMetric: 16.8082, val_loss: 17.1990, val_MinusLogProbMetric: 17.1990

Epoch 391: val_loss improved from 17.22266 to 17.19904, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 65s - loss: 16.8082 - MinusLogProbMetric: 16.8082 - val_loss: 17.1990 - val_MinusLogProbMetric: 17.1990 - lr: 1.6667e-04 - 65s/epoch - 331ms/step
Epoch 392/1000
2023-09-28 08:51:20.829 
Epoch 392/1000 
	 loss: 16.8057, MinusLogProbMetric: 16.8057, val_loss: 17.5947, val_MinusLogProbMetric: 17.5947

Epoch 392: val_loss did not improve from 17.19904
196/196 - 68s - loss: 16.8057 - MinusLogProbMetric: 16.8057 - val_loss: 17.5947 - val_MinusLogProbMetric: 17.5947 - lr: 1.6667e-04 - 68s/epoch - 346ms/step
Epoch 393/1000
2023-09-28 08:52:28.289 
Epoch 393/1000 
	 loss: 16.8112, MinusLogProbMetric: 16.8112, val_loss: 17.2505, val_MinusLogProbMetric: 17.2505

Epoch 393: val_loss did not improve from 17.19904
196/196 - 67s - loss: 16.8112 - MinusLogProbMetric: 16.8112 - val_loss: 17.2505 - val_MinusLogProbMetric: 17.2505 - lr: 1.6667e-04 - 67s/epoch - 344ms/step
Epoch 394/1000
2023-09-28 08:53:34.995 
Epoch 394/1000 
	 loss: 16.7753, MinusLogProbMetric: 16.7753, val_loss: 17.1891, val_MinusLogProbMetric: 17.1891

Epoch 394: val_loss improved from 17.19904 to 17.18910, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 68s - loss: 16.7753 - MinusLogProbMetric: 16.7753 - val_loss: 17.1891 - val_MinusLogProbMetric: 17.1891 - lr: 1.6667e-04 - 68s/epoch - 346ms/step
Epoch 395/1000
2023-09-28 08:54:42.955 
Epoch 395/1000 
	 loss: 16.7720, MinusLogProbMetric: 16.7720, val_loss: 17.7054, val_MinusLogProbMetric: 17.7054

Epoch 395: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7720 - MinusLogProbMetric: 16.7720 - val_loss: 17.7054 - val_MinusLogProbMetric: 17.7054 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 396/1000
2023-09-28 08:55:49.549 
Epoch 396/1000 
	 loss: 16.7982, MinusLogProbMetric: 16.7982, val_loss: 17.6121, val_MinusLogProbMetric: 17.6121

Epoch 396: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7982 - MinusLogProbMetric: 16.7982 - val_loss: 17.6121 - val_MinusLogProbMetric: 17.6121 - lr: 1.6667e-04 - 67s/epoch - 340ms/step
Epoch 397/1000
2023-09-28 08:56:56.515 
Epoch 397/1000 
	 loss: 16.8072, MinusLogProbMetric: 16.8072, val_loss: 17.3139, val_MinusLogProbMetric: 17.3139

Epoch 397: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.8072 - MinusLogProbMetric: 16.8072 - val_loss: 17.3139 - val_MinusLogProbMetric: 17.3139 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 398/1000
2023-09-28 08:58:03.079 
Epoch 398/1000 
	 loss: 16.7655, MinusLogProbMetric: 16.7655, val_loss: 17.2396, val_MinusLogProbMetric: 17.2396

Epoch 398: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7655 - MinusLogProbMetric: 16.7655 - val_loss: 17.2396 - val_MinusLogProbMetric: 17.2396 - lr: 1.6667e-04 - 67s/epoch - 340ms/step
Epoch 399/1000
2023-09-28 08:59:09.944 
Epoch 399/1000 
	 loss: 16.7544, MinusLogProbMetric: 16.7544, val_loss: 17.2429, val_MinusLogProbMetric: 17.2429

Epoch 399: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7544 - MinusLogProbMetric: 16.7544 - val_loss: 17.2429 - val_MinusLogProbMetric: 17.2429 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 400/1000
2023-09-28 09:00:17.454 
Epoch 400/1000 
	 loss: 16.7883, MinusLogProbMetric: 16.7883, val_loss: 17.3406, val_MinusLogProbMetric: 17.3406

Epoch 400: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7883 - MinusLogProbMetric: 16.7883 - val_loss: 17.3406 - val_MinusLogProbMetric: 17.3406 - lr: 1.6667e-04 - 67s/epoch - 344ms/step
Epoch 401/1000
2023-09-28 09:01:24.213 
Epoch 401/1000 
	 loss: 16.8063, MinusLogProbMetric: 16.8063, val_loss: 17.2629, val_MinusLogProbMetric: 17.2629

Epoch 401: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.8063 - MinusLogProbMetric: 16.8063 - val_loss: 17.2629 - val_MinusLogProbMetric: 17.2629 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 402/1000
2023-09-28 09:02:31.445 
Epoch 402/1000 
	 loss: 16.7564, MinusLogProbMetric: 16.7564, val_loss: 17.2381, val_MinusLogProbMetric: 17.2381

Epoch 402: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7564 - MinusLogProbMetric: 16.7564 - val_loss: 17.2381 - val_MinusLogProbMetric: 17.2381 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 403/1000
2023-09-28 09:03:38.625 
Epoch 403/1000 
	 loss: 16.7765, MinusLogProbMetric: 16.7765, val_loss: 17.1990, val_MinusLogProbMetric: 17.1990

Epoch 403: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7765 - MinusLogProbMetric: 16.7765 - val_loss: 17.1990 - val_MinusLogProbMetric: 17.1990 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 404/1000
2023-09-28 09:04:45.606 
Epoch 404/1000 
	 loss: 16.7600, MinusLogProbMetric: 16.7600, val_loss: 17.3166, val_MinusLogProbMetric: 17.3166

Epoch 404: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7600 - MinusLogProbMetric: 16.7600 - val_loss: 17.3166 - val_MinusLogProbMetric: 17.3166 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 405/1000
2023-09-28 09:05:52.792 
Epoch 405/1000 
	 loss: 16.7960, MinusLogProbMetric: 16.7960, val_loss: 17.3659, val_MinusLogProbMetric: 17.3659

Epoch 405: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7960 - MinusLogProbMetric: 16.7960 - val_loss: 17.3659 - val_MinusLogProbMetric: 17.3659 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 406/1000
2023-09-28 09:06:59.534 
Epoch 406/1000 
	 loss: 16.7579, MinusLogProbMetric: 16.7579, val_loss: 17.2091, val_MinusLogProbMetric: 17.2091

Epoch 406: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7579 - MinusLogProbMetric: 16.7579 - val_loss: 17.2091 - val_MinusLogProbMetric: 17.2091 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 407/1000
2023-09-28 09:08:06.691 
Epoch 407/1000 
	 loss: 16.7353, MinusLogProbMetric: 16.7353, val_loss: 17.5024, val_MinusLogProbMetric: 17.5024

Epoch 407: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7353 - MinusLogProbMetric: 16.7353 - val_loss: 17.5024 - val_MinusLogProbMetric: 17.5024 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 408/1000
2023-09-28 09:09:13.960 
Epoch 408/1000 
	 loss: 16.7789, MinusLogProbMetric: 16.7789, val_loss: 17.2171, val_MinusLogProbMetric: 17.2171

Epoch 408: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7789 - MinusLogProbMetric: 16.7789 - val_loss: 17.2171 - val_MinusLogProbMetric: 17.2171 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 409/1000
2023-09-28 09:10:20.998 
Epoch 409/1000 
	 loss: 16.8141, MinusLogProbMetric: 16.8141, val_loss: 17.2964, val_MinusLogProbMetric: 17.2964

Epoch 409: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.8141 - MinusLogProbMetric: 16.8141 - val_loss: 17.2964 - val_MinusLogProbMetric: 17.2964 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 410/1000
2023-09-28 09:11:28.245 
Epoch 410/1000 
	 loss: 16.7548, MinusLogProbMetric: 16.7548, val_loss: 17.3679, val_MinusLogProbMetric: 17.3679

Epoch 410: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7548 - MinusLogProbMetric: 16.7548 - val_loss: 17.3679 - val_MinusLogProbMetric: 17.3679 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 411/1000
2023-09-28 09:12:35.599 
Epoch 411/1000 
	 loss: 16.7953, MinusLogProbMetric: 16.7953, val_loss: 17.3093, val_MinusLogProbMetric: 17.3093

Epoch 411: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7953 - MinusLogProbMetric: 16.7953 - val_loss: 17.3093 - val_MinusLogProbMetric: 17.3093 - lr: 1.6667e-04 - 67s/epoch - 344ms/step
Epoch 412/1000
2023-09-28 09:13:43.008 
Epoch 412/1000 
	 loss: 16.7602, MinusLogProbMetric: 16.7602, val_loss: 17.2184, val_MinusLogProbMetric: 17.2184

Epoch 412: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7602 - MinusLogProbMetric: 16.7602 - val_loss: 17.2184 - val_MinusLogProbMetric: 17.2184 - lr: 1.6667e-04 - 67s/epoch - 344ms/step
Epoch 413/1000
2023-09-28 09:14:49.852 
Epoch 413/1000 
	 loss: 16.7724, MinusLogProbMetric: 16.7724, val_loss: 17.2504, val_MinusLogProbMetric: 17.2504

Epoch 413: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7724 - MinusLogProbMetric: 16.7724 - val_loss: 17.2504 - val_MinusLogProbMetric: 17.2504 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 414/1000
2023-09-28 09:15:56.549 
Epoch 414/1000 
	 loss: 16.7446, MinusLogProbMetric: 16.7446, val_loss: 17.2319, val_MinusLogProbMetric: 17.2319

Epoch 414: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7446 - MinusLogProbMetric: 16.7446 - val_loss: 17.2319 - val_MinusLogProbMetric: 17.2319 - lr: 1.6667e-04 - 67s/epoch - 340ms/step
Epoch 415/1000
2023-09-28 09:17:03.714 
Epoch 415/1000 
	 loss: 16.7429, MinusLogProbMetric: 16.7429, val_loss: 17.6747, val_MinusLogProbMetric: 17.6747

Epoch 415: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7429 - MinusLogProbMetric: 16.7429 - val_loss: 17.6747 - val_MinusLogProbMetric: 17.6747 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 416/1000
2023-09-28 09:18:10.797 
Epoch 416/1000 
	 loss: 16.7812, MinusLogProbMetric: 16.7812, val_loss: 17.2690, val_MinusLogProbMetric: 17.2690

Epoch 416: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7812 - MinusLogProbMetric: 16.7812 - val_loss: 17.2690 - val_MinusLogProbMetric: 17.2690 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 417/1000
2023-09-28 09:19:17.422 
Epoch 417/1000 
	 loss: 16.8067, MinusLogProbMetric: 16.8067, val_loss: 17.2132, val_MinusLogProbMetric: 17.2132

Epoch 417: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.8067 - MinusLogProbMetric: 16.8067 - val_loss: 17.2132 - val_MinusLogProbMetric: 17.2132 - lr: 1.6667e-04 - 67s/epoch - 340ms/step
Epoch 418/1000
2023-09-28 09:20:24.561 
Epoch 418/1000 
	 loss: 16.7398, MinusLogProbMetric: 16.7398, val_loss: 17.3776, val_MinusLogProbMetric: 17.3776

Epoch 418: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7398 - MinusLogProbMetric: 16.7398 - val_loss: 17.3776 - val_MinusLogProbMetric: 17.3776 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 419/1000
2023-09-28 09:21:31.735 
Epoch 419/1000 
	 loss: 16.7533, MinusLogProbMetric: 16.7533, val_loss: 17.2958, val_MinusLogProbMetric: 17.2958

Epoch 419: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7533 - MinusLogProbMetric: 16.7533 - val_loss: 17.2958 - val_MinusLogProbMetric: 17.2958 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 420/1000
2023-09-28 09:22:38.598 
Epoch 420/1000 
	 loss: 16.7354, MinusLogProbMetric: 16.7354, val_loss: 17.7335, val_MinusLogProbMetric: 17.7335

Epoch 420: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7354 - MinusLogProbMetric: 16.7354 - val_loss: 17.7335 - val_MinusLogProbMetric: 17.7335 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 421/1000
2023-09-28 09:23:45.943 
Epoch 421/1000 
	 loss: 16.7676, MinusLogProbMetric: 16.7676, val_loss: 17.4009, val_MinusLogProbMetric: 17.4009

Epoch 421: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7676 - MinusLogProbMetric: 16.7676 - val_loss: 17.4009 - val_MinusLogProbMetric: 17.4009 - lr: 1.6667e-04 - 67s/epoch - 344ms/step
Epoch 422/1000
2023-09-28 09:24:53.672 
Epoch 422/1000 
	 loss: 16.7947, MinusLogProbMetric: 16.7947, val_loss: 17.3478, val_MinusLogProbMetric: 17.3478

Epoch 422: val_loss did not improve from 17.18910
196/196 - 68s - loss: 16.7947 - MinusLogProbMetric: 16.7947 - val_loss: 17.3478 - val_MinusLogProbMetric: 17.3478 - lr: 1.6667e-04 - 68s/epoch - 346ms/step
Epoch 423/1000
2023-09-28 09:26:00.931 
Epoch 423/1000 
	 loss: 16.7460, MinusLogProbMetric: 16.7460, val_loss: 17.2388, val_MinusLogProbMetric: 17.2388

Epoch 423: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7460 - MinusLogProbMetric: 16.7460 - val_loss: 17.2388 - val_MinusLogProbMetric: 17.2388 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 424/1000
2023-09-28 09:27:08.072 
Epoch 424/1000 
	 loss: 16.7624, MinusLogProbMetric: 16.7624, val_loss: 17.2138, val_MinusLogProbMetric: 17.2138

Epoch 424: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7624 - MinusLogProbMetric: 16.7624 - val_loss: 17.2138 - val_MinusLogProbMetric: 17.2138 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 425/1000
2023-09-28 09:28:15.240 
Epoch 425/1000 
	 loss: 16.7587, MinusLogProbMetric: 16.7587, val_loss: 17.3966, val_MinusLogProbMetric: 17.3966

Epoch 425: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7587 - MinusLogProbMetric: 16.7587 - val_loss: 17.3966 - val_MinusLogProbMetric: 17.3966 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 426/1000
2023-09-28 09:29:22.538 
Epoch 426/1000 
	 loss: 16.7374, MinusLogProbMetric: 16.7374, val_loss: 17.4827, val_MinusLogProbMetric: 17.4827

Epoch 426: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7374 - MinusLogProbMetric: 16.7374 - val_loss: 17.4827 - val_MinusLogProbMetric: 17.4827 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 427/1000
2023-09-28 09:30:29.829 
Epoch 427/1000 
	 loss: 16.7412, MinusLogProbMetric: 16.7412, val_loss: 17.4097, val_MinusLogProbMetric: 17.4097

Epoch 427: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7412 - MinusLogProbMetric: 16.7412 - val_loss: 17.4097 - val_MinusLogProbMetric: 17.4097 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 428/1000
2023-09-28 09:31:37.450 
Epoch 428/1000 
	 loss: 16.7698, MinusLogProbMetric: 16.7698, val_loss: 17.2447, val_MinusLogProbMetric: 17.2447

Epoch 428: val_loss did not improve from 17.18910
196/196 - 68s - loss: 16.7698 - MinusLogProbMetric: 16.7698 - val_loss: 17.2447 - val_MinusLogProbMetric: 17.2447 - lr: 1.6667e-04 - 68s/epoch - 345ms/step
Epoch 429/1000
2023-09-28 09:32:44.691 
Epoch 429/1000 
	 loss: 16.7732, MinusLogProbMetric: 16.7732, val_loss: 17.3958, val_MinusLogProbMetric: 17.3958

Epoch 429: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7732 - MinusLogProbMetric: 16.7732 - val_loss: 17.3958 - val_MinusLogProbMetric: 17.3958 - lr: 1.6667e-04 - 67s/epoch - 343ms/step
Epoch 430/1000
2023-09-28 09:33:51.784 
Epoch 430/1000 
	 loss: 16.7276, MinusLogProbMetric: 16.7276, val_loss: 17.2010, val_MinusLogProbMetric: 17.2010

Epoch 430: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7276 - MinusLogProbMetric: 16.7276 - val_loss: 17.2010 - val_MinusLogProbMetric: 17.2010 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 431/1000
2023-09-28 09:34:57.715 
Epoch 431/1000 
	 loss: 16.7572, MinusLogProbMetric: 16.7572, val_loss: 17.1959, val_MinusLogProbMetric: 17.1959

Epoch 431: val_loss did not improve from 17.18910
196/196 - 66s - loss: 16.7572 - MinusLogProbMetric: 16.7572 - val_loss: 17.1959 - val_MinusLogProbMetric: 17.1959 - lr: 1.6667e-04 - 66s/epoch - 336ms/step
Epoch 432/1000
2023-09-28 09:36:04.658 
Epoch 432/1000 
	 loss: 16.7521, MinusLogProbMetric: 16.7521, val_loss: 17.3894, val_MinusLogProbMetric: 17.3894

Epoch 432: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7521 - MinusLogProbMetric: 16.7521 - val_loss: 17.3894 - val_MinusLogProbMetric: 17.3894 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 433/1000
2023-09-28 09:37:11.715 
Epoch 433/1000 
	 loss: 16.7493, MinusLogProbMetric: 16.7493, val_loss: 17.4033, val_MinusLogProbMetric: 17.4033

Epoch 433: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7493 - MinusLogProbMetric: 16.7493 - val_loss: 17.4033 - val_MinusLogProbMetric: 17.4033 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 434/1000
2023-09-28 09:38:18.099 
Epoch 434/1000 
	 loss: 16.8085, MinusLogProbMetric: 16.8085, val_loss: 17.4967, val_MinusLogProbMetric: 17.4967

Epoch 434: val_loss did not improve from 17.18910
196/196 - 66s - loss: 16.8085 - MinusLogProbMetric: 16.8085 - val_loss: 17.4967 - val_MinusLogProbMetric: 17.4967 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 435/1000
2023-09-28 09:39:24.883 
Epoch 435/1000 
	 loss: 16.7620, MinusLogProbMetric: 16.7620, val_loss: 17.3265, val_MinusLogProbMetric: 17.3265

Epoch 435: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7620 - MinusLogProbMetric: 16.7620 - val_loss: 17.3265 - val_MinusLogProbMetric: 17.3265 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 436/1000
2023-09-28 09:40:31.835 
Epoch 436/1000 
	 loss: 16.7571, MinusLogProbMetric: 16.7571, val_loss: 17.3108, val_MinusLogProbMetric: 17.3108

Epoch 436: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7571 - MinusLogProbMetric: 16.7571 - val_loss: 17.3108 - val_MinusLogProbMetric: 17.3108 - lr: 1.6667e-04 - 67s/epoch - 342ms/step
Epoch 437/1000
2023-09-28 09:41:38.672 
Epoch 437/1000 
	 loss: 16.7512, MinusLogProbMetric: 16.7512, val_loss: 17.3943, val_MinusLogProbMetric: 17.3943

Epoch 437: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7512 - MinusLogProbMetric: 16.7512 - val_loss: 17.3943 - val_MinusLogProbMetric: 17.3943 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 438/1000
2023-09-28 09:42:45.480 
Epoch 438/1000 
	 loss: 16.7585, MinusLogProbMetric: 16.7585, val_loss: 17.5037, val_MinusLogProbMetric: 17.5037

Epoch 438: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7585 - MinusLogProbMetric: 16.7585 - val_loss: 17.5037 - val_MinusLogProbMetric: 17.5037 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 439/1000
2023-09-28 09:43:52.265 
Epoch 439/1000 
	 loss: 16.7616, MinusLogProbMetric: 16.7616, val_loss: 17.7363, val_MinusLogProbMetric: 17.7363

Epoch 439: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7616 - MinusLogProbMetric: 16.7616 - val_loss: 17.7363 - val_MinusLogProbMetric: 17.7363 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 440/1000
2023-09-28 09:44:58.790 
Epoch 440/1000 
	 loss: 16.8278, MinusLogProbMetric: 16.8278, val_loss: 17.3604, val_MinusLogProbMetric: 17.3604

Epoch 440: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.8278 - MinusLogProbMetric: 16.8278 - val_loss: 17.3604 - val_MinusLogProbMetric: 17.3604 - lr: 1.6667e-04 - 67s/epoch - 339ms/step
Epoch 441/1000
2023-09-28 09:46:05.724 
Epoch 441/1000 
	 loss: 16.7314, MinusLogProbMetric: 16.7314, val_loss: 17.2269, val_MinusLogProbMetric: 17.2269

Epoch 441: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7314 - MinusLogProbMetric: 16.7314 - val_loss: 17.2269 - val_MinusLogProbMetric: 17.2269 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 442/1000
2023-09-28 09:47:12.111 
Epoch 442/1000 
	 loss: 16.7826, MinusLogProbMetric: 16.7826, val_loss: 17.8847, val_MinusLogProbMetric: 17.8847

Epoch 442: val_loss did not improve from 17.18910
196/196 - 66s - loss: 16.7826 - MinusLogProbMetric: 16.7826 - val_loss: 17.8847 - val_MinusLogProbMetric: 17.8847 - lr: 1.6667e-04 - 66s/epoch - 339ms/step
Epoch 443/1000
2023-09-28 09:48:19.022 
Epoch 443/1000 
	 loss: 16.7254, MinusLogProbMetric: 16.7254, val_loss: 17.3692, val_MinusLogProbMetric: 17.3692

Epoch 443: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7254 - MinusLogProbMetric: 16.7254 - val_loss: 17.3692 - val_MinusLogProbMetric: 17.3692 - lr: 1.6667e-04 - 67s/epoch - 341ms/step
Epoch 444/1000
2023-09-28 09:49:25.693 
Epoch 444/1000 
	 loss: 16.7742, MinusLogProbMetric: 16.7742, val_loss: 17.3407, val_MinusLogProbMetric: 17.3407

Epoch 444: val_loss did not improve from 17.18910
196/196 - 67s - loss: 16.7742 - MinusLogProbMetric: 16.7742 - val_loss: 17.3407 - val_MinusLogProbMetric: 17.3407 - lr: 1.6667e-04 - 67s/epoch - 340ms/step
Epoch 445/1000
2023-09-28 09:50:31.638 
Epoch 445/1000 
	 loss: 16.5542, MinusLogProbMetric: 16.5542, val_loss: 17.1204, val_MinusLogProbMetric: 17.1204

Epoch 445: val_loss improved from 17.18910 to 17.12042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 16.5542 - MinusLogProbMetric: 16.5542 - val_loss: 17.1204 - val_MinusLogProbMetric: 17.1204 - lr: 8.3333e-05 - 67s/epoch - 341ms/step
Epoch 446/1000
2023-09-28 09:51:37.731 
Epoch 446/1000 
	 loss: 16.5519, MinusLogProbMetric: 16.5519, val_loss: 17.1257, val_MinusLogProbMetric: 17.1257

Epoch 446: val_loss did not improve from 17.12042
196/196 - 65s - loss: 16.5519 - MinusLogProbMetric: 16.5519 - val_loss: 17.1257 - val_MinusLogProbMetric: 17.1257 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 447/1000
2023-09-28 09:52:45.959 
Epoch 447/1000 
	 loss: 16.5447, MinusLogProbMetric: 16.5447, val_loss: 17.1191, val_MinusLogProbMetric: 17.1191

Epoch 447: val_loss improved from 17.12042 to 17.11907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 69s - loss: 16.5447 - MinusLogProbMetric: 16.5447 - val_loss: 17.1191 - val_MinusLogProbMetric: 17.1191 - lr: 8.3333e-05 - 69s/epoch - 354ms/step
Epoch 448/1000
2023-09-28 09:53:54.974 
Epoch 448/1000 
	 loss: 16.5569, MinusLogProbMetric: 16.5569, val_loss: 17.1095, val_MinusLogProbMetric: 17.1095

Epoch 448: val_loss improved from 17.11907 to 17.10954, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 69s - loss: 16.5569 - MinusLogProbMetric: 16.5569 - val_loss: 17.1095 - val_MinusLogProbMetric: 17.1095 - lr: 8.3333e-05 - 69s/epoch - 352ms/step
Epoch 449/1000
2023-09-28 09:55:04.681 
Epoch 449/1000 
	 loss: 16.5465, MinusLogProbMetric: 16.5465, val_loss: 17.2024, val_MinusLogProbMetric: 17.2024

Epoch 449: val_loss did not improve from 17.10954
196/196 - 69s - loss: 16.5465 - MinusLogProbMetric: 16.5465 - val_loss: 17.2024 - val_MinusLogProbMetric: 17.2024 - lr: 8.3333e-05 - 69s/epoch - 350ms/step
Epoch 450/1000
2023-09-28 09:56:12.582 
Epoch 450/1000 
	 loss: 16.5447, MinusLogProbMetric: 16.5447, val_loss: 17.1993, val_MinusLogProbMetric: 17.1993

Epoch 450: val_loss did not improve from 17.10954
196/196 - 68s - loss: 16.5447 - MinusLogProbMetric: 16.5447 - val_loss: 17.1993 - val_MinusLogProbMetric: 17.1993 - lr: 8.3333e-05 - 68s/epoch - 346ms/step
Epoch 451/1000
2023-09-28 09:57:20.698 
Epoch 451/1000 
	 loss: 16.5408, MinusLogProbMetric: 16.5408, val_loss: 17.1592, val_MinusLogProbMetric: 17.1592

Epoch 451: val_loss did not improve from 17.10954
196/196 - 68s - loss: 16.5408 - MinusLogProbMetric: 16.5408 - val_loss: 17.1592 - val_MinusLogProbMetric: 17.1592 - lr: 8.3333e-05 - 68s/epoch - 348ms/step
Epoch 452/1000
2023-09-28 09:58:28.761 
Epoch 452/1000 
	 loss: 16.5544, MinusLogProbMetric: 16.5544, val_loss: 17.1184, val_MinusLogProbMetric: 17.1184

Epoch 452: val_loss did not improve from 17.10954
196/196 - 68s - loss: 16.5544 - MinusLogProbMetric: 16.5544 - val_loss: 17.1184 - val_MinusLogProbMetric: 17.1184 - lr: 8.3333e-05 - 68s/epoch - 347ms/step
Epoch 453/1000
2023-09-28 09:59:37.145 
Epoch 453/1000 
	 loss: 16.5433, MinusLogProbMetric: 16.5433, val_loss: 17.2033, val_MinusLogProbMetric: 17.2033

Epoch 453: val_loss did not improve from 17.10954
196/196 - 68s - loss: 16.5433 - MinusLogProbMetric: 16.5433 - val_loss: 17.2033 - val_MinusLogProbMetric: 17.2033 - lr: 8.3333e-05 - 68s/epoch - 349ms/step
Epoch 454/1000
2023-09-28 10:00:45.427 
Epoch 454/1000 
	 loss: 16.5491, MinusLogProbMetric: 16.5491, val_loss: 17.1340, val_MinusLogProbMetric: 17.1340

Epoch 454: val_loss did not improve from 17.10954
196/196 - 68s - loss: 16.5491 - MinusLogProbMetric: 16.5491 - val_loss: 17.1340 - val_MinusLogProbMetric: 17.1340 - lr: 8.3333e-05 - 68s/epoch - 348ms/step
Epoch 455/1000
2023-09-28 10:01:53.524 
Epoch 455/1000 
	 loss: 16.5416, MinusLogProbMetric: 16.5416, val_loss: 17.1075, val_MinusLogProbMetric: 17.1075

Epoch 455: val_loss improved from 17.10954 to 17.10746, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 69s - loss: 16.5416 - MinusLogProbMetric: 16.5416 - val_loss: 17.1075 - val_MinusLogProbMetric: 17.1075 - lr: 8.3333e-05 - 69s/epoch - 352ms/step
Epoch 456/1000
2023-09-28 10:03:02.406 
Epoch 456/1000 
	 loss: 16.5101, MinusLogProbMetric: 16.5101, val_loss: 17.1563, val_MinusLogProbMetric: 17.1563

Epoch 456: val_loss did not improve from 17.10746
196/196 - 68s - loss: 16.5101 - MinusLogProbMetric: 16.5101 - val_loss: 17.1563 - val_MinusLogProbMetric: 17.1563 - lr: 8.3333e-05 - 68s/epoch - 346ms/step
Epoch 457/1000
2023-09-28 10:04:10.500 
Epoch 457/1000 
	 loss: 16.5284, MinusLogProbMetric: 16.5284, val_loss: 17.1478, val_MinusLogProbMetric: 17.1478

Epoch 457: val_loss did not improve from 17.10746
196/196 - 68s - loss: 16.5284 - MinusLogProbMetric: 16.5284 - val_loss: 17.1478 - val_MinusLogProbMetric: 17.1478 - lr: 8.3333e-05 - 68s/epoch - 347ms/step
Epoch 458/1000
2023-09-28 10:05:18.665 
Epoch 458/1000 
	 loss: 16.5251, MinusLogProbMetric: 16.5251, val_loss: 17.1262, val_MinusLogProbMetric: 17.1262

Epoch 458: val_loss did not improve from 17.10746
196/196 - 68s - loss: 16.5251 - MinusLogProbMetric: 16.5251 - val_loss: 17.1262 - val_MinusLogProbMetric: 17.1262 - lr: 8.3333e-05 - 68s/epoch - 348ms/step
Epoch 459/1000
2023-09-28 10:06:26.154 
Epoch 459/1000 
	 loss: 16.5334, MinusLogProbMetric: 16.5334, val_loss: 17.1113, val_MinusLogProbMetric: 17.1113

Epoch 459: val_loss did not improve from 17.10746
196/196 - 67s - loss: 16.5334 - MinusLogProbMetric: 16.5334 - val_loss: 17.1113 - val_MinusLogProbMetric: 17.1113 - lr: 8.3333e-05 - 67s/epoch - 344ms/step
Epoch 460/1000
2023-09-28 10:07:33.349 
Epoch 460/1000 
	 loss: 16.5575, MinusLogProbMetric: 16.5575, val_loss: 17.1940, val_MinusLogProbMetric: 17.1940

Epoch 460: val_loss did not improve from 17.10746
196/196 - 67s - loss: 16.5575 - MinusLogProbMetric: 16.5575 - val_loss: 17.1940 - val_MinusLogProbMetric: 17.1940 - lr: 8.3333e-05 - 67s/epoch - 343ms/step
Epoch 461/1000
2023-09-28 10:08:40.927 
Epoch 461/1000 
	 loss: 16.5301, MinusLogProbMetric: 16.5301, val_loss: 17.0899, val_MinusLogProbMetric: 17.0899

Epoch 461: val_loss improved from 17.10746 to 17.08988, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 69s - loss: 16.5301 - MinusLogProbMetric: 16.5301 - val_loss: 17.0899 - val_MinusLogProbMetric: 17.0899 - lr: 8.3333e-05 - 69s/epoch - 350ms/step
Epoch 462/1000
2023-09-28 10:09:49.011 
Epoch 462/1000 
	 loss: 16.5471, MinusLogProbMetric: 16.5471, val_loss: 17.2886, val_MinusLogProbMetric: 17.2886

Epoch 462: val_loss did not improve from 17.08988
196/196 - 67s - loss: 16.5471 - MinusLogProbMetric: 16.5471 - val_loss: 17.2886 - val_MinusLogProbMetric: 17.2886 - lr: 8.3333e-05 - 67s/epoch - 342ms/step
Epoch 463/1000
2023-09-28 10:10:56.081 
Epoch 463/1000 
	 loss: 16.5398, MinusLogProbMetric: 16.5398, val_loss: 17.1417, val_MinusLogProbMetric: 17.1417

Epoch 463: val_loss did not improve from 17.08988
196/196 - 67s - loss: 16.5398 - MinusLogProbMetric: 16.5398 - val_loss: 17.1417 - val_MinusLogProbMetric: 17.1417 - lr: 8.3333e-05 - 67s/epoch - 342ms/step
Epoch 464/1000
2023-09-28 10:12:02.558 
Epoch 464/1000 
	 loss: 16.5396, MinusLogProbMetric: 16.5396, val_loss: 17.1922, val_MinusLogProbMetric: 17.1922

Epoch 464: val_loss did not improve from 17.08988
196/196 - 66s - loss: 16.5396 - MinusLogProbMetric: 16.5396 - val_loss: 17.1922 - val_MinusLogProbMetric: 17.1922 - lr: 8.3333e-05 - 66s/epoch - 339ms/step
Epoch 465/1000
2023-09-28 10:13:09.485 
Epoch 465/1000 
	 loss: 16.5661, MinusLogProbMetric: 16.5661, val_loss: 17.2194, val_MinusLogProbMetric: 17.2194

Epoch 465: val_loss did not improve from 17.08988
196/196 - 67s - loss: 16.5661 - MinusLogProbMetric: 16.5661 - val_loss: 17.2194 - val_MinusLogProbMetric: 17.2194 - lr: 8.3333e-05 - 67s/epoch - 341ms/step
Epoch 466/1000
2023-09-28 10:14:15.239 
Epoch 466/1000 
	 loss: 16.5480, MinusLogProbMetric: 16.5480, val_loss: 17.0812, val_MinusLogProbMetric: 17.0812

Epoch 466: val_loss improved from 17.08988 to 17.08125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 67s - loss: 16.5480 - MinusLogProbMetric: 16.5480 - val_loss: 17.0812 - val_MinusLogProbMetric: 17.0812 - lr: 8.3333e-05 - 67s/epoch - 342ms/step
Epoch 467/1000
2023-09-28 10:15:22.895 
Epoch 467/1000 
	 loss: 16.5134, MinusLogProbMetric: 16.5134, val_loss: 17.1649, val_MinusLogProbMetric: 17.1649

Epoch 467: val_loss did not improve from 17.08125
196/196 - 66s - loss: 16.5134 - MinusLogProbMetric: 16.5134 - val_loss: 17.1649 - val_MinusLogProbMetric: 17.1649 - lr: 8.3333e-05 - 66s/epoch - 339ms/step
Epoch 468/1000
2023-09-28 10:16:30.094 
Epoch 468/1000 
	 loss: 16.5327, MinusLogProbMetric: 16.5327, val_loss: 17.1975, val_MinusLogProbMetric: 17.1975

Epoch 468: val_loss did not improve from 17.08125
196/196 - 67s - loss: 16.5327 - MinusLogProbMetric: 16.5327 - val_loss: 17.1975 - val_MinusLogProbMetric: 17.1975 - lr: 8.3333e-05 - 67s/epoch - 343ms/step
Epoch 469/1000
2023-09-28 10:17:37.612 
Epoch 469/1000 
	 loss: 16.5296, MinusLogProbMetric: 16.5296, val_loss: 17.1150, val_MinusLogProbMetric: 17.1150

Epoch 469: val_loss did not improve from 17.08125
196/196 - 68s - loss: 16.5296 - MinusLogProbMetric: 16.5296 - val_loss: 17.1150 - val_MinusLogProbMetric: 17.1150 - lr: 8.3333e-05 - 68s/epoch - 344ms/step
Epoch 470/1000
2023-09-28 10:18:44.590 
Epoch 470/1000 
	 loss: 16.5543, MinusLogProbMetric: 16.5543, val_loss: 17.2170, val_MinusLogProbMetric: 17.2170

Epoch 470: val_loss did not improve from 17.08125
196/196 - 67s - loss: 16.5543 - MinusLogProbMetric: 16.5543 - val_loss: 17.2170 - val_MinusLogProbMetric: 17.2170 - lr: 8.3333e-05 - 67s/epoch - 342ms/step
Epoch 471/1000
2023-09-28 10:19:50.466 
Epoch 471/1000 
	 loss: 16.5338, MinusLogProbMetric: 16.5338, val_loss: 17.1378, val_MinusLogProbMetric: 17.1378

Epoch 471: val_loss did not improve from 17.08125
196/196 - 66s - loss: 16.5338 - MinusLogProbMetric: 16.5338 - val_loss: 17.1378 - val_MinusLogProbMetric: 17.1378 - lr: 8.3333e-05 - 66s/epoch - 336ms/step
Epoch 472/1000
2023-09-28 10:20:55.966 
Epoch 472/1000 
	 loss: 16.5418, MinusLogProbMetric: 16.5418, val_loss: 17.1402, val_MinusLogProbMetric: 17.1402

Epoch 472: val_loss did not improve from 17.08125
196/196 - 65s - loss: 16.5418 - MinusLogProbMetric: 16.5418 - val_loss: 17.1402 - val_MinusLogProbMetric: 17.1402 - lr: 8.3333e-05 - 65s/epoch - 334ms/step
Epoch 473/1000
2023-09-28 10:22:01.996 
Epoch 473/1000 
	 loss: 16.5403, MinusLogProbMetric: 16.5403, val_loss: 17.1194, val_MinusLogProbMetric: 17.1194

Epoch 473: val_loss did not improve from 17.08125
196/196 - 66s - loss: 16.5403 - MinusLogProbMetric: 16.5403 - val_loss: 17.1194 - val_MinusLogProbMetric: 17.1194 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 474/1000
2023-09-28 10:23:07.531 
Epoch 474/1000 
	 loss: 16.5212, MinusLogProbMetric: 16.5212, val_loss: 17.1753, val_MinusLogProbMetric: 17.1753

Epoch 474: val_loss did not improve from 17.08125
196/196 - 66s - loss: 16.5212 - MinusLogProbMetric: 16.5212 - val_loss: 17.1753 - val_MinusLogProbMetric: 17.1753 - lr: 8.3333e-05 - 66s/epoch - 334ms/step
Epoch 475/1000
2023-09-28 10:24:13.612 
Epoch 475/1000 
	 loss: 16.5469, MinusLogProbMetric: 16.5469, val_loss: 17.1327, val_MinusLogProbMetric: 17.1327

Epoch 475: val_loss did not improve from 17.08125
196/196 - 66s - loss: 16.5469 - MinusLogProbMetric: 16.5469 - val_loss: 17.1327 - val_MinusLogProbMetric: 17.1327 - lr: 8.3333e-05 - 66s/epoch - 337ms/step
Epoch 476/1000
2023-09-28 10:25:18.627 
Epoch 476/1000 
	 loss: 16.5603, MinusLogProbMetric: 16.5603, val_loss: 17.1651, val_MinusLogProbMetric: 17.1651

Epoch 476: val_loss did not improve from 17.08125
196/196 - 65s - loss: 16.5603 - MinusLogProbMetric: 16.5603 - val_loss: 17.1651 - val_MinusLogProbMetric: 17.1651 - lr: 8.3333e-05 - 65s/epoch - 332ms/step
Epoch 477/1000
2023-09-28 10:26:24.331 
Epoch 477/1000 
	 loss: 16.5397, MinusLogProbMetric: 16.5397, val_loss: 17.1094, val_MinusLogProbMetric: 17.1094

Epoch 477: val_loss did not improve from 17.08125
196/196 - 66s - loss: 16.5397 - MinusLogProbMetric: 16.5397 - val_loss: 17.1094 - val_MinusLogProbMetric: 17.1094 - lr: 8.3333e-05 - 66s/epoch - 335ms/step
Epoch 478/1000
2023-09-28 10:27:31.742 
Epoch 478/1000 
	 loss: 16.5651, MinusLogProbMetric: 16.5651, val_loss: 17.1678, val_MinusLogProbMetric: 17.1678

Epoch 478: val_loss did not improve from 17.08125
196/196 - 67s - loss: 16.5651 - MinusLogProbMetric: 16.5651 - val_loss: 17.1678 - val_MinusLogProbMetric: 17.1678 - lr: 8.3333e-05 - 67s/epoch - 344ms/step
Epoch 479/1000
2023-09-28 10:28:38.737 
Epoch 479/1000 
	 loss: 16.5277, MinusLogProbMetric: 16.5277, val_loss: 17.0832, val_MinusLogProbMetric: 17.0832

Epoch 479: val_loss did not improve from 17.08125
196/196 - 67s - loss: 16.5277 - MinusLogProbMetric: 16.5277 - val_loss: 17.0832 - val_MinusLogProbMetric: 17.0832 - lr: 8.3333e-05 - 67s/epoch - 342ms/step
Epoch 480/1000
2023-09-28 10:29:46.055 
Epoch 480/1000 
	 loss: 16.5198, MinusLogProbMetric: 16.5198, val_loss: 17.1736, val_MinusLogProbMetric: 17.1736

Epoch 480: val_loss did not improve from 17.08125
196/196 - 67s - loss: 16.5198 - MinusLogProbMetric: 16.5198 - val_loss: 17.1736 - val_MinusLogProbMetric: 17.1736 - lr: 8.3333e-05 - 67s/epoch - 343ms/step
Epoch 481/1000
2023-09-28 10:30:53.323 
Epoch 481/1000 
	 loss: 16.5341, MinusLogProbMetric: 16.5341, val_loss: 17.0848, val_MinusLogProbMetric: 17.0848

Epoch 481: val_loss did not improve from 17.08125
196/196 - 67s - loss: 16.5341 - MinusLogProbMetric: 16.5341 - val_loss: 17.0848 - val_MinusLogProbMetric: 17.0848 - lr: 8.3333e-05 - 67s/epoch - 343ms/step
Epoch 482/1000
2023-09-28 10:32:01.244 
Epoch 482/1000 
	 loss: 16.5275, MinusLogProbMetric: 16.5275, val_loss: 17.1466, val_MinusLogProbMetric: 17.1466

Epoch 482: val_loss did not improve from 17.08125
196/196 - 68s - loss: 16.5275 - MinusLogProbMetric: 16.5275 - val_loss: 17.1466 - val_MinusLogProbMetric: 17.1466 - lr: 8.3333e-05 - 68s/epoch - 347ms/step
Epoch 483/1000
2023-09-28 10:33:09.112 
Epoch 483/1000 
	 loss: 16.5401, MinusLogProbMetric: 16.5401, val_loss: 17.2180, val_MinusLogProbMetric: 17.2180

Epoch 483: val_loss did not improve from 17.08125
196/196 - 68s - loss: 16.5401 - MinusLogProbMetric: 16.5401 - val_loss: 17.2180 - val_MinusLogProbMetric: 17.2180 - lr: 8.3333e-05 - 68s/epoch - 346ms/step
Epoch 484/1000
2023-09-28 10:34:17.325 
Epoch 484/1000 
	 loss: 16.5698, MinusLogProbMetric: 16.5698, val_loss: 17.1217, val_MinusLogProbMetric: 17.1217

Epoch 484: val_loss did not improve from 17.08125
196/196 - 68s - loss: 16.5698 - MinusLogProbMetric: 16.5698 - val_loss: 17.1217 - val_MinusLogProbMetric: 17.1217 - lr: 8.3333e-05 - 68s/epoch - 348ms/step
Epoch 485/1000
2023-09-28 10:35:25.200 
Epoch 485/1000 
	 loss: 16.5380, MinusLogProbMetric: 16.5380, val_loss: 17.0995, val_MinusLogProbMetric: 17.0995

Epoch 485: val_loss did not improve from 17.08125
196/196 - 68s - loss: 16.5380 - MinusLogProbMetric: 16.5380 - val_loss: 17.0995 - val_MinusLogProbMetric: 17.0995 - lr: 8.3333e-05 - 68s/epoch - 346ms/step
Epoch 486/1000
2023-09-28 10:36:30.430 
Epoch 486/1000 
	 loss: 16.5261, MinusLogProbMetric: 16.5261, val_loss: 17.1878, val_MinusLogProbMetric: 17.1878

Epoch 486: val_loss did not improve from 17.08125
196/196 - 65s - loss: 16.5261 - MinusLogProbMetric: 16.5261 - val_loss: 17.1878 - val_MinusLogProbMetric: 17.1878 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 487/1000
2023-09-28 10:37:34.548 
Epoch 487/1000 
	 loss: 16.5263, MinusLogProbMetric: 16.5263, val_loss: 17.1342, val_MinusLogProbMetric: 17.1342

Epoch 487: val_loss did not improve from 17.08125
196/196 - 64s - loss: 16.5263 - MinusLogProbMetric: 16.5263 - val_loss: 17.1342 - val_MinusLogProbMetric: 17.1342 - lr: 8.3333e-05 - 64s/epoch - 327ms/step
Epoch 488/1000
2023-09-28 10:38:38.501 
Epoch 488/1000 
	 loss: 16.5350, MinusLogProbMetric: 16.5350, val_loss: 17.1722, val_MinusLogProbMetric: 17.1722

Epoch 488: val_loss did not improve from 17.08125
196/196 - 64s - loss: 16.5350 - MinusLogProbMetric: 16.5350 - val_loss: 17.1722 - val_MinusLogProbMetric: 17.1722 - lr: 8.3333e-05 - 64s/epoch - 326ms/step
Epoch 489/1000
2023-09-28 10:39:43.673 
Epoch 489/1000 
	 loss: 16.5195, MinusLogProbMetric: 16.5195, val_loss: 17.1593, val_MinusLogProbMetric: 17.1593

Epoch 489: val_loss did not improve from 17.08125
196/196 - 65s - loss: 16.5195 - MinusLogProbMetric: 16.5195 - val_loss: 17.1593 - val_MinusLogProbMetric: 17.1593 - lr: 8.3333e-05 - 65s/epoch - 333ms/step
Epoch 490/1000
2023-09-28 10:41:07.779 
Epoch 490/1000 
	 loss: 16.5415, MinusLogProbMetric: 16.5415, val_loss: 17.1366, val_MinusLogProbMetric: 17.1366

Epoch 490: val_loss did not improve from 17.08125
196/196 - 84s - loss: 16.5415 - MinusLogProbMetric: 16.5415 - val_loss: 17.1366 - val_MinusLogProbMetric: 17.1366 - lr: 8.3333e-05 - 84s/epoch - 429ms/step
Epoch 491/1000
2023-09-28 10:42:32.231 
Epoch 491/1000 
	 loss: 16.5221, MinusLogProbMetric: 16.5221, val_loss: 17.2243, val_MinusLogProbMetric: 17.2243

Epoch 491: val_loss did not improve from 17.08125
196/196 - 84s - loss: 16.5221 - MinusLogProbMetric: 16.5221 - val_loss: 17.2243 - val_MinusLogProbMetric: 17.2243 - lr: 8.3333e-05 - 84s/epoch - 431ms/step
Epoch 492/1000
2023-09-28 10:43:57.736 
Epoch 492/1000 
	 loss: 16.5597, MinusLogProbMetric: 16.5597, val_loss: 17.1304, val_MinusLogProbMetric: 17.1304

Epoch 492: val_loss did not improve from 17.08125
196/196 - 85s - loss: 16.5597 - MinusLogProbMetric: 16.5597 - val_loss: 17.1304 - val_MinusLogProbMetric: 17.1304 - lr: 8.3333e-05 - 85s/epoch - 436ms/step
Epoch 493/1000
2023-09-28 10:45:16.558 
Epoch 493/1000 
	 loss: 16.5312, MinusLogProbMetric: 16.5312, val_loss: 17.1959, val_MinusLogProbMetric: 17.1959

Epoch 493: val_loss did not improve from 17.08125
196/196 - 79s - loss: 16.5312 - MinusLogProbMetric: 16.5312 - val_loss: 17.1959 - val_MinusLogProbMetric: 17.1959 - lr: 8.3333e-05 - 79s/epoch - 402ms/step
Epoch 494/1000
2023-09-28 10:46:33.378 
Epoch 494/1000 
	 loss: 16.5212, MinusLogProbMetric: 16.5212, val_loss: 17.1263, val_MinusLogProbMetric: 17.1263

Epoch 494: val_loss did not improve from 17.08125
196/196 - 77s - loss: 16.5212 - MinusLogProbMetric: 16.5212 - val_loss: 17.1263 - val_MinusLogProbMetric: 17.1263 - lr: 8.3333e-05 - 77s/epoch - 392ms/step
Epoch 495/1000
2023-09-28 10:47:51.727 
Epoch 495/1000 
	 loss: 16.5258, MinusLogProbMetric: 16.5258, val_loss: 17.1866, val_MinusLogProbMetric: 17.1866

Epoch 495: val_loss did not improve from 17.08125
196/196 - 78s - loss: 16.5258 - MinusLogProbMetric: 16.5258 - val_loss: 17.1866 - val_MinusLogProbMetric: 17.1866 - lr: 8.3333e-05 - 78s/epoch - 400ms/step
Epoch 496/1000
2023-09-28 10:49:11.660 
Epoch 496/1000 
	 loss: 16.5713, MinusLogProbMetric: 16.5713, val_loss: 17.5412, val_MinusLogProbMetric: 17.5412

Epoch 496: val_loss did not improve from 17.08125
196/196 - 80s - loss: 16.5713 - MinusLogProbMetric: 16.5713 - val_loss: 17.5412 - val_MinusLogProbMetric: 17.5412 - lr: 8.3333e-05 - 80s/epoch - 408ms/step
Epoch 497/1000
2023-09-28 10:50:29.480 
Epoch 497/1000 
	 loss: 16.5273, MinusLogProbMetric: 16.5273, val_loss: 17.1311, val_MinusLogProbMetric: 17.1311

Epoch 497: val_loss did not improve from 17.08125
196/196 - 78s - loss: 16.5273 - MinusLogProbMetric: 16.5273 - val_loss: 17.1311 - val_MinusLogProbMetric: 17.1311 - lr: 8.3333e-05 - 78s/epoch - 397ms/step
Epoch 498/1000
2023-09-28 10:51:49.648 
Epoch 498/1000 
	 loss: 16.5264, MinusLogProbMetric: 16.5264, val_loss: 17.1910, val_MinusLogProbMetric: 17.1910

Epoch 498: val_loss did not improve from 17.08125
196/196 - 80s - loss: 16.5264 - MinusLogProbMetric: 16.5264 - val_loss: 17.1910 - val_MinusLogProbMetric: 17.1910 - lr: 8.3333e-05 - 80s/epoch - 409ms/step
Epoch 499/1000
2023-09-28 10:53:09.472 
Epoch 499/1000 
	 loss: 16.5202, MinusLogProbMetric: 16.5202, val_loss: 17.1635, val_MinusLogProbMetric: 17.1635

Epoch 499: val_loss did not improve from 17.08125
196/196 - 80s - loss: 16.5202 - MinusLogProbMetric: 16.5202 - val_loss: 17.1635 - val_MinusLogProbMetric: 17.1635 - lr: 8.3333e-05 - 80s/epoch - 407ms/step
Epoch 500/1000
2023-09-28 10:54:30.002 
Epoch 500/1000 
	 loss: 16.5299, MinusLogProbMetric: 16.5299, val_loss: 17.2043, val_MinusLogProbMetric: 17.2043

Epoch 500: val_loss did not improve from 17.08125
196/196 - 81s - loss: 16.5299 - MinusLogProbMetric: 16.5299 - val_loss: 17.2043 - val_MinusLogProbMetric: 17.2043 - lr: 8.3333e-05 - 81s/epoch - 411ms/step
Epoch 501/1000
2023-09-28 10:55:46.787 
Epoch 501/1000 
	 loss: 16.5206, MinusLogProbMetric: 16.5206, val_loss: 17.1731, val_MinusLogProbMetric: 17.1731

Epoch 501: val_loss did not improve from 17.08125
196/196 - 77s - loss: 16.5206 - MinusLogProbMetric: 16.5206 - val_loss: 17.1731 - val_MinusLogProbMetric: 17.1731 - lr: 8.3333e-05 - 77s/epoch - 392ms/step
Epoch 502/1000
2023-09-28 10:57:04.903 
Epoch 502/1000 
	 loss: 16.5244, MinusLogProbMetric: 16.5244, val_loss: 17.1206, val_MinusLogProbMetric: 17.1206

Epoch 502: val_loss did not improve from 17.08125
196/196 - 78s - loss: 16.5244 - MinusLogProbMetric: 16.5244 - val_loss: 17.1206 - val_MinusLogProbMetric: 17.1206 - lr: 8.3333e-05 - 78s/epoch - 399ms/step
Epoch 503/1000
2023-09-28 10:58:20.977 
Epoch 503/1000 
	 loss: 16.5299, MinusLogProbMetric: 16.5299, val_loss: 17.1659, val_MinusLogProbMetric: 17.1659

Epoch 503: val_loss did not improve from 17.08125
196/196 - 76s - loss: 16.5299 - MinusLogProbMetric: 16.5299 - val_loss: 17.1659 - val_MinusLogProbMetric: 17.1659 - lr: 8.3333e-05 - 76s/epoch - 388ms/step
Epoch 504/1000
2023-09-28 10:59:38.661 
Epoch 504/1000 
	 loss: 16.5420, MinusLogProbMetric: 16.5420, val_loss: 17.1849, val_MinusLogProbMetric: 17.1849

Epoch 504: val_loss did not improve from 17.08125
196/196 - 78s - loss: 16.5420 - MinusLogProbMetric: 16.5420 - val_loss: 17.1849 - val_MinusLogProbMetric: 17.1849 - lr: 8.3333e-05 - 78s/epoch - 396ms/step
Epoch 505/1000
2023-09-28 11:00:54.576 
Epoch 505/1000 
	 loss: 16.5470, MinusLogProbMetric: 16.5470, val_loss: 17.0820, val_MinusLogProbMetric: 17.0820

Epoch 505: val_loss did not improve from 17.08125
196/196 - 76s - loss: 16.5470 - MinusLogProbMetric: 16.5470 - val_loss: 17.0820 - val_MinusLogProbMetric: 17.0820 - lr: 8.3333e-05 - 76s/epoch - 387ms/step
Epoch 506/1000
2023-09-28 11:02:13.312 
Epoch 506/1000 
	 loss: 16.5249, MinusLogProbMetric: 16.5249, val_loss: 17.2480, val_MinusLogProbMetric: 17.2480

Epoch 506: val_loss did not improve from 17.08125
196/196 - 79s - loss: 16.5249 - MinusLogProbMetric: 16.5249 - val_loss: 17.2480 - val_MinusLogProbMetric: 17.2480 - lr: 8.3333e-05 - 79s/epoch - 402ms/step
Epoch 507/1000
2023-09-28 11:03:28.915 
Epoch 507/1000 
	 loss: 16.5566, MinusLogProbMetric: 16.5566, val_loss: 17.1746, val_MinusLogProbMetric: 17.1746

Epoch 507: val_loss did not improve from 17.08125
196/196 - 76s - loss: 16.5566 - MinusLogProbMetric: 16.5566 - val_loss: 17.1746 - val_MinusLogProbMetric: 17.1746 - lr: 8.3333e-05 - 76s/epoch - 386ms/step
Epoch 508/1000
2023-09-28 11:04:49.511 
Epoch 508/1000 
	 loss: 16.5121, MinusLogProbMetric: 16.5121, val_loss: 17.2275, val_MinusLogProbMetric: 17.2275

Epoch 508: val_loss did not improve from 17.08125
196/196 - 81s - loss: 16.5121 - MinusLogProbMetric: 16.5121 - val_loss: 17.2275 - val_MinusLogProbMetric: 17.2275 - lr: 8.3333e-05 - 81s/epoch - 411ms/step
Epoch 509/1000
2023-09-28 11:06:06.880 
Epoch 509/1000 
	 loss: 16.5334, MinusLogProbMetric: 16.5334, val_loss: 17.2622, val_MinusLogProbMetric: 17.2622

Epoch 509: val_loss did not improve from 17.08125
196/196 - 77s - loss: 16.5334 - MinusLogProbMetric: 16.5334 - val_loss: 17.2622 - val_MinusLogProbMetric: 17.2622 - lr: 8.3333e-05 - 77s/epoch - 395ms/step
Epoch 510/1000
2023-09-28 11:07:30.207 
Epoch 510/1000 
	 loss: 16.5483, MinusLogProbMetric: 16.5483, val_loss: 17.2590, val_MinusLogProbMetric: 17.2590

Epoch 510: val_loss did not improve from 17.08125
196/196 - 83s - loss: 16.5483 - MinusLogProbMetric: 16.5483 - val_loss: 17.2590 - val_MinusLogProbMetric: 17.2590 - lr: 8.3333e-05 - 83s/epoch - 425ms/step
Epoch 511/1000
2023-09-28 11:08:46.197 
Epoch 511/1000 
	 loss: 16.5258, MinusLogProbMetric: 16.5258, val_loss: 17.1914, val_MinusLogProbMetric: 17.1914

Epoch 511: val_loss did not improve from 17.08125
196/196 - 76s - loss: 16.5258 - MinusLogProbMetric: 16.5258 - val_loss: 17.1914 - val_MinusLogProbMetric: 17.1914 - lr: 8.3333e-05 - 76s/epoch - 388ms/step
Epoch 512/1000
2023-09-28 11:10:02.461 
Epoch 512/1000 
	 loss: 16.5195, MinusLogProbMetric: 16.5195, val_loss: 17.3001, val_MinusLogProbMetric: 17.3001

Epoch 512: val_loss did not improve from 17.08125
196/196 - 76s - loss: 16.5195 - MinusLogProbMetric: 16.5195 - val_loss: 17.3001 - val_MinusLogProbMetric: 17.3001 - lr: 8.3333e-05 - 76s/epoch - 389ms/step
Epoch 513/1000
2023-09-28 11:11:17.675 
Epoch 513/1000 
	 loss: 16.5531, MinusLogProbMetric: 16.5531, val_loss: 17.1320, val_MinusLogProbMetric: 17.1320

Epoch 513: val_loss did not improve from 17.08125
196/196 - 75s - loss: 16.5531 - MinusLogProbMetric: 16.5531 - val_loss: 17.1320 - val_MinusLogProbMetric: 17.1320 - lr: 8.3333e-05 - 75s/epoch - 384ms/step
Epoch 514/1000
2023-09-28 11:12:34.361 
Epoch 514/1000 
	 loss: 16.5187, MinusLogProbMetric: 16.5187, val_loss: 17.0954, val_MinusLogProbMetric: 17.0954

Epoch 514: val_loss did not improve from 17.08125
196/196 - 77s - loss: 16.5187 - MinusLogProbMetric: 16.5187 - val_loss: 17.0954 - val_MinusLogProbMetric: 17.0954 - lr: 8.3333e-05 - 77s/epoch - 391ms/step
Epoch 515/1000
2023-09-28 11:13:58.067 
Epoch 515/1000 
	 loss: 16.5253, MinusLogProbMetric: 16.5253, val_loss: 17.2406, val_MinusLogProbMetric: 17.2406

Epoch 515: val_loss did not improve from 17.08125
196/196 - 84s - loss: 16.5253 - MinusLogProbMetric: 16.5253 - val_loss: 17.2406 - val_MinusLogProbMetric: 17.2406 - lr: 8.3333e-05 - 84s/epoch - 427ms/step
Epoch 516/1000
2023-09-28 11:15:24.574 
Epoch 516/1000 
	 loss: 16.5153, MinusLogProbMetric: 16.5153, val_loss: 17.2959, val_MinusLogProbMetric: 17.2959

Epoch 516: val_loss did not improve from 17.08125
196/196 - 86s - loss: 16.5153 - MinusLogProbMetric: 16.5153 - val_loss: 17.2959 - val_MinusLogProbMetric: 17.2959 - lr: 8.3333e-05 - 86s/epoch - 441ms/step
Epoch 517/1000
2023-09-28 11:16:46.982 
Epoch 517/1000 
	 loss: 16.4427, MinusLogProbMetric: 16.4427, val_loss: 17.0713, val_MinusLogProbMetric: 17.0713

Epoch 517: val_loss improved from 17.08125 to 17.07127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 83s - loss: 16.4427 - MinusLogProbMetric: 16.4427 - val_loss: 17.0713 - val_MinusLogProbMetric: 17.0713 - lr: 4.1667e-05 - 83s/epoch - 425ms/step
Epoch 518/1000
2023-09-28 11:18:12.355 
Epoch 518/1000 
	 loss: 16.4380, MinusLogProbMetric: 16.4380, val_loss: 17.0541, val_MinusLogProbMetric: 17.0541

Epoch 518: val_loss improved from 17.07127 to 17.05409, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 85s - loss: 16.4380 - MinusLogProbMetric: 16.4380 - val_loss: 17.0541 - val_MinusLogProbMetric: 17.0541 - lr: 4.1667e-05 - 85s/epoch - 436ms/step
Epoch 519/1000
2023-09-28 11:19:40.023 
Epoch 519/1000 
	 loss: 16.4371, MinusLogProbMetric: 16.4371, val_loss: 17.0748, val_MinusLogProbMetric: 17.0748

Epoch 519: val_loss did not improve from 17.05409
196/196 - 87s - loss: 16.4371 - MinusLogProbMetric: 16.4371 - val_loss: 17.0748 - val_MinusLogProbMetric: 17.0748 - lr: 4.1667e-05 - 87s/epoch - 443ms/step
Epoch 520/1000
2023-09-28 11:21:03.626 
Epoch 520/1000 
	 loss: 16.4376, MinusLogProbMetric: 16.4376, val_loss: 17.0889, val_MinusLogProbMetric: 17.0889

Epoch 520: val_loss did not improve from 17.05409
196/196 - 84s - loss: 16.4376 - MinusLogProbMetric: 16.4376 - val_loss: 17.0889 - val_MinusLogProbMetric: 17.0889 - lr: 4.1667e-05 - 84s/epoch - 426ms/step
Epoch 521/1000
2023-09-28 11:22:26.855 
Epoch 521/1000 
	 loss: 16.4289, MinusLogProbMetric: 16.4289, val_loss: 17.0647, val_MinusLogProbMetric: 17.0647

Epoch 521: val_loss did not improve from 17.05409
196/196 - 83s - loss: 16.4289 - MinusLogProbMetric: 16.4289 - val_loss: 17.0647 - val_MinusLogProbMetric: 17.0647 - lr: 4.1667e-05 - 83s/epoch - 425ms/step
Epoch 522/1000
2023-09-28 11:23:49.958 
Epoch 522/1000 
	 loss: 16.4369, MinusLogProbMetric: 16.4369, val_loss: 17.1028, val_MinusLogProbMetric: 17.1028

Epoch 522: val_loss did not improve from 17.05409
196/196 - 83s - loss: 16.4369 - MinusLogProbMetric: 16.4369 - val_loss: 17.1028 - val_MinusLogProbMetric: 17.1028 - lr: 4.1667e-05 - 83s/epoch - 424ms/step
Epoch 523/1000
2023-09-28 11:25:16.987 
Epoch 523/1000 
	 loss: 16.4306, MinusLogProbMetric: 16.4306, val_loss: 17.0632, val_MinusLogProbMetric: 17.0632

Epoch 523: val_loss did not improve from 17.05409
196/196 - 87s - loss: 16.4306 - MinusLogProbMetric: 16.4306 - val_loss: 17.0632 - val_MinusLogProbMetric: 17.0632 - lr: 4.1667e-05 - 87s/epoch - 444ms/step
Epoch 524/1000
2023-09-28 11:26:42.105 
Epoch 524/1000 
	 loss: 16.4301, MinusLogProbMetric: 16.4301, val_loss: 17.1016, val_MinusLogProbMetric: 17.1016

Epoch 524: val_loss did not improve from 17.05409
196/196 - 85s - loss: 16.4301 - MinusLogProbMetric: 16.4301 - val_loss: 17.1016 - val_MinusLogProbMetric: 17.1016 - lr: 4.1667e-05 - 85s/epoch - 434ms/step
Epoch 525/1000
2023-09-28 11:28:08.580 
Epoch 525/1000 
	 loss: 16.4314, MinusLogProbMetric: 16.4314, val_loss: 17.0834, val_MinusLogProbMetric: 17.0834

Epoch 525: val_loss did not improve from 17.05409
196/196 - 86s - loss: 16.4314 - MinusLogProbMetric: 16.4314 - val_loss: 17.0834 - val_MinusLogProbMetric: 17.0834 - lr: 4.1667e-05 - 86s/epoch - 441ms/step
Epoch 526/1000
2023-09-28 11:29:34.718 
Epoch 526/1000 
	 loss: 16.4277, MinusLogProbMetric: 16.4277, val_loss: 17.0665, val_MinusLogProbMetric: 17.0665

Epoch 526: val_loss did not improve from 17.05409
196/196 - 86s - loss: 16.4277 - MinusLogProbMetric: 16.4277 - val_loss: 17.0665 - val_MinusLogProbMetric: 17.0665 - lr: 4.1667e-05 - 86s/epoch - 439ms/step
Epoch 527/1000
2023-09-28 11:31:01.888 
Epoch 527/1000 
	 loss: 16.4337, MinusLogProbMetric: 16.4337, val_loss: 17.0898, val_MinusLogProbMetric: 17.0898

Epoch 527: val_loss did not improve from 17.05409
196/196 - 87s - loss: 16.4337 - MinusLogProbMetric: 16.4337 - val_loss: 17.0898 - val_MinusLogProbMetric: 17.0898 - lr: 4.1667e-05 - 87s/epoch - 445ms/step
Epoch 528/1000
2023-09-28 11:32:28.072 
Epoch 528/1000 
	 loss: 16.4395, MinusLogProbMetric: 16.4395, val_loss: 17.0696, val_MinusLogProbMetric: 17.0696

Epoch 528: val_loss did not improve from 17.05409
196/196 - 86s - loss: 16.4395 - MinusLogProbMetric: 16.4395 - val_loss: 17.0696 - val_MinusLogProbMetric: 17.0696 - lr: 4.1667e-05 - 86s/epoch - 440ms/step
Epoch 529/1000
2023-09-28 11:33:54.968 
Epoch 529/1000 
	 loss: 16.4277, MinusLogProbMetric: 16.4277, val_loss: 17.0794, val_MinusLogProbMetric: 17.0794

Epoch 529: val_loss did not improve from 17.05409
196/196 - 87s - loss: 16.4277 - MinusLogProbMetric: 16.4277 - val_loss: 17.0794 - val_MinusLogProbMetric: 17.0794 - lr: 4.1667e-05 - 87s/epoch - 443ms/step
Epoch 530/1000
2023-09-28 11:35:20.107 
Epoch 530/1000 
	 loss: 16.4372, MinusLogProbMetric: 16.4372, val_loss: 17.0971, val_MinusLogProbMetric: 17.0971

Epoch 530: val_loss did not improve from 17.05409
196/196 - 85s - loss: 16.4372 - MinusLogProbMetric: 16.4372 - val_loss: 17.0971 - val_MinusLogProbMetric: 17.0971 - lr: 4.1667e-05 - 85s/epoch - 434ms/step
Epoch 531/1000
2023-09-28 11:36:45.067 
Epoch 531/1000 
	 loss: 16.4296, MinusLogProbMetric: 16.4296, val_loss: 17.0900, val_MinusLogProbMetric: 17.0900

Epoch 531: val_loss did not improve from 17.05409
196/196 - 85s - loss: 16.4296 - MinusLogProbMetric: 16.4296 - val_loss: 17.0900 - val_MinusLogProbMetric: 17.0900 - lr: 4.1667e-05 - 85s/epoch - 433ms/step
Epoch 532/1000
2023-09-28 11:38:10.192 
Epoch 532/1000 
	 loss: 16.4456, MinusLogProbMetric: 16.4456, val_loss: 17.1269, val_MinusLogProbMetric: 17.1269

Epoch 532: val_loss did not improve from 17.05409
196/196 - 85s - loss: 16.4456 - MinusLogProbMetric: 16.4456 - val_loss: 17.1269 - val_MinusLogProbMetric: 17.1269 - lr: 4.1667e-05 - 85s/epoch - 434ms/step
Epoch 533/1000
2023-09-28 11:39:34.826 
Epoch 533/1000 
	 loss: 16.4348, MinusLogProbMetric: 16.4348, val_loss: 17.0946, val_MinusLogProbMetric: 17.0946

Epoch 533: val_loss did not improve from 17.05409
196/196 - 85s - loss: 16.4348 - MinusLogProbMetric: 16.4348 - val_loss: 17.0946 - val_MinusLogProbMetric: 17.0946 - lr: 4.1667e-05 - 85s/epoch - 432ms/step
Epoch 534/1000
2023-09-28 11:41:00.392 
Epoch 534/1000 
	 loss: 16.4287, MinusLogProbMetric: 16.4287, val_loss: 17.0628, val_MinusLogProbMetric: 17.0628

Epoch 534: val_loss did not improve from 17.05409
196/196 - 86s - loss: 16.4287 - MinusLogProbMetric: 16.4287 - val_loss: 17.0628 - val_MinusLogProbMetric: 17.0628 - lr: 4.1667e-05 - 86s/epoch - 436ms/step
Epoch 535/1000
2023-09-28 11:42:26.152 
Epoch 535/1000 
	 loss: 16.4339, MinusLogProbMetric: 16.4339, val_loss: 17.1044, val_MinusLogProbMetric: 17.1044

Epoch 535: val_loss did not improve from 17.05409
196/196 - 86s - loss: 16.4339 - MinusLogProbMetric: 16.4339 - val_loss: 17.1044 - val_MinusLogProbMetric: 17.1044 - lr: 4.1667e-05 - 86s/epoch - 438ms/step
Epoch 536/1000
2023-09-28 11:43:53.992 
Epoch 536/1000 
	 loss: 16.4317, MinusLogProbMetric: 16.4317, val_loss: 17.0813, val_MinusLogProbMetric: 17.0813

Epoch 536: val_loss did not improve from 17.05409
196/196 - 88s - loss: 16.4317 - MinusLogProbMetric: 16.4317 - val_loss: 17.0813 - val_MinusLogProbMetric: 17.0813 - lr: 4.1667e-05 - 88s/epoch - 448ms/step
Epoch 537/1000
2023-09-28 11:45:19.264 
Epoch 537/1000 
	 loss: 16.4269, MinusLogProbMetric: 16.4269, val_loss: 17.1056, val_MinusLogProbMetric: 17.1056

Epoch 537: val_loss did not improve from 17.05409
196/196 - 85s - loss: 16.4269 - MinusLogProbMetric: 16.4269 - val_loss: 17.1056 - val_MinusLogProbMetric: 17.1056 - lr: 4.1667e-05 - 85s/epoch - 435ms/step
Epoch 538/1000
2023-09-28 11:46:47.412 
Epoch 538/1000 
	 loss: 16.4315, MinusLogProbMetric: 16.4315, val_loss: 17.0973, val_MinusLogProbMetric: 17.0973

Epoch 538: val_loss did not improve from 17.05409
196/196 - 88s - loss: 16.4315 - MinusLogProbMetric: 16.4315 - val_loss: 17.0973 - val_MinusLogProbMetric: 17.0973 - lr: 4.1667e-05 - 88s/epoch - 450ms/step
Epoch 539/1000
2023-09-28 11:48:07.600 
Epoch 539/1000 
	 loss: 16.4315, MinusLogProbMetric: 16.4315, val_loss: 17.0508, val_MinusLogProbMetric: 17.0508

Epoch 539: val_loss improved from 17.05409 to 17.05083, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 81s - loss: 16.4315 - MinusLogProbMetric: 16.4315 - val_loss: 17.0508 - val_MinusLogProbMetric: 17.0508 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 540/1000
2023-09-28 11:49:27.545 
Epoch 540/1000 
	 loss: 16.4297, MinusLogProbMetric: 16.4297, val_loss: 17.0838, val_MinusLogProbMetric: 17.0838

Epoch 540: val_loss did not improve from 17.05083
196/196 - 79s - loss: 16.4297 - MinusLogProbMetric: 16.4297 - val_loss: 17.0838 - val_MinusLogProbMetric: 17.0838 - lr: 4.1667e-05 - 79s/epoch - 403ms/step
Epoch 541/1000
2023-09-28 11:50:50.579 
Epoch 541/1000 
	 loss: 16.4332, MinusLogProbMetric: 16.4332, val_loss: 17.0663, val_MinusLogProbMetric: 17.0663

Epoch 541: val_loss did not improve from 17.05083
196/196 - 83s - loss: 16.4332 - MinusLogProbMetric: 16.4332 - val_loss: 17.0663 - val_MinusLogProbMetric: 17.0663 - lr: 4.1667e-05 - 83s/epoch - 424ms/step
Epoch 542/1000
2023-09-28 11:52:08.983 
Epoch 542/1000 
	 loss: 16.4255, MinusLogProbMetric: 16.4255, val_loss: 17.0562, val_MinusLogProbMetric: 17.0562

Epoch 542: val_loss did not improve from 17.05083
196/196 - 78s - loss: 16.4255 - MinusLogProbMetric: 16.4255 - val_loss: 17.0562 - val_MinusLogProbMetric: 17.0562 - lr: 4.1667e-05 - 78s/epoch - 400ms/step
Epoch 543/1000
2023-09-28 11:53:28.058 
Epoch 543/1000 
	 loss: 16.4311, MinusLogProbMetric: 16.4311, val_loss: 17.0726, val_MinusLogProbMetric: 17.0726

Epoch 543: val_loss did not improve from 17.05083
196/196 - 79s - loss: 16.4311 - MinusLogProbMetric: 16.4311 - val_loss: 17.0726 - val_MinusLogProbMetric: 17.0726 - lr: 4.1667e-05 - 79s/epoch - 403ms/step
Epoch 544/1000
2023-09-28 11:54:49.296 
Epoch 544/1000 
	 loss: 16.4319, MinusLogProbMetric: 16.4319, val_loss: 17.0693, val_MinusLogProbMetric: 17.0693

Epoch 544: val_loss did not improve from 17.05083
196/196 - 81s - loss: 16.4319 - MinusLogProbMetric: 16.4319 - val_loss: 17.0693 - val_MinusLogProbMetric: 17.0693 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 545/1000
2023-09-28 11:56:10.385 
Epoch 545/1000 
	 loss: 16.4252, MinusLogProbMetric: 16.4252, val_loss: 17.1112, val_MinusLogProbMetric: 17.1112

Epoch 545: val_loss did not improve from 17.05083
196/196 - 81s - loss: 16.4252 - MinusLogProbMetric: 16.4252 - val_loss: 17.1112 - val_MinusLogProbMetric: 17.1112 - lr: 4.1667e-05 - 81s/epoch - 414ms/step
Epoch 546/1000
2023-09-28 11:57:32.335 
Epoch 546/1000 
	 loss: 16.4283, MinusLogProbMetric: 16.4283, val_loss: 17.0756, val_MinusLogProbMetric: 17.0756

Epoch 546: val_loss did not improve from 17.05083
196/196 - 82s - loss: 16.4283 - MinusLogProbMetric: 16.4283 - val_loss: 17.0756 - val_MinusLogProbMetric: 17.0756 - lr: 4.1667e-05 - 82s/epoch - 418ms/step
Epoch 547/1000
2023-09-28 11:58:58.522 
Epoch 547/1000 
	 loss: 16.4425, MinusLogProbMetric: 16.4425, val_loss: 17.0700, val_MinusLogProbMetric: 17.0700

Epoch 547: val_loss did not improve from 17.05083
196/196 - 86s - loss: 16.4425 - MinusLogProbMetric: 16.4425 - val_loss: 17.0700 - val_MinusLogProbMetric: 17.0700 - lr: 4.1667e-05 - 86s/epoch - 440ms/step
Epoch 548/1000
2023-09-28 12:00:25.155 
Epoch 548/1000 
	 loss: 16.4287, MinusLogProbMetric: 16.4287, val_loss: 17.0937, val_MinusLogProbMetric: 17.0937

Epoch 548: val_loss did not improve from 17.05083
196/196 - 87s - loss: 16.4287 - MinusLogProbMetric: 16.4287 - val_loss: 17.0937 - val_MinusLogProbMetric: 17.0937 - lr: 4.1667e-05 - 87s/epoch - 442ms/step
Epoch 549/1000
2023-09-28 12:01:51.219 
Epoch 549/1000 
	 loss: 16.4258, MinusLogProbMetric: 16.4258, val_loss: 17.1179, val_MinusLogProbMetric: 17.1179

Epoch 549: val_loss did not improve from 17.05083
196/196 - 86s - loss: 16.4258 - MinusLogProbMetric: 16.4258 - val_loss: 17.1179 - val_MinusLogProbMetric: 17.1179 - lr: 4.1667e-05 - 86s/epoch - 439ms/step
Epoch 550/1000
2023-09-28 12:03:15.105 
Epoch 550/1000 
	 loss: 16.4301, MinusLogProbMetric: 16.4301, val_loss: 17.0772, val_MinusLogProbMetric: 17.0772

Epoch 550: val_loss did not improve from 17.05083
196/196 - 84s - loss: 16.4301 - MinusLogProbMetric: 16.4301 - val_loss: 17.0772 - val_MinusLogProbMetric: 17.0772 - lr: 4.1667e-05 - 84s/epoch - 428ms/step
Epoch 551/1000
2023-09-28 12:04:40.399 
Epoch 551/1000 
	 loss: 16.4253, MinusLogProbMetric: 16.4253, val_loss: 17.1058, val_MinusLogProbMetric: 17.1058

Epoch 551: val_loss did not improve from 17.05083
196/196 - 85s - loss: 16.4253 - MinusLogProbMetric: 16.4253 - val_loss: 17.1058 - val_MinusLogProbMetric: 17.1058 - lr: 4.1667e-05 - 85s/epoch - 435ms/step
Epoch 552/1000
2023-09-28 12:06:00.028 
Epoch 552/1000 
	 loss: 16.4315, MinusLogProbMetric: 16.4315, val_loss: 17.1291, val_MinusLogProbMetric: 17.1291

Epoch 552: val_loss did not improve from 17.05083
196/196 - 80s - loss: 16.4315 - MinusLogProbMetric: 16.4315 - val_loss: 17.1291 - val_MinusLogProbMetric: 17.1291 - lr: 4.1667e-05 - 80s/epoch - 406ms/step
Epoch 553/1000
2023-09-28 12:07:15.740 
Epoch 553/1000 
	 loss: 16.4334, MinusLogProbMetric: 16.4334, val_loss: 17.0602, val_MinusLogProbMetric: 17.0602

Epoch 553: val_loss did not improve from 17.05083
196/196 - 76s - loss: 16.4334 - MinusLogProbMetric: 16.4334 - val_loss: 17.0602 - val_MinusLogProbMetric: 17.0602 - lr: 4.1667e-05 - 76s/epoch - 386ms/step
Epoch 554/1000
2023-09-28 12:08:28.324 
Epoch 554/1000 
	 loss: 16.4247, MinusLogProbMetric: 16.4247, val_loss: 17.0602, val_MinusLogProbMetric: 17.0602

Epoch 554: val_loss did not improve from 17.05083
196/196 - 73s - loss: 16.4247 - MinusLogProbMetric: 16.4247 - val_loss: 17.0602 - val_MinusLogProbMetric: 17.0602 - lr: 4.1667e-05 - 73s/epoch - 370ms/step
Epoch 555/1000
2023-09-28 12:09:43.213 
Epoch 555/1000 
	 loss: 16.4196, MinusLogProbMetric: 16.4196, val_loss: 17.0506, val_MinusLogProbMetric: 17.0506

Epoch 555: val_loss improved from 17.05083 to 17.05060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 76s - loss: 16.4196 - MinusLogProbMetric: 16.4196 - val_loss: 17.0506 - val_MinusLogProbMetric: 17.0506 - lr: 4.1667e-05 - 76s/epoch - 386ms/step
Epoch 556/1000
2023-09-28 12:10:59.516 
Epoch 556/1000 
	 loss: 16.4338, MinusLogProbMetric: 16.4338, val_loss: 17.0746, val_MinusLogProbMetric: 17.0746

Epoch 556: val_loss did not improve from 17.05060
196/196 - 75s - loss: 16.4338 - MinusLogProbMetric: 16.4338 - val_loss: 17.0746 - val_MinusLogProbMetric: 17.0746 - lr: 4.1667e-05 - 75s/epoch - 385ms/step
Epoch 557/1000
2023-09-28 12:12:13.119 
Epoch 557/1000 
	 loss: 16.4298, MinusLogProbMetric: 16.4298, val_loss: 17.1117, val_MinusLogProbMetric: 17.1117

Epoch 557: val_loss did not improve from 17.05060
196/196 - 74s - loss: 16.4298 - MinusLogProbMetric: 16.4298 - val_loss: 17.1117 - val_MinusLogProbMetric: 17.1117 - lr: 4.1667e-05 - 74s/epoch - 375ms/step
Epoch 558/1000
2023-09-28 12:13:28.978 
Epoch 558/1000 
	 loss: 16.4273, MinusLogProbMetric: 16.4273, val_loss: 17.1368, val_MinusLogProbMetric: 17.1368

Epoch 558: val_loss did not improve from 17.05060
196/196 - 76s - loss: 16.4273 - MinusLogProbMetric: 16.4273 - val_loss: 17.1368 - val_MinusLogProbMetric: 17.1368 - lr: 4.1667e-05 - 76s/epoch - 387ms/step
Epoch 559/1000
2023-09-28 12:14:46.088 
Epoch 559/1000 
	 loss: 16.4258, MinusLogProbMetric: 16.4258, val_loss: 17.0800, val_MinusLogProbMetric: 17.0800

Epoch 559: val_loss did not improve from 17.05060
196/196 - 77s - loss: 16.4258 - MinusLogProbMetric: 16.4258 - val_loss: 17.0800 - val_MinusLogProbMetric: 17.0800 - lr: 4.1667e-05 - 77s/epoch - 393ms/step
Epoch 560/1000
2023-09-28 12:16:02.156 
Epoch 560/1000 
	 loss: 16.4222, MinusLogProbMetric: 16.4222, val_loss: 17.1423, val_MinusLogProbMetric: 17.1423

Epoch 560: val_loss did not improve from 17.05060
196/196 - 76s - loss: 16.4222 - MinusLogProbMetric: 16.4222 - val_loss: 17.1423 - val_MinusLogProbMetric: 17.1423 - lr: 4.1667e-05 - 76s/epoch - 388ms/step
Epoch 561/1000
2023-09-28 12:17:16.013 
Epoch 561/1000 
	 loss: 16.4376, MinusLogProbMetric: 16.4376, val_loss: 17.0986, val_MinusLogProbMetric: 17.0986

Epoch 561: val_loss did not improve from 17.05060
196/196 - 74s - loss: 16.4376 - MinusLogProbMetric: 16.4376 - val_loss: 17.0986 - val_MinusLogProbMetric: 17.0986 - lr: 4.1667e-05 - 74s/epoch - 377ms/step
Epoch 562/1000
2023-09-28 12:18:33.442 
Epoch 562/1000 
	 loss: 16.4127, MinusLogProbMetric: 16.4127, val_loss: 17.0990, val_MinusLogProbMetric: 17.0990

Epoch 562: val_loss did not improve from 17.05060
196/196 - 77s - loss: 16.4127 - MinusLogProbMetric: 16.4127 - val_loss: 17.0990 - val_MinusLogProbMetric: 17.0990 - lr: 4.1667e-05 - 77s/epoch - 395ms/step
Epoch 563/1000
2023-09-28 12:19:50.250 
Epoch 563/1000 
	 loss: 16.4391, MinusLogProbMetric: 16.4391, val_loss: 17.1106, val_MinusLogProbMetric: 17.1106

Epoch 563: val_loss did not improve from 17.05060
196/196 - 77s - loss: 16.4391 - MinusLogProbMetric: 16.4391 - val_loss: 17.1106 - val_MinusLogProbMetric: 17.1106 - lr: 4.1667e-05 - 77s/epoch - 392ms/step
Epoch 564/1000
2023-09-28 12:21:06.119 
Epoch 564/1000 
	 loss: 16.4360, MinusLogProbMetric: 16.4360, val_loss: 17.0895, val_MinusLogProbMetric: 17.0895

Epoch 564: val_loss did not improve from 17.05060
196/196 - 76s - loss: 16.4360 - MinusLogProbMetric: 16.4360 - val_loss: 17.0895 - val_MinusLogProbMetric: 17.0895 - lr: 4.1667e-05 - 76s/epoch - 387ms/step
Epoch 565/1000
2023-09-28 12:22:26.711 
Epoch 565/1000 
	 loss: 16.4184, MinusLogProbMetric: 16.4184, val_loss: 17.0758, val_MinusLogProbMetric: 17.0758

Epoch 565: val_loss did not improve from 17.05060
196/196 - 81s - loss: 16.4184 - MinusLogProbMetric: 16.4184 - val_loss: 17.0758 - val_MinusLogProbMetric: 17.0758 - lr: 4.1667e-05 - 81s/epoch - 411ms/step
Epoch 566/1000
2023-09-28 12:23:51.231 
Epoch 566/1000 
	 loss: 16.4231, MinusLogProbMetric: 16.4231, val_loss: 17.0939, val_MinusLogProbMetric: 17.0939

Epoch 566: val_loss did not improve from 17.05060
196/196 - 85s - loss: 16.4231 - MinusLogProbMetric: 16.4231 - val_loss: 17.0939 - val_MinusLogProbMetric: 17.0939 - lr: 4.1667e-05 - 85s/epoch - 431ms/step
Epoch 567/1000
2023-09-28 12:25:13.835 
Epoch 567/1000 
	 loss: 16.4261, MinusLogProbMetric: 16.4261, val_loss: 17.0666, val_MinusLogProbMetric: 17.0666

Epoch 567: val_loss did not improve from 17.05060
196/196 - 83s - loss: 16.4261 - MinusLogProbMetric: 16.4261 - val_loss: 17.0666 - val_MinusLogProbMetric: 17.0666 - lr: 4.1667e-05 - 83s/epoch - 421ms/step
Epoch 568/1000
2023-09-28 12:26:34.845 
Epoch 568/1000 
	 loss: 16.4204, MinusLogProbMetric: 16.4204, val_loss: 17.0758, val_MinusLogProbMetric: 17.0758

Epoch 568: val_loss did not improve from 17.05060
196/196 - 81s - loss: 16.4204 - MinusLogProbMetric: 16.4204 - val_loss: 17.0758 - val_MinusLogProbMetric: 17.0758 - lr: 4.1667e-05 - 81s/epoch - 413ms/step
Epoch 569/1000
2023-09-28 12:27:58.359 
Epoch 569/1000 
	 loss: 16.4156, MinusLogProbMetric: 16.4156, val_loss: 17.0923, val_MinusLogProbMetric: 17.0923

Epoch 569: val_loss did not improve from 17.05060
196/196 - 84s - loss: 16.4156 - MinusLogProbMetric: 16.4156 - val_loss: 17.0923 - val_MinusLogProbMetric: 17.0923 - lr: 4.1667e-05 - 84s/epoch - 426ms/step
Epoch 570/1000
2023-09-28 12:29:20.588 
Epoch 570/1000 
	 loss: 16.4219, MinusLogProbMetric: 16.4219, val_loss: 17.0909, val_MinusLogProbMetric: 17.0909

Epoch 570: val_loss did not improve from 17.05060
196/196 - 82s - loss: 16.4219 - MinusLogProbMetric: 16.4219 - val_loss: 17.0909 - val_MinusLogProbMetric: 17.0909 - lr: 4.1667e-05 - 82s/epoch - 419ms/step
Epoch 571/1000
2023-09-28 12:30:45.844 
Epoch 571/1000 
	 loss: 16.4277, MinusLogProbMetric: 16.4277, val_loss: 17.1233, val_MinusLogProbMetric: 17.1233

Epoch 571: val_loss did not improve from 17.05060
196/196 - 85s - loss: 16.4277 - MinusLogProbMetric: 16.4277 - val_loss: 17.1233 - val_MinusLogProbMetric: 17.1233 - lr: 4.1667e-05 - 85s/epoch - 435ms/step
Epoch 572/1000
2023-09-28 12:32:08.844 
Epoch 572/1000 
	 loss: 16.4237, MinusLogProbMetric: 16.4237, val_loss: 17.0615, val_MinusLogProbMetric: 17.0615

Epoch 572: val_loss did not improve from 17.05060
196/196 - 83s - loss: 16.4237 - MinusLogProbMetric: 16.4237 - val_loss: 17.0615 - val_MinusLogProbMetric: 17.0615 - lr: 4.1667e-05 - 83s/epoch - 423ms/step
Epoch 573/1000
2023-09-28 12:33:35.140 
Epoch 573/1000 
	 loss: 16.4391, MinusLogProbMetric: 16.4391, val_loss: 17.0595, val_MinusLogProbMetric: 17.0595

Epoch 573: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4391 - MinusLogProbMetric: 16.4391 - val_loss: 17.0595 - val_MinusLogProbMetric: 17.0595 - lr: 4.1667e-05 - 86s/epoch - 440ms/step
Epoch 574/1000
2023-09-28 12:35:01.971 
Epoch 574/1000 
	 loss: 16.4282, MinusLogProbMetric: 16.4282, val_loss: 17.0880, val_MinusLogProbMetric: 17.0880

Epoch 574: val_loss did not improve from 17.05060
196/196 - 87s - loss: 16.4282 - MinusLogProbMetric: 16.4282 - val_loss: 17.0880 - val_MinusLogProbMetric: 17.0880 - lr: 4.1667e-05 - 87s/epoch - 443ms/step
Epoch 575/1000
2023-09-28 12:36:28.324 
Epoch 575/1000 
	 loss: 16.4239, MinusLogProbMetric: 16.4239, val_loss: 17.0879, val_MinusLogProbMetric: 17.0879

Epoch 575: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4239 - MinusLogProbMetric: 16.4239 - val_loss: 17.0879 - val_MinusLogProbMetric: 17.0879 - lr: 4.1667e-05 - 86s/epoch - 441ms/step
Epoch 576/1000
2023-09-28 12:37:53.513 
Epoch 576/1000 
	 loss: 16.4198, MinusLogProbMetric: 16.4198, val_loss: 17.1156, val_MinusLogProbMetric: 17.1156

Epoch 576: val_loss did not improve from 17.05060
196/196 - 85s - loss: 16.4198 - MinusLogProbMetric: 16.4198 - val_loss: 17.1156 - val_MinusLogProbMetric: 17.1156 - lr: 4.1667e-05 - 85s/epoch - 435ms/step
Epoch 577/1000
2023-09-28 12:39:19.915 
Epoch 577/1000 
	 loss: 16.4231, MinusLogProbMetric: 16.4231, val_loss: 17.1762, val_MinusLogProbMetric: 17.1762

Epoch 577: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4231 - MinusLogProbMetric: 16.4231 - val_loss: 17.1762 - val_MinusLogProbMetric: 17.1762 - lr: 4.1667e-05 - 86s/epoch - 441ms/step
Epoch 578/1000
2023-09-28 12:40:45.745 
Epoch 578/1000 
	 loss: 16.4372, MinusLogProbMetric: 16.4372, val_loss: 17.0865, val_MinusLogProbMetric: 17.0865

Epoch 578: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4372 - MinusLogProbMetric: 16.4372 - val_loss: 17.0865 - val_MinusLogProbMetric: 17.0865 - lr: 4.1667e-05 - 86s/epoch - 438ms/step
Epoch 579/1000
2023-09-28 12:42:12.241 
Epoch 579/1000 
	 loss: 16.4278, MinusLogProbMetric: 16.4278, val_loss: 17.0589, val_MinusLogProbMetric: 17.0589

Epoch 579: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4278 - MinusLogProbMetric: 16.4278 - val_loss: 17.0589 - val_MinusLogProbMetric: 17.0589 - lr: 4.1667e-05 - 86s/epoch - 441ms/step
Epoch 580/1000
2023-09-28 12:43:38.729 
Epoch 580/1000 
	 loss: 16.4384, MinusLogProbMetric: 16.4384, val_loss: 17.1042, val_MinusLogProbMetric: 17.1042

Epoch 580: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4384 - MinusLogProbMetric: 16.4384 - val_loss: 17.1042 - val_MinusLogProbMetric: 17.1042 - lr: 4.1667e-05 - 86s/epoch - 441ms/step
Epoch 581/1000
2023-09-28 12:45:05.621 
Epoch 581/1000 
	 loss: 16.4196, MinusLogProbMetric: 16.4196, val_loss: 17.1084, val_MinusLogProbMetric: 17.1084

Epoch 581: val_loss did not improve from 17.05060
196/196 - 87s - loss: 16.4196 - MinusLogProbMetric: 16.4196 - val_loss: 17.1084 - val_MinusLogProbMetric: 17.1084 - lr: 4.1667e-05 - 87s/epoch - 443ms/step
Epoch 582/1000
2023-09-28 12:46:32.164 
Epoch 582/1000 
	 loss: 16.4354, MinusLogProbMetric: 16.4354, val_loss: 17.0918, val_MinusLogProbMetric: 17.0918

Epoch 582: val_loss did not improve from 17.05060
196/196 - 87s - loss: 16.4354 - MinusLogProbMetric: 16.4354 - val_loss: 17.0918 - val_MinusLogProbMetric: 17.0918 - lr: 4.1667e-05 - 87s/epoch - 441ms/step
Epoch 583/1000
2023-09-28 12:47:58.416 
Epoch 583/1000 
	 loss: 16.4382, MinusLogProbMetric: 16.4382, val_loss: 17.0827, val_MinusLogProbMetric: 17.0827

Epoch 583: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4382 - MinusLogProbMetric: 16.4382 - val_loss: 17.0827 - val_MinusLogProbMetric: 17.0827 - lr: 4.1667e-05 - 86s/epoch - 440ms/step
Epoch 584/1000
2023-09-28 12:49:24.433 
Epoch 584/1000 
	 loss: 16.4173, MinusLogProbMetric: 16.4173, val_loss: 17.0724, val_MinusLogProbMetric: 17.0724

Epoch 584: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4173 - MinusLogProbMetric: 16.4173 - val_loss: 17.0724 - val_MinusLogProbMetric: 17.0724 - lr: 4.1667e-05 - 86s/epoch - 439ms/step
Epoch 585/1000
2023-09-28 12:50:49.909 
Epoch 585/1000 
	 loss: 16.4275, MinusLogProbMetric: 16.4275, val_loss: 17.0625, val_MinusLogProbMetric: 17.0625

Epoch 585: val_loss did not improve from 17.05060
196/196 - 85s - loss: 16.4275 - MinusLogProbMetric: 16.4275 - val_loss: 17.0625 - val_MinusLogProbMetric: 17.0625 - lr: 4.1667e-05 - 85s/epoch - 436ms/step
Epoch 586/1000
2023-09-28 12:52:15.243 
Epoch 586/1000 
	 loss: 16.4183, MinusLogProbMetric: 16.4183, val_loss: 17.0708, val_MinusLogProbMetric: 17.0708

Epoch 586: val_loss did not improve from 17.05060
196/196 - 85s - loss: 16.4183 - MinusLogProbMetric: 16.4183 - val_loss: 17.0708 - val_MinusLogProbMetric: 17.0708 - lr: 4.1667e-05 - 85s/epoch - 435ms/step
Epoch 587/1000
2023-09-28 12:53:41.443 
Epoch 587/1000 
	 loss: 16.4171, MinusLogProbMetric: 16.4171, val_loss: 17.0875, val_MinusLogProbMetric: 17.0875

Epoch 587: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4171 - MinusLogProbMetric: 16.4171 - val_loss: 17.0875 - val_MinusLogProbMetric: 17.0875 - lr: 4.1667e-05 - 86s/epoch - 440ms/step
Epoch 588/1000
2023-09-28 12:55:08.929 
Epoch 588/1000 
	 loss: 16.4197, MinusLogProbMetric: 16.4197, val_loss: 17.1181, val_MinusLogProbMetric: 17.1181

Epoch 588: val_loss did not improve from 17.05060
196/196 - 87s - loss: 16.4197 - MinusLogProbMetric: 16.4197 - val_loss: 17.1181 - val_MinusLogProbMetric: 17.1181 - lr: 4.1667e-05 - 87s/epoch - 446ms/step
Epoch 589/1000
2023-09-28 12:56:34.803 
Epoch 589/1000 
	 loss: 16.4386, MinusLogProbMetric: 16.4386, val_loss: 17.1460, val_MinusLogProbMetric: 17.1460

Epoch 589: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4386 - MinusLogProbMetric: 16.4386 - val_loss: 17.1460 - val_MinusLogProbMetric: 17.1460 - lr: 4.1667e-05 - 86s/epoch - 438ms/step
Epoch 590/1000
2023-09-28 12:58:01.203 
Epoch 590/1000 
	 loss: 16.4179, MinusLogProbMetric: 16.4179, val_loss: 17.0618, val_MinusLogProbMetric: 17.0618

Epoch 590: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4179 - MinusLogProbMetric: 16.4179 - val_loss: 17.0618 - val_MinusLogProbMetric: 17.0618 - lr: 4.1667e-05 - 86s/epoch - 441ms/step
Epoch 591/1000
2023-09-28 12:59:28.112 
Epoch 591/1000 
	 loss: 16.4226, MinusLogProbMetric: 16.4226, val_loss: 17.0736, val_MinusLogProbMetric: 17.0736

Epoch 591: val_loss did not improve from 17.05060
196/196 - 87s - loss: 16.4226 - MinusLogProbMetric: 16.4226 - val_loss: 17.0736 - val_MinusLogProbMetric: 17.0736 - lr: 4.1667e-05 - 87s/epoch - 443ms/step
Epoch 592/1000
2023-09-28 13:00:54.251 
Epoch 592/1000 
	 loss: 16.4395, MinusLogProbMetric: 16.4395, val_loss: 17.1151, val_MinusLogProbMetric: 17.1151

Epoch 592: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4395 - MinusLogProbMetric: 16.4395 - val_loss: 17.1151 - val_MinusLogProbMetric: 17.1151 - lr: 4.1667e-05 - 86s/epoch - 439ms/step
Epoch 593/1000
2023-09-28 13:02:20.041 
Epoch 593/1000 
	 loss: 16.4268, MinusLogProbMetric: 16.4268, val_loss: 17.1015, val_MinusLogProbMetric: 17.1015

Epoch 593: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4268 - MinusLogProbMetric: 16.4268 - val_loss: 17.1015 - val_MinusLogProbMetric: 17.1015 - lr: 4.1667e-05 - 86s/epoch - 438ms/step
Epoch 594/1000
2023-09-28 13:03:46.483 
Epoch 594/1000 
	 loss: 16.4139, MinusLogProbMetric: 16.4139, val_loss: 17.1314, val_MinusLogProbMetric: 17.1314

Epoch 594: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4139 - MinusLogProbMetric: 16.4139 - val_loss: 17.1314 - val_MinusLogProbMetric: 17.1314 - lr: 4.1667e-05 - 86s/epoch - 441ms/step
Epoch 595/1000
2023-09-28 13:05:12.109 
Epoch 595/1000 
	 loss: 16.4218, MinusLogProbMetric: 16.4218, val_loss: 17.1036, val_MinusLogProbMetric: 17.1036

Epoch 595: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4218 - MinusLogProbMetric: 16.4218 - val_loss: 17.1036 - val_MinusLogProbMetric: 17.1036 - lr: 4.1667e-05 - 86s/epoch - 437ms/step
Epoch 596/1000
2023-09-28 13:06:37.808 
Epoch 596/1000 
	 loss: 16.4186, MinusLogProbMetric: 16.4186, val_loss: 17.0838, val_MinusLogProbMetric: 17.0838

Epoch 596: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4186 - MinusLogProbMetric: 16.4186 - val_loss: 17.0838 - val_MinusLogProbMetric: 17.0838 - lr: 4.1667e-05 - 86s/epoch - 437ms/step
Epoch 597/1000
2023-09-28 13:08:04.968 
Epoch 597/1000 
	 loss: 16.4237, MinusLogProbMetric: 16.4237, val_loss: 17.0764, val_MinusLogProbMetric: 17.0764

Epoch 597: val_loss did not improve from 17.05060
196/196 - 87s - loss: 16.4237 - MinusLogProbMetric: 16.4237 - val_loss: 17.0764 - val_MinusLogProbMetric: 17.0764 - lr: 4.1667e-05 - 87s/epoch - 445ms/step
Epoch 598/1000
2023-09-28 13:09:33.035 
Epoch 598/1000 
	 loss: 16.4175, MinusLogProbMetric: 16.4175, val_loss: 17.0646, val_MinusLogProbMetric: 17.0646

Epoch 598: val_loss did not improve from 17.05060
196/196 - 88s - loss: 16.4175 - MinusLogProbMetric: 16.4175 - val_loss: 17.0646 - val_MinusLogProbMetric: 17.0646 - lr: 4.1667e-05 - 88s/epoch - 449ms/step
Epoch 599/1000
2023-09-28 13:10:58.504 
Epoch 599/1000 
	 loss: 16.4178, MinusLogProbMetric: 16.4178, val_loss: 17.1054, val_MinusLogProbMetric: 17.1054

Epoch 599: val_loss did not improve from 17.05060
196/196 - 85s - loss: 16.4178 - MinusLogProbMetric: 16.4178 - val_loss: 17.1054 - val_MinusLogProbMetric: 17.1054 - lr: 4.1667e-05 - 85s/epoch - 436ms/step
Epoch 600/1000
2023-09-28 13:12:24.066 
Epoch 600/1000 
	 loss: 16.4330, MinusLogProbMetric: 16.4330, val_loss: 17.0741, val_MinusLogProbMetric: 17.0741

Epoch 600: val_loss did not improve from 17.05060
196/196 - 86s - loss: 16.4330 - MinusLogProbMetric: 16.4330 - val_loss: 17.0741 - val_MinusLogProbMetric: 17.0741 - lr: 4.1667e-05 - 86s/epoch - 437ms/step
Epoch 601/1000
2023-09-28 13:13:50.619 
Epoch 601/1000 
	 loss: 16.4302, MinusLogProbMetric: 16.4302, val_loss: 17.0729, val_MinusLogProbMetric: 17.0729

Epoch 601: val_loss did not improve from 17.05060
196/196 - 87s - loss: 16.4302 - MinusLogProbMetric: 16.4302 - val_loss: 17.0729 - val_MinusLogProbMetric: 17.0729 - lr: 4.1667e-05 - 87s/epoch - 442ms/step
Epoch 602/1000
2023-09-28 13:15:14.978 
Epoch 602/1000 
	 loss: 16.4126, MinusLogProbMetric: 16.4126, val_loss: 17.0654, val_MinusLogProbMetric: 17.0654

Epoch 602: val_loss did not improve from 17.05060
196/196 - 84s - loss: 16.4126 - MinusLogProbMetric: 16.4126 - val_loss: 17.0654 - val_MinusLogProbMetric: 17.0654 - lr: 4.1667e-05 - 84s/epoch - 430ms/step
Epoch 603/1000
2023-09-28 13:16:40.163 
Epoch 603/1000 
	 loss: 16.4114, MinusLogProbMetric: 16.4114, val_loss: 17.0668, val_MinusLogProbMetric: 17.0668

Epoch 603: val_loss did not improve from 17.05060
196/196 - 85s - loss: 16.4114 - MinusLogProbMetric: 16.4114 - val_loss: 17.0668 - val_MinusLogProbMetric: 17.0668 - lr: 4.1667e-05 - 85s/epoch - 435ms/step
Epoch 604/1000
2023-09-28 13:18:05.640 
Epoch 604/1000 
	 loss: 16.4213, MinusLogProbMetric: 16.4213, val_loss: 17.0934, val_MinusLogProbMetric: 17.0934

Epoch 604: val_loss did not improve from 17.05060
196/196 - 85s - loss: 16.4213 - MinusLogProbMetric: 16.4213 - val_loss: 17.0934 - val_MinusLogProbMetric: 17.0934 - lr: 4.1667e-05 - 85s/epoch - 436ms/step
Epoch 605/1000
2023-09-28 13:19:29.946 
Epoch 605/1000 
	 loss: 16.4105, MinusLogProbMetric: 16.4105, val_loss: 17.0995, val_MinusLogProbMetric: 17.0995

Epoch 605: val_loss did not improve from 17.05060
196/196 - 84s - loss: 16.4105 - MinusLogProbMetric: 16.4105 - val_loss: 17.0995 - val_MinusLogProbMetric: 17.0995 - lr: 4.1667e-05 - 84s/epoch - 430ms/step
Epoch 606/1000
2023-09-28 13:20:55.005 
Epoch 606/1000 
	 loss: 16.3832, MinusLogProbMetric: 16.3832, val_loss: 17.0448, val_MinusLogProbMetric: 17.0448

Epoch 606: val_loss improved from 17.05060 to 17.04484, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 86s - loss: 16.3832 - MinusLogProbMetric: 16.3832 - val_loss: 17.0448 - val_MinusLogProbMetric: 17.0448 - lr: 2.0833e-05 - 86s/epoch - 440ms/step
Epoch 607/1000
2023-09-28 13:22:19.903 
Epoch 607/1000 
	 loss: 16.3795, MinusLogProbMetric: 16.3795, val_loss: 17.0445, val_MinusLogProbMetric: 17.0445

Epoch 607: val_loss improved from 17.04484 to 17.04453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 85s - loss: 16.3795 - MinusLogProbMetric: 16.3795 - val_loss: 17.0445 - val_MinusLogProbMetric: 17.0445 - lr: 2.0833e-05 - 85s/epoch - 432ms/step
Epoch 608/1000
2023-09-28 13:23:46.632 
Epoch 608/1000 
	 loss: 16.3760, MinusLogProbMetric: 16.3760, val_loss: 17.0555, val_MinusLogProbMetric: 17.0555

Epoch 608: val_loss did not improve from 17.04453
196/196 - 86s - loss: 16.3760 - MinusLogProbMetric: 16.3760 - val_loss: 17.0555 - val_MinusLogProbMetric: 17.0555 - lr: 2.0833e-05 - 86s/epoch - 437ms/step
Epoch 609/1000
2023-09-28 13:25:09.822 
Epoch 609/1000 
	 loss: 16.3857, MinusLogProbMetric: 16.3857, val_loss: 17.0614, val_MinusLogProbMetric: 17.0614

Epoch 609: val_loss did not improve from 17.04453
196/196 - 83s - loss: 16.3857 - MinusLogProbMetric: 16.3857 - val_loss: 17.0614 - val_MinusLogProbMetric: 17.0614 - lr: 2.0833e-05 - 83s/epoch - 424ms/step
Epoch 610/1000
2023-09-28 13:26:32.224 
Epoch 610/1000 
	 loss: 16.3812, MinusLogProbMetric: 16.3812, val_loss: 17.0488, val_MinusLogProbMetric: 17.0488

Epoch 610: val_loss did not improve from 17.04453
196/196 - 82s - loss: 16.3812 - MinusLogProbMetric: 16.3812 - val_loss: 17.0488 - val_MinusLogProbMetric: 17.0488 - lr: 2.0833e-05 - 82s/epoch - 420ms/step
Epoch 611/1000
2023-09-28 13:27:58.819 
Epoch 611/1000 
	 loss: 16.3794, MinusLogProbMetric: 16.3794, val_loss: 17.0538, val_MinusLogProbMetric: 17.0538

Epoch 611: val_loss did not improve from 17.04453
196/196 - 87s - loss: 16.3794 - MinusLogProbMetric: 16.3794 - val_loss: 17.0538 - val_MinusLogProbMetric: 17.0538 - lr: 2.0833e-05 - 87s/epoch - 442ms/step
Epoch 612/1000
2023-09-28 13:29:24.799 
Epoch 612/1000 
	 loss: 16.3818, MinusLogProbMetric: 16.3818, val_loss: 17.0423, val_MinusLogProbMetric: 17.0423

Epoch 612: val_loss improved from 17.04453 to 17.04225, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 87s - loss: 16.3818 - MinusLogProbMetric: 16.3818 - val_loss: 17.0423 - val_MinusLogProbMetric: 17.0423 - lr: 2.0833e-05 - 87s/epoch - 444ms/step
Epoch 613/1000
2023-09-28 13:30:51.489 
Epoch 613/1000 
	 loss: 16.3789, MinusLogProbMetric: 16.3789, val_loss: 17.0464, val_MinusLogProbMetric: 17.0464

Epoch 613: val_loss did not improve from 17.04225
196/196 - 86s - loss: 16.3789 - MinusLogProbMetric: 16.3789 - val_loss: 17.0464 - val_MinusLogProbMetric: 17.0464 - lr: 2.0833e-05 - 86s/epoch - 436ms/step
Epoch 614/1000
2023-09-28 13:32:18.597 
Epoch 614/1000 
	 loss: 16.3773, MinusLogProbMetric: 16.3773, val_loss: 17.0494, val_MinusLogProbMetric: 17.0494

Epoch 614: val_loss did not improve from 17.04225
196/196 - 87s - loss: 16.3773 - MinusLogProbMetric: 16.3773 - val_loss: 17.0494 - val_MinusLogProbMetric: 17.0494 - lr: 2.0833e-05 - 87s/epoch - 444ms/step
Epoch 615/1000
2023-09-28 13:33:46.205 
Epoch 615/1000 
	 loss: 16.3758, MinusLogProbMetric: 16.3758, val_loss: 17.0483, val_MinusLogProbMetric: 17.0483

Epoch 615: val_loss did not improve from 17.04225
196/196 - 88s - loss: 16.3758 - MinusLogProbMetric: 16.3758 - val_loss: 17.0483 - val_MinusLogProbMetric: 17.0483 - lr: 2.0833e-05 - 88s/epoch - 447ms/step
Epoch 616/1000
2023-09-28 13:35:12.914 
Epoch 616/1000 
	 loss: 16.3780, MinusLogProbMetric: 16.3780, val_loss: 17.0530, val_MinusLogProbMetric: 17.0530

Epoch 616: val_loss did not improve from 17.04225
196/196 - 87s - loss: 16.3780 - MinusLogProbMetric: 16.3780 - val_loss: 17.0530 - val_MinusLogProbMetric: 17.0530 - lr: 2.0833e-05 - 87s/epoch - 442ms/step
Epoch 617/1000
2023-09-28 13:36:39.115 
Epoch 617/1000 
	 loss: 16.3781, MinusLogProbMetric: 16.3781, val_loss: 17.0485, val_MinusLogProbMetric: 17.0485

Epoch 617: val_loss did not improve from 17.04225
196/196 - 86s - loss: 16.3781 - MinusLogProbMetric: 16.3781 - val_loss: 17.0485 - val_MinusLogProbMetric: 17.0485 - lr: 2.0833e-05 - 86s/epoch - 440ms/step
Epoch 618/1000
2023-09-28 13:38:06.263 
Epoch 618/1000 
	 loss: 16.3730, MinusLogProbMetric: 16.3730, val_loss: 17.0471, val_MinusLogProbMetric: 17.0471

Epoch 618: val_loss did not improve from 17.04225
196/196 - 87s - loss: 16.3730 - MinusLogProbMetric: 16.3730 - val_loss: 17.0471 - val_MinusLogProbMetric: 17.0471 - lr: 2.0833e-05 - 87s/epoch - 445ms/step
Epoch 619/1000
2023-09-28 13:39:34.161 
Epoch 619/1000 
	 loss: 16.3775, MinusLogProbMetric: 16.3775, val_loss: 17.0458, val_MinusLogProbMetric: 17.0458

Epoch 619: val_loss did not improve from 17.04225
196/196 - 88s - loss: 16.3775 - MinusLogProbMetric: 16.3775 - val_loss: 17.0458 - val_MinusLogProbMetric: 17.0458 - lr: 2.0833e-05 - 88s/epoch - 448ms/step
Epoch 620/1000
2023-09-28 13:41:01.937 
Epoch 620/1000 
	 loss: 16.3740, MinusLogProbMetric: 16.3740, val_loss: 17.0501, val_MinusLogProbMetric: 17.0501

Epoch 620: val_loss did not improve from 17.04225
196/196 - 88s - loss: 16.3740 - MinusLogProbMetric: 16.3740 - val_loss: 17.0501 - val_MinusLogProbMetric: 17.0501 - lr: 2.0833e-05 - 88s/epoch - 448ms/step
Epoch 621/1000
2023-09-28 13:42:30.099 
Epoch 621/1000 
	 loss: 16.3734, MinusLogProbMetric: 16.3734, val_loss: 17.0504, val_MinusLogProbMetric: 17.0504

Epoch 621: val_loss did not improve from 17.04225
196/196 - 88s - loss: 16.3734 - MinusLogProbMetric: 16.3734 - val_loss: 17.0504 - val_MinusLogProbMetric: 17.0504 - lr: 2.0833e-05 - 88s/epoch - 450ms/step
Epoch 622/1000
2023-09-28 13:43:57.967 
Epoch 622/1000 
	 loss: 16.3762, MinusLogProbMetric: 16.3762, val_loss: 17.0476, val_MinusLogProbMetric: 17.0476

Epoch 622: val_loss did not improve from 17.04225
196/196 - 88s - loss: 16.3762 - MinusLogProbMetric: 16.3762 - val_loss: 17.0476 - val_MinusLogProbMetric: 17.0476 - lr: 2.0833e-05 - 88s/epoch - 448ms/step
Epoch 623/1000
2023-09-28 13:45:23.586 
Epoch 623/1000 
	 loss: 16.3793, MinusLogProbMetric: 16.3793, val_loss: 17.0738, val_MinusLogProbMetric: 17.0738

Epoch 623: val_loss did not improve from 17.04225
196/196 - 86s - loss: 16.3793 - MinusLogProbMetric: 16.3793 - val_loss: 17.0738 - val_MinusLogProbMetric: 17.0738 - lr: 2.0833e-05 - 86s/epoch - 437ms/step
Epoch 624/1000
2023-09-28 13:46:49.564 
Epoch 624/1000 
	 loss: 16.3792, MinusLogProbMetric: 16.3792, val_loss: 17.0739, val_MinusLogProbMetric: 17.0739

Epoch 624: val_loss did not improve from 17.04225
196/196 - 86s - loss: 16.3792 - MinusLogProbMetric: 16.3792 - val_loss: 17.0739 - val_MinusLogProbMetric: 17.0739 - lr: 2.0833e-05 - 86s/epoch - 439ms/step
Epoch 625/1000
2023-09-28 13:48:15.692 
Epoch 625/1000 
	 loss: 16.3808, MinusLogProbMetric: 16.3808, val_loss: 17.0739, val_MinusLogProbMetric: 17.0739

Epoch 625: val_loss did not improve from 17.04225
196/196 - 86s - loss: 16.3808 - MinusLogProbMetric: 16.3808 - val_loss: 17.0739 - val_MinusLogProbMetric: 17.0739 - lr: 2.0833e-05 - 86s/epoch - 439ms/step
Epoch 626/1000
2023-09-28 13:49:40.193 
Epoch 626/1000 
	 loss: 16.3755, MinusLogProbMetric: 16.3755, val_loss: 17.0550, val_MinusLogProbMetric: 17.0550

Epoch 626: val_loss did not improve from 17.04225
196/196 - 84s - loss: 16.3755 - MinusLogProbMetric: 16.3755 - val_loss: 17.0550 - val_MinusLogProbMetric: 17.0550 - lr: 2.0833e-05 - 84s/epoch - 431ms/step
Epoch 627/1000
2023-09-28 13:51:06.216 
Epoch 627/1000 
	 loss: 16.3743, MinusLogProbMetric: 16.3743, val_loss: 17.0382, val_MinusLogProbMetric: 17.0382

Epoch 627: val_loss improved from 17.04225 to 17.03822, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 87s - loss: 16.3743 - MinusLogProbMetric: 16.3743 - val_loss: 17.0382 - val_MinusLogProbMetric: 17.0382 - lr: 2.0833e-05 - 87s/epoch - 444ms/step
Epoch 628/1000
2023-09-28 13:52:33.434 
Epoch 628/1000 
	 loss: 16.3757, MinusLogProbMetric: 16.3757, val_loss: 17.0401, val_MinusLogProbMetric: 17.0401

Epoch 628: val_loss did not improve from 17.03822
196/196 - 86s - loss: 16.3757 - MinusLogProbMetric: 16.3757 - val_loss: 17.0401 - val_MinusLogProbMetric: 17.0401 - lr: 2.0833e-05 - 86s/epoch - 440ms/step
Epoch 629/1000
2023-09-28 13:53:59.774 
Epoch 629/1000 
	 loss: 16.3759, MinusLogProbMetric: 16.3759, val_loss: 17.0481, val_MinusLogProbMetric: 17.0481

Epoch 629: val_loss did not improve from 17.03822
196/196 - 86s - loss: 16.3759 - MinusLogProbMetric: 16.3759 - val_loss: 17.0481 - val_MinusLogProbMetric: 17.0481 - lr: 2.0833e-05 - 86s/epoch - 440ms/step
Epoch 630/1000
2023-09-28 13:55:24.158 
Epoch 630/1000 
	 loss: 16.3727, MinusLogProbMetric: 16.3727, val_loss: 17.0459, val_MinusLogProbMetric: 17.0459

Epoch 630: val_loss did not improve from 17.03822
196/196 - 84s - loss: 16.3727 - MinusLogProbMetric: 16.3727 - val_loss: 17.0459 - val_MinusLogProbMetric: 17.0459 - lr: 2.0833e-05 - 84s/epoch - 430ms/step
Epoch 631/1000
2023-09-28 13:56:50.004 
Epoch 631/1000 
	 loss: 16.3718, MinusLogProbMetric: 16.3718, val_loss: 17.0492, val_MinusLogProbMetric: 17.0492

Epoch 631: val_loss did not improve from 17.03822
196/196 - 86s - loss: 16.3718 - MinusLogProbMetric: 16.3718 - val_loss: 17.0492 - val_MinusLogProbMetric: 17.0492 - lr: 2.0833e-05 - 86s/epoch - 438ms/step
Epoch 632/1000
2023-09-28 13:58:15.533 
Epoch 632/1000 
	 loss: 16.3755, MinusLogProbMetric: 16.3755, val_loss: 17.0422, val_MinusLogProbMetric: 17.0422

Epoch 632: val_loss did not improve from 17.03822
196/196 - 86s - loss: 16.3755 - MinusLogProbMetric: 16.3755 - val_loss: 17.0422 - val_MinusLogProbMetric: 17.0422 - lr: 2.0833e-05 - 86s/epoch - 436ms/step
Epoch 633/1000
2023-09-28 13:59:42.552 
Epoch 633/1000 
	 loss: 16.3718, MinusLogProbMetric: 16.3718, val_loss: 17.0399, val_MinusLogProbMetric: 17.0399

Epoch 633: val_loss did not improve from 17.03822
196/196 - 87s - loss: 16.3718 - MinusLogProbMetric: 16.3718 - val_loss: 17.0399 - val_MinusLogProbMetric: 17.0399 - lr: 2.0833e-05 - 87s/epoch - 444ms/step
Epoch 634/1000
2023-09-28 14:01:07.758 
Epoch 634/1000 
	 loss: 16.3765, MinusLogProbMetric: 16.3765, val_loss: 17.0540, val_MinusLogProbMetric: 17.0540

Epoch 634: val_loss did not improve from 17.03822
196/196 - 85s - loss: 16.3765 - MinusLogProbMetric: 16.3765 - val_loss: 17.0540 - val_MinusLogProbMetric: 17.0540 - lr: 2.0833e-05 - 85s/epoch - 435ms/step
Epoch 635/1000
2023-09-28 14:02:34.409 
Epoch 635/1000 
	 loss: 16.3732, MinusLogProbMetric: 16.3732, val_loss: 17.0653, val_MinusLogProbMetric: 17.0653

Epoch 635: val_loss did not improve from 17.03822
196/196 - 87s - loss: 16.3732 - MinusLogProbMetric: 16.3732 - val_loss: 17.0653 - val_MinusLogProbMetric: 17.0653 - lr: 2.0833e-05 - 87s/epoch - 442ms/step
Epoch 636/1000
2023-09-28 14:04:00.556 
Epoch 636/1000 
	 loss: 16.3747, MinusLogProbMetric: 16.3747, val_loss: 17.0723, val_MinusLogProbMetric: 17.0723

Epoch 636: val_loss did not improve from 17.03822
196/196 - 86s - loss: 16.3747 - MinusLogProbMetric: 16.3747 - val_loss: 17.0723 - val_MinusLogProbMetric: 17.0723 - lr: 2.0833e-05 - 86s/epoch - 439ms/step
Epoch 637/1000
2023-09-28 14:05:25.867 
Epoch 637/1000 
	 loss: 16.3756, MinusLogProbMetric: 16.3756, val_loss: 17.0470, val_MinusLogProbMetric: 17.0470

Epoch 637: val_loss did not improve from 17.03822
196/196 - 85s - loss: 16.3756 - MinusLogProbMetric: 16.3756 - val_loss: 17.0470 - val_MinusLogProbMetric: 17.0470 - lr: 2.0833e-05 - 85s/epoch - 435ms/step
Epoch 638/1000
2023-09-28 14:06:50.080 
Epoch 638/1000 
	 loss: 16.3746, MinusLogProbMetric: 16.3746, val_loss: 17.0513, val_MinusLogProbMetric: 17.0513

Epoch 638: val_loss did not improve from 17.03822
196/196 - 84s - loss: 16.3746 - MinusLogProbMetric: 16.3746 - val_loss: 17.0513 - val_MinusLogProbMetric: 17.0513 - lr: 2.0833e-05 - 84s/epoch - 430ms/step
Epoch 639/1000
2023-09-28 14:08:14.452 
Epoch 639/1000 
	 loss: 16.3750, MinusLogProbMetric: 16.3750, val_loss: 17.0479, val_MinusLogProbMetric: 17.0479

Epoch 639: val_loss did not improve from 17.03822
196/196 - 84s - loss: 16.3750 - MinusLogProbMetric: 16.3750 - val_loss: 17.0479 - val_MinusLogProbMetric: 17.0479 - lr: 2.0833e-05 - 84s/epoch - 430ms/step
Epoch 640/1000
2023-09-28 14:09:40.767 
Epoch 640/1000 
	 loss: 16.3742, MinusLogProbMetric: 16.3742, val_loss: 17.1027, val_MinusLogProbMetric: 17.1027

Epoch 640: val_loss did not improve from 17.03822
196/196 - 86s - loss: 16.3742 - MinusLogProbMetric: 16.3742 - val_loss: 17.1027 - val_MinusLogProbMetric: 17.1027 - lr: 2.0833e-05 - 86s/epoch - 440ms/step
Epoch 641/1000
2023-09-28 14:11:08.135 
Epoch 641/1000 
	 loss: 16.3757, MinusLogProbMetric: 16.3757, val_loss: 17.0457, val_MinusLogProbMetric: 17.0457

Epoch 641: val_loss did not improve from 17.03822
196/196 - 87s - loss: 16.3757 - MinusLogProbMetric: 16.3757 - val_loss: 17.0457 - val_MinusLogProbMetric: 17.0457 - lr: 2.0833e-05 - 87s/epoch - 446ms/step
Epoch 642/1000
2023-09-28 14:12:33.144 
Epoch 642/1000 
	 loss: 16.3757, MinusLogProbMetric: 16.3757, val_loss: 17.0689, val_MinusLogProbMetric: 17.0689

Epoch 642: val_loss did not improve from 17.03822
196/196 - 85s - loss: 16.3757 - MinusLogProbMetric: 16.3757 - val_loss: 17.0689 - val_MinusLogProbMetric: 17.0689 - lr: 2.0833e-05 - 85s/epoch - 434ms/step
Epoch 643/1000
2023-09-28 14:13:56.548 
Epoch 643/1000 
	 loss: 16.3779, MinusLogProbMetric: 16.3779, val_loss: 17.0549, val_MinusLogProbMetric: 17.0549

Epoch 643: val_loss did not improve from 17.03822
196/196 - 83s - loss: 16.3779 - MinusLogProbMetric: 16.3779 - val_loss: 17.0549 - val_MinusLogProbMetric: 17.0549 - lr: 2.0833e-05 - 83s/epoch - 426ms/step
Epoch 644/1000
2023-09-28 14:15:20.931 
Epoch 644/1000 
	 loss: 16.3791, MinusLogProbMetric: 16.3791, val_loss: 17.1332, val_MinusLogProbMetric: 17.1332

Epoch 644: val_loss did not improve from 17.03822
196/196 - 84s - loss: 16.3791 - MinusLogProbMetric: 16.3791 - val_loss: 17.1332 - val_MinusLogProbMetric: 17.1332 - lr: 2.0833e-05 - 84s/epoch - 430ms/step
Epoch 645/1000
2023-09-28 14:16:44.608 
Epoch 645/1000 
	 loss: 16.3822, MinusLogProbMetric: 16.3822, val_loss: 17.0552, val_MinusLogProbMetric: 17.0552

Epoch 645: val_loss did not improve from 17.03822
196/196 - 84s - loss: 16.3822 - MinusLogProbMetric: 16.3822 - val_loss: 17.0552 - val_MinusLogProbMetric: 17.0552 - lr: 2.0833e-05 - 84s/epoch - 427ms/step
Epoch 646/1000
2023-09-28 14:18:07.658 
Epoch 646/1000 
	 loss: 16.3729, MinusLogProbMetric: 16.3729, val_loss: 17.0483, val_MinusLogProbMetric: 17.0483

Epoch 646: val_loss did not improve from 17.03822
196/196 - 83s - loss: 16.3729 - MinusLogProbMetric: 16.3729 - val_loss: 17.0483 - val_MinusLogProbMetric: 17.0483 - lr: 2.0833e-05 - 83s/epoch - 424ms/step
Epoch 647/1000
2023-09-28 14:19:33.296 
Epoch 647/1000 
	 loss: 16.3709, MinusLogProbMetric: 16.3709, val_loss: 17.0435, val_MinusLogProbMetric: 17.0435

Epoch 647: val_loss did not improve from 17.03822
196/196 - 86s - loss: 16.3709 - MinusLogProbMetric: 16.3709 - val_loss: 17.0435 - val_MinusLogProbMetric: 17.0435 - lr: 2.0833e-05 - 86s/epoch - 437ms/step
Epoch 648/1000
2023-09-28 14:20:59.774 
Epoch 648/1000 
	 loss: 16.3747, MinusLogProbMetric: 16.3747, val_loss: 17.0495, val_MinusLogProbMetric: 17.0495

Epoch 648: val_loss did not improve from 17.03822
196/196 - 86s - loss: 16.3747 - MinusLogProbMetric: 16.3747 - val_loss: 17.0495 - val_MinusLogProbMetric: 17.0495 - lr: 2.0833e-05 - 86s/epoch - 441ms/step
Epoch 649/1000
2023-09-28 14:22:17.729 
Epoch 649/1000 
	 loss: 16.3712, MinusLogProbMetric: 16.3712, val_loss: 17.0530, val_MinusLogProbMetric: 17.0530

Epoch 649: val_loss did not improve from 17.03822
196/196 - 78s - loss: 16.3712 - MinusLogProbMetric: 16.3712 - val_loss: 17.0530 - val_MinusLogProbMetric: 17.0530 - lr: 2.0833e-05 - 78s/epoch - 398ms/step
Epoch 650/1000
2023-09-28 14:23:40.563 
Epoch 650/1000 
	 loss: 16.3739, MinusLogProbMetric: 16.3739, val_loss: 17.0504, val_MinusLogProbMetric: 17.0504

Epoch 650: val_loss did not improve from 17.03822
196/196 - 83s - loss: 16.3739 - MinusLogProbMetric: 16.3739 - val_loss: 17.0504 - val_MinusLogProbMetric: 17.0504 - lr: 2.0833e-05 - 83s/epoch - 423ms/step
Epoch 651/1000
2023-09-28 14:25:02.495 
Epoch 651/1000 
	 loss: 16.3705, MinusLogProbMetric: 16.3705, val_loss: 17.0855, val_MinusLogProbMetric: 17.0855

Epoch 651: val_loss did not improve from 17.03822
196/196 - 82s - loss: 16.3705 - MinusLogProbMetric: 16.3705 - val_loss: 17.0855 - val_MinusLogProbMetric: 17.0855 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 652/1000
2023-09-28 14:26:18.855 
Epoch 652/1000 
	 loss: 16.3756, MinusLogProbMetric: 16.3756, val_loss: 17.0493, val_MinusLogProbMetric: 17.0493

Epoch 652: val_loss did not improve from 17.03822
196/196 - 76s - loss: 16.3756 - MinusLogProbMetric: 16.3756 - val_loss: 17.0493 - val_MinusLogProbMetric: 17.0493 - lr: 2.0833e-05 - 76s/epoch - 390ms/step
Epoch 653/1000
2023-09-28 14:27:39.978 
Epoch 653/1000 
	 loss: 16.3746, MinusLogProbMetric: 16.3746, val_loss: 17.0450, val_MinusLogProbMetric: 17.0450

Epoch 653: val_loss did not improve from 17.03822
196/196 - 81s - loss: 16.3746 - MinusLogProbMetric: 16.3746 - val_loss: 17.0450 - val_MinusLogProbMetric: 17.0450 - lr: 2.0833e-05 - 81s/epoch - 414ms/step
Epoch 654/1000
2023-09-28 14:29:02.552 
Epoch 654/1000 
	 loss: 16.3673, MinusLogProbMetric: 16.3673, val_loss: 17.0469, val_MinusLogProbMetric: 17.0469

Epoch 654: val_loss did not improve from 17.03822
196/196 - 83s - loss: 16.3673 - MinusLogProbMetric: 16.3673 - val_loss: 17.0469 - val_MinusLogProbMetric: 17.0469 - lr: 2.0833e-05 - 83s/epoch - 421ms/step
Epoch 655/1000
2023-09-28 14:30:17.634 
Epoch 655/1000 
	 loss: 16.3731, MinusLogProbMetric: 16.3731, val_loss: 17.0613, val_MinusLogProbMetric: 17.0613

Epoch 655: val_loss did not improve from 17.03822
196/196 - 75s - loss: 16.3731 - MinusLogProbMetric: 16.3731 - val_loss: 17.0613 - val_MinusLogProbMetric: 17.0613 - lr: 2.0833e-05 - 75s/epoch - 383ms/step
Epoch 656/1000
2023-09-28 14:31:31.983 
Epoch 656/1000 
	 loss: 16.3744, MinusLogProbMetric: 16.3744, val_loss: 17.0623, val_MinusLogProbMetric: 17.0623

Epoch 656: val_loss did not improve from 17.03822
196/196 - 74s - loss: 16.3744 - MinusLogProbMetric: 16.3744 - val_loss: 17.0623 - val_MinusLogProbMetric: 17.0623 - lr: 2.0833e-05 - 74s/epoch - 379ms/step
Epoch 657/1000
2023-09-28 14:32:45.413 
Epoch 657/1000 
	 loss: 16.3693, MinusLogProbMetric: 16.3693, val_loss: 17.0456, val_MinusLogProbMetric: 17.0456

Epoch 657: val_loss did not improve from 17.03822
196/196 - 73s - loss: 16.3693 - MinusLogProbMetric: 16.3693 - val_loss: 17.0456 - val_MinusLogProbMetric: 17.0456 - lr: 2.0833e-05 - 73s/epoch - 375ms/step
Epoch 658/1000
2023-09-28 14:34:03.524 
Epoch 658/1000 
	 loss: 16.3701, MinusLogProbMetric: 16.3701, val_loss: 17.0476, val_MinusLogProbMetric: 17.0476

Epoch 658: val_loss did not improve from 17.03822
196/196 - 78s - loss: 16.3701 - MinusLogProbMetric: 16.3701 - val_loss: 17.0476 - val_MinusLogProbMetric: 17.0476 - lr: 2.0833e-05 - 78s/epoch - 398ms/step
Epoch 659/1000
2023-09-28 14:35:16.520 
Epoch 659/1000 
	 loss: 16.3725, MinusLogProbMetric: 16.3725, val_loss: 17.0464, val_MinusLogProbMetric: 17.0464

Epoch 659: val_loss did not improve from 17.03822
196/196 - 73s - loss: 16.3725 - MinusLogProbMetric: 16.3725 - val_loss: 17.0464 - val_MinusLogProbMetric: 17.0464 - lr: 2.0833e-05 - 73s/epoch - 372ms/step
Epoch 660/1000
2023-09-28 14:36:31.226 
Epoch 660/1000 
	 loss: 16.3703, MinusLogProbMetric: 16.3703, val_loss: 17.0456, val_MinusLogProbMetric: 17.0456

Epoch 660: val_loss did not improve from 17.03822
196/196 - 75s - loss: 16.3703 - MinusLogProbMetric: 16.3703 - val_loss: 17.0456 - val_MinusLogProbMetric: 17.0456 - lr: 2.0833e-05 - 75s/epoch - 381ms/step
Epoch 661/1000
2023-09-28 14:37:44.862 
Epoch 661/1000 
	 loss: 16.3700, MinusLogProbMetric: 16.3700, val_loss: 17.0474, val_MinusLogProbMetric: 17.0474

Epoch 661: val_loss did not improve from 17.03822
196/196 - 74s - loss: 16.3700 - MinusLogProbMetric: 16.3700 - val_loss: 17.0474 - val_MinusLogProbMetric: 17.0474 - lr: 2.0833e-05 - 74s/epoch - 376ms/step
Epoch 662/1000
2023-09-28 14:38:59.786 
Epoch 662/1000 
	 loss: 16.3699, MinusLogProbMetric: 16.3699, val_loss: 17.0762, val_MinusLogProbMetric: 17.0762

Epoch 662: val_loss did not improve from 17.03822
196/196 - 75s - loss: 16.3699 - MinusLogProbMetric: 16.3699 - val_loss: 17.0762 - val_MinusLogProbMetric: 17.0762 - lr: 2.0833e-05 - 75s/epoch - 382ms/step
Epoch 663/1000
2023-09-28 14:40:19.697 
Epoch 663/1000 
	 loss: 16.3705, MinusLogProbMetric: 16.3705, val_loss: 17.0439, val_MinusLogProbMetric: 17.0439

Epoch 663: val_loss did not improve from 17.03822
196/196 - 80s - loss: 16.3705 - MinusLogProbMetric: 16.3705 - val_loss: 17.0439 - val_MinusLogProbMetric: 17.0439 - lr: 2.0833e-05 - 80s/epoch - 408ms/step
Epoch 664/1000
2023-09-28 14:41:41.234 
Epoch 664/1000 
	 loss: 16.3685, MinusLogProbMetric: 16.3685, val_loss: 17.0410, val_MinusLogProbMetric: 17.0410

Epoch 664: val_loss did not improve from 17.03822
196/196 - 82s - loss: 16.3685 - MinusLogProbMetric: 16.3685 - val_loss: 17.0410 - val_MinusLogProbMetric: 17.0410 - lr: 2.0833e-05 - 82s/epoch - 416ms/step
Epoch 665/1000
2023-09-28 14:43:04.224 
Epoch 665/1000 
	 loss: 16.3696, MinusLogProbMetric: 16.3696, val_loss: 17.0604, val_MinusLogProbMetric: 17.0604

Epoch 665: val_loss did not improve from 17.03822
196/196 - 83s - loss: 16.3696 - MinusLogProbMetric: 16.3696 - val_loss: 17.0604 - val_MinusLogProbMetric: 17.0604 - lr: 2.0833e-05 - 83s/epoch - 423ms/step
Epoch 666/1000
2023-09-28 14:44:21.000 
Epoch 666/1000 
	 loss: 16.3712, MinusLogProbMetric: 16.3712, val_loss: 17.0522, val_MinusLogProbMetric: 17.0522

Epoch 666: val_loss did not improve from 17.03822
196/196 - 77s - loss: 16.3712 - MinusLogProbMetric: 16.3712 - val_loss: 17.0522 - val_MinusLogProbMetric: 17.0522 - lr: 2.0833e-05 - 77s/epoch - 392ms/step
Epoch 667/1000
2023-09-28 14:45:45.547 
Epoch 667/1000 
	 loss: 16.3688, MinusLogProbMetric: 16.3688, val_loss: 17.0493, val_MinusLogProbMetric: 17.0493

Epoch 667: val_loss did not improve from 17.03822
196/196 - 85s - loss: 16.3688 - MinusLogProbMetric: 16.3688 - val_loss: 17.0493 - val_MinusLogProbMetric: 17.0493 - lr: 2.0833e-05 - 85s/epoch - 431ms/step
Epoch 668/1000
2023-09-28 14:47:10.140 
Epoch 668/1000 
	 loss: 16.3711, MinusLogProbMetric: 16.3711, val_loss: 17.0725, val_MinusLogProbMetric: 17.0725

Epoch 668: val_loss did not improve from 17.03822
196/196 - 85s - loss: 16.3711 - MinusLogProbMetric: 16.3711 - val_loss: 17.0725 - val_MinusLogProbMetric: 17.0725 - lr: 2.0833e-05 - 85s/epoch - 432ms/step
Epoch 669/1000
2023-09-28 14:48:33.189 
Epoch 669/1000 
	 loss: 16.3749, MinusLogProbMetric: 16.3749, val_loss: 17.0571, val_MinusLogProbMetric: 17.0571

Epoch 669: val_loss did not improve from 17.03822
196/196 - 83s - loss: 16.3749 - MinusLogProbMetric: 16.3749 - val_loss: 17.0571 - val_MinusLogProbMetric: 17.0571 - lr: 2.0833e-05 - 83s/epoch - 424ms/step
Epoch 670/1000
2023-09-28 14:49:55.130 
Epoch 670/1000 
	 loss: 16.3724, MinusLogProbMetric: 16.3724, val_loss: 17.0403, val_MinusLogProbMetric: 17.0403

Epoch 670: val_loss did not improve from 17.03822
196/196 - 82s - loss: 16.3724 - MinusLogProbMetric: 16.3724 - val_loss: 17.0403 - val_MinusLogProbMetric: 17.0403 - lr: 2.0833e-05 - 82s/epoch - 418ms/step
Epoch 671/1000
2023-09-28 14:51:22.481 
Epoch 671/1000 
	 loss: 16.3704, MinusLogProbMetric: 16.3704, val_loss: 17.0540, val_MinusLogProbMetric: 17.0540

Epoch 671: val_loss did not improve from 17.03822
196/196 - 87s - loss: 16.3704 - MinusLogProbMetric: 16.3704 - val_loss: 17.0540 - val_MinusLogProbMetric: 17.0540 - lr: 2.0833e-05 - 87s/epoch - 446ms/step
Epoch 672/1000
2023-09-28 14:52:49.504 
Epoch 672/1000 
	 loss: 16.3723, MinusLogProbMetric: 16.3723, val_loss: 17.0603, val_MinusLogProbMetric: 17.0603

Epoch 672: val_loss did not improve from 17.03822
196/196 - 87s - loss: 16.3723 - MinusLogProbMetric: 16.3723 - val_loss: 17.0603 - val_MinusLogProbMetric: 17.0603 - lr: 2.0833e-05 - 87s/epoch - 444ms/step
Epoch 673/1000
2023-09-28 14:54:16.464 
Epoch 673/1000 
	 loss: 16.3756, MinusLogProbMetric: 16.3756, val_loss: 17.0599, val_MinusLogProbMetric: 17.0599

Epoch 673: val_loss did not improve from 17.03822
196/196 - 87s - loss: 16.3756 - MinusLogProbMetric: 16.3756 - val_loss: 17.0599 - val_MinusLogProbMetric: 17.0599 - lr: 2.0833e-05 - 87s/epoch - 444ms/step
Epoch 674/1000
2023-09-28 14:55:42.934 
Epoch 674/1000 
	 loss: 16.3699, MinusLogProbMetric: 16.3699, val_loss: 17.0406, val_MinusLogProbMetric: 17.0406

Epoch 674: val_loss did not improve from 17.03822
196/196 - 86s - loss: 16.3699 - MinusLogProbMetric: 16.3699 - val_loss: 17.0406 - val_MinusLogProbMetric: 17.0406 - lr: 2.0833e-05 - 86s/epoch - 441ms/step
Epoch 675/1000
2023-09-28 14:57:10.689 
Epoch 675/1000 
	 loss: 16.3704, MinusLogProbMetric: 16.3704, val_loss: 17.0538, val_MinusLogProbMetric: 17.0538

Epoch 675: val_loss did not improve from 17.03822
196/196 - 88s - loss: 16.3704 - MinusLogProbMetric: 16.3704 - val_loss: 17.0538 - val_MinusLogProbMetric: 17.0538 - lr: 2.0833e-05 - 88s/epoch - 448ms/step
Epoch 676/1000
2023-09-28 14:58:37.580 
Epoch 676/1000 
	 loss: 16.3749, MinusLogProbMetric: 16.3749, val_loss: 17.0927, val_MinusLogProbMetric: 17.0927

Epoch 676: val_loss did not improve from 17.03822
196/196 - 87s - loss: 16.3749 - MinusLogProbMetric: 16.3749 - val_loss: 17.0927 - val_MinusLogProbMetric: 17.0927 - lr: 2.0833e-05 - 87s/epoch - 443ms/step
Epoch 677/1000
2023-09-28 15:00:04.058 
Epoch 677/1000 
	 loss: 16.3690, MinusLogProbMetric: 16.3690, val_loss: 17.0751, val_MinusLogProbMetric: 17.0751

Epoch 677: val_loss did not improve from 17.03822
196/196 - 86s - loss: 16.3690 - MinusLogProbMetric: 16.3690 - val_loss: 17.0751 - val_MinusLogProbMetric: 17.0751 - lr: 2.0833e-05 - 86s/epoch - 441ms/step
Epoch 678/1000
2023-09-28 15:01:30.310 
Epoch 678/1000 
	 loss: 16.3526, MinusLogProbMetric: 16.3526, val_loss: 17.0346, val_MinusLogProbMetric: 17.0346

Epoch 678: val_loss improved from 17.03822 to 17.03459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 87s - loss: 16.3526 - MinusLogProbMetric: 16.3526 - val_loss: 17.0346 - val_MinusLogProbMetric: 17.0346 - lr: 1.0417e-05 - 87s/epoch - 446ms/step
Epoch 679/1000
2023-09-28 15:02:58.316 
Epoch 679/1000 
	 loss: 16.3499, MinusLogProbMetric: 16.3499, val_loss: 17.0379, val_MinusLogProbMetric: 17.0379

Epoch 679: val_loss did not improve from 17.03459
196/196 - 87s - loss: 16.3499 - MinusLogProbMetric: 16.3499 - val_loss: 17.0379 - val_MinusLogProbMetric: 17.0379 - lr: 1.0417e-05 - 87s/epoch - 443ms/step
Epoch 680/1000
2023-09-28 15:04:25.323 
Epoch 680/1000 
	 loss: 16.3526, MinusLogProbMetric: 16.3526, val_loss: 17.0400, val_MinusLogProbMetric: 17.0400

Epoch 680: val_loss did not improve from 17.03459
196/196 - 87s - loss: 16.3526 - MinusLogProbMetric: 16.3526 - val_loss: 17.0400 - val_MinusLogProbMetric: 17.0400 - lr: 1.0417e-05 - 87s/epoch - 444ms/step
Epoch 681/1000
2023-09-28 15:05:51.675 
Epoch 681/1000 
	 loss: 16.3529, MinusLogProbMetric: 16.3529, val_loss: 17.0346, val_MinusLogProbMetric: 17.0346

Epoch 681: val_loss did not improve from 17.03459
196/196 - 86s - loss: 16.3529 - MinusLogProbMetric: 16.3529 - val_loss: 17.0346 - val_MinusLogProbMetric: 17.0346 - lr: 1.0417e-05 - 86s/epoch - 441ms/step
Epoch 682/1000
2023-09-28 15:07:17.392 
Epoch 682/1000 
	 loss: 16.3510, MinusLogProbMetric: 16.3510, val_loss: 17.0430, val_MinusLogProbMetric: 17.0430

Epoch 682: val_loss did not improve from 17.03459
196/196 - 86s - loss: 16.3510 - MinusLogProbMetric: 16.3510 - val_loss: 17.0430 - val_MinusLogProbMetric: 17.0430 - lr: 1.0417e-05 - 86s/epoch - 437ms/step
Epoch 683/1000
2023-09-28 15:08:44.138 
Epoch 683/1000 
	 loss: 16.3510, MinusLogProbMetric: 16.3510, val_loss: 17.0441, val_MinusLogProbMetric: 17.0441

Epoch 683: val_loss did not improve from 17.03459
196/196 - 87s - loss: 16.3510 - MinusLogProbMetric: 16.3510 - val_loss: 17.0441 - val_MinusLogProbMetric: 17.0441 - lr: 1.0417e-05 - 87s/epoch - 443ms/step
Epoch 684/1000
2023-09-28 15:10:08.948 
Epoch 684/1000 
	 loss: 16.3525, MinusLogProbMetric: 16.3525, val_loss: 17.0348, val_MinusLogProbMetric: 17.0348

Epoch 684: val_loss did not improve from 17.03459
196/196 - 85s - loss: 16.3525 - MinusLogProbMetric: 16.3525 - val_loss: 17.0348 - val_MinusLogProbMetric: 17.0348 - lr: 1.0417e-05 - 85s/epoch - 433ms/step
Epoch 685/1000
2023-09-28 15:11:35.686 
Epoch 685/1000 
	 loss: 16.3509, MinusLogProbMetric: 16.3509, val_loss: 17.0446, val_MinusLogProbMetric: 17.0446

Epoch 685: val_loss did not improve from 17.03459
196/196 - 87s - loss: 16.3509 - MinusLogProbMetric: 16.3509 - val_loss: 17.0446 - val_MinusLogProbMetric: 17.0446 - lr: 1.0417e-05 - 87s/epoch - 442ms/step
Epoch 686/1000
2023-09-28 15:13:02.181 
Epoch 686/1000 
	 loss: 16.3518, MinusLogProbMetric: 16.3518, val_loss: 17.0336, val_MinusLogProbMetric: 17.0336

Epoch 686: val_loss improved from 17.03459 to 17.03359, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 88s - loss: 16.3518 - MinusLogProbMetric: 16.3518 - val_loss: 17.0336 - val_MinusLogProbMetric: 17.0336 - lr: 1.0417e-05 - 88s/epoch - 447ms/step
Epoch 687/1000
2023-09-28 15:14:30.015 
Epoch 687/1000 
	 loss: 16.3505, MinusLogProbMetric: 16.3505, val_loss: 17.0349, val_MinusLogProbMetric: 17.0349

Epoch 687: val_loss did not improve from 17.03359
196/196 - 87s - loss: 16.3505 - MinusLogProbMetric: 16.3505 - val_loss: 17.0349 - val_MinusLogProbMetric: 17.0349 - lr: 1.0417e-05 - 87s/epoch - 443ms/step
Epoch 688/1000
2023-09-28 15:15:57.155 
Epoch 688/1000 
	 loss: 16.3512, MinusLogProbMetric: 16.3512, val_loss: 17.0383, val_MinusLogProbMetric: 17.0383

Epoch 688: val_loss did not improve from 17.03359
196/196 - 87s - loss: 16.3512 - MinusLogProbMetric: 16.3512 - val_loss: 17.0383 - val_MinusLogProbMetric: 17.0383 - lr: 1.0417e-05 - 87s/epoch - 445ms/step
Epoch 689/1000
2023-09-28 15:17:24.331 
Epoch 689/1000 
	 loss: 16.3496, MinusLogProbMetric: 16.3496, val_loss: 17.0361, val_MinusLogProbMetric: 17.0361

Epoch 689: val_loss did not improve from 17.03359
196/196 - 87s - loss: 16.3496 - MinusLogProbMetric: 16.3496 - val_loss: 17.0361 - val_MinusLogProbMetric: 17.0361 - lr: 1.0417e-05 - 87s/epoch - 445ms/step
Epoch 690/1000
2023-09-28 15:18:50.632 
Epoch 690/1000 
	 loss: 16.3501, MinusLogProbMetric: 16.3501, val_loss: 17.0367, val_MinusLogProbMetric: 17.0367

Epoch 690: val_loss did not improve from 17.03359
196/196 - 86s - loss: 16.3501 - MinusLogProbMetric: 16.3501 - val_loss: 17.0367 - val_MinusLogProbMetric: 17.0367 - lr: 1.0417e-05 - 86s/epoch - 440ms/step
Epoch 691/1000
2023-09-28 15:20:17.132 
Epoch 691/1000 
	 loss: 16.3492, MinusLogProbMetric: 16.3492, val_loss: 17.0405, val_MinusLogProbMetric: 17.0405

Epoch 691: val_loss did not improve from 17.03359
196/196 - 86s - loss: 16.3492 - MinusLogProbMetric: 16.3492 - val_loss: 17.0405 - val_MinusLogProbMetric: 17.0405 - lr: 1.0417e-05 - 86s/epoch - 441ms/step
Epoch 692/1000
2023-09-28 15:21:44.861 
Epoch 692/1000 
	 loss: 16.3510, MinusLogProbMetric: 16.3510, val_loss: 17.0403, val_MinusLogProbMetric: 17.0403

Epoch 692: val_loss did not improve from 17.03359
196/196 - 88s - loss: 16.3510 - MinusLogProbMetric: 16.3510 - val_loss: 17.0403 - val_MinusLogProbMetric: 17.0403 - lr: 1.0417e-05 - 88s/epoch - 448ms/step
Epoch 693/1000
2023-09-28 15:23:12.814 
Epoch 693/1000 
	 loss: 16.3496, MinusLogProbMetric: 16.3496, val_loss: 17.0492, val_MinusLogProbMetric: 17.0492

Epoch 693: val_loss did not improve from 17.03359
196/196 - 88s - loss: 16.3496 - MinusLogProbMetric: 16.3496 - val_loss: 17.0492 - val_MinusLogProbMetric: 17.0492 - lr: 1.0417e-05 - 88s/epoch - 449ms/step
Epoch 694/1000
2023-09-28 15:24:40.315 
Epoch 694/1000 
	 loss: 16.3506, MinusLogProbMetric: 16.3506, val_loss: 17.0368, val_MinusLogProbMetric: 17.0368

Epoch 694: val_loss did not improve from 17.03359
196/196 - 87s - loss: 16.3506 - MinusLogProbMetric: 16.3506 - val_loss: 17.0368 - val_MinusLogProbMetric: 17.0368 - lr: 1.0417e-05 - 87s/epoch - 446ms/step
Epoch 695/1000
2023-09-28 15:26:02.223 
Epoch 695/1000 
	 loss: 16.3506, MinusLogProbMetric: 16.3506, val_loss: 17.0420, val_MinusLogProbMetric: 17.0420

Epoch 695: val_loss did not improve from 17.03359
196/196 - 82s - loss: 16.3506 - MinusLogProbMetric: 16.3506 - val_loss: 17.0420 - val_MinusLogProbMetric: 17.0420 - lr: 1.0417e-05 - 82s/epoch - 418ms/step
Epoch 696/1000
2023-09-28 15:27:28.692 
Epoch 696/1000 
	 loss: 16.3498, MinusLogProbMetric: 16.3498, val_loss: 17.0387, val_MinusLogProbMetric: 17.0387

Epoch 696: val_loss did not improve from 17.03359
196/196 - 86s - loss: 16.3498 - MinusLogProbMetric: 16.3498 - val_loss: 17.0387 - val_MinusLogProbMetric: 17.0387 - lr: 1.0417e-05 - 86s/epoch - 441ms/step
Epoch 697/1000
2023-09-28 15:28:51.657 
Epoch 697/1000 
	 loss: 16.3483, MinusLogProbMetric: 16.3483, val_loss: 17.0393, val_MinusLogProbMetric: 17.0393

Epoch 697: val_loss did not improve from 17.03359
196/196 - 83s - loss: 16.3483 - MinusLogProbMetric: 16.3483 - val_loss: 17.0393 - val_MinusLogProbMetric: 17.0393 - lr: 1.0417e-05 - 83s/epoch - 423ms/step
Epoch 698/1000
2023-09-28 15:30:18.186 
Epoch 698/1000 
	 loss: 16.3500, MinusLogProbMetric: 16.3500, val_loss: 17.0392, val_MinusLogProbMetric: 17.0392

Epoch 698: val_loss did not improve from 17.03359
196/196 - 87s - loss: 16.3500 - MinusLogProbMetric: 16.3500 - val_loss: 17.0392 - val_MinusLogProbMetric: 17.0392 - lr: 1.0417e-05 - 87s/epoch - 441ms/step
Epoch 699/1000
2023-09-28 15:31:42.222 
Epoch 699/1000 
	 loss: 16.3500, MinusLogProbMetric: 16.3500, val_loss: 17.0444, val_MinusLogProbMetric: 17.0444

Epoch 699: val_loss did not improve from 17.03359
196/196 - 84s - loss: 16.3500 - MinusLogProbMetric: 16.3500 - val_loss: 17.0444 - val_MinusLogProbMetric: 17.0444 - lr: 1.0417e-05 - 84s/epoch - 429ms/step
Epoch 700/1000
2023-09-28 15:33:07.696 
Epoch 700/1000 
	 loss: 16.3491, MinusLogProbMetric: 16.3491, val_loss: 17.0375, val_MinusLogProbMetric: 17.0375

Epoch 700: val_loss did not improve from 17.03359
196/196 - 85s - loss: 16.3491 - MinusLogProbMetric: 16.3491 - val_loss: 17.0375 - val_MinusLogProbMetric: 17.0375 - lr: 1.0417e-05 - 85s/epoch - 436ms/step
Epoch 701/1000
2023-09-28 15:34:33.132 
Epoch 701/1000 
	 loss: 16.3504, MinusLogProbMetric: 16.3504, val_loss: 17.0379, val_MinusLogProbMetric: 17.0379

Epoch 701: val_loss did not improve from 17.03359
196/196 - 85s - loss: 16.3504 - MinusLogProbMetric: 16.3504 - val_loss: 17.0379 - val_MinusLogProbMetric: 17.0379 - lr: 1.0417e-05 - 85s/epoch - 436ms/step
Epoch 702/1000
2023-09-28 15:35:58.650 
Epoch 702/1000 
	 loss: 16.3496, MinusLogProbMetric: 16.3496, val_loss: 17.0369, val_MinusLogProbMetric: 17.0369

Epoch 702: val_loss did not improve from 17.03359
196/196 - 86s - loss: 16.3496 - MinusLogProbMetric: 16.3496 - val_loss: 17.0369 - val_MinusLogProbMetric: 17.0369 - lr: 1.0417e-05 - 86s/epoch - 436ms/step
Epoch 703/1000
2023-09-28 15:37:23.809 
Epoch 703/1000 
	 loss: 16.3513, MinusLogProbMetric: 16.3513, val_loss: 17.0371, val_MinusLogProbMetric: 17.0371

Epoch 703: val_loss did not improve from 17.03359
196/196 - 85s - loss: 16.3513 - MinusLogProbMetric: 16.3513 - val_loss: 17.0371 - val_MinusLogProbMetric: 17.0371 - lr: 1.0417e-05 - 85s/epoch - 434ms/step
Epoch 704/1000
2023-09-28 15:38:48.177 
Epoch 704/1000 
	 loss: 16.3472, MinusLogProbMetric: 16.3472, val_loss: 17.0348, val_MinusLogProbMetric: 17.0348

Epoch 704: val_loss did not improve from 17.03359
196/196 - 84s - loss: 16.3472 - MinusLogProbMetric: 16.3472 - val_loss: 17.0348 - val_MinusLogProbMetric: 17.0348 - lr: 1.0417e-05 - 84s/epoch - 430ms/step
Epoch 705/1000
2023-09-28 15:40:10.196 
Epoch 705/1000 
	 loss: 16.3489, MinusLogProbMetric: 16.3489, val_loss: 17.0418, val_MinusLogProbMetric: 17.0418

Epoch 705: val_loss did not improve from 17.03359
196/196 - 82s - loss: 16.3489 - MinusLogProbMetric: 16.3489 - val_loss: 17.0418 - val_MinusLogProbMetric: 17.0418 - lr: 1.0417e-05 - 82s/epoch - 418ms/step
Epoch 706/1000
2023-09-28 15:41:33.958 
Epoch 706/1000 
	 loss: 16.3492, MinusLogProbMetric: 16.3492, val_loss: 17.0366, val_MinusLogProbMetric: 17.0366

Epoch 706: val_loss did not improve from 17.03359
196/196 - 84s - loss: 16.3492 - MinusLogProbMetric: 16.3492 - val_loss: 17.0366 - val_MinusLogProbMetric: 17.0366 - lr: 1.0417e-05 - 84s/epoch - 427ms/step
Epoch 707/1000
2023-09-28 15:42:59.656 
Epoch 707/1000 
	 loss: 16.3482, MinusLogProbMetric: 16.3482, val_loss: 17.0308, val_MinusLogProbMetric: 17.0308

Epoch 707: val_loss improved from 17.03359 to 17.03082, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 87s - loss: 16.3482 - MinusLogProbMetric: 16.3482 - val_loss: 17.0308 - val_MinusLogProbMetric: 17.0308 - lr: 1.0417e-05 - 87s/epoch - 443ms/step
Epoch 708/1000
2023-09-28 15:44:25.735 
Epoch 708/1000 
	 loss: 16.3509, MinusLogProbMetric: 16.3509, val_loss: 17.0401, val_MinusLogProbMetric: 17.0401

Epoch 708: val_loss did not improve from 17.03082
196/196 - 85s - loss: 16.3509 - MinusLogProbMetric: 16.3509 - val_loss: 17.0401 - val_MinusLogProbMetric: 17.0401 - lr: 1.0417e-05 - 85s/epoch - 433ms/step
Epoch 709/1000
2023-09-28 15:45:51.890 
Epoch 709/1000 
	 loss: 16.3487, MinusLogProbMetric: 16.3487, val_loss: 17.0436, val_MinusLogProbMetric: 17.0436

Epoch 709: val_loss did not improve from 17.03082
196/196 - 86s - loss: 16.3487 - MinusLogProbMetric: 16.3487 - val_loss: 17.0436 - val_MinusLogProbMetric: 17.0436 - lr: 1.0417e-05 - 86s/epoch - 440ms/step
Epoch 710/1000
2023-09-28 15:47:19.142 
Epoch 710/1000 
	 loss: 16.3500, MinusLogProbMetric: 16.3500, val_loss: 17.0418, val_MinusLogProbMetric: 17.0418

Epoch 710: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3500 - MinusLogProbMetric: 16.3500 - val_loss: 17.0418 - val_MinusLogProbMetric: 17.0418 - lr: 1.0417e-05 - 87s/epoch - 445ms/step
Epoch 711/1000
2023-09-28 15:48:46.749 
Epoch 711/1000 
	 loss: 16.3504, MinusLogProbMetric: 16.3504, val_loss: 17.0367, val_MinusLogProbMetric: 17.0367

Epoch 711: val_loss did not improve from 17.03082
196/196 - 88s - loss: 16.3504 - MinusLogProbMetric: 16.3504 - val_loss: 17.0367 - val_MinusLogProbMetric: 17.0367 - lr: 1.0417e-05 - 88s/epoch - 447ms/step
Epoch 712/1000
2023-09-28 15:50:13.120 
Epoch 712/1000 
	 loss: 16.3470, MinusLogProbMetric: 16.3470, val_loss: 17.0423, val_MinusLogProbMetric: 17.0423

Epoch 712: val_loss did not improve from 17.03082
196/196 - 86s - loss: 16.3470 - MinusLogProbMetric: 16.3470 - val_loss: 17.0423 - val_MinusLogProbMetric: 17.0423 - lr: 1.0417e-05 - 86s/epoch - 441ms/step
Epoch 713/1000
2023-09-28 15:51:37.293 
Epoch 713/1000 
	 loss: 16.3494, MinusLogProbMetric: 16.3494, val_loss: 17.0361, val_MinusLogProbMetric: 17.0361

Epoch 713: val_loss did not improve from 17.03082
196/196 - 84s - loss: 16.3494 - MinusLogProbMetric: 16.3494 - val_loss: 17.0361 - val_MinusLogProbMetric: 17.0361 - lr: 1.0417e-05 - 84s/epoch - 429ms/step
Epoch 714/1000
2023-09-28 15:53:04.562 
Epoch 714/1000 
	 loss: 16.3472, MinusLogProbMetric: 16.3472, val_loss: 17.0380, val_MinusLogProbMetric: 17.0380

Epoch 714: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3472 - MinusLogProbMetric: 16.3472 - val_loss: 17.0380 - val_MinusLogProbMetric: 17.0380 - lr: 1.0417e-05 - 87s/epoch - 445ms/step
Epoch 715/1000
2023-09-28 15:54:31.276 
Epoch 715/1000 
	 loss: 16.3457, MinusLogProbMetric: 16.3457, val_loss: 17.0389, val_MinusLogProbMetric: 17.0389

Epoch 715: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3457 - MinusLogProbMetric: 16.3457 - val_loss: 17.0389 - val_MinusLogProbMetric: 17.0389 - lr: 1.0417e-05 - 87s/epoch - 442ms/step
Epoch 716/1000
2023-09-28 15:55:58.039 
Epoch 716/1000 
	 loss: 16.3475, MinusLogProbMetric: 16.3475, val_loss: 17.0376, val_MinusLogProbMetric: 17.0376

Epoch 716: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3475 - MinusLogProbMetric: 16.3475 - val_loss: 17.0376 - val_MinusLogProbMetric: 17.0376 - lr: 1.0417e-05 - 87s/epoch - 443ms/step
Epoch 717/1000
2023-09-28 15:57:22.508 
Epoch 717/1000 
	 loss: 16.3480, MinusLogProbMetric: 16.3480, val_loss: 17.0359, val_MinusLogProbMetric: 17.0359

Epoch 717: val_loss did not improve from 17.03082
196/196 - 84s - loss: 16.3480 - MinusLogProbMetric: 16.3480 - val_loss: 17.0359 - val_MinusLogProbMetric: 17.0359 - lr: 1.0417e-05 - 84s/epoch - 431ms/step
Epoch 718/1000
2023-09-28 15:58:47.981 
Epoch 718/1000 
	 loss: 16.3485, MinusLogProbMetric: 16.3485, val_loss: 17.0425, val_MinusLogProbMetric: 17.0425

Epoch 718: val_loss did not improve from 17.03082
196/196 - 85s - loss: 16.3485 - MinusLogProbMetric: 16.3485 - val_loss: 17.0425 - val_MinusLogProbMetric: 17.0425 - lr: 1.0417e-05 - 85s/epoch - 436ms/step
Epoch 719/1000
2023-09-28 16:00:13.282 
Epoch 719/1000 
	 loss: 16.3493, MinusLogProbMetric: 16.3493, val_loss: 17.0421, val_MinusLogProbMetric: 17.0421

Epoch 719: val_loss did not improve from 17.03082
196/196 - 85s - loss: 16.3493 - MinusLogProbMetric: 16.3493 - val_loss: 17.0421 - val_MinusLogProbMetric: 17.0421 - lr: 1.0417e-05 - 85s/epoch - 435ms/step
Epoch 720/1000
2023-09-28 16:01:39.011 
Epoch 720/1000 
	 loss: 16.3464, MinusLogProbMetric: 16.3464, val_loss: 17.0382, val_MinusLogProbMetric: 17.0382

Epoch 720: val_loss did not improve from 17.03082
196/196 - 86s - loss: 16.3464 - MinusLogProbMetric: 16.3464 - val_loss: 17.0382 - val_MinusLogProbMetric: 17.0382 - lr: 1.0417e-05 - 86s/epoch - 437ms/step
Epoch 721/1000
2023-09-28 16:03:04.873 
Epoch 721/1000 
	 loss: 16.3496, MinusLogProbMetric: 16.3496, val_loss: 17.0372, val_MinusLogProbMetric: 17.0372

Epoch 721: val_loss did not improve from 17.03082
196/196 - 86s - loss: 16.3496 - MinusLogProbMetric: 16.3496 - val_loss: 17.0372 - val_MinusLogProbMetric: 17.0372 - lr: 1.0417e-05 - 86s/epoch - 438ms/step
Epoch 722/1000
2023-09-28 16:04:32.717 
Epoch 722/1000 
	 loss: 16.3485, MinusLogProbMetric: 16.3485, val_loss: 17.0351, val_MinusLogProbMetric: 17.0351

Epoch 722: val_loss did not improve from 17.03082
196/196 - 88s - loss: 16.3485 - MinusLogProbMetric: 16.3485 - val_loss: 17.0351 - val_MinusLogProbMetric: 17.0351 - lr: 1.0417e-05 - 88s/epoch - 448ms/step
Epoch 723/1000
2023-09-28 16:05:58.035 
Epoch 723/1000 
	 loss: 16.3486, MinusLogProbMetric: 16.3486, val_loss: 17.0446, val_MinusLogProbMetric: 17.0446

Epoch 723: val_loss did not improve from 17.03082
196/196 - 85s - loss: 16.3486 - MinusLogProbMetric: 16.3486 - val_loss: 17.0446 - val_MinusLogProbMetric: 17.0446 - lr: 1.0417e-05 - 85s/epoch - 435ms/step
Epoch 724/1000
2023-09-28 16:07:24.961 
Epoch 724/1000 
	 loss: 16.3472, MinusLogProbMetric: 16.3472, val_loss: 17.0458, val_MinusLogProbMetric: 17.0458

Epoch 724: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3472 - MinusLogProbMetric: 16.3472 - val_loss: 17.0458 - val_MinusLogProbMetric: 17.0458 - lr: 1.0417e-05 - 87s/epoch - 443ms/step
Epoch 725/1000
2023-09-28 16:08:51.026 
Epoch 725/1000 
	 loss: 16.3476, MinusLogProbMetric: 16.3476, val_loss: 17.0444, val_MinusLogProbMetric: 17.0444

Epoch 725: val_loss did not improve from 17.03082
196/196 - 86s - loss: 16.3476 - MinusLogProbMetric: 16.3476 - val_loss: 17.0444 - val_MinusLogProbMetric: 17.0444 - lr: 1.0417e-05 - 86s/epoch - 439ms/step
Epoch 726/1000
2023-09-28 16:10:17.798 
Epoch 726/1000 
	 loss: 16.3507, MinusLogProbMetric: 16.3507, val_loss: 17.0442, val_MinusLogProbMetric: 17.0442

Epoch 726: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3507 - MinusLogProbMetric: 16.3507 - val_loss: 17.0442 - val_MinusLogProbMetric: 17.0442 - lr: 1.0417e-05 - 87s/epoch - 443ms/step
Epoch 727/1000
2023-09-28 16:11:44.100 
Epoch 727/1000 
	 loss: 16.3485, MinusLogProbMetric: 16.3485, val_loss: 17.0481, val_MinusLogProbMetric: 17.0481

Epoch 727: val_loss did not improve from 17.03082
196/196 - 86s - loss: 16.3485 - MinusLogProbMetric: 16.3485 - val_loss: 17.0481 - val_MinusLogProbMetric: 17.0481 - lr: 1.0417e-05 - 86s/epoch - 440ms/step
Epoch 728/1000
2023-09-28 16:13:11.496 
Epoch 728/1000 
	 loss: 16.3475, MinusLogProbMetric: 16.3475, val_loss: 17.0402, val_MinusLogProbMetric: 17.0402

Epoch 728: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3475 - MinusLogProbMetric: 16.3475 - val_loss: 17.0402 - val_MinusLogProbMetric: 17.0402 - lr: 1.0417e-05 - 87s/epoch - 446ms/step
Epoch 729/1000
2023-09-28 16:14:35.102 
Epoch 729/1000 
	 loss: 16.3484, MinusLogProbMetric: 16.3484, val_loss: 17.0552, val_MinusLogProbMetric: 17.0552

Epoch 729: val_loss did not improve from 17.03082
196/196 - 84s - loss: 16.3484 - MinusLogProbMetric: 16.3484 - val_loss: 17.0552 - val_MinusLogProbMetric: 17.0552 - lr: 1.0417e-05 - 84s/epoch - 427ms/step
Epoch 730/1000
2023-09-28 16:16:03.102 
Epoch 730/1000 
	 loss: 16.3489, MinusLogProbMetric: 16.3489, val_loss: 17.0384, val_MinusLogProbMetric: 17.0384

Epoch 730: val_loss did not improve from 17.03082
196/196 - 88s - loss: 16.3489 - MinusLogProbMetric: 16.3489 - val_loss: 17.0384 - val_MinusLogProbMetric: 17.0384 - lr: 1.0417e-05 - 88s/epoch - 449ms/step
Epoch 731/1000
2023-09-28 16:17:29.878 
Epoch 731/1000 
	 loss: 16.3473, MinusLogProbMetric: 16.3473, val_loss: 17.0406, val_MinusLogProbMetric: 17.0406

Epoch 731: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3473 - MinusLogProbMetric: 16.3473 - val_loss: 17.0406 - val_MinusLogProbMetric: 17.0406 - lr: 1.0417e-05 - 87s/epoch - 443ms/step
Epoch 732/1000
2023-09-28 16:18:57.343 
Epoch 732/1000 
	 loss: 16.3466, MinusLogProbMetric: 16.3466, val_loss: 17.0574, val_MinusLogProbMetric: 17.0574

Epoch 732: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3466 - MinusLogProbMetric: 16.3466 - val_loss: 17.0574 - val_MinusLogProbMetric: 17.0574 - lr: 1.0417e-05 - 87s/epoch - 446ms/step
Epoch 733/1000
2023-09-28 16:20:24.683 
Epoch 733/1000 
	 loss: 16.3480, MinusLogProbMetric: 16.3480, val_loss: 17.0365, val_MinusLogProbMetric: 17.0365

Epoch 733: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3480 - MinusLogProbMetric: 16.3480 - val_loss: 17.0365 - val_MinusLogProbMetric: 17.0365 - lr: 1.0417e-05 - 87s/epoch - 446ms/step
Epoch 734/1000
2023-09-28 16:21:52.000 
Epoch 734/1000 
	 loss: 16.3485, MinusLogProbMetric: 16.3485, val_loss: 17.0365, val_MinusLogProbMetric: 17.0365

Epoch 734: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3485 - MinusLogProbMetric: 16.3485 - val_loss: 17.0365 - val_MinusLogProbMetric: 17.0365 - lr: 1.0417e-05 - 87s/epoch - 445ms/step
Epoch 735/1000
2023-09-28 16:23:18.803 
Epoch 735/1000 
	 loss: 16.3473, MinusLogProbMetric: 16.3473, val_loss: 17.0340, val_MinusLogProbMetric: 17.0340

Epoch 735: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3473 - MinusLogProbMetric: 16.3473 - val_loss: 17.0340 - val_MinusLogProbMetric: 17.0340 - lr: 1.0417e-05 - 87s/epoch - 443ms/step
Epoch 736/1000
2023-09-28 16:24:45.345 
Epoch 736/1000 
	 loss: 16.3475, MinusLogProbMetric: 16.3475, val_loss: 17.0376, val_MinusLogProbMetric: 17.0376

Epoch 736: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3475 - MinusLogProbMetric: 16.3475 - val_loss: 17.0376 - val_MinusLogProbMetric: 17.0376 - lr: 1.0417e-05 - 87s/epoch - 441ms/step
Epoch 737/1000
2023-09-28 16:26:12.690 
Epoch 737/1000 
	 loss: 16.3477, MinusLogProbMetric: 16.3477, val_loss: 17.0415, val_MinusLogProbMetric: 17.0415

Epoch 737: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3477 - MinusLogProbMetric: 16.3477 - val_loss: 17.0415 - val_MinusLogProbMetric: 17.0415 - lr: 1.0417e-05 - 87s/epoch - 446ms/step
Epoch 738/1000
2023-09-28 16:27:39.666 
Epoch 738/1000 
	 loss: 16.3487, MinusLogProbMetric: 16.3487, val_loss: 17.0362, val_MinusLogProbMetric: 17.0362

Epoch 738: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3487 - MinusLogProbMetric: 16.3487 - val_loss: 17.0362 - val_MinusLogProbMetric: 17.0362 - lr: 1.0417e-05 - 87s/epoch - 444ms/step
Epoch 739/1000
2023-09-28 16:29:05.762 
Epoch 739/1000 
	 loss: 16.3471, MinusLogProbMetric: 16.3471, val_loss: 17.0352, val_MinusLogProbMetric: 17.0352

Epoch 739: val_loss did not improve from 17.03082
196/196 - 86s - loss: 16.3471 - MinusLogProbMetric: 16.3471 - val_loss: 17.0352 - val_MinusLogProbMetric: 17.0352 - lr: 1.0417e-05 - 86s/epoch - 439ms/step
Epoch 740/1000
2023-09-28 16:30:34.458 
Epoch 740/1000 
	 loss: 16.3478, MinusLogProbMetric: 16.3478, val_loss: 17.0537, val_MinusLogProbMetric: 17.0537

Epoch 740: val_loss did not improve from 17.03082
196/196 - 89s - loss: 16.3478 - MinusLogProbMetric: 16.3478 - val_loss: 17.0537 - val_MinusLogProbMetric: 17.0537 - lr: 1.0417e-05 - 89s/epoch - 452ms/step
Epoch 741/1000
2023-09-28 16:31:59.131 
Epoch 741/1000 
	 loss: 16.3471, MinusLogProbMetric: 16.3471, val_loss: 17.0383, val_MinusLogProbMetric: 17.0383

Epoch 741: val_loss did not improve from 17.03082
196/196 - 85s - loss: 16.3471 - MinusLogProbMetric: 16.3471 - val_loss: 17.0383 - val_MinusLogProbMetric: 17.0383 - lr: 1.0417e-05 - 85s/epoch - 432ms/step
Epoch 742/1000
2023-09-28 16:33:26.602 
Epoch 742/1000 
	 loss: 16.3468, MinusLogProbMetric: 16.3468, val_loss: 17.0370, val_MinusLogProbMetric: 17.0370

Epoch 742: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3468 - MinusLogProbMetric: 16.3468 - val_loss: 17.0370 - val_MinusLogProbMetric: 17.0370 - lr: 1.0417e-05 - 87s/epoch - 446ms/step
Epoch 743/1000
2023-09-28 16:34:56.408 
Epoch 743/1000 
	 loss: 16.3454, MinusLogProbMetric: 16.3454, val_loss: 17.0350, val_MinusLogProbMetric: 17.0350

Epoch 743: val_loss did not improve from 17.03082
196/196 - 90s - loss: 16.3454 - MinusLogProbMetric: 16.3454 - val_loss: 17.0350 - val_MinusLogProbMetric: 17.0350 - lr: 1.0417e-05 - 90s/epoch - 458ms/step
Epoch 744/1000
2023-09-28 16:36:38.005 
Epoch 744/1000 
	 loss: 16.3473, MinusLogProbMetric: 16.3473, val_loss: 17.0504, val_MinusLogProbMetric: 17.0504

Epoch 744: val_loss did not improve from 17.03082
196/196 - 102s - loss: 16.3473 - MinusLogProbMetric: 16.3473 - val_loss: 17.0504 - val_MinusLogProbMetric: 17.0504 - lr: 1.0417e-05 - 102s/epoch - 518ms/step
Epoch 745/1000
2023-09-28 16:37:57.017 
Epoch 745/1000 
	 loss: 16.3477, MinusLogProbMetric: 16.3477, val_loss: 17.0439, val_MinusLogProbMetric: 17.0439

Epoch 745: val_loss did not improve from 17.03082
196/196 - 79s - loss: 16.3477 - MinusLogProbMetric: 16.3477 - val_loss: 17.0439 - val_MinusLogProbMetric: 17.0439 - lr: 1.0417e-05 - 79s/epoch - 403ms/step
Epoch 746/1000
2023-09-28 16:39:22.801 
Epoch 746/1000 
	 loss: 16.3475, MinusLogProbMetric: 16.3475, val_loss: 17.0406, val_MinusLogProbMetric: 17.0406

Epoch 746: val_loss did not improve from 17.03082
196/196 - 86s - loss: 16.3475 - MinusLogProbMetric: 16.3475 - val_loss: 17.0406 - val_MinusLogProbMetric: 17.0406 - lr: 1.0417e-05 - 86s/epoch - 438ms/step
Epoch 747/1000
2023-09-28 16:40:49.068 
Epoch 747/1000 
	 loss: 16.3455, MinusLogProbMetric: 16.3455, val_loss: 17.0387, val_MinusLogProbMetric: 17.0387

Epoch 747: val_loss did not improve from 17.03082
196/196 - 86s - loss: 16.3455 - MinusLogProbMetric: 16.3455 - val_loss: 17.0387 - val_MinusLogProbMetric: 17.0387 - lr: 1.0417e-05 - 86s/epoch - 440ms/step
Epoch 748/1000
2023-09-28 16:42:08.067 
Epoch 748/1000 
	 loss: 16.3466, MinusLogProbMetric: 16.3466, val_loss: 17.0402, val_MinusLogProbMetric: 17.0402

Epoch 748: val_loss did not improve from 17.03082
196/196 - 79s - loss: 16.3466 - MinusLogProbMetric: 16.3466 - val_loss: 17.0402 - val_MinusLogProbMetric: 17.0402 - lr: 1.0417e-05 - 79s/epoch - 403ms/step
Epoch 749/1000
2023-09-28 16:43:28.377 
Epoch 749/1000 
	 loss: 16.3468, MinusLogProbMetric: 16.3468, val_loss: 17.0333, val_MinusLogProbMetric: 17.0333

Epoch 749: val_loss did not improve from 17.03082
196/196 - 80s - loss: 16.3468 - MinusLogProbMetric: 16.3468 - val_loss: 17.0333 - val_MinusLogProbMetric: 17.0333 - lr: 1.0417e-05 - 80s/epoch - 410ms/step
Epoch 750/1000
2023-09-28 16:44:56.249 
Epoch 750/1000 
	 loss: 16.3458, MinusLogProbMetric: 16.3458, val_loss: 17.0394, val_MinusLogProbMetric: 17.0394

Epoch 750: val_loss did not improve from 17.03082
196/196 - 88s - loss: 16.3458 - MinusLogProbMetric: 16.3458 - val_loss: 17.0394 - val_MinusLogProbMetric: 17.0394 - lr: 1.0417e-05 - 88s/epoch - 448ms/step
Epoch 751/1000
2023-09-28 16:46:22.127 
Epoch 751/1000 
	 loss: 16.3462, MinusLogProbMetric: 16.3462, val_loss: 17.0335, val_MinusLogProbMetric: 17.0335

Epoch 751: val_loss did not improve from 17.03082
196/196 - 86s - loss: 16.3462 - MinusLogProbMetric: 16.3462 - val_loss: 17.0335 - val_MinusLogProbMetric: 17.0335 - lr: 1.0417e-05 - 86s/epoch - 438ms/step
Epoch 752/1000
2023-09-28 16:47:45.621 
Epoch 752/1000 
	 loss: 16.3499, MinusLogProbMetric: 16.3499, val_loss: 17.0345, val_MinusLogProbMetric: 17.0345

Epoch 752: val_loss did not improve from 17.03082
196/196 - 84s - loss: 16.3499 - MinusLogProbMetric: 16.3499 - val_loss: 17.0345 - val_MinusLogProbMetric: 17.0345 - lr: 1.0417e-05 - 84s/epoch - 426ms/step
Epoch 753/1000
2023-09-28 16:49:10.886 
Epoch 753/1000 
	 loss: 16.3469, MinusLogProbMetric: 16.3469, val_loss: 17.0389, val_MinusLogProbMetric: 17.0389

Epoch 753: val_loss did not improve from 17.03082
196/196 - 85s - loss: 16.3469 - MinusLogProbMetric: 16.3469 - val_loss: 17.0389 - val_MinusLogProbMetric: 17.0389 - lr: 1.0417e-05 - 85s/epoch - 435ms/step
Epoch 754/1000
2023-09-28 16:50:37.717 
Epoch 754/1000 
	 loss: 16.3452, MinusLogProbMetric: 16.3452, val_loss: 17.0428, val_MinusLogProbMetric: 17.0428

Epoch 754: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3452 - MinusLogProbMetric: 16.3452 - val_loss: 17.0428 - val_MinusLogProbMetric: 17.0428 - lr: 1.0417e-05 - 87s/epoch - 443ms/step
Epoch 755/1000
2023-09-28 16:52:04.826 
Epoch 755/1000 
	 loss: 16.3447, MinusLogProbMetric: 16.3447, val_loss: 17.0463, val_MinusLogProbMetric: 17.0463

Epoch 755: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3447 - MinusLogProbMetric: 16.3447 - val_loss: 17.0463 - val_MinusLogProbMetric: 17.0463 - lr: 1.0417e-05 - 87s/epoch - 444ms/step
Epoch 756/1000
2023-09-28 16:53:30.898 
Epoch 756/1000 
	 loss: 16.3464, MinusLogProbMetric: 16.3464, val_loss: 17.0394, val_MinusLogProbMetric: 17.0394

Epoch 756: val_loss did not improve from 17.03082
196/196 - 86s - loss: 16.3464 - MinusLogProbMetric: 16.3464 - val_loss: 17.0394 - val_MinusLogProbMetric: 17.0394 - lr: 1.0417e-05 - 86s/epoch - 439ms/step
Epoch 757/1000
2023-09-28 16:54:57.752 
Epoch 757/1000 
	 loss: 16.3429, MinusLogProbMetric: 16.3429, val_loss: 17.0384, val_MinusLogProbMetric: 17.0384

Epoch 757: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3429 - MinusLogProbMetric: 16.3429 - val_loss: 17.0384 - val_MinusLogProbMetric: 17.0384 - lr: 1.0417e-05 - 87s/epoch - 443ms/step
Epoch 758/1000
2023-09-28 16:56:24.337 
Epoch 758/1000 
	 loss: 16.3369, MinusLogProbMetric: 16.3369, val_loss: 17.0327, val_MinusLogProbMetric: 17.0327

Epoch 758: val_loss did not improve from 17.03082
196/196 - 87s - loss: 16.3369 - MinusLogProbMetric: 16.3369 - val_loss: 17.0327 - val_MinusLogProbMetric: 17.0327 - lr: 5.2083e-06 - 87s/epoch - 442ms/step
Epoch 759/1000
2023-09-28 16:57:50.808 
Epoch 759/1000 
	 loss: 16.3372, MinusLogProbMetric: 16.3372, val_loss: 17.0296, val_MinusLogProbMetric: 17.0296

Epoch 759: val_loss improved from 17.03082 to 17.02957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 87s - loss: 16.3372 - MinusLogProbMetric: 16.3372 - val_loss: 17.0296 - val_MinusLogProbMetric: 17.0296 - lr: 5.2083e-06 - 87s/epoch - 445ms/step
Epoch 760/1000
2023-09-28 16:59:16.897 
Epoch 760/1000 
	 loss: 16.3374, MinusLogProbMetric: 16.3374, val_loss: 17.0317, val_MinusLogProbMetric: 17.0317

Epoch 760: val_loss did not improve from 17.02957
196/196 - 85s - loss: 16.3374 - MinusLogProbMetric: 16.3374 - val_loss: 17.0317 - val_MinusLogProbMetric: 17.0317 - lr: 5.2083e-06 - 85s/epoch - 435ms/step
Epoch 761/1000
2023-09-28 17:00:40.750 
Epoch 761/1000 
	 loss: 16.3371, MinusLogProbMetric: 16.3371, val_loss: 17.0416, val_MinusLogProbMetric: 17.0416

Epoch 761: val_loss did not improve from 17.02957
196/196 - 84s - loss: 16.3371 - MinusLogProbMetric: 16.3371 - val_loss: 17.0416 - val_MinusLogProbMetric: 17.0416 - lr: 5.2083e-06 - 84s/epoch - 428ms/step
Epoch 762/1000
2023-09-28 17:02:06.392 
Epoch 762/1000 
	 loss: 16.3377, MinusLogProbMetric: 16.3377, val_loss: 17.0301, val_MinusLogProbMetric: 17.0301

Epoch 762: val_loss did not improve from 17.02957
196/196 - 86s - loss: 16.3377 - MinusLogProbMetric: 16.3377 - val_loss: 17.0301 - val_MinusLogProbMetric: 17.0301 - lr: 5.2083e-06 - 86s/epoch - 437ms/step
Epoch 763/1000
2023-09-28 17:03:30.868 
Epoch 763/1000 
	 loss: 16.3368, MinusLogProbMetric: 16.3368, val_loss: 17.0336, val_MinusLogProbMetric: 17.0336

Epoch 763: val_loss did not improve from 17.02957
196/196 - 84s - loss: 16.3368 - MinusLogProbMetric: 16.3368 - val_loss: 17.0336 - val_MinusLogProbMetric: 17.0336 - lr: 5.2083e-06 - 84s/epoch - 431ms/step
Epoch 764/1000
2023-09-28 17:04:56.039 
Epoch 764/1000 
	 loss: 16.3356, MinusLogProbMetric: 16.3356, val_loss: 17.0329, val_MinusLogProbMetric: 17.0329

Epoch 764: val_loss did not improve from 17.02957
196/196 - 85s - loss: 16.3356 - MinusLogProbMetric: 16.3356 - val_loss: 17.0329 - val_MinusLogProbMetric: 17.0329 - lr: 5.2083e-06 - 85s/epoch - 434ms/step
Epoch 765/1000
2023-09-28 17:06:21.431 
Epoch 765/1000 
	 loss: 16.3364, MinusLogProbMetric: 16.3364, val_loss: 17.0311, val_MinusLogProbMetric: 17.0311

Epoch 765: val_loss did not improve from 17.02957
196/196 - 85s - loss: 16.3364 - MinusLogProbMetric: 16.3364 - val_loss: 17.0311 - val_MinusLogProbMetric: 17.0311 - lr: 5.2083e-06 - 85s/epoch - 436ms/step
Epoch 766/1000
2023-09-28 17:07:46.524 
Epoch 766/1000 
	 loss: 16.3358, MinusLogProbMetric: 16.3358, val_loss: 17.0330, val_MinusLogProbMetric: 17.0330

Epoch 766: val_loss did not improve from 17.02957
196/196 - 85s - loss: 16.3358 - MinusLogProbMetric: 16.3358 - val_loss: 17.0330 - val_MinusLogProbMetric: 17.0330 - lr: 5.2083e-06 - 85s/epoch - 434ms/step
Epoch 767/1000
2023-09-28 17:09:12.504 
Epoch 767/1000 
	 loss: 16.3373, MinusLogProbMetric: 16.3373, val_loss: 17.0356, val_MinusLogProbMetric: 17.0356

Epoch 767: val_loss did not improve from 17.02957
196/196 - 86s - loss: 16.3373 - MinusLogProbMetric: 16.3373 - val_loss: 17.0356 - val_MinusLogProbMetric: 17.0356 - lr: 5.2083e-06 - 86s/epoch - 439ms/step
Epoch 768/1000
2023-09-28 17:10:38.360 
Epoch 768/1000 
	 loss: 16.3361, MinusLogProbMetric: 16.3361, val_loss: 17.0324, val_MinusLogProbMetric: 17.0324

Epoch 768: val_loss did not improve from 17.02957
196/196 - 86s - loss: 16.3361 - MinusLogProbMetric: 16.3361 - val_loss: 17.0324 - val_MinusLogProbMetric: 17.0324 - lr: 5.2083e-06 - 86s/epoch - 438ms/step
Epoch 769/1000
2023-09-28 17:12:04.611 
Epoch 769/1000 
	 loss: 16.3362, MinusLogProbMetric: 16.3362, val_loss: 17.0335, val_MinusLogProbMetric: 17.0335

Epoch 769: val_loss did not improve from 17.02957
196/196 - 86s - loss: 16.3362 - MinusLogProbMetric: 16.3362 - val_loss: 17.0335 - val_MinusLogProbMetric: 17.0335 - lr: 5.2083e-06 - 86s/epoch - 440ms/step
Epoch 770/1000
2023-09-28 17:13:33.068 
Epoch 770/1000 
	 loss: 16.3370, MinusLogProbMetric: 16.3370, val_loss: 17.0387, val_MinusLogProbMetric: 17.0387

Epoch 770: val_loss did not improve from 17.02957
196/196 - 88s - loss: 16.3370 - MinusLogProbMetric: 16.3370 - val_loss: 17.0387 - val_MinusLogProbMetric: 17.0387 - lr: 5.2083e-06 - 88s/epoch - 451ms/step
Epoch 771/1000
2023-09-28 17:15:00.368 
Epoch 771/1000 
	 loss: 16.3362, MinusLogProbMetric: 16.3362, val_loss: 17.0326, val_MinusLogProbMetric: 17.0326

Epoch 771: val_loss did not improve from 17.02957
196/196 - 87s - loss: 16.3362 - MinusLogProbMetric: 16.3362 - val_loss: 17.0326 - val_MinusLogProbMetric: 17.0326 - lr: 5.2083e-06 - 87s/epoch - 445ms/step
Epoch 772/1000
2023-09-28 17:16:24.175 
Epoch 772/1000 
	 loss: 16.3358, MinusLogProbMetric: 16.3358, val_loss: 17.0360, val_MinusLogProbMetric: 17.0360

Epoch 772: val_loss did not improve from 17.02957
196/196 - 84s - loss: 16.3358 - MinusLogProbMetric: 16.3358 - val_loss: 17.0360 - val_MinusLogProbMetric: 17.0360 - lr: 5.2083e-06 - 84s/epoch - 428ms/step
Epoch 773/1000
2023-09-28 17:17:47.863 
Epoch 773/1000 
	 loss: 16.3363, MinusLogProbMetric: 16.3363, val_loss: 17.0319, val_MinusLogProbMetric: 17.0319

Epoch 773: val_loss did not improve from 17.02957
196/196 - 84s - loss: 16.3363 - MinusLogProbMetric: 16.3363 - val_loss: 17.0319 - val_MinusLogProbMetric: 17.0319 - lr: 5.2083e-06 - 84s/epoch - 427ms/step
Epoch 774/1000
2023-09-28 17:19:12.824 
Epoch 774/1000 
	 loss: 16.3365, MinusLogProbMetric: 16.3365, val_loss: 17.0314, val_MinusLogProbMetric: 17.0314

Epoch 774: val_loss did not improve from 17.02957
196/196 - 85s - loss: 16.3365 - MinusLogProbMetric: 16.3365 - val_loss: 17.0314 - val_MinusLogProbMetric: 17.0314 - lr: 5.2083e-06 - 85s/epoch - 433ms/step
Epoch 775/1000
2023-09-28 17:20:35.474 
Epoch 775/1000 
	 loss: 16.3363, MinusLogProbMetric: 16.3363, val_loss: 17.0351, val_MinusLogProbMetric: 17.0351

Epoch 775: val_loss did not improve from 17.02957
196/196 - 83s - loss: 16.3363 - MinusLogProbMetric: 16.3363 - val_loss: 17.0351 - val_MinusLogProbMetric: 17.0351 - lr: 5.2083e-06 - 83s/epoch - 422ms/step
Epoch 776/1000
2023-09-28 17:22:00.346 
Epoch 776/1000 
	 loss: 16.3371, MinusLogProbMetric: 16.3371, val_loss: 17.0352, val_MinusLogProbMetric: 17.0352

Epoch 776: val_loss did not improve from 17.02957
196/196 - 85s - loss: 16.3371 - MinusLogProbMetric: 16.3371 - val_loss: 17.0352 - val_MinusLogProbMetric: 17.0352 - lr: 5.2083e-06 - 85s/epoch - 433ms/step
Epoch 777/1000
2023-09-28 17:23:21.806 
Epoch 777/1000 
	 loss: 16.3365, MinusLogProbMetric: 16.3365, val_loss: 17.0364, val_MinusLogProbMetric: 17.0364

Epoch 777: val_loss did not improve from 17.02957
196/196 - 81s - loss: 16.3365 - MinusLogProbMetric: 16.3365 - val_loss: 17.0364 - val_MinusLogProbMetric: 17.0364 - lr: 5.2083e-06 - 81s/epoch - 416ms/step
Epoch 778/1000
2023-09-28 17:24:45.683 
Epoch 778/1000 
	 loss: 16.3358, MinusLogProbMetric: 16.3358, val_loss: 17.0330, val_MinusLogProbMetric: 17.0330

Epoch 778: val_loss did not improve from 17.02957
196/196 - 84s - loss: 16.3358 - MinusLogProbMetric: 16.3358 - val_loss: 17.0330 - val_MinusLogProbMetric: 17.0330 - lr: 5.2083e-06 - 84s/epoch - 428ms/step
Epoch 779/1000
2023-09-28 17:26:07.677 
Epoch 779/1000 
	 loss: 16.3360, MinusLogProbMetric: 16.3360, val_loss: 17.0385, val_MinusLogProbMetric: 17.0385

Epoch 779: val_loss did not improve from 17.02957
196/196 - 82s - loss: 16.3360 - MinusLogProbMetric: 16.3360 - val_loss: 17.0385 - val_MinusLogProbMetric: 17.0385 - lr: 5.2083e-06 - 82s/epoch - 418ms/step
Epoch 780/1000
2023-09-28 17:27:30.645 
Epoch 780/1000 
	 loss: 16.3368, MinusLogProbMetric: 16.3368, val_loss: 17.0373, val_MinusLogProbMetric: 17.0373

Epoch 780: val_loss did not improve from 17.02957
196/196 - 83s - loss: 16.3368 - MinusLogProbMetric: 16.3368 - val_loss: 17.0373 - val_MinusLogProbMetric: 17.0373 - lr: 5.2083e-06 - 83s/epoch - 423ms/step
Epoch 781/1000
2023-09-28 17:28:51.493 
Epoch 781/1000 
	 loss: 16.3353, MinusLogProbMetric: 16.3353, val_loss: 17.0383, val_MinusLogProbMetric: 17.0383

Epoch 781: val_loss did not improve from 17.02957
196/196 - 81s - loss: 16.3353 - MinusLogProbMetric: 16.3353 - val_loss: 17.0383 - val_MinusLogProbMetric: 17.0383 - lr: 5.2083e-06 - 81s/epoch - 412ms/step
Epoch 782/1000
2023-09-28 17:30:14.059 
Epoch 782/1000 
	 loss: 16.3356, MinusLogProbMetric: 16.3356, val_loss: 17.0314, val_MinusLogProbMetric: 17.0314

Epoch 782: val_loss did not improve from 17.02957
196/196 - 83s - loss: 16.3356 - MinusLogProbMetric: 16.3356 - val_loss: 17.0314 - val_MinusLogProbMetric: 17.0314 - lr: 5.2083e-06 - 83s/epoch - 421ms/step
Epoch 783/1000
2023-09-28 17:31:35.095 
Epoch 783/1000 
	 loss: 16.3349, MinusLogProbMetric: 16.3349, val_loss: 17.0311, val_MinusLogProbMetric: 17.0311

Epoch 783: val_loss did not improve from 17.02957
196/196 - 81s - loss: 16.3349 - MinusLogProbMetric: 16.3349 - val_loss: 17.0311 - val_MinusLogProbMetric: 17.0311 - lr: 5.2083e-06 - 81s/epoch - 413ms/step
Epoch 784/1000
2023-09-28 17:32:57.219 
Epoch 784/1000 
	 loss: 16.3368, MinusLogProbMetric: 16.3368, val_loss: 17.0344, val_MinusLogProbMetric: 17.0344

Epoch 784: val_loss did not improve from 17.02957
196/196 - 82s - loss: 16.3368 - MinusLogProbMetric: 16.3368 - val_loss: 17.0344 - val_MinusLogProbMetric: 17.0344 - lr: 5.2083e-06 - 82s/epoch - 419ms/step
Epoch 785/1000
2023-09-28 17:34:24.578 
Epoch 785/1000 
	 loss: 16.3358, MinusLogProbMetric: 16.3358, val_loss: 17.0317, val_MinusLogProbMetric: 17.0317

Epoch 785: val_loss did not improve from 17.02957
196/196 - 87s - loss: 16.3358 - MinusLogProbMetric: 16.3358 - val_loss: 17.0317 - val_MinusLogProbMetric: 17.0317 - lr: 5.2083e-06 - 87s/epoch - 446ms/step
Epoch 786/1000
2023-09-28 17:35:51.326 
Epoch 786/1000 
	 loss: 16.3344, MinusLogProbMetric: 16.3344, val_loss: 17.0336, val_MinusLogProbMetric: 17.0336

Epoch 786: val_loss did not improve from 17.02957
196/196 - 87s - loss: 16.3344 - MinusLogProbMetric: 16.3344 - val_loss: 17.0336 - val_MinusLogProbMetric: 17.0336 - lr: 5.2083e-06 - 87s/epoch - 443ms/step
Epoch 787/1000
2023-09-28 17:37:17.746 
Epoch 787/1000 
	 loss: 16.3349, MinusLogProbMetric: 16.3349, val_loss: 17.0313, val_MinusLogProbMetric: 17.0313

Epoch 787: val_loss did not improve from 17.02957
196/196 - 86s - loss: 16.3349 - MinusLogProbMetric: 16.3349 - val_loss: 17.0313 - val_MinusLogProbMetric: 17.0313 - lr: 5.2083e-06 - 86s/epoch - 441ms/step
Epoch 788/1000
2023-09-28 17:38:44.400 
Epoch 788/1000 
	 loss: 16.3359, MinusLogProbMetric: 16.3359, val_loss: 17.0320, val_MinusLogProbMetric: 17.0320

Epoch 788: val_loss did not improve from 17.02957
196/196 - 87s - loss: 16.3359 - MinusLogProbMetric: 16.3359 - val_loss: 17.0320 - val_MinusLogProbMetric: 17.0320 - lr: 5.2083e-06 - 87s/epoch - 442ms/step
Epoch 789/1000
2023-09-28 17:40:10.881 
Epoch 789/1000 
	 loss: 16.3354, MinusLogProbMetric: 16.3354, val_loss: 17.0316, val_MinusLogProbMetric: 17.0316

Epoch 789: val_loss did not improve from 17.02957
196/196 - 86s - loss: 16.3354 - MinusLogProbMetric: 16.3354 - val_loss: 17.0316 - val_MinusLogProbMetric: 17.0316 - lr: 5.2083e-06 - 86s/epoch - 441ms/step
Epoch 790/1000
2023-09-28 17:41:37.699 
Epoch 790/1000 
	 loss: 16.3353, MinusLogProbMetric: 16.3353, val_loss: 17.0334, val_MinusLogProbMetric: 17.0334

Epoch 790: val_loss did not improve from 17.02957
196/196 - 87s - loss: 16.3353 - MinusLogProbMetric: 16.3353 - val_loss: 17.0334 - val_MinusLogProbMetric: 17.0334 - lr: 5.2083e-06 - 87s/epoch - 443ms/step
Epoch 791/1000
2023-09-28 17:43:03.494 
Epoch 791/1000 
	 loss: 16.3346, MinusLogProbMetric: 16.3346, val_loss: 17.0299, val_MinusLogProbMetric: 17.0299

Epoch 791: val_loss did not improve from 17.02957
196/196 - 86s - loss: 16.3346 - MinusLogProbMetric: 16.3346 - val_loss: 17.0299 - val_MinusLogProbMetric: 17.0299 - lr: 5.2083e-06 - 86s/epoch - 438ms/step
Epoch 792/1000
2023-09-28 17:44:30.576 
Epoch 792/1000 
	 loss: 16.3357, MinusLogProbMetric: 16.3357, val_loss: 17.0309, val_MinusLogProbMetric: 17.0309

Epoch 792: val_loss did not improve from 17.02957
196/196 - 87s - loss: 16.3357 - MinusLogProbMetric: 16.3357 - val_loss: 17.0309 - val_MinusLogProbMetric: 17.0309 - lr: 5.2083e-06 - 87s/epoch - 444ms/step
Epoch 793/1000
2023-09-28 17:45:57.396 
Epoch 793/1000 
	 loss: 16.3357, MinusLogProbMetric: 16.3357, val_loss: 17.0326, val_MinusLogProbMetric: 17.0326

Epoch 793: val_loss did not improve from 17.02957
196/196 - 87s - loss: 16.3357 - MinusLogProbMetric: 16.3357 - val_loss: 17.0326 - val_MinusLogProbMetric: 17.0326 - lr: 5.2083e-06 - 87s/epoch - 443ms/step
Epoch 794/1000
2023-09-28 17:47:24.871 
Epoch 794/1000 
	 loss: 16.3355, MinusLogProbMetric: 16.3355, val_loss: 17.0319, val_MinusLogProbMetric: 17.0319

Epoch 794: val_loss did not improve from 17.02957
196/196 - 87s - loss: 16.3355 - MinusLogProbMetric: 16.3355 - val_loss: 17.0319 - val_MinusLogProbMetric: 17.0319 - lr: 5.2083e-06 - 87s/epoch - 446ms/step
Epoch 795/1000
2023-09-28 17:48:52.215 
Epoch 795/1000 
	 loss: 16.3346, MinusLogProbMetric: 16.3346, val_loss: 17.0303, val_MinusLogProbMetric: 17.0303

Epoch 795: val_loss did not improve from 17.02957
196/196 - 87s - loss: 16.3346 - MinusLogProbMetric: 16.3346 - val_loss: 17.0303 - val_MinusLogProbMetric: 17.0303 - lr: 5.2083e-06 - 87s/epoch - 446ms/step
Epoch 796/1000
2023-09-28 17:50:19.096 
Epoch 796/1000 
	 loss: 16.3345, MinusLogProbMetric: 16.3345, val_loss: 17.0338, val_MinusLogProbMetric: 17.0338

Epoch 796: val_loss did not improve from 17.02957
196/196 - 87s - loss: 16.3345 - MinusLogProbMetric: 16.3345 - val_loss: 17.0338 - val_MinusLogProbMetric: 17.0338 - lr: 5.2083e-06 - 87s/epoch - 443ms/step
Epoch 797/1000
2023-09-28 17:51:45.778 
Epoch 797/1000 
	 loss: 16.3353, MinusLogProbMetric: 16.3353, val_loss: 17.0355, val_MinusLogProbMetric: 17.0355

Epoch 797: val_loss did not improve from 17.02957
196/196 - 87s - loss: 16.3353 - MinusLogProbMetric: 16.3353 - val_loss: 17.0355 - val_MinusLogProbMetric: 17.0355 - lr: 5.2083e-06 - 87s/epoch - 442ms/step
Epoch 798/1000
2023-09-28 17:53:12.145 
Epoch 798/1000 
	 loss: 16.3356, MinusLogProbMetric: 16.3356, val_loss: 17.0320, val_MinusLogProbMetric: 17.0320

Epoch 798: val_loss did not improve from 17.02957
196/196 - 86s - loss: 16.3356 - MinusLogProbMetric: 16.3356 - val_loss: 17.0320 - val_MinusLogProbMetric: 17.0320 - lr: 5.2083e-06 - 86s/epoch - 441ms/step
Epoch 799/1000
2023-09-28 17:54:38.317 
Epoch 799/1000 
	 loss: 16.3341, MinusLogProbMetric: 16.3341, val_loss: 17.0378, val_MinusLogProbMetric: 17.0378

Epoch 799: val_loss did not improve from 17.02957
196/196 - 86s - loss: 16.3341 - MinusLogProbMetric: 16.3341 - val_loss: 17.0378 - val_MinusLogProbMetric: 17.0378 - lr: 5.2083e-06 - 86s/epoch - 440ms/step
Epoch 800/1000
2023-09-28 17:56:04.640 
Epoch 800/1000 
	 loss: 16.3358, MinusLogProbMetric: 16.3358, val_loss: 17.0319, val_MinusLogProbMetric: 17.0319

Epoch 800: val_loss did not improve from 17.02957
196/196 - 86s - loss: 16.3358 - MinusLogProbMetric: 16.3358 - val_loss: 17.0319 - val_MinusLogProbMetric: 17.0319 - lr: 5.2083e-06 - 86s/epoch - 440ms/step
Epoch 801/1000
2023-09-28 17:57:31.390 
Epoch 801/1000 
	 loss: 16.3336, MinusLogProbMetric: 16.3336, val_loss: 17.0308, val_MinusLogProbMetric: 17.0308

Epoch 801: val_loss did not improve from 17.02957
196/196 - 87s - loss: 16.3336 - MinusLogProbMetric: 16.3336 - val_loss: 17.0308 - val_MinusLogProbMetric: 17.0308 - lr: 5.2083e-06 - 87s/epoch - 443ms/step
Epoch 802/1000
2023-09-28 17:58:54.402 
Epoch 802/1000 
	 loss: 16.3347, MinusLogProbMetric: 16.3347, val_loss: 17.0374, val_MinusLogProbMetric: 17.0374

Epoch 802: val_loss did not improve from 17.02957
196/196 - 83s - loss: 16.3347 - MinusLogProbMetric: 16.3347 - val_loss: 17.0374 - val_MinusLogProbMetric: 17.0374 - lr: 5.2083e-06 - 83s/epoch - 423ms/step
Epoch 803/1000
2023-09-28 18:00:12.280 
Epoch 803/1000 
	 loss: 16.3339, MinusLogProbMetric: 16.3339, val_loss: 17.0341, val_MinusLogProbMetric: 17.0341

Epoch 803: val_loss did not improve from 17.02957
196/196 - 78s - loss: 16.3339 - MinusLogProbMetric: 16.3339 - val_loss: 17.0341 - val_MinusLogProbMetric: 17.0341 - lr: 5.2083e-06 - 78s/epoch - 397ms/step
Epoch 804/1000
2023-09-28 18:01:34.475 
Epoch 804/1000 
	 loss: 16.3349, MinusLogProbMetric: 16.3349, val_loss: 17.0313, val_MinusLogProbMetric: 17.0313

Epoch 804: val_loss did not improve from 17.02957
196/196 - 82s - loss: 16.3349 - MinusLogProbMetric: 16.3349 - val_loss: 17.0313 - val_MinusLogProbMetric: 17.0313 - lr: 5.2083e-06 - 82s/epoch - 419ms/step
Epoch 805/1000
2023-09-28 18:02:52.172 
Epoch 805/1000 
	 loss: 16.3347, MinusLogProbMetric: 16.3347, val_loss: 17.0324, val_MinusLogProbMetric: 17.0324

Epoch 805: val_loss did not improve from 17.02957
196/196 - 78s - loss: 16.3347 - MinusLogProbMetric: 16.3347 - val_loss: 17.0324 - val_MinusLogProbMetric: 17.0324 - lr: 5.2083e-06 - 78s/epoch - 396ms/step
Epoch 806/1000
2023-09-28 18:04:07.809 
Epoch 806/1000 
	 loss: 16.3347, MinusLogProbMetric: 16.3347, val_loss: 17.0340, val_MinusLogProbMetric: 17.0340

Epoch 806: val_loss did not improve from 17.02957
196/196 - 76s - loss: 16.3347 - MinusLogProbMetric: 16.3347 - val_loss: 17.0340 - val_MinusLogProbMetric: 17.0340 - lr: 5.2083e-06 - 76s/epoch - 386ms/step
Epoch 807/1000
2023-09-28 18:05:18.832 
Epoch 807/1000 
	 loss: 16.3345, MinusLogProbMetric: 16.3345, val_loss: 17.0331, val_MinusLogProbMetric: 17.0331

Epoch 807: val_loss did not improve from 17.02957
196/196 - 71s - loss: 16.3345 - MinusLogProbMetric: 16.3345 - val_loss: 17.0331 - val_MinusLogProbMetric: 17.0331 - lr: 5.2083e-06 - 71s/epoch - 362ms/step
Epoch 808/1000
2023-09-28 18:06:29.335 
Epoch 808/1000 
	 loss: 16.3342, MinusLogProbMetric: 16.3342, val_loss: 17.0326, val_MinusLogProbMetric: 17.0326

Epoch 808: val_loss did not improve from 17.02957
196/196 - 70s - loss: 16.3342 - MinusLogProbMetric: 16.3342 - val_loss: 17.0326 - val_MinusLogProbMetric: 17.0326 - lr: 5.2083e-06 - 70s/epoch - 360ms/step
Epoch 809/1000
2023-09-28 18:07:45.919 
Epoch 809/1000 
	 loss: 16.3351, MinusLogProbMetric: 16.3351, val_loss: 17.0344, val_MinusLogProbMetric: 17.0344

Epoch 809: val_loss did not improve from 17.02957
196/196 - 77s - loss: 16.3351 - MinusLogProbMetric: 16.3351 - val_loss: 17.0344 - val_MinusLogProbMetric: 17.0344 - lr: 5.2083e-06 - 77s/epoch - 391ms/step
Epoch 810/1000
2023-09-28 18:09:02.132 
Epoch 810/1000 
	 loss: 16.3306, MinusLogProbMetric: 16.3306, val_loss: 17.0286, val_MinusLogProbMetric: 17.0286

Epoch 810: val_loss improved from 17.02957 to 17.02860, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 78s - loss: 16.3306 - MinusLogProbMetric: 16.3306 - val_loss: 17.0286 - val_MinusLogProbMetric: 17.0286 - lr: 2.6042e-06 - 78s/epoch - 396ms/step
Epoch 811/1000
2023-09-28 18:10:20.224 
Epoch 811/1000 
	 loss: 16.3309, MinusLogProbMetric: 16.3309, val_loss: 17.0290, val_MinusLogProbMetric: 17.0290

Epoch 811: val_loss did not improve from 17.02860
196/196 - 77s - loss: 16.3309 - MinusLogProbMetric: 16.3309 - val_loss: 17.0290 - val_MinusLogProbMetric: 17.0290 - lr: 2.6042e-06 - 77s/epoch - 392ms/step
Epoch 812/1000
2023-09-28 18:11:40.974 
Epoch 812/1000 
	 loss: 16.3303, MinusLogProbMetric: 16.3303, val_loss: 17.0280, val_MinusLogProbMetric: 17.0280

Epoch 812: val_loss improved from 17.02860 to 17.02798, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 81s - loss: 16.3303 - MinusLogProbMetric: 16.3303 - val_loss: 17.0280 - val_MinusLogProbMetric: 17.0280 - lr: 2.6042e-06 - 81s/epoch - 416ms/step
Epoch 813/1000
2023-09-28 18:13:00.730 
Epoch 813/1000 
	 loss: 16.3301, MinusLogProbMetric: 16.3301, val_loss: 17.0301, val_MinusLogProbMetric: 17.0301

Epoch 813: val_loss did not improve from 17.02798
196/196 - 79s - loss: 16.3301 - MinusLogProbMetric: 16.3301 - val_loss: 17.0301 - val_MinusLogProbMetric: 17.0301 - lr: 2.6042e-06 - 79s/epoch - 403ms/step
Epoch 814/1000
2023-09-28 18:14:22.954 
Epoch 814/1000 
	 loss: 16.3304, MinusLogProbMetric: 16.3304, val_loss: 17.0288, val_MinusLogProbMetric: 17.0288

Epoch 814: val_loss did not improve from 17.02798
196/196 - 82s - loss: 16.3304 - MinusLogProbMetric: 16.3304 - val_loss: 17.0288 - val_MinusLogProbMetric: 17.0288 - lr: 2.6042e-06 - 82s/epoch - 419ms/step
Epoch 815/1000
2023-09-28 18:15:46.673 
Epoch 815/1000 
	 loss: 16.3301, MinusLogProbMetric: 16.3301, val_loss: 17.0297, val_MinusLogProbMetric: 17.0297

Epoch 815: val_loss did not improve from 17.02798
196/196 - 84s - loss: 16.3301 - MinusLogProbMetric: 16.3301 - val_loss: 17.0297 - val_MinusLogProbMetric: 17.0297 - lr: 2.6042e-06 - 84s/epoch - 427ms/step
Epoch 816/1000
2023-09-28 18:17:07.336 
Epoch 816/1000 
	 loss: 16.3302, MinusLogProbMetric: 16.3302, val_loss: 17.0295, val_MinusLogProbMetric: 17.0295

Epoch 816: val_loss did not improve from 17.02798
196/196 - 81s - loss: 16.3302 - MinusLogProbMetric: 16.3302 - val_loss: 17.0295 - val_MinusLogProbMetric: 17.0295 - lr: 2.6042e-06 - 81s/epoch - 411ms/step
Epoch 817/1000
2023-09-28 18:18:32.735 
Epoch 817/1000 
	 loss: 16.3303, MinusLogProbMetric: 16.3303, val_loss: 17.0285, val_MinusLogProbMetric: 17.0285

Epoch 817: val_loss did not improve from 17.02798
196/196 - 85s - loss: 16.3303 - MinusLogProbMetric: 16.3303 - val_loss: 17.0285 - val_MinusLogProbMetric: 17.0285 - lr: 2.6042e-06 - 85s/epoch - 436ms/step
Epoch 818/1000
2023-09-28 18:19:52.402 
Epoch 818/1000 
	 loss: 16.3302, MinusLogProbMetric: 16.3302, val_loss: 17.0292, val_MinusLogProbMetric: 17.0292

Epoch 818: val_loss did not improve from 17.02798
196/196 - 80s - loss: 16.3302 - MinusLogProbMetric: 16.3302 - val_loss: 17.0292 - val_MinusLogProbMetric: 17.0292 - lr: 2.6042e-06 - 80s/epoch - 406ms/step
Epoch 819/1000
2023-09-28 18:21:07.239 
Epoch 819/1000 
	 loss: 16.3303, MinusLogProbMetric: 16.3303, val_loss: 17.0282, val_MinusLogProbMetric: 17.0282

Epoch 819: val_loss did not improve from 17.02798
196/196 - 75s - loss: 16.3303 - MinusLogProbMetric: 16.3303 - val_loss: 17.0282 - val_MinusLogProbMetric: 17.0282 - lr: 2.6042e-06 - 75s/epoch - 382ms/step
Epoch 820/1000
2023-09-28 18:22:21.906 
Epoch 820/1000 
	 loss: 16.3300, MinusLogProbMetric: 16.3300, val_loss: 17.0301, val_MinusLogProbMetric: 17.0301

Epoch 820: val_loss did not improve from 17.02798
196/196 - 75s - loss: 16.3300 - MinusLogProbMetric: 16.3300 - val_loss: 17.0301 - val_MinusLogProbMetric: 17.0301 - lr: 2.6042e-06 - 75s/epoch - 381ms/step
Epoch 821/1000
2023-09-28 18:23:36.840 
Epoch 821/1000 
	 loss: 16.3301, MinusLogProbMetric: 16.3301, val_loss: 17.0314, val_MinusLogProbMetric: 17.0314

Epoch 821: val_loss did not improve from 17.02798
196/196 - 75s - loss: 16.3301 - MinusLogProbMetric: 16.3301 - val_loss: 17.0314 - val_MinusLogProbMetric: 17.0314 - lr: 2.6042e-06 - 75s/epoch - 382ms/step
Epoch 822/1000
2023-09-28 18:24:51.720 
Epoch 822/1000 
	 loss: 16.3306, MinusLogProbMetric: 16.3306, val_loss: 17.0290, val_MinusLogProbMetric: 17.0290

Epoch 822: val_loss did not improve from 17.02798
196/196 - 75s - loss: 16.3306 - MinusLogProbMetric: 16.3306 - val_loss: 17.0290 - val_MinusLogProbMetric: 17.0290 - lr: 2.6042e-06 - 75s/epoch - 382ms/step
Epoch 823/1000
2023-09-28 18:26:06.381 
Epoch 823/1000 
	 loss: 16.3292, MinusLogProbMetric: 16.3292, val_loss: 17.0301, val_MinusLogProbMetric: 17.0301

Epoch 823: val_loss did not improve from 17.02798
196/196 - 75s - loss: 16.3292 - MinusLogProbMetric: 16.3292 - val_loss: 17.0301 - val_MinusLogProbMetric: 17.0301 - lr: 2.6042e-06 - 75s/epoch - 381ms/step
Epoch 824/1000
2023-09-28 18:27:23.709 
Epoch 824/1000 
	 loss: 16.3307, MinusLogProbMetric: 16.3307, val_loss: 17.0294, val_MinusLogProbMetric: 17.0294

Epoch 824: val_loss did not improve from 17.02798
196/196 - 77s - loss: 16.3307 - MinusLogProbMetric: 16.3307 - val_loss: 17.0294 - val_MinusLogProbMetric: 17.0294 - lr: 2.6042e-06 - 77s/epoch - 394ms/step
Epoch 825/1000
2023-09-28 18:28:40.320 
Epoch 825/1000 
	 loss: 16.3308, MinusLogProbMetric: 16.3308, val_loss: 17.0302, val_MinusLogProbMetric: 17.0302

Epoch 825: val_loss did not improve from 17.02798
196/196 - 77s - loss: 16.3308 - MinusLogProbMetric: 16.3308 - val_loss: 17.0302 - val_MinusLogProbMetric: 17.0302 - lr: 2.6042e-06 - 77s/epoch - 391ms/step
Epoch 826/1000
2023-09-28 18:30:01.749 
Epoch 826/1000 
	 loss: 16.3308, MinusLogProbMetric: 16.3308, val_loss: 17.0286, val_MinusLogProbMetric: 17.0286

Epoch 826: val_loss did not improve from 17.02798
196/196 - 81s - loss: 16.3308 - MinusLogProbMetric: 16.3308 - val_loss: 17.0286 - val_MinusLogProbMetric: 17.0286 - lr: 2.6042e-06 - 81s/epoch - 415ms/step
Epoch 827/1000
2023-09-28 18:31:20.656 
Epoch 827/1000 
	 loss: 16.3299, MinusLogProbMetric: 16.3299, val_loss: 17.0322, val_MinusLogProbMetric: 17.0322

Epoch 827: val_loss did not improve from 17.02798
196/196 - 79s - loss: 16.3299 - MinusLogProbMetric: 16.3299 - val_loss: 17.0322 - val_MinusLogProbMetric: 17.0322 - lr: 2.6042e-06 - 79s/epoch - 403ms/step
Epoch 828/1000
2023-09-28 18:32:40.766 
Epoch 828/1000 
	 loss: 16.3299, MinusLogProbMetric: 16.3299, val_loss: 17.0310, val_MinusLogProbMetric: 17.0310

Epoch 828: val_loss did not improve from 17.02798
196/196 - 80s - loss: 16.3299 - MinusLogProbMetric: 16.3299 - val_loss: 17.0310 - val_MinusLogProbMetric: 17.0310 - lr: 2.6042e-06 - 80s/epoch - 409ms/step
Epoch 829/1000
2023-09-28 18:34:03.928 
Epoch 829/1000 
	 loss: 16.3307, MinusLogProbMetric: 16.3307, val_loss: 17.0301, val_MinusLogProbMetric: 17.0301

Epoch 829: val_loss did not improve from 17.02798
196/196 - 83s - loss: 16.3307 - MinusLogProbMetric: 16.3307 - val_loss: 17.0301 - val_MinusLogProbMetric: 17.0301 - lr: 2.6042e-06 - 83s/epoch - 424ms/step
Epoch 830/1000
2023-09-28 18:35:30.430 
Epoch 830/1000 
	 loss: 16.3305, MinusLogProbMetric: 16.3305, val_loss: 17.0296, val_MinusLogProbMetric: 17.0296

Epoch 830: val_loss did not improve from 17.02798
196/196 - 86s - loss: 16.3305 - MinusLogProbMetric: 16.3305 - val_loss: 17.0296 - val_MinusLogProbMetric: 17.0296 - lr: 2.6042e-06 - 86s/epoch - 441ms/step
Epoch 831/1000
2023-09-28 18:36:54.582 
Epoch 831/1000 
	 loss: 16.3305, MinusLogProbMetric: 16.3305, val_loss: 17.0319, val_MinusLogProbMetric: 17.0319

Epoch 831: val_loss did not improve from 17.02798
196/196 - 84s - loss: 16.3305 - MinusLogProbMetric: 16.3305 - val_loss: 17.0319 - val_MinusLogProbMetric: 17.0319 - lr: 2.6042e-06 - 84s/epoch - 429ms/step
Epoch 832/1000
2023-09-28 18:38:21.708 
Epoch 832/1000 
	 loss: 16.3301, MinusLogProbMetric: 16.3301, val_loss: 17.0274, val_MinusLogProbMetric: 17.0274

Epoch 832: val_loss improved from 17.02798 to 17.02739, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_318/weights/best_weights.h5
196/196 - 88s - loss: 16.3301 - MinusLogProbMetric: 16.3301 - val_loss: 17.0274 - val_MinusLogProbMetric: 17.0274 - lr: 2.6042e-06 - 88s/epoch - 449ms/step
Epoch 833/1000
2023-09-28 18:39:48.377 
Epoch 833/1000 
	 loss: 16.3305, MinusLogProbMetric: 16.3305, val_loss: 17.0298, val_MinusLogProbMetric: 17.0298

Epoch 833: val_loss did not improve from 17.02739
196/196 - 86s - loss: 16.3305 - MinusLogProbMetric: 16.3305 - val_loss: 17.0298 - val_MinusLogProbMetric: 17.0298 - lr: 2.6042e-06 - 86s/epoch - 437ms/step
Epoch 834/1000
2023-09-28 18:41:15.902 
Epoch 834/1000 
	 loss: 16.3309, MinusLogProbMetric: 16.3309, val_loss: 17.0325, val_MinusLogProbMetric: 17.0325

Epoch 834: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3309 - MinusLogProbMetric: 16.3309 - val_loss: 17.0325 - val_MinusLogProbMetric: 17.0325 - lr: 2.6042e-06 - 88s/epoch - 447ms/step
Epoch 835/1000
2023-09-28 18:42:42.203 
Epoch 835/1000 
	 loss: 16.3308, MinusLogProbMetric: 16.3308, val_loss: 17.0279, val_MinusLogProbMetric: 17.0279

Epoch 835: val_loss did not improve from 17.02739
196/196 - 86s - loss: 16.3308 - MinusLogProbMetric: 16.3308 - val_loss: 17.0279 - val_MinusLogProbMetric: 17.0279 - lr: 2.6042e-06 - 86s/epoch - 440ms/step
Epoch 836/1000
2023-09-28 18:44:08.494 
Epoch 836/1000 
	 loss: 16.3301, MinusLogProbMetric: 16.3301, val_loss: 17.0282, val_MinusLogProbMetric: 17.0282

Epoch 836: val_loss did not improve from 17.02739
196/196 - 86s - loss: 16.3301 - MinusLogProbMetric: 16.3301 - val_loss: 17.0282 - val_MinusLogProbMetric: 17.0282 - lr: 2.6042e-06 - 86s/epoch - 440ms/step
Epoch 837/1000
2023-09-28 18:45:36.678 
Epoch 837/1000 
	 loss: 16.3298, MinusLogProbMetric: 16.3298, val_loss: 17.0318, val_MinusLogProbMetric: 17.0318

Epoch 837: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3298 - MinusLogProbMetric: 16.3298 - val_loss: 17.0318 - val_MinusLogProbMetric: 17.0318 - lr: 2.6042e-06 - 88s/epoch - 450ms/step
Epoch 838/1000
2023-09-28 18:47:04.586 
Epoch 838/1000 
	 loss: 16.3308, MinusLogProbMetric: 16.3308, val_loss: 17.0295, val_MinusLogProbMetric: 17.0295

Epoch 838: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3308 - MinusLogProbMetric: 16.3308 - val_loss: 17.0295 - val_MinusLogProbMetric: 17.0295 - lr: 2.6042e-06 - 88s/epoch - 448ms/step
Epoch 839/1000
2023-09-28 18:48:33.207 
Epoch 839/1000 
	 loss: 16.3310, MinusLogProbMetric: 16.3310, val_loss: 17.0308, val_MinusLogProbMetric: 17.0308

Epoch 839: val_loss did not improve from 17.02739
196/196 - 89s - loss: 16.3310 - MinusLogProbMetric: 16.3310 - val_loss: 17.0308 - val_MinusLogProbMetric: 17.0308 - lr: 2.6042e-06 - 89s/epoch - 452ms/step
Epoch 840/1000
2023-09-28 18:50:01.424 
Epoch 840/1000 
	 loss: 16.3307, MinusLogProbMetric: 16.3307, val_loss: 17.0303, val_MinusLogProbMetric: 17.0303

Epoch 840: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3307 - MinusLogProbMetric: 16.3307 - val_loss: 17.0303 - val_MinusLogProbMetric: 17.0303 - lr: 2.6042e-06 - 88s/epoch - 450ms/step
Epoch 841/1000
2023-09-28 18:51:29.792 
Epoch 841/1000 
	 loss: 16.3300, MinusLogProbMetric: 16.3300, val_loss: 17.0285, val_MinusLogProbMetric: 17.0285

Epoch 841: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3300 - MinusLogProbMetric: 16.3300 - val_loss: 17.0285 - val_MinusLogProbMetric: 17.0285 - lr: 2.6042e-06 - 88s/epoch - 451ms/step
Epoch 842/1000
2023-09-28 18:52:53.022 
Epoch 842/1000 
	 loss: 16.3307, MinusLogProbMetric: 16.3307, val_loss: 17.0332, val_MinusLogProbMetric: 17.0332

Epoch 842: val_loss did not improve from 17.02739
196/196 - 83s - loss: 16.3307 - MinusLogProbMetric: 16.3307 - val_loss: 17.0332 - val_MinusLogProbMetric: 17.0332 - lr: 2.6042e-06 - 83s/epoch - 425ms/step
Epoch 843/1000
2023-09-28 18:54:14.866 
Epoch 843/1000 
	 loss: 16.3302, MinusLogProbMetric: 16.3302, val_loss: 17.0300, val_MinusLogProbMetric: 17.0300

Epoch 843: val_loss did not improve from 17.02739
196/196 - 82s - loss: 16.3302 - MinusLogProbMetric: 16.3302 - val_loss: 17.0300 - val_MinusLogProbMetric: 17.0300 - lr: 2.6042e-06 - 82s/epoch - 418ms/step
Epoch 844/1000
2023-09-28 18:55:37.045 
Epoch 844/1000 
	 loss: 16.3301, MinusLogProbMetric: 16.3301, val_loss: 17.0298, val_MinusLogProbMetric: 17.0298

Epoch 844: val_loss did not improve from 17.02739
196/196 - 82s - loss: 16.3301 - MinusLogProbMetric: 16.3301 - val_loss: 17.0298 - val_MinusLogProbMetric: 17.0298 - lr: 2.6042e-06 - 82s/epoch - 419ms/step
Epoch 845/1000
2023-09-28 18:57:00.431 
Epoch 845/1000 
	 loss: 16.3295, MinusLogProbMetric: 16.3295, val_loss: 17.0315, val_MinusLogProbMetric: 17.0315

Epoch 845: val_loss did not improve from 17.02739
196/196 - 83s - loss: 16.3295 - MinusLogProbMetric: 16.3295 - val_loss: 17.0315 - val_MinusLogProbMetric: 17.0315 - lr: 2.6042e-06 - 83s/epoch - 425ms/step
Epoch 846/1000
2023-09-28 18:58:25.237 
Epoch 846/1000 
	 loss: 16.3299, MinusLogProbMetric: 16.3299, val_loss: 17.0304, val_MinusLogProbMetric: 17.0304

Epoch 846: val_loss did not improve from 17.02739
196/196 - 85s - loss: 16.3299 - MinusLogProbMetric: 16.3299 - val_loss: 17.0304 - val_MinusLogProbMetric: 17.0304 - lr: 2.6042e-06 - 85s/epoch - 433ms/step
Epoch 847/1000
2023-09-28 18:59:51.349 
Epoch 847/1000 
	 loss: 16.3295, MinusLogProbMetric: 16.3295, val_loss: 17.0295, val_MinusLogProbMetric: 17.0295

Epoch 847: val_loss did not improve from 17.02739
196/196 - 86s - loss: 16.3295 - MinusLogProbMetric: 16.3295 - val_loss: 17.0295 - val_MinusLogProbMetric: 17.0295 - lr: 2.6042e-06 - 86s/epoch - 439ms/step
Epoch 848/1000
2023-09-28 19:01:17.785 
Epoch 848/1000 
	 loss: 16.3303, MinusLogProbMetric: 16.3303, val_loss: 17.0298, val_MinusLogProbMetric: 17.0298

Epoch 848: val_loss did not improve from 17.02739
196/196 - 86s - loss: 16.3303 - MinusLogProbMetric: 16.3303 - val_loss: 17.0298 - val_MinusLogProbMetric: 17.0298 - lr: 2.6042e-06 - 86s/epoch - 441ms/step
Epoch 849/1000
2023-09-28 19:02:45.741 
Epoch 849/1000 
	 loss: 16.3299, MinusLogProbMetric: 16.3299, val_loss: 17.0297, val_MinusLogProbMetric: 17.0297

Epoch 849: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3299 - MinusLogProbMetric: 16.3299 - val_loss: 17.0297 - val_MinusLogProbMetric: 17.0297 - lr: 2.6042e-06 - 88s/epoch - 449ms/step
Epoch 850/1000
2023-09-28 19:04:12.888 
Epoch 850/1000 
	 loss: 16.3298, MinusLogProbMetric: 16.3298, val_loss: 17.0309, val_MinusLogProbMetric: 17.0309

Epoch 850: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3298 - MinusLogProbMetric: 16.3298 - val_loss: 17.0309 - val_MinusLogProbMetric: 17.0309 - lr: 2.6042e-06 - 87s/epoch - 445ms/step
Epoch 851/1000
2023-09-28 19:05:40.936 
Epoch 851/1000 
	 loss: 16.3293, MinusLogProbMetric: 16.3293, val_loss: 17.0305, val_MinusLogProbMetric: 17.0305

Epoch 851: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3293 - MinusLogProbMetric: 16.3293 - val_loss: 17.0305 - val_MinusLogProbMetric: 17.0305 - lr: 2.6042e-06 - 88s/epoch - 449ms/step
Epoch 852/1000
2023-09-28 19:07:08.088 
Epoch 852/1000 
	 loss: 16.3295, MinusLogProbMetric: 16.3295, val_loss: 17.0327, val_MinusLogProbMetric: 17.0327

Epoch 852: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3295 - MinusLogProbMetric: 16.3295 - val_loss: 17.0327 - val_MinusLogProbMetric: 17.0327 - lr: 2.6042e-06 - 87s/epoch - 445ms/step
Epoch 853/1000
2023-09-28 19:08:36.955 
Epoch 853/1000 
	 loss: 16.3300, MinusLogProbMetric: 16.3300, val_loss: 17.0306, val_MinusLogProbMetric: 17.0306

Epoch 853: val_loss did not improve from 17.02739
196/196 - 89s - loss: 16.3300 - MinusLogProbMetric: 16.3300 - val_loss: 17.0306 - val_MinusLogProbMetric: 17.0306 - lr: 2.6042e-06 - 89s/epoch - 453ms/step
Epoch 854/1000
2023-09-28 19:10:03.493 
Epoch 854/1000 
	 loss: 16.3292, MinusLogProbMetric: 16.3292, val_loss: 17.0292, val_MinusLogProbMetric: 17.0292

Epoch 854: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3292 - MinusLogProbMetric: 16.3292 - val_loss: 17.0292 - val_MinusLogProbMetric: 17.0292 - lr: 2.6042e-06 - 87s/epoch - 441ms/step
Epoch 855/1000
2023-09-28 19:11:30.649 
Epoch 855/1000 
	 loss: 16.3302, MinusLogProbMetric: 16.3302, val_loss: 17.0308, val_MinusLogProbMetric: 17.0308

Epoch 855: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3302 - MinusLogProbMetric: 16.3302 - val_loss: 17.0308 - val_MinusLogProbMetric: 17.0308 - lr: 2.6042e-06 - 87s/epoch - 445ms/step
Epoch 856/1000
2023-09-28 19:12:58.062 
Epoch 856/1000 
	 loss: 16.3296, MinusLogProbMetric: 16.3296, val_loss: 17.0309, val_MinusLogProbMetric: 17.0309

Epoch 856: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3296 - MinusLogProbMetric: 16.3296 - val_loss: 17.0309 - val_MinusLogProbMetric: 17.0309 - lr: 2.6042e-06 - 87s/epoch - 446ms/step
Epoch 857/1000
2023-09-28 19:14:25.588 
Epoch 857/1000 
	 loss: 16.3299, MinusLogProbMetric: 16.3299, val_loss: 17.0289, val_MinusLogProbMetric: 17.0289

Epoch 857: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3299 - MinusLogProbMetric: 16.3299 - val_loss: 17.0289 - val_MinusLogProbMetric: 17.0289 - lr: 2.6042e-06 - 88s/epoch - 447ms/step
Epoch 858/1000
2023-09-28 19:15:52.937 
Epoch 858/1000 
	 loss: 16.3292, MinusLogProbMetric: 16.3292, val_loss: 17.0302, val_MinusLogProbMetric: 17.0302

Epoch 858: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3292 - MinusLogProbMetric: 16.3292 - val_loss: 17.0302 - val_MinusLogProbMetric: 17.0302 - lr: 2.6042e-06 - 87s/epoch - 446ms/step
Epoch 859/1000
2023-09-28 19:17:20.679 
Epoch 859/1000 
	 loss: 16.3299, MinusLogProbMetric: 16.3299, val_loss: 17.0301, val_MinusLogProbMetric: 17.0301

Epoch 859: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3299 - MinusLogProbMetric: 16.3299 - val_loss: 17.0301 - val_MinusLogProbMetric: 17.0301 - lr: 2.6042e-06 - 88s/epoch - 448ms/step
Epoch 860/1000
2023-09-28 19:18:46.076 
Epoch 860/1000 
	 loss: 16.3294, MinusLogProbMetric: 16.3294, val_loss: 17.0298, val_MinusLogProbMetric: 17.0298

Epoch 860: val_loss did not improve from 17.02739
196/196 - 85s - loss: 16.3294 - MinusLogProbMetric: 16.3294 - val_loss: 17.0298 - val_MinusLogProbMetric: 17.0298 - lr: 2.6042e-06 - 85s/epoch - 436ms/step
Epoch 861/1000
2023-09-28 19:20:13.128 
Epoch 861/1000 
	 loss: 16.3296, MinusLogProbMetric: 16.3296, val_loss: 17.0286, val_MinusLogProbMetric: 17.0286

Epoch 861: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3296 - MinusLogProbMetric: 16.3296 - val_loss: 17.0286 - val_MinusLogProbMetric: 17.0286 - lr: 2.6042e-06 - 87s/epoch - 444ms/step
Epoch 862/1000
2023-09-28 19:21:40.379 
Epoch 862/1000 
	 loss: 16.3293, MinusLogProbMetric: 16.3293, val_loss: 17.0320, val_MinusLogProbMetric: 17.0320

Epoch 862: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3293 - MinusLogProbMetric: 16.3293 - val_loss: 17.0320 - val_MinusLogProbMetric: 17.0320 - lr: 2.6042e-06 - 87s/epoch - 445ms/step
Epoch 863/1000
2023-09-28 19:23:08.486 
Epoch 863/1000 
	 loss: 16.3297, MinusLogProbMetric: 16.3297, val_loss: 17.0304, val_MinusLogProbMetric: 17.0304

Epoch 863: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3297 - MinusLogProbMetric: 16.3297 - val_loss: 17.0304 - val_MinusLogProbMetric: 17.0304 - lr: 2.6042e-06 - 88s/epoch - 449ms/step
Epoch 864/1000
2023-09-28 19:24:35.473 
Epoch 864/1000 
	 loss: 16.3295, MinusLogProbMetric: 16.3295, val_loss: 17.0311, val_MinusLogProbMetric: 17.0311

Epoch 864: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3295 - MinusLogProbMetric: 16.3295 - val_loss: 17.0311 - val_MinusLogProbMetric: 17.0311 - lr: 2.6042e-06 - 87s/epoch - 444ms/step
Epoch 865/1000
2023-09-28 19:26:02.107 
Epoch 865/1000 
	 loss: 16.3292, MinusLogProbMetric: 16.3292, val_loss: 17.0318, val_MinusLogProbMetric: 17.0318

Epoch 865: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3292 - MinusLogProbMetric: 16.3292 - val_loss: 17.0318 - val_MinusLogProbMetric: 17.0318 - lr: 2.6042e-06 - 87s/epoch - 442ms/step
Epoch 866/1000
2023-09-28 19:27:28.774 
Epoch 866/1000 
	 loss: 16.3296, MinusLogProbMetric: 16.3296, val_loss: 17.0342, val_MinusLogProbMetric: 17.0342

Epoch 866: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3296 - MinusLogProbMetric: 16.3296 - val_loss: 17.0342 - val_MinusLogProbMetric: 17.0342 - lr: 2.6042e-06 - 87s/epoch - 442ms/step
Epoch 867/1000
2023-09-28 19:28:55.114 
Epoch 867/1000 
	 loss: 16.3284, MinusLogProbMetric: 16.3284, val_loss: 17.0292, val_MinusLogProbMetric: 17.0292

Epoch 867: val_loss did not improve from 17.02739
196/196 - 86s - loss: 16.3284 - MinusLogProbMetric: 16.3284 - val_loss: 17.0292 - val_MinusLogProbMetric: 17.0292 - lr: 2.6042e-06 - 86s/epoch - 440ms/step
Epoch 868/1000
2023-09-28 19:30:20.605 
Epoch 868/1000 
	 loss: 16.3289, MinusLogProbMetric: 16.3289, val_loss: 17.0326, val_MinusLogProbMetric: 17.0326

Epoch 868: val_loss did not improve from 17.02739
196/196 - 85s - loss: 16.3289 - MinusLogProbMetric: 16.3289 - val_loss: 17.0326 - val_MinusLogProbMetric: 17.0326 - lr: 2.6042e-06 - 85s/epoch - 436ms/step
Epoch 869/1000
2023-09-28 19:31:43.879 
Epoch 869/1000 
	 loss: 16.3289, MinusLogProbMetric: 16.3289, val_loss: 17.0285, val_MinusLogProbMetric: 17.0285

Epoch 869: val_loss did not improve from 17.02739
196/196 - 83s - loss: 16.3289 - MinusLogProbMetric: 16.3289 - val_loss: 17.0285 - val_MinusLogProbMetric: 17.0285 - lr: 2.6042e-06 - 83s/epoch - 425ms/step
Epoch 870/1000
2023-09-28 19:33:08.598 
Epoch 870/1000 
	 loss: 16.3288, MinusLogProbMetric: 16.3288, val_loss: 17.0307, val_MinusLogProbMetric: 17.0307

Epoch 870: val_loss did not improve from 17.02739
196/196 - 85s - loss: 16.3288 - MinusLogProbMetric: 16.3288 - val_loss: 17.0307 - val_MinusLogProbMetric: 17.0307 - lr: 2.6042e-06 - 85s/epoch - 432ms/step
Epoch 871/1000
2023-09-28 19:34:31.496 
Epoch 871/1000 
	 loss: 16.3294, MinusLogProbMetric: 16.3294, val_loss: 17.0299, val_MinusLogProbMetric: 17.0299

Epoch 871: val_loss did not improve from 17.02739
196/196 - 83s - loss: 16.3294 - MinusLogProbMetric: 16.3294 - val_loss: 17.0299 - val_MinusLogProbMetric: 17.0299 - lr: 2.6042e-06 - 83s/epoch - 423ms/step
Epoch 872/1000
2023-09-28 19:35:56.061 
Epoch 872/1000 
	 loss: 16.3297, MinusLogProbMetric: 16.3297, val_loss: 17.0306, val_MinusLogProbMetric: 17.0306

Epoch 872: val_loss did not improve from 17.02739
196/196 - 85s - loss: 16.3297 - MinusLogProbMetric: 16.3297 - val_loss: 17.0306 - val_MinusLogProbMetric: 17.0306 - lr: 2.6042e-06 - 85s/epoch - 431ms/step
Epoch 873/1000
2023-09-28 19:37:18.794 
Epoch 873/1000 
	 loss: 16.3296, MinusLogProbMetric: 16.3296, val_loss: 17.0287, val_MinusLogProbMetric: 17.0287

Epoch 873: val_loss did not improve from 17.02739
196/196 - 83s - loss: 16.3296 - MinusLogProbMetric: 16.3296 - val_loss: 17.0287 - val_MinusLogProbMetric: 17.0287 - lr: 2.6042e-06 - 83s/epoch - 422ms/step
Epoch 874/1000
2023-09-28 19:38:38.710 
Epoch 874/1000 
	 loss: 16.3295, MinusLogProbMetric: 16.3295, val_loss: 17.0307, val_MinusLogProbMetric: 17.0307

Epoch 874: val_loss did not improve from 17.02739
196/196 - 80s - loss: 16.3295 - MinusLogProbMetric: 16.3295 - val_loss: 17.0307 - val_MinusLogProbMetric: 17.0307 - lr: 2.6042e-06 - 80s/epoch - 408ms/step
Epoch 875/1000
2023-09-28 19:39:56.828 
Epoch 875/1000 
	 loss: 16.3295, MinusLogProbMetric: 16.3295, val_loss: 17.0297, val_MinusLogProbMetric: 17.0297

Epoch 875: val_loss did not improve from 17.02739
196/196 - 78s - loss: 16.3295 - MinusLogProbMetric: 16.3295 - val_loss: 17.0297 - val_MinusLogProbMetric: 17.0297 - lr: 2.6042e-06 - 78s/epoch - 399ms/step
Epoch 876/1000
2023-09-28 19:41:11.032 
Epoch 876/1000 
	 loss: 16.3293, MinusLogProbMetric: 16.3293, val_loss: 17.0281, val_MinusLogProbMetric: 17.0281

Epoch 876: val_loss did not improve from 17.02739
196/196 - 74s - loss: 16.3293 - MinusLogProbMetric: 16.3293 - val_loss: 17.0281 - val_MinusLogProbMetric: 17.0281 - lr: 2.6042e-06 - 74s/epoch - 379ms/step
Epoch 877/1000
2023-09-28 19:42:30.101 
Epoch 877/1000 
	 loss: 16.3286, MinusLogProbMetric: 16.3286, val_loss: 17.0288, val_MinusLogProbMetric: 17.0288

Epoch 877: val_loss did not improve from 17.02739
196/196 - 79s - loss: 16.3286 - MinusLogProbMetric: 16.3286 - val_loss: 17.0288 - val_MinusLogProbMetric: 17.0288 - lr: 2.6042e-06 - 79s/epoch - 403ms/step
Epoch 878/1000
2023-09-28 19:43:46.385 
Epoch 878/1000 
	 loss: 16.3298, MinusLogProbMetric: 16.3298, val_loss: 17.0295, val_MinusLogProbMetric: 17.0295

Epoch 878: val_loss did not improve from 17.02739
196/196 - 76s - loss: 16.3298 - MinusLogProbMetric: 16.3298 - val_loss: 17.0295 - val_MinusLogProbMetric: 17.0295 - lr: 2.6042e-06 - 76s/epoch - 389ms/step
Epoch 879/1000
2023-09-28 19:45:02.058 
Epoch 879/1000 
	 loss: 16.3290, MinusLogProbMetric: 16.3290, val_loss: 17.0306, val_MinusLogProbMetric: 17.0306

Epoch 879: val_loss did not improve from 17.02739
196/196 - 76s - loss: 16.3290 - MinusLogProbMetric: 16.3290 - val_loss: 17.0306 - val_MinusLogProbMetric: 17.0306 - lr: 2.6042e-06 - 76s/epoch - 386ms/step
Epoch 880/1000
2023-09-28 19:46:24.432 
Epoch 880/1000 
	 loss: 16.3287, MinusLogProbMetric: 16.3287, val_loss: 17.0335, val_MinusLogProbMetric: 17.0335

Epoch 880: val_loss did not improve from 17.02739
196/196 - 82s - loss: 16.3287 - MinusLogProbMetric: 16.3287 - val_loss: 17.0335 - val_MinusLogProbMetric: 17.0335 - lr: 2.6042e-06 - 82s/epoch - 420ms/step
Epoch 881/1000
2023-09-28 19:47:45.952 
Epoch 881/1000 
	 loss: 16.3290, MinusLogProbMetric: 16.3290, val_loss: 17.0314, val_MinusLogProbMetric: 17.0314

Epoch 881: val_loss did not improve from 17.02739
196/196 - 82s - loss: 16.3290 - MinusLogProbMetric: 16.3290 - val_loss: 17.0314 - val_MinusLogProbMetric: 17.0314 - lr: 2.6042e-06 - 82s/epoch - 416ms/step
Epoch 882/1000
2023-09-28 19:49:05.676 
Epoch 882/1000 
	 loss: 16.3289, MinusLogProbMetric: 16.3289, val_loss: 17.0303, val_MinusLogProbMetric: 17.0303

Epoch 882: val_loss did not improve from 17.02739
196/196 - 80s - loss: 16.3289 - MinusLogProbMetric: 16.3289 - val_loss: 17.0303 - val_MinusLogProbMetric: 17.0303 - lr: 2.6042e-06 - 80s/epoch - 407ms/step
Epoch 883/1000
2023-09-28 19:50:24.971 
Epoch 883/1000 
	 loss: 16.3268, MinusLogProbMetric: 16.3268, val_loss: 17.0294, val_MinusLogProbMetric: 17.0294

Epoch 883: val_loss did not improve from 17.02739
196/196 - 79s - loss: 16.3268 - MinusLogProbMetric: 16.3268 - val_loss: 17.0294 - val_MinusLogProbMetric: 17.0294 - lr: 1.3021e-06 - 79s/epoch - 405ms/step
Epoch 884/1000
2023-09-28 19:51:46.656 
Epoch 884/1000 
	 loss: 16.3272, MinusLogProbMetric: 16.3272, val_loss: 17.0286, val_MinusLogProbMetric: 17.0286

Epoch 884: val_loss did not improve from 17.02739
196/196 - 82s - loss: 16.3272 - MinusLogProbMetric: 16.3272 - val_loss: 17.0286 - val_MinusLogProbMetric: 17.0286 - lr: 1.3021e-06 - 82s/epoch - 417ms/step
Epoch 885/1000
2023-09-28 19:53:06.088 
Epoch 885/1000 
	 loss: 16.3268, MinusLogProbMetric: 16.3268, val_loss: 17.0298, val_MinusLogProbMetric: 17.0298

Epoch 885: val_loss did not improve from 17.02739
196/196 - 79s - loss: 16.3268 - MinusLogProbMetric: 16.3268 - val_loss: 17.0298 - val_MinusLogProbMetric: 17.0298 - lr: 1.3021e-06 - 79s/epoch - 405ms/step
Epoch 886/1000
2023-09-28 19:54:28.537 
Epoch 886/1000 
	 loss: 16.3272, MinusLogProbMetric: 16.3272, val_loss: 17.0279, val_MinusLogProbMetric: 17.0279

Epoch 886: val_loss did not improve from 17.02739
196/196 - 82s - loss: 16.3272 - MinusLogProbMetric: 16.3272 - val_loss: 17.0279 - val_MinusLogProbMetric: 17.0279 - lr: 1.3021e-06 - 82s/epoch - 421ms/step
Epoch 887/1000
2023-09-28 19:55:49.618 
Epoch 887/1000 
	 loss: 16.3269, MinusLogProbMetric: 16.3269, val_loss: 17.0300, val_MinusLogProbMetric: 17.0300

Epoch 887: val_loss did not improve from 17.02739
196/196 - 81s - loss: 16.3269 - MinusLogProbMetric: 16.3269 - val_loss: 17.0300 - val_MinusLogProbMetric: 17.0300 - lr: 1.3021e-06 - 81s/epoch - 414ms/step
Epoch 888/1000
2023-09-28 19:57:12.917 
Epoch 888/1000 
	 loss: 16.3268, MinusLogProbMetric: 16.3268, val_loss: 17.0298, val_MinusLogProbMetric: 17.0298

Epoch 888: val_loss did not improve from 17.02739
196/196 - 83s - loss: 16.3268 - MinusLogProbMetric: 16.3268 - val_loss: 17.0298 - val_MinusLogProbMetric: 17.0298 - lr: 1.3021e-06 - 83s/epoch - 425ms/step
Epoch 889/1000
2023-09-28 19:58:35.416 
Epoch 889/1000 
	 loss: 16.3269, MinusLogProbMetric: 16.3269, val_loss: 17.0293, val_MinusLogProbMetric: 17.0293

Epoch 889: val_loss did not improve from 17.02739
196/196 - 82s - loss: 16.3269 - MinusLogProbMetric: 16.3269 - val_loss: 17.0293 - val_MinusLogProbMetric: 17.0293 - lr: 1.3021e-06 - 82s/epoch - 421ms/step
Epoch 890/1000
2023-09-28 19:59:55.868 
Epoch 890/1000 
	 loss: 16.3266, MinusLogProbMetric: 16.3266, val_loss: 17.0310, val_MinusLogProbMetric: 17.0310

Epoch 890: val_loss did not improve from 17.02739
196/196 - 80s - loss: 16.3266 - MinusLogProbMetric: 16.3266 - val_loss: 17.0310 - val_MinusLogProbMetric: 17.0310 - lr: 1.3021e-06 - 80s/epoch - 410ms/step
Epoch 891/1000
2023-09-28 20:01:19.506 
Epoch 891/1000 
	 loss: 16.3269, MinusLogProbMetric: 16.3269, val_loss: 17.0306, val_MinusLogProbMetric: 17.0306

Epoch 891: val_loss did not improve from 17.02739
196/196 - 84s - loss: 16.3269 - MinusLogProbMetric: 16.3269 - val_loss: 17.0306 - val_MinusLogProbMetric: 17.0306 - lr: 1.3021e-06 - 84s/epoch - 427ms/step
Epoch 892/1000
2023-09-28 20:02:39.428 
Epoch 892/1000 
	 loss: 16.3269, MinusLogProbMetric: 16.3269, val_loss: 17.0297, val_MinusLogProbMetric: 17.0297

Epoch 892: val_loss did not improve from 17.02739
196/196 - 80s - loss: 16.3269 - MinusLogProbMetric: 16.3269 - val_loss: 17.0297 - val_MinusLogProbMetric: 17.0297 - lr: 1.3021e-06 - 80s/epoch - 408ms/step
Epoch 893/1000
2023-09-28 20:04:02.600 
Epoch 893/1000 
	 loss: 16.3272, MinusLogProbMetric: 16.3272, val_loss: 17.0285, val_MinusLogProbMetric: 17.0285

Epoch 893: val_loss did not improve from 17.02739
196/196 - 83s - loss: 16.3272 - MinusLogProbMetric: 16.3272 - val_loss: 17.0285 - val_MinusLogProbMetric: 17.0285 - lr: 1.3021e-06 - 83s/epoch - 424ms/step
Epoch 894/1000
2023-09-28 20:05:24.753 
Epoch 894/1000 
	 loss: 16.3268, MinusLogProbMetric: 16.3268, val_loss: 17.0291, val_MinusLogProbMetric: 17.0291

Epoch 894: val_loss did not improve from 17.02739
196/196 - 82s - loss: 16.3268 - MinusLogProbMetric: 16.3268 - val_loss: 17.0291 - val_MinusLogProbMetric: 17.0291 - lr: 1.3021e-06 - 82s/epoch - 419ms/step
Epoch 895/1000
2023-09-28 20:06:47.035 
Epoch 895/1000 
	 loss: 16.3268, MinusLogProbMetric: 16.3268, val_loss: 17.0289, val_MinusLogProbMetric: 17.0289

Epoch 895: val_loss did not improve from 17.02739
196/196 - 82s - loss: 16.3268 - MinusLogProbMetric: 16.3268 - val_loss: 17.0289 - val_MinusLogProbMetric: 17.0289 - lr: 1.3021e-06 - 82s/epoch - 420ms/step
Epoch 896/1000
2023-09-28 20:08:13.520 
Epoch 896/1000 
	 loss: 16.3265, MinusLogProbMetric: 16.3265, val_loss: 17.0327, val_MinusLogProbMetric: 17.0327

Epoch 896: val_loss did not improve from 17.02739
196/196 - 86s - loss: 16.3265 - MinusLogProbMetric: 16.3265 - val_loss: 17.0327 - val_MinusLogProbMetric: 17.0327 - lr: 1.3021e-06 - 86s/epoch - 441ms/step
Epoch 897/1000
2023-09-28 20:09:39.730 
Epoch 897/1000 
	 loss: 16.3271, MinusLogProbMetric: 16.3271, val_loss: 17.0299, val_MinusLogProbMetric: 17.0299

Epoch 897: val_loss did not improve from 17.02739
196/196 - 86s - loss: 16.3271 - MinusLogProbMetric: 16.3271 - val_loss: 17.0299 - val_MinusLogProbMetric: 17.0299 - lr: 1.3021e-06 - 86s/epoch - 440ms/step
Epoch 898/1000
2023-09-28 20:11:04.872 
Epoch 898/1000 
	 loss: 16.3267, MinusLogProbMetric: 16.3267, val_loss: 17.0304, val_MinusLogProbMetric: 17.0304

Epoch 898: val_loss did not improve from 17.02739
196/196 - 85s - loss: 16.3267 - MinusLogProbMetric: 16.3267 - val_loss: 17.0304 - val_MinusLogProbMetric: 17.0304 - lr: 1.3021e-06 - 85s/epoch - 434ms/step
Epoch 899/1000
2023-09-28 20:12:30.391 
Epoch 899/1000 
	 loss: 16.3264, MinusLogProbMetric: 16.3264, val_loss: 17.0296, val_MinusLogProbMetric: 17.0296

Epoch 899: val_loss did not improve from 17.02739
196/196 - 86s - loss: 16.3264 - MinusLogProbMetric: 16.3264 - val_loss: 17.0296 - val_MinusLogProbMetric: 17.0296 - lr: 1.3021e-06 - 86s/epoch - 436ms/step
Epoch 900/1000
2023-09-28 20:13:57.389 
Epoch 900/1000 
	 loss: 16.3268, MinusLogProbMetric: 16.3268, val_loss: 17.0291, val_MinusLogProbMetric: 17.0291

Epoch 900: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3268 - MinusLogProbMetric: 16.3268 - val_loss: 17.0291 - val_MinusLogProbMetric: 17.0291 - lr: 1.3021e-06 - 87s/epoch - 444ms/step
Epoch 901/1000
2023-09-28 20:15:23.857 
Epoch 901/1000 
	 loss: 16.3271, MinusLogProbMetric: 16.3271, val_loss: 17.0289, val_MinusLogProbMetric: 17.0289

Epoch 901: val_loss did not improve from 17.02739
196/196 - 86s - loss: 16.3271 - MinusLogProbMetric: 16.3271 - val_loss: 17.0289 - val_MinusLogProbMetric: 17.0289 - lr: 1.3021e-06 - 86s/epoch - 441ms/step
Epoch 902/1000
2023-09-28 20:16:51.158 
Epoch 902/1000 
	 loss: 16.3267, MinusLogProbMetric: 16.3267, val_loss: 17.0296, val_MinusLogProbMetric: 17.0296

Epoch 902: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3267 - MinusLogProbMetric: 16.3267 - val_loss: 17.0296 - val_MinusLogProbMetric: 17.0296 - lr: 1.3021e-06 - 87s/epoch - 445ms/step
Epoch 903/1000
2023-09-28 20:18:19.227 
Epoch 903/1000 
	 loss: 16.3268, MinusLogProbMetric: 16.3268, val_loss: 17.0303, val_MinusLogProbMetric: 17.0303

Epoch 903: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3268 - MinusLogProbMetric: 16.3268 - val_loss: 17.0303 - val_MinusLogProbMetric: 17.0303 - lr: 1.3021e-06 - 88s/epoch - 449ms/step
Epoch 904/1000
2023-09-28 20:19:46.014 
Epoch 904/1000 
	 loss: 16.3264, MinusLogProbMetric: 16.3264, val_loss: 17.0302, val_MinusLogProbMetric: 17.0302

Epoch 904: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3264 - MinusLogProbMetric: 16.3264 - val_loss: 17.0302 - val_MinusLogProbMetric: 17.0302 - lr: 1.3021e-06 - 87s/epoch - 443ms/step
Epoch 905/1000
2023-09-28 20:21:14.124 
Epoch 905/1000 
	 loss: 16.3267, MinusLogProbMetric: 16.3267, val_loss: 17.0302, val_MinusLogProbMetric: 17.0302

Epoch 905: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3267 - MinusLogProbMetric: 16.3267 - val_loss: 17.0302 - val_MinusLogProbMetric: 17.0302 - lr: 1.3021e-06 - 88s/epoch - 449ms/step
Epoch 906/1000
2023-09-28 20:22:40.885 
Epoch 906/1000 
	 loss: 16.3266, MinusLogProbMetric: 16.3266, val_loss: 17.0286, val_MinusLogProbMetric: 17.0286

Epoch 906: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3266 - MinusLogProbMetric: 16.3266 - val_loss: 17.0286 - val_MinusLogProbMetric: 17.0286 - lr: 1.3021e-06 - 87s/epoch - 443ms/step
Epoch 907/1000
2023-09-28 20:24:06.875 
Epoch 907/1000 
	 loss: 16.3266, MinusLogProbMetric: 16.3266, val_loss: 17.0291, val_MinusLogProbMetric: 17.0291

Epoch 907: val_loss did not improve from 17.02739
196/196 - 86s - loss: 16.3266 - MinusLogProbMetric: 16.3266 - val_loss: 17.0291 - val_MinusLogProbMetric: 17.0291 - lr: 1.3021e-06 - 86s/epoch - 439ms/step
Epoch 908/1000
2023-09-28 20:25:31.825 
Epoch 908/1000 
	 loss: 16.3265, MinusLogProbMetric: 16.3265, val_loss: 17.0292, val_MinusLogProbMetric: 17.0292

Epoch 908: val_loss did not improve from 17.02739
196/196 - 85s - loss: 16.3265 - MinusLogProbMetric: 16.3265 - val_loss: 17.0292 - val_MinusLogProbMetric: 17.0292 - lr: 1.3021e-06 - 85s/epoch - 433ms/step
Epoch 909/1000
2023-09-28 20:26:59.025 
Epoch 909/1000 
	 loss: 16.3264, MinusLogProbMetric: 16.3264, val_loss: 17.0297, val_MinusLogProbMetric: 17.0297

Epoch 909: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3264 - MinusLogProbMetric: 16.3264 - val_loss: 17.0297 - val_MinusLogProbMetric: 17.0297 - lr: 1.3021e-06 - 87s/epoch - 445ms/step
Epoch 910/1000
2023-09-28 20:28:26.835 
Epoch 910/1000 
	 loss: 16.3274, MinusLogProbMetric: 16.3274, val_loss: 17.0280, val_MinusLogProbMetric: 17.0280

Epoch 910: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3274 - MinusLogProbMetric: 16.3274 - val_loss: 17.0280 - val_MinusLogProbMetric: 17.0280 - lr: 1.3021e-06 - 88s/epoch - 448ms/step
Epoch 911/1000
2023-09-28 20:29:54.437 
Epoch 911/1000 
	 loss: 16.3264, MinusLogProbMetric: 16.3264, val_loss: 17.0292, val_MinusLogProbMetric: 17.0292

Epoch 911: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3264 - MinusLogProbMetric: 16.3264 - val_loss: 17.0292 - val_MinusLogProbMetric: 17.0292 - lr: 1.3021e-06 - 88s/epoch - 447ms/step
Epoch 912/1000
2023-09-28 20:31:21.932 
Epoch 912/1000 
	 loss: 16.3265, MinusLogProbMetric: 16.3265, val_loss: 17.0301, val_MinusLogProbMetric: 17.0301

Epoch 912: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3265 - MinusLogProbMetric: 16.3265 - val_loss: 17.0301 - val_MinusLogProbMetric: 17.0301 - lr: 1.3021e-06 - 87s/epoch - 446ms/step
Epoch 913/1000
2023-09-28 20:32:49.422 
Epoch 913/1000 
	 loss: 16.3264, MinusLogProbMetric: 16.3264, val_loss: 17.0286, val_MinusLogProbMetric: 17.0286

Epoch 913: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3264 - MinusLogProbMetric: 16.3264 - val_loss: 17.0286 - val_MinusLogProbMetric: 17.0286 - lr: 1.3021e-06 - 87s/epoch - 446ms/step
Epoch 914/1000
2023-09-28 20:34:17.338 
Epoch 914/1000 
	 loss: 16.3266, MinusLogProbMetric: 16.3266, val_loss: 17.0293, val_MinusLogProbMetric: 17.0293

Epoch 914: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3266 - MinusLogProbMetric: 16.3266 - val_loss: 17.0293 - val_MinusLogProbMetric: 17.0293 - lr: 1.3021e-06 - 88s/epoch - 449ms/step
Epoch 915/1000
2023-09-28 20:35:45.300 
Epoch 915/1000 
	 loss: 16.3266, MinusLogProbMetric: 16.3266, val_loss: 17.0289, val_MinusLogProbMetric: 17.0289

Epoch 915: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3266 - MinusLogProbMetric: 16.3266 - val_loss: 17.0289 - val_MinusLogProbMetric: 17.0289 - lr: 1.3021e-06 - 88s/epoch - 449ms/step
Epoch 916/1000
2023-09-28 20:37:12.331 
Epoch 916/1000 
	 loss: 16.3262, MinusLogProbMetric: 16.3262, val_loss: 17.0290, val_MinusLogProbMetric: 17.0290

Epoch 916: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3262 - MinusLogProbMetric: 16.3262 - val_loss: 17.0290 - val_MinusLogProbMetric: 17.0290 - lr: 1.3021e-06 - 87s/epoch - 444ms/step
Epoch 917/1000
2023-09-28 20:38:40.101 
Epoch 917/1000 
	 loss: 16.3268, MinusLogProbMetric: 16.3268, val_loss: 17.0288, val_MinusLogProbMetric: 17.0288

Epoch 917: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3268 - MinusLogProbMetric: 16.3268 - val_loss: 17.0288 - val_MinusLogProbMetric: 17.0288 - lr: 1.3021e-06 - 88s/epoch - 448ms/step
Epoch 918/1000
2023-09-28 20:40:07.752 
Epoch 918/1000 
	 loss: 16.3271, MinusLogProbMetric: 16.3271, val_loss: 17.0301, val_MinusLogProbMetric: 17.0301

Epoch 918: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3271 - MinusLogProbMetric: 16.3271 - val_loss: 17.0301 - val_MinusLogProbMetric: 17.0301 - lr: 1.3021e-06 - 88s/epoch - 447ms/step
Epoch 919/1000
2023-09-28 20:41:35.622 
Epoch 919/1000 
	 loss: 16.3264, MinusLogProbMetric: 16.3264, val_loss: 17.0298, val_MinusLogProbMetric: 17.0298

Epoch 919: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3264 - MinusLogProbMetric: 16.3264 - val_loss: 17.0298 - val_MinusLogProbMetric: 17.0298 - lr: 1.3021e-06 - 88s/epoch - 448ms/step
Epoch 920/1000
2023-09-28 20:43:02.519 
Epoch 920/1000 
	 loss: 16.3268, MinusLogProbMetric: 16.3268, val_loss: 17.0285, val_MinusLogProbMetric: 17.0285

Epoch 920: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3268 - MinusLogProbMetric: 16.3268 - val_loss: 17.0285 - val_MinusLogProbMetric: 17.0285 - lr: 1.3021e-06 - 87s/epoch - 443ms/step
Epoch 921/1000
2023-09-28 20:44:28.440 
Epoch 921/1000 
	 loss: 16.3261, MinusLogProbMetric: 16.3261, val_loss: 17.0301, val_MinusLogProbMetric: 17.0301

Epoch 921: val_loss did not improve from 17.02739
196/196 - 86s - loss: 16.3261 - MinusLogProbMetric: 16.3261 - val_loss: 17.0301 - val_MinusLogProbMetric: 17.0301 - lr: 1.3021e-06 - 86s/epoch - 438ms/step
Epoch 922/1000
2023-09-28 20:45:56.326 
Epoch 922/1000 
	 loss: 16.3264, MinusLogProbMetric: 16.3264, val_loss: 17.0288, val_MinusLogProbMetric: 17.0288

Epoch 922: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3264 - MinusLogProbMetric: 16.3264 - val_loss: 17.0288 - val_MinusLogProbMetric: 17.0288 - lr: 1.3021e-06 - 88s/epoch - 448ms/step
Epoch 923/1000
2023-09-28 20:47:23.308 
Epoch 923/1000 
	 loss: 16.3261, MinusLogProbMetric: 16.3261, val_loss: 17.0310, val_MinusLogProbMetric: 17.0310

Epoch 923: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3261 - MinusLogProbMetric: 16.3261 - val_loss: 17.0310 - val_MinusLogProbMetric: 17.0310 - lr: 1.3021e-06 - 87s/epoch - 444ms/step
Epoch 924/1000
2023-09-28 20:48:50.032 
Epoch 924/1000 
	 loss: 16.3263, MinusLogProbMetric: 16.3263, val_loss: 17.0285, val_MinusLogProbMetric: 17.0285

Epoch 924: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3263 - MinusLogProbMetric: 16.3263 - val_loss: 17.0285 - val_MinusLogProbMetric: 17.0285 - lr: 1.3021e-06 - 87s/epoch - 442ms/step
Epoch 925/1000
2023-09-28 20:50:16.977 
Epoch 925/1000 
	 loss: 16.3267, MinusLogProbMetric: 16.3267, val_loss: 17.0292, val_MinusLogProbMetric: 17.0292

Epoch 925: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3267 - MinusLogProbMetric: 16.3267 - val_loss: 17.0292 - val_MinusLogProbMetric: 17.0292 - lr: 1.3021e-06 - 87s/epoch - 444ms/step
Epoch 926/1000
2023-09-28 20:51:43.732 
Epoch 926/1000 
	 loss: 16.3263, MinusLogProbMetric: 16.3263, val_loss: 17.0282, val_MinusLogProbMetric: 17.0282

Epoch 926: val_loss did not improve from 17.02739
196/196 - 87s - loss: 16.3263 - MinusLogProbMetric: 16.3263 - val_loss: 17.0282 - val_MinusLogProbMetric: 17.0282 - lr: 1.3021e-06 - 87s/epoch - 443ms/step
Epoch 927/1000
2023-09-28 20:53:11.521 
Epoch 927/1000 
	 loss: 16.3262, MinusLogProbMetric: 16.3262, val_loss: 17.0281, val_MinusLogProbMetric: 17.0281

Epoch 927: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3262 - MinusLogProbMetric: 16.3262 - val_loss: 17.0281 - val_MinusLogProbMetric: 17.0281 - lr: 1.3021e-06 - 88s/epoch - 448ms/step
Epoch 928/1000
2023-09-28 20:54:39.374 
Epoch 928/1000 
	 loss: 16.3262, MinusLogProbMetric: 16.3262, val_loss: 17.0295, val_MinusLogProbMetric: 17.0295

Epoch 928: val_loss did not improve from 17.02739
196/196 - 88s - loss: 16.3262 - MinusLogProbMetric: 16.3262 - val_loss: 17.0295 - val_MinusLogProbMetric: 17.0295 - lr: 1.3021e-06 - 88s/epoch - 448ms/step
Epoch 929/1000
2023-09-28 20:56:05.614 
Epoch 929/1000 
	 loss: 16.3270, MinusLogProbMetric: 16.3270, val_loss: 17.0307, val_MinusLogProbMetric: 17.0307

Epoch 929: val_loss did not improve from 17.02739
196/196 - 86s - loss: 16.3270 - MinusLogProbMetric: 16.3270 - val_loss: 17.0307 - val_MinusLogProbMetric: 17.0307 - lr: 1.3021e-06 - 86s/epoch - 440ms/step
Epoch 930/1000
2023-09-28 20:57:30.868 
Epoch 930/1000 
	 loss: 16.3265, MinusLogProbMetric: 16.3265, val_loss: 17.0291, val_MinusLogProbMetric: 17.0291

Epoch 930: val_loss did not improve from 17.02739
196/196 - 85s - loss: 16.3265 - MinusLogProbMetric: 16.3265 - val_loss: 17.0291 - val_MinusLogProbMetric: 17.0291 - lr: 1.3021e-06 - 85s/epoch - 435ms/step
Epoch 931/1000
2023-09-28 20:58:53.763 
Epoch 931/1000 
	 loss: 16.3260, MinusLogProbMetric: 16.3260, val_loss: 17.0298, val_MinusLogProbMetric: 17.0298

Epoch 931: val_loss did not improve from 17.02739
196/196 - 83s - loss: 16.3260 - MinusLogProbMetric: 16.3260 - val_loss: 17.0298 - val_MinusLogProbMetric: 17.0298 - lr: 1.3021e-06 - 83s/epoch - 423ms/step
Epoch 932/1000
2023-09-28 21:00:21.371 
Epoch 932/1000 
	 loss: 16.3265, MinusLogProbMetric: 16.3265, val_loss: 17.0295, val_MinusLogProbMetric: 17.0295

Epoch 932: val_loss did not improve from 17.02739
Restoring model weights from the end of the best epoch: 832.
196/196 - 90s - loss: 16.3265 - MinusLogProbMetric: 16.3265 - val_loss: 17.0295 - val_MinusLogProbMetric: 17.0295 - lr: 1.3021e-06 - 90s/epoch - 459ms/step
Epoch 932: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function LRMetric.Test_tf.<locals>.compute_test at 0x7f9168545e10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
LR metric calculation completed in 62.35143975896062 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function KSTest.Test_tf.<locals>.compute_test at 0x7f9168544790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
KS tests calculation completed in 25.140430325001944 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function SWDMetric.Test_tf.<locals>.compute_test at 0x7f9168544c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
SWD metric calculation completed in 26.479290751041844 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:5 out of the last 5 calls to <function FNMetric.Test_tf.<locals>.compute_test at 0x7f95f62f9090> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
FN metric calculation completed in 24.422984405013267 seconds.
Training succeeded with seed 933.
Model trained in 69463.19 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 140.81 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 141.08 s.
===========
Run 318/720 done in 69758.81 s.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

===========
Generating train data for run 327.
===========
Train data generated in 0.36 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_99"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_100 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  1817280   
 yer)                                                            
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f8e04662470>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8e8461aad0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8e8461aad0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8d6c334220>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8dd470fbb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8dd471c610>, <keras.callbacks.ModelCheckpoint object at 0x7f8dd471c6d0>, <keras.callbacks.EarlyStopping object at 0x7f8dd471c940>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8dd471c970>, <keras.callbacks.TerminateOnNaN object at 0x7f8dd471c5b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-09-28 21:02:57.129449
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:05:47.224 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 170s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 170s/epoch - 867ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 327.
===========
Train data generated in 0.42 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_110"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_111 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f9577b2aef0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f957803fbe0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f957803fbe0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8d2d06efb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f9577b55630>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f9577b55ba0>, <keras.callbacks.ModelCheckpoint object at 0x7f9577b55c60>, <keras.callbacks.EarlyStopping object at 0x7f9577b55ed0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f9577b55f00>, <keras.callbacks.TerminateOnNaN object at 0x7f9577b55b40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-09-28 21:06:00.373466
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:08:48.041 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 168s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 168s/epoch - 855ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 327.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_121"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_122 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f8e443c99f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f95cc803340>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f95cc803340>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8d216c5870>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8e444b7af0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8e444b7fd0>, <keras.callbacks.ModelCheckpoint object at 0x7f8dd424c160>, <keras.callbacks.EarlyStopping object at 0x7f8dd424c3d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8dd424c400>, <keras.callbacks.TerminateOnNaN object at 0x7f8dd424c040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-09-28 21:09:01.410843
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:11:54.338 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 173s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 173s/epoch - 880ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 3.703703703703703e-05.
===========
Generating train data for run 327.
===========
Train data generated in 0.37 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_132"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_133 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f955599b610>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f9555675330>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f9555675330>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8ee4929960>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f9555448f10>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f9555449480>, <keras.callbacks.ModelCheckpoint object at 0x7f9555449540>, <keras.callbacks.EarlyStopping object at 0x7f95554497b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f95554497e0>, <keras.callbacks.TerminateOnNaN object at 0x7f9555449420>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-09-28 21:12:11.539620
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:5 out of the last 182676 calls to <function Model.make_train_function.<locals>.train_function at 0x7f8dbc5728c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:15:01.463 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 170s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 170s/epoch - 866ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.2345679012345677e-05.
===========
Generating train data for run 327.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_143"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_144 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f8dec1feda0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f9590eb53f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f9590eb53f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8dcc5af6d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8dec2354e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8dec235a50>, <keras.callbacks.ModelCheckpoint object at 0x7f8dec235b10>, <keras.callbacks.EarlyStopping object at 0x7f8dec235d80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8dec235db0>, <keras.callbacks.TerminateOnNaN object at 0x7f8dec2359f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-09-28 21:15:15.975466
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:6 out of the last 182677 calls to <function Model.make_train_function.<locals>.train_function at 0x7f8ee59b1d80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:18:14.724 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 179s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 179s/epoch - 911ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.115226337448558e-06.
===========
Generating train data for run 327.
===========
Train data generated in 0.41 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_154"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_155 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f8e7e32d750>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8ea981d510>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8ea981d510>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8e7e5bda20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8e7e5b4940>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8e7e5b4eb0>, <keras.callbacks.ModelCheckpoint object at 0x7f8e7e5b4f70>, <keras.callbacks.EarlyStopping object at 0x7f8e7e5b51e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8e7e5b5210>, <keras.callbacks.TerminateOnNaN object at 0x7f8e7e5b4e50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-09-28 21:18:25.955473
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:21:19.707 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 174s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 174s/epoch - 886ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.3717421124828526e-06.
===========
Generating train data for run 327.
===========
Train data generated in 0.44 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_165"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_166 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f8dcc5895d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f958077f6a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f958077f6a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8e465a2110>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f95cca47cd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f95cca60280>, <keras.callbacks.ModelCheckpoint object at 0x7f95cca60340>, <keras.callbacks.EarlyStopping object at 0x7f95cca605b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f95cca605e0>, <keras.callbacks.TerminateOnNaN object at 0x7f95cca60220>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-09-28 21:21:34.823798
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:24:21.095 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 166s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 166s/epoch - 848ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.572473708276175e-07.
===========
Generating train data for run 327.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_176"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_177 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f8ecbe1d690>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8d21d5d030>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8d21d5d030>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8f40553010>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8dcc31e1d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8dcc31d300>, <keras.callbacks.ModelCheckpoint object at 0x7f8dcc31c160>, <keras.callbacks.EarlyStopping object at 0x7f8dcc31fdc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8dcc31d540>, <keras.callbacks.TerminateOnNaN object at 0x7f8dcc31ffa0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-09-28 21:24:38.781345
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:27:25.917 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 167s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 167s/epoch - 852ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.524157902758725e-07.
===========
Generating train data for run 327.
===========
Train data generated in 0.38 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_187"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_188 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f953c7efbe0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f953bee23b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f953bee23b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f953bf59900>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f953bdb3430>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f953bdb39a0>, <keras.callbacks.ModelCheckpoint object at 0x7f953bdb3a60>, <keras.callbacks.EarlyStopping object at 0x7f953bdb3cd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f953bdb3d00>, <keras.callbacks.TerminateOnNaN object at 0x7f953bdb3940>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-09-28 21:27:40.936265
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:30:36.509 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 175s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 175s/epoch - 894ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.0805263425290834e-08.
===========
Generating train data for run 327.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_198"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_199 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f8ea8f4a140>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8d7827d870>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8d7827d870>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8eab194ac0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8d78201de0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8d78202350>, <keras.callbacks.ModelCheckpoint object at 0x7f8d78202410>, <keras.callbacks.EarlyStopping object at 0x7f8d78202680>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8d782026b0>, <keras.callbacks.TerminateOnNaN object at 0x7f8d782022f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-09-28 21:30:51.131341
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:33:42.240 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 171s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 171s/epoch - 872ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.6935087808430278e-08.
===========
Generating train data for run 327.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_327/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_327
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_209"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_210 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f8dd4516950>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8e84248400>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8e84248400>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8ea8363fa0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8ea83b6fe0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_327/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8ea83b7550>, <keras.callbacks.ModelCheckpoint object at 0x7f8ea83b7610>, <keras.callbacks.EarlyStopping object at 0x7f8ea83b7880>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8ea83b78b0>, <keras.callbacks.TerminateOnNaN object at 0x7f8ea83b74f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_327/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 327/720 with hyperparameters:
timestamp = 2023-09-28 21:33:54.234359
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:36:42.297 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7812.1685, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 168s - loss: nan - MinusLogProbMetric: 7812.1685 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 168s/epoch - 857ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 5.645029269476759e-09.
===========
Run 327/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 328.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_328/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_328/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_328
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_220"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_221 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f8e85763dc0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8e7e408700>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8e7e408700>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8ecb4489a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8eab822380>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8eab822e60>, <keras.callbacks.ModelCheckpoint object at 0x7f8eab822fe0>, <keras.callbacks.EarlyStopping object at 0x7f8eab8234c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8eab823520>, <keras.callbacks.TerminateOnNaN object at 0x7f8eab822da0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_328/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 328/720 with hyperparameters:
timestamp = 2023-09-28 21:36:55.624978
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 9: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:39:45.960 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6427.5640, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 170s - loss: nan - MinusLogProbMetric: 6427.5640 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 170s/epoch - 869ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0003333333333333333.
===========
Generating train data for run 328.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_328/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_328/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_328
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_231"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_232 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f951aae3dc0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f951ad8f1c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f951ad8f1c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f951b0f3d60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f951ab12f50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f951ab134c0>, <keras.callbacks.ModelCheckpoint object at 0x7f951ab13580>, <keras.callbacks.EarlyStopping object at 0x7f951ab137f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f951ab13820>, <keras.callbacks.TerminateOnNaN object at 0x7f951ab13460>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_328/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 328/720 with hyperparameters:
timestamp = 2023-09-28 21:39:58.761454
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 24: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:42:54.992 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 5823.1538, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 176s - loss: nan - MinusLogProbMetric: 5823.1538 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 176s/epoch - 898ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 0.0001111111111111111.
===========
Generating train data for run 328.
===========
Train data generated in 0.34 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_328/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_328/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_328
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_242"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_243 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f8e7d7d3f70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8ea90fa350>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8ea90fa350>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8e05c20e80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8e063f4f40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8e063f54b0>, <keras.callbacks.ModelCheckpoint object at 0x7f8e063f5570>, <keras.callbacks.EarlyStopping object at 0x7f8e063f57e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8e063f5810>, <keras.callbacks.TerminateOnNaN object at 0x7f8e063f5450>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_328/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 328/720 with hyperparameters:
timestamp = 2023-09-28 21:43:05.004228
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 91: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:46:36.676 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 4855.0791, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 212s - loss: nan - MinusLogProbMetric: 4855.0791 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 212s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 3.703703703703703e-05.
===========
Generating train data for run 328.
===========
Train data generated in 0.39 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_328/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_328/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_328
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_253"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_254 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f8e7ea67c70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8e058a82b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8e058a82b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8eaa52f0d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8e7ea0ec80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8e7ea0f1f0>, <keras.callbacks.ModelCheckpoint object at 0x7f8e7ea0f2b0>, <keras.callbacks.EarlyStopping object at 0x7f8e7ea0f520>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8e7ea0f550>, <keras.callbacks.TerminateOnNaN object at 0x7f8e7ea0f190>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_328/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 328/720 with hyperparameters:
timestamp = 2023-09-28 21:46:50.529591
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
2023-09-28 21:51:05.628 
Epoch 1/1000 
	 loss: 5400.7017, MinusLogProbMetric: 5400.7017, val_loss: 4167.7773, val_MinusLogProbMetric: 4167.7773

Epoch 1: val_loss improved from inf to 4167.77734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 256s - loss: 5400.7017 - MinusLogProbMetric: 5400.7017 - val_loss: 4167.7773 - val_MinusLogProbMetric: 4167.7773 - lr: 3.7037e-05 - 256s/epoch - 1s/step
Epoch 2/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 146: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 21:52:26.553 
Epoch 2/1000 
	 loss: nan, MinusLogProbMetric: 3711.8545, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 2: val_loss did not improve from 4167.77734
196/196 - 79s - loss: nan - MinusLogProbMetric: 3711.8545 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 79s/epoch - 404ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 1.2345679012345677e-05.
===========
Generating train data for run 328.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_328/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_328/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_328
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_264"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_265 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f951a1df970>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8d6c977dc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8d6c977dc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8d6cce26b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f9519bf6dd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f9519bf7340>, <keras.callbacks.ModelCheckpoint object at 0x7f9519bf7400>, <keras.callbacks.EarlyStopping object at 0x7f9519bf7670>, <keras.callbacks.ReduceLROnPlateau object at 0x7f9519bf76a0>, <keras.callbacks.TerminateOnNaN object at 0x7f9519bf72e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 328/720 with hyperparameters:
timestamp = 2023-09-28 21:52:43.332299
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
2023-09-28 21:57:09.557 
Epoch 1/1000 
	 loss: 3697.7942, MinusLogProbMetric: 3697.7942, val_loss: 3743.6357, val_MinusLogProbMetric: 3743.6357

Epoch 1: val_loss improved from inf to 3743.63574, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 267s - loss: 3697.7942 - MinusLogProbMetric: 3697.7942 - val_loss: 3743.6357 - val_MinusLogProbMetric: 3743.6357 - lr: 1.2346e-05 - 267s/epoch - 1s/step
Epoch 2/1000
2023-09-28 21:58:51.989 
Epoch 2/1000 
	 loss: 3322.9363, MinusLogProbMetric: 3322.9363, val_loss: 3013.6370, val_MinusLogProbMetric: 3013.6370

Epoch 2: val_loss improved from 3743.63574 to 3013.63696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 102s - loss: 3322.9363 - MinusLogProbMetric: 3322.9363 - val_loss: 3013.6370 - val_MinusLogProbMetric: 3013.6370 - lr: 1.2346e-05 - 102s/epoch - 519ms/step
Epoch 3/1000
2023-09-28 22:00:31.758 
Epoch 3/1000 
	 loss: 2828.7876, MinusLogProbMetric: 2828.7876, val_loss: 2605.2695, val_MinusLogProbMetric: 2605.2695

Epoch 3: val_loss improved from 3013.63696 to 2605.26953, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 100s - loss: 2828.7876 - MinusLogProbMetric: 2828.7876 - val_loss: 2605.2695 - val_MinusLogProbMetric: 2605.2695 - lr: 1.2346e-05 - 100s/epoch - 509ms/step
Epoch 4/1000
2023-09-28 22:02:12.315 
Epoch 4/1000 
	 loss: 2253.2773, MinusLogProbMetric: 2253.2773, val_loss: 2186.8264, val_MinusLogProbMetric: 2186.8264

Epoch 4: val_loss improved from 2605.26953 to 2186.82642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 101s - loss: 2253.2773 - MinusLogProbMetric: 2253.2773 - val_loss: 2186.8264 - val_MinusLogProbMetric: 2186.8264 - lr: 1.2346e-05 - 101s/epoch - 514ms/step
Epoch 5/1000
2023-09-28 22:03:53.653 
Epoch 5/1000 
	 loss: 2188.2803, MinusLogProbMetric: 2188.2803, val_loss: 2064.1755, val_MinusLogProbMetric: 2064.1755

Epoch 5: val_loss improved from 2186.82642 to 2064.17554, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 101s - loss: 2188.2803 - MinusLogProbMetric: 2188.2803 - val_loss: 2064.1755 - val_MinusLogProbMetric: 2064.1755 - lr: 1.2346e-05 - 101s/epoch - 517ms/step
Epoch 6/1000
2023-09-28 22:05:40.504 
Epoch 6/1000 
	 loss: 2087.3796, MinusLogProbMetric: 2087.3796, val_loss: 1849.4384, val_MinusLogProbMetric: 1849.4384

Epoch 6: val_loss improved from 2064.17554 to 1849.43835, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 107s - loss: 2087.3796 - MinusLogProbMetric: 2087.3796 - val_loss: 1849.4384 - val_MinusLogProbMetric: 1849.4384 - lr: 1.2346e-05 - 107s/epoch - 545ms/step
Epoch 7/1000
2023-09-28 22:07:28.128 
Epoch 7/1000 
	 loss: 1662.2059, MinusLogProbMetric: 1662.2059, val_loss: 1583.6533, val_MinusLogProbMetric: 1583.6533

Epoch 7: val_loss improved from 1849.43835 to 1583.65332, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 108s - loss: 1662.2059 - MinusLogProbMetric: 1662.2059 - val_loss: 1583.6533 - val_MinusLogProbMetric: 1583.6533 - lr: 1.2346e-05 - 108s/epoch - 552ms/step
Epoch 8/1000
2023-09-28 22:09:16.614 
Epoch 8/1000 
	 loss: 2206.2798, MinusLogProbMetric: 2206.2798, val_loss: 2555.9641, val_MinusLogProbMetric: 2555.9641

Epoch 8: val_loss did not improve from 1583.65332
196/196 - 107s - loss: 2206.2798 - MinusLogProbMetric: 2206.2798 - val_loss: 2555.9641 - val_MinusLogProbMetric: 2555.9641 - lr: 1.2346e-05 - 107s/epoch - 545ms/step
Epoch 9/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 107: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-28 22:10:19.011 
Epoch 9/1000 
	 loss: nan, MinusLogProbMetric: 2790.1924, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 9: val_loss did not improve from 1583.65332
196/196 - 62s - loss: nan - MinusLogProbMetric: 2790.1924 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 62s/epoch - 318ms/step
The loss history contains NaN values.
Training failed: trying again with seed 985772 and lr 4.115226337448558e-06.
===========
Generating train data for run 328.
===========
Train data generated in 0.38 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_328/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 0}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_328/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_328
self.data_kwargs: {'seed': 0}
self.x_data: [[ 6.6721983   2.7232099   6.150583   ...  3.6440284   4.565682
   2.645935  ]
 [ 1.86333     2.9602146   9.095053   ...  6.366897   -0.51058275
   3.6708503 ]
 [ 5.5981846   7.771953    4.990416   ...  1.6793485   6.8585963
   1.4719678 ]
 ...
 [ 1.690249    4.096276    8.408299   ...  5.907632    0.7324666
   3.17104   ]
 [ 1.5949699   2.976067    6.189886   ...  6.072563    1.3667407
   3.7841148 ]
 [ 6.6605377   2.9536047   6.141097   ...  3.8587723   3.7946694
   1.5606189 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_275"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_276 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f8e44ff3eb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8deecea6b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8deecea6b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8e44f631c0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8e1d7c46a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8e1d7c4c10>, <keras.callbacks.ModelCheckpoint object at 0x7f8e1d7c4cd0>, <keras.callbacks.EarlyStopping object at 0x7f8e1d7c4f40>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8e1d7c4f70>, <keras.callbacks.TerminateOnNaN object at 0x7f8e1d7c4bb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.843066  , 6.885543  , 5.4616323 , ..., 0.60136265, 7.6057563 ,
        1.4468012 ],
       [6.941901  , 2.9639573 , 6.1196294 , ..., 3.4089205 , 5.0531244 ,
        1.7321708 ],
       [6.6497912 , 2.6758661 , 6.1924143 , ..., 2.4044154 , 3.5041332 ,
        2.3901901 ],
       ...,
       [5.4960246 , 7.681503  , 5.4965014 , ..., 1.6430786 , 5.8353615 ,
        1.3267541 ],
       [5.6652064 , 7.894577  , 5.7936563 , ..., 1.3511211 , 6.9882894 ,
        1.2434273 ],
       [5.24072   , 7.783675  , 5.8090463 , ..., 1.1913109 , 5.8178825 ,
        1.3373194 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 328/720 with hyperparameters:
timestamp = 2023-09-28 22:10:37.085337
ndims = 64
seed_train = 0
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.6721983   2.7232099   6.150583    4.580923    1.5652926   2.6565456
  7.1790314   4.9339914   5.7110434   6.7498174   6.386396    3.9689803
  8.906224    3.806384    4.1225185   9.359665    7.9177184   7.261933
  0.1027934   9.199222    7.196588   10.08044     2.3912797   8.714102
  2.4626331   6.1591697   1.6348177   8.815128    7.787716    5.5076594
  3.3412566   0.7007286   7.2919664   4.3464866   6.6338673   7.1474853
  9.7648      8.511822   -0.5374965   5.0141335   7.365311    0.6638582
  5.9037857   0.7669884   1.383983    0.33108917  8.1157875   2.689867
  4.5051117   9.846115    7.6886387   0.35349095  2.5627694   4.5428576
  5.8595724   2.7118683   9.446846    6.2642756   4.7844443   6.0548897
  8.103386    3.6440284   4.565682    2.645935  ]
Epoch 1/1000
2023-09-28 22:15:08.710 
Epoch 1/1000 
	 loss: 1623.8491, MinusLogProbMetric: 1623.8491, val_loss: 1614.0444, val_MinusLogProbMetric: 1614.0444

Epoch 1: val_loss improved from inf to 1614.04443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 273s - loss: 1623.8491 - MinusLogProbMetric: 1623.8491 - val_loss: 1614.0444 - val_MinusLogProbMetric: 1614.0444 - lr: 4.1152e-06 - 273s/epoch - 1s/step
Epoch 2/1000
2023-09-28 22:16:58.495 
Epoch 2/1000 
	 loss: 1416.1021, MinusLogProbMetric: 1416.1021, val_loss: 1356.7748, val_MinusLogProbMetric: 1356.7748

Epoch 2: val_loss improved from 1614.04443 to 1356.77478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 109s - loss: 1416.1021 - MinusLogProbMetric: 1416.1021 - val_loss: 1356.7748 - val_MinusLogProbMetric: 1356.7748 - lr: 4.1152e-06 - 109s/epoch - 557ms/step
Epoch 3/1000
2023-09-28 22:18:45.622 
Epoch 3/1000 
	 loss: 1255.4513, MinusLogProbMetric: 1255.4513, val_loss: 1119.3988, val_MinusLogProbMetric: 1119.3988

Epoch 3: val_loss improved from 1356.77478 to 1119.39880, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 107s - loss: 1255.4513 - MinusLogProbMetric: 1255.4513 - val_loss: 1119.3988 - val_MinusLogProbMetric: 1119.3988 - lr: 4.1152e-06 - 107s/epoch - 547ms/step
Epoch 4/1000
2023-09-28 22:20:34.745 
Epoch 4/1000 
	 loss: 1024.9030, MinusLogProbMetric: 1024.9030, val_loss: 962.4678, val_MinusLogProbMetric: 962.4678

Epoch 4: val_loss improved from 1119.39880 to 962.46777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 109s - loss: 1024.9030 - MinusLogProbMetric: 1024.9030 - val_loss: 962.4678 - val_MinusLogProbMetric: 962.4678 - lr: 4.1152e-06 - 109s/epoch - 558ms/step
Epoch 5/1000
2023-09-28 22:22:22.988 
Epoch 5/1000 
	 loss: 978.2055, MinusLogProbMetric: 978.2055, val_loss: 945.7643, val_MinusLogProbMetric: 945.7643

Epoch 5: val_loss improved from 962.46777 to 945.76434, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 108s - loss: 978.2055 - MinusLogProbMetric: 978.2055 - val_loss: 945.7643 - val_MinusLogProbMetric: 945.7643 - lr: 4.1152e-06 - 108s/epoch - 550ms/step
Epoch 6/1000
2023-09-28 22:24:11.553 
Epoch 6/1000 
	 loss: 926.5077, MinusLogProbMetric: 926.5077, val_loss: 902.7036, val_MinusLogProbMetric: 902.7036

Epoch 6: val_loss improved from 945.76434 to 902.70361, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 109s - loss: 926.5077 - MinusLogProbMetric: 926.5077 - val_loss: 902.7036 - val_MinusLogProbMetric: 902.7036 - lr: 4.1152e-06 - 109s/epoch - 555ms/step
Epoch 7/1000
2023-09-28 22:26:00.964 
Epoch 7/1000 
	 loss: 883.1520, MinusLogProbMetric: 883.1520, val_loss: 861.8982, val_MinusLogProbMetric: 861.8982

Epoch 7: val_loss improved from 902.70361 to 861.89819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 109s - loss: 883.1520 - MinusLogProbMetric: 883.1520 - val_loss: 861.8982 - val_MinusLogProbMetric: 861.8982 - lr: 4.1152e-06 - 109s/epoch - 558ms/step
Epoch 8/1000
2023-09-28 22:27:49.991 
Epoch 8/1000 
	 loss: 852.4117, MinusLogProbMetric: 852.4117, val_loss: 850.4508, val_MinusLogProbMetric: 850.4508

Epoch 8: val_loss improved from 861.89819 to 850.45081, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 109s - loss: 852.4117 - MinusLogProbMetric: 852.4117 - val_loss: 850.4508 - val_MinusLogProbMetric: 850.4508 - lr: 4.1152e-06 - 109s/epoch - 556ms/step
Epoch 9/1000
2023-09-28 22:29:39.298 
Epoch 9/1000 
	 loss: 820.2141, MinusLogProbMetric: 820.2141, val_loss: 807.5076, val_MinusLogProbMetric: 807.5076

Epoch 9: val_loss improved from 850.45081 to 807.50763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 109s - loss: 820.2141 - MinusLogProbMetric: 820.2141 - val_loss: 807.5076 - val_MinusLogProbMetric: 807.5076 - lr: 4.1152e-06 - 109s/epoch - 557ms/step
Epoch 10/1000
2023-09-28 22:31:26.726 
Epoch 10/1000 
	 loss: 880.9764, MinusLogProbMetric: 880.9764, val_loss: 893.4940, val_MinusLogProbMetric: 893.4940

Epoch 10: val_loss did not improve from 807.50763
196/196 - 106s - loss: 880.9764 - MinusLogProbMetric: 880.9764 - val_loss: 893.4940 - val_MinusLogProbMetric: 893.4940 - lr: 4.1152e-06 - 106s/epoch - 541ms/step
Epoch 11/1000
2023-09-28 22:33:09.892 
Epoch 11/1000 
	 loss: 846.8998, MinusLogProbMetric: 846.8998, val_loss: 808.6229, val_MinusLogProbMetric: 808.6229

Epoch 11: val_loss did not improve from 807.50763
196/196 - 103s - loss: 846.8998 - MinusLogProbMetric: 846.8998 - val_loss: 808.6229 - val_MinusLogProbMetric: 808.6229 - lr: 4.1152e-06 - 103s/epoch - 526ms/step
Epoch 12/1000
2023-09-28 22:34:53.535 
Epoch 12/1000 
	 loss: 793.4387, MinusLogProbMetric: 793.4387, val_loss: 769.6279, val_MinusLogProbMetric: 769.6279

Epoch 12: val_loss improved from 807.50763 to 769.62793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 105s - loss: 793.4387 - MinusLogProbMetric: 793.4387 - val_loss: 769.6279 - val_MinusLogProbMetric: 769.6279 - lr: 4.1152e-06 - 105s/epoch - 535ms/step
Epoch 13/1000
2023-09-28 22:36:39.433 
Epoch 13/1000 
	 loss: 777.4390, MinusLogProbMetric: 777.4390, val_loss: 793.0439, val_MinusLogProbMetric: 793.0439

Epoch 13: val_loss did not improve from 769.62793
196/196 - 105s - loss: 777.4390 - MinusLogProbMetric: 777.4390 - val_loss: 793.0439 - val_MinusLogProbMetric: 793.0439 - lr: 4.1152e-06 - 105s/epoch - 534ms/step
Epoch 14/1000
2023-09-28 22:38:21.524 
Epoch 14/1000 
	 loss: 762.9382, MinusLogProbMetric: 762.9382, val_loss: 742.8224, val_MinusLogProbMetric: 742.8224

Epoch 14: val_loss improved from 769.62793 to 742.82245, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 103s - loss: 762.9382 - MinusLogProbMetric: 762.9382 - val_loss: 742.8224 - val_MinusLogProbMetric: 742.8224 - lr: 4.1152e-06 - 103s/epoch - 527ms/step
Epoch 15/1000
2023-09-28 22:40:05.261 
Epoch 15/1000 
	 loss: 730.8911, MinusLogProbMetric: 730.8911, val_loss: 713.3606, val_MinusLogProbMetric: 713.3606

Epoch 15: val_loss improved from 742.82245 to 713.36060, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 104s - loss: 730.8911 - MinusLogProbMetric: 730.8911 - val_loss: 713.3606 - val_MinusLogProbMetric: 713.3606 - lr: 4.1152e-06 - 104s/epoch - 531ms/step
Epoch 16/1000
2023-09-28 22:41:49.007 
Epoch 16/1000 
	 loss: 698.8865, MinusLogProbMetric: 698.8865, val_loss: 687.5388, val_MinusLogProbMetric: 687.5388

Epoch 16: val_loss improved from 713.36060 to 687.53882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 103s - loss: 698.8865 - MinusLogProbMetric: 698.8865 - val_loss: 687.5388 - val_MinusLogProbMetric: 687.5388 - lr: 4.1152e-06 - 103s/epoch - 527ms/step
Epoch 17/1000
2023-09-28 22:43:34.405 
Epoch 17/1000 
	 loss: 667.5518, MinusLogProbMetric: 667.5518, val_loss: 656.8840, val_MinusLogProbMetric: 656.8840

Epoch 17: val_loss improved from 687.53882 to 656.88397, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 106s - loss: 667.5518 - MinusLogProbMetric: 667.5518 - val_loss: 656.8840 - val_MinusLogProbMetric: 656.8840 - lr: 4.1152e-06 - 106s/epoch - 538ms/step
Epoch 18/1000
2023-09-28 22:45:22.500 
Epoch 18/1000 
	 loss: 643.5089, MinusLogProbMetric: 643.5089, val_loss: 634.4641, val_MinusLogProbMetric: 634.4641

Epoch 18: val_loss improved from 656.88397 to 634.46411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 108s - loss: 643.5089 - MinusLogProbMetric: 643.5089 - val_loss: 634.4641 - val_MinusLogProbMetric: 634.4641 - lr: 4.1152e-06 - 108s/epoch - 551ms/step
Epoch 19/1000
2023-09-28 22:47:08.190 
Epoch 19/1000 
	 loss: 639.9078, MinusLogProbMetric: 639.9078, val_loss: 660.3876, val_MinusLogProbMetric: 660.3876

Epoch 19: val_loss did not improve from 634.46411
196/196 - 104s - loss: 639.9078 - MinusLogProbMetric: 639.9078 - val_loss: 660.3876 - val_MinusLogProbMetric: 660.3876 - lr: 4.1152e-06 - 104s/epoch - 532ms/step
Epoch 20/1000
2023-09-28 22:48:50.033 
Epoch 20/1000 
	 loss: 643.4626, MinusLogProbMetric: 643.4626, val_loss: 624.3329, val_MinusLogProbMetric: 624.3329

Epoch 20: val_loss improved from 634.46411 to 624.33289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 103s - loss: 643.4626 - MinusLogProbMetric: 643.4626 - val_loss: 624.3329 - val_MinusLogProbMetric: 624.3329 - lr: 4.1152e-06 - 103s/epoch - 527ms/step
Epoch 21/1000
2023-09-28 22:50:35.592 
Epoch 21/1000 
	 loss: 617.4171, MinusLogProbMetric: 617.4171, val_loss: 609.1490, val_MinusLogProbMetric: 609.1490

Epoch 21: val_loss improved from 624.33289 to 609.14905, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 106s - loss: 617.4171 - MinusLogProbMetric: 617.4171 - val_loss: 609.1490 - val_MinusLogProbMetric: 609.1490 - lr: 4.1152e-06 - 106s/epoch - 538ms/step
Epoch 22/1000
2023-09-28 22:52:21.094 
Epoch 22/1000 
	 loss: 604.1765, MinusLogProbMetric: 604.1765, val_loss: 599.4423, val_MinusLogProbMetric: 599.4423

Epoch 22: val_loss improved from 609.14905 to 599.44232, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 105s - loss: 604.1765 - MinusLogProbMetric: 604.1765 - val_loss: 599.4423 - val_MinusLogProbMetric: 599.4423 - lr: 4.1152e-06 - 105s/epoch - 537ms/step
Epoch 23/1000
2023-09-28 22:54:06.236 
Epoch 23/1000 
	 loss: 594.9672, MinusLogProbMetric: 594.9672, val_loss: 588.7672, val_MinusLogProbMetric: 588.7672

Epoch 23: val_loss improved from 599.44232 to 588.76721, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 105s - loss: 594.9672 - MinusLogProbMetric: 594.9672 - val_loss: 588.7672 - val_MinusLogProbMetric: 588.7672 - lr: 4.1152e-06 - 105s/epoch - 537ms/step
Epoch 24/1000
2023-09-28 22:55:47.695 
Epoch 24/1000 
	 loss: 588.4487, MinusLogProbMetric: 588.4487, val_loss: 585.2805, val_MinusLogProbMetric: 585.2805

Epoch 24: val_loss improved from 588.76721 to 585.28046, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 101s - loss: 588.4487 - MinusLogProbMetric: 588.4487 - val_loss: 585.2805 - val_MinusLogProbMetric: 585.2805 - lr: 4.1152e-06 - 101s/epoch - 517ms/step
Epoch 25/1000
2023-09-28 22:57:31.169 
Epoch 25/1000 
	 loss: 584.8524, MinusLogProbMetric: 584.8524, val_loss: 574.2517, val_MinusLogProbMetric: 574.2517

Epoch 25: val_loss improved from 585.28046 to 574.25171, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 103s - loss: 584.8524 - MinusLogProbMetric: 584.8524 - val_loss: 574.2517 - val_MinusLogProbMetric: 574.2517 - lr: 4.1152e-06 - 103s/epoch - 527ms/step
Epoch 26/1000
2023-09-28 22:59:13.204 
Epoch 26/1000 
	 loss: 572.2363, MinusLogProbMetric: 572.2363, val_loss: 571.2136, val_MinusLogProbMetric: 571.2136

Epoch 26: val_loss improved from 574.25171 to 571.21362, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 102s - loss: 572.2363 - MinusLogProbMetric: 572.2363 - val_loss: 571.2136 - val_MinusLogProbMetric: 571.2136 - lr: 4.1152e-06 - 102s/epoch - 522ms/step
Epoch 27/1000
2023-09-28 23:00:53.038 
Epoch 27/1000 
	 loss: 565.8533, MinusLogProbMetric: 565.8533, val_loss: 561.3265, val_MinusLogProbMetric: 561.3265

Epoch 27: val_loss improved from 571.21362 to 561.32654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 100s - loss: 565.8533 - MinusLogProbMetric: 565.8533 - val_loss: 561.3265 - val_MinusLogProbMetric: 561.3265 - lr: 4.1152e-06 - 100s/epoch - 508ms/step
Epoch 28/1000
2023-09-28 23:02:32.827 
Epoch 28/1000 
	 loss: 558.9133, MinusLogProbMetric: 558.9133, val_loss: 556.0593, val_MinusLogProbMetric: 556.0593

Epoch 28: val_loss improved from 561.32654 to 556.05933, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 100s - loss: 558.9133 - MinusLogProbMetric: 558.9133 - val_loss: 556.0593 - val_MinusLogProbMetric: 556.0593 - lr: 4.1152e-06 - 100s/epoch - 509ms/step
Epoch 29/1000
2023-09-28 23:04:16.848 
Epoch 29/1000 
	 loss: 554.2347, MinusLogProbMetric: 554.2347, val_loss: 550.8013, val_MinusLogProbMetric: 550.8013

Epoch 29: val_loss improved from 556.05933 to 550.80127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 104s - loss: 554.2347 - MinusLogProbMetric: 554.2347 - val_loss: 550.8013 - val_MinusLogProbMetric: 550.8013 - lr: 4.1152e-06 - 104s/epoch - 530ms/step
Epoch 30/1000
2023-09-28 23:05:58.385 
Epoch 30/1000 
	 loss: 549.0739, MinusLogProbMetric: 549.0739, val_loss: 546.4768, val_MinusLogProbMetric: 546.4768

Epoch 30: val_loss improved from 550.80127 to 546.47681, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 102s - loss: 549.0739 - MinusLogProbMetric: 549.0739 - val_loss: 546.4768 - val_MinusLogProbMetric: 546.4768 - lr: 4.1152e-06 - 102s/epoch - 518ms/step
Epoch 31/1000
2023-09-28 23:07:38.616 
Epoch 31/1000 
	 loss: 544.3537, MinusLogProbMetric: 544.3537, val_loss: 541.7505, val_MinusLogProbMetric: 541.7505

Epoch 31: val_loss improved from 546.47681 to 541.75055, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 100s - loss: 544.3537 - MinusLogProbMetric: 544.3537 - val_loss: 541.7505 - val_MinusLogProbMetric: 541.7505 - lr: 4.1152e-06 - 100s/epoch - 511ms/step
Epoch 32/1000
2023-09-28 23:09:19.954 
Epoch 32/1000 
	 loss: 539.5154, MinusLogProbMetric: 539.5154, val_loss: 537.1661, val_MinusLogProbMetric: 537.1661

Epoch 32: val_loss improved from 541.75055 to 537.16614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 101s - loss: 539.5154 - MinusLogProbMetric: 539.5154 - val_loss: 537.1661 - val_MinusLogProbMetric: 537.1661 - lr: 4.1152e-06 - 101s/epoch - 517ms/step
Epoch 33/1000
2023-09-28 23:10:59.457 
Epoch 33/1000 
	 loss: 537.2812, MinusLogProbMetric: 537.2812, val_loss: 542.4899, val_MinusLogProbMetric: 542.4899

Epoch 33: val_loss did not improve from 537.16614
196/196 - 99s - loss: 537.2812 - MinusLogProbMetric: 537.2812 - val_loss: 542.4899 - val_MinusLogProbMetric: 542.4899 - lr: 4.1152e-06 - 99s/epoch - 503ms/step
Epoch 34/1000
2023-09-28 23:12:44.794 
Epoch 34/1000 
	 loss: 535.8416, MinusLogProbMetric: 535.8416, val_loss: 534.2845, val_MinusLogProbMetric: 534.2845

Epoch 34: val_loss improved from 537.16614 to 534.28455, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 106s - loss: 535.8416 - MinusLogProbMetric: 535.8416 - val_loss: 534.2845 - val_MinusLogProbMetric: 534.2845 - lr: 4.1152e-06 - 106s/epoch - 543ms/step
Epoch 35/1000
2023-09-28 23:14:31.301 
Epoch 35/1000 
	 loss: 529.7188, MinusLogProbMetric: 529.7188, val_loss: 526.7853, val_MinusLogProbMetric: 526.7853

Epoch 35: val_loss improved from 534.28455 to 526.78528, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 106s - loss: 529.7188 - MinusLogProbMetric: 529.7188 - val_loss: 526.7853 - val_MinusLogProbMetric: 526.7853 - lr: 4.1152e-06 - 106s/epoch - 543ms/step
Epoch 36/1000
2023-09-28 23:16:11.530 
Epoch 36/1000 
	 loss: 529.7851, MinusLogProbMetric: 529.7851, val_loss: 527.0363, val_MinusLogProbMetric: 527.0363

Epoch 36: val_loss did not improve from 526.78528
196/196 - 99s - loss: 529.7851 - MinusLogProbMetric: 529.7851 - val_loss: 527.0363 - val_MinusLogProbMetric: 527.0363 - lr: 4.1152e-06 - 99s/epoch - 506ms/step
Epoch 37/1000
2023-09-28 23:17:51.516 
Epoch 37/1000 
	 loss: 530.0336, MinusLogProbMetric: 530.0336, val_loss: 524.2768, val_MinusLogProbMetric: 524.2768

Epoch 37: val_loss improved from 526.78528 to 524.27679, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 101s - loss: 530.0336 - MinusLogProbMetric: 530.0336 - val_loss: 524.2768 - val_MinusLogProbMetric: 524.2768 - lr: 4.1152e-06 - 101s/epoch - 516ms/step
Epoch 38/1000
2023-09-28 23:19:38.754 
Epoch 38/1000 
	 loss: 521.5874, MinusLogProbMetric: 521.5874, val_loss: 519.0937, val_MinusLogProbMetric: 519.0937

Epoch 38: val_loss improved from 524.27679 to 519.09369, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 107s - loss: 521.5874 - MinusLogProbMetric: 521.5874 - val_loss: 519.0937 - val_MinusLogProbMetric: 519.0937 - lr: 4.1152e-06 - 107s/epoch - 547ms/step
Epoch 39/1000
2023-09-28 23:21:21.304 
Epoch 39/1000 
	 loss: 808.6045, MinusLogProbMetric: 808.6045, val_loss: 874.1044, val_MinusLogProbMetric: 874.1044

Epoch 39: val_loss did not improve from 519.09369
196/196 - 101s - loss: 808.6045 - MinusLogProbMetric: 808.6045 - val_loss: 874.1044 - val_MinusLogProbMetric: 874.1044 - lr: 4.1152e-06 - 101s/epoch - 517ms/step
Epoch 40/1000
2023-09-28 23:22:56.907 
Epoch 40/1000 
	 loss: 749.6100, MinusLogProbMetric: 749.6100, val_loss: 697.2947, val_MinusLogProbMetric: 697.2947

Epoch 40: val_loss did not improve from 519.09369
196/196 - 96s - loss: 749.6100 - MinusLogProbMetric: 749.6100 - val_loss: 697.2947 - val_MinusLogProbMetric: 697.2947 - lr: 4.1152e-06 - 96s/epoch - 488ms/step
Epoch 41/1000
2023-09-28 23:24:39.168 
Epoch 41/1000 
	 loss: 648.4747, MinusLogProbMetric: 648.4747, val_loss: 736.8257, val_MinusLogProbMetric: 736.8257

Epoch 41: val_loss did not improve from 519.09369
196/196 - 102s - loss: 648.4747 - MinusLogProbMetric: 648.4747 - val_loss: 736.8257 - val_MinusLogProbMetric: 736.8257 - lr: 4.1152e-06 - 102s/epoch - 522ms/step
Epoch 42/1000
2023-09-28 23:26:19.592 
Epoch 42/1000 
	 loss: 883.5997, MinusLogProbMetric: 883.5997, val_loss: 928.2172, val_MinusLogProbMetric: 928.2172

Epoch 42: val_loss did not improve from 519.09369
196/196 - 100s - loss: 883.5997 - MinusLogProbMetric: 883.5997 - val_loss: 928.2172 - val_MinusLogProbMetric: 928.2172 - lr: 4.1152e-06 - 100s/epoch - 512ms/step
Epoch 43/1000
2023-09-28 23:27:58.900 
Epoch 43/1000 
	 loss: 835.8898, MinusLogProbMetric: 835.8898, val_loss: 768.2260, val_MinusLogProbMetric: 768.2260

Epoch 43: val_loss did not improve from 519.09369
196/196 - 99s - loss: 835.8898 - MinusLogProbMetric: 835.8898 - val_loss: 768.2260 - val_MinusLogProbMetric: 768.2260 - lr: 4.1152e-06 - 99s/epoch - 507ms/step
Epoch 44/1000
2023-09-28 23:29:41.912 
Epoch 44/1000 
	 loss: 706.7255, MinusLogProbMetric: 706.7255, val_loss: 666.9226, val_MinusLogProbMetric: 666.9226

Epoch 44: val_loss did not improve from 519.09369
196/196 - 103s - loss: 706.7255 - MinusLogProbMetric: 706.7255 - val_loss: 666.9226 - val_MinusLogProbMetric: 666.9226 - lr: 4.1152e-06 - 103s/epoch - 526ms/step
Epoch 45/1000
2023-09-28 23:31:30.217 
Epoch 45/1000 
	 loss: 648.4040, MinusLogProbMetric: 648.4040, val_loss: 622.6345, val_MinusLogProbMetric: 622.6345

Epoch 45: val_loss did not improve from 519.09369
196/196 - 108s - loss: 648.4040 - MinusLogProbMetric: 648.4040 - val_loss: 622.6345 - val_MinusLogProbMetric: 622.6345 - lr: 4.1152e-06 - 108s/epoch - 553ms/step
Epoch 46/1000
2023-09-28 23:33:14.497 
Epoch 46/1000 
	 loss: 614.0353, MinusLogProbMetric: 614.0353, val_loss: 604.9933, val_MinusLogProbMetric: 604.9933

Epoch 46: val_loss did not improve from 519.09369
196/196 - 104s - loss: 614.0353 - MinusLogProbMetric: 614.0353 - val_loss: 604.9933 - val_MinusLogProbMetric: 604.9933 - lr: 4.1152e-06 - 104s/epoch - 532ms/step
Epoch 47/1000
2023-09-28 23:35:00.734 
Epoch 47/1000 
	 loss: 597.3765, MinusLogProbMetric: 597.3765, val_loss: 589.9803, val_MinusLogProbMetric: 589.9803

Epoch 47: val_loss did not improve from 519.09369
196/196 - 106s - loss: 597.3765 - MinusLogProbMetric: 597.3765 - val_loss: 589.9803 - val_MinusLogProbMetric: 589.9803 - lr: 4.1152e-06 - 106s/epoch - 542ms/step
Epoch 48/1000
2023-09-28 23:36:47.968 
Epoch 48/1000 
	 loss: 584.0354, MinusLogProbMetric: 584.0354, val_loss: 578.4523, val_MinusLogProbMetric: 578.4523

Epoch 48: val_loss did not improve from 519.09369
196/196 - 107s - loss: 584.0354 - MinusLogProbMetric: 584.0354 - val_loss: 578.4523 - val_MinusLogProbMetric: 578.4523 - lr: 4.1152e-06 - 107s/epoch - 547ms/step
Epoch 49/1000
2023-09-28 23:38:33.976 
Epoch 49/1000 
	 loss: 574.9003, MinusLogProbMetric: 574.9003, val_loss: 569.7770, val_MinusLogProbMetric: 569.7770

Epoch 49: val_loss did not improve from 519.09369
196/196 - 106s - loss: 574.9003 - MinusLogProbMetric: 574.9003 - val_loss: 569.7770 - val_MinusLogProbMetric: 569.7770 - lr: 4.1152e-06 - 106s/epoch - 541ms/step
Epoch 50/1000
2023-09-28 23:40:21.997 
Epoch 50/1000 
	 loss: 569.3914, MinusLogProbMetric: 569.3914, val_loss: 565.1404, val_MinusLogProbMetric: 565.1404

Epoch 50: val_loss did not improve from 519.09369
196/196 - 108s - loss: 569.3914 - MinusLogProbMetric: 569.3914 - val_loss: 565.1404 - val_MinusLogProbMetric: 565.1404 - lr: 4.1152e-06 - 108s/epoch - 551ms/step
Epoch 51/1000
2023-09-28 23:42:10.192 
Epoch 51/1000 
	 loss: 560.4872, MinusLogProbMetric: 560.4872, val_loss: 556.3871, val_MinusLogProbMetric: 556.3871

Epoch 51: val_loss did not improve from 519.09369
196/196 - 108s - loss: 560.4872 - MinusLogProbMetric: 560.4872 - val_loss: 556.3871 - val_MinusLogProbMetric: 556.3871 - lr: 4.1152e-06 - 108s/epoch - 552ms/step
Epoch 52/1000
2023-09-28 23:43:56.754 
Epoch 52/1000 
	 loss: 554.7347, MinusLogProbMetric: 554.7347, val_loss: 554.1799, val_MinusLogProbMetric: 554.1799

Epoch 52: val_loss did not improve from 519.09369
196/196 - 107s - loss: 554.7347 - MinusLogProbMetric: 554.7347 - val_loss: 554.1799 - val_MinusLogProbMetric: 554.1799 - lr: 4.1152e-06 - 107s/epoch - 544ms/step
Epoch 53/1000
2023-09-28 23:45:43.606 
Epoch 53/1000 
	 loss: 785.2713, MinusLogProbMetric: 785.2713, val_loss: 805.3570, val_MinusLogProbMetric: 805.3570

Epoch 53: val_loss did not improve from 519.09369
196/196 - 107s - loss: 785.2713 - MinusLogProbMetric: 785.2713 - val_loss: 805.3570 - val_MinusLogProbMetric: 805.3570 - lr: 4.1152e-06 - 107s/epoch - 545ms/step
Epoch 54/1000
2023-09-28 23:47:30.118 
Epoch 54/1000 
	 loss: 750.3906, MinusLogProbMetric: 750.3906, val_loss: 780.3018, val_MinusLogProbMetric: 780.3018

Epoch 54: val_loss did not improve from 519.09369
196/196 - 107s - loss: 750.3906 - MinusLogProbMetric: 750.3906 - val_loss: 780.3018 - val_MinusLogProbMetric: 780.3018 - lr: 4.1152e-06 - 107s/epoch - 543ms/step
Epoch 55/1000
2023-09-28 23:49:18.048 
Epoch 55/1000 
	 loss: 839.6636, MinusLogProbMetric: 839.6636, val_loss: 736.9936, val_MinusLogProbMetric: 736.9936

Epoch 55: val_loss did not improve from 519.09369
196/196 - 108s - loss: 839.6636 - MinusLogProbMetric: 839.6636 - val_loss: 736.9936 - val_MinusLogProbMetric: 736.9936 - lr: 4.1152e-06 - 108s/epoch - 551ms/step
Epoch 56/1000
2023-09-28 23:51:05.769 
Epoch 56/1000 
	 loss: 690.3699, MinusLogProbMetric: 690.3699, val_loss: 656.0861, val_MinusLogProbMetric: 656.0861

Epoch 56: val_loss did not improve from 519.09369
196/196 - 108s - loss: 690.3699 - MinusLogProbMetric: 690.3699 - val_loss: 656.0861 - val_MinusLogProbMetric: 656.0861 - lr: 4.1152e-06 - 108s/epoch - 550ms/step
Epoch 57/1000
2023-09-28 23:52:51.893 
Epoch 57/1000 
	 loss: 635.1849, MinusLogProbMetric: 635.1849, val_loss: 637.3989, val_MinusLogProbMetric: 637.3989

Epoch 57: val_loss did not improve from 519.09369
196/196 - 106s - loss: 635.1849 - MinusLogProbMetric: 635.1849 - val_loss: 637.3989 - val_MinusLogProbMetric: 637.3989 - lr: 4.1152e-06 - 106s/epoch - 541ms/step
Epoch 58/1000
2023-09-28 23:54:38.308 
Epoch 58/1000 
	 loss: 642.6717, MinusLogProbMetric: 642.6717, val_loss: 626.6647, val_MinusLogProbMetric: 626.6647

Epoch 58: val_loss did not improve from 519.09369
196/196 - 106s - loss: 642.6717 - MinusLogProbMetric: 642.6717 - val_loss: 626.6647 - val_MinusLogProbMetric: 626.6647 - lr: 4.1152e-06 - 106s/epoch - 543ms/step
Epoch 59/1000
2023-09-28 23:56:25.662 
Epoch 59/1000 
	 loss: 606.0553, MinusLogProbMetric: 606.0553, val_loss: 599.4349, val_MinusLogProbMetric: 599.4349

Epoch 59: val_loss did not improve from 519.09369
196/196 - 107s - loss: 606.0553 - MinusLogProbMetric: 606.0553 - val_loss: 599.4349 - val_MinusLogProbMetric: 599.4349 - lr: 4.1152e-06 - 107s/epoch - 548ms/step
Epoch 60/1000
2023-09-28 23:58:12.887 
Epoch 60/1000 
	 loss: 588.3362, MinusLogProbMetric: 588.3362, val_loss: 579.8238, val_MinusLogProbMetric: 579.8238

Epoch 60: val_loss did not improve from 519.09369
196/196 - 107s - loss: 588.3362 - MinusLogProbMetric: 588.3362 - val_loss: 579.8238 - val_MinusLogProbMetric: 579.8238 - lr: 4.1152e-06 - 107s/epoch - 547ms/step
Epoch 61/1000
2023-09-28 23:59:59.503 
Epoch 61/1000 
	 loss: 605.2102, MinusLogProbMetric: 605.2102, val_loss: 579.5720, val_MinusLogProbMetric: 579.5720

Epoch 61: val_loss did not improve from 519.09369
196/196 - 107s - loss: 605.2102 - MinusLogProbMetric: 605.2102 - val_loss: 579.5720 - val_MinusLogProbMetric: 579.5720 - lr: 4.1152e-06 - 107s/epoch - 544ms/step
Epoch 62/1000
2023-09-29 00:01:47.263 
Epoch 62/1000 
	 loss: 572.6391, MinusLogProbMetric: 572.6391, val_loss: 570.3549, val_MinusLogProbMetric: 570.3549

Epoch 62: val_loss did not improve from 519.09369
196/196 - 108s - loss: 572.6391 - MinusLogProbMetric: 572.6391 - val_loss: 570.3549 - val_MinusLogProbMetric: 570.3549 - lr: 4.1152e-06 - 108s/epoch - 550ms/step
Epoch 63/1000
2023-09-29 00:03:35.413 
Epoch 63/1000 
	 loss: 597.5226, MinusLogProbMetric: 597.5226, val_loss: 625.1700, val_MinusLogProbMetric: 625.1700

Epoch 63: val_loss did not improve from 519.09369
196/196 - 108s - loss: 597.5226 - MinusLogProbMetric: 597.5226 - val_loss: 625.1700 - val_MinusLogProbMetric: 625.1700 - lr: 4.1152e-06 - 108s/epoch - 552ms/step
Epoch 64/1000
2023-09-29 00:05:23.499 
Epoch 64/1000 
	 loss: 818.1526, MinusLogProbMetric: 818.1526, val_loss: 778.7071, val_MinusLogProbMetric: 778.7071

Epoch 64: val_loss did not improve from 519.09369
196/196 - 108s - loss: 818.1526 - MinusLogProbMetric: 818.1526 - val_loss: 778.7071 - val_MinusLogProbMetric: 778.7071 - lr: 4.1152e-06 - 108s/epoch - 551ms/step
Epoch 65/1000
2023-09-29 00:07:11.201 
Epoch 65/1000 
	 loss: 756.6091, MinusLogProbMetric: 756.6091, val_loss: 721.6473, val_MinusLogProbMetric: 721.6473

Epoch 65: val_loss did not improve from 519.09369
196/196 - 108s - loss: 756.6091 - MinusLogProbMetric: 756.6091 - val_loss: 721.6473 - val_MinusLogProbMetric: 721.6473 - lr: 4.1152e-06 - 108s/epoch - 549ms/step
Epoch 66/1000
2023-09-29 00:08:55.420 
Epoch 66/1000 
	 loss: 665.6641, MinusLogProbMetric: 665.6641, val_loss: 626.7786, val_MinusLogProbMetric: 626.7786

Epoch 66: val_loss did not improve from 519.09369
196/196 - 104s - loss: 665.6641 - MinusLogProbMetric: 665.6641 - val_loss: 626.7786 - val_MinusLogProbMetric: 626.7786 - lr: 4.1152e-06 - 104s/epoch - 532ms/step
Epoch 67/1000
2023-09-29 00:10:42.277 
Epoch 67/1000 
	 loss: 609.2752, MinusLogProbMetric: 609.2752, val_loss: 602.9899, val_MinusLogProbMetric: 602.9899

Epoch 67: val_loss did not improve from 519.09369
196/196 - 107s - loss: 609.2752 - MinusLogProbMetric: 609.2752 - val_loss: 602.9899 - val_MinusLogProbMetric: 602.9899 - lr: 4.1152e-06 - 107s/epoch - 545ms/step
Epoch 68/1000
2023-09-29 00:12:30.572 
Epoch 68/1000 
	 loss: 590.2028, MinusLogProbMetric: 590.2028, val_loss: 577.0009, val_MinusLogProbMetric: 577.0009

Epoch 68: val_loss did not improve from 519.09369
196/196 - 108s - loss: 590.2028 - MinusLogProbMetric: 590.2028 - val_loss: 577.0009 - val_MinusLogProbMetric: 577.0009 - lr: 4.1152e-06 - 108s/epoch - 553ms/step
Epoch 69/1000
2023-09-29 00:14:17.916 
Epoch 69/1000 
	 loss: 598.4226, MinusLogProbMetric: 598.4226, val_loss: 604.9385, val_MinusLogProbMetric: 604.9385

Epoch 69: val_loss did not improve from 519.09369
196/196 - 107s - loss: 598.4226 - MinusLogProbMetric: 598.4226 - val_loss: 604.9385 - val_MinusLogProbMetric: 604.9385 - lr: 4.1152e-06 - 107s/epoch - 548ms/step
Epoch 70/1000
2023-09-29 00:16:03.035 
Epoch 70/1000 
	 loss: 584.5574, MinusLogProbMetric: 584.5574, val_loss: 568.7484, val_MinusLogProbMetric: 568.7484

Epoch 70: val_loss did not improve from 519.09369
196/196 - 105s - loss: 584.5574 - MinusLogProbMetric: 584.5574 - val_loss: 568.7484 - val_MinusLogProbMetric: 568.7484 - lr: 4.1152e-06 - 105s/epoch - 536ms/step
Epoch 71/1000
2023-09-29 00:17:50.645 
Epoch 71/1000 
	 loss: 567.6086, MinusLogProbMetric: 567.6086, val_loss: 559.5954, val_MinusLogProbMetric: 559.5954

Epoch 71: val_loss did not improve from 519.09369
196/196 - 108s - loss: 567.6086 - MinusLogProbMetric: 567.6086 - val_loss: 559.5954 - val_MinusLogProbMetric: 559.5954 - lr: 4.1152e-06 - 108s/epoch - 549ms/step
Epoch 72/1000
2023-09-29 00:19:34.561 
Epoch 72/1000 
	 loss: 552.2009, MinusLogProbMetric: 552.2009, val_loss: 555.2151, val_MinusLogProbMetric: 555.2151

Epoch 72: val_loss did not improve from 519.09369
196/196 - 104s - loss: 552.2009 - MinusLogProbMetric: 552.2009 - val_loss: 555.2151 - val_MinusLogProbMetric: 555.2151 - lr: 4.1152e-06 - 104s/epoch - 530ms/step
Epoch 73/1000
2023-09-29 00:21:18.051 
Epoch 73/1000 
	 loss: 548.9340, MinusLogProbMetric: 548.9340, val_loss: 544.5118, val_MinusLogProbMetric: 544.5118

Epoch 73: val_loss did not improve from 519.09369
196/196 - 103s - loss: 548.9340 - MinusLogProbMetric: 548.9340 - val_loss: 544.5118 - val_MinusLogProbMetric: 544.5118 - lr: 4.1152e-06 - 103s/epoch - 528ms/step
Epoch 74/1000
2023-09-29 00:22:59.287 
Epoch 74/1000 
	 loss: 541.1331, MinusLogProbMetric: 541.1331, val_loss: 534.6398, val_MinusLogProbMetric: 534.6398

Epoch 74: val_loss did not improve from 519.09369
196/196 - 101s - loss: 541.1331 - MinusLogProbMetric: 541.1331 - val_loss: 534.6398 - val_MinusLogProbMetric: 534.6398 - lr: 4.1152e-06 - 101s/epoch - 516ms/step
Epoch 75/1000
2023-09-29 00:24:38.012 
Epoch 75/1000 
	 loss: 530.5099, MinusLogProbMetric: 530.5099, val_loss: 528.4367, val_MinusLogProbMetric: 528.4367

Epoch 75: val_loss did not improve from 519.09369
196/196 - 99s - loss: 530.5099 - MinusLogProbMetric: 530.5099 - val_loss: 528.4367 - val_MinusLogProbMetric: 528.4367 - lr: 4.1152e-06 - 99s/epoch - 504ms/step
Epoch 76/1000
2023-09-29 00:26:17.341 
Epoch 76/1000 
	 loss: 524.7889, MinusLogProbMetric: 524.7889, val_loss: 525.0753, val_MinusLogProbMetric: 525.0753

Epoch 76: val_loss did not improve from 519.09369
196/196 - 99s - loss: 524.7889 - MinusLogProbMetric: 524.7889 - val_loss: 525.0753 - val_MinusLogProbMetric: 525.0753 - lr: 4.1152e-06 - 99s/epoch - 507ms/step
Epoch 77/1000
2023-09-29 00:27:56.912 
Epoch 77/1000 
	 loss: 522.8969, MinusLogProbMetric: 522.8969, val_loss: 523.2286, val_MinusLogProbMetric: 523.2286

Epoch 77: val_loss did not improve from 519.09369
196/196 - 100s - loss: 522.8969 - MinusLogProbMetric: 522.8969 - val_loss: 523.2286 - val_MinusLogProbMetric: 523.2286 - lr: 4.1152e-06 - 100s/epoch - 508ms/step
Epoch 78/1000
2023-09-29 00:29:34.732 
Epoch 78/1000 
	 loss: 519.0024, MinusLogProbMetric: 519.0024, val_loss: 516.5530, val_MinusLogProbMetric: 516.5530

Epoch 78: val_loss improved from 519.09369 to 516.55298, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 99s - loss: 519.0024 - MinusLogProbMetric: 519.0024 - val_loss: 516.5530 - val_MinusLogProbMetric: 516.5530 - lr: 4.1152e-06 - 99s/epoch - 505ms/step
Epoch 79/1000
2023-09-29 00:31:13.445 
Epoch 79/1000 
	 loss: 516.8506, MinusLogProbMetric: 516.8506, val_loss: 514.7347, val_MinusLogProbMetric: 514.7347

Epoch 79: val_loss improved from 516.55298 to 514.73468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 99s - loss: 516.8506 - MinusLogProbMetric: 516.8506 - val_loss: 514.7347 - val_MinusLogProbMetric: 514.7347 - lr: 4.1152e-06 - 99s/epoch - 506ms/step
Epoch 80/1000
2023-09-29 00:32:55.444 
Epoch 80/1000 
	 loss: 514.1581, MinusLogProbMetric: 514.1581, val_loss: 511.4296, val_MinusLogProbMetric: 511.4296

Epoch 80: val_loss improved from 514.73468 to 511.42960, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 102s - loss: 514.1581 - MinusLogProbMetric: 514.1581 - val_loss: 511.4296 - val_MinusLogProbMetric: 511.4296 - lr: 4.1152e-06 - 102s/epoch - 518ms/step
Epoch 81/1000
2023-09-29 00:34:32.330 
Epoch 81/1000 
	 loss: 508.8265, MinusLogProbMetric: 508.8265, val_loss: 507.5263, val_MinusLogProbMetric: 507.5263

Epoch 81: val_loss improved from 511.42960 to 507.52631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 98s - loss: 508.8265 - MinusLogProbMetric: 508.8265 - val_loss: 507.5263 - val_MinusLogProbMetric: 507.5263 - lr: 4.1152e-06 - 98s/epoch - 499ms/step
Epoch 82/1000
2023-09-29 00:36:18.169 
Epoch 82/1000 
	 loss: 501.6433, MinusLogProbMetric: 501.6433, val_loss: 499.2319, val_MinusLogProbMetric: 499.2319

Epoch 82: val_loss improved from 507.52631 to 499.23190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 105s - loss: 501.6433 - MinusLogProbMetric: 501.6433 - val_loss: 499.2319 - val_MinusLogProbMetric: 499.2319 - lr: 4.1152e-06 - 105s/epoch - 536ms/step
Epoch 83/1000
2023-09-29 00:38:00.103 
Epoch 83/1000 
	 loss: 499.2556, MinusLogProbMetric: 499.2556, val_loss: 503.2591, val_MinusLogProbMetric: 503.2591

Epoch 83: val_loss did not improve from 499.23190
196/196 - 101s - loss: 499.2556 - MinusLogProbMetric: 499.2556 - val_loss: 503.2591 - val_MinusLogProbMetric: 503.2591 - lr: 4.1152e-06 - 101s/epoch - 514ms/step
Epoch 84/1000
2023-09-29 00:39:37.310 
Epoch 84/1000 
	 loss: 500.1400, MinusLogProbMetric: 500.1400, val_loss: 498.9225, val_MinusLogProbMetric: 498.9225

Epoch 84: val_loss improved from 499.23190 to 498.92249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_328/weights/best_weights.h5
196/196 - 99s - loss: 500.1400 - MinusLogProbMetric: 500.1400 - val_loss: 498.9225 - val_MinusLogProbMetric: 498.9225 - lr: 4.1152e-06 - 99s/epoch - 504ms/step
Epoch 85/1000
2023-09-29 00:41:21.670 
Epoch 85/1000 
	 loss: 501.0209, MinusLogProbMetric: 501.0209, val_loss: 504.1929, val_MinusLogProbMetric: 504.1929

Epoch 85: val_loss did not improve from 498.92249
196/196 - 103s - loss: 501.0209 - MinusLogProbMetric: 501.0209 - val_loss: 504.1929 - val_MinusLogProbMetric: 504.1929 - lr: 4.1152e-06 - 103s/epoch - 525ms/step
Epoch 86/1000
2023-09-29 00:43:00.288 
Epoch 86/1000 
	 loss: 517.1989, MinusLogProbMetric: 517.1989, val_loss: 526.1687, val_MinusLogProbMetric: 526.1687

Epoch 86: val_loss did not improve from 498.92249
196/196 - 99s - loss: 517.1989 - MinusLogProbMetric: 517.1989 - val_loss: 526.1687 - val_MinusLogProbMetric: 526.1687 - lr: 4.1152e-06 - 99s/epoch - 503ms/step
Epoch 87/1000
2023-09-29 00:44:38.900 
Epoch 87/1000 
	 loss: 681.2314, MinusLogProbMetric: 681.2314, val_loss: 708.6992, val_MinusLogProbMetric: 708.6992

Epoch 87: val_loss did not improve from 498.92249
196/196 - 99s - loss: 681.2314 - MinusLogProbMetric: 681.2314 - val_loss: 708.6992 - val_MinusLogProbMetric: 708.6992 - lr: 4.1152e-06 - 99s/epoch - 503ms/step
Epoch 88/1000
2023-09-29 00:46:04.901 
Epoch 88/1000 
	 loss: 649.6605, MinusLogProbMetric: 649.6605, val_loss: 741.5103, val_MinusLogProbMetric: 741.5103

Epoch 88: val_loss did not improve from 498.92249
196/196 - 86s - loss: 649.6605 - MinusLogProbMetric: 649.6605 - val_loss: 741.5103 - val_MinusLogProbMetric: 741.5103 - lr: 4.1152e-06 - 86s/epoch - 439ms/step
Epoch 89/1000
2023-09-29 00:47:38.727 
Epoch 89/1000 
	 loss: 635.1710, MinusLogProbMetric: 635.1710, val_loss: 546.7090, val_MinusLogProbMetric: 546.7090

Epoch 89: val_loss did not improve from 498.92249
196/196 - 94s - loss: 635.1710 - MinusLogProbMetric: 635.1710 - val_loss: 546.7090 - val_MinusLogProbMetric: 546.7090 - lr: 4.1152e-06 - 94s/epoch - 479ms/step
Epoch 90/1000
2023-09-29 00:49:26.281 
Epoch 90/1000 
	 loss: 747.5717, MinusLogProbMetric: 747.5717, val_loss: 1167.8934, val_MinusLogProbMetric: 1167.8934

Epoch 90: val_loss did not improve from 498.92249
196/196 - 108s - loss: 747.5717 - MinusLogProbMetric: 747.5717 - val_loss: 1167.8934 - val_MinusLogProbMetric: 1167.8934 - lr: 4.1152e-06 - 108s/epoch - 549ms/step
Epoch 91/1000
2023-09-29 00:51:09.450 
Epoch 91/1000 
	 loss: 984.8414, MinusLogProbMetric: 984.8414, val_loss: 813.7773, val_MinusLogProbMetric: 813.7773

Epoch 91: val_loss did not improve from 498.92249
196/196 - 103s - loss: 984.8414 - MinusLogProbMetric: 984.8414 - val_loss: 813.7773 - val_MinusLogProbMetric: 813.7773 - lr: 4.1152e-06 - 103s/epoch - 526ms/step
Epoch 92/1000
2023-09-29 00:52:50.373 
Epoch 92/1000 
	 loss: 763.8654, MinusLogProbMetric: 763.8654, val_loss: 700.4862, val_MinusLogProbMetric: 700.4862

Epoch 92: val_loss did not improve from 498.92249
196/196 - 101s - loss: 763.8654 - MinusLogProbMetric: 763.8654 - val_loss: 700.4862 - val_MinusLogProbMetric: 700.4862 - lr: 4.1152e-06 - 101s/epoch - 515ms/step
Epoch 93/1000
2023-09-29 00:54:31.347 
Epoch 93/1000 
	 loss: 692.0587, MinusLogProbMetric: 692.0587, val_loss: 672.6010, val_MinusLogProbMetric: 672.6010

Epoch 93: val_loss did not improve from 498.92249
196/196 - 101s - loss: 692.0587 - MinusLogProbMetric: 692.0587 - val_loss: 672.6010 - val_MinusLogProbMetric: 672.6010 - lr: 4.1152e-06 - 101s/epoch - 515ms/step
Epoch 94/1000
2023-09-29 00:56:09.654 
Epoch 94/1000 
	 loss: 851.4115, MinusLogProbMetric: 851.4115, val_loss: 984.3590, val_MinusLogProbMetric: 984.3590

Epoch 94: val_loss did not improve from 498.92249
196/196 - 98s - loss: 851.4115 - MinusLogProbMetric: 851.4115 - val_loss: 984.3590 - val_MinusLogProbMetric: 984.3590 - lr: 4.1152e-06 - 98s/epoch - 502ms/step
Epoch 95/1000
2023-09-29 00:57:50.690 
Epoch 95/1000 
	 loss: 891.5466, MinusLogProbMetric: 891.5466, val_loss: 727.4814, val_MinusLogProbMetric: 727.4814

Epoch 95: val_loss did not improve from 498.92249
196/196 - 101s - loss: 891.5466 - MinusLogProbMetric: 891.5466 - val_loss: 727.4814 - val_MinusLogProbMetric: 727.4814 - lr: 4.1152e-06 - 101s/epoch - 515ms/step
Epoch 96/1000
2023-09-29 00:59:26.285 
Epoch 96/1000 
	 loss: 693.1462, MinusLogProbMetric: 693.1462, val_loss: 662.2169, val_MinusLogProbMetric: 662.2169

Epoch 96: val_loss did not improve from 498.92249
196/196 - 96s - loss: 693.1462 - MinusLogProbMetric: 693.1462 - val_loss: 662.2169 - val_MinusLogProbMetric: 662.2169 - lr: 4.1152e-06 - 96s/epoch - 488ms/step
Epoch 97/1000
2023-09-29 01:01:02.385 
Epoch 97/1000 
	 loss: 826.0121, MinusLogProbMetric: 826.0121, val_loss: 930.4456, val_MinusLogProbMetric: 930.4456

Epoch 97: val_loss did not improve from 498.92249
196/196 - 96s - loss: 826.0121 - MinusLogProbMetric: 826.0121 - val_loss: 930.4456 - val_MinusLogProbMetric: 930.4456 - lr: 4.1152e-06 - 96s/epoch - 490ms/step
Epoch 98/1000
2023-09-29 01:02:44.142 
Epoch 98/1000 
	 loss: 981.4767, MinusLogProbMetric: 981.4767, val_loss: 909.6134, val_MinusLogProbMetric: 909.6134

Epoch 98: val_loss did not improve from 498.92249
196/196 - 102s - loss: 981.4767 - MinusLogProbMetric: 981.4767 - val_loss: 909.6134 - val_MinusLogProbMetric: 909.6134 - lr: 4.1152e-06 - 102s/epoch - 519ms/step
Epoch 99/1000
2023-09-29 01:04:25.812 
Epoch 99/1000 
	 loss: 903.9241, MinusLogProbMetric: 903.9241, val_loss: 873.0562, val_MinusLogProbMetric: 873.0562

Epoch 99: val_loss did not improve from 498.92249
196/196 - 102s - loss: 903.9241 - MinusLogProbMetric: 903.9241 - val_loss: 873.0562 - val_MinusLogProbMetric: 873.0562 - lr: 4.1152e-06 - 102s/epoch - 519ms/step
Epoch 100/1000
2023-09-29 01:06:10.478 
Epoch 100/1000 
	 loss: 837.8297, MinusLogProbMetric: 837.8297, val_loss: 819.1353, val_MinusLogProbMetric: 819.1353

Epoch 100: val_loss did not improve from 498.92249
196/196 - 105s - loss: 837.8297 - MinusLogProbMetric: 837.8297 - val_loss: 819.1353 - val_MinusLogProbMetric: 819.1353 - lr: 4.1152e-06 - 105s/epoch - 534ms/step
Epoch 101/1000
2023-09-29 01:07:53.578 
Epoch 101/1000 
	 loss: 779.3407, MinusLogProbMetric: 779.3407, val_loss: 741.2228, val_MinusLogProbMetric: 741.2228

Epoch 101: val_loss did not improve from 498.92249
196/196 - 103s - loss: 779.3407 - MinusLogProbMetric: 779.3407 - val_loss: 741.2228 - val_MinusLogProbMetric: 741.2228 - lr: 4.1152e-06 - 103s/epoch - 526ms/step
Epoch 102/1000
2023-09-29 01:09:29.409 
Epoch 102/1000 
	 loss: 877.4387, MinusLogProbMetric: 877.4387, val_loss: 943.9464, val_MinusLogProbMetric: 943.9464

Epoch 102: val_loss did not improve from 498.92249
196/196 - 96s - loss: 877.4387 - MinusLogProbMetric: 877.4387 - val_loss: 943.9464 - val_MinusLogProbMetric: 943.9464 - lr: 4.1152e-06 - 96s/epoch - 489ms/step
Epoch 103/1000
2023-09-29 01:11:01.888 
Epoch 103/1000 
	 loss: 939.1073, MinusLogProbMetric: 939.1073, val_loss: 905.3208, val_MinusLogProbMetric: 905.3208

Epoch 103: val_loss did not improve from 498.92249
196/196 - 92s - loss: 939.1073 - MinusLogProbMetric: 939.1073 - val_loss: 905.3208 - val_MinusLogProbMetric: 905.3208 - lr: 4.1152e-06 - 92s/epoch - 472ms/step
Epoch 104/1000
2023-09-29 01:12:32.644 
Epoch 104/1000 
	 loss: 906.3035, MinusLogProbMetric: 906.3035, val_loss: 903.6807, val_MinusLogProbMetric: 903.6807

Epoch 104: val_loss did not improve from 498.92249
196/196 - 91s - loss: 906.3035 - MinusLogProbMetric: 906.3035 - val_loss: 903.6807 - val_MinusLogProbMetric: 903.6807 - lr: 4.1152e-06 - 91s/epoch - 463ms/step
Epoch 105/1000
2023-09-29 01:14:02.833 
Epoch 105/1000 
	 loss: 889.9622, MinusLogProbMetric: 889.9622, val_loss: 910.5536, val_MinusLogProbMetric: 910.5536

Epoch 105: val_loss did not improve from 498.92249
196/196 - 90s - loss: 889.9622 - MinusLogProbMetric: 889.9622 - val_loss: 910.5536 - val_MinusLogProbMetric: 910.5536 - lr: 4.1152e-06 - 90s/epoch - 460ms/step
Epoch 106/1000
2023-09-29 01:15:34.439 
Epoch 106/1000 
	 loss: 860.9300, MinusLogProbMetric: 860.9300, val_loss: 832.0432, val_MinusLogProbMetric: 832.0432

Epoch 106: val_loss did not improve from 498.92249
196/196 - 92s - loss: 860.9300 - MinusLogProbMetric: 860.9300 - val_loss: 832.0432 - val_MinusLogProbMetric: 832.0432 - lr: 4.1152e-06 - 92s/epoch - 467ms/step
Epoch 107/1000
2023-09-29 01:17:05.400 
Epoch 107/1000 
	 loss: 840.4738, MinusLogProbMetric: 840.4738, val_loss: 837.4118, val_MinusLogProbMetric: 837.4118

Epoch 107: val_loss did not improve from 498.92249
196/196 - 91s - loss: 840.4738 - MinusLogProbMetric: 840.4738 - val_loss: 837.4118 - val_MinusLogProbMetric: 837.4118 - lr: 4.1152e-06 - 91s/epoch - 464ms/step
Epoch 108/1000
2023-09-29 01:18:35.891 
Epoch 108/1000 
	 loss: 945.9581, MinusLogProbMetric: 945.9581, val_loss: 1002.9485, val_MinusLogProbMetric: 1002.9485

Epoch 108: val_loss did not improve from 498.92249
196/196 - 90s - loss: 945.9581 - MinusLogProbMetric: 945.9581 - val_loss: 1002.9485 - val_MinusLogProbMetric: 1002.9485 - lr: 4.1152e-06 - 90s/epoch - 462ms/step
Epoch 109/1000
2023-09-29 01:20:07.724 
Epoch 109/1000 
	 loss: 959.8846, MinusLogProbMetric: 959.8846, val_loss: 909.8419, val_MinusLogProbMetric: 909.8419

Epoch 109: val_loss did not improve from 498.92249
196/196 - 92s - loss: 959.8846 - MinusLogProbMetric: 959.8846 - val_loss: 909.8419 - val_MinusLogProbMetric: 909.8419 - lr: 4.1152e-06 - 92s/epoch - 468ms/step
Epoch 110/1000
2023-09-29 01:21:37.061 
Epoch 110/1000 
	 loss: 883.2956, MinusLogProbMetric: 883.2956, val_loss: 872.1531, val_MinusLogProbMetric: 872.1531

Epoch 110: val_loss did not improve from 498.92249
196/196 - 89s - loss: 883.2956 - MinusLogProbMetric: 883.2956 - val_loss: 872.1531 - val_MinusLogProbMetric: 872.1531 - lr: 4.1152e-06 - 89s/epoch - 456ms/step
Epoch 111/1000
2023-09-29 01:23:10.216 
Epoch 111/1000 
	 loss: 920.0436, MinusLogProbMetric: 920.0436, val_loss: 854.4531, val_MinusLogProbMetric: 854.4531

Epoch 111: val_loss did not improve from 498.92249
196/196 - 93s - loss: 920.0436 - MinusLogProbMetric: 920.0436 - val_loss: 854.4531 - val_MinusLogProbMetric: 854.4531 - lr: 4.1152e-06 - 93s/epoch - 475ms/step
Epoch 112/1000
2023-09-29 01:24:45.072 
Epoch 112/1000 
	 loss: 805.1993, MinusLogProbMetric: 805.1993, val_loss: 783.4993, val_MinusLogProbMetric: 783.4993

Epoch 112: val_loss did not improve from 498.92249
196/196 - 95s - loss: 805.1993 - MinusLogProbMetric: 805.1993 - val_loss: 783.4993 - val_MinusLogProbMetric: 783.4993 - lr: 4.1152e-06 - 95s/epoch - 484ms/step
Epoch 113/1000
2023-09-29 01:26:16.369 
Epoch 113/1000 
	 loss: 770.0703, MinusLogProbMetric: 770.0703, val_loss: 756.6827, val_MinusLogProbMetric: 756.6827

Epoch 113: val_loss did not improve from 498.92249
196/196 - 91s - loss: 770.0703 - MinusLogProbMetric: 770.0703 - val_loss: 756.6827 - val_MinusLogProbMetric: 756.6827 - lr: 4.1152e-06 - 91s/epoch - 466ms/step
Epoch 114/1000
2023-09-29 01:27:48.029 
Epoch 114/1000 
	 loss: 766.2168, MinusLogProbMetric: 766.2168, val_loss: 795.6746, val_MinusLogProbMetric: 795.6746

Epoch 114: val_loss did not improve from 498.92249
196/196 - 92s - loss: 766.2168 - MinusLogProbMetric: 766.2168 - val_loss: 795.6746 - val_MinusLogProbMetric: 795.6746 - lr: 4.1152e-06 - 92s/epoch - 468ms/step
Epoch 115/1000
2023-09-29 01:29:18.292 
Epoch 115/1000 
	 loss: 785.6345, MinusLogProbMetric: 785.6345, val_loss: 772.7174, val_MinusLogProbMetric: 772.7174

Epoch 115: val_loss did not improve from 498.92249
196/196 - 90s - loss: 785.6345 - MinusLogProbMetric: 785.6345 - val_loss: 772.7174 - val_MinusLogProbMetric: 772.7174 - lr: 4.1152e-06 - 90s/epoch - 461ms/step
Epoch 116/1000
2023-09-29 01:30:47.511 
Epoch 116/1000 
	 loss: 760.1919, MinusLogProbMetric: 760.1919, val_loss: 746.0954, val_MinusLogProbMetric: 746.0954

Epoch 116: val_loss did not improve from 498.92249
196/196 - 89s - loss: 760.1919 - MinusLogProbMetric: 760.1919 - val_loss: 746.0954 - val_MinusLogProbMetric: 746.0954 - lr: 4.1152e-06 - 89s/epoch - 455ms/step
Epoch 117/1000
2023-09-29 01:32:21.914 
Epoch 117/1000 
	 loss: 792.4308, MinusLogProbMetric: 792.4308, val_loss: 783.1534, val_MinusLogProbMetric: 783.1534

Epoch 117: val_loss did not improve from 498.92249
196/196 - 94s - loss: 792.4308 - MinusLogProbMetric: 792.4308 - val_loss: 783.1534 - val_MinusLogProbMetric: 783.1534 - lr: 4.1152e-06 - 94s/epoch - 482ms/step
Epoch 118/1000
2023-09-29 01:33:51.940 
Epoch 118/1000 
	 loss: 760.6464, MinusLogProbMetric: 760.6464, val_loss: 737.6217, val_MinusLogProbMetric: 737.6217

Epoch 118: val_loss did not improve from 498.92249
196/196 - 90s - loss: 760.6464 - MinusLogProbMetric: 760.6464 - val_loss: 737.6217 - val_MinusLogProbMetric: 737.6217 - lr: 4.1152e-06 - 90s/epoch - 459ms/step
Epoch 119/1000
2023-09-29 01:35:23.665 
Epoch 119/1000 
	 loss: 804.9706, MinusLogProbMetric: 804.9706, val_loss: 844.1711, val_MinusLogProbMetric: 844.1711

Epoch 119: val_loss did not improve from 498.92249
196/196 - 92s - loss: 804.9706 - MinusLogProbMetric: 804.9706 - val_loss: 844.1711 - val_MinusLogProbMetric: 844.1711 - lr: 4.1152e-06 - 92s/epoch - 468ms/step
Epoch 120/1000
2023-09-29 01:36:55.756 
Epoch 120/1000 
	 loss: 818.8284, MinusLogProbMetric: 818.8284, val_loss: 779.3967, val_MinusLogProbMetric: 779.3967

Epoch 120: val_loss did not improve from 498.92249
196/196 - 92s - loss: 818.8284 - MinusLogProbMetric: 818.8284 - val_loss: 779.3967 - val_MinusLogProbMetric: 779.3967 - lr: 4.1152e-06 - 92s/epoch - 470ms/step
Epoch 121/1000
2023-09-29 01:38:26.808 
Epoch 121/1000 
	 loss: 774.6646, MinusLogProbMetric: 774.6646, val_loss: 761.6302, val_MinusLogProbMetric: 761.6302

Epoch 121: val_loss did not improve from 498.92249
196/196 - 91s - loss: 774.6646 - MinusLogProbMetric: 774.6646 - val_loss: 761.6302 - val_MinusLogProbMetric: 761.6302 - lr: 4.1152e-06 - 91s/epoch - 464ms/step
Epoch 122/1000
2023-09-29 01:39:58.363 
Epoch 122/1000 
	 loss: 758.5049, MinusLogProbMetric: 758.5049, val_loss: 751.5020, val_MinusLogProbMetric: 751.5020

Epoch 122: val_loss did not improve from 498.92249
196/196 - 92s - loss: 758.5049 - MinusLogProbMetric: 758.5049 - val_loss: 751.5020 - val_MinusLogProbMetric: 751.5020 - lr: 4.1152e-06 - 92s/epoch - 467ms/step
Epoch 123/1000
2023-09-29 01:41:35.338 
Epoch 123/1000 
	 loss: 746.2520, MinusLogProbMetric: 746.2520, val_loss: 737.1050, val_MinusLogProbMetric: 737.1050

Epoch 123: val_loss did not improve from 498.92249
196/196 - 97s - loss: 746.2520 - MinusLogProbMetric: 746.2520 - val_loss: 737.1050 - val_MinusLogProbMetric: 737.1050 - lr: 4.1152e-06 - 97s/epoch - 495ms/step
Epoch 124/1000
2023-09-29 01:43:10.492 
Epoch 124/1000 
	 loss: 731.7083, MinusLogProbMetric: 731.7083, val_loss: 731.2657, val_MinusLogProbMetric: 731.2657

Epoch 124: val_loss did not improve from 498.92249
196/196 - 95s - loss: 731.7083 - MinusLogProbMetric: 731.7083 - val_loss: 731.2657 - val_MinusLogProbMetric: 731.2657 - lr: 4.1152e-06 - 95s/epoch - 485ms/step
Epoch 125/1000
2023-09-29 01:44:44.087 
Epoch 125/1000 
	 loss: 1023.9366, MinusLogProbMetric: 1023.9366, val_loss: 953.0675, val_MinusLogProbMetric: 953.0675

Epoch 125: val_loss did not improve from 498.92249
196/196 - 94s - loss: 1023.9366 - MinusLogProbMetric: 1023.9366 - val_loss: 953.0675 - val_MinusLogProbMetric: 953.0675 - lr: 4.1152e-06 - 94s/epoch - 477ms/step
Epoch 126/1000
2023-09-29 01:46:17.962 
Epoch 126/1000 
	 loss: 904.2320, MinusLogProbMetric: 904.2320, val_loss: 875.4235, val_MinusLogProbMetric: 875.4235

Epoch 126: val_loss did not improve from 498.92249
196/196 - 94s - loss: 904.2320 - MinusLogProbMetric: 904.2320 - val_loss: 875.4235 - val_MinusLogProbMetric: 875.4235 - lr: 4.1152e-06 - 94s/epoch - 479ms/step
Epoch 127/1000
2023-09-29 01:47:52.818 
Epoch 127/1000 
	 loss: 848.2807, MinusLogProbMetric: 848.2807, val_loss: 824.7684, val_MinusLogProbMetric: 824.7684

Epoch 127: val_loss did not improve from 498.92249
196/196 - 95s - loss: 848.2807 - MinusLogProbMetric: 848.2807 - val_loss: 824.7684 - val_MinusLogProbMetric: 824.7684 - lr: 4.1152e-06 - 95s/epoch - 484ms/step
Epoch 128/1000
2023-09-29 01:49:28.611 
Epoch 128/1000 
	 loss: 815.4575, MinusLogProbMetric: 815.4575, val_loss: 801.0449, val_MinusLogProbMetric: 801.0449

Epoch 128: val_loss did not improve from 498.92249
196/196 - 96s - loss: 815.4575 - MinusLogProbMetric: 815.4575 - val_loss: 801.0449 - val_MinusLogProbMetric: 801.0449 - lr: 4.1152e-06 - 96s/epoch - 489ms/step
Epoch 129/1000
2023-09-29 01:51:01.497 
Epoch 129/1000 
	 loss: 786.8550, MinusLogProbMetric: 786.8550, val_loss: 771.9183, val_MinusLogProbMetric: 771.9183

Epoch 129: val_loss did not improve from 498.92249
196/196 - 93s - loss: 786.8550 - MinusLogProbMetric: 786.8550 - val_loss: 771.9183 - val_MinusLogProbMetric: 771.9183 - lr: 4.1152e-06 - 93s/epoch - 474ms/step
Epoch 130/1000
2023-09-29 01:52:33.097 
Epoch 130/1000 
	 loss: 764.6629, MinusLogProbMetric: 764.6629, val_loss: 758.0657, val_MinusLogProbMetric: 758.0657

Epoch 130: val_loss did not improve from 498.92249
196/196 - 92s - loss: 764.6629 - MinusLogProbMetric: 764.6629 - val_loss: 758.0657 - val_MinusLogProbMetric: 758.0657 - lr: 4.1152e-06 - 92s/epoch - 467ms/step
Epoch 131/1000
2023-09-29 01:54:04.849 
Epoch 131/1000 
	 loss: 753.2366, MinusLogProbMetric: 753.2366, val_loss: 743.2703, val_MinusLogProbMetric: 743.2703

Epoch 131: val_loss did not improve from 498.92249
196/196 - 92s - loss: 753.2366 - MinusLogProbMetric: 753.2366 - val_loss: 743.2703 - val_MinusLogProbMetric: 743.2703 - lr: 4.1152e-06 - 92s/epoch - 468ms/step
Epoch 132/1000
2023-09-29 01:55:36.756 
Epoch 132/1000 
	 loss: 737.3077, MinusLogProbMetric: 737.3077, val_loss: 729.9270, val_MinusLogProbMetric: 729.9270

Epoch 132: val_loss did not improve from 498.92249
196/196 - 92s - loss: 737.3077 - MinusLogProbMetric: 737.3077 - val_loss: 729.9270 - val_MinusLogProbMetric: 729.9270 - lr: 4.1152e-06 - 92s/epoch - 469ms/step
Epoch 133/1000
2023-09-29 01:57:07.751 
Epoch 133/1000 
	 loss: 752.4670, MinusLogProbMetric: 752.4670, val_loss: 747.1084, val_MinusLogProbMetric: 747.1084

Epoch 133: val_loss did not improve from 498.92249
196/196 - 91s - loss: 752.4670 - MinusLogProbMetric: 752.4670 - val_loss: 747.1084 - val_MinusLogProbMetric: 747.1084 - lr: 4.1152e-06 - 91s/epoch - 464ms/step
Epoch 134/1000
2023-09-29 01:58:36.898 
Epoch 134/1000 
	 loss: 734.7907, MinusLogProbMetric: 734.7907, val_loss: 725.1035, val_MinusLogProbMetric: 725.1035

Epoch 134: val_loss did not improve from 498.92249
196/196 - 89s - loss: 734.7907 - MinusLogProbMetric: 734.7907 - val_loss: 725.1035 - val_MinusLogProbMetric: 725.1035 - lr: 4.1152e-06 - 89s/epoch - 455ms/step
Epoch 135/1000
2023-09-29 02:00:11.823 
Epoch 135/1000 
	 loss: 721.6418, MinusLogProbMetric: 721.6418, val_loss: 717.2220, val_MinusLogProbMetric: 717.2220

Epoch 135: val_loss did not improve from 498.92249
196/196 - 95s - loss: 721.6418 - MinusLogProbMetric: 721.6418 - val_loss: 717.2220 - val_MinusLogProbMetric: 717.2220 - lr: 2.0576e-06 - 95s/epoch - 484ms/step
Epoch 136/1000
2023-09-29 02:01:46.283 
Epoch 136/1000 
	 loss: 712.7383, MinusLogProbMetric: 712.7383, val_loss: 709.6650, val_MinusLogProbMetric: 709.6650

Epoch 136: val_loss did not improve from 498.92249
196/196 - 94s - loss: 712.7383 - MinusLogProbMetric: 712.7383 - val_loss: 709.6650 - val_MinusLogProbMetric: 709.6650 - lr: 2.0576e-06 - 94s/epoch - 482ms/step
Epoch 137/1000
2023-09-29 02:03:17.022 
Epoch 137/1000 
	 loss: 704.0575, MinusLogProbMetric: 704.0575, val_loss: 700.0142, val_MinusLogProbMetric: 700.0142

Epoch 137: val_loss did not improve from 498.92249
196/196 - 91s - loss: 704.0575 - MinusLogProbMetric: 704.0575 - val_loss: 700.0142 - val_MinusLogProbMetric: 700.0142 - lr: 2.0576e-06 - 91s/epoch - 463ms/step
Epoch 138/1000
2023-09-29 02:04:45.125 
Epoch 138/1000 
	 loss: 696.0348, MinusLogProbMetric: 696.0348, val_loss: 705.1549, val_MinusLogProbMetric: 705.1549

Epoch 138: val_loss did not improve from 498.92249
196/196 - 88s - loss: 696.0348 - MinusLogProbMetric: 696.0348 - val_loss: 705.1549 - val_MinusLogProbMetric: 705.1549 - lr: 2.0576e-06 - 88s/epoch - 449ms/step
Epoch 139/1000
2023-09-29 02:06:17.786 
Epoch 139/1000 
	 loss: 698.8214, MinusLogProbMetric: 698.8214, val_loss: 693.7793, val_MinusLogProbMetric: 693.7793

Epoch 139: val_loss did not improve from 498.92249
196/196 - 93s - loss: 698.8214 - MinusLogProbMetric: 698.8214 - val_loss: 693.7793 - val_MinusLogProbMetric: 693.7793 - lr: 2.0576e-06 - 93s/epoch - 473ms/step
Epoch 140/1000
2023-09-29 02:07:49.282 
Epoch 140/1000 
	 loss: 690.1092, MinusLogProbMetric: 690.1092, val_loss: 685.1191, val_MinusLogProbMetric: 685.1191

Epoch 140: val_loss did not improve from 498.92249
196/196 - 91s - loss: 690.1092 - MinusLogProbMetric: 690.1092 - val_loss: 685.1191 - val_MinusLogProbMetric: 685.1191 - lr: 2.0576e-06 - 91s/epoch - 467ms/step
Epoch 141/1000
2023-09-29 02:09:23.516 
Epoch 141/1000 
	 loss: 682.4277, MinusLogProbMetric: 682.4277, val_loss: 679.4120, val_MinusLogProbMetric: 679.4120

Epoch 141: val_loss did not improve from 498.92249
196/196 - 94s - loss: 682.4277 - MinusLogProbMetric: 682.4277 - val_loss: 679.4120 - val_MinusLogProbMetric: 679.4120 - lr: 2.0576e-06 - 94s/epoch - 481ms/step
Epoch 142/1000
2023-09-29 02:11:01.981 
Epoch 142/1000 
	 loss: 686.8267, MinusLogProbMetric: 686.8267, val_loss: 685.5793, val_MinusLogProbMetric: 685.5793

Epoch 142: val_loss did not improve from 498.92249
196/196 - 98s - loss: 686.8267 - MinusLogProbMetric: 686.8267 - val_loss: 685.5793 - val_MinusLogProbMetric: 685.5793 - lr: 2.0576e-06 - 98s/epoch - 502ms/step
Epoch 143/1000
2023-09-29 02:12:47.670 
Epoch 143/1000 
	 loss: 685.1281, MinusLogProbMetric: 685.1281, val_loss: 681.4370, val_MinusLogProbMetric: 681.4370

Epoch 143: val_loss did not improve from 498.92249
196/196 - 106s - loss: 685.1281 - MinusLogProbMetric: 685.1281 - val_loss: 681.4370 - val_MinusLogProbMetric: 681.4370 - lr: 2.0576e-06 - 106s/epoch - 539ms/step
Epoch 144/1000
2023-09-29 02:14:31.597 
Epoch 144/1000 
	 loss: 684.2789, MinusLogProbMetric: 684.2789, val_loss: 681.5270, val_MinusLogProbMetric: 681.5270

Epoch 144: val_loss did not improve from 498.92249
196/196 - 104s - loss: 684.2789 - MinusLogProbMetric: 684.2789 - val_loss: 681.5270 - val_MinusLogProbMetric: 681.5270 - lr: 2.0576e-06 - 104s/epoch - 530ms/step
Epoch 145/1000
2023-09-29 02:16:11.030 
Epoch 145/1000 
	 loss: 678.2953, MinusLogProbMetric: 678.2953, val_loss: 673.8221, val_MinusLogProbMetric: 673.8221

Epoch 145: val_loss did not improve from 498.92249
196/196 - 99s - loss: 678.2953 - MinusLogProbMetric: 678.2953 - val_loss: 673.8221 - val_MinusLogProbMetric: 673.8221 - lr: 2.0576e-06 - 99s/epoch - 507ms/step
Epoch 146/1000
2023-09-29 02:17:51.880 
Epoch 146/1000 
	 loss: 671.3232, MinusLogProbMetric: 671.3232, val_loss: 668.4343, val_MinusLogProbMetric: 668.4343

Epoch 146: val_loss did not improve from 498.92249
196/196 - 101s - loss: 671.3232 - MinusLogProbMetric: 671.3232 - val_loss: 668.4343 - val_MinusLogProbMetric: 668.4343 - lr: 2.0576e-06 - 101s/epoch - 514ms/step
Epoch 147/1000
2023-09-29 02:19:36.290 
Epoch 147/1000 
	 loss: 666.7755, MinusLogProbMetric: 666.7755, val_loss: 665.1924, val_MinusLogProbMetric: 665.1924

Epoch 147: val_loss did not improve from 498.92249
196/196 - 104s - loss: 666.7755 - MinusLogProbMetric: 666.7755 - val_loss: 665.1924 - val_MinusLogProbMetric: 665.1924 - lr: 2.0576e-06 - 104s/epoch - 533ms/step
Epoch 148/1000
2023-09-29 02:21:18.010 
Epoch 148/1000 
	 loss: 663.4867, MinusLogProbMetric: 663.4867, val_loss: 661.0873, val_MinusLogProbMetric: 661.0873

Epoch 148: val_loss did not improve from 498.92249
196/196 - 102s - loss: 663.4867 - MinusLogProbMetric: 663.4867 - val_loss: 661.0873 - val_MinusLogProbMetric: 661.0873 - lr: 2.0576e-06 - 102s/epoch - 519ms/step
Epoch 149/1000
2023-09-29 02:22:58.462 
Epoch 149/1000 
	 loss: 658.0955, MinusLogProbMetric: 658.0955, val_loss: 654.6867, val_MinusLogProbMetric: 654.6867

Epoch 149: val_loss did not improve from 498.92249
196/196 - 100s - loss: 658.0955 - MinusLogProbMetric: 658.0955 - val_loss: 654.6867 - val_MinusLogProbMetric: 654.6867 - lr: 2.0576e-06 - 100s/epoch - 512ms/step
Epoch 150/1000
2023-09-29 02:24:42.898 
Epoch 150/1000 
	 loss: 653.3915, MinusLogProbMetric: 653.3915, val_loss: 654.5713, val_MinusLogProbMetric: 654.5713

Epoch 150: val_loss did not improve from 498.92249
196/196 - 104s - loss: 653.3915 - MinusLogProbMetric: 653.3915 - val_loss: 654.5713 - val_MinusLogProbMetric: 654.5713 - lr: 2.0576e-06 - 104s/epoch - 533ms/step
Epoch 151/1000
2023-09-29 02:26:21.138 
Epoch 151/1000 
	 loss: 651.5500, MinusLogProbMetric: 651.5500, val_loss: 648.5837, val_MinusLogProbMetric: 648.5837

Epoch 151: val_loss did not improve from 498.92249
196/196 - 98s - loss: 651.5500 - MinusLogProbMetric: 651.5500 - val_loss: 648.5837 - val_MinusLogProbMetric: 648.5837 - lr: 2.0576e-06 - 98s/epoch - 501ms/step
Epoch 152/1000
2023-09-29 02:28:08.650 
Epoch 152/1000 
	 loss: 646.7927, MinusLogProbMetric: 646.7927, val_loss: 645.2802, val_MinusLogProbMetric: 645.2802

Epoch 152: val_loss did not improve from 498.92249
196/196 - 108s - loss: 646.7927 - MinusLogProbMetric: 646.7927 - val_loss: 645.2802 - val_MinusLogProbMetric: 645.2802 - lr: 2.0576e-06 - 108s/epoch - 548ms/step
Epoch 153/1000
2023-09-29 02:29:52.401 
Epoch 153/1000 
	 loss: 647.9772, MinusLogProbMetric: 647.9772, val_loss: 646.4020, val_MinusLogProbMetric: 646.4020

Epoch 153: val_loss did not improve from 498.92249
196/196 - 104s - loss: 647.9772 - MinusLogProbMetric: 647.9772 - val_loss: 646.4020 - val_MinusLogProbMetric: 646.4020 - lr: 2.0576e-06 - 104s/epoch - 529ms/step
Epoch 154/1000
2023-09-29 02:31:35.578 
Epoch 154/1000 
	 loss: 652.2184, MinusLogProbMetric: 652.2184, val_loss: 653.8083, val_MinusLogProbMetric: 653.8083

Epoch 154: val_loss did not improve from 498.92249
196/196 - 103s - loss: 652.2184 - MinusLogProbMetric: 652.2184 - val_loss: 653.8083 - val_MinusLogProbMetric: 653.8083 - lr: 2.0576e-06 - 103s/epoch - 526ms/step
Epoch 155/1000
2023-09-29 02:33:16.965 
Epoch 155/1000 
	 loss: 650.0604, MinusLogProbMetric: 650.0604, val_loss: 647.6440, val_MinusLogProbMetric: 647.6440

Epoch 155: val_loss did not improve from 498.92249
196/196 - 101s - loss: 650.0604 - MinusLogProbMetric: 650.0604 - val_loss: 647.6440 - val_MinusLogProbMetric: 647.6440 - lr: 2.0576e-06 - 101s/epoch - 517ms/step
Epoch 156/1000
2023-09-29 02:34:59.271 
Epoch 156/1000 
	 loss: 645.2083, MinusLogProbMetric: 645.2083, val_loss: 643.0451, val_MinusLogProbMetric: 643.0451

Epoch 156: val_loss did not improve from 498.92249
196/196 - 102s - loss: 645.2083 - MinusLogProbMetric: 645.2083 - val_loss: 643.0451 - val_MinusLogProbMetric: 643.0451 - lr: 2.0576e-06 - 102s/epoch - 522ms/step
Epoch 157/1000
2023-09-29 02:36:45.523 
Epoch 157/1000 
	 loss: 641.5450, MinusLogProbMetric: 641.5450, val_loss: 640.2935, val_MinusLogProbMetric: 640.2935

Epoch 157: val_loss did not improve from 498.92249
196/196 - 106s - loss: 641.5450 - MinusLogProbMetric: 641.5450 - val_loss: 640.2935 - val_MinusLogProbMetric: 640.2935 - lr: 2.0576e-06 - 106s/epoch - 542ms/step
Epoch 158/1000
2023-09-29 02:38:31.509 
Epoch 158/1000 
	 loss: 638.2090, MinusLogProbMetric: 638.2090, val_loss: 636.4308, val_MinusLogProbMetric: 636.4308

Epoch 158: val_loss did not improve from 498.92249
196/196 - 106s - loss: 638.2090 - MinusLogProbMetric: 638.2090 - val_loss: 636.4308 - val_MinusLogProbMetric: 636.4308 - lr: 2.0576e-06 - 106s/epoch - 541ms/step
Epoch 159/1000
2023-09-29 02:40:16.388 
Epoch 159/1000 
	 loss: 634.7629, MinusLogProbMetric: 634.7629, val_loss: 633.1086, val_MinusLogProbMetric: 633.1086

Epoch 159: val_loss did not improve from 498.92249
196/196 - 105s - loss: 634.7629 - MinusLogProbMetric: 634.7629 - val_loss: 633.1086 - val_MinusLogProbMetric: 633.1086 - lr: 2.0576e-06 - 105s/epoch - 535ms/step
Epoch 160/1000
2023-09-29 02:41:59.503 
Epoch 160/1000 
	 loss: 631.4020, MinusLogProbMetric: 631.4020, val_loss: 630.0253, val_MinusLogProbMetric: 630.0253

Epoch 160: val_loss did not improve from 498.92249
196/196 - 103s - loss: 631.4020 - MinusLogProbMetric: 631.4020 - val_loss: 630.0253 - val_MinusLogProbMetric: 630.0253 - lr: 2.0576e-06 - 103s/epoch - 526ms/step
Epoch 161/1000
2023-09-29 02:43:39.986 
Epoch 161/1000 
	 loss: 627.9957, MinusLogProbMetric: 627.9957, val_loss: 626.4052, val_MinusLogProbMetric: 626.4052

Epoch 161: val_loss did not improve from 498.92249
196/196 - 100s - loss: 627.9957 - MinusLogProbMetric: 627.9957 - val_loss: 626.4052 - val_MinusLogProbMetric: 626.4052 - lr: 2.0576e-06 - 100s/epoch - 513ms/step
Epoch 162/1000
2023-09-29 02:45:24.257 
Epoch 162/1000 
	 loss: 625.3710, MinusLogProbMetric: 625.3710, val_loss: 623.7812, val_MinusLogProbMetric: 623.7812

Epoch 162: val_loss did not improve from 498.92249
196/196 - 104s - loss: 625.3710 - MinusLogProbMetric: 625.3710 - val_loss: 623.7812 - val_MinusLogProbMetric: 623.7812 - lr: 2.0576e-06 - 104s/epoch - 532ms/step
Epoch 163/1000
2023-09-29 02:47:12.940 
Epoch 163/1000 
	 loss: 622.8221, MinusLogProbMetric: 622.8221, val_loss: 621.5748, val_MinusLogProbMetric: 621.5748

Epoch 163: val_loss did not improve from 498.92249
196/196 - 109s - loss: 622.8221 - MinusLogProbMetric: 622.8221 - val_loss: 621.5748 - val_MinusLogProbMetric: 621.5748 - lr: 2.0576e-06 - 109s/epoch - 554ms/step
Epoch 164/1000
2023-09-29 02:48:58.986 
Epoch 164/1000 
	 loss: 620.5150, MinusLogProbMetric: 620.5150, val_loss: 618.9988, val_MinusLogProbMetric: 618.9988

Epoch 164: val_loss did not improve from 498.92249
196/196 - 106s - loss: 620.5150 - MinusLogProbMetric: 620.5150 - val_loss: 618.9988 - val_MinusLogProbMetric: 618.9988 - lr: 2.0576e-06 - 106s/epoch - 541ms/step
Epoch 165/1000
2023-09-29 02:50:39.021 
Epoch 165/1000 
	 loss: 618.1851, MinusLogProbMetric: 618.1851, val_loss: 616.6638, val_MinusLogProbMetric: 616.6638

Epoch 165: val_loss did not improve from 498.92249
196/196 - 100s - loss: 618.1851 - MinusLogProbMetric: 618.1851 - val_loss: 616.6638 - val_MinusLogProbMetric: 616.6638 - lr: 2.0576e-06 - 100s/epoch - 510ms/step
Epoch 166/1000
2023-09-29 02:52:22.224 
Epoch 166/1000 
	 loss: 616.3059, MinusLogProbMetric: 616.3059, val_loss: 614.9355, val_MinusLogProbMetric: 614.9355

Epoch 166: val_loss did not improve from 498.92249
196/196 - 103s - loss: 616.3059 - MinusLogProbMetric: 616.3059 - val_loss: 614.9355 - val_MinusLogProbMetric: 614.9355 - lr: 2.0576e-06 - 103s/epoch - 526ms/step
Epoch 167/1000
2023-09-29 02:54:05.650 
Epoch 167/1000 
	 loss: 613.9711, MinusLogProbMetric: 613.9711, val_loss: 613.1653, val_MinusLogProbMetric: 613.1653

Epoch 167: val_loss did not improve from 498.92249
196/196 - 103s - loss: 613.9711 - MinusLogProbMetric: 613.9711 - val_loss: 613.1653 - val_MinusLogProbMetric: 613.1653 - lr: 2.0576e-06 - 103s/epoch - 528ms/step
Epoch 168/1000
2023-09-29 02:55:53.846 
Epoch 168/1000 
	 loss: 612.4401, MinusLogProbMetric: 612.4401, val_loss: 611.2343, val_MinusLogProbMetric: 611.2343

Epoch 168: val_loss did not improve from 498.92249
196/196 - 108s - loss: 612.4401 - MinusLogProbMetric: 612.4401 - val_loss: 611.2343 - val_MinusLogProbMetric: 611.2343 - lr: 2.0576e-06 - 108s/epoch - 552ms/step
Epoch 169/1000
2023-09-29 02:57:41.911 
Epoch 169/1000 
	 loss: 610.4195, MinusLogProbMetric: 610.4195, val_loss: 609.2065, val_MinusLogProbMetric: 609.2065

Epoch 169: val_loss did not improve from 498.92249
196/196 - 108s - loss: 610.4195 - MinusLogProbMetric: 610.4195 - val_loss: 609.2065 - val_MinusLogProbMetric: 609.2065 - lr: 2.0576e-06 - 108s/epoch - 551ms/step
Epoch 170/1000
2023-09-29 02:59:30.101 
Epoch 170/1000 
	 loss: 608.7568, MinusLogProbMetric: 608.7568, val_loss: 608.7195, val_MinusLogProbMetric: 608.7195

Epoch 170: val_loss did not improve from 498.92249
196/196 - 108s - loss: 608.7568 - MinusLogProbMetric: 608.7568 - val_loss: 608.7195 - val_MinusLogProbMetric: 608.7195 - lr: 2.0576e-06 - 108s/epoch - 552ms/step
Epoch 171/1000
2023-09-29 03:01:14.277 
Epoch 171/1000 
	 loss: 607.9250, MinusLogProbMetric: 607.9250, val_loss: 607.0602, val_MinusLogProbMetric: 607.0602

Epoch 171: val_loss did not improve from 498.92249
196/196 - 104s - loss: 607.9250 - MinusLogProbMetric: 607.9250 - val_loss: 607.0602 - val_MinusLogProbMetric: 607.0602 - lr: 2.0576e-06 - 104s/epoch - 531ms/step
Epoch 172/1000
2023-09-29 03:03:02.018 
Epoch 172/1000 
	 loss: 606.4307, MinusLogProbMetric: 606.4307, val_loss: 605.7232, val_MinusLogProbMetric: 605.7232

Epoch 172: val_loss did not improve from 498.92249
196/196 - 108s - loss: 606.4307 - MinusLogProbMetric: 606.4307 - val_loss: 605.7232 - val_MinusLogProbMetric: 605.7232 - lr: 2.0576e-06 - 108s/epoch - 550ms/step
Epoch 173/1000
2023-09-29 03:04:49.191 
Epoch 173/1000 
	 loss: 605.1636, MinusLogProbMetric: 605.1636, val_loss: 604.1243, val_MinusLogProbMetric: 604.1243

Epoch 173: val_loss did not improve from 498.92249
196/196 - 107s - loss: 605.1636 - MinusLogProbMetric: 605.1636 - val_loss: 604.1243 - val_MinusLogProbMetric: 604.1243 - lr: 2.0576e-06 - 107s/epoch - 547ms/step
Epoch 174/1000
2023-09-29 03:06:37.477 
Epoch 174/1000 
	 loss: 603.1992, MinusLogProbMetric: 603.1992, val_loss: 602.5335, val_MinusLogProbMetric: 602.5335

Epoch 174: val_loss did not improve from 498.92249
196/196 - 108s - loss: 603.1992 - MinusLogProbMetric: 603.1992 - val_loss: 602.5335 - val_MinusLogProbMetric: 602.5335 - lr: 2.0576e-06 - 108s/epoch - 552ms/step
Epoch 175/1000
2023-09-29 03:08:24.183 
Epoch 175/1000 
	 loss: 602.7821, MinusLogProbMetric: 602.7821, val_loss: 601.8227, val_MinusLogProbMetric: 601.8227

Epoch 175: val_loss did not improve from 498.92249
196/196 - 107s - loss: 602.7821 - MinusLogProbMetric: 602.7821 - val_loss: 601.8227 - val_MinusLogProbMetric: 601.8227 - lr: 2.0576e-06 - 107s/epoch - 544ms/step
Epoch 176/1000
2023-09-29 03:10:12.400 
Epoch 176/1000 
	 loss: 601.7509, MinusLogProbMetric: 601.7509, val_loss: 601.2073, val_MinusLogProbMetric: 601.2073

Epoch 176: val_loss did not improve from 498.92249
196/196 - 108s - loss: 601.7509 - MinusLogProbMetric: 601.7509 - val_loss: 601.2073 - val_MinusLogProbMetric: 601.2073 - lr: 2.0576e-06 - 108s/epoch - 552ms/step
Epoch 177/1000
2023-09-29 03:12:00.308 
Epoch 177/1000 
	 loss: 600.1526, MinusLogProbMetric: 600.1526, val_loss: 599.4356, val_MinusLogProbMetric: 599.4356

Epoch 177: val_loss did not improve from 498.92249
196/196 - 108s - loss: 600.1526 - MinusLogProbMetric: 600.1526 - val_loss: 599.4356 - val_MinusLogProbMetric: 599.4356 - lr: 2.0576e-06 - 108s/epoch - 550ms/step
Epoch 178/1000
2023-09-29 03:13:49.138 
Epoch 178/1000 
	 loss: 598.6902, MinusLogProbMetric: 598.6902, val_loss: 598.0944, val_MinusLogProbMetric: 598.0944

Epoch 178: val_loss did not improve from 498.92249
196/196 - 109s - loss: 598.6902 - MinusLogProbMetric: 598.6902 - val_loss: 598.0944 - val_MinusLogProbMetric: 598.0944 - lr: 2.0576e-06 - 109s/epoch - 555ms/step
Epoch 179/1000
2023-09-29 03:15:37.492 
Epoch 179/1000 
	 loss: 597.1696, MinusLogProbMetric: 597.1696, val_loss: 596.0236, val_MinusLogProbMetric: 596.0236

Epoch 179: val_loss did not improve from 498.92249
196/196 - 108s - loss: 597.1696 - MinusLogProbMetric: 597.1696 - val_loss: 596.0236 - val_MinusLogProbMetric: 596.0236 - lr: 2.0576e-06 - 108s/epoch - 553ms/step
Epoch 180/1000
2023-09-29 03:17:23.501 
Epoch 180/1000 
	 loss: 595.5886, MinusLogProbMetric: 595.5886, val_loss: 595.0939, val_MinusLogProbMetric: 595.0939

Epoch 180: val_loss did not improve from 498.92249
196/196 - 106s - loss: 595.5886 - MinusLogProbMetric: 595.5886 - val_loss: 595.0939 - val_MinusLogProbMetric: 595.0939 - lr: 2.0576e-06 - 106s/epoch - 541ms/step
Epoch 181/1000
2023-09-29 03:19:10.720 
Epoch 181/1000 
	 loss: 594.5156, MinusLogProbMetric: 594.5156, val_loss: 593.7367, val_MinusLogProbMetric: 593.7367

Epoch 181: val_loss did not improve from 498.92249
196/196 - 107s - loss: 594.5156 - MinusLogProbMetric: 594.5156 - val_loss: 593.7367 - val_MinusLogProbMetric: 593.7367 - lr: 2.0576e-06 - 107s/epoch - 547ms/step
Epoch 182/1000
2023-09-29 03:20:57.050 
Epoch 182/1000 
	 loss: 593.0246, MinusLogProbMetric: 593.0246, val_loss: 592.2136, val_MinusLogProbMetric: 592.2136

Epoch 182: val_loss did not improve from 498.92249
196/196 - 106s - loss: 593.0246 - MinusLogProbMetric: 593.0246 - val_loss: 592.2136 - val_MinusLogProbMetric: 592.2136 - lr: 2.0576e-06 - 106s/epoch - 542ms/step
Epoch 183/1000
2023-09-29 03:22:44.792 
Epoch 183/1000 
	 loss: 600.8969, MinusLogProbMetric: 600.8969, val_loss: 599.2216, val_MinusLogProbMetric: 599.2216

Epoch 183: val_loss did not improve from 498.92249
196/196 - 108s - loss: 600.8969 - MinusLogProbMetric: 600.8969 - val_loss: 599.2216 - val_MinusLogProbMetric: 599.2216 - lr: 2.0576e-06 - 108s/epoch - 550ms/step
Epoch 184/1000
2023-09-29 03:24:32.824 
Epoch 184/1000 
	 loss: 597.3000, MinusLogProbMetric: 597.3000, val_loss: 595.7628, val_MinusLogProbMetric: 595.7628

Epoch 184: val_loss did not improve from 498.92249
Restoring model weights from the end of the best epoch: 84.
196/196 - 111s - loss: 597.3000 - MinusLogProbMetric: 597.3000 - val_loss: 595.7628 - val_MinusLogProbMetric: 595.7628 - lr: 2.0576e-06 - 111s/epoch - 568ms/step
Epoch 184: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
WARNING:tensorflow:6 out of the last 6 calls to <function LRMetric.Test_tf.<locals>.compute_test at 0x7f8e871ef9a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
LR metric calculation completed in 123.01199483999517 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function KSTest.Test_tf.<locals>.compute_test at 0x7f8e871ec040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
KS tests calculation completed in 47.02985200303374 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function SWDMetric.Test_tf.<locals>.compute_test at 0x7f8e871ec040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
SWD metric calculation completed in 41.748018170997966 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
WARNING:tensorflow:6 out of the last 6 calls to <function FNMetric.Test_tf.<locals>.compute_test at 0x7f8e871ecaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
FN metric calculation completed in 40.35835337097524 seconds.
Training succeeded with seed 0.
Model trained in 18839.16 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 254.95 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 255.25 s.
===========
Run 328/720 done in 21128.57 s.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

===========
Generating train data for run 331.
===========
Train data generated in 0.39 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_331/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_331/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_331/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_331
self.data_kwargs: {'seed': 187}
self.x_data: [[ 6.8153725   2.8430347   6.1778684  ...  2.16058     2.2525268
   1.7267085 ]
 [ 2.1836705   1.800204    9.035551   ...  6.5503054   1.5567943
   2.9656742 ]
 [ 0.87054014  3.1855702   7.8237348  ...  6.3044343  -0.7088827
   2.3190107 ]
 ...
 [ 0.35442734  3.573855   10.922302   ...  5.8448343   0.8037275
   2.171598  ]
 [ 1.4908957   3.8775673   8.757837   ...  5.7039022  -0.88000226
   2.3087878 ]
 [ 1.7291393   3.7802503   9.770119   ...  5.5898924   0.22995539
   4.1106014 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_281"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_282 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  908640    
 ayer)                                                           
                                                                 
=================================================================
Total params: 908,640
Trainable params: 908,640
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f8d84a4cc70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8eca90fd30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8eca90fd30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8d84a90520>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8e84d9ce80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8e84d9d360>, <keras.callbacks.ModelCheckpoint object at 0x7f8e84d9fc70>, <keras.callbacks.EarlyStopping object at 0x7f8e84d9d390>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8e84d9feb0>, <keras.callbacks.TerminateOnNaN object at 0x7f8e84d9e230>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 1.5031981 ,  2.9551353 ,  9.211626  , ...,  5.7474284 ,
         0.7829296 ,  3.3359163 ],
       [ 1.3903152 ,  3.1254141 ,  7.28215   , ...,  5.92416   ,
        -2.8417296 ,  2.9486704 ],
       [ 5.2294745 ,  8.137994  ,  5.014993  , ...,  0.691412  ,
         7.469742  ,  1.3593842 ],
       ...,
       [ 1.2930627 ,  4.346054  ,  8.208784  , ...,  5.237335  ,
         0.53958243,  2.6734426 ],
       [ 6.993642  ,  2.9345863 ,  6.120318  , ...,  2.501727  ,
         3.405333  ,  1.9821649 ],
       [ 5.214714  ,  6.173666  ,  5.530484  , ...,  0.3141932 ,
         6.645546  ,  1.3872935 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_331/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 331/720 with hyperparameters:
timestamp = 2023-09-29 03:28:57.695459
ndims = 64
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 908640
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8153725e+00  2.8430347e+00  6.1778684e+00  4.2452879e+00
  1.6284513e+00  3.3207846e+00  7.0928354e+00  6.4340367e+00
  6.0301833e+00  6.4328456e+00  6.5495849e+00  4.9321003e+00
  8.9532204e+00  3.4481030e+00  4.8284545e+00  8.0901794e+00
  8.1549883e+00  7.3900299e+00  1.3510420e+00  9.5149851e+00
  7.5267930e+00  1.0017762e+01  1.1636026e+00  8.6080103e+00
  2.2276068e+00  6.2524571e+00  1.5594521e+00  8.2755499e+00
  8.5135918e+00  6.3978677e+00  3.8566861e+00  6.8423492e-01
  7.5210962e+00  5.4144373e+00  6.8548636e+00  8.7448330e+00
  9.6529083e+00  8.6500130e+00  9.1009712e-01  4.1672401e+00
  7.6439004e+00  9.0885794e-01  5.2691178e+00  8.8166595e-03
  7.4780917e-01 -4.8575500e-01  7.9515800e+00  2.7509699e+00
  4.8898096e+00  1.0073138e+01  7.4751706e+00  6.5633142e-01
  1.5831217e+00  6.0002384e+00  5.6322212e+00  2.2999794e+00
  9.1524439e+00  5.5740438e+00  5.8525505e+00  6.6422234e+00
  6.9292612e+00  2.1605799e+00  2.2525268e+00  1.7267085e+00]
Epoch 1/1000
2023-09-29 03:31:05.191 
Epoch 1/1000 
	 loss: 1045.3960, MinusLogProbMetric: 1045.3960, val_loss: 247.2980, val_MinusLogProbMetric: 247.2980

Epoch 1: val_loss improved from inf to 247.29800, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 128s - loss: 1045.3960 - MinusLogProbMetric: 1045.3960 - val_loss: 247.2980 - val_MinusLogProbMetric: 247.2980 - lr: 0.0010 - 128s/epoch - 654ms/step
Epoch 2/1000
2023-09-29 03:32:01.076 
Epoch 2/1000 
	 loss: 220.7566, MinusLogProbMetric: 220.7566, val_loss: 164.2692, val_MinusLogProbMetric: 164.2692

Epoch 2: val_loss improved from 247.29800 to 164.26918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 220.7566 - MinusLogProbMetric: 220.7566 - val_loss: 164.2692 - val_MinusLogProbMetric: 164.2692 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 3/1000
2023-09-29 03:32:57.462 
Epoch 3/1000 
	 loss: 150.2392, MinusLogProbMetric: 150.2392, val_loss: 127.4151, val_MinusLogProbMetric: 127.4151

Epoch 3: val_loss improved from 164.26918 to 127.41508, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 57s - loss: 150.2392 - MinusLogProbMetric: 150.2392 - val_loss: 127.4151 - val_MinusLogProbMetric: 127.4151 - lr: 0.0010 - 57s/epoch - 288ms/step
Epoch 4/1000
2023-09-29 03:33:54.319 
Epoch 4/1000 
	 loss: 113.9365, MinusLogProbMetric: 113.9365, val_loss: 106.7012, val_MinusLogProbMetric: 106.7012

Epoch 4: val_loss improved from 127.41508 to 106.70121, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 57s - loss: 113.9365 - MinusLogProbMetric: 113.9365 - val_loss: 106.7012 - val_MinusLogProbMetric: 106.7012 - lr: 0.0010 - 57s/epoch - 289ms/step
Epoch 5/1000
2023-09-29 03:34:50.512 
Epoch 5/1000 
	 loss: 100.2814, MinusLogProbMetric: 100.2814, val_loss: 93.1095, val_MinusLogProbMetric: 93.1095

Epoch 5: val_loss improved from 106.70121 to 93.10948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 56s - loss: 100.2814 - MinusLogProbMetric: 100.2814 - val_loss: 93.1095 - val_MinusLogProbMetric: 93.1095 - lr: 0.0010 - 56s/epoch - 288ms/step
Epoch 6/1000
2023-09-29 03:35:45.972 
Epoch 6/1000 
	 loss: 88.5336, MinusLogProbMetric: 88.5336, val_loss: 83.3381, val_MinusLogProbMetric: 83.3381

Epoch 6: val_loss improved from 93.10948 to 83.33814, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 88.5336 - MinusLogProbMetric: 88.5336 - val_loss: 83.3381 - val_MinusLogProbMetric: 83.3381 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 7/1000
2023-09-29 03:36:40.964 
Epoch 7/1000 
	 loss: 86.3343, MinusLogProbMetric: 86.3343, val_loss: 78.2895, val_MinusLogProbMetric: 78.2895

Epoch 7: val_loss improved from 83.33814 to 78.28946, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 86.3343 - MinusLogProbMetric: 86.3343 - val_loss: 78.2895 - val_MinusLogProbMetric: 78.2895 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 8/1000
2023-09-29 03:37:34.673 
Epoch 8/1000 
	 loss: 74.8829, MinusLogProbMetric: 74.8829, val_loss: 71.7845, val_MinusLogProbMetric: 71.7845

Epoch 8: val_loss improved from 78.28946 to 71.78452, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 53s - loss: 74.8829 - MinusLogProbMetric: 74.8829 - val_loss: 71.7845 - val_MinusLogProbMetric: 71.7845 - lr: 0.0010 - 53s/epoch - 273ms/step
Epoch 9/1000
2023-09-29 03:38:31.244 
Epoch 9/1000 
	 loss: 69.7947, MinusLogProbMetric: 69.7947, val_loss: 72.3846, val_MinusLogProbMetric: 72.3846

Epoch 9: val_loss did not improve from 71.78452
196/196 - 56s - loss: 69.7947 - MinusLogProbMetric: 69.7947 - val_loss: 72.3846 - val_MinusLogProbMetric: 72.3846 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 10/1000
2023-09-29 03:39:25.846 
Epoch 10/1000 
	 loss: 65.3302, MinusLogProbMetric: 65.3302, val_loss: 63.8464, val_MinusLogProbMetric: 63.8464

Epoch 10: val_loss improved from 71.78452 to 63.84642, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 65.3302 - MinusLogProbMetric: 65.3302 - val_loss: 63.8464 - val_MinusLogProbMetric: 63.8464 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 11/1000
2023-09-29 03:40:21.883 
Epoch 11/1000 
	 loss: 63.4806, MinusLogProbMetric: 63.4806, val_loss: 61.6427, val_MinusLogProbMetric: 61.6427

Epoch 11: val_loss improved from 63.84642 to 61.64267, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 56s - loss: 63.4806 - MinusLogProbMetric: 63.4806 - val_loss: 61.6427 - val_MinusLogProbMetric: 61.6427 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 12/1000
2023-09-29 03:41:17.968 
Epoch 12/1000 
	 loss: 59.3759, MinusLogProbMetric: 59.3759, val_loss: 62.6082, val_MinusLogProbMetric: 62.6082

Epoch 12: val_loss did not improve from 61.64267
196/196 - 55s - loss: 59.3759 - MinusLogProbMetric: 59.3759 - val_loss: 62.6082 - val_MinusLogProbMetric: 62.6082 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 13/1000
2023-09-29 03:42:13.440 
Epoch 13/1000 
	 loss: 57.1612, MinusLogProbMetric: 57.1612, val_loss: 59.2566, val_MinusLogProbMetric: 59.2566

Epoch 13: val_loss improved from 61.64267 to 59.25657, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 56s - loss: 57.1612 - MinusLogProbMetric: 57.1612 - val_loss: 59.2566 - val_MinusLogProbMetric: 59.2566 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 14/1000
2023-09-29 03:43:10.334 
Epoch 14/1000 
	 loss: 54.9471, MinusLogProbMetric: 54.9471, val_loss: 61.2248, val_MinusLogProbMetric: 61.2248

Epoch 14: val_loss did not improve from 59.25657
196/196 - 56s - loss: 54.9471 - MinusLogProbMetric: 54.9471 - val_loss: 61.2248 - val_MinusLogProbMetric: 61.2248 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 15/1000
2023-09-29 03:44:06.220 
Epoch 15/1000 
	 loss: 53.3550, MinusLogProbMetric: 53.3550, val_loss: 51.0529, val_MinusLogProbMetric: 51.0529

Epoch 15: val_loss improved from 59.25657 to 51.05290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 57s - loss: 53.3550 - MinusLogProbMetric: 53.3550 - val_loss: 51.0529 - val_MinusLogProbMetric: 51.0529 - lr: 0.0010 - 57s/epoch - 288ms/step
Epoch 16/1000
2023-09-29 03:45:02.059 
Epoch 16/1000 
	 loss: 51.0986, MinusLogProbMetric: 51.0986, val_loss: 51.5515, val_MinusLogProbMetric: 51.5515

Epoch 16: val_loss did not improve from 51.05290
196/196 - 55s - loss: 51.0986 - MinusLogProbMetric: 51.0986 - val_loss: 51.5515 - val_MinusLogProbMetric: 51.5515 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 17/1000
2023-09-29 03:45:57.945 
Epoch 17/1000 
	 loss: 50.3330, MinusLogProbMetric: 50.3330, val_loss: 52.0731, val_MinusLogProbMetric: 52.0731

Epoch 17: val_loss did not improve from 51.05290
196/196 - 56s - loss: 50.3330 - MinusLogProbMetric: 50.3330 - val_loss: 52.0731 - val_MinusLogProbMetric: 52.0731 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 18/1000
2023-09-29 03:46:53.804 
Epoch 18/1000 
	 loss: 49.2934, MinusLogProbMetric: 49.2934, val_loss: 48.4177, val_MinusLogProbMetric: 48.4177

Epoch 18: val_loss improved from 51.05290 to 48.41772, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 57s - loss: 49.2934 - MinusLogProbMetric: 49.2934 - val_loss: 48.4177 - val_MinusLogProbMetric: 48.4177 - lr: 0.0010 - 57s/epoch - 289ms/step
Epoch 19/1000
2023-09-29 03:47:47.441 
Epoch 19/1000 
	 loss: 47.8648, MinusLogProbMetric: 47.8648, val_loss: 48.6033, val_MinusLogProbMetric: 48.6033

Epoch 19: val_loss did not improve from 48.41772
196/196 - 53s - loss: 47.8648 - MinusLogProbMetric: 47.8648 - val_loss: 48.6033 - val_MinusLogProbMetric: 48.6033 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 20/1000
2023-09-29 03:48:40.366 
Epoch 20/1000 
	 loss: 46.8927, MinusLogProbMetric: 46.8927, val_loss: 47.2733, val_MinusLogProbMetric: 47.2733

Epoch 20: val_loss improved from 48.41772 to 47.27328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 46.8927 - MinusLogProbMetric: 46.8927 - val_loss: 47.2733 - val_MinusLogProbMetric: 47.2733 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 21/1000
2023-09-29 03:49:34.909 
Epoch 21/1000 
	 loss: 46.2904, MinusLogProbMetric: 46.2904, val_loss: 46.8459, val_MinusLogProbMetric: 46.8459

Epoch 21: val_loss improved from 47.27328 to 46.84586, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 46.2904 - MinusLogProbMetric: 46.2904 - val_loss: 46.8459 - val_MinusLogProbMetric: 46.8459 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 22/1000
2023-09-29 03:50:27.956 
Epoch 22/1000 
	 loss: 45.0814, MinusLogProbMetric: 45.0814, val_loss: 43.9187, val_MinusLogProbMetric: 43.9187

Epoch 22: val_loss improved from 46.84586 to 43.91867, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 53s - loss: 45.0814 - MinusLogProbMetric: 45.0814 - val_loss: 43.9187 - val_MinusLogProbMetric: 43.9187 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 23/1000
2023-09-29 03:51:21.819 
Epoch 23/1000 
	 loss: 43.9196, MinusLogProbMetric: 43.9196, val_loss: 46.8971, val_MinusLogProbMetric: 46.8971

Epoch 23: val_loss did not improve from 43.91867
196/196 - 53s - loss: 43.9196 - MinusLogProbMetric: 43.9196 - val_loss: 46.8971 - val_MinusLogProbMetric: 46.8971 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 24/1000
2023-09-29 03:52:14.239 
Epoch 24/1000 
	 loss: 43.1747, MinusLogProbMetric: 43.1747, val_loss: 44.7186, val_MinusLogProbMetric: 44.7186

Epoch 24: val_loss did not improve from 43.91867
196/196 - 52s - loss: 43.1747 - MinusLogProbMetric: 43.1747 - val_loss: 44.7186 - val_MinusLogProbMetric: 44.7186 - lr: 0.0010 - 52s/epoch - 267ms/step
Epoch 25/1000
2023-09-29 03:53:04.974 
Epoch 25/1000 
	 loss: 42.3858, MinusLogProbMetric: 42.3858, val_loss: 41.1770, val_MinusLogProbMetric: 41.1770

Epoch 25: val_loss improved from 43.91867 to 41.17699, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 51s - loss: 42.3858 - MinusLogProbMetric: 42.3858 - val_loss: 41.1770 - val_MinusLogProbMetric: 41.1770 - lr: 0.0010 - 51s/epoch - 263ms/step
Epoch 26/1000
2023-09-29 03:54:00.324 
Epoch 26/1000 
	 loss: 41.5433, MinusLogProbMetric: 41.5433, val_loss: 43.6239, val_MinusLogProbMetric: 43.6239

Epoch 26: val_loss did not improve from 41.17699
196/196 - 55s - loss: 41.5433 - MinusLogProbMetric: 41.5433 - val_loss: 43.6239 - val_MinusLogProbMetric: 43.6239 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 27/1000
2023-09-29 03:54:51.472 
Epoch 27/1000 
	 loss: 40.6716, MinusLogProbMetric: 40.6716, val_loss: 41.2092, val_MinusLogProbMetric: 41.2092

Epoch 27: val_loss did not improve from 41.17699
196/196 - 51s - loss: 40.6716 - MinusLogProbMetric: 40.6716 - val_loss: 41.2092 - val_MinusLogProbMetric: 41.2092 - lr: 0.0010 - 51s/epoch - 261ms/step
Epoch 28/1000
2023-09-29 03:55:45.469 
Epoch 28/1000 
	 loss: 40.2169, MinusLogProbMetric: 40.2169, val_loss: 40.9540, val_MinusLogProbMetric: 40.9540

Epoch 28: val_loss improved from 41.17699 to 40.95395, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 40.2169 - MinusLogProbMetric: 40.2169 - val_loss: 40.9540 - val_MinusLogProbMetric: 40.9540 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 29/1000
2023-09-29 03:56:38.401 
Epoch 29/1000 
	 loss: 41.3423, MinusLogProbMetric: 41.3423, val_loss: 40.9966, val_MinusLogProbMetric: 40.9966

Epoch 29: val_loss did not improve from 40.95395
196/196 - 52s - loss: 41.3423 - MinusLogProbMetric: 41.3423 - val_loss: 40.9966 - val_MinusLogProbMetric: 40.9966 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 30/1000
2023-09-29 03:57:31.688 
Epoch 30/1000 
	 loss: 47.3382, MinusLogProbMetric: 47.3382, val_loss: 42.4894, val_MinusLogProbMetric: 42.4894

Epoch 30: val_loss did not improve from 40.95395
196/196 - 53s - loss: 47.3382 - MinusLogProbMetric: 47.3382 - val_loss: 42.4894 - val_MinusLogProbMetric: 42.4894 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 31/1000
2023-09-29 03:58:26.074 
Epoch 31/1000 
	 loss: 40.6420, MinusLogProbMetric: 40.6420, val_loss: 40.1945, val_MinusLogProbMetric: 40.1945

Epoch 31: val_loss improved from 40.95395 to 40.19453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 40.6420 - MinusLogProbMetric: 40.6420 - val_loss: 40.1945 - val_MinusLogProbMetric: 40.1945 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 32/1000
2023-09-29 03:59:19.870 
Epoch 32/1000 
	 loss: 39.5118, MinusLogProbMetric: 39.5118, val_loss: 38.8578, val_MinusLogProbMetric: 38.8578

Epoch 32: val_loss improved from 40.19453 to 38.85783, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 39.5118 - MinusLogProbMetric: 39.5118 - val_loss: 38.8578 - val_MinusLogProbMetric: 38.8578 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 33/1000
2023-09-29 04:00:12.156 
Epoch 33/1000 
	 loss: 38.6028, MinusLogProbMetric: 38.6028, val_loss: 38.5379, val_MinusLogProbMetric: 38.5379

Epoch 33: val_loss improved from 38.85783 to 38.53790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 52s - loss: 38.6028 - MinusLogProbMetric: 38.6028 - val_loss: 38.5379 - val_MinusLogProbMetric: 38.5379 - lr: 0.0010 - 52s/epoch - 267ms/step
Epoch 34/1000
2023-09-29 04:01:07.334 
Epoch 34/1000 
	 loss: 38.8417, MinusLogProbMetric: 38.8417, val_loss: 38.5020, val_MinusLogProbMetric: 38.5020

Epoch 34: val_loss improved from 38.53790 to 38.50198, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 38.8417 - MinusLogProbMetric: 38.8417 - val_loss: 38.5020 - val_MinusLogProbMetric: 38.5020 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 35/1000
2023-09-29 04:02:00.725 
Epoch 35/1000 
	 loss: 37.9718, MinusLogProbMetric: 37.9718, val_loss: 37.1347, val_MinusLogProbMetric: 37.1347

Epoch 35: val_loss improved from 38.50198 to 37.13471, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 37.9718 - MinusLogProbMetric: 37.9718 - val_loss: 37.1347 - val_MinusLogProbMetric: 37.1347 - lr: 0.0010 - 54s/epoch - 273ms/step
Epoch 36/1000
2023-09-29 04:02:56.380 
Epoch 36/1000 
	 loss: 37.4601, MinusLogProbMetric: 37.4601, val_loss: 38.4833, val_MinusLogProbMetric: 38.4833

Epoch 36: val_loss did not improve from 37.13471
196/196 - 55s - loss: 37.4601 - MinusLogProbMetric: 37.4601 - val_loss: 38.4833 - val_MinusLogProbMetric: 38.4833 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 37/1000
2023-09-29 04:03:50.517 
Epoch 37/1000 
	 loss: 36.9556, MinusLogProbMetric: 36.9556, val_loss: 37.5478, val_MinusLogProbMetric: 37.5478

Epoch 37: val_loss did not improve from 37.13471
196/196 - 54s - loss: 36.9556 - MinusLogProbMetric: 36.9556 - val_loss: 37.5478 - val_MinusLogProbMetric: 37.5478 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 38/1000
2023-09-29 04:04:44.127 
Epoch 38/1000 
	 loss: 36.7890, MinusLogProbMetric: 36.7890, val_loss: 36.7758, val_MinusLogProbMetric: 36.7758

Epoch 38: val_loss improved from 37.13471 to 36.77579, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 36.7890 - MinusLogProbMetric: 36.7890 - val_loss: 36.7758 - val_MinusLogProbMetric: 36.7758 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 39/1000
2023-09-29 04:05:39.399 
Epoch 39/1000 
	 loss: 36.5581, MinusLogProbMetric: 36.5581, val_loss: 35.7075, val_MinusLogProbMetric: 35.7075

Epoch 39: val_loss improved from 36.77579 to 35.70754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 36.5581 - MinusLogProbMetric: 36.5581 - val_loss: 35.7075 - val_MinusLogProbMetric: 35.7075 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 40/1000
2023-09-29 04:06:34.088 
Epoch 40/1000 
	 loss: 36.0442, MinusLogProbMetric: 36.0442, val_loss: 37.1652, val_MinusLogProbMetric: 37.1652

Epoch 40: val_loss did not improve from 35.70754
196/196 - 54s - loss: 36.0442 - MinusLogProbMetric: 36.0442 - val_loss: 37.1652 - val_MinusLogProbMetric: 37.1652 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 41/1000
2023-09-29 04:07:29.140 
Epoch 41/1000 
	 loss: 35.8808, MinusLogProbMetric: 35.8808, val_loss: 36.3618, val_MinusLogProbMetric: 36.3618

Epoch 41: val_loss did not improve from 35.70754
196/196 - 55s - loss: 35.8808 - MinusLogProbMetric: 35.8808 - val_loss: 36.3618 - val_MinusLogProbMetric: 36.3618 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 42/1000
2023-09-29 04:08:22.398 
Epoch 42/1000 
	 loss: 35.5352, MinusLogProbMetric: 35.5352, val_loss: 35.1084, val_MinusLogProbMetric: 35.1084

Epoch 42: val_loss improved from 35.70754 to 35.10839, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 35.5352 - MinusLogProbMetric: 35.5352 - val_loss: 35.1084 - val_MinusLogProbMetric: 35.1084 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 43/1000
2023-09-29 04:09:17.142 
Epoch 43/1000 
	 loss: 35.3535, MinusLogProbMetric: 35.3535, val_loss: 34.8710, val_MinusLogProbMetric: 34.8710

Epoch 43: val_loss improved from 35.10839 to 34.87100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 35.3535 - MinusLogProbMetric: 35.3535 - val_loss: 34.8710 - val_MinusLogProbMetric: 34.8710 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 44/1000
2023-09-29 04:10:10.391 
Epoch 44/1000 
	 loss: 35.9355, MinusLogProbMetric: 35.9355, val_loss: 36.1893, val_MinusLogProbMetric: 36.1893

Epoch 44: val_loss did not improve from 34.87100
196/196 - 53s - loss: 35.9355 - MinusLogProbMetric: 35.9355 - val_loss: 36.1893 - val_MinusLogProbMetric: 36.1893 - lr: 0.0010 - 53s/epoch - 268ms/step
Epoch 45/1000
2023-09-29 04:11:04.933 
Epoch 45/1000 
	 loss: 35.0131, MinusLogProbMetric: 35.0131, val_loss: 36.1477, val_MinusLogProbMetric: 36.1477

Epoch 45: val_loss did not improve from 34.87100
196/196 - 55s - loss: 35.0131 - MinusLogProbMetric: 35.0131 - val_loss: 36.1477 - val_MinusLogProbMetric: 36.1477 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 46/1000
2023-09-29 04:11:57.153 
Epoch 46/1000 
	 loss: 35.0265, MinusLogProbMetric: 35.0265, val_loss: 34.8016, val_MinusLogProbMetric: 34.8016

Epoch 46: val_loss improved from 34.87100 to 34.80160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 53s - loss: 35.0265 - MinusLogProbMetric: 35.0265 - val_loss: 34.8016 - val_MinusLogProbMetric: 34.8016 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 47/1000
2023-09-29 04:12:50.440 
Epoch 47/1000 
	 loss: 34.7901, MinusLogProbMetric: 34.7901, val_loss: 34.8368, val_MinusLogProbMetric: 34.8368

Epoch 47: val_loss did not improve from 34.80160
196/196 - 52s - loss: 34.7901 - MinusLogProbMetric: 34.7901 - val_loss: 34.8368 - val_MinusLogProbMetric: 34.8368 - lr: 0.0010 - 52s/epoch - 268ms/step
Epoch 48/1000
2023-09-29 04:13:42.275 
Epoch 48/1000 
	 loss: 34.6283, MinusLogProbMetric: 34.6283, val_loss: 34.5285, val_MinusLogProbMetric: 34.5285

Epoch 48: val_loss improved from 34.80160 to 34.52852, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 53s - loss: 34.6283 - MinusLogProbMetric: 34.6283 - val_loss: 34.5285 - val_MinusLogProbMetric: 34.5285 - lr: 0.0010 - 53s/epoch - 268ms/step
Epoch 49/1000
2023-09-29 04:14:36.956 
Epoch 49/1000 
	 loss: 34.4618, MinusLogProbMetric: 34.4618, val_loss: 34.5979, val_MinusLogProbMetric: 34.5979

Epoch 49: val_loss did not improve from 34.52852
196/196 - 54s - loss: 34.4618 - MinusLogProbMetric: 34.4618 - val_loss: 34.5979 - val_MinusLogProbMetric: 34.5979 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 50/1000
2023-09-29 04:15:29.230 
Epoch 50/1000 
	 loss: 34.4897, MinusLogProbMetric: 34.4897, val_loss: 34.3282, val_MinusLogProbMetric: 34.3282

Epoch 50: val_loss improved from 34.52852 to 34.32820, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 53s - loss: 34.4897 - MinusLogProbMetric: 34.4897 - val_loss: 34.3282 - val_MinusLogProbMetric: 34.3282 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 51/1000
2023-09-29 04:16:21.604 
Epoch 51/1000 
	 loss: 34.1473, MinusLogProbMetric: 34.1473, val_loss: 34.9830, val_MinusLogProbMetric: 34.9830

Epoch 51: val_loss did not improve from 34.32820
196/196 - 52s - loss: 34.1473 - MinusLogProbMetric: 34.1473 - val_loss: 34.9830 - val_MinusLogProbMetric: 34.9830 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 52/1000
2023-09-29 04:17:16.038 
Epoch 52/1000 
	 loss: 34.1502, MinusLogProbMetric: 34.1502, val_loss: 33.5642, val_MinusLogProbMetric: 33.5642

Epoch 52: val_loss improved from 34.32820 to 33.56423, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 34.1502 - MinusLogProbMetric: 34.1502 - val_loss: 33.5642 - val_MinusLogProbMetric: 33.5642 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 53/1000
2023-09-29 04:18:11.154 
Epoch 53/1000 
	 loss: 33.7205, MinusLogProbMetric: 33.7205, val_loss: 34.3037, val_MinusLogProbMetric: 34.3037

Epoch 53: val_loss did not improve from 33.56423
196/196 - 54s - loss: 33.7205 - MinusLogProbMetric: 33.7205 - val_loss: 34.3037 - val_MinusLogProbMetric: 34.3037 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 54/1000
2023-09-29 04:19:05.262 
Epoch 54/1000 
	 loss: 34.0241, MinusLogProbMetric: 34.0241, val_loss: 34.2176, val_MinusLogProbMetric: 34.2176

Epoch 54: val_loss did not improve from 33.56423
196/196 - 54s - loss: 34.0241 - MinusLogProbMetric: 34.0241 - val_loss: 34.2176 - val_MinusLogProbMetric: 34.2176 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 55/1000
2023-09-29 04:19:57.543 
Epoch 55/1000 
	 loss: 33.6183, MinusLogProbMetric: 33.6183, val_loss: 34.1679, val_MinusLogProbMetric: 34.1679

Epoch 55: val_loss did not improve from 33.56423
196/196 - 52s - loss: 33.6183 - MinusLogProbMetric: 33.6183 - val_loss: 34.1679 - val_MinusLogProbMetric: 34.1679 - lr: 0.0010 - 52s/epoch - 267ms/step
Epoch 56/1000
2023-09-29 04:20:52.300 
Epoch 56/1000 
	 loss: 33.6893, MinusLogProbMetric: 33.6893, val_loss: 33.2967, val_MinusLogProbMetric: 33.2967

Epoch 56: val_loss improved from 33.56423 to 33.29670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 33.6893 - MinusLogProbMetric: 33.6893 - val_loss: 33.2967 - val_MinusLogProbMetric: 33.2967 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 57/1000
2023-09-29 04:21:46.435 
Epoch 57/1000 
	 loss: 33.3861, MinusLogProbMetric: 33.3861, val_loss: 33.4646, val_MinusLogProbMetric: 33.4646

Epoch 57: val_loss did not improve from 33.29670
196/196 - 53s - loss: 33.3861 - MinusLogProbMetric: 33.3861 - val_loss: 33.4646 - val_MinusLogProbMetric: 33.4646 - lr: 0.0010 - 53s/epoch - 273ms/step
Epoch 58/1000
2023-09-29 04:22:41.327 
Epoch 58/1000 
	 loss: 33.4559, MinusLogProbMetric: 33.4559, val_loss: 34.4598, val_MinusLogProbMetric: 34.4598

Epoch 58: val_loss did not improve from 33.29670
196/196 - 55s - loss: 33.4559 - MinusLogProbMetric: 33.4559 - val_loss: 34.4598 - val_MinusLogProbMetric: 34.4598 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 59/1000
2023-09-29 04:23:35.126 
Epoch 59/1000 
	 loss: 33.2589, MinusLogProbMetric: 33.2589, val_loss: 36.0939, val_MinusLogProbMetric: 36.0939

Epoch 59: val_loss did not improve from 33.29670
196/196 - 54s - loss: 33.2589 - MinusLogProbMetric: 33.2589 - val_loss: 36.0939 - val_MinusLogProbMetric: 36.0939 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 60/1000
2023-09-29 04:24:28.960 
Epoch 60/1000 
	 loss: 33.2898, MinusLogProbMetric: 33.2898, val_loss: 33.2647, val_MinusLogProbMetric: 33.2647

Epoch 60: val_loss improved from 33.29670 to 33.26466, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 33.2898 - MinusLogProbMetric: 33.2898 - val_loss: 33.2647 - val_MinusLogProbMetric: 33.2647 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 61/1000
2023-09-29 04:25:20.468 
Epoch 61/1000 
	 loss: 33.0165, MinusLogProbMetric: 33.0165, val_loss: 32.7924, val_MinusLogProbMetric: 32.7924

Epoch 61: val_loss improved from 33.26466 to 32.79242, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 52s - loss: 33.0165 - MinusLogProbMetric: 33.0165 - val_loss: 32.7924 - val_MinusLogProbMetric: 32.7924 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 62/1000
2023-09-29 04:26:14.160 
Epoch 62/1000 
	 loss: 33.0593, MinusLogProbMetric: 33.0593, val_loss: 33.1613, val_MinusLogProbMetric: 33.1613

Epoch 62: val_loss did not improve from 32.79242
196/196 - 53s - loss: 33.0593 - MinusLogProbMetric: 33.0593 - val_loss: 33.1613 - val_MinusLogProbMetric: 33.1613 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 63/1000
2023-09-29 04:27:07.568 
Epoch 63/1000 
	 loss: 33.0311, MinusLogProbMetric: 33.0311, val_loss: 32.7887, val_MinusLogProbMetric: 32.7887

Epoch 63: val_loss improved from 32.79242 to 32.78873, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 33.0311 - MinusLogProbMetric: 33.0311 - val_loss: 32.7887 - val_MinusLogProbMetric: 32.7887 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 64/1000
2023-09-29 04:28:02.729 
Epoch 64/1000 
	 loss: 32.7674, MinusLogProbMetric: 32.7674, val_loss: 33.6906, val_MinusLogProbMetric: 33.6906

Epoch 64: val_loss did not improve from 32.78873
196/196 - 54s - loss: 32.7674 - MinusLogProbMetric: 32.7674 - val_loss: 33.6906 - val_MinusLogProbMetric: 33.6906 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 65/1000
2023-09-29 04:28:56.113 
Epoch 65/1000 
	 loss: 32.8700, MinusLogProbMetric: 32.8700, val_loss: 32.7395, val_MinusLogProbMetric: 32.7395

Epoch 65: val_loss improved from 32.78873 to 32.73948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 32.8700 - MinusLogProbMetric: 32.8700 - val_loss: 32.7395 - val_MinusLogProbMetric: 32.7395 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 66/1000
2023-09-29 04:29:52.179 
Epoch 66/1000 
	 loss: 32.6549, MinusLogProbMetric: 32.6549, val_loss: 33.6357, val_MinusLogProbMetric: 33.6357

Epoch 66: val_loss did not improve from 32.73948
196/196 - 55s - loss: 32.6549 - MinusLogProbMetric: 32.6549 - val_loss: 33.6357 - val_MinusLogProbMetric: 33.6357 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 67/1000
2023-09-29 04:30:42.424 
Epoch 67/1000 
	 loss: 32.6905, MinusLogProbMetric: 32.6905, val_loss: 33.3133, val_MinusLogProbMetric: 33.3133

Epoch 67: val_loss did not improve from 32.73948
196/196 - 50s - loss: 32.6905 - MinusLogProbMetric: 32.6905 - val_loss: 33.3133 - val_MinusLogProbMetric: 33.3133 - lr: 0.0010 - 50s/epoch - 256ms/step
Epoch 68/1000
2023-09-29 04:31:31.729 
Epoch 68/1000 
	 loss: 32.7002, MinusLogProbMetric: 32.7002, val_loss: 34.0602, val_MinusLogProbMetric: 34.0602

Epoch 68: val_loss did not improve from 32.73948
196/196 - 49s - loss: 32.7002 - MinusLogProbMetric: 32.7002 - val_loss: 34.0602 - val_MinusLogProbMetric: 34.0602 - lr: 0.0010 - 49s/epoch - 252ms/step
Epoch 69/1000
2023-09-29 04:32:22.027 
Epoch 69/1000 
	 loss: 32.4919, MinusLogProbMetric: 32.4919, val_loss: 31.9180, val_MinusLogProbMetric: 31.9180

Epoch 69: val_loss improved from 32.73948 to 31.91796, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 51s - loss: 32.4919 - MinusLogProbMetric: 32.4919 - val_loss: 31.9180 - val_MinusLogProbMetric: 31.9180 - lr: 0.0010 - 51s/epoch - 259ms/step
Epoch 70/1000
2023-09-29 04:33:10.224 
Epoch 70/1000 
	 loss: 32.4401, MinusLogProbMetric: 32.4401, val_loss: 32.4907, val_MinusLogProbMetric: 32.4907

Epoch 70: val_loss did not improve from 31.91796
196/196 - 48s - loss: 32.4401 - MinusLogProbMetric: 32.4401 - val_loss: 32.4907 - val_MinusLogProbMetric: 32.4907 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 71/1000
2023-09-29 04:33:58.141 
Epoch 71/1000 
	 loss: 32.6289, MinusLogProbMetric: 32.6289, val_loss: 31.9676, val_MinusLogProbMetric: 31.9676

Epoch 71: val_loss did not improve from 31.91796
196/196 - 48s - loss: 32.6289 - MinusLogProbMetric: 32.6289 - val_loss: 31.9676 - val_MinusLogProbMetric: 31.9676 - lr: 0.0010 - 48s/epoch - 244ms/step
Epoch 72/1000
2023-09-29 04:34:45.724 
Epoch 72/1000 
	 loss: 32.3221, MinusLogProbMetric: 32.3221, val_loss: 32.2162, val_MinusLogProbMetric: 32.2162

Epoch 72: val_loss did not improve from 31.91796
196/196 - 48s - loss: 32.3221 - MinusLogProbMetric: 32.3221 - val_loss: 32.2162 - val_MinusLogProbMetric: 32.2162 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 73/1000
2023-09-29 04:35:32.304 
Epoch 73/1000 
	 loss: 32.3599, MinusLogProbMetric: 32.3599, val_loss: 32.3682, val_MinusLogProbMetric: 32.3682

Epoch 73: val_loss did not improve from 31.91796
196/196 - 47s - loss: 32.3599 - MinusLogProbMetric: 32.3599 - val_loss: 32.3682 - val_MinusLogProbMetric: 32.3682 - lr: 0.0010 - 47s/epoch - 238ms/step
Epoch 74/1000
2023-09-29 04:36:20.270 
Epoch 74/1000 
	 loss: 32.3915, MinusLogProbMetric: 32.3915, val_loss: 33.0870, val_MinusLogProbMetric: 33.0870

Epoch 74: val_loss did not improve from 31.91796
196/196 - 48s - loss: 32.3915 - MinusLogProbMetric: 32.3915 - val_loss: 33.0870 - val_MinusLogProbMetric: 33.0870 - lr: 0.0010 - 48s/epoch - 245ms/step
Epoch 75/1000
2023-09-29 04:37:09.488 
Epoch 75/1000 
	 loss: 32.2671, MinusLogProbMetric: 32.2671, val_loss: 31.9633, val_MinusLogProbMetric: 31.9633

Epoch 75: val_loss did not improve from 31.91796
196/196 - 49s - loss: 32.2671 - MinusLogProbMetric: 32.2671 - val_loss: 31.9633 - val_MinusLogProbMetric: 31.9633 - lr: 0.0010 - 49s/epoch - 251ms/step
Epoch 76/1000
2023-09-29 04:38:02.594 
Epoch 76/1000 
	 loss: 32.1166, MinusLogProbMetric: 32.1166, val_loss: 33.0041, val_MinusLogProbMetric: 33.0041

Epoch 76: val_loss did not improve from 31.91796
196/196 - 53s - loss: 32.1166 - MinusLogProbMetric: 32.1166 - val_loss: 33.0041 - val_MinusLogProbMetric: 33.0041 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 77/1000
2023-09-29 04:38:55.521 
Epoch 77/1000 
	 loss: 32.1871, MinusLogProbMetric: 32.1871, val_loss: 33.2364, val_MinusLogProbMetric: 33.2364

Epoch 77: val_loss did not improve from 31.91796
196/196 - 53s - loss: 32.1871 - MinusLogProbMetric: 32.1871 - val_loss: 33.2364 - val_MinusLogProbMetric: 33.2364 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 78/1000
2023-09-29 04:39:48.343 
Epoch 78/1000 
	 loss: 32.0217, MinusLogProbMetric: 32.0217, val_loss: 32.0097, val_MinusLogProbMetric: 32.0097

Epoch 78: val_loss did not improve from 31.91796
196/196 - 53s - loss: 32.0217 - MinusLogProbMetric: 32.0217 - val_loss: 32.0097 - val_MinusLogProbMetric: 32.0097 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 79/1000
2023-09-29 04:40:40.624 
Epoch 79/1000 
	 loss: 32.0139, MinusLogProbMetric: 32.0139, val_loss: 32.1649, val_MinusLogProbMetric: 32.1649

Epoch 79: val_loss did not improve from 31.91796
196/196 - 52s - loss: 32.0139 - MinusLogProbMetric: 32.0139 - val_loss: 32.1649 - val_MinusLogProbMetric: 32.1649 - lr: 0.0010 - 52s/epoch - 267ms/step
Epoch 80/1000
2023-09-29 04:41:32.352 
Epoch 80/1000 
	 loss: 31.9824, MinusLogProbMetric: 31.9824, val_loss: 32.9111, val_MinusLogProbMetric: 32.9111

Epoch 80: val_loss did not improve from 31.91796
196/196 - 52s - loss: 31.9824 - MinusLogProbMetric: 31.9824 - val_loss: 32.9111 - val_MinusLogProbMetric: 32.9111 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 81/1000
2023-09-29 04:42:25.381 
Epoch 81/1000 
	 loss: 32.3382, MinusLogProbMetric: 32.3382, val_loss: 32.9888, val_MinusLogProbMetric: 32.9888

Epoch 81: val_loss did not improve from 31.91796
196/196 - 53s - loss: 32.3382 - MinusLogProbMetric: 32.3382 - val_loss: 32.9888 - val_MinusLogProbMetric: 32.9888 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 82/1000
2023-09-29 04:43:17.969 
Epoch 82/1000 
	 loss: 31.9347, MinusLogProbMetric: 31.9347, val_loss: 32.0726, val_MinusLogProbMetric: 32.0726

Epoch 82: val_loss did not improve from 31.91796
196/196 - 53s - loss: 31.9347 - MinusLogProbMetric: 31.9347 - val_loss: 32.0726 - val_MinusLogProbMetric: 32.0726 - lr: 0.0010 - 53s/epoch - 268ms/step
Epoch 83/1000
2023-09-29 04:44:08.339 
Epoch 83/1000 
	 loss: 31.7649, MinusLogProbMetric: 31.7649, val_loss: 31.9024, val_MinusLogProbMetric: 31.9024

Epoch 83: val_loss improved from 31.91796 to 31.90238, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 51s - loss: 31.7649 - MinusLogProbMetric: 31.7649 - val_loss: 31.9024 - val_MinusLogProbMetric: 31.9024 - lr: 0.0010 - 51s/epoch - 261ms/step
Epoch 84/1000
2023-09-29 04:45:00.697 
Epoch 84/1000 
	 loss: 31.8143, MinusLogProbMetric: 31.8143, val_loss: 31.9361, val_MinusLogProbMetric: 31.9361

Epoch 84: val_loss did not improve from 31.90238
196/196 - 52s - loss: 31.8143 - MinusLogProbMetric: 31.8143 - val_loss: 31.9361 - val_MinusLogProbMetric: 31.9361 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 85/1000
2023-09-29 04:45:52.541 
Epoch 85/1000 
	 loss: 31.8509, MinusLogProbMetric: 31.8509, val_loss: 31.9506, val_MinusLogProbMetric: 31.9506

Epoch 85: val_loss did not improve from 31.90238
196/196 - 52s - loss: 31.8509 - MinusLogProbMetric: 31.8509 - val_loss: 31.9506 - val_MinusLogProbMetric: 31.9506 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 86/1000
2023-09-29 04:46:44.023 
Epoch 86/1000 
	 loss: 31.6643, MinusLogProbMetric: 31.6643, val_loss: 31.8569, val_MinusLogProbMetric: 31.8569

Epoch 86: val_loss improved from 31.90238 to 31.85694, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 52s - loss: 31.6643 - MinusLogProbMetric: 31.6643 - val_loss: 31.8569 - val_MinusLogProbMetric: 31.8569 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 87/1000
2023-09-29 04:47:35.975 
Epoch 87/1000 
	 loss: 31.5970, MinusLogProbMetric: 31.5970, val_loss: 31.7320, val_MinusLogProbMetric: 31.7320

Epoch 87: val_loss improved from 31.85694 to 31.73195, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 52s - loss: 31.5970 - MinusLogProbMetric: 31.5970 - val_loss: 31.7320 - val_MinusLogProbMetric: 31.7320 - lr: 0.0010 - 52s/epoch - 265ms/step
Epoch 88/1000
2023-09-29 04:48:29.570 
Epoch 88/1000 
	 loss: 31.6173, MinusLogProbMetric: 31.6173, val_loss: 31.1368, val_MinusLogProbMetric: 31.1368

Epoch 88: val_loss improved from 31.73195 to 31.13684, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 31.6173 - MinusLogProbMetric: 31.6173 - val_loss: 31.1368 - val_MinusLogProbMetric: 31.1368 - lr: 0.0010 - 54s/epoch - 273ms/step
Epoch 89/1000
2023-09-29 04:49:22.975 
Epoch 89/1000 
	 loss: 31.5739, MinusLogProbMetric: 31.5739, val_loss: 32.5008, val_MinusLogProbMetric: 32.5008

Epoch 89: val_loss did not improve from 31.13684
196/196 - 53s - loss: 31.5739 - MinusLogProbMetric: 31.5739 - val_loss: 32.5008 - val_MinusLogProbMetric: 32.5008 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 90/1000
2023-09-29 04:50:15.112 
Epoch 90/1000 
	 loss: 31.7611, MinusLogProbMetric: 31.7611, val_loss: 33.0132, val_MinusLogProbMetric: 33.0132

Epoch 90: val_loss did not improve from 31.13684
196/196 - 52s - loss: 31.7611 - MinusLogProbMetric: 31.7611 - val_loss: 33.0132 - val_MinusLogProbMetric: 33.0132 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 91/1000
2023-09-29 04:51:05.021 
Epoch 91/1000 
	 loss: 31.4748, MinusLogProbMetric: 31.4748, val_loss: 31.5976, val_MinusLogProbMetric: 31.5976

Epoch 91: val_loss did not improve from 31.13684
196/196 - 50s - loss: 31.4748 - MinusLogProbMetric: 31.4748 - val_loss: 31.5976 - val_MinusLogProbMetric: 31.5976 - lr: 0.0010 - 50s/epoch - 255ms/step
Epoch 92/1000
2023-09-29 04:51:56.794 
Epoch 92/1000 
	 loss: 31.4872, MinusLogProbMetric: 31.4872, val_loss: 31.6437, val_MinusLogProbMetric: 31.6437

Epoch 92: val_loss did not improve from 31.13684
196/196 - 52s - loss: 31.4872 - MinusLogProbMetric: 31.4872 - val_loss: 31.6437 - val_MinusLogProbMetric: 31.6437 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 93/1000
2023-09-29 04:52:50.216 
Epoch 93/1000 
	 loss: 38.1505, MinusLogProbMetric: 38.1505, val_loss: 39.6527, val_MinusLogProbMetric: 39.6527

Epoch 93: val_loss did not improve from 31.13684
196/196 - 53s - loss: 38.1505 - MinusLogProbMetric: 38.1505 - val_loss: 39.6527 - val_MinusLogProbMetric: 39.6527 - lr: 0.0010 - 53s/epoch - 273ms/step
Epoch 94/1000
2023-09-29 04:53:43.497 
Epoch 94/1000 
	 loss: 34.8567, MinusLogProbMetric: 34.8567, val_loss: 33.2822, val_MinusLogProbMetric: 33.2822

Epoch 94: val_loss did not improve from 31.13684
196/196 - 53s - loss: 34.8567 - MinusLogProbMetric: 34.8567 - val_loss: 33.2822 - val_MinusLogProbMetric: 33.2822 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 95/1000
2023-09-29 04:54:33.562 
Epoch 95/1000 
	 loss: 32.7553, MinusLogProbMetric: 32.7553, val_loss: 32.2807, val_MinusLogProbMetric: 32.2807

Epoch 95: val_loss did not improve from 31.13684
196/196 - 50s - loss: 32.7553 - MinusLogProbMetric: 32.7553 - val_loss: 32.2807 - val_MinusLogProbMetric: 32.2807 - lr: 0.0010 - 50s/epoch - 255ms/step
Epoch 96/1000
2023-09-29 04:55:27.564 
Epoch 96/1000 
	 loss: 33.7329, MinusLogProbMetric: 33.7329, val_loss: 32.8028, val_MinusLogProbMetric: 32.8028

Epoch 96: val_loss did not improve from 31.13684
196/196 - 54s - loss: 33.7329 - MinusLogProbMetric: 33.7329 - val_loss: 32.8028 - val_MinusLogProbMetric: 32.8028 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 97/1000
2023-09-29 04:56:20.321 
Epoch 97/1000 
	 loss: 32.6011, MinusLogProbMetric: 32.6011, val_loss: 34.3652, val_MinusLogProbMetric: 34.3652

Epoch 97: val_loss did not improve from 31.13684
196/196 - 53s - loss: 32.6011 - MinusLogProbMetric: 32.6011 - val_loss: 34.3652 - val_MinusLogProbMetric: 34.3652 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 98/1000
2023-09-29 04:57:12.146 
Epoch 98/1000 
	 loss: 33.0005, MinusLogProbMetric: 33.0005, val_loss: 35.0623, val_MinusLogProbMetric: 35.0623

Epoch 98: val_loss did not improve from 31.13684
196/196 - 52s - loss: 33.0005 - MinusLogProbMetric: 33.0005 - val_loss: 35.0623 - val_MinusLogProbMetric: 35.0623 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 99/1000
2023-09-29 04:58:02.556 
Epoch 99/1000 
	 loss: 32.6895, MinusLogProbMetric: 32.6895, val_loss: 32.7675, val_MinusLogProbMetric: 32.7675

Epoch 99: val_loss did not improve from 31.13684
196/196 - 50s - loss: 32.6895 - MinusLogProbMetric: 32.6895 - val_loss: 32.7675 - val_MinusLogProbMetric: 32.7675 - lr: 0.0010 - 50s/epoch - 257ms/step
Epoch 100/1000
2023-09-29 04:58:55.862 
Epoch 100/1000 
	 loss: 32.2764, MinusLogProbMetric: 32.2764, val_loss: 32.6303, val_MinusLogProbMetric: 32.6303

Epoch 100: val_loss did not improve from 31.13684
196/196 - 53s - loss: 32.2764 - MinusLogProbMetric: 32.2764 - val_loss: 32.6303 - val_MinusLogProbMetric: 32.6303 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 101/1000
2023-09-29 04:59:48.256 
Epoch 101/1000 
	 loss: 32.4209, MinusLogProbMetric: 32.4209, val_loss: 32.2446, val_MinusLogProbMetric: 32.2446

Epoch 101: val_loss did not improve from 31.13684
196/196 - 52s - loss: 32.4209 - MinusLogProbMetric: 32.4209 - val_loss: 32.2446 - val_MinusLogProbMetric: 32.2446 - lr: 0.0010 - 52s/epoch - 267ms/step
Epoch 102/1000
2023-09-29 05:00:39.826 
Epoch 102/1000 
	 loss: 32.3216, MinusLogProbMetric: 32.3216, val_loss: 31.7639, val_MinusLogProbMetric: 31.7639

Epoch 102: val_loss did not improve from 31.13684
196/196 - 52s - loss: 32.3216 - MinusLogProbMetric: 32.3216 - val_loss: 31.7639 - val_MinusLogProbMetric: 31.7639 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 103/1000
2023-09-29 05:01:32.023 
Epoch 103/1000 
	 loss: 31.9407, MinusLogProbMetric: 31.9407, val_loss: 31.5935, val_MinusLogProbMetric: 31.5935

Epoch 103: val_loss did not improve from 31.13684
196/196 - 52s - loss: 31.9407 - MinusLogProbMetric: 31.9407 - val_loss: 31.5935 - val_MinusLogProbMetric: 31.5935 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 104/1000
2023-09-29 05:02:21.646 
Epoch 104/1000 
	 loss: 33.2750, MinusLogProbMetric: 33.2750, val_loss: 32.3348, val_MinusLogProbMetric: 32.3348

Epoch 104: val_loss did not improve from 31.13684
196/196 - 50s - loss: 33.2750 - MinusLogProbMetric: 33.2750 - val_loss: 32.3348 - val_MinusLogProbMetric: 32.3348 - lr: 0.0010 - 50s/epoch - 253ms/step
Epoch 105/1000
2023-09-29 05:03:11.225 
Epoch 105/1000 
	 loss: 32.7751, MinusLogProbMetric: 32.7751, val_loss: 31.8819, val_MinusLogProbMetric: 31.8819

Epoch 105: val_loss did not improve from 31.13684
196/196 - 50s - loss: 32.7751 - MinusLogProbMetric: 32.7751 - val_loss: 31.8819 - val_MinusLogProbMetric: 31.8819 - lr: 0.0010 - 50s/epoch - 253ms/step
Epoch 106/1000
2023-09-29 05:04:05.267 
Epoch 106/1000 
	 loss: 32.2044, MinusLogProbMetric: 32.2044, val_loss: 31.8519, val_MinusLogProbMetric: 31.8519

Epoch 106: val_loss did not improve from 31.13684
196/196 - 54s - loss: 32.2044 - MinusLogProbMetric: 32.2044 - val_loss: 31.8519 - val_MinusLogProbMetric: 31.8519 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 107/1000
2023-09-29 05:04:57.203 
Epoch 107/1000 
	 loss: 32.4407, MinusLogProbMetric: 32.4407, val_loss: 31.7485, val_MinusLogProbMetric: 31.7485

Epoch 107: val_loss did not improve from 31.13684
196/196 - 52s - loss: 32.4407 - MinusLogProbMetric: 32.4407 - val_loss: 31.7485 - val_MinusLogProbMetric: 31.7485 - lr: 0.0010 - 52s/epoch - 265ms/step
Epoch 108/1000
2023-09-29 05:05:48.410 
Epoch 108/1000 
	 loss: 32.2741, MinusLogProbMetric: 32.2741, val_loss: 33.7613, val_MinusLogProbMetric: 33.7613

Epoch 108: val_loss did not improve from 31.13684
196/196 - 51s - loss: 32.2741 - MinusLogProbMetric: 32.2741 - val_loss: 33.7613 - val_MinusLogProbMetric: 33.7613 - lr: 0.0010 - 51s/epoch - 261ms/step
Epoch 109/1000
2023-09-29 05:06:38.448 
Epoch 109/1000 
	 loss: 32.5308, MinusLogProbMetric: 32.5308, val_loss: 31.6843, val_MinusLogProbMetric: 31.6843

Epoch 109: val_loss did not improve from 31.13684
196/196 - 50s - loss: 32.5308 - MinusLogProbMetric: 32.5308 - val_loss: 31.6843 - val_MinusLogProbMetric: 31.6843 - lr: 0.0010 - 50s/epoch - 255ms/step
Epoch 110/1000
2023-09-29 05:07:28.530 
Epoch 110/1000 
	 loss: 32.1206, MinusLogProbMetric: 32.1206, val_loss: 33.4712, val_MinusLogProbMetric: 33.4712

Epoch 110: val_loss did not improve from 31.13684
196/196 - 50s - loss: 32.1206 - MinusLogProbMetric: 32.1206 - val_loss: 33.4712 - val_MinusLogProbMetric: 33.4712 - lr: 0.0010 - 50s/epoch - 255ms/step
Epoch 111/1000
2023-09-29 05:08:19.480 
Epoch 111/1000 
	 loss: 32.0120, MinusLogProbMetric: 32.0120, val_loss: 32.0255, val_MinusLogProbMetric: 32.0255

Epoch 111: val_loss did not improve from 31.13684
196/196 - 51s - loss: 32.0120 - MinusLogProbMetric: 32.0120 - val_loss: 32.0255 - val_MinusLogProbMetric: 32.0255 - lr: 0.0010 - 51s/epoch - 260ms/step
Epoch 112/1000
2023-09-29 05:09:10.638 
Epoch 112/1000 
	 loss: 31.9722, MinusLogProbMetric: 31.9722, val_loss: 34.7262, val_MinusLogProbMetric: 34.7262

Epoch 112: val_loss did not improve from 31.13684
196/196 - 51s - loss: 31.9722 - MinusLogProbMetric: 31.9722 - val_loss: 34.7262 - val_MinusLogProbMetric: 34.7262 - lr: 0.0010 - 51s/epoch - 261ms/step
Epoch 113/1000
2023-09-29 05:10:03.199 
Epoch 113/1000 
	 loss: 32.0592, MinusLogProbMetric: 32.0592, val_loss: 31.9818, val_MinusLogProbMetric: 31.9818

Epoch 113: val_loss did not improve from 31.13684
196/196 - 53s - loss: 32.0592 - MinusLogProbMetric: 32.0592 - val_loss: 31.9818 - val_MinusLogProbMetric: 31.9818 - lr: 0.0010 - 53s/epoch - 268ms/step
Epoch 114/1000
2023-09-29 05:10:52.989 
Epoch 114/1000 
	 loss: 32.0397, MinusLogProbMetric: 32.0397, val_loss: 34.9462, val_MinusLogProbMetric: 34.9462

Epoch 114: val_loss did not improve from 31.13684
196/196 - 50s - loss: 32.0397 - MinusLogProbMetric: 32.0397 - val_loss: 34.9462 - val_MinusLogProbMetric: 34.9462 - lr: 0.0010 - 50s/epoch - 254ms/step
Epoch 115/1000
2023-09-29 05:11:44.212 
Epoch 115/1000 
	 loss: 32.0327, MinusLogProbMetric: 32.0327, val_loss: 31.8182, val_MinusLogProbMetric: 31.8182

Epoch 115: val_loss did not improve from 31.13684
196/196 - 51s - loss: 32.0327 - MinusLogProbMetric: 32.0327 - val_loss: 31.8182 - val_MinusLogProbMetric: 31.8182 - lr: 0.0010 - 51s/epoch - 261ms/step
Epoch 116/1000
2023-09-29 05:12:36.837 
Epoch 116/1000 
	 loss: 32.0727, MinusLogProbMetric: 32.0727, val_loss: 32.0134, val_MinusLogProbMetric: 32.0134

Epoch 116: val_loss did not improve from 31.13684
196/196 - 53s - loss: 32.0727 - MinusLogProbMetric: 32.0727 - val_loss: 32.0134 - val_MinusLogProbMetric: 32.0134 - lr: 0.0010 - 53s/epoch - 268ms/step
Epoch 117/1000
2023-09-29 05:13:28.252 
Epoch 117/1000 
	 loss: 31.7746, MinusLogProbMetric: 31.7746, val_loss: 34.4560, val_MinusLogProbMetric: 34.4560

Epoch 117: val_loss did not improve from 31.13684
196/196 - 51s - loss: 31.7746 - MinusLogProbMetric: 31.7746 - val_loss: 34.4560 - val_MinusLogProbMetric: 34.4560 - lr: 0.0010 - 51s/epoch - 262ms/step
Epoch 118/1000
2023-09-29 05:14:20.767 
Epoch 118/1000 
	 loss: 31.4160, MinusLogProbMetric: 31.4160, val_loss: 31.6622, val_MinusLogProbMetric: 31.6622

Epoch 118: val_loss did not improve from 31.13684
196/196 - 53s - loss: 31.4160 - MinusLogProbMetric: 31.4160 - val_loss: 31.6622 - val_MinusLogProbMetric: 31.6622 - lr: 0.0010 - 53s/epoch - 268ms/step
Epoch 119/1000
2023-09-29 05:15:14.406 
Epoch 119/1000 
	 loss: 31.6088, MinusLogProbMetric: 31.6088, val_loss: 32.0385, val_MinusLogProbMetric: 32.0385

Epoch 119: val_loss did not improve from 31.13684
196/196 - 54s - loss: 31.6088 - MinusLogProbMetric: 31.6088 - val_loss: 32.0385 - val_MinusLogProbMetric: 32.0385 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 120/1000
2023-09-29 05:16:06.115 
Epoch 120/1000 
	 loss: 31.1476, MinusLogProbMetric: 31.1476, val_loss: 31.4216, val_MinusLogProbMetric: 31.4216

Epoch 120: val_loss did not improve from 31.13684
196/196 - 52s - loss: 31.1476 - MinusLogProbMetric: 31.1476 - val_loss: 31.4216 - val_MinusLogProbMetric: 31.4216 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 121/1000
2023-09-29 05:16:59.745 
Epoch 121/1000 
	 loss: 31.1460, MinusLogProbMetric: 31.1460, val_loss: 31.2602, val_MinusLogProbMetric: 31.2602

Epoch 121: val_loss did not improve from 31.13684
196/196 - 54s - loss: 31.1460 - MinusLogProbMetric: 31.1460 - val_loss: 31.2602 - val_MinusLogProbMetric: 31.2602 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 122/1000
2023-09-29 05:17:52.106 
Epoch 122/1000 
	 loss: 30.9366, MinusLogProbMetric: 30.9366, val_loss: 31.0296, val_MinusLogProbMetric: 31.0296

Epoch 122: val_loss improved from 31.13684 to 31.02957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 53s - loss: 30.9366 - MinusLogProbMetric: 30.9366 - val_loss: 31.0296 - val_MinusLogProbMetric: 31.0296 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 123/1000
2023-09-29 05:18:46.895 
Epoch 123/1000 
	 loss: 31.0785, MinusLogProbMetric: 31.0785, val_loss: 31.5332, val_MinusLogProbMetric: 31.5332

Epoch 123: val_loss did not improve from 31.02957
196/196 - 54s - loss: 31.0785 - MinusLogProbMetric: 31.0785 - val_loss: 31.5332 - val_MinusLogProbMetric: 31.5332 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 124/1000
2023-09-29 05:19:40.631 
Epoch 124/1000 
	 loss: 30.9303, MinusLogProbMetric: 30.9303, val_loss: 31.1769, val_MinusLogProbMetric: 31.1769

Epoch 124: val_loss did not improve from 31.02957
196/196 - 54s - loss: 30.9303 - MinusLogProbMetric: 30.9303 - val_loss: 31.1769 - val_MinusLogProbMetric: 31.1769 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 125/1000
2023-09-29 05:20:34.170 
Epoch 125/1000 
	 loss: 31.2326, MinusLogProbMetric: 31.2326, val_loss: 31.1027, val_MinusLogProbMetric: 31.1027

Epoch 125: val_loss did not improve from 31.02957
196/196 - 54s - loss: 31.2326 - MinusLogProbMetric: 31.2326 - val_loss: 31.1027 - val_MinusLogProbMetric: 31.1027 - lr: 0.0010 - 54s/epoch - 273ms/step
Epoch 126/1000
2023-09-29 05:21:26.992 
Epoch 126/1000 
	 loss: 30.9333, MinusLogProbMetric: 30.9333, val_loss: 31.0717, val_MinusLogProbMetric: 31.0717

Epoch 126: val_loss did not improve from 31.02957
196/196 - 53s - loss: 30.9333 - MinusLogProbMetric: 30.9333 - val_loss: 31.0717 - val_MinusLogProbMetric: 31.0717 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 127/1000
2023-09-29 05:22:18.446 
Epoch 127/1000 
	 loss: 30.7906, MinusLogProbMetric: 30.7906, val_loss: 30.8912, val_MinusLogProbMetric: 30.8912

Epoch 127: val_loss improved from 31.02957 to 30.89118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 52s - loss: 30.7906 - MinusLogProbMetric: 30.7906 - val_loss: 30.8912 - val_MinusLogProbMetric: 30.8912 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 128/1000
2023-09-29 05:23:10.087 
Epoch 128/1000 
	 loss: 30.8313, MinusLogProbMetric: 30.8313, val_loss: 30.9868, val_MinusLogProbMetric: 30.9868

Epoch 128: val_loss did not improve from 30.89118
196/196 - 51s - loss: 30.8313 - MinusLogProbMetric: 30.8313 - val_loss: 30.9868 - val_MinusLogProbMetric: 30.9868 - lr: 0.0010 - 51s/epoch - 260ms/step
Epoch 129/1000
2023-09-29 05:24:01.892 
Epoch 129/1000 
	 loss: 30.7498, MinusLogProbMetric: 30.7498, val_loss: 30.7183, val_MinusLogProbMetric: 30.7183

Epoch 129: val_loss improved from 30.89118 to 30.71834, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 53s - loss: 30.7498 - MinusLogProbMetric: 30.7498 - val_loss: 30.7183 - val_MinusLogProbMetric: 30.7183 - lr: 0.0010 - 53s/epoch - 268ms/step
Epoch 130/1000
2023-09-29 05:24:53.367 
Epoch 130/1000 
	 loss: 30.7431, MinusLogProbMetric: 30.7431, val_loss: 30.9678, val_MinusLogProbMetric: 30.9678

Epoch 130: val_loss did not improve from 30.71834
196/196 - 51s - loss: 30.7431 - MinusLogProbMetric: 30.7431 - val_loss: 30.9678 - val_MinusLogProbMetric: 30.9678 - lr: 0.0010 - 51s/epoch - 259ms/step
Epoch 131/1000
2023-09-29 05:25:44.830 
Epoch 131/1000 
	 loss: 30.6399, MinusLogProbMetric: 30.6399, val_loss: 30.4773, val_MinusLogProbMetric: 30.4773

Epoch 131: val_loss improved from 30.71834 to 30.47726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 52s - loss: 30.6399 - MinusLogProbMetric: 30.6399 - val_loss: 30.4773 - val_MinusLogProbMetric: 30.4773 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 132/1000
2023-09-29 05:26:39.490 
Epoch 132/1000 
	 loss: 30.8678, MinusLogProbMetric: 30.8678, val_loss: 30.5755, val_MinusLogProbMetric: 30.5755

Epoch 132: val_loss did not improve from 30.47726
196/196 - 54s - loss: 30.8678 - MinusLogProbMetric: 30.8678 - val_loss: 30.5755 - val_MinusLogProbMetric: 30.5755 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 133/1000
2023-09-29 05:27:33.075 
Epoch 133/1000 
	 loss: 30.5664, MinusLogProbMetric: 30.5664, val_loss: 30.4805, val_MinusLogProbMetric: 30.4805

Epoch 133: val_loss did not improve from 30.47726
196/196 - 54s - loss: 30.5664 - MinusLogProbMetric: 30.5664 - val_loss: 30.4805 - val_MinusLogProbMetric: 30.4805 - lr: 0.0010 - 54s/epoch - 273ms/step
Epoch 134/1000
2023-09-29 05:28:22.466 
Epoch 134/1000 
	 loss: 30.5469, MinusLogProbMetric: 30.5469, val_loss: 32.3106, val_MinusLogProbMetric: 32.3106

Epoch 134: val_loss did not improve from 30.47726
196/196 - 49s - loss: 30.5469 - MinusLogProbMetric: 30.5469 - val_loss: 32.3106 - val_MinusLogProbMetric: 32.3106 - lr: 0.0010 - 49s/epoch - 252ms/step
Epoch 135/1000
2023-09-29 05:29:16.671 
Epoch 135/1000 
	 loss: 30.5722, MinusLogProbMetric: 30.5722, val_loss: 30.3025, val_MinusLogProbMetric: 30.3025

Epoch 135: val_loss improved from 30.47726 to 30.30250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 30.5722 - MinusLogProbMetric: 30.5722 - val_loss: 30.3025 - val_MinusLogProbMetric: 30.3025 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 136/1000
2023-09-29 05:30:10.482 
Epoch 136/1000 
	 loss: 30.5723, MinusLogProbMetric: 30.5723, val_loss: 30.4686, val_MinusLogProbMetric: 30.4686

Epoch 136: val_loss did not improve from 30.30250
196/196 - 53s - loss: 30.5723 - MinusLogProbMetric: 30.5723 - val_loss: 30.4686 - val_MinusLogProbMetric: 30.4686 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 137/1000
2023-09-29 05:31:04.299 
Epoch 137/1000 
	 loss: 30.5192, MinusLogProbMetric: 30.5192, val_loss: 30.6054, val_MinusLogProbMetric: 30.6054

Epoch 137: val_loss did not improve from 30.30250
196/196 - 54s - loss: 30.5192 - MinusLogProbMetric: 30.5192 - val_loss: 30.6054 - val_MinusLogProbMetric: 30.6054 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 138/1000
2023-09-29 05:31:58.361 
Epoch 138/1000 
	 loss: 31.0190, MinusLogProbMetric: 31.0190, val_loss: 30.9051, val_MinusLogProbMetric: 30.9051

Epoch 138: val_loss did not improve from 30.30250
196/196 - 54s - loss: 31.0190 - MinusLogProbMetric: 31.0190 - val_loss: 30.9051 - val_MinusLogProbMetric: 30.9051 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 139/1000
2023-09-29 05:32:50.029 
Epoch 139/1000 
	 loss: 30.6849, MinusLogProbMetric: 30.6849, val_loss: 30.4929, val_MinusLogProbMetric: 30.4929

Epoch 139: val_loss did not improve from 30.30250
196/196 - 52s - loss: 30.6849 - MinusLogProbMetric: 30.6849 - val_loss: 30.4929 - val_MinusLogProbMetric: 30.4929 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 140/1000
2023-09-29 05:33:42.263 
Epoch 140/1000 
	 loss: 30.4976, MinusLogProbMetric: 30.4976, val_loss: 30.5463, val_MinusLogProbMetric: 30.5463

Epoch 140: val_loss did not improve from 30.30250
196/196 - 52s - loss: 30.4976 - MinusLogProbMetric: 30.4976 - val_loss: 30.5463 - val_MinusLogProbMetric: 30.5463 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 141/1000
2023-09-29 05:34:34.306 
Epoch 141/1000 
	 loss: 30.4969, MinusLogProbMetric: 30.4969, val_loss: 31.7081, val_MinusLogProbMetric: 31.7081

Epoch 141: val_loss did not improve from 30.30250
196/196 - 52s - loss: 30.4969 - MinusLogProbMetric: 30.4969 - val_loss: 31.7081 - val_MinusLogProbMetric: 31.7081 - lr: 0.0010 - 52s/epoch - 265ms/step
Epoch 142/1000
2023-09-29 05:35:29.168 
Epoch 142/1000 
	 loss: 30.5190, MinusLogProbMetric: 30.5190, val_loss: 30.9171, val_MinusLogProbMetric: 30.9171

Epoch 142: val_loss did not improve from 30.30250
196/196 - 55s - loss: 30.5190 - MinusLogProbMetric: 30.5190 - val_loss: 30.9171 - val_MinusLogProbMetric: 30.9171 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 143/1000
2023-09-29 05:36:20.231 
Epoch 143/1000 
	 loss: 30.6982, MinusLogProbMetric: 30.6982, val_loss: 30.4138, val_MinusLogProbMetric: 30.4138

Epoch 143: val_loss did not improve from 30.30250
196/196 - 51s - loss: 30.6982 - MinusLogProbMetric: 30.6982 - val_loss: 30.4138 - val_MinusLogProbMetric: 30.4138 - lr: 0.0010 - 51s/epoch - 260ms/step
Epoch 144/1000
2023-09-29 05:37:12.436 
Epoch 144/1000 
	 loss: 30.3747, MinusLogProbMetric: 30.3747, val_loss: 30.2166, val_MinusLogProbMetric: 30.2166

Epoch 144: val_loss improved from 30.30250 to 30.21659, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 53s - loss: 30.3747 - MinusLogProbMetric: 30.3747 - val_loss: 30.2166 - val_MinusLogProbMetric: 30.2166 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 145/1000
2023-09-29 05:38:04.576 
Epoch 145/1000 
	 loss: 30.4253, MinusLogProbMetric: 30.4253, val_loss: 30.7129, val_MinusLogProbMetric: 30.7129

Epoch 145: val_loss did not improve from 30.21659
196/196 - 52s - loss: 30.4253 - MinusLogProbMetric: 30.4253 - val_loss: 30.7129 - val_MinusLogProbMetric: 30.7129 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 146/1000
2023-09-29 05:38:57.681 
Epoch 146/1000 
	 loss: 30.3427, MinusLogProbMetric: 30.3427, val_loss: 31.2840, val_MinusLogProbMetric: 31.2840

Epoch 146: val_loss did not improve from 30.21659
196/196 - 53s - loss: 30.3427 - MinusLogProbMetric: 30.3427 - val_loss: 31.2840 - val_MinusLogProbMetric: 31.2840 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 147/1000
2023-09-29 05:39:51.877 
Epoch 147/1000 
	 loss: 30.3194, MinusLogProbMetric: 30.3194, val_loss: 30.1179, val_MinusLogProbMetric: 30.1179

Epoch 147: val_loss improved from 30.21659 to 30.11790, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 30.3194 - MinusLogProbMetric: 30.3194 - val_loss: 30.1179 - val_MinusLogProbMetric: 30.1179 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 148/1000
2023-09-29 05:40:46.360 
Epoch 148/1000 
	 loss: 30.5124, MinusLogProbMetric: 30.5124, val_loss: 31.0567, val_MinusLogProbMetric: 31.0567

Epoch 148: val_loss did not improve from 30.11790
196/196 - 54s - loss: 30.5124 - MinusLogProbMetric: 30.5124 - val_loss: 31.0567 - val_MinusLogProbMetric: 31.0567 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 149/1000
2023-09-29 05:41:39.330 
Epoch 149/1000 
	 loss: 30.5389, MinusLogProbMetric: 30.5389, val_loss: 31.6302, val_MinusLogProbMetric: 31.6302

Epoch 149: val_loss did not improve from 30.11790
196/196 - 53s - loss: 30.5389 - MinusLogProbMetric: 30.5389 - val_loss: 31.6302 - val_MinusLogProbMetric: 31.6302 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 150/1000
2023-09-29 05:42:32.912 
Epoch 150/1000 
	 loss: 30.4412, MinusLogProbMetric: 30.4412, val_loss: 30.9669, val_MinusLogProbMetric: 30.9669

Epoch 150: val_loss did not improve from 30.11790
196/196 - 54s - loss: 30.4412 - MinusLogProbMetric: 30.4412 - val_loss: 30.9669 - val_MinusLogProbMetric: 30.9669 - lr: 0.0010 - 54s/epoch - 273ms/step
Epoch 151/1000
2023-09-29 05:43:24.494 
Epoch 151/1000 
	 loss: 30.2412, MinusLogProbMetric: 30.2412, val_loss: 30.1977, val_MinusLogProbMetric: 30.1977

Epoch 151: val_loss did not improve from 30.11790
196/196 - 52s - loss: 30.2412 - MinusLogProbMetric: 30.2412 - val_loss: 30.1977 - val_MinusLogProbMetric: 30.1977 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 152/1000
2023-09-29 05:44:18.223 
Epoch 152/1000 
	 loss: 30.3679, MinusLogProbMetric: 30.3679, val_loss: 30.5804, val_MinusLogProbMetric: 30.5804

Epoch 152: val_loss did not improve from 30.11790
196/196 - 54s - loss: 30.3679 - MinusLogProbMetric: 30.3679 - val_loss: 30.5804 - val_MinusLogProbMetric: 30.5804 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 153/1000
2023-09-29 05:45:11.622 
Epoch 153/1000 
	 loss: 30.2600, MinusLogProbMetric: 30.2600, val_loss: 30.5126, val_MinusLogProbMetric: 30.5126

Epoch 153: val_loss did not improve from 30.11790
196/196 - 53s - loss: 30.2600 - MinusLogProbMetric: 30.2600 - val_loss: 30.5126 - val_MinusLogProbMetric: 30.5126 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 154/1000
2023-09-29 05:46:05.742 
Epoch 154/1000 
	 loss: 30.2723, MinusLogProbMetric: 30.2723, val_loss: 32.4666, val_MinusLogProbMetric: 32.4666

Epoch 154: val_loss did not improve from 30.11790
196/196 - 54s - loss: 30.2723 - MinusLogProbMetric: 30.2723 - val_loss: 32.4666 - val_MinusLogProbMetric: 32.4666 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 155/1000
2023-09-29 05:46:59.309 
Epoch 155/1000 
	 loss: 30.2500, MinusLogProbMetric: 30.2500, val_loss: 29.8807, val_MinusLogProbMetric: 29.8807

Epoch 155: val_loss improved from 30.11790 to 29.88072, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 30.2500 - MinusLogProbMetric: 30.2500 - val_loss: 29.8807 - val_MinusLogProbMetric: 29.8807 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 156/1000
2023-09-29 05:47:53.470 
Epoch 156/1000 
	 loss: 30.2242, MinusLogProbMetric: 30.2242, val_loss: 30.1032, val_MinusLogProbMetric: 30.1032

Epoch 156: val_loss did not improve from 29.88072
196/196 - 53s - loss: 30.2242 - MinusLogProbMetric: 30.2242 - val_loss: 30.1032 - val_MinusLogProbMetric: 30.1032 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 157/1000
2023-09-29 05:48:46.007 
Epoch 157/1000 
	 loss: 30.1176, MinusLogProbMetric: 30.1176, val_loss: 30.0370, val_MinusLogProbMetric: 30.0370

Epoch 157: val_loss did not improve from 29.88072
196/196 - 53s - loss: 30.1176 - MinusLogProbMetric: 30.1176 - val_loss: 30.0370 - val_MinusLogProbMetric: 30.0370 - lr: 0.0010 - 53s/epoch - 268ms/step
Epoch 158/1000
2023-09-29 05:49:38.186 
Epoch 158/1000 
	 loss: 30.1995, MinusLogProbMetric: 30.1995, val_loss: 30.1312, val_MinusLogProbMetric: 30.1312

Epoch 158: val_loss did not improve from 29.88072
196/196 - 52s - loss: 30.1995 - MinusLogProbMetric: 30.1995 - val_loss: 30.1312 - val_MinusLogProbMetric: 30.1312 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 159/1000
2023-09-29 05:50:31.881 
Epoch 159/1000 
	 loss: 30.1750, MinusLogProbMetric: 30.1750, val_loss: 31.2460, val_MinusLogProbMetric: 31.2460

Epoch 159: val_loss did not improve from 29.88072
196/196 - 54s - loss: 30.1750 - MinusLogProbMetric: 30.1750 - val_loss: 31.2460 - val_MinusLogProbMetric: 31.2460 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 160/1000
2023-09-29 05:51:24.779 
Epoch 160/1000 
	 loss: 30.1143, MinusLogProbMetric: 30.1143, val_loss: 31.0933, val_MinusLogProbMetric: 31.0933

Epoch 160: val_loss did not improve from 29.88072
196/196 - 53s - loss: 30.1143 - MinusLogProbMetric: 30.1143 - val_loss: 31.0933 - val_MinusLogProbMetric: 31.0933 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 161/1000
2023-09-29 05:52:17.800 
Epoch 161/1000 
	 loss: 30.1739, MinusLogProbMetric: 30.1739, val_loss: 31.1257, val_MinusLogProbMetric: 31.1257

Epoch 161: val_loss did not improve from 29.88072
196/196 - 53s - loss: 30.1739 - MinusLogProbMetric: 30.1739 - val_loss: 31.1257 - val_MinusLogProbMetric: 31.1257 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 162/1000
2023-09-29 05:53:11.056 
Epoch 162/1000 
	 loss: 30.1641, MinusLogProbMetric: 30.1641, val_loss: 30.3536, val_MinusLogProbMetric: 30.3536

Epoch 162: val_loss did not improve from 29.88072
196/196 - 53s - loss: 30.1641 - MinusLogProbMetric: 30.1641 - val_loss: 30.3536 - val_MinusLogProbMetric: 30.3536 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 163/1000
2023-09-29 05:54:04.046 
Epoch 163/1000 
	 loss: 30.2973, MinusLogProbMetric: 30.2973, val_loss: 30.3484, val_MinusLogProbMetric: 30.3484

Epoch 163: val_loss did not improve from 29.88072
196/196 - 53s - loss: 30.2973 - MinusLogProbMetric: 30.2973 - val_loss: 30.3484 - val_MinusLogProbMetric: 30.3484 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 164/1000
2023-09-29 05:54:57.351 
Epoch 164/1000 
	 loss: 30.1268, MinusLogProbMetric: 30.1268, val_loss: 30.4463, val_MinusLogProbMetric: 30.4463

Epoch 164: val_loss did not improve from 29.88072
196/196 - 53s - loss: 30.1268 - MinusLogProbMetric: 30.1268 - val_loss: 30.4463 - val_MinusLogProbMetric: 30.4463 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 165/1000
2023-09-29 05:55:48.993 
Epoch 165/1000 
	 loss: 30.0964, MinusLogProbMetric: 30.0964, val_loss: 30.1623, val_MinusLogProbMetric: 30.1623

Epoch 165: val_loss did not improve from 29.88072
196/196 - 52s - loss: 30.0964 - MinusLogProbMetric: 30.0964 - val_loss: 30.1623 - val_MinusLogProbMetric: 30.1623 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 166/1000
2023-09-29 05:56:40.455 
Epoch 166/1000 
	 loss: 30.1278, MinusLogProbMetric: 30.1278, val_loss: 30.2641, val_MinusLogProbMetric: 30.2641

Epoch 166: val_loss did not improve from 29.88072
196/196 - 51s - loss: 30.1278 - MinusLogProbMetric: 30.1278 - val_loss: 30.2641 - val_MinusLogProbMetric: 30.2641 - lr: 0.0010 - 51s/epoch - 263ms/step
Epoch 167/1000
2023-09-29 05:57:31.744 
Epoch 167/1000 
	 loss: 30.1802, MinusLogProbMetric: 30.1802, val_loss: 30.3785, val_MinusLogProbMetric: 30.3785

Epoch 167: val_loss did not improve from 29.88072
196/196 - 51s - loss: 30.1802 - MinusLogProbMetric: 30.1802 - val_loss: 30.3785 - val_MinusLogProbMetric: 30.3785 - lr: 0.0010 - 51s/epoch - 262ms/step
Epoch 168/1000
2023-09-29 05:58:25.287 
Epoch 168/1000 
	 loss: 30.0857, MinusLogProbMetric: 30.0857, val_loss: 30.7515, val_MinusLogProbMetric: 30.7515

Epoch 168: val_loss did not improve from 29.88072
196/196 - 54s - loss: 30.0857 - MinusLogProbMetric: 30.0857 - val_loss: 30.7515 - val_MinusLogProbMetric: 30.7515 - lr: 0.0010 - 54s/epoch - 273ms/step
Epoch 169/1000
2023-09-29 05:59:19.521 
Epoch 169/1000 
	 loss: 30.0918, MinusLogProbMetric: 30.0918, val_loss: 31.0952, val_MinusLogProbMetric: 31.0952

Epoch 169: val_loss did not improve from 29.88072
196/196 - 54s - loss: 30.0918 - MinusLogProbMetric: 30.0918 - val_loss: 31.0952 - val_MinusLogProbMetric: 31.0952 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 170/1000
2023-09-29 06:00:12.899 
Epoch 170/1000 
	 loss: 30.0949, MinusLogProbMetric: 30.0949, val_loss: 29.8170, val_MinusLogProbMetric: 29.8170

Epoch 170: val_loss improved from 29.88072 to 29.81704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 30.0949 - MinusLogProbMetric: 30.0949 - val_loss: 29.8170 - val_MinusLogProbMetric: 29.8170 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 171/1000
2023-09-29 06:01:07.900 
Epoch 171/1000 
	 loss: 30.0782, MinusLogProbMetric: 30.0782, val_loss: 30.2307, val_MinusLogProbMetric: 30.2307

Epoch 171: val_loss did not improve from 29.81704
196/196 - 54s - loss: 30.0782 - MinusLogProbMetric: 30.0782 - val_loss: 30.2307 - val_MinusLogProbMetric: 30.2307 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 172/1000
2023-09-29 06:02:01.113 
Epoch 172/1000 
	 loss: 29.9667, MinusLogProbMetric: 29.9667, val_loss: 29.9789, val_MinusLogProbMetric: 29.9789

Epoch 172: val_loss did not improve from 29.81704
196/196 - 53s - loss: 29.9667 - MinusLogProbMetric: 29.9667 - val_loss: 29.9789 - val_MinusLogProbMetric: 29.9789 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 173/1000
2023-09-29 06:02:53.509 
Epoch 173/1000 
	 loss: 30.0022, MinusLogProbMetric: 30.0022, val_loss: 29.9549, val_MinusLogProbMetric: 29.9549

Epoch 173: val_loss did not improve from 29.81704
196/196 - 52s - loss: 30.0022 - MinusLogProbMetric: 30.0022 - val_loss: 29.9549 - val_MinusLogProbMetric: 29.9549 - lr: 0.0010 - 52s/epoch - 267ms/step
Epoch 174/1000
2023-09-29 06:03:46.633 
Epoch 174/1000 
	 loss: 29.9564, MinusLogProbMetric: 29.9564, val_loss: 30.3985, val_MinusLogProbMetric: 30.3985

Epoch 174: val_loss did not improve from 29.81704
196/196 - 53s - loss: 29.9564 - MinusLogProbMetric: 29.9564 - val_loss: 30.3985 - val_MinusLogProbMetric: 30.3985 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 175/1000
2023-09-29 06:04:39.319 
Epoch 175/1000 
	 loss: 30.0771, MinusLogProbMetric: 30.0771, val_loss: 30.2553, val_MinusLogProbMetric: 30.2553

Epoch 175: val_loss did not improve from 29.81704
196/196 - 53s - loss: 30.0771 - MinusLogProbMetric: 30.0771 - val_loss: 30.2553 - val_MinusLogProbMetric: 30.2553 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 176/1000
2023-09-29 06:05:30.739 
Epoch 176/1000 
	 loss: 30.0160, MinusLogProbMetric: 30.0160, val_loss: 30.1775, val_MinusLogProbMetric: 30.1775

Epoch 176: val_loss did not improve from 29.81704
196/196 - 51s - loss: 30.0160 - MinusLogProbMetric: 30.0160 - val_loss: 30.1775 - val_MinusLogProbMetric: 30.1775 - lr: 0.0010 - 51s/epoch - 262ms/step
Epoch 177/1000
2023-09-29 06:06:24.047 
Epoch 177/1000 
	 loss: 30.0050, MinusLogProbMetric: 30.0050, val_loss: 30.0489, val_MinusLogProbMetric: 30.0489

Epoch 177: val_loss did not improve from 29.81704
196/196 - 53s - loss: 30.0050 - MinusLogProbMetric: 30.0050 - val_loss: 30.0489 - val_MinusLogProbMetric: 30.0489 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 178/1000
2023-09-29 06:07:17.076 
Epoch 178/1000 
	 loss: 30.0230, MinusLogProbMetric: 30.0230, val_loss: 29.7821, val_MinusLogProbMetric: 29.7821

Epoch 178: val_loss improved from 29.81704 to 29.78208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 30.0230 - MinusLogProbMetric: 30.0230 - val_loss: 29.7821 - val_MinusLogProbMetric: 29.7821 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 179/1000
2023-09-29 06:08:09.450 
Epoch 179/1000 
	 loss: 29.9055, MinusLogProbMetric: 29.9055, val_loss: 29.8127, val_MinusLogProbMetric: 29.8127

Epoch 179: val_loss did not improve from 29.78208
196/196 - 52s - loss: 29.9055 - MinusLogProbMetric: 29.9055 - val_loss: 29.8127 - val_MinusLogProbMetric: 29.8127 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 180/1000
2023-09-29 06:09:00.336 
Epoch 180/1000 
	 loss: 30.0182, MinusLogProbMetric: 30.0182, val_loss: 31.2252, val_MinusLogProbMetric: 31.2252

Epoch 180: val_loss did not improve from 29.78208
196/196 - 51s - loss: 30.0182 - MinusLogProbMetric: 30.0182 - val_loss: 31.2252 - val_MinusLogProbMetric: 31.2252 - lr: 0.0010 - 51s/epoch - 260ms/step
Epoch 181/1000
2023-09-29 06:09:54.940 
Epoch 181/1000 
	 loss: 29.9577, MinusLogProbMetric: 29.9577, val_loss: 29.9606, val_MinusLogProbMetric: 29.9606

Epoch 181: val_loss did not improve from 29.78208
196/196 - 55s - loss: 29.9577 - MinusLogProbMetric: 29.9577 - val_loss: 29.9606 - val_MinusLogProbMetric: 29.9606 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 182/1000
2023-09-29 06:10:48.210 
Epoch 182/1000 
	 loss: 29.9072, MinusLogProbMetric: 29.9072, val_loss: 30.4483, val_MinusLogProbMetric: 30.4483

Epoch 182: val_loss did not improve from 29.78208
196/196 - 53s - loss: 29.9072 - MinusLogProbMetric: 29.9072 - val_loss: 30.4483 - val_MinusLogProbMetric: 30.4483 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 183/1000
2023-09-29 06:11:39.801 
Epoch 183/1000 
	 loss: 30.0983, MinusLogProbMetric: 30.0983, val_loss: 30.0862, val_MinusLogProbMetric: 30.0862

Epoch 183: val_loss did not improve from 29.78208
196/196 - 52s - loss: 30.0983 - MinusLogProbMetric: 30.0983 - val_loss: 30.0862 - val_MinusLogProbMetric: 30.0862 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 184/1000
2023-09-29 06:12:30.571 
Epoch 184/1000 
	 loss: 29.7900, MinusLogProbMetric: 29.7900, val_loss: 29.6105, val_MinusLogProbMetric: 29.6105

Epoch 184: val_loss improved from 29.78208 to 29.61052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 51s - loss: 29.7900 - MinusLogProbMetric: 29.7900 - val_loss: 29.6105 - val_MinusLogProbMetric: 29.6105 - lr: 0.0010 - 51s/epoch - 262ms/step
Epoch 185/1000
2023-09-29 06:13:24.001 
Epoch 185/1000 
	 loss: 29.9757, MinusLogProbMetric: 29.9757, val_loss: 30.0511, val_MinusLogProbMetric: 30.0511

Epoch 185: val_loss did not improve from 29.61052
196/196 - 53s - loss: 29.9757 - MinusLogProbMetric: 29.9757 - val_loss: 30.0511 - val_MinusLogProbMetric: 30.0511 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 186/1000
2023-09-29 06:14:19.171 
Epoch 186/1000 
	 loss: 29.9352, MinusLogProbMetric: 29.9352, val_loss: 30.5544, val_MinusLogProbMetric: 30.5544

Epoch 186: val_loss did not improve from 29.61052
196/196 - 55s - loss: 29.9352 - MinusLogProbMetric: 29.9352 - val_loss: 30.5544 - val_MinusLogProbMetric: 30.5544 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 187/1000
2023-09-29 06:15:14.590 
Epoch 187/1000 
	 loss: 29.7714, MinusLogProbMetric: 29.7714, val_loss: 29.9411, val_MinusLogProbMetric: 29.9411

Epoch 187: val_loss did not improve from 29.61052
196/196 - 55s - loss: 29.7714 - MinusLogProbMetric: 29.7714 - val_loss: 29.9411 - val_MinusLogProbMetric: 29.9411 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 188/1000
2023-09-29 06:16:10.599 
Epoch 188/1000 
	 loss: 30.0119, MinusLogProbMetric: 30.0119, val_loss: 29.8207, val_MinusLogProbMetric: 29.8207

Epoch 188: val_loss did not improve from 29.61052
196/196 - 56s - loss: 30.0119 - MinusLogProbMetric: 30.0119 - val_loss: 29.8207 - val_MinusLogProbMetric: 29.8207 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 189/1000
2023-09-29 06:17:03.952 
Epoch 189/1000 
	 loss: 29.7944, MinusLogProbMetric: 29.7944, val_loss: 30.5046, val_MinusLogProbMetric: 30.5046

Epoch 189: val_loss did not improve from 29.61052
196/196 - 53s - loss: 29.7944 - MinusLogProbMetric: 29.7944 - val_loss: 30.5046 - val_MinusLogProbMetric: 30.5046 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 190/1000
2023-09-29 06:17:58.155 
Epoch 190/1000 
	 loss: 29.8652, MinusLogProbMetric: 29.8652, val_loss: 30.8074, val_MinusLogProbMetric: 30.8074

Epoch 190: val_loss did not improve from 29.61052
196/196 - 54s - loss: 29.8652 - MinusLogProbMetric: 29.8652 - val_loss: 30.8074 - val_MinusLogProbMetric: 30.8074 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 191/1000
2023-09-29 06:18:53.930 
Epoch 191/1000 
	 loss: 29.8919, MinusLogProbMetric: 29.8919, val_loss: 30.5263, val_MinusLogProbMetric: 30.5263

Epoch 191: val_loss did not improve from 29.61052
196/196 - 56s - loss: 29.8919 - MinusLogProbMetric: 29.8919 - val_loss: 30.5263 - val_MinusLogProbMetric: 30.5263 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 192/1000
2023-09-29 06:19:48.508 
Epoch 192/1000 
	 loss: 29.9154, MinusLogProbMetric: 29.9154, val_loss: 30.1006, val_MinusLogProbMetric: 30.1006

Epoch 192: val_loss did not improve from 29.61052
196/196 - 55s - loss: 29.9154 - MinusLogProbMetric: 29.9154 - val_loss: 30.1006 - val_MinusLogProbMetric: 30.1006 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 193/1000
2023-09-29 06:20:42.702 
Epoch 193/1000 
	 loss: 29.7547, MinusLogProbMetric: 29.7547, val_loss: 29.7931, val_MinusLogProbMetric: 29.7931

Epoch 193: val_loss did not improve from 29.61052
196/196 - 54s - loss: 29.7547 - MinusLogProbMetric: 29.7547 - val_loss: 29.7931 - val_MinusLogProbMetric: 29.7931 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 194/1000
2023-09-29 06:21:37.199 
Epoch 194/1000 
	 loss: 29.8493, MinusLogProbMetric: 29.8493, val_loss: 29.7629, val_MinusLogProbMetric: 29.7629

Epoch 194: val_loss did not improve from 29.61052
196/196 - 54s - loss: 29.8493 - MinusLogProbMetric: 29.8493 - val_loss: 29.7629 - val_MinusLogProbMetric: 29.7629 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 195/1000
2023-09-29 06:22:31.592 
Epoch 195/1000 
	 loss: 29.7771, MinusLogProbMetric: 29.7771, val_loss: 29.7294, val_MinusLogProbMetric: 29.7294

Epoch 195: val_loss did not improve from 29.61052
196/196 - 54s - loss: 29.7771 - MinusLogProbMetric: 29.7771 - val_loss: 29.7294 - val_MinusLogProbMetric: 29.7294 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 196/1000
2023-09-29 06:23:26.172 
Epoch 196/1000 
	 loss: 29.8394, MinusLogProbMetric: 29.8394, val_loss: 29.7919, val_MinusLogProbMetric: 29.7919

Epoch 196: val_loss did not improve from 29.61052
196/196 - 55s - loss: 29.8394 - MinusLogProbMetric: 29.8394 - val_loss: 29.7919 - val_MinusLogProbMetric: 29.7919 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 197/1000
2023-09-29 06:24:21.102 
Epoch 197/1000 
	 loss: 29.8277, MinusLogProbMetric: 29.8277, val_loss: 29.8172, val_MinusLogProbMetric: 29.8172

Epoch 197: val_loss did not improve from 29.61052
196/196 - 55s - loss: 29.8277 - MinusLogProbMetric: 29.8277 - val_loss: 29.8172 - val_MinusLogProbMetric: 29.8172 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 198/1000
2023-09-29 06:25:16.904 
Epoch 198/1000 
	 loss: 29.8282, MinusLogProbMetric: 29.8282, val_loss: 31.4339, val_MinusLogProbMetric: 31.4339

Epoch 198: val_loss did not improve from 29.61052
196/196 - 56s - loss: 29.8282 - MinusLogProbMetric: 29.8282 - val_loss: 31.4339 - val_MinusLogProbMetric: 31.4339 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 199/1000
2023-09-29 06:26:12.513 
Epoch 199/1000 
	 loss: 29.8028, MinusLogProbMetric: 29.8028, val_loss: 29.6842, val_MinusLogProbMetric: 29.6842

Epoch 199: val_loss did not improve from 29.61052
196/196 - 56s - loss: 29.8028 - MinusLogProbMetric: 29.8028 - val_loss: 29.6842 - val_MinusLogProbMetric: 29.6842 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 200/1000
2023-09-29 06:27:08.206 
Epoch 200/1000 
	 loss: 29.9903, MinusLogProbMetric: 29.9903, val_loss: 29.5281, val_MinusLogProbMetric: 29.5281

Epoch 200: val_loss improved from 29.61052 to 29.52807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 56s - loss: 29.9903 - MinusLogProbMetric: 29.9903 - val_loss: 29.5281 - val_MinusLogProbMetric: 29.5281 - lr: 0.0010 - 56s/epoch - 288ms/step
Epoch 201/1000
2023-09-29 06:28:03.386 
Epoch 201/1000 
	 loss: 29.8279, MinusLogProbMetric: 29.8279, val_loss: 30.3496, val_MinusLogProbMetric: 30.3496

Epoch 201: val_loss did not improve from 29.52807
196/196 - 54s - loss: 29.8279 - MinusLogProbMetric: 29.8279 - val_loss: 30.3496 - val_MinusLogProbMetric: 30.3496 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 202/1000
2023-09-29 06:28:58.946 
Epoch 202/1000 
	 loss: 29.6790, MinusLogProbMetric: 29.6790, val_loss: 30.8526, val_MinusLogProbMetric: 30.8526

Epoch 202: val_loss did not improve from 29.52807
196/196 - 56s - loss: 29.6790 - MinusLogProbMetric: 29.6790 - val_loss: 30.8526 - val_MinusLogProbMetric: 30.8526 - lr: 0.0010 - 56s/epoch - 283ms/step
Epoch 203/1000
2023-09-29 06:29:54.194 
Epoch 203/1000 
	 loss: 29.6532, MinusLogProbMetric: 29.6532, val_loss: 31.4475, val_MinusLogProbMetric: 31.4475

Epoch 203: val_loss did not improve from 29.52807
196/196 - 55s - loss: 29.6532 - MinusLogProbMetric: 29.6532 - val_loss: 31.4475 - val_MinusLogProbMetric: 31.4475 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 204/1000
2023-09-29 06:30:50.019 
Epoch 204/1000 
	 loss: 29.9227, MinusLogProbMetric: 29.9227, val_loss: 31.2014, val_MinusLogProbMetric: 31.2014

Epoch 204: val_loss did not improve from 29.52807
196/196 - 56s - loss: 29.9227 - MinusLogProbMetric: 29.9227 - val_loss: 31.2014 - val_MinusLogProbMetric: 31.2014 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 205/1000
2023-09-29 06:31:45.728 
Epoch 205/1000 
	 loss: 29.8922, MinusLogProbMetric: 29.8922, val_loss: 29.5336, val_MinusLogProbMetric: 29.5336

Epoch 205: val_loss did not improve from 29.52807
196/196 - 56s - loss: 29.8922 - MinusLogProbMetric: 29.8922 - val_loss: 29.5336 - val_MinusLogProbMetric: 29.5336 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 206/1000
2023-09-29 06:32:41.189 
Epoch 206/1000 
	 loss: 29.7641, MinusLogProbMetric: 29.7641, val_loss: 30.2582, val_MinusLogProbMetric: 30.2582

Epoch 206: val_loss did not improve from 29.52807
196/196 - 55s - loss: 29.7641 - MinusLogProbMetric: 29.7641 - val_loss: 30.2582 - val_MinusLogProbMetric: 30.2582 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 207/1000
2023-09-29 06:33:36.774 
Epoch 207/1000 
	 loss: 29.7673, MinusLogProbMetric: 29.7673, val_loss: 29.9936, val_MinusLogProbMetric: 29.9936

Epoch 207: val_loss did not improve from 29.52807
196/196 - 56s - loss: 29.7673 - MinusLogProbMetric: 29.7673 - val_loss: 29.9936 - val_MinusLogProbMetric: 29.9936 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 208/1000
2023-09-29 06:34:32.199 
Epoch 208/1000 
	 loss: 29.6919, MinusLogProbMetric: 29.6919, val_loss: 30.1530, val_MinusLogProbMetric: 30.1530

Epoch 208: val_loss did not improve from 29.52807
196/196 - 55s - loss: 29.6919 - MinusLogProbMetric: 29.6919 - val_loss: 30.1530 - val_MinusLogProbMetric: 30.1530 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 209/1000
2023-09-29 06:35:27.942 
Epoch 209/1000 
	 loss: 29.8276, MinusLogProbMetric: 29.8276, val_loss: 29.7680, val_MinusLogProbMetric: 29.7680

Epoch 209: val_loss did not improve from 29.52807
196/196 - 56s - loss: 29.8276 - MinusLogProbMetric: 29.8276 - val_loss: 29.7680 - val_MinusLogProbMetric: 29.7680 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 210/1000
2023-09-29 06:36:23.404 
Epoch 210/1000 
	 loss: 29.6028, MinusLogProbMetric: 29.6028, val_loss: 29.7195, val_MinusLogProbMetric: 29.7195

Epoch 210: val_loss did not improve from 29.52807
196/196 - 55s - loss: 29.6028 - MinusLogProbMetric: 29.6028 - val_loss: 29.7195 - val_MinusLogProbMetric: 29.7195 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 211/1000
2023-09-29 06:37:18.846 
Epoch 211/1000 
	 loss: 29.7067, MinusLogProbMetric: 29.7067, val_loss: 29.7002, val_MinusLogProbMetric: 29.7002

Epoch 211: val_loss did not improve from 29.52807
196/196 - 55s - loss: 29.7067 - MinusLogProbMetric: 29.7067 - val_loss: 29.7002 - val_MinusLogProbMetric: 29.7002 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 212/1000
2023-09-29 06:38:14.321 
Epoch 212/1000 
	 loss: 29.8689, MinusLogProbMetric: 29.8689, val_loss: 29.8547, val_MinusLogProbMetric: 29.8547

Epoch 212: val_loss did not improve from 29.52807
196/196 - 55s - loss: 29.8689 - MinusLogProbMetric: 29.8689 - val_loss: 29.8547 - val_MinusLogProbMetric: 29.8547 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 213/1000
2023-09-29 06:39:09.888 
Epoch 213/1000 
	 loss: 29.6848, MinusLogProbMetric: 29.6848, val_loss: 30.6181, val_MinusLogProbMetric: 30.6181

Epoch 213: val_loss did not improve from 29.52807
196/196 - 56s - loss: 29.6848 - MinusLogProbMetric: 29.6848 - val_loss: 30.6181 - val_MinusLogProbMetric: 30.6181 - lr: 0.0010 - 56s/epoch - 283ms/step
Epoch 214/1000
2023-09-29 06:40:05.171 
Epoch 214/1000 
	 loss: 29.7485, MinusLogProbMetric: 29.7485, val_loss: 30.6544, val_MinusLogProbMetric: 30.6544

Epoch 214: val_loss did not improve from 29.52807
196/196 - 55s - loss: 29.7485 - MinusLogProbMetric: 29.7485 - val_loss: 30.6544 - val_MinusLogProbMetric: 30.6544 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 215/1000
2023-09-29 06:41:00.678 
Epoch 215/1000 
	 loss: 29.6088, MinusLogProbMetric: 29.6088, val_loss: 29.7623, val_MinusLogProbMetric: 29.7623

Epoch 215: val_loss did not improve from 29.52807
196/196 - 55s - loss: 29.6088 - MinusLogProbMetric: 29.6088 - val_loss: 29.7623 - val_MinusLogProbMetric: 29.7623 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 216/1000
2023-09-29 06:41:55.892 
Epoch 216/1000 
	 loss: 29.9702, MinusLogProbMetric: 29.9702, val_loss: 29.4125, val_MinusLogProbMetric: 29.4125

Epoch 216: val_loss improved from 29.52807 to 29.41249, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 56s - loss: 29.9702 - MinusLogProbMetric: 29.9702 - val_loss: 29.4125 - val_MinusLogProbMetric: 29.4125 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 217/1000
2023-09-29 06:42:52.073 
Epoch 217/1000 
	 loss: 29.6625, MinusLogProbMetric: 29.6625, val_loss: 29.9210, val_MinusLogProbMetric: 29.9210

Epoch 217: val_loss did not improve from 29.41249
196/196 - 55s - loss: 29.6625 - MinusLogProbMetric: 29.6625 - val_loss: 29.9210 - val_MinusLogProbMetric: 29.9210 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 218/1000
2023-09-29 06:43:47.243 
Epoch 218/1000 
	 loss: 29.6318, MinusLogProbMetric: 29.6318, val_loss: 29.7294, val_MinusLogProbMetric: 29.7294

Epoch 218: val_loss did not improve from 29.41249
196/196 - 55s - loss: 29.6318 - MinusLogProbMetric: 29.6318 - val_loss: 29.7294 - val_MinusLogProbMetric: 29.7294 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 219/1000
2023-09-29 06:44:42.699 
Epoch 219/1000 
	 loss: 29.6300, MinusLogProbMetric: 29.6300, val_loss: 29.6093, val_MinusLogProbMetric: 29.6093

Epoch 219: val_loss did not improve from 29.41249
196/196 - 55s - loss: 29.6300 - MinusLogProbMetric: 29.6300 - val_loss: 29.6093 - val_MinusLogProbMetric: 29.6093 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 220/1000
2023-09-29 06:45:37.145 
Epoch 220/1000 
	 loss: 29.6263, MinusLogProbMetric: 29.6263, val_loss: 30.1257, val_MinusLogProbMetric: 30.1257

Epoch 220: val_loss did not improve from 29.41249
196/196 - 54s - loss: 29.6263 - MinusLogProbMetric: 29.6263 - val_loss: 30.1257 - val_MinusLogProbMetric: 30.1257 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 221/1000
2023-09-29 06:46:21.342 
Epoch 221/1000 
	 loss: 29.6390, MinusLogProbMetric: 29.6390, val_loss: 30.0772, val_MinusLogProbMetric: 30.0772

Epoch 221: val_loss did not improve from 29.41249
196/196 - 44s - loss: 29.6390 - MinusLogProbMetric: 29.6390 - val_loss: 30.0772 - val_MinusLogProbMetric: 30.0772 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 222/1000
2023-09-29 06:47:05.250 
Epoch 222/1000 
	 loss: 29.6312, MinusLogProbMetric: 29.6312, val_loss: 29.6720, val_MinusLogProbMetric: 29.6720

Epoch 222: val_loss did not improve from 29.41249
196/196 - 44s - loss: 29.6312 - MinusLogProbMetric: 29.6312 - val_loss: 29.6720 - val_MinusLogProbMetric: 29.6720 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 223/1000
2023-09-29 06:47:53.337 
Epoch 223/1000 
	 loss: 29.7550, MinusLogProbMetric: 29.7550, val_loss: 29.8706, val_MinusLogProbMetric: 29.8706

Epoch 223: val_loss did not improve from 29.41249
196/196 - 48s - loss: 29.7550 - MinusLogProbMetric: 29.7550 - val_loss: 29.8706 - val_MinusLogProbMetric: 29.8706 - lr: 0.0010 - 48s/epoch - 245ms/step
Epoch 224/1000
2023-09-29 06:48:48.673 
Epoch 224/1000 
	 loss: 29.7545, MinusLogProbMetric: 29.7545, val_loss: 29.4994, val_MinusLogProbMetric: 29.4994

Epoch 224: val_loss did not improve from 29.41249
196/196 - 55s - loss: 29.7545 - MinusLogProbMetric: 29.7545 - val_loss: 29.4994 - val_MinusLogProbMetric: 29.4994 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 225/1000
2023-09-29 06:49:43.419 
Epoch 225/1000 
	 loss: 29.4944, MinusLogProbMetric: 29.4944, val_loss: 29.8117, val_MinusLogProbMetric: 29.8117

Epoch 225: val_loss did not improve from 29.41249
196/196 - 55s - loss: 29.4944 - MinusLogProbMetric: 29.4944 - val_loss: 29.8117 - val_MinusLogProbMetric: 29.8117 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 226/1000
2023-09-29 06:50:39.093 
Epoch 226/1000 
	 loss: 29.5860, MinusLogProbMetric: 29.5860, val_loss: 30.0687, val_MinusLogProbMetric: 30.0687

Epoch 226: val_loss did not improve from 29.41249
196/196 - 56s - loss: 29.5860 - MinusLogProbMetric: 29.5860 - val_loss: 30.0687 - val_MinusLogProbMetric: 30.0687 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 227/1000
2023-09-29 06:51:34.761 
Epoch 227/1000 
	 loss: 29.5746, MinusLogProbMetric: 29.5746, val_loss: 30.0347, val_MinusLogProbMetric: 30.0347

Epoch 227: val_loss did not improve from 29.41249
196/196 - 56s - loss: 29.5746 - MinusLogProbMetric: 29.5746 - val_loss: 30.0347 - val_MinusLogProbMetric: 30.0347 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 228/1000
2023-09-29 06:52:30.061 
Epoch 228/1000 
	 loss: 29.6503, MinusLogProbMetric: 29.6503, val_loss: 29.7400, val_MinusLogProbMetric: 29.7400

Epoch 228: val_loss did not improve from 29.41249
196/196 - 55s - loss: 29.6503 - MinusLogProbMetric: 29.6503 - val_loss: 29.7400 - val_MinusLogProbMetric: 29.7400 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 229/1000
2023-09-29 06:53:24.625 
Epoch 229/1000 
	 loss: 29.7167, MinusLogProbMetric: 29.7167, val_loss: 29.5242, val_MinusLogProbMetric: 29.5242

Epoch 229: val_loss did not improve from 29.41249
196/196 - 55s - loss: 29.7167 - MinusLogProbMetric: 29.7167 - val_loss: 29.5242 - val_MinusLogProbMetric: 29.5242 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 230/1000
2023-09-29 06:54:20.202 
Epoch 230/1000 
	 loss: 29.6799, MinusLogProbMetric: 29.6799, val_loss: 29.7346, val_MinusLogProbMetric: 29.7346

Epoch 230: val_loss did not improve from 29.41249
196/196 - 56s - loss: 29.6799 - MinusLogProbMetric: 29.6799 - val_loss: 29.7346 - val_MinusLogProbMetric: 29.7346 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 231/1000
2023-09-29 06:55:15.418 
Epoch 231/1000 
	 loss: 29.5416, MinusLogProbMetric: 29.5416, val_loss: 29.3193, val_MinusLogProbMetric: 29.3193

Epoch 231: val_loss improved from 29.41249 to 29.31934, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 56s - loss: 29.5416 - MinusLogProbMetric: 29.5416 - val_loss: 29.3193 - val_MinusLogProbMetric: 29.3193 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 232/1000
2023-09-29 06:56:11.527 
Epoch 232/1000 
	 loss: 29.5628, MinusLogProbMetric: 29.5628, val_loss: 29.9893, val_MinusLogProbMetric: 29.9893

Epoch 232: val_loss did not improve from 29.31934
196/196 - 55s - loss: 29.5628 - MinusLogProbMetric: 29.5628 - val_loss: 29.9893 - val_MinusLogProbMetric: 29.9893 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 233/1000
2023-09-29 06:57:06.862 
Epoch 233/1000 
	 loss: 29.6159, MinusLogProbMetric: 29.6159, val_loss: 30.2039, val_MinusLogProbMetric: 30.2039

Epoch 233: val_loss did not improve from 29.31934
196/196 - 55s - loss: 29.6159 - MinusLogProbMetric: 29.6159 - val_loss: 30.2039 - val_MinusLogProbMetric: 30.2039 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 234/1000
2023-09-29 06:58:02.427 
Epoch 234/1000 
	 loss: 29.4820, MinusLogProbMetric: 29.4820, val_loss: 29.8326, val_MinusLogProbMetric: 29.8326

Epoch 234: val_loss did not improve from 29.31934
196/196 - 56s - loss: 29.4820 - MinusLogProbMetric: 29.4820 - val_loss: 29.8326 - val_MinusLogProbMetric: 29.8326 - lr: 0.0010 - 56s/epoch - 283ms/step
Epoch 235/1000
2023-09-29 06:58:58.006 
Epoch 235/1000 
	 loss: 29.5197, MinusLogProbMetric: 29.5197, val_loss: 29.4867, val_MinusLogProbMetric: 29.4867

Epoch 235: val_loss did not improve from 29.31934
196/196 - 56s - loss: 29.5197 - MinusLogProbMetric: 29.5197 - val_loss: 29.4867 - val_MinusLogProbMetric: 29.4867 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 236/1000
2023-09-29 06:59:53.453 
Epoch 236/1000 
	 loss: 29.5363, MinusLogProbMetric: 29.5363, val_loss: 29.5444, val_MinusLogProbMetric: 29.5444

Epoch 236: val_loss did not improve from 29.31934
196/196 - 55s - loss: 29.5363 - MinusLogProbMetric: 29.5363 - val_loss: 29.5444 - val_MinusLogProbMetric: 29.5444 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 237/1000
2023-09-29 07:00:48.548 
Epoch 237/1000 
	 loss: 29.5060, MinusLogProbMetric: 29.5060, val_loss: 30.3615, val_MinusLogProbMetric: 30.3615

Epoch 237: val_loss did not improve from 29.31934
196/196 - 55s - loss: 29.5060 - MinusLogProbMetric: 29.5060 - val_loss: 30.3615 - val_MinusLogProbMetric: 30.3615 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 238/1000
2023-09-29 07:01:44.076 
Epoch 238/1000 
	 loss: 29.5895, MinusLogProbMetric: 29.5895, val_loss: 30.0856, val_MinusLogProbMetric: 30.0856

Epoch 238: val_loss did not improve from 29.31934
196/196 - 56s - loss: 29.5895 - MinusLogProbMetric: 29.5895 - val_loss: 30.0856 - val_MinusLogProbMetric: 30.0856 - lr: 0.0010 - 56s/epoch - 283ms/step
Epoch 239/1000
2023-09-29 07:02:34.682 
Epoch 239/1000 
	 loss: 29.5546, MinusLogProbMetric: 29.5546, val_loss: 29.5278, val_MinusLogProbMetric: 29.5278

Epoch 239: val_loss did not improve from 29.31934
196/196 - 51s - loss: 29.5546 - MinusLogProbMetric: 29.5546 - val_loss: 29.5278 - val_MinusLogProbMetric: 29.5278 - lr: 0.0010 - 51s/epoch - 258ms/step
Epoch 240/1000
2023-09-29 07:03:29.204 
Epoch 240/1000 
	 loss: 29.5680, MinusLogProbMetric: 29.5680, val_loss: 31.6145, val_MinusLogProbMetric: 31.6145

Epoch 240: val_loss did not improve from 29.31934
196/196 - 55s - loss: 29.5680 - MinusLogProbMetric: 29.5680 - val_loss: 31.6145 - val_MinusLogProbMetric: 31.6145 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 241/1000
2023-09-29 07:04:24.147 
Epoch 241/1000 
	 loss: 29.5685, MinusLogProbMetric: 29.5685, val_loss: 29.7524, val_MinusLogProbMetric: 29.7524

Epoch 241: val_loss did not improve from 29.31934
196/196 - 55s - loss: 29.5685 - MinusLogProbMetric: 29.5685 - val_loss: 29.7524 - val_MinusLogProbMetric: 29.7524 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 242/1000
2023-09-29 07:05:19.227 
Epoch 242/1000 
	 loss: 29.4263, MinusLogProbMetric: 29.4263, val_loss: 29.4034, val_MinusLogProbMetric: 29.4034

Epoch 242: val_loss did not improve from 29.31934
196/196 - 55s - loss: 29.4263 - MinusLogProbMetric: 29.4263 - val_loss: 29.4034 - val_MinusLogProbMetric: 29.4034 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 243/1000
2023-09-29 07:06:14.390 
Epoch 243/1000 
	 loss: 29.4951, MinusLogProbMetric: 29.4951, val_loss: 29.6087, val_MinusLogProbMetric: 29.6087

Epoch 243: val_loss did not improve from 29.31934
196/196 - 55s - loss: 29.4951 - MinusLogProbMetric: 29.4951 - val_loss: 29.6087 - val_MinusLogProbMetric: 29.6087 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 244/1000
2023-09-29 07:07:09.638 
Epoch 244/1000 
	 loss: 29.4976, MinusLogProbMetric: 29.4976, val_loss: 29.3458, val_MinusLogProbMetric: 29.3458

Epoch 244: val_loss did not improve from 29.31934
196/196 - 55s - loss: 29.4976 - MinusLogProbMetric: 29.4976 - val_loss: 29.3458 - val_MinusLogProbMetric: 29.3458 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 245/1000
2023-09-29 07:08:04.352 
Epoch 245/1000 
	 loss: 29.5418, MinusLogProbMetric: 29.5418, val_loss: 29.7473, val_MinusLogProbMetric: 29.7473

Epoch 245: val_loss did not improve from 29.31934
196/196 - 55s - loss: 29.5418 - MinusLogProbMetric: 29.5418 - val_loss: 29.7473 - val_MinusLogProbMetric: 29.7473 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 246/1000
2023-09-29 07:08:59.728 
Epoch 246/1000 
	 loss: 29.5438, MinusLogProbMetric: 29.5438, val_loss: 29.3039, val_MinusLogProbMetric: 29.3039

Epoch 246: val_loss improved from 29.31934 to 29.30388, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 56s - loss: 29.5438 - MinusLogProbMetric: 29.5438 - val_loss: 29.3039 - val_MinusLogProbMetric: 29.3039 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 247/1000
2023-09-29 07:09:54.927 
Epoch 247/1000 
	 loss: 29.5231, MinusLogProbMetric: 29.5231, val_loss: 29.6158, val_MinusLogProbMetric: 29.6158

Epoch 247: val_loss did not improve from 29.30388
196/196 - 54s - loss: 29.5231 - MinusLogProbMetric: 29.5231 - val_loss: 29.6158 - val_MinusLogProbMetric: 29.6158 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 248/1000
2023-09-29 07:10:48.675 
Epoch 248/1000 
	 loss: 29.4084, MinusLogProbMetric: 29.4084, val_loss: 30.7294, val_MinusLogProbMetric: 30.7294

Epoch 248: val_loss did not improve from 29.30388
196/196 - 54s - loss: 29.4084 - MinusLogProbMetric: 29.4084 - val_loss: 30.7294 - val_MinusLogProbMetric: 30.7294 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 249/1000
2023-09-29 07:11:40.490 
Epoch 249/1000 
	 loss: 29.5020, MinusLogProbMetric: 29.5020, val_loss: 29.5405, val_MinusLogProbMetric: 29.5405

Epoch 249: val_loss did not improve from 29.30388
196/196 - 52s - loss: 29.5020 - MinusLogProbMetric: 29.5020 - val_loss: 29.5405 - val_MinusLogProbMetric: 29.5405 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 250/1000
2023-09-29 07:12:32.778 
Epoch 250/1000 
	 loss: 29.4618, MinusLogProbMetric: 29.4618, val_loss: 29.6806, val_MinusLogProbMetric: 29.6806

Epoch 250: val_loss did not improve from 29.30388
196/196 - 52s - loss: 29.4618 - MinusLogProbMetric: 29.4618 - val_loss: 29.6806 - val_MinusLogProbMetric: 29.6806 - lr: 0.0010 - 52s/epoch - 267ms/step
Epoch 251/1000
2023-09-29 07:13:26.966 
Epoch 251/1000 
	 loss: 29.5964, MinusLogProbMetric: 29.5964, val_loss: 29.5159, val_MinusLogProbMetric: 29.5159

Epoch 251: val_loss did not improve from 29.30388
196/196 - 54s - loss: 29.5964 - MinusLogProbMetric: 29.5964 - val_loss: 29.5159 - val_MinusLogProbMetric: 29.5159 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 252/1000
2023-09-29 07:14:20.783 
Epoch 252/1000 
	 loss: 29.4261, MinusLogProbMetric: 29.4261, val_loss: 29.6858, val_MinusLogProbMetric: 29.6858

Epoch 252: val_loss did not improve from 29.30388
196/196 - 54s - loss: 29.4261 - MinusLogProbMetric: 29.4261 - val_loss: 29.6858 - val_MinusLogProbMetric: 29.6858 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 253/1000
2023-09-29 07:15:15.483 
Epoch 253/1000 
	 loss: 29.3722, MinusLogProbMetric: 29.3722, val_loss: 30.0618, val_MinusLogProbMetric: 30.0618

Epoch 253: val_loss did not improve from 29.30388
196/196 - 55s - loss: 29.3722 - MinusLogProbMetric: 29.3722 - val_loss: 30.0618 - val_MinusLogProbMetric: 30.0618 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 254/1000
2023-09-29 07:16:19.281 
Epoch 254/1000 
	 loss: 29.5631, MinusLogProbMetric: 29.5631, val_loss: 29.3029, val_MinusLogProbMetric: 29.3029

Epoch 254: val_loss improved from 29.30388 to 29.30288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 64s - loss: 29.5631 - MinusLogProbMetric: 29.5631 - val_loss: 29.3029 - val_MinusLogProbMetric: 29.3029 - lr: 0.0010 - 64s/epoch - 329ms/step
Epoch 255/1000
2023-09-29 07:17:18.331 
Epoch 255/1000 
	 loss: 29.3251, MinusLogProbMetric: 29.3251, val_loss: 30.2965, val_MinusLogProbMetric: 30.2965

Epoch 255: val_loss did not improve from 29.30288
196/196 - 58s - loss: 29.3251 - MinusLogProbMetric: 29.3251 - val_loss: 30.2965 - val_MinusLogProbMetric: 30.2965 - lr: 0.0010 - 58s/epoch - 298ms/step
Epoch 256/1000
2023-09-29 07:18:19.941 
Epoch 256/1000 
	 loss: 29.4977, MinusLogProbMetric: 29.4977, val_loss: 29.4779, val_MinusLogProbMetric: 29.4779

Epoch 256: val_loss did not improve from 29.30288
196/196 - 62s - loss: 29.4977 - MinusLogProbMetric: 29.4977 - val_loss: 29.4779 - val_MinusLogProbMetric: 29.4779 - lr: 0.0010 - 62s/epoch - 314ms/step
Epoch 257/1000
2023-09-29 07:19:09.306 
Epoch 257/1000 
	 loss: 29.3927, MinusLogProbMetric: 29.3927, val_loss: 29.4630, val_MinusLogProbMetric: 29.4630

Epoch 257: val_loss did not improve from 29.30288
196/196 - 49s - loss: 29.3927 - MinusLogProbMetric: 29.3927 - val_loss: 29.4630 - val_MinusLogProbMetric: 29.4630 - lr: 0.0010 - 49s/epoch - 252ms/step
Epoch 258/1000
2023-09-29 07:19:58.472 
Epoch 258/1000 
	 loss: 29.4721, MinusLogProbMetric: 29.4721, val_loss: 29.8225, val_MinusLogProbMetric: 29.8225

Epoch 258: val_loss did not improve from 29.30288
196/196 - 49s - loss: 29.4721 - MinusLogProbMetric: 29.4721 - val_loss: 29.8225 - val_MinusLogProbMetric: 29.8225 - lr: 0.0010 - 49s/epoch - 251ms/step
Epoch 259/1000
2023-09-29 07:20:45.239 
Epoch 259/1000 
	 loss: 29.4656, MinusLogProbMetric: 29.4656, val_loss: 29.1739, val_MinusLogProbMetric: 29.1739

Epoch 259: val_loss improved from 29.30288 to 29.17389, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 48s - loss: 29.4656 - MinusLogProbMetric: 29.4656 - val_loss: 29.1739 - val_MinusLogProbMetric: 29.1739 - lr: 0.0010 - 48s/epoch - 242ms/step
Epoch 260/1000
2023-09-29 07:21:32.913 
Epoch 260/1000 
	 loss: 29.4534, MinusLogProbMetric: 29.4534, val_loss: 29.8894, val_MinusLogProbMetric: 29.8894

Epoch 260: val_loss did not improve from 29.17389
196/196 - 47s - loss: 29.4534 - MinusLogProbMetric: 29.4534 - val_loss: 29.8894 - val_MinusLogProbMetric: 29.8894 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 261/1000
2023-09-29 07:22:18.548 
Epoch 261/1000 
	 loss: 29.3163, MinusLogProbMetric: 29.3163, val_loss: 29.9021, val_MinusLogProbMetric: 29.9021

Epoch 261: val_loss did not improve from 29.17389
196/196 - 46s - loss: 29.3163 - MinusLogProbMetric: 29.3163 - val_loss: 29.9021 - val_MinusLogProbMetric: 29.9021 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 262/1000
2023-09-29 07:23:03.940 
Epoch 262/1000 
	 loss: 29.3858, MinusLogProbMetric: 29.3858, val_loss: 29.8543, val_MinusLogProbMetric: 29.8543

Epoch 262: val_loss did not improve from 29.17389
196/196 - 45s - loss: 29.3858 - MinusLogProbMetric: 29.3858 - val_loss: 29.8543 - val_MinusLogProbMetric: 29.8543 - lr: 0.0010 - 45s/epoch - 232ms/step
Epoch 263/1000
2023-09-29 07:23:50.288 
Epoch 263/1000 
	 loss: 29.5422, MinusLogProbMetric: 29.5422, val_loss: 29.6313, val_MinusLogProbMetric: 29.6313

Epoch 263: val_loss did not improve from 29.17389
196/196 - 46s - loss: 29.5422 - MinusLogProbMetric: 29.5422 - val_loss: 29.6313 - val_MinusLogProbMetric: 29.6313 - lr: 0.0010 - 46s/epoch - 236ms/step
Epoch 264/1000
2023-09-29 07:24:35.156 
Epoch 264/1000 
	 loss: 29.3679, MinusLogProbMetric: 29.3679, val_loss: 29.6910, val_MinusLogProbMetric: 29.6910

Epoch 264: val_loss did not improve from 29.17389
196/196 - 45s - loss: 29.3679 - MinusLogProbMetric: 29.3679 - val_loss: 29.6910 - val_MinusLogProbMetric: 29.6910 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 265/1000
2023-09-29 07:25:19.083 
Epoch 265/1000 
	 loss: 29.3760, MinusLogProbMetric: 29.3760, val_loss: 29.5974, val_MinusLogProbMetric: 29.5974

Epoch 265: val_loss did not improve from 29.17389
196/196 - 44s - loss: 29.3760 - MinusLogProbMetric: 29.3760 - val_loss: 29.5974 - val_MinusLogProbMetric: 29.5974 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 266/1000
2023-09-29 07:26:04.054 
Epoch 266/1000 
	 loss: 29.4360, MinusLogProbMetric: 29.4360, val_loss: 30.1193, val_MinusLogProbMetric: 30.1193

Epoch 266: val_loss did not improve from 29.17389
196/196 - 45s - loss: 29.4360 - MinusLogProbMetric: 29.4360 - val_loss: 30.1193 - val_MinusLogProbMetric: 30.1193 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 267/1000
2023-09-29 07:26:51.241 
Epoch 267/1000 
	 loss: 29.3406, MinusLogProbMetric: 29.3406, val_loss: 29.3076, val_MinusLogProbMetric: 29.3076

Epoch 267: val_loss did not improve from 29.17389
196/196 - 47s - loss: 29.3406 - MinusLogProbMetric: 29.3406 - val_loss: 29.3076 - val_MinusLogProbMetric: 29.3076 - lr: 0.0010 - 47s/epoch - 241ms/step
Epoch 268/1000
2023-09-29 07:27:35.032 
Epoch 268/1000 
	 loss: 29.4619, MinusLogProbMetric: 29.4619, val_loss: 31.1526, val_MinusLogProbMetric: 31.1526

Epoch 268: val_loss did not improve from 29.17389
196/196 - 44s - loss: 29.4619 - MinusLogProbMetric: 29.4619 - val_loss: 31.1526 - val_MinusLogProbMetric: 31.1526 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 269/1000
2023-09-29 07:28:19.286 
Epoch 269/1000 
	 loss: 29.4092, MinusLogProbMetric: 29.4092, val_loss: 29.4181, val_MinusLogProbMetric: 29.4181

Epoch 269: val_loss did not improve from 29.17389
196/196 - 44s - loss: 29.4092 - MinusLogProbMetric: 29.4092 - val_loss: 29.4181 - val_MinusLogProbMetric: 29.4181 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 270/1000
2023-09-29 07:29:06.893 
Epoch 270/1000 
	 loss: 29.3366, MinusLogProbMetric: 29.3366, val_loss: 29.6777, val_MinusLogProbMetric: 29.6777

Epoch 270: val_loss did not improve from 29.17389
196/196 - 48s - loss: 29.3366 - MinusLogProbMetric: 29.3366 - val_loss: 29.6777 - val_MinusLogProbMetric: 29.6777 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 271/1000
2023-09-29 07:29:51.718 
Epoch 271/1000 
	 loss: 29.3171, MinusLogProbMetric: 29.3171, val_loss: 30.7027, val_MinusLogProbMetric: 30.7027

Epoch 271: val_loss did not improve from 29.17389
196/196 - 45s - loss: 29.3171 - MinusLogProbMetric: 29.3171 - val_loss: 30.7027 - val_MinusLogProbMetric: 30.7027 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 272/1000
2023-09-29 07:30:36.127 
Epoch 272/1000 
	 loss: 29.4234, MinusLogProbMetric: 29.4234, val_loss: 29.5369, val_MinusLogProbMetric: 29.5369

Epoch 272: val_loss did not improve from 29.17389
196/196 - 44s - loss: 29.4234 - MinusLogProbMetric: 29.4234 - val_loss: 29.5369 - val_MinusLogProbMetric: 29.5369 - lr: 0.0010 - 44s/epoch - 227ms/step
Epoch 273/1000
2023-09-29 07:31:22.242 
Epoch 273/1000 
	 loss: 29.2709, MinusLogProbMetric: 29.2709, val_loss: 29.3600, val_MinusLogProbMetric: 29.3600

Epoch 273: val_loss did not improve from 29.17389
196/196 - 46s - loss: 29.2709 - MinusLogProbMetric: 29.2709 - val_loss: 29.3600 - val_MinusLogProbMetric: 29.3600 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 274/1000
2023-09-29 07:32:10.399 
Epoch 274/1000 
	 loss: 29.3645, MinusLogProbMetric: 29.3645, val_loss: 31.7129, val_MinusLogProbMetric: 31.7129

Epoch 274: val_loss did not improve from 29.17389
196/196 - 48s - loss: 29.3645 - MinusLogProbMetric: 29.3645 - val_loss: 31.7129 - val_MinusLogProbMetric: 31.7129 - lr: 0.0010 - 48s/epoch - 246ms/step
Epoch 275/1000
2023-09-29 07:32:58.762 
Epoch 275/1000 
	 loss: 29.3567, MinusLogProbMetric: 29.3567, val_loss: 29.3891, val_MinusLogProbMetric: 29.3891

Epoch 275: val_loss did not improve from 29.17389
196/196 - 48s - loss: 29.3567 - MinusLogProbMetric: 29.3567 - val_loss: 29.3891 - val_MinusLogProbMetric: 29.3891 - lr: 0.0010 - 48s/epoch - 247ms/step
Epoch 276/1000
2023-09-29 07:33:46.668 
Epoch 276/1000 
	 loss: 29.4447, MinusLogProbMetric: 29.4447, val_loss: 29.8585, val_MinusLogProbMetric: 29.8585

Epoch 276: val_loss did not improve from 29.17389
196/196 - 48s - loss: 29.4447 - MinusLogProbMetric: 29.4447 - val_loss: 29.8585 - val_MinusLogProbMetric: 29.8585 - lr: 0.0010 - 48s/epoch - 244ms/step
Epoch 277/1000
2023-09-29 07:34:35.500 
Epoch 277/1000 
	 loss: 29.3659, MinusLogProbMetric: 29.3659, val_loss: 31.4613, val_MinusLogProbMetric: 31.4613

Epoch 277: val_loss did not improve from 29.17389
196/196 - 49s - loss: 29.3659 - MinusLogProbMetric: 29.3659 - val_loss: 31.4613 - val_MinusLogProbMetric: 31.4613 - lr: 0.0010 - 49s/epoch - 249ms/step
Epoch 278/1000
2023-09-29 07:35:22.788 
Epoch 278/1000 
	 loss: 29.8441, MinusLogProbMetric: 29.8441, val_loss: 29.6006, val_MinusLogProbMetric: 29.6006

Epoch 278: val_loss did not improve from 29.17389
196/196 - 47s - loss: 29.8441 - MinusLogProbMetric: 29.8441 - val_loss: 29.6006 - val_MinusLogProbMetric: 29.6006 - lr: 0.0010 - 47s/epoch - 241ms/step
Epoch 279/1000
2023-09-29 07:36:05.486 
Epoch 279/1000 
	 loss: 29.1570, MinusLogProbMetric: 29.1570, val_loss: 29.6087, val_MinusLogProbMetric: 29.6087

Epoch 279: val_loss did not improve from 29.17389
196/196 - 43s - loss: 29.1570 - MinusLogProbMetric: 29.1570 - val_loss: 29.6087 - val_MinusLogProbMetric: 29.6087 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 280/1000
2023-09-29 07:36:47.879 
Epoch 280/1000 
	 loss: 29.2590, MinusLogProbMetric: 29.2590, val_loss: 29.3250, val_MinusLogProbMetric: 29.3250

Epoch 280: val_loss did not improve from 29.17389
196/196 - 42s - loss: 29.2590 - MinusLogProbMetric: 29.2590 - val_loss: 29.3250 - val_MinusLogProbMetric: 29.3250 - lr: 0.0010 - 42s/epoch - 216ms/step
Epoch 281/1000
2023-09-29 07:37:32.243 
Epoch 281/1000 
	 loss: 29.3356, MinusLogProbMetric: 29.3356, val_loss: 29.5942, val_MinusLogProbMetric: 29.5942

Epoch 281: val_loss did not improve from 29.17389
196/196 - 44s - loss: 29.3356 - MinusLogProbMetric: 29.3356 - val_loss: 29.5942 - val_MinusLogProbMetric: 29.5942 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 282/1000
2023-09-29 07:38:14.980 
Epoch 282/1000 
	 loss: 29.2665, MinusLogProbMetric: 29.2665, val_loss: 29.4321, val_MinusLogProbMetric: 29.4321

Epoch 282: val_loss did not improve from 29.17389
196/196 - 43s - loss: 29.2665 - MinusLogProbMetric: 29.2665 - val_loss: 29.4321 - val_MinusLogProbMetric: 29.4321 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 283/1000
2023-09-29 07:39:00.508 
Epoch 283/1000 
	 loss: 29.2902, MinusLogProbMetric: 29.2902, val_loss: 29.3161, val_MinusLogProbMetric: 29.3161

Epoch 283: val_loss did not improve from 29.17389
196/196 - 46s - loss: 29.2902 - MinusLogProbMetric: 29.2902 - val_loss: 29.3161 - val_MinusLogProbMetric: 29.3161 - lr: 0.0010 - 46s/epoch - 232ms/step
Epoch 284/1000
2023-09-29 07:39:42.125 
Epoch 284/1000 
	 loss: 29.3738, MinusLogProbMetric: 29.3738, val_loss: 29.7106, val_MinusLogProbMetric: 29.7106

Epoch 284: val_loss did not improve from 29.17389
196/196 - 42s - loss: 29.3738 - MinusLogProbMetric: 29.3738 - val_loss: 29.7106 - val_MinusLogProbMetric: 29.7106 - lr: 0.0010 - 42s/epoch - 212ms/step
Epoch 285/1000
2023-09-29 07:40:28.953 
Epoch 285/1000 
	 loss: 29.3463, MinusLogProbMetric: 29.3463, val_loss: 29.4450, val_MinusLogProbMetric: 29.4450

Epoch 285: val_loss did not improve from 29.17389
196/196 - 47s - loss: 29.3463 - MinusLogProbMetric: 29.3463 - val_loss: 29.4450 - val_MinusLogProbMetric: 29.4450 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 286/1000
2023-09-29 07:41:12.716 
Epoch 286/1000 
	 loss: 29.2458, MinusLogProbMetric: 29.2458, val_loss: 29.3585, val_MinusLogProbMetric: 29.3585

Epoch 286: val_loss did not improve from 29.17389
196/196 - 44s - loss: 29.2458 - MinusLogProbMetric: 29.2458 - val_loss: 29.3585 - val_MinusLogProbMetric: 29.3585 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 287/1000
2023-09-29 07:41:55.392 
Epoch 287/1000 
	 loss: 29.2651, MinusLogProbMetric: 29.2651, val_loss: 29.0106, val_MinusLogProbMetric: 29.0106

Epoch 287: val_loss improved from 29.17389 to 29.01059, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 43s - loss: 29.2651 - MinusLogProbMetric: 29.2651 - val_loss: 29.0106 - val_MinusLogProbMetric: 29.0106 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 288/1000
2023-09-29 07:42:40.017 
Epoch 288/1000 
	 loss: 29.2915, MinusLogProbMetric: 29.2915, val_loss: 29.2979, val_MinusLogProbMetric: 29.2979

Epoch 288: val_loss did not improve from 29.01059
196/196 - 44s - loss: 29.2915 - MinusLogProbMetric: 29.2915 - val_loss: 29.2979 - val_MinusLogProbMetric: 29.2979 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 289/1000
2023-09-29 07:43:25.464 
Epoch 289/1000 
	 loss: 29.6680, MinusLogProbMetric: 29.6680, val_loss: 29.8238, val_MinusLogProbMetric: 29.8238

Epoch 289: val_loss did not improve from 29.01059
196/196 - 45s - loss: 29.6680 - MinusLogProbMetric: 29.6680 - val_loss: 29.8238 - val_MinusLogProbMetric: 29.8238 - lr: 0.0010 - 45s/epoch - 232ms/step
Epoch 290/1000
2023-09-29 07:44:10.852 
Epoch 290/1000 
	 loss: 29.3078, MinusLogProbMetric: 29.3078, val_loss: 29.6900, val_MinusLogProbMetric: 29.6900

Epoch 290: val_loss did not improve from 29.01059
196/196 - 45s - loss: 29.3078 - MinusLogProbMetric: 29.3078 - val_loss: 29.6900 - val_MinusLogProbMetric: 29.6900 - lr: 0.0010 - 45s/epoch - 232ms/step
Epoch 291/1000
2023-09-29 07:44:56.028 
Epoch 291/1000 
	 loss: 29.2406, MinusLogProbMetric: 29.2406, val_loss: 29.0953, val_MinusLogProbMetric: 29.0953

Epoch 291: val_loss did not improve from 29.01059
196/196 - 45s - loss: 29.2406 - MinusLogProbMetric: 29.2406 - val_loss: 29.0953 - val_MinusLogProbMetric: 29.0953 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 292/1000
2023-09-29 07:45:42.948 
Epoch 292/1000 
	 loss: 29.3347, MinusLogProbMetric: 29.3347, val_loss: 29.4774, val_MinusLogProbMetric: 29.4774

Epoch 292: val_loss did not improve from 29.01059
196/196 - 47s - loss: 29.3347 - MinusLogProbMetric: 29.3347 - val_loss: 29.4774 - val_MinusLogProbMetric: 29.4774 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 293/1000
2023-09-29 07:46:28.487 
Epoch 293/1000 
	 loss: 29.3674, MinusLogProbMetric: 29.3674, val_loss: 29.3760, val_MinusLogProbMetric: 29.3760

Epoch 293: val_loss did not improve from 29.01059
196/196 - 46s - loss: 29.3674 - MinusLogProbMetric: 29.3674 - val_loss: 29.3760 - val_MinusLogProbMetric: 29.3760 - lr: 0.0010 - 46s/epoch - 232ms/step
Epoch 294/1000
2023-09-29 07:47:13.546 
Epoch 294/1000 
	 loss: 29.3345, MinusLogProbMetric: 29.3345, val_loss: 29.5590, val_MinusLogProbMetric: 29.5590

Epoch 294: val_loss did not improve from 29.01059
196/196 - 45s - loss: 29.3345 - MinusLogProbMetric: 29.3345 - val_loss: 29.5590 - val_MinusLogProbMetric: 29.5590 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 295/1000
2023-09-29 07:47:59.767 
Epoch 295/1000 
	 loss: 29.4221, MinusLogProbMetric: 29.4221, val_loss: 29.3068, val_MinusLogProbMetric: 29.3068

Epoch 295: val_loss did not improve from 29.01059
196/196 - 46s - loss: 29.4221 - MinusLogProbMetric: 29.4221 - val_loss: 29.3068 - val_MinusLogProbMetric: 29.3068 - lr: 0.0010 - 46s/epoch - 236ms/step
Epoch 296/1000
2023-09-29 07:48:44.175 
Epoch 296/1000 
	 loss: 29.5140, MinusLogProbMetric: 29.5140, val_loss: 29.2288, val_MinusLogProbMetric: 29.2288

Epoch 296: val_loss did not improve from 29.01059
196/196 - 44s - loss: 29.5140 - MinusLogProbMetric: 29.5140 - val_loss: 29.2288 - val_MinusLogProbMetric: 29.2288 - lr: 0.0010 - 44s/epoch - 227ms/step
Epoch 297/1000
2023-09-29 07:49:28.992 
Epoch 297/1000 
	 loss: 29.2360, MinusLogProbMetric: 29.2360, val_loss: 29.5697, val_MinusLogProbMetric: 29.5697

Epoch 297: val_loss did not improve from 29.01059
196/196 - 45s - loss: 29.2360 - MinusLogProbMetric: 29.2360 - val_loss: 29.5697 - val_MinusLogProbMetric: 29.5697 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 298/1000
2023-09-29 07:50:13.016 
Epoch 298/1000 
	 loss: 29.2830, MinusLogProbMetric: 29.2830, val_loss: 29.2817, val_MinusLogProbMetric: 29.2817

Epoch 298: val_loss did not improve from 29.01059
196/196 - 44s - loss: 29.2830 - MinusLogProbMetric: 29.2830 - val_loss: 29.2817 - val_MinusLogProbMetric: 29.2817 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 299/1000
2023-09-29 07:50:58.687 
Epoch 299/1000 
	 loss: 29.2547, MinusLogProbMetric: 29.2547, val_loss: 31.3543, val_MinusLogProbMetric: 31.3543

Epoch 299: val_loss did not improve from 29.01059
196/196 - 46s - loss: 29.2547 - MinusLogProbMetric: 29.2547 - val_loss: 31.3543 - val_MinusLogProbMetric: 31.3543 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 300/1000
2023-09-29 07:51:41.684 
Epoch 300/1000 
	 loss: 29.3506, MinusLogProbMetric: 29.3506, val_loss: 29.1634, val_MinusLogProbMetric: 29.1634

Epoch 300: val_loss did not improve from 29.01059
196/196 - 43s - loss: 29.3506 - MinusLogProbMetric: 29.3506 - val_loss: 29.1634 - val_MinusLogProbMetric: 29.1634 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 301/1000
2023-09-29 07:52:26.525 
Epoch 301/1000 
	 loss: 29.2389, MinusLogProbMetric: 29.2389, val_loss: 29.1326, val_MinusLogProbMetric: 29.1326

Epoch 301: val_loss did not improve from 29.01059
196/196 - 45s - loss: 29.2389 - MinusLogProbMetric: 29.2389 - val_loss: 29.1326 - val_MinusLogProbMetric: 29.1326 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 302/1000
2023-09-29 07:53:12.361 
Epoch 302/1000 
	 loss: 29.2676, MinusLogProbMetric: 29.2676, val_loss: 29.4983, val_MinusLogProbMetric: 29.4983

Epoch 302: val_loss did not improve from 29.01059
196/196 - 46s - loss: 29.2676 - MinusLogProbMetric: 29.2676 - val_loss: 29.4983 - val_MinusLogProbMetric: 29.4983 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 303/1000
2023-09-29 07:53:57.863 
Epoch 303/1000 
	 loss: 29.2193, MinusLogProbMetric: 29.2193, val_loss: 29.2680, val_MinusLogProbMetric: 29.2680

Epoch 303: val_loss did not improve from 29.01059
196/196 - 45s - loss: 29.2193 - MinusLogProbMetric: 29.2193 - val_loss: 29.2680 - val_MinusLogProbMetric: 29.2680 - lr: 0.0010 - 45s/epoch - 232ms/step
Epoch 304/1000
2023-09-29 07:54:43.212 
Epoch 304/1000 
	 loss: 29.2792, MinusLogProbMetric: 29.2792, val_loss: 29.1593, val_MinusLogProbMetric: 29.1593

Epoch 304: val_loss did not improve from 29.01059
196/196 - 45s - loss: 29.2792 - MinusLogProbMetric: 29.2792 - val_loss: 29.1593 - val_MinusLogProbMetric: 29.1593 - lr: 0.0010 - 45s/epoch - 231ms/step
Epoch 305/1000
2023-09-29 07:55:26.222 
Epoch 305/1000 
	 loss: 29.2132, MinusLogProbMetric: 29.2132, val_loss: 29.4057, val_MinusLogProbMetric: 29.4057

Epoch 305: val_loss did not improve from 29.01059
196/196 - 43s - loss: 29.2132 - MinusLogProbMetric: 29.2132 - val_loss: 29.4057 - val_MinusLogProbMetric: 29.4057 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 306/1000
2023-09-29 07:56:09.960 
Epoch 306/1000 
	 loss: 29.2459, MinusLogProbMetric: 29.2459, val_loss: 29.7335, val_MinusLogProbMetric: 29.7335

Epoch 306: val_loss did not improve from 29.01059
196/196 - 44s - loss: 29.2459 - MinusLogProbMetric: 29.2459 - val_loss: 29.7335 - val_MinusLogProbMetric: 29.7335 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 307/1000
2023-09-29 07:56:53.633 
Epoch 307/1000 
	 loss: 29.3497, MinusLogProbMetric: 29.3497, val_loss: 29.2892, val_MinusLogProbMetric: 29.2892

Epoch 307: val_loss did not improve from 29.01059
196/196 - 44s - loss: 29.3497 - MinusLogProbMetric: 29.3497 - val_loss: 29.2892 - val_MinusLogProbMetric: 29.2892 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 308/1000
2023-09-29 07:57:38.904 
Epoch 308/1000 
	 loss: 29.1306, MinusLogProbMetric: 29.1306, val_loss: 30.0458, val_MinusLogProbMetric: 30.0458

Epoch 308: val_loss did not improve from 29.01059
196/196 - 45s - loss: 29.1306 - MinusLogProbMetric: 29.1306 - val_loss: 30.0458 - val_MinusLogProbMetric: 30.0458 - lr: 0.0010 - 45s/epoch - 231ms/step
Epoch 309/1000
2023-09-29 07:58:23.214 
Epoch 309/1000 
	 loss: 29.1464, MinusLogProbMetric: 29.1464, val_loss: 29.4174, val_MinusLogProbMetric: 29.4174

Epoch 309: val_loss did not improve from 29.01059
196/196 - 44s - loss: 29.1464 - MinusLogProbMetric: 29.1464 - val_loss: 29.4174 - val_MinusLogProbMetric: 29.4174 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 310/1000
2023-09-29 07:59:08.302 
Epoch 310/1000 
	 loss: 29.2180, MinusLogProbMetric: 29.2180, val_loss: 30.0619, val_MinusLogProbMetric: 30.0619

Epoch 310: val_loss did not improve from 29.01059
196/196 - 45s - loss: 29.2180 - MinusLogProbMetric: 29.2180 - val_loss: 30.0619 - val_MinusLogProbMetric: 30.0619 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 311/1000
2023-09-29 07:59:55.230 
Epoch 311/1000 
	 loss: 29.1823, MinusLogProbMetric: 29.1823, val_loss: 30.4156, val_MinusLogProbMetric: 30.4156

Epoch 311: val_loss did not improve from 29.01059
196/196 - 47s - loss: 29.1823 - MinusLogProbMetric: 29.1823 - val_loss: 30.4156 - val_MinusLogProbMetric: 30.4156 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 312/1000
2023-09-29 08:00:39.819 
Epoch 312/1000 
	 loss: 29.5842, MinusLogProbMetric: 29.5842, val_loss: 29.1943, val_MinusLogProbMetric: 29.1943

Epoch 312: val_loss did not improve from 29.01059
196/196 - 45s - loss: 29.5842 - MinusLogProbMetric: 29.5842 - val_loss: 29.1943 - val_MinusLogProbMetric: 29.1943 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 313/1000
2023-09-29 08:01:25.504 
Epoch 313/1000 
	 loss: 29.3868, MinusLogProbMetric: 29.3868, val_loss: 29.1173, val_MinusLogProbMetric: 29.1173

Epoch 313: val_loss did not improve from 29.01059
196/196 - 46s - loss: 29.3868 - MinusLogProbMetric: 29.3868 - val_loss: 29.1173 - val_MinusLogProbMetric: 29.1173 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 314/1000
2023-09-29 08:02:09.538 
Epoch 314/1000 
	 loss: 29.1582, MinusLogProbMetric: 29.1582, val_loss: 29.3637, val_MinusLogProbMetric: 29.3637

Epoch 314: val_loss did not improve from 29.01059
196/196 - 44s - loss: 29.1582 - MinusLogProbMetric: 29.1582 - val_loss: 29.3637 - val_MinusLogProbMetric: 29.3637 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 315/1000
2023-09-29 08:02:52.964 
Epoch 315/1000 
	 loss: 29.2470, MinusLogProbMetric: 29.2470, val_loss: 29.3326, val_MinusLogProbMetric: 29.3326

Epoch 315: val_loss did not improve from 29.01059
196/196 - 43s - loss: 29.2470 - MinusLogProbMetric: 29.2470 - val_loss: 29.3326 - val_MinusLogProbMetric: 29.3326 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 316/1000
2023-09-29 08:03:36.159 
Epoch 316/1000 
	 loss: 29.1621, MinusLogProbMetric: 29.1621, val_loss: 30.1547, val_MinusLogProbMetric: 30.1547

Epoch 316: val_loss did not improve from 29.01059
196/196 - 43s - loss: 29.1621 - MinusLogProbMetric: 29.1621 - val_loss: 30.1547 - val_MinusLogProbMetric: 30.1547 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 317/1000
2023-09-29 08:04:22.070 
Epoch 317/1000 
	 loss: 29.2902, MinusLogProbMetric: 29.2902, val_loss: 29.4912, val_MinusLogProbMetric: 29.4912

Epoch 317: val_loss did not improve from 29.01059
196/196 - 46s - loss: 29.2902 - MinusLogProbMetric: 29.2902 - val_loss: 29.4912 - val_MinusLogProbMetric: 29.4912 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 318/1000
2023-09-29 08:05:08.576 
Epoch 318/1000 
	 loss: 29.3890, MinusLogProbMetric: 29.3890, val_loss: 29.9513, val_MinusLogProbMetric: 29.9513

Epoch 318: val_loss did not improve from 29.01059
196/196 - 47s - loss: 29.3890 - MinusLogProbMetric: 29.3890 - val_loss: 29.9513 - val_MinusLogProbMetric: 29.9513 - lr: 0.0010 - 47s/epoch - 237ms/step
Epoch 319/1000
2023-09-29 08:05:53.483 
Epoch 319/1000 
	 loss: 29.2135, MinusLogProbMetric: 29.2135, val_loss: 28.9545, val_MinusLogProbMetric: 28.9545

Epoch 319: val_loss improved from 29.01059 to 28.95452, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 46s - loss: 29.2135 - MinusLogProbMetric: 29.2135 - val_loss: 28.9545 - val_MinusLogProbMetric: 28.9545 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 320/1000
2023-09-29 08:06:39.991 
Epoch 320/1000 
	 loss: 29.0951, MinusLogProbMetric: 29.0951, val_loss: 29.4355, val_MinusLogProbMetric: 29.4355

Epoch 320: val_loss did not improve from 28.95452
196/196 - 46s - loss: 29.0951 - MinusLogProbMetric: 29.0951 - val_loss: 29.4355 - val_MinusLogProbMetric: 29.4355 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 321/1000
2023-09-29 08:07:25.110 
Epoch 321/1000 
	 loss: 29.1218, MinusLogProbMetric: 29.1218, val_loss: 29.3811, val_MinusLogProbMetric: 29.3811

Epoch 321: val_loss did not improve from 28.95452
196/196 - 45s - loss: 29.1218 - MinusLogProbMetric: 29.1218 - val_loss: 29.3811 - val_MinusLogProbMetric: 29.3811 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 322/1000
2023-09-29 08:08:12.314 
Epoch 322/1000 
	 loss: 29.5728, MinusLogProbMetric: 29.5728, val_loss: 29.4405, val_MinusLogProbMetric: 29.4405

Epoch 322: val_loss did not improve from 28.95452
196/196 - 47s - loss: 29.5728 - MinusLogProbMetric: 29.5728 - val_loss: 29.4405 - val_MinusLogProbMetric: 29.4405 - lr: 0.0010 - 47s/epoch - 241ms/step
Epoch 323/1000
2023-09-29 08:08:56.169 
Epoch 323/1000 
	 loss: 29.2475, MinusLogProbMetric: 29.2475, val_loss: 29.6562, val_MinusLogProbMetric: 29.6562

Epoch 323: val_loss did not improve from 28.95452
196/196 - 44s - loss: 29.2475 - MinusLogProbMetric: 29.2475 - val_loss: 29.6562 - val_MinusLogProbMetric: 29.6562 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 324/1000
2023-09-29 08:09:40.043 
Epoch 324/1000 
	 loss: 29.0911, MinusLogProbMetric: 29.0911, val_loss: 29.5581, val_MinusLogProbMetric: 29.5581

Epoch 324: val_loss did not improve from 28.95452
196/196 - 44s - loss: 29.0911 - MinusLogProbMetric: 29.0911 - val_loss: 29.5581 - val_MinusLogProbMetric: 29.5581 - lr: 0.0010 - 44s/epoch - 224ms/step
Epoch 325/1000
2023-09-29 08:10:23.474 
Epoch 325/1000 
	 loss: 29.1921, MinusLogProbMetric: 29.1921, val_loss: 29.1651, val_MinusLogProbMetric: 29.1651

Epoch 325: val_loss did not improve from 28.95452
196/196 - 43s - loss: 29.1921 - MinusLogProbMetric: 29.1921 - val_loss: 29.1651 - val_MinusLogProbMetric: 29.1651 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 326/1000
2023-09-29 08:11:08.276 
Epoch 326/1000 
	 loss: 29.1744, MinusLogProbMetric: 29.1744, val_loss: 29.4805, val_MinusLogProbMetric: 29.4805

Epoch 326: val_loss did not improve from 28.95452
196/196 - 45s - loss: 29.1744 - MinusLogProbMetric: 29.1744 - val_loss: 29.4805 - val_MinusLogProbMetric: 29.4805 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 327/1000
2023-09-29 08:11:54.217 
Epoch 327/1000 
	 loss: 29.2490, MinusLogProbMetric: 29.2490, val_loss: 30.0879, val_MinusLogProbMetric: 30.0879

Epoch 327: val_loss did not improve from 28.95452
196/196 - 46s - loss: 29.2490 - MinusLogProbMetric: 29.2490 - val_loss: 30.0879 - val_MinusLogProbMetric: 30.0879 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 328/1000
2023-09-29 08:12:36.981 
Epoch 328/1000 
	 loss: 29.4146, MinusLogProbMetric: 29.4146, val_loss: 29.7324, val_MinusLogProbMetric: 29.7324

Epoch 328: val_loss did not improve from 28.95452
196/196 - 43s - loss: 29.4146 - MinusLogProbMetric: 29.4146 - val_loss: 29.7324 - val_MinusLogProbMetric: 29.7324 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 329/1000
2023-09-29 08:13:20.355 
Epoch 329/1000 
	 loss: 29.0841, MinusLogProbMetric: 29.0841, val_loss: 29.0770, val_MinusLogProbMetric: 29.0770

Epoch 329: val_loss did not improve from 28.95452
196/196 - 43s - loss: 29.0841 - MinusLogProbMetric: 29.0841 - val_loss: 29.0770 - val_MinusLogProbMetric: 29.0770 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 330/1000
2023-09-29 08:14:04.055 
Epoch 330/1000 
	 loss: 29.1128, MinusLogProbMetric: 29.1128, val_loss: 29.1708, val_MinusLogProbMetric: 29.1708

Epoch 330: val_loss did not improve from 28.95452
196/196 - 44s - loss: 29.1128 - MinusLogProbMetric: 29.1128 - val_loss: 29.1708 - val_MinusLogProbMetric: 29.1708 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 331/1000
2023-09-29 08:14:50.109 
Epoch 331/1000 
	 loss: 29.2404, MinusLogProbMetric: 29.2404, val_loss: 30.0822, val_MinusLogProbMetric: 30.0822

Epoch 331: val_loss did not improve from 28.95452
196/196 - 46s - loss: 29.2404 - MinusLogProbMetric: 29.2404 - val_loss: 30.0822 - val_MinusLogProbMetric: 30.0822 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 332/1000
2023-09-29 08:15:33.429 
Epoch 332/1000 
	 loss: 29.1631, MinusLogProbMetric: 29.1631, val_loss: 29.4382, val_MinusLogProbMetric: 29.4382

Epoch 332: val_loss did not improve from 28.95452
196/196 - 43s - loss: 29.1631 - MinusLogProbMetric: 29.1631 - val_loss: 29.4382 - val_MinusLogProbMetric: 29.4382 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 333/1000
2023-09-29 08:16:17.704 
Epoch 333/1000 
	 loss: 29.1476, MinusLogProbMetric: 29.1476, val_loss: 29.2859, val_MinusLogProbMetric: 29.2859

Epoch 333: val_loss did not improve from 28.95452
196/196 - 44s - loss: 29.1476 - MinusLogProbMetric: 29.1476 - val_loss: 29.2859 - val_MinusLogProbMetric: 29.2859 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 334/1000
2023-09-29 08:17:01.342 
Epoch 334/1000 
	 loss: 28.9926, MinusLogProbMetric: 28.9926, val_loss: 29.7682, val_MinusLogProbMetric: 29.7682

Epoch 334: val_loss did not improve from 28.95452
196/196 - 44s - loss: 28.9926 - MinusLogProbMetric: 28.9926 - val_loss: 29.7682 - val_MinusLogProbMetric: 29.7682 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 335/1000
2023-09-29 08:17:45.005 
Epoch 335/1000 
	 loss: 29.5168, MinusLogProbMetric: 29.5168, val_loss: 29.2597, val_MinusLogProbMetric: 29.2597

Epoch 335: val_loss did not improve from 28.95452
196/196 - 44s - loss: 29.5168 - MinusLogProbMetric: 29.5168 - val_loss: 29.2597 - val_MinusLogProbMetric: 29.2597 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 336/1000
2023-09-29 08:18:28.769 
Epoch 336/1000 
	 loss: 29.1566, MinusLogProbMetric: 29.1566, val_loss: 28.9379, val_MinusLogProbMetric: 28.9379

Epoch 336: val_loss improved from 28.95452 to 28.93793, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 44s - loss: 29.1566 - MinusLogProbMetric: 29.1566 - val_loss: 28.9379 - val_MinusLogProbMetric: 28.9379 - lr: 0.0010 - 44s/epoch - 226ms/step
Epoch 337/1000
2023-09-29 08:19:12.836 
Epoch 337/1000 
	 loss: 29.1141, MinusLogProbMetric: 29.1141, val_loss: 29.2440, val_MinusLogProbMetric: 29.2440

Epoch 337: val_loss did not improve from 28.93793
196/196 - 43s - loss: 29.1141 - MinusLogProbMetric: 29.1141 - val_loss: 29.2440 - val_MinusLogProbMetric: 29.2440 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 338/1000
2023-09-29 08:19:55.345 
Epoch 338/1000 
	 loss: 29.1197, MinusLogProbMetric: 29.1197, val_loss: 30.1635, val_MinusLogProbMetric: 30.1635

Epoch 338: val_loss did not improve from 28.93793
196/196 - 43s - loss: 29.1197 - MinusLogProbMetric: 29.1197 - val_loss: 30.1635 - val_MinusLogProbMetric: 30.1635 - lr: 0.0010 - 43s/epoch - 217ms/step
Epoch 339/1000
2023-09-29 08:20:38.387 
Epoch 339/1000 
	 loss: 29.1718, MinusLogProbMetric: 29.1718, val_loss: 29.5422, val_MinusLogProbMetric: 29.5422

Epoch 339: val_loss did not improve from 28.93793
196/196 - 43s - loss: 29.1718 - MinusLogProbMetric: 29.1718 - val_loss: 29.5422 - val_MinusLogProbMetric: 29.5422 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 340/1000
2023-09-29 08:21:23.193 
Epoch 340/1000 
	 loss: 29.2926, MinusLogProbMetric: 29.2926, val_loss: 29.1549, val_MinusLogProbMetric: 29.1549

Epoch 340: val_loss did not improve from 28.93793
196/196 - 45s - loss: 29.2926 - MinusLogProbMetric: 29.2926 - val_loss: 29.1549 - val_MinusLogProbMetric: 29.1549 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 341/1000
2023-09-29 08:22:09.676 
Epoch 341/1000 
	 loss: 29.1491, MinusLogProbMetric: 29.1491, val_loss: 28.9897, val_MinusLogProbMetric: 28.9897

Epoch 341: val_loss did not improve from 28.93793
196/196 - 46s - loss: 29.1491 - MinusLogProbMetric: 29.1491 - val_loss: 28.9897 - val_MinusLogProbMetric: 28.9897 - lr: 0.0010 - 46s/epoch - 237ms/step
Epoch 342/1000
2023-09-29 08:22:52.873 
Epoch 342/1000 
	 loss: 29.2802, MinusLogProbMetric: 29.2802, val_loss: 29.2048, val_MinusLogProbMetric: 29.2048

Epoch 342: val_loss did not improve from 28.93793
196/196 - 43s - loss: 29.2802 - MinusLogProbMetric: 29.2802 - val_loss: 29.2048 - val_MinusLogProbMetric: 29.2048 - lr: 0.0010 - 43s/epoch - 220ms/step
Epoch 343/1000
2023-09-29 08:23:36.375 
Epoch 343/1000 
	 loss: 29.1200, MinusLogProbMetric: 29.1200, val_loss: 29.1164, val_MinusLogProbMetric: 29.1164

Epoch 343: val_loss did not improve from 28.93793
196/196 - 43s - loss: 29.1200 - MinusLogProbMetric: 29.1200 - val_loss: 29.1164 - val_MinusLogProbMetric: 29.1164 - lr: 0.0010 - 43s/epoch - 222ms/step
Epoch 344/1000
2023-09-29 08:24:20.506 
Epoch 344/1000 
	 loss: 29.2851, MinusLogProbMetric: 29.2851, val_loss: 29.0489, val_MinusLogProbMetric: 29.0489

Epoch 344: val_loss did not improve from 28.93793
196/196 - 44s - loss: 29.2851 - MinusLogProbMetric: 29.2851 - val_loss: 29.0489 - val_MinusLogProbMetric: 29.0489 - lr: 0.0010 - 44s/epoch - 225ms/step
Epoch 345/1000
2023-09-29 08:25:04.178 
Epoch 345/1000 
	 loss: 29.0974, MinusLogProbMetric: 29.0974, val_loss: 29.5137, val_MinusLogProbMetric: 29.5137

Epoch 345: val_loss did not improve from 28.93793
196/196 - 44s - loss: 29.0974 - MinusLogProbMetric: 29.0974 - val_loss: 29.5137 - val_MinusLogProbMetric: 29.5137 - lr: 0.0010 - 44s/epoch - 223ms/step
Epoch 346/1000
2023-09-29 08:25:49.120 
Epoch 346/1000 
	 loss: 29.0652, MinusLogProbMetric: 29.0652, val_loss: 29.0609, val_MinusLogProbMetric: 29.0609

Epoch 346: val_loss did not improve from 28.93793
196/196 - 45s - loss: 29.0652 - MinusLogProbMetric: 29.0652 - val_loss: 29.0609 - val_MinusLogProbMetric: 29.0609 - lr: 0.0010 - 45s/epoch - 229ms/step
Epoch 347/1000
2023-09-29 08:26:36.625 
Epoch 347/1000 
	 loss: 29.2347, MinusLogProbMetric: 29.2347, val_loss: 29.5029, val_MinusLogProbMetric: 29.5029

Epoch 347: val_loss did not improve from 28.93793
196/196 - 47s - loss: 29.2347 - MinusLogProbMetric: 29.2347 - val_loss: 29.5029 - val_MinusLogProbMetric: 29.5029 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 348/1000
2023-09-29 08:27:23.761 
Epoch 348/1000 
	 loss: 29.1945, MinusLogProbMetric: 29.1945, val_loss: 29.0797, val_MinusLogProbMetric: 29.0797

Epoch 348: val_loss did not improve from 28.93793
196/196 - 47s - loss: 29.1945 - MinusLogProbMetric: 29.1945 - val_loss: 29.0797 - val_MinusLogProbMetric: 29.0797 - lr: 0.0010 - 47s/epoch - 240ms/step
Epoch 349/1000
2023-09-29 08:28:15.290 
Epoch 349/1000 
	 loss: 29.3143, MinusLogProbMetric: 29.3143, val_loss: 29.3945, val_MinusLogProbMetric: 29.3945

Epoch 349: val_loss did not improve from 28.93793
196/196 - 52s - loss: 29.3143 - MinusLogProbMetric: 29.3143 - val_loss: 29.3945 - val_MinusLogProbMetric: 29.3945 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 350/1000
2023-09-29 08:29:05.950 
Epoch 350/1000 
	 loss: 29.0023, MinusLogProbMetric: 29.0023, val_loss: 29.7156, val_MinusLogProbMetric: 29.7156

Epoch 350: val_loss did not improve from 28.93793
196/196 - 51s - loss: 29.0023 - MinusLogProbMetric: 29.0023 - val_loss: 29.7156 - val_MinusLogProbMetric: 29.7156 - lr: 0.0010 - 51s/epoch - 258ms/step
Epoch 351/1000
2023-09-29 08:29:52.748 
Epoch 351/1000 
	 loss: 29.0615, MinusLogProbMetric: 29.0615, val_loss: 28.8457, val_MinusLogProbMetric: 28.8457

Epoch 351: val_loss improved from 28.93793 to 28.84567, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 48s - loss: 29.0615 - MinusLogProbMetric: 29.0615 - val_loss: 28.8457 - val_MinusLogProbMetric: 28.8457 - lr: 0.0010 - 48s/epoch - 242ms/step
Epoch 352/1000
2023-09-29 08:30:41.198 
Epoch 352/1000 
	 loss: 29.0741, MinusLogProbMetric: 29.0741, val_loss: 29.3832, val_MinusLogProbMetric: 29.3832

Epoch 352: val_loss did not improve from 28.84567
196/196 - 48s - loss: 29.0741 - MinusLogProbMetric: 29.0741 - val_loss: 29.3832 - val_MinusLogProbMetric: 29.3832 - lr: 0.0010 - 48s/epoch - 244ms/step
Epoch 353/1000
2023-09-29 08:31:32.735 
Epoch 353/1000 
	 loss: 29.0306, MinusLogProbMetric: 29.0306, val_loss: 29.6449, val_MinusLogProbMetric: 29.6449

Epoch 353: val_loss did not improve from 28.84567
196/196 - 52s - loss: 29.0306 - MinusLogProbMetric: 29.0306 - val_loss: 29.6449 - val_MinusLogProbMetric: 29.6449 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 354/1000
2023-09-29 08:32:23.364 
Epoch 354/1000 
	 loss: 29.1523, MinusLogProbMetric: 29.1523, val_loss: 29.0783, val_MinusLogProbMetric: 29.0783

Epoch 354: val_loss did not improve from 28.84567
196/196 - 51s - loss: 29.1523 - MinusLogProbMetric: 29.1523 - val_loss: 29.0783 - val_MinusLogProbMetric: 29.0783 - lr: 0.0010 - 51s/epoch - 258ms/step
Epoch 355/1000
2023-09-29 08:33:09.970 
Epoch 355/1000 
	 loss: 29.0296, MinusLogProbMetric: 29.0296, val_loss: 29.8330, val_MinusLogProbMetric: 29.8330

Epoch 355: val_loss did not improve from 28.84567
196/196 - 47s - loss: 29.0296 - MinusLogProbMetric: 29.0296 - val_loss: 29.8330 - val_MinusLogProbMetric: 29.8330 - lr: 0.0010 - 47s/epoch - 238ms/step
Epoch 356/1000
2023-09-29 08:33:56.332 
Epoch 356/1000 
	 loss: 29.0752, MinusLogProbMetric: 29.0752, val_loss: 29.3017, val_MinusLogProbMetric: 29.3017

Epoch 356: val_loss did not improve from 28.84567
196/196 - 46s - loss: 29.0752 - MinusLogProbMetric: 29.0752 - val_loss: 29.3017 - val_MinusLogProbMetric: 29.3017 - lr: 0.0010 - 46s/epoch - 236ms/step
Epoch 357/1000
2023-09-29 08:34:45.113 
Epoch 357/1000 
	 loss: 29.0302, MinusLogProbMetric: 29.0302, val_loss: 28.9497, val_MinusLogProbMetric: 28.9497

Epoch 357: val_loss did not improve from 28.84567
196/196 - 49s - loss: 29.0302 - MinusLogProbMetric: 29.0302 - val_loss: 28.9497 - val_MinusLogProbMetric: 28.9497 - lr: 0.0010 - 49s/epoch - 249ms/step
Epoch 358/1000
2023-09-29 08:35:39.142 
Epoch 358/1000 
	 loss: 28.9812, MinusLogProbMetric: 28.9812, val_loss: 29.2628, val_MinusLogProbMetric: 29.2628

Epoch 358: val_loss did not improve from 28.84567
196/196 - 54s - loss: 28.9812 - MinusLogProbMetric: 28.9812 - val_loss: 29.2628 - val_MinusLogProbMetric: 29.2628 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 359/1000
2023-09-29 08:36:30.127 
Epoch 359/1000 
	 loss: 29.1021, MinusLogProbMetric: 29.1021, val_loss: 29.3137, val_MinusLogProbMetric: 29.3137

Epoch 359: val_loss did not improve from 28.84567
196/196 - 51s - loss: 29.1021 - MinusLogProbMetric: 29.1021 - val_loss: 29.3137 - val_MinusLogProbMetric: 29.3137 - lr: 0.0010 - 51s/epoch - 260ms/step
Epoch 360/1000
2023-09-29 08:37:18.477 
Epoch 360/1000 
	 loss: 29.1529, MinusLogProbMetric: 29.1529, val_loss: 29.1178, val_MinusLogProbMetric: 29.1178

Epoch 360: val_loss did not improve from 28.84567
196/196 - 48s - loss: 29.1529 - MinusLogProbMetric: 29.1529 - val_loss: 29.1178 - val_MinusLogProbMetric: 29.1178 - lr: 0.0010 - 48s/epoch - 247ms/step
Epoch 361/1000
2023-09-29 08:38:05.558 
Epoch 361/1000 
	 loss: 28.9844, MinusLogProbMetric: 28.9844, val_loss: 28.9170, val_MinusLogProbMetric: 28.9170

Epoch 361: val_loss did not improve from 28.84567
196/196 - 47s - loss: 28.9844 - MinusLogProbMetric: 28.9844 - val_loss: 28.9170 - val_MinusLogProbMetric: 28.9170 - lr: 0.0010 - 47s/epoch - 240ms/step
Epoch 362/1000
2023-09-29 08:38:52.313 
Epoch 362/1000 
	 loss: 29.0399, MinusLogProbMetric: 29.0399, val_loss: 29.2124, val_MinusLogProbMetric: 29.2124

Epoch 362: val_loss did not improve from 28.84567
196/196 - 47s - loss: 29.0399 - MinusLogProbMetric: 29.0399 - val_loss: 29.2124 - val_MinusLogProbMetric: 29.2124 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 363/1000
2023-09-29 08:39:45.439 
Epoch 363/1000 
	 loss: 29.0280, MinusLogProbMetric: 29.0280, val_loss: 29.2281, val_MinusLogProbMetric: 29.2281

Epoch 363: val_loss did not improve from 28.84567
196/196 - 53s - loss: 29.0280 - MinusLogProbMetric: 29.0280 - val_loss: 29.2281 - val_MinusLogProbMetric: 29.2281 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 364/1000
2023-09-29 08:40:38.495 
Epoch 364/1000 
	 loss: 29.2272, MinusLogProbMetric: 29.2272, val_loss: 29.2574, val_MinusLogProbMetric: 29.2574

Epoch 364: val_loss did not improve from 28.84567
196/196 - 53s - loss: 29.2272 - MinusLogProbMetric: 29.2272 - val_loss: 29.2574 - val_MinusLogProbMetric: 29.2574 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 365/1000
2023-09-29 08:41:26.052 
Epoch 365/1000 
	 loss: 28.9712, MinusLogProbMetric: 28.9712, val_loss: 29.1666, val_MinusLogProbMetric: 29.1666

Epoch 365: val_loss did not improve from 28.84567
196/196 - 48s - loss: 28.9712 - MinusLogProbMetric: 28.9712 - val_loss: 29.1666 - val_MinusLogProbMetric: 29.1666 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 366/1000
2023-09-29 08:42:15.227 
Epoch 366/1000 
	 loss: 29.0615, MinusLogProbMetric: 29.0615, val_loss: 29.6790, val_MinusLogProbMetric: 29.6790

Epoch 366: val_loss did not improve from 28.84567
196/196 - 49s - loss: 29.0615 - MinusLogProbMetric: 29.0615 - val_loss: 29.6790 - val_MinusLogProbMetric: 29.6790 - lr: 0.0010 - 49s/epoch - 251ms/step
Epoch 367/1000
2023-09-29 08:43:04.133 
Epoch 367/1000 
	 loss: 29.1486, MinusLogProbMetric: 29.1486, val_loss: 29.7463, val_MinusLogProbMetric: 29.7463

Epoch 367: val_loss did not improve from 28.84567
196/196 - 49s - loss: 29.1486 - MinusLogProbMetric: 29.1486 - val_loss: 29.7463 - val_MinusLogProbMetric: 29.7463 - lr: 0.0010 - 49s/epoch - 249ms/step
Epoch 368/1000
2023-09-29 08:43:57.516 
Epoch 368/1000 
	 loss: 28.9313, MinusLogProbMetric: 28.9313, val_loss: 29.3136, val_MinusLogProbMetric: 29.3136

Epoch 368: val_loss did not improve from 28.84567
196/196 - 53s - loss: 28.9313 - MinusLogProbMetric: 28.9313 - val_loss: 29.3136 - val_MinusLogProbMetric: 29.3136 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 369/1000
2023-09-29 08:44:47.324 
Epoch 369/1000 
	 loss: 28.8643, MinusLogProbMetric: 28.8643, val_loss: 29.3020, val_MinusLogProbMetric: 29.3020

Epoch 369: val_loss did not improve from 28.84567
196/196 - 50s - loss: 28.8643 - MinusLogProbMetric: 28.8643 - val_loss: 29.3020 - val_MinusLogProbMetric: 29.3020 - lr: 0.0010 - 50s/epoch - 254ms/step
Epoch 370/1000
2023-09-29 08:45:34.980 
Epoch 370/1000 
	 loss: 28.9637, MinusLogProbMetric: 28.9637, val_loss: 28.7697, val_MinusLogProbMetric: 28.7697

Epoch 370: val_loss improved from 28.84567 to 28.76966, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 48s - loss: 28.9637 - MinusLogProbMetric: 28.9637 - val_loss: 28.7697 - val_MinusLogProbMetric: 28.7697 - lr: 0.0010 - 48s/epoch - 247ms/step
Epoch 371/1000
2023-09-29 08:46:23.873 
Epoch 371/1000 
	 loss: 28.8991, MinusLogProbMetric: 28.8991, val_loss: 29.0137, val_MinusLogProbMetric: 29.0137

Epoch 371: val_loss did not improve from 28.76966
196/196 - 48s - loss: 28.8991 - MinusLogProbMetric: 28.8991 - val_loss: 29.0137 - val_MinusLogProbMetric: 29.0137 - lr: 0.0010 - 48s/epoch - 246ms/step
Epoch 372/1000
2023-09-29 08:47:16.906 
Epoch 372/1000 
	 loss: 28.9542, MinusLogProbMetric: 28.9542, val_loss: 29.5539, val_MinusLogProbMetric: 29.5539

Epoch 372: val_loss did not improve from 28.76966
196/196 - 53s - loss: 28.9542 - MinusLogProbMetric: 28.9542 - val_loss: 29.5539 - val_MinusLogProbMetric: 29.5539 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 373/1000
2023-09-29 08:48:12.064 
Epoch 373/1000 
	 loss: 29.0825, MinusLogProbMetric: 29.0825, val_loss: 29.6459, val_MinusLogProbMetric: 29.6459

Epoch 373: val_loss did not improve from 28.76966
196/196 - 55s - loss: 29.0825 - MinusLogProbMetric: 29.0825 - val_loss: 29.6459 - val_MinusLogProbMetric: 29.6459 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 374/1000
2023-09-29 08:49:05.282 
Epoch 374/1000 
	 loss: 28.9326, MinusLogProbMetric: 28.9326, val_loss: 28.8255, val_MinusLogProbMetric: 28.8255

Epoch 374: val_loss did not improve from 28.76966
196/196 - 53s - loss: 28.9326 - MinusLogProbMetric: 28.9326 - val_loss: 28.8255 - val_MinusLogProbMetric: 28.8255 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 375/1000
2023-09-29 08:50:00.283 
Epoch 375/1000 
	 loss: 29.1080, MinusLogProbMetric: 29.1080, val_loss: 29.0079, val_MinusLogProbMetric: 29.0079

Epoch 375: val_loss did not improve from 28.76966
196/196 - 55s - loss: 29.1080 - MinusLogProbMetric: 29.1080 - val_loss: 29.0079 - val_MinusLogProbMetric: 29.0079 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 376/1000
2023-09-29 08:50:51.966 
Epoch 376/1000 
	 loss: 28.9019, MinusLogProbMetric: 28.9019, val_loss: 29.0668, val_MinusLogProbMetric: 29.0668

Epoch 376: val_loss did not improve from 28.76966
196/196 - 52s - loss: 28.9019 - MinusLogProbMetric: 28.9019 - val_loss: 29.0668 - val_MinusLogProbMetric: 29.0668 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 377/1000
2023-09-29 08:51:45.013 
Epoch 377/1000 
	 loss: 28.9216, MinusLogProbMetric: 28.9216, val_loss: 29.2762, val_MinusLogProbMetric: 29.2762

Epoch 377: val_loss did not improve from 28.76966
196/196 - 53s - loss: 28.9216 - MinusLogProbMetric: 28.9216 - val_loss: 29.2762 - val_MinusLogProbMetric: 29.2762 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 378/1000
2023-09-29 08:52:40.533 
Epoch 378/1000 
	 loss: 28.8944, MinusLogProbMetric: 28.8944, val_loss: 29.1750, val_MinusLogProbMetric: 29.1750

Epoch 378: val_loss did not improve from 28.76966
196/196 - 56s - loss: 28.8944 - MinusLogProbMetric: 28.8944 - val_loss: 29.1750 - val_MinusLogProbMetric: 29.1750 - lr: 0.0010 - 56s/epoch - 283ms/step
Epoch 379/1000
2023-09-29 08:53:35.084 
Epoch 379/1000 
	 loss: 29.0174, MinusLogProbMetric: 29.0174, val_loss: 28.9986, val_MinusLogProbMetric: 28.9986

Epoch 379: val_loss did not improve from 28.76966
196/196 - 55s - loss: 29.0174 - MinusLogProbMetric: 29.0174 - val_loss: 28.9986 - val_MinusLogProbMetric: 28.9986 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 380/1000
2023-09-29 08:54:30.326 
Epoch 380/1000 
	 loss: 28.9391, MinusLogProbMetric: 28.9391, val_loss: 29.7073, val_MinusLogProbMetric: 29.7073

Epoch 380: val_loss did not improve from 28.76966
196/196 - 55s - loss: 28.9391 - MinusLogProbMetric: 28.9391 - val_loss: 29.7073 - val_MinusLogProbMetric: 29.7073 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 381/1000
2023-09-29 08:55:25.008 
Epoch 381/1000 
	 loss: 28.9201, MinusLogProbMetric: 28.9201, val_loss: 29.0475, val_MinusLogProbMetric: 29.0475

Epoch 381: val_loss did not improve from 28.76966
196/196 - 55s - loss: 28.9201 - MinusLogProbMetric: 28.9201 - val_loss: 29.0475 - val_MinusLogProbMetric: 29.0475 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 382/1000
2023-09-29 08:56:20.636 
Epoch 382/1000 
	 loss: 28.8906, MinusLogProbMetric: 28.8906, val_loss: 29.1681, val_MinusLogProbMetric: 29.1681

Epoch 382: val_loss did not improve from 28.76966
196/196 - 56s - loss: 28.8906 - MinusLogProbMetric: 28.8906 - val_loss: 29.1681 - val_MinusLogProbMetric: 29.1681 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 383/1000
2023-09-29 08:57:15.627 
Epoch 383/1000 
	 loss: 29.1407, MinusLogProbMetric: 29.1407, val_loss: 29.3683, val_MinusLogProbMetric: 29.3683

Epoch 383: val_loss did not improve from 28.76966
196/196 - 55s - loss: 29.1407 - MinusLogProbMetric: 29.1407 - val_loss: 29.3683 - val_MinusLogProbMetric: 29.3683 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 384/1000
2023-09-29 08:58:10.471 
Epoch 384/1000 
	 loss: 28.9673, MinusLogProbMetric: 28.9673, val_loss: 28.9268, val_MinusLogProbMetric: 28.9268

Epoch 384: val_loss did not improve from 28.76966
196/196 - 55s - loss: 28.9673 - MinusLogProbMetric: 28.9673 - val_loss: 28.9268 - val_MinusLogProbMetric: 28.9268 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 385/1000
2023-09-29 08:59:04.942 
Epoch 385/1000 
	 loss: 29.0171, MinusLogProbMetric: 29.0171, val_loss: 29.4717, val_MinusLogProbMetric: 29.4717

Epoch 385: val_loss did not improve from 28.76966
196/196 - 54s - loss: 29.0171 - MinusLogProbMetric: 29.0171 - val_loss: 29.4717 - val_MinusLogProbMetric: 29.4717 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 386/1000
2023-09-29 08:59:58.429 
Epoch 386/1000 
	 loss: 28.9485, MinusLogProbMetric: 28.9485, val_loss: 29.5404, val_MinusLogProbMetric: 29.5404

Epoch 386: val_loss did not improve from 28.76966
196/196 - 53s - loss: 28.9485 - MinusLogProbMetric: 28.9485 - val_loss: 29.5404 - val_MinusLogProbMetric: 29.5404 - lr: 0.0010 - 53s/epoch - 273ms/step
Epoch 387/1000
2023-09-29 09:00:51.850 
Epoch 387/1000 
	 loss: 28.8913, MinusLogProbMetric: 28.8913, val_loss: 28.7660, val_MinusLogProbMetric: 28.7660

Epoch 387: val_loss improved from 28.76966 to 28.76598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 28.8913 - MinusLogProbMetric: 28.8913 - val_loss: 28.7660 - val_MinusLogProbMetric: 28.7660 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 388/1000
2023-09-29 09:01:40.170 
Epoch 388/1000 
	 loss: 28.9254, MinusLogProbMetric: 28.9254, val_loss: 29.0811, val_MinusLogProbMetric: 29.0811

Epoch 388: val_loss did not improve from 28.76598
196/196 - 48s - loss: 28.9254 - MinusLogProbMetric: 28.9254 - val_loss: 29.0811 - val_MinusLogProbMetric: 29.0811 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 389/1000
2023-09-29 09:02:27.665 
Epoch 389/1000 
	 loss: 28.8430, MinusLogProbMetric: 28.8430, val_loss: 29.5099, val_MinusLogProbMetric: 29.5099

Epoch 389: val_loss did not improve from 28.76598
196/196 - 47s - loss: 28.8430 - MinusLogProbMetric: 28.8430 - val_loss: 29.5099 - val_MinusLogProbMetric: 29.5099 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 390/1000
2023-09-29 09:03:16.647 
Epoch 390/1000 
	 loss: 29.0172, MinusLogProbMetric: 29.0172, val_loss: 28.8201, val_MinusLogProbMetric: 28.8201

Epoch 390: val_loss did not improve from 28.76598
196/196 - 49s - loss: 29.0172 - MinusLogProbMetric: 29.0172 - val_loss: 28.8201 - val_MinusLogProbMetric: 28.8201 - lr: 0.0010 - 49s/epoch - 250ms/step
Epoch 391/1000
2023-09-29 09:04:11.839 
Epoch 391/1000 
	 loss: 29.9072, MinusLogProbMetric: 29.9072, val_loss: 31.0381, val_MinusLogProbMetric: 31.0381

Epoch 391: val_loss did not improve from 28.76598
196/196 - 55s - loss: 29.9072 - MinusLogProbMetric: 29.9072 - val_loss: 31.0381 - val_MinusLogProbMetric: 31.0381 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 392/1000
2023-09-29 09:05:06.075 
Epoch 392/1000 
	 loss: 29.9835, MinusLogProbMetric: 29.9835, val_loss: 29.5687, val_MinusLogProbMetric: 29.5687

Epoch 392: val_loss did not improve from 28.76598
196/196 - 54s - loss: 29.9835 - MinusLogProbMetric: 29.9835 - val_loss: 29.5687 - val_MinusLogProbMetric: 29.5687 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 393/1000
2023-09-29 09:05:54.263 
Epoch 393/1000 
	 loss: 29.5781, MinusLogProbMetric: 29.5781, val_loss: 29.4751, val_MinusLogProbMetric: 29.4751

Epoch 393: val_loss did not improve from 28.76598
196/196 - 48s - loss: 29.5781 - MinusLogProbMetric: 29.5781 - val_loss: 29.4751 - val_MinusLogProbMetric: 29.4751 - lr: 0.0010 - 48s/epoch - 246ms/step
Epoch 394/1000
2023-09-29 09:06:41.384 
Epoch 394/1000 
	 loss: 29.4272, MinusLogProbMetric: 29.4272, val_loss: 29.6297, val_MinusLogProbMetric: 29.6297

Epoch 394: val_loss did not improve from 28.76598
196/196 - 47s - loss: 29.4272 - MinusLogProbMetric: 29.4272 - val_loss: 29.6297 - val_MinusLogProbMetric: 29.6297 - lr: 0.0010 - 47s/epoch - 240ms/step
Epoch 395/1000
2023-09-29 09:07:31.185 
Epoch 395/1000 
	 loss: 29.3405, MinusLogProbMetric: 29.3405, val_loss: 29.5003, val_MinusLogProbMetric: 29.5003

Epoch 395: val_loss did not improve from 28.76598
196/196 - 50s - loss: 29.3405 - MinusLogProbMetric: 29.3405 - val_loss: 29.5003 - val_MinusLogProbMetric: 29.5003 - lr: 0.0010 - 50s/epoch - 254ms/step
Epoch 396/1000
2023-09-29 09:08:22.764 
Epoch 396/1000 
	 loss: 29.2811, MinusLogProbMetric: 29.2811, val_loss: 29.2548, val_MinusLogProbMetric: 29.2548

Epoch 396: val_loss did not improve from 28.76598
196/196 - 52s - loss: 29.2811 - MinusLogProbMetric: 29.2811 - val_loss: 29.2548 - val_MinusLogProbMetric: 29.2548 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 397/1000
2023-09-29 09:09:18.364 
Epoch 397/1000 
	 loss: 29.2046, MinusLogProbMetric: 29.2046, val_loss: 29.1654, val_MinusLogProbMetric: 29.1654

Epoch 397: val_loss did not improve from 28.76598
196/196 - 56s - loss: 29.2046 - MinusLogProbMetric: 29.2046 - val_loss: 29.1654 - val_MinusLogProbMetric: 29.1654 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 398/1000
2023-09-29 09:10:11.019 
Epoch 398/1000 
	 loss: 29.0895, MinusLogProbMetric: 29.0895, val_loss: 30.2607, val_MinusLogProbMetric: 30.2607

Epoch 398: val_loss did not improve from 28.76598
196/196 - 53s - loss: 29.0895 - MinusLogProbMetric: 29.0895 - val_loss: 30.2607 - val_MinusLogProbMetric: 30.2607 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 399/1000
2023-09-29 09:11:06.254 
Epoch 399/1000 
	 loss: 29.0654, MinusLogProbMetric: 29.0654, val_loss: 29.3372, val_MinusLogProbMetric: 29.3372

Epoch 399: val_loss did not improve from 28.76598
196/196 - 55s - loss: 29.0654 - MinusLogProbMetric: 29.0654 - val_loss: 29.3372 - val_MinusLogProbMetric: 29.3372 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 400/1000
2023-09-29 09:11:59.730 
Epoch 400/1000 
	 loss: 29.0529, MinusLogProbMetric: 29.0529, val_loss: 29.2049, val_MinusLogProbMetric: 29.2049

Epoch 400: val_loss did not improve from 28.76598
196/196 - 53s - loss: 29.0529 - MinusLogProbMetric: 29.0529 - val_loss: 29.2049 - val_MinusLogProbMetric: 29.2049 - lr: 0.0010 - 53s/epoch - 273ms/step
Epoch 401/1000
2023-09-29 09:12:53.259 
Epoch 401/1000 
	 loss: 29.0815, MinusLogProbMetric: 29.0815, val_loss: 29.1491, val_MinusLogProbMetric: 29.1491

Epoch 401: val_loss did not improve from 28.76598
196/196 - 54s - loss: 29.0815 - MinusLogProbMetric: 29.0815 - val_loss: 29.1491 - val_MinusLogProbMetric: 29.1491 - lr: 0.0010 - 54s/epoch - 273ms/step
Epoch 402/1000
2023-09-29 09:13:46.863 
Epoch 402/1000 
	 loss: 28.9323, MinusLogProbMetric: 28.9323, val_loss: 29.3384, val_MinusLogProbMetric: 29.3384

Epoch 402: val_loss did not improve from 28.76598
196/196 - 54s - loss: 28.9323 - MinusLogProbMetric: 28.9323 - val_loss: 29.3384 - val_MinusLogProbMetric: 29.3384 - lr: 0.0010 - 54s/epoch - 273ms/step
Epoch 403/1000
2023-09-29 09:14:40.489 
Epoch 403/1000 
	 loss: 29.0297, MinusLogProbMetric: 29.0297, val_loss: 29.4854, val_MinusLogProbMetric: 29.4854

Epoch 403: val_loss did not improve from 28.76598
196/196 - 54s - loss: 29.0297 - MinusLogProbMetric: 29.0297 - val_loss: 29.4854 - val_MinusLogProbMetric: 29.4854 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 404/1000
2023-09-29 09:15:36.822 
Epoch 404/1000 
	 loss: 28.8276, MinusLogProbMetric: 28.8276, val_loss: 28.8335, val_MinusLogProbMetric: 28.8335

Epoch 404: val_loss did not improve from 28.76598
196/196 - 56s - loss: 28.8276 - MinusLogProbMetric: 28.8276 - val_loss: 28.8335 - val_MinusLogProbMetric: 28.8335 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 405/1000
2023-09-29 09:16:32.745 
Epoch 405/1000 
	 loss: 28.8611, MinusLogProbMetric: 28.8611, val_loss: 29.6873, val_MinusLogProbMetric: 29.6873

Epoch 405: val_loss did not improve from 28.76598
196/196 - 56s - loss: 28.8611 - MinusLogProbMetric: 28.8611 - val_loss: 29.6873 - val_MinusLogProbMetric: 29.6873 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 406/1000
2023-09-29 09:17:27.028 
Epoch 406/1000 
	 loss: 28.9885, MinusLogProbMetric: 28.9885, val_loss: 28.9967, val_MinusLogProbMetric: 28.9967

Epoch 406: val_loss did not improve from 28.76598
196/196 - 54s - loss: 28.9885 - MinusLogProbMetric: 28.9885 - val_loss: 28.9967 - val_MinusLogProbMetric: 28.9967 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 407/1000
2023-09-29 09:18:21.617 
Epoch 407/1000 
	 loss: 28.8807, MinusLogProbMetric: 28.8807, val_loss: 29.5875, val_MinusLogProbMetric: 29.5875

Epoch 407: val_loss did not improve from 28.76598
196/196 - 55s - loss: 28.8807 - MinusLogProbMetric: 28.8807 - val_loss: 29.5875 - val_MinusLogProbMetric: 29.5875 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 408/1000
2023-09-29 09:19:17.205 
Epoch 408/1000 
	 loss: 28.9905, MinusLogProbMetric: 28.9905, val_loss: 28.9705, val_MinusLogProbMetric: 28.9705

Epoch 408: val_loss did not improve from 28.76598
196/196 - 56s - loss: 28.9905 - MinusLogProbMetric: 28.9905 - val_loss: 28.9705 - val_MinusLogProbMetric: 28.9705 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 409/1000
2023-09-29 09:20:12.652 
Epoch 409/1000 
	 loss: 28.8591, MinusLogProbMetric: 28.8591, val_loss: 29.3004, val_MinusLogProbMetric: 29.3004

Epoch 409: val_loss did not improve from 28.76598
196/196 - 55s - loss: 28.8591 - MinusLogProbMetric: 28.8591 - val_loss: 29.3004 - val_MinusLogProbMetric: 29.3004 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 410/1000
2023-09-29 09:21:08.667 
Epoch 410/1000 
	 loss: 28.8345, MinusLogProbMetric: 28.8345, val_loss: 28.9224, val_MinusLogProbMetric: 28.9224

Epoch 410: val_loss did not improve from 28.76598
196/196 - 56s - loss: 28.8345 - MinusLogProbMetric: 28.8345 - val_loss: 28.9224 - val_MinusLogProbMetric: 28.9224 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 411/1000
2023-09-29 09:21:57.598 
Epoch 411/1000 
	 loss: 29.0209, MinusLogProbMetric: 29.0209, val_loss: 29.1340, val_MinusLogProbMetric: 29.1340

Epoch 411: val_loss did not improve from 28.76598
196/196 - 49s - loss: 29.0209 - MinusLogProbMetric: 29.0209 - val_loss: 29.1340 - val_MinusLogProbMetric: 29.1340 - lr: 0.0010 - 49s/epoch - 250ms/step
Epoch 412/1000
2023-09-29 09:22:45.045 
Epoch 412/1000 
	 loss: 28.7799, MinusLogProbMetric: 28.7799, val_loss: 28.6463, val_MinusLogProbMetric: 28.6463

Epoch 412: val_loss improved from 28.76598 to 28.64627, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 48s - loss: 28.7799 - MinusLogProbMetric: 28.7799 - val_loss: 28.6463 - val_MinusLogProbMetric: 28.6463 - lr: 0.0010 - 48s/epoch - 246ms/step
Epoch 413/1000
2023-09-29 09:23:33.258 
Epoch 413/1000 
	 loss: 29.1692, MinusLogProbMetric: 29.1692, val_loss: 29.1799, val_MinusLogProbMetric: 29.1799

Epoch 413: val_loss did not improve from 28.64627
196/196 - 47s - loss: 29.1692 - MinusLogProbMetric: 29.1692 - val_loss: 29.1799 - val_MinusLogProbMetric: 29.1799 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 414/1000
2023-09-29 09:24:23.367 
Epoch 414/1000 
	 loss: 29.0701, MinusLogProbMetric: 29.0701, val_loss: 29.2331, val_MinusLogProbMetric: 29.2331

Epoch 414: val_loss did not improve from 28.64627
196/196 - 50s - loss: 29.0701 - MinusLogProbMetric: 29.0701 - val_loss: 29.2331 - val_MinusLogProbMetric: 29.2331 - lr: 0.0010 - 50s/epoch - 256ms/step
Epoch 415/1000
2023-09-29 09:25:16.778 
Epoch 415/1000 
	 loss: 28.7793, MinusLogProbMetric: 28.7793, val_loss: 29.0461, val_MinusLogProbMetric: 29.0461

Epoch 415: val_loss did not improve from 28.64627
196/196 - 53s - loss: 28.7793 - MinusLogProbMetric: 28.7793 - val_loss: 29.0461 - val_MinusLogProbMetric: 29.0461 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 416/1000
2023-09-29 09:26:11.017 
Epoch 416/1000 
	 loss: 28.7678, MinusLogProbMetric: 28.7678, val_loss: 28.8707, val_MinusLogProbMetric: 28.8707

Epoch 416: val_loss did not improve from 28.64627
196/196 - 54s - loss: 28.7678 - MinusLogProbMetric: 28.7678 - val_loss: 28.8707 - val_MinusLogProbMetric: 28.8707 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 417/1000
2023-09-29 09:27:06.352 
Epoch 417/1000 
	 loss: 28.8961, MinusLogProbMetric: 28.8961, val_loss: 28.8670, val_MinusLogProbMetric: 28.8670

Epoch 417: val_loss did not improve from 28.64627
196/196 - 55s - loss: 28.8961 - MinusLogProbMetric: 28.8961 - val_loss: 28.8670 - val_MinusLogProbMetric: 28.8670 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 418/1000
2023-09-29 09:28:02.475 
Epoch 418/1000 
	 loss: 28.7923, MinusLogProbMetric: 28.7923, val_loss: 28.7450, val_MinusLogProbMetric: 28.7450

Epoch 418: val_loss did not improve from 28.64627
196/196 - 56s - loss: 28.7923 - MinusLogProbMetric: 28.7923 - val_loss: 28.7450 - val_MinusLogProbMetric: 28.7450 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 419/1000
2023-09-29 09:28:58.682 
Epoch 419/1000 
	 loss: 28.9205, MinusLogProbMetric: 28.9205, val_loss: 28.8635, val_MinusLogProbMetric: 28.8635

Epoch 419: val_loss did not improve from 28.64627
196/196 - 56s - loss: 28.9205 - MinusLogProbMetric: 28.9205 - val_loss: 28.8635 - val_MinusLogProbMetric: 28.8635 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 420/1000
2023-09-29 09:29:54.523 
Epoch 420/1000 
	 loss: 28.8427, MinusLogProbMetric: 28.8427, val_loss: 28.8164, val_MinusLogProbMetric: 28.8164

Epoch 420: val_loss did not improve from 28.64627
196/196 - 56s - loss: 28.8427 - MinusLogProbMetric: 28.8427 - val_loss: 28.8164 - val_MinusLogProbMetric: 28.8164 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 421/1000
2023-09-29 09:30:50.085 
Epoch 421/1000 
	 loss: 28.8502, MinusLogProbMetric: 28.8502, val_loss: 29.4633, val_MinusLogProbMetric: 29.4633

Epoch 421: val_loss did not improve from 28.64627
196/196 - 56s - loss: 28.8502 - MinusLogProbMetric: 28.8502 - val_loss: 29.4633 - val_MinusLogProbMetric: 29.4633 - lr: 0.0010 - 56s/epoch - 283ms/step
Epoch 422/1000
2023-09-29 09:31:44.427 
Epoch 422/1000 
	 loss: 28.8567, MinusLogProbMetric: 28.8567, val_loss: 28.9855, val_MinusLogProbMetric: 28.9855

Epoch 422: val_loss did not improve from 28.64627
196/196 - 54s - loss: 28.8567 - MinusLogProbMetric: 28.8567 - val_loss: 28.9855 - val_MinusLogProbMetric: 28.9855 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 423/1000
2023-09-29 09:32:38.900 
Epoch 423/1000 
	 loss: 28.8628, MinusLogProbMetric: 28.8628, val_loss: 28.6871, val_MinusLogProbMetric: 28.6871

Epoch 423: val_loss did not improve from 28.64627
196/196 - 54s - loss: 28.8628 - MinusLogProbMetric: 28.8628 - val_loss: 28.6871 - val_MinusLogProbMetric: 28.6871 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 424/1000
2023-09-29 09:33:32.090 
Epoch 424/1000 
	 loss: 28.8369, MinusLogProbMetric: 28.8369, val_loss: 28.7405, val_MinusLogProbMetric: 28.7405

Epoch 424: val_loss did not improve from 28.64627
196/196 - 53s - loss: 28.8369 - MinusLogProbMetric: 28.8369 - val_loss: 28.7405 - val_MinusLogProbMetric: 28.7405 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 425/1000
2023-09-29 09:34:25.218 
Epoch 425/1000 
	 loss: 28.7958, MinusLogProbMetric: 28.7958, val_loss: 29.0909, val_MinusLogProbMetric: 29.0909

Epoch 425: val_loss did not improve from 28.64627
196/196 - 53s - loss: 28.7958 - MinusLogProbMetric: 28.7958 - val_loss: 29.0909 - val_MinusLogProbMetric: 29.0909 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 426/1000
2023-09-29 09:35:19.528 
Epoch 426/1000 
	 loss: 28.8859, MinusLogProbMetric: 28.8859, val_loss: 29.2677, val_MinusLogProbMetric: 29.2677

Epoch 426: val_loss did not improve from 28.64627
196/196 - 54s - loss: 28.8859 - MinusLogProbMetric: 28.8859 - val_loss: 29.2677 - val_MinusLogProbMetric: 29.2677 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 427/1000
2023-09-29 09:36:13.476 
Epoch 427/1000 
	 loss: 28.9035, MinusLogProbMetric: 28.9035, val_loss: 28.7786, val_MinusLogProbMetric: 28.7786

Epoch 427: val_loss did not improve from 28.64627
196/196 - 54s - loss: 28.9035 - MinusLogProbMetric: 28.9035 - val_loss: 28.7786 - val_MinusLogProbMetric: 28.7786 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 428/1000
2023-09-29 09:37:08.827 
Epoch 428/1000 
	 loss: 28.8207, MinusLogProbMetric: 28.8207, val_loss: 29.1523, val_MinusLogProbMetric: 29.1523

Epoch 428: val_loss did not improve from 28.64627
196/196 - 55s - loss: 28.8207 - MinusLogProbMetric: 28.8207 - val_loss: 29.1523 - val_MinusLogProbMetric: 29.1523 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 429/1000
2023-09-29 09:38:02.760 
Epoch 429/1000 
	 loss: 29.0665, MinusLogProbMetric: 29.0665, val_loss: 29.3123, val_MinusLogProbMetric: 29.3123

Epoch 429: val_loss did not improve from 28.64627
196/196 - 54s - loss: 29.0665 - MinusLogProbMetric: 29.0665 - val_loss: 29.3123 - val_MinusLogProbMetric: 29.3123 - lr: 0.0010 - 54s/epoch - 275ms/step
Epoch 430/1000
2023-09-29 09:38:57.404 
Epoch 430/1000 
	 loss: 28.7577, MinusLogProbMetric: 28.7577, val_loss: 29.0133, val_MinusLogProbMetric: 29.0133

Epoch 430: val_loss did not improve from 28.64627
196/196 - 55s - loss: 28.7577 - MinusLogProbMetric: 28.7577 - val_loss: 29.0133 - val_MinusLogProbMetric: 29.0133 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 431/1000
2023-09-29 09:39:52.948 
Epoch 431/1000 
	 loss: 28.8260, MinusLogProbMetric: 28.8260, val_loss: 28.9167, val_MinusLogProbMetric: 28.9167

Epoch 431: val_loss did not improve from 28.64627
196/196 - 56s - loss: 28.8260 - MinusLogProbMetric: 28.8260 - val_loss: 28.9167 - val_MinusLogProbMetric: 28.9167 - lr: 0.0010 - 56s/epoch - 283ms/step
Epoch 432/1000
2023-09-29 09:40:48.775 
Epoch 432/1000 
	 loss: 28.8014, MinusLogProbMetric: 28.8014, val_loss: 29.1015, val_MinusLogProbMetric: 29.1015

Epoch 432: val_loss did not improve from 28.64627
196/196 - 56s - loss: 28.8014 - MinusLogProbMetric: 28.8014 - val_loss: 29.1015 - val_MinusLogProbMetric: 29.1015 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 433/1000
2023-09-29 09:41:44.474 
Epoch 433/1000 
	 loss: 28.9520, MinusLogProbMetric: 28.9520, val_loss: 29.0010, val_MinusLogProbMetric: 29.0010

Epoch 433: val_loss did not improve from 28.64627
196/196 - 56s - loss: 28.9520 - MinusLogProbMetric: 28.9520 - val_loss: 29.0010 - val_MinusLogProbMetric: 29.0010 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 434/1000
2023-09-29 09:42:38.027 
Epoch 434/1000 
	 loss: 28.9575, MinusLogProbMetric: 28.9575, val_loss: 29.2839, val_MinusLogProbMetric: 29.2839

Epoch 434: val_loss did not improve from 28.64627
196/196 - 54s - loss: 28.9575 - MinusLogProbMetric: 28.9575 - val_loss: 29.2839 - val_MinusLogProbMetric: 29.2839 - lr: 0.0010 - 54s/epoch - 273ms/step
Epoch 435/1000
2023-09-29 09:43:30.914 
Epoch 435/1000 
	 loss: 28.7136, MinusLogProbMetric: 28.7136, val_loss: 29.2167, val_MinusLogProbMetric: 29.2167

Epoch 435: val_loss did not improve from 28.64627
196/196 - 53s - loss: 28.7136 - MinusLogProbMetric: 28.7136 - val_loss: 29.2167 - val_MinusLogProbMetric: 29.2167 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 436/1000
2023-09-29 09:44:25.264 
Epoch 436/1000 
	 loss: 29.0354, MinusLogProbMetric: 29.0354, val_loss: 28.9357, val_MinusLogProbMetric: 28.9357

Epoch 436: val_loss did not improve from 28.64627
196/196 - 54s - loss: 29.0354 - MinusLogProbMetric: 29.0354 - val_loss: 28.9357 - val_MinusLogProbMetric: 28.9357 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 437/1000
2023-09-29 09:45:20.192 
Epoch 437/1000 
	 loss: 28.7796, MinusLogProbMetric: 28.7796, val_loss: 28.8072, val_MinusLogProbMetric: 28.8072

Epoch 437: val_loss did not improve from 28.64627
196/196 - 55s - loss: 28.7796 - MinusLogProbMetric: 28.7796 - val_loss: 28.8072 - val_MinusLogProbMetric: 28.8072 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 438/1000
2023-09-29 09:46:14.284 
Epoch 438/1000 
	 loss: 28.7627, MinusLogProbMetric: 28.7627, val_loss: 29.4791, val_MinusLogProbMetric: 29.4791

Epoch 438: val_loss did not improve from 28.64627
196/196 - 54s - loss: 28.7627 - MinusLogProbMetric: 28.7627 - val_loss: 29.4791 - val_MinusLogProbMetric: 29.4791 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 439/1000
2023-09-29 09:47:09.667 
Epoch 439/1000 
	 loss: 28.8100, MinusLogProbMetric: 28.8100, val_loss: 29.1210, val_MinusLogProbMetric: 29.1210

Epoch 439: val_loss did not improve from 28.64627
196/196 - 55s - loss: 28.8100 - MinusLogProbMetric: 28.8100 - val_loss: 29.1210 - val_MinusLogProbMetric: 29.1210 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 440/1000
2023-09-29 09:48:04.815 
Epoch 440/1000 
	 loss: 28.9311, MinusLogProbMetric: 28.9311, val_loss: 28.7995, val_MinusLogProbMetric: 28.7995

Epoch 440: val_loss did not improve from 28.64627
196/196 - 55s - loss: 28.9311 - MinusLogProbMetric: 28.9311 - val_loss: 28.7995 - val_MinusLogProbMetric: 28.7995 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 441/1000
2023-09-29 09:49:01.290 
Epoch 441/1000 
	 loss: 28.8423, MinusLogProbMetric: 28.8423, val_loss: 28.6744, val_MinusLogProbMetric: 28.6744

Epoch 441: val_loss did not improve from 28.64627
196/196 - 56s - loss: 28.8423 - MinusLogProbMetric: 28.8423 - val_loss: 28.6744 - val_MinusLogProbMetric: 28.6744 - lr: 0.0010 - 56s/epoch - 288ms/step
Epoch 442/1000
2023-09-29 09:49:56.491 
Epoch 442/1000 
	 loss: 28.8565, MinusLogProbMetric: 28.8565, val_loss: 28.9203, val_MinusLogProbMetric: 28.9203

Epoch 442: val_loss did not improve from 28.64627
196/196 - 55s - loss: 28.8565 - MinusLogProbMetric: 28.8565 - val_loss: 28.9203 - val_MinusLogProbMetric: 28.9203 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 443/1000
2023-09-29 09:50:51.914 
Epoch 443/1000 
	 loss: 28.8017, MinusLogProbMetric: 28.8017, val_loss: 28.7391, val_MinusLogProbMetric: 28.7391

Epoch 443: val_loss did not improve from 28.64627
196/196 - 55s - loss: 28.8017 - MinusLogProbMetric: 28.8017 - val_loss: 28.7391 - val_MinusLogProbMetric: 28.7391 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 444/1000
2023-09-29 09:51:47.708 
Epoch 444/1000 
	 loss: 28.7671, MinusLogProbMetric: 28.7671, val_loss: 28.7084, val_MinusLogProbMetric: 28.7084

Epoch 444: val_loss did not improve from 28.64627
196/196 - 56s - loss: 28.7671 - MinusLogProbMetric: 28.7671 - val_loss: 28.7084 - val_MinusLogProbMetric: 28.7084 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 445/1000
2023-09-29 09:52:43.709 
Epoch 445/1000 
	 loss: 28.8028, MinusLogProbMetric: 28.8028, val_loss: 29.0130, val_MinusLogProbMetric: 29.0130

Epoch 445: val_loss did not improve from 28.64627
196/196 - 56s - loss: 28.8028 - MinusLogProbMetric: 28.8028 - val_loss: 29.0130 - val_MinusLogProbMetric: 29.0130 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 446/1000
2023-09-29 09:53:36.223 
Epoch 446/1000 
	 loss: 28.7408, MinusLogProbMetric: 28.7408, val_loss: 29.0149, val_MinusLogProbMetric: 29.0149

Epoch 446: val_loss did not improve from 28.64627
196/196 - 52s - loss: 28.7408 - MinusLogProbMetric: 28.7408 - val_loss: 29.0149 - val_MinusLogProbMetric: 29.0149 - lr: 0.0010 - 52s/epoch - 268ms/step
Epoch 447/1000
2023-09-29 09:54:29.685 
Epoch 447/1000 
	 loss: 28.8071, MinusLogProbMetric: 28.8071, val_loss: 29.1051, val_MinusLogProbMetric: 29.1051

Epoch 447: val_loss did not improve from 28.64627
196/196 - 53s - loss: 28.8071 - MinusLogProbMetric: 28.8071 - val_loss: 29.1051 - val_MinusLogProbMetric: 29.1051 - lr: 0.0010 - 53s/epoch - 273ms/step
Epoch 448/1000
2023-09-29 09:55:24.251 
Epoch 448/1000 
	 loss: 28.7368, MinusLogProbMetric: 28.7368, val_loss: 29.4260, val_MinusLogProbMetric: 29.4260

Epoch 448: val_loss did not improve from 28.64627
196/196 - 55s - loss: 28.7368 - MinusLogProbMetric: 28.7368 - val_loss: 29.4260 - val_MinusLogProbMetric: 29.4260 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 449/1000
2023-09-29 09:56:16.752 
Epoch 449/1000 
	 loss: 28.7137, MinusLogProbMetric: 28.7137, val_loss: 30.4215, val_MinusLogProbMetric: 30.4215

Epoch 449: val_loss did not improve from 28.64627
196/196 - 52s - loss: 28.7137 - MinusLogProbMetric: 28.7137 - val_loss: 30.4215 - val_MinusLogProbMetric: 30.4215 - lr: 0.0010 - 52s/epoch - 268ms/step
Epoch 450/1000
2023-09-29 09:57:10.251 
Epoch 450/1000 
	 loss: 28.7791, MinusLogProbMetric: 28.7791, val_loss: 29.0103, val_MinusLogProbMetric: 29.0103

Epoch 450: val_loss did not improve from 28.64627
196/196 - 53s - loss: 28.7791 - MinusLogProbMetric: 28.7791 - val_loss: 29.0103 - val_MinusLogProbMetric: 29.0103 - lr: 0.0010 - 53s/epoch - 273ms/step
Epoch 451/1000
2023-09-29 09:58:03.076 
Epoch 451/1000 
	 loss: 28.7544, MinusLogProbMetric: 28.7544, val_loss: 29.1456, val_MinusLogProbMetric: 29.1456

Epoch 451: val_loss did not improve from 28.64627
196/196 - 53s - loss: 28.7544 - MinusLogProbMetric: 28.7544 - val_loss: 29.1456 - val_MinusLogProbMetric: 29.1456 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 452/1000
2023-09-29 09:58:57.810 
Epoch 452/1000 
	 loss: 28.6942, MinusLogProbMetric: 28.6942, val_loss: 29.0324, val_MinusLogProbMetric: 29.0324

Epoch 452: val_loss did not improve from 28.64627
196/196 - 55s - loss: 28.6942 - MinusLogProbMetric: 28.6942 - val_loss: 29.0324 - val_MinusLogProbMetric: 29.0324 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 453/1000
2023-09-29 09:59:53.312 
Epoch 453/1000 
	 loss: 28.8417, MinusLogProbMetric: 28.8417, val_loss: 28.7648, val_MinusLogProbMetric: 28.7648

Epoch 453: val_loss did not improve from 28.64627
196/196 - 55s - loss: 28.8417 - MinusLogProbMetric: 28.8417 - val_loss: 28.7648 - val_MinusLogProbMetric: 28.7648 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 454/1000
2023-09-29 10:00:48.836 
Epoch 454/1000 
	 loss: 28.7148, MinusLogProbMetric: 28.7148, val_loss: 29.5836, val_MinusLogProbMetric: 29.5836

Epoch 454: val_loss did not improve from 28.64627
196/196 - 56s - loss: 28.7148 - MinusLogProbMetric: 28.7148 - val_loss: 29.5836 - val_MinusLogProbMetric: 29.5836 - lr: 0.0010 - 56s/epoch - 283ms/step
Epoch 455/1000
2023-09-29 10:01:42.130 
Epoch 455/1000 
	 loss: 28.7245, MinusLogProbMetric: 28.7245, val_loss: 28.9820, val_MinusLogProbMetric: 28.9820

Epoch 455: val_loss did not improve from 28.64627
196/196 - 53s - loss: 28.7245 - MinusLogProbMetric: 28.7245 - val_loss: 28.9820 - val_MinusLogProbMetric: 28.9820 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 456/1000
2023-09-29 10:02:37.730 
Epoch 456/1000 
	 loss: 28.7100, MinusLogProbMetric: 28.7100, val_loss: 28.7306, val_MinusLogProbMetric: 28.7306

Epoch 456: val_loss did not improve from 28.64627
196/196 - 56s - loss: 28.7100 - MinusLogProbMetric: 28.7100 - val_loss: 28.7306 - val_MinusLogProbMetric: 28.7306 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 457/1000
2023-09-29 10:03:31.856 
Epoch 457/1000 
	 loss: 28.7072, MinusLogProbMetric: 28.7072, val_loss: 28.8789, val_MinusLogProbMetric: 28.8789

Epoch 457: val_loss did not improve from 28.64627
196/196 - 54s - loss: 28.7072 - MinusLogProbMetric: 28.7072 - val_loss: 28.8789 - val_MinusLogProbMetric: 28.8789 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 458/1000
2023-09-29 10:04:25.280 
Epoch 458/1000 
	 loss: 28.6263, MinusLogProbMetric: 28.6263, val_loss: 28.8912, val_MinusLogProbMetric: 28.8912

Epoch 458: val_loss did not improve from 28.64627
196/196 - 53s - loss: 28.6263 - MinusLogProbMetric: 28.6263 - val_loss: 28.8912 - val_MinusLogProbMetric: 28.8912 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 459/1000
2023-09-29 10:05:19.868 
Epoch 459/1000 
	 loss: 28.7392, MinusLogProbMetric: 28.7392, val_loss: 28.5877, val_MinusLogProbMetric: 28.5877

Epoch 459: val_loss improved from 28.64627 to 28.58769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 28.7392 - MinusLogProbMetric: 28.7392 - val_loss: 28.5877 - val_MinusLogProbMetric: 28.5877 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 460/1000
2023-09-29 10:06:15.423 
Epoch 460/1000 
	 loss: 28.7551, MinusLogProbMetric: 28.7551, val_loss: 30.0890, val_MinusLogProbMetric: 30.0890

Epoch 460: val_loss did not improve from 28.58769
196/196 - 55s - loss: 28.7551 - MinusLogProbMetric: 28.7551 - val_loss: 30.0890 - val_MinusLogProbMetric: 30.0890 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 461/1000
2023-09-29 10:07:09.610 
Epoch 461/1000 
	 loss: 28.6973, MinusLogProbMetric: 28.6973, val_loss: 29.4881, val_MinusLogProbMetric: 29.4881

Epoch 461: val_loss did not improve from 28.58769
196/196 - 54s - loss: 28.6973 - MinusLogProbMetric: 28.6973 - val_loss: 29.4881 - val_MinusLogProbMetric: 29.4881 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 462/1000
2023-09-29 10:08:02.274 
Epoch 462/1000 
	 loss: 28.8444, MinusLogProbMetric: 28.8444, val_loss: 28.9717, val_MinusLogProbMetric: 28.9717

Epoch 462: val_loss did not improve from 28.58769
196/196 - 53s - loss: 28.8444 - MinusLogProbMetric: 28.8444 - val_loss: 28.9717 - val_MinusLogProbMetric: 28.9717 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 463/1000
2023-09-29 10:08:56.380 
Epoch 463/1000 
	 loss: 28.6533, MinusLogProbMetric: 28.6533, val_loss: 29.1629, val_MinusLogProbMetric: 29.1629

Epoch 463: val_loss did not improve from 28.58769
196/196 - 54s - loss: 28.6533 - MinusLogProbMetric: 28.6533 - val_loss: 29.1629 - val_MinusLogProbMetric: 29.1629 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 464/1000
2023-09-29 10:09:49.378 
Epoch 464/1000 
	 loss: 28.6778, MinusLogProbMetric: 28.6778, val_loss: 29.3280, val_MinusLogProbMetric: 29.3280

Epoch 464: val_loss did not improve from 28.58769
196/196 - 53s - loss: 28.6778 - MinusLogProbMetric: 28.6778 - val_loss: 29.3280 - val_MinusLogProbMetric: 29.3280 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 465/1000
2023-09-29 10:10:43.762 
Epoch 465/1000 
	 loss: 28.6428, MinusLogProbMetric: 28.6428, val_loss: 29.5630, val_MinusLogProbMetric: 29.5630

Epoch 465: val_loss did not improve from 28.58769
196/196 - 54s - loss: 28.6428 - MinusLogProbMetric: 28.6428 - val_loss: 29.5630 - val_MinusLogProbMetric: 29.5630 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 466/1000
2023-09-29 10:11:38.541 
Epoch 466/1000 
	 loss: 28.8294, MinusLogProbMetric: 28.8294, val_loss: 28.8735, val_MinusLogProbMetric: 28.8735

Epoch 466: val_loss did not improve from 28.58769
196/196 - 55s - loss: 28.8294 - MinusLogProbMetric: 28.8294 - val_loss: 28.8735 - val_MinusLogProbMetric: 28.8735 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 467/1000
2023-09-29 10:12:33.896 
Epoch 467/1000 
	 loss: 28.8142, MinusLogProbMetric: 28.8142, val_loss: 29.0235, val_MinusLogProbMetric: 29.0235

Epoch 467: val_loss did not improve from 28.58769
196/196 - 55s - loss: 28.8142 - MinusLogProbMetric: 28.8142 - val_loss: 29.0235 - val_MinusLogProbMetric: 29.0235 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 468/1000
2023-09-29 10:13:29.394 
Epoch 468/1000 
	 loss: 28.6236, MinusLogProbMetric: 28.6236, val_loss: 29.2693, val_MinusLogProbMetric: 29.2693

Epoch 468: val_loss did not improve from 28.58769
196/196 - 55s - loss: 28.6236 - MinusLogProbMetric: 28.6236 - val_loss: 29.2693 - val_MinusLogProbMetric: 29.2693 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 469/1000
2023-09-29 10:14:24.537 
Epoch 469/1000 
	 loss: 28.6870, MinusLogProbMetric: 28.6870, val_loss: 28.8805, val_MinusLogProbMetric: 28.8805

Epoch 469: val_loss did not improve from 28.58769
196/196 - 55s - loss: 28.6870 - MinusLogProbMetric: 28.6870 - val_loss: 28.8805 - val_MinusLogProbMetric: 28.8805 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 470/1000
2023-09-29 10:15:17.905 
Epoch 470/1000 
	 loss: 28.7585, MinusLogProbMetric: 28.7585, val_loss: 29.1308, val_MinusLogProbMetric: 29.1308

Epoch 470: val_loss did not improve from 28.58769
196/196 - 53s - loss: 28.7585 - MinusLogProbMetric: 28.7585 - val_loss: 29.1308 - val_MinusLogProbMetric: 29.1308 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 471/1000
2023-09-29 10:16:09.935 
Epoch 471/1000 
	 loss: 28.6613, MinusLogProbMetric: 28.6613, val_loss: 28.7720, val_MinusLogProbMetric: 28.7720

Epoch 471: val_loss did not improve from 28.58769
196/196 - 52s - loss: 28.6613 - MinusLogProbMetric: 28.6613 - val_loss: 28.7720 - val_MinusLogProbMetric: 28.7720 - lr: 0.0010 - 52s/epoch - 265ms/step
Epoch 472/1000
2023-09-29 10:17:02.659 
Epoch 472/1000 
	 loss: 28.6700, MinusLogProbMetric: 28.6700, val_loss: 29.6986, val_MinusLogProbMetric: 29.6986

Epoch 472: val_loss did not improve from 28.58769
196/196 - 53s - loss: 28.6700 - MinusLogProbMetric: 28.6700 - val_loss: 29.6986 - val_MinusLogProbMetric: 29.6986 - lr: 0.0010 - 53s/epoch - 269ms/step
Epoch 473/1000
2023-09-29 10:17:55.906 
Epoch 473/1000 
	 loss: 28.6873, MinusLogProbMetric: 28.6873, val_loss: 29.0402, val_MinusLogProbMetric: 29.0402

Epoch 473: val_loss did not improve from 28.58769
196/196 - 53s - loss: 28.6873 - MinusLogProbMetric: 28.6873 - val_loss: 29.0402 - val_MinusLogProbMetric: 29.0402 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 474/1000
2023-09-29 10:18:47.987 
Epoch 474/1000 
	 loss: 28.6661, MinusLogProbMetric: 28.6661, val_loss: 28.7597, val_MinusLogProbMetric: 28.7597

Epoch 474: val_loss did not improve from 28.58769
196/196 - 52s - loss: 28.6661 - MinusLogProbMetric: 28.6661 - val_loss: 28.7597 - val_MinusLogProbMetric: 28.7597 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 475/1000
2023-09-29 10:19:39.340 
Epoch 475/1000 
	 loss: 28.6788, MinusLogProbMetric: 28.6788, val_loss: 29.2245, val_MinusLogProbMetric: 29.2245

Epoch 475: val_loss did not improve from 28.58769
196/196 - 51s - loss: 28.6788 - MinusLogProbMetric: 28.6788 - val_loss: 29.2245 - val_MinusLogProbMetric: 29.2245 - lr: 0.0010 - 51s/epoch - 262ms/step
Epoch 476/1000
2023-09-29 10:20:34.576 
Epoch 476/1000 
	 loss: 28.6816, MinusLogProbMetric: 28.6816, val_loss: 29.1647, val_MinusLogProbMetric: 29.1647

Epoch 476: val_loss did not improve from 28.58769
196/196 - 55s - loss: 28.6816 - MinusLogProbMetric: 28.6816 - val_loss: 29.1647 - val_MinusLogProbMetric: 29.1647 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 477/1000
2023-09-29 10:21:28.686 
Epoch 477/1000 
	 loss: 28.8526, MinusLogProbMetric: 28.8526, val_loss: 28.8585, val_MinusLogProbMetric: 28.8585

Epoch 477: val_loss did not improve from 28.58769
196/196 - 54s - loss: 28.8526 - MinusLogProbMetric: 28.8526 - val_loss: 28.8585 - val_MinusLogProbMetric: 28.8585 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 478/1000
2023-09-29 10:22:22.368 
Epoch 478/1000 
	 loss: 28.6222, MinusLogProbMetric: 28.6222, val_loss: 29.6443, val_MinusLogProbMetric: 29.6443

Epoch 478: val_loss did not improve from 28.58769
196/196 - 54s - loss: 28.6222 - MinusLogProbMetric: 28.6222 - val_loss: 29.6443 - val_MinusLogProbMetric: 29.6443 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 479/1000
2023-09-29 10:23:15.749 
Epoch 479/1000 
	 loss: 28.6129, MinusLogProbMetric: 28.6129, val_loss: 28.6613, val_MinusLogProbMetric: 28.6613

Epoch 479: val_loss did not improve from 28.58769
196/196 - 53s - loss: 28.6129 - MinusLogProbMetric: 28.6129 - val_loss: 28.6613 - val_MinusLogProbMetric: 28.6613 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 480/1000
2023-09-29 10:24:10.812 
Epoch 480/1000 
	 loss: 28.7088, MinusLogProbMetric: 28.7088, val_loss: 28.7868, val_MinusLogProbMetric: 28.7868

Epoch 480: val_loss did not improve from 28.58769
196/196 - 55s - loss: 28.7088 - MinusLogProbMetric: 28.7088 - val_loss: 28.7868 - val_MinusLogProbMetric: 28.7868 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 481/1000
2023-09-29 10:25:04.534 
Epoch 481/1000 
	 loss: 28.6153, MinusLogProbMetric: 28.6153, val_loss: 28.9022, val_MinusLogProbMetric: 28.9022

Epoch 481: val_loss did not improve from 28.58769
196/196 - 54s - loss: 28.6153 - MinusLogProbMetric: 28.6153 - val_loss: 28.9022 - val_MinusLogProbMetric: 28.9022 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 482/1000
2023-09-29 10:26:00.028 
Epoch 482/1000 
	 loss: 28.7443, MinusLogProbMetric: 28.7443, val_loss: 29.0357, val_MinusLogProbMetric: 29.0357

Epoch 482: val_loss did not improve from 28.58769
196/196 - 55s - loss: 28.7443 - MinusLogProbMetric: 28.7443 - val_loss: 29.0357 - val_MinusLogProbMetric: 29.0357 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 483/1000
2023-09-29 10:26:55.673 
Epoch 483/1000 
	 loss: 28.6228, MinusLogProbMetric: 28.6228, val_loss: 29.8888, val_MinusLogProbMetric: 29.8888

Epoch 483: val_loss did not improve from 28.58769
196/196 - 56s - loss: 28.6228 - MinusLogProbMetric: 28.6228 - val_loss: 29.8888 - val_MinusLogProbMetric: 29.8888 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 484/1000
2023-09-29 10:27:50.288 
Epoch 484/1000 
	 loss: 28.6614, MinusLogProbMetric: 28.6614, val_loss: 29.2738, val_MinusLogProbMetric: 29.2738

Epoch 484: val_loss did not improve from 28.58769
196/196 - 55s - loss: 28.6614 - MinusLogProbMetric: 28.6614 - val_loss: 29.2738 - val_MinusLogProbMetric: 29.2738 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 485/1000
2023-09-29 10:28:43.620 
Epoch 485/1000 
	 loss: 28.6616, MinusLogProbMetric: 28.6616, val_loss: 29.3900, val_MinusLogProbMetric: 29.3900

Epoch 485: val_loss did not improve from 28.58769
196/196 - 53s - loss: 28.6616 - MinusLogProbMetric: 28.6616 - val_loss: 29.3900 - val_MinusLogProbMetric: 29.3900 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 486/1000
2023-09-29 10:29:37.238 
Epoch 486/1000 
	 loss: 28.5807, MinusLogProbMetric: 28.5807, val_loss: 28.8907, val_MinusLogProbMetric: 28.8907

Epoch 486: val_loss did not improve from 28.58769
196/196 - 54s - loss: 28.5807 - MinusLogProbMetric: 28.5807 - val_loss: 28.8907 - val_MinusLogProbMetric: 28.8907 - lr: 0.0010 - 54s/epoch - 274ms/step
Epoch 487/1000
2023-09-29 10:30:32.717 
Epoch 487/1000 
	 loss: 28.7163, MinusLogProbMetric: 28.7163, val_loss: 29.5282, val_MinusLogProbMetric: 29.5282

Epoch 487: val_loss did not improve from 28.58769
196/196 - 55s - loss: 28.7163 - MinusLogProbMetric: 28.7163 - val_loss: 29.5282 - val_MinusLogProbMetric: 29.5282 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 488/1000
2023-09-29 10:31:27.837 
Epoch 488/1000 
	 loss: 28.8592, MinusLogProbMetric: 28.8592, val_loss: 28.9511, val_MinusLogProbMetric: 28.9511

Epoch 488: val_loss did not improve from 28.58769
196/196 - 55s - loss: 28.8592 - MinusLogProbMetric: 28.8592 - val_loss: 28.9511 - val_MinusLogProbMetric: 28.9511 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 489/1000
2023-09-29 10:32:22.081 
Epoch 489/1000 
	 loss: 28.5988, MinusLogProbMetric: 28.5988, val_loss: 28.7622, val_MinusLogProbMetric: 28.7622

Epoch 489: val_loss did not improve from 28.58769
196/196 - 54s - loss: 28.5988 - MinusLogProbMetric: 28.5988 - val_loss: 28.7622 - val_MinusLogProbMetric: 28.7622 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 490/1000
2023-09-29 10:33:14.994 
Epoch 490/1000 
	 loss: 28.7304, MinusLogProbMetric: 28.7304, val_loss: 28.7440, val_MinusLogProbMetric: 28.7440

Epoch 490: val_loss did not improve from 28.58769
196/196 - 53s - loss: 28.7304 - MinusLogProbMetric: 28.7304 - val_loss: 28.7440 - val_MinusLogProbMetric: 28.7440 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 491/1000
2023-09-29 10:34:10.004 
Epoch 491/1000 
	 loss: 28.5926, MinusLogProbMetric: 28.5926, val_loss: 28.8590, val_MinusLogProbMetric: 28.8590

Epoch 491: val_loss did not improve from 28.58769
196/196 - 55s - loss: 28.5926 - MinusLogProbMetric: 28.5926 - val_loss: 28.8590 - val_MinusLogProbMetric: 28.8590 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 492/1000
2023-09-29 10:35:04.871 
Epoch 492/1000 
	 loss: 28.7146, MinusLogProbMetric: 28.7146, val_loss: 28.9572, val_MinusLogProbMetric: 28.9572

Epoch 492: val_loss did not improve from 28.58769
196/196 - 55s - loss: 28.7146 - MinusLogProbMetric: 28.7146 - val_loss: 28.9572 - val_MinusLogProbMetric: 28.9572 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 493/1000
2023-09-29 10:35:58.394 
Epoch 493/1000 
	 loss: 28.6559, MinusLogProbMetric: 28.6559, val_loss: 29.1998, val_MinusLogProbMetric: 29.1998

Epoch 493: val_loss did not improve from 28.58769
196/196 - 53s - loss: 28.6559 - MinusLogProbMetric: 28.6559 - val_loss: 29.1998 - val_MinusLogProbMetric: 29.1998 - lr: 0.0010 - 53s/epoch - 273ms/step
Epoch 494/1000
2023-09-29 10:36:50.607 
Epoch 494/1000 
	 loss: 28.6043, MinusLogProbMetric: 28.6043, val_loss: 28.6559, val_MinusLogProbMetric: 28.6559

Epoch 494: val_loss did not improve from 28.58769
196/196 - 52s - loss: 28.6043 - MinusLogProbMetric: 28.6043 - val_loss: 28.6559 - val_MinusLogProbMetric: 28.6559 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 495/1000
2023-09-29 10:37:42.128 
Epoch 495/1000 
	 loss: 28.6396, MinusLogProbMetric: 28.6396, val_loss: 28.6793, val_MinusLogProbMetric: 28.6793

Epoch 495: val_loss did not improve from 28.58769
196/196 - 52s - loss: 28.6396 - MinusLogProbMetric: 28.6396 - val_loss: 28.6793 - val_MinusLogProbMetric: 28.6793 - lr: 0.0010 - 52s/epoch - 263ms/step
Epoch 496/1000
2023-09-29 10:38:35.256 
Epoch 496/1000 
	 loss: 28.6675, MinusLogProbMetric: 28.6675, val_loss: 28.6386, val_MinusLogProbMetric: 28.6386

Epoch 496: val_loss did not improve from 28.58769
196/196 - 53s - loss: 28.6675 - MinusLogProbMetric: 28.6675 - val_loss: 28.6386 - val_MinusLogProbMetric: 28.6386 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 497/1000
2023-09-29 10:39:28.647 
Epoch 497/1000 
	 loss: 28.6217, MinusLogProbMetric: 28.6217, val_loss: 28.6900, val_MinusLogProbMetric: 28.6900

Epoch 497: val_loss did not improve from 28.58769
196/196 - 53s - loss: 28.6217 - MinusLogProbMetric: 28.6217 - val_loss: 28.6900 - val_MinusLogProbMetric: 28.6900 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 498/1000
2023-09-29 10:40:22.966 
Epoch 498/1000 
	 loss: 28.6014, MinusLogProbMetric: 28.6014, val_loss: 28.5726, val_MinusLogProbMetric: 28.5726

Epoch 498: val_loss improved from 28.58769 to 28.57264, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 28.6014 - MinusLogProbMetric: 28.6014 - val_loss: 28.5726 - val_MinusLogProbMetric: 28.5726 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 499/1000
2023-09-29 10:41:18.222 
Epoch 499/1000 
	 loss: 28.6087, MinusLogProbMetric: 28.6087, val_loss: 28.6434, val_MinusLogProbMetric: 28.6434

Epoch 499: val_loss did not improve from 28.57264
196/196 - 54s - loss: 28.6087 - MinusLogProbMetric: 28.6087 - val_loss: 28.6434 - val_MinusLogProbMetric: 28.6434 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 500/1000
2023-09-29 10:42:10.448 
Epoch 500/1000 
	 loss: 28.5669, MinusLogProbMetric: 28.5669, val_loss: 29.3245, val_MinusLogProbMetric: 29.3245

Epoch 500: val_loss did not improve from 28.57264
196/196 - 52s - loss: 28.5669 - MinusLogProbMetric: 28.5669 - val_loss: 29.3245 - val_MinusLogProbMetric: 29.3245 - lr: 0.0010 - 52s/epoch - 266ms/step
Epoch 501/1000
2023-09-29 10:43:05.201 
Epoch 501/1000 
	 loss: 28.6573, MinusLogProbMetric: 28.6573, val_loss: 29.3621, val_MinusLogProbMetric: 29.3621

Epoch 501: val_loss did not improve from 28.57264
196/196 - 55s - loss: 28.6573 - MinusLogProbMetric: 28.6573 - val_loss: 29.3621 - val_MinusLogProbMetric: 29.3621 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 502/1000
2023-09-29 10:44:00.601 
Epoch 502/1000 
	 loss: 28.6226, MinusLogProbMetric: 28.6226, val_loss: 29.3096, val_MinusLogProbMetric: 29.3096

Epoch 502: val_loss did not improve from 28.57264
196/196 - 55s - loss: 28.6226 - MinusLogProbMetric: 28.6226 - val_loss: 29.3096 - val_MinusLogProbMetric: 29.3096 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 503/1000
2023-09-29 10:44:55.672 
Epoch 503/1000 
	 loss: 28.6015, MinusLogProbMetric: 28.6015, val_loss: 28.6156, val_MinusLogProbMetric: 28.6156

Epoch 503: val_loss did not improve from 28.57264
196/196 - 55s - loss: 28.6015 - MinusLogProbMetric: 28.6015 - val_loss: 28.6156 - val_MinusLogProbMetric: 28.6156 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 504/1000
2023-09-29 10:45:50.631 
Epoch 504/1000 
	 loss: 28.6730, MinusLogProbMetric: 28.6730, val_loss: 29.0866, val_MinusLogProbMetric: 29.0866

Epoch 504: val_loss did not improve from 28.57264
196/196 - 55s - loss: 28.6730 - MinusLogProbMetric: 28.6730 - val_loss: 29.0866 - val_MinusLogProbMetric: 29.0866 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 505/1000
2023-09-29 10:46:46.130 
Epoch 505/1000 
	 loss: 28.6255, MinusLogProbMetric: 28.6255, val_loss: 28.8843, val_MinusLogProbMetric: 28.8843

Epoch 505: val_loss did not improve from 28.57264
196/196 - 55s - loss: 28.6255 - MinusLogProbMetric: 28.6255 - val_loss: 28.8843 - val_MinusLogProbMetric: 28.8843 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 506/1000
2023-09-29 10:47:40.569 
Epoch 506/1000 
	 loss: 28.5800, MinusLogProbMetric: 28.5800, val_loss: 28.5978, val_MinusLogProbMetric: 28.5978

Epoch 506: val_loss did not improve from 28.57264
196/196 - 54s - loss: 28.5800 - MinusLogProbMetric: 28.5800 - val_loss: 28.5978 - val_MinusLogProbMetric: 28.5978 - lr: 0.0010 - 54s/epoch - 278ms/step
Epoch 507/1000
2023-09-29 10:48:36.254 
Epoch 507/1000 
	 loss: 28.5811, MinusLogProbMetric: 28.5811, val_loss: 28.8424, val_MinusLogProbMetric: 28.8424

Epoch 507: val_loss did not improve from 28.57264
196/196 - 56s - loss: 28.5811 - MinusLogProbMetric: 28.5811 - val_loss: 28.8424 - val_MinusLogProbMetric: 28.8424 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 508/1000
2023-09-29 10:49:29.274 
Epoch 508/1000 
	 loss: 28.6099, MinusLogProbMetric: 28.6099, val_loss: 29.0057, val_MinusLogProbMetric: 29.0057

Epoch 508: val_loss did not improve from 28.57264
196/196 - 53s - loss: 28.6099 - MinusLogProbMetric: 28.6099 - val_loss: 29.0057 - val_MinusLogProbMetric: 29.0057 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 509/1000
2023-09-29 10:50:22.476 
Epoch 509/1000 
	 loss: 28.6075, MinusLogProbMetric: 28.6075, val_loss: 28.7517, val_MinusLogProbMetric: 28.7517

Epoch 509: val_loss did not improve from 28.57264
196/196 - 53s - loss: 28.6075 - MinusLogProbMetric: 28.6075 - val_loss: 28.7517 - val_MinusLogProbMetric: 28.7517 - lr: 0.0010 - 53s/epoch - 271ms/step
Epoch 510/1000
2023-09-29 10:51:18.085 
Epoch 510/1000 
	 loss: 28.6093, MinusLogProbMetric: 28.6093, val_loss: 29.2467, val_MinusLogProbMetric: 29.2467

Epoch 510: val_loss did not improve from 28.57264
196/196 - 56s - loss: 28.6093 - MinusLogProbMetric: 28.6093 - val_loss: 29.2467 - val_MinusLogProbMetric: 29.2467 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 511/1000
2023-09-29 10:52:12.320 
Epoch 511/1000 
	 loss: 28.5989, MinusLogProbMetric: 28.5989, val_loss: 28.8042, val_MinusLogProbMetric: 28.8042

Epoch 511: val_loss did not improve from 28.57264
196/196 - 54s - loss: 28.5989 - MinusLogProbMetric: 28.5989 - val_loss: 28.8042 - val_MinusLogProbMetric: 28.8042 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 512/1000
2023-09-29 10:53:06.513 
Epoch 512/1000 
	 loss: 28.6459, MinusLogProbMetric: 28.6459, val_loss: 28.9809, val_MinusLogProbMetric: 28.9809

Epoch 512: val_loss did not improve from 28.57264
196/196 - 54s - loss: 28.6459 - MinusLogProbMetric: 28.6459 - val_loss: 28.9809 - val_MinusLogProbMetric: 28.9809 - lr: 0.0010 - 54s/epoch - 276ms/step
Epoch 513/1000
2023-09-29 10:54:00.903 
Epoch 513/1000 
	 loss: 28.5433, MinusLogProbMetric: 28.5433, val_loss: 28.5094, val_MinusLogProbMetric: 28.5094

Epoch 513: val_loss improved from 28.57264 to 28.50943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 28.5433 - MinusLogProbMetric: 28.5433 - val_loss: 28.5094 - val_MinusLogProbMetric: 28.5094 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 514/1000
2023-09-29 10:54:49.096 
Epoch 514/1000 
	 loss: 28.7282, MinusLogProbMetric: 28.7282, val_loss: 28.7095, val_MinusLogProbMetric: 28.7095

Epoch 514: val_loss did not improve from 28.50943
196/196 - 47s - loss: 28.7282 - MinusLogProbMetric: 28.7282 - val_loss: 28.7095 - val_MinusLogProbMetric: 28.7095 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 515/1000
2023-09-29 10:55:35.994 
Epoch 515/1000 
	 loss: 28.6324, MinusLogProbMetric: 28.6324, val_loss: 28.8529, val_MinusLogProbMetric: 28.8529

Epoch 515: val_loss did not improve from 28.50943
196/196 - 47s - loss: 28.6324 - MinusLogProbMetric: 28.6324 - val_loss: 28.8529 - val_MinusLogProbMetric: 28.8529 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 516/1000
2023-09-29 10:56:22.511 
Epoch 516/1000 
	 loss: 28.7431, MinusLogProbMetric: 28.7431, val_loss: 28.8679, val_MinusLogProbMetric: 28.8679

Epoch 516: val_loss did not improve from 28.50943
196/196 - 47s - loss: 28.7431 - MinusLogProbMetric: 28.7431 - val_loss: 28.8679 - val_MinusLogProbMetric: 28.8679 - lr: 0.0010 - 47s/epoch - 237ms/step
Epoch 517/1000
2023-09-29 10:57:11.455 
Epoch 517/1000 
	 loss: 28.6038, MinusLogProbMetric: 28.6038, val_loss: 28.6607, val_MinusLogProbMetric: 28.6607

Epoch 517: val_loss did not improve from 28.50943
196/196 - 49s - loss: 28.6038 - MinusLogProbMetric: 28.6038 - val_loss: 28.6607 - val_MinusLogProbMetric: 28.6607 - lr: 0.0010 - 49s/epoch - 250ms/step
Epoch 518/1000
2023-09-29 10:57:57.042 
Epoch 518/1000 
	 loss: 28.7667, MinusLogProbMetric: 28.7667, val_loss: 29.4640, val_MinusLogProbMetric: 29.4640

Epoch 518: val_loss did not improve from 28.50943
196/196 - 46s - loss: 28.7667 - MinusLogProbMetric: 28.7667 - val_loss: 29.4640 - val_MinusLogProbMetric: 29.4640 - lr: 0.0010 - 46s/epoch - 233ms/step
Epoch 519/1000
2023-09-29 10:58:43.985 
Epoch 519/1000 
	 loss: 28.5883, MinusLogProbMetric: 28.5883, val_loss: 31.4599, val_MinusLogProbMetric: 31.4599

Epoch 519: val_loss did not improve from 28.50943
196/196 - 47s - loss: 28.5883 - MinusLogProbMetric: 28.5883 - val_loss: 31.4599 - val_MinusLogProbMetric: 31.4599 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 520/1000
2023-09-29 10:59:31.114 
Epoch 520/1000 
	 loss: 28.5584, MinusLogProbMetric: 28.5584, val_loss: 28.7522, val_MinusLogProbMetric: 28.7522

Epoch 520: val_loss did not improve from 28.50943
196/196 - 47s - loss: 28.5584 - MinusLogProbMetric: 28.5584 - val_loss: 28.7522 - val_MinusLogProbMetric: 28.7522 - lr: 0.0010 - 47s/epoch - 240ms/step
Epoch 521/1000
2023-09-29 11:00:20.065 
Epoch 521/1000 
	 loss: 28.5775, MinusLogProbMetric: 28.5775, val_loss: 28.4628, val_MinusLogProbMetric: 28.4628

Epoch 521: val_loss improved from 28.50943 to 28.46277, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 50s - loss: 28.5775 - MinusLogProbMetric: 28.5775 - val_loss: 28.4628 - val_MinusLogProbMetric: 28.4628 - lr: 0.0010 - 50s/epoch - 253ms/step
Epoch 522/1000
2023-09-29 11:01:10.431 
Epoch 522/1000 
	 loss: 28.5360, MinusLogProbMetric: 28.5360, val_loss: 28.7087, val_MinusLogProbMetric: 28.7087

Epoch 522: val_loss did not improve from 28.46277
196/196 - 50s - loss: 28.5360 - MinusLogProbMetric: 28.5360 - val_loss: 28.7087 - val_MinusLogProbMetric: 28.7087 - lr: 0.0010 - 50s/epoch - 254ms/step
Epoch 523/1000
2023-09-29 11:01:59.367 
Epoch 523/1000 
	 loss: 28.6681, MinusLogProbMetric: 28.6681, val_loss: 29.4654, val_MinusLogProbMetric: 29.4654

Epoch 523: val_loss did not improve from 28.46277
196/196 - 49s - loss: 28.6681 - MinusLogProbMetric: 28.6681 - val_loss: 29.4654 - val_MinusLogProbMetric: 29.4654 - lr: 0.0010 - 49s/epoch - 250ms/step
Epoch 524/1000
2023-09-29 11:02:49.402 
Epoch 524/1000 
	 loss: 28.5236, MinusLogProbMetric: 28.5236, val_loss: 28.7207, val_MinusLogProbMetric: 28.7207

Epoch 524: val_loss did not improve from 28.46277
196/196 - 50s - loss: 28.5236 - MinusLogProbMetric: 28.5236 - val_loss: 28.7207 - val_MinusLogProbMetric: 28.7207 - lr: 0.0010 - 50s/epoch - 255ms/step
Epoch 525/1000
2023-09-29 11:03:40.539 
Epoch 525/1000 
	 loss: 28.6172, MinusLogProbMetric: 28.6172, val_loss: 29.2533, val_MinusLogProbMetric: 29.2533

Epoch 525: val_loss did not improve from 28.46277
196/196 - 51s - loss: 28.6172 - MinusLogProbMetric: 28.6172 - val_loss: 29.2533 - val_MinusLogProbMetric: 29.2533 - lr: 0.0010 - 51s/epoch - 261ms/step
Epoch 526/1000
2023-09-29 11:04:30.268 
Epoch 526/1000 
	 loss: 28.5897, MinusLogProbMetric: 28.5897, val_loss: 29.0522, val_MinusLogProbMetric: 29.0522

Epoch 526: val_loss did not improve from 28.46277
196/196 - 50s - loss: 28.5897 - MinusLogProbMetric: 28.5897 - val_loss: 29.0522 - val_MinusLogProbMetric: 29.0522 - lr: 0.0010 - 50s/epoch - 254ms/step
Epoch 527/1000
2023-09-29 11:05:16.187 
Epoch 527/1000 
	 loss: 28.5476, MinusLogProbMetric: 28.5476, val_loss: 29.6193, val_MinusLogProbMetric: 29.6193

Epoch 527: val_loss did not improve from 28.46277
196/196 - 46s - loss: 28.5476 - MinusLogProbMetric: 28.5476 - val_loss: 29.6193 - val_MinusLogProbMetric: 29.6193 - lr: 0.0010 - 46s/epoch - 234ms/step
Epoch 528/1000
2023-09-29 11:06:04.956 
Epoch 528/1000 
	 loss: 28.5879, MinusLogProbMetric: 28.5879, val_loss: 28.6046, val_MinusLogProbMetric: 28.6046

Epoch 528: val_loss did not improve from 28.46277
196/196 - 49s - loss: 28.5879 - MinusLogProbMetric: 28.5879 - val_loss: 28.6046 - val_MinusLogProbMetric: 28.6046 - lr: 0.0010 - 49s/epoch - 249ms/step
Epoch 529/1000
2023-09-29 11:06:55.278 
Epoch 529/1000 
	 loss: 28.5748, MinusLogProbMetric: 28.5748, val_loss: 28.8254, val_MinusLogProbMetric: 28.8254

Epoch 529: val_loss did not improve from 28.46277
196/196 - 50s - loss: 28.5748 - MinusLogProbMetric: 28.5748 - val_loss: 28.8254 - val_MinusLogProbMetric: 28.8254 - lr: 0.0010 - 50s/epoch - 257ms/step
Epoch 530/1000
2023-09-29 11:07:47.296 
Epoch 530/1000 
	 loss: 28.5439, MinusLogProbMetric: 28.5439, val_loss: 28.7852, val_MinusLogProbMetric: 28.7852

Epoch 530: val_loss did not improve from 28.46277
196/196 - 52s - loss: 28.5439 - MinusLogProbMetric: 28.5439 - val_loss: 28.7852 - val_MinusLogProbMetric: 28.7852 - lr: 0.0010 - 52s/epoch - 265ms/step
Epoch 531/1000
2023-09-29 11:08:34.283 
Epoch 531/1000 
	 loss: 28.6020, MinusLogProbMetric: 28.6020, val_loss: 28.9759, val_MinusLogProbMetric: 28.9759

Epoch 531: val_loss did not improve from 28.46277
196/196 - 47s - loss: 28.6020 - MinusLogProbMetric: 28.6020 - val_loss: 28.9759 - val_MinusLogProbMetric: 28.9759 - lr: 0.0010 - 47s/epoch - 240ms/step
Epoch 532/1000
2023-09-29 11:09:23.649 
Epoch 532/1000 
	 loss: 28.5441, MinusLogProbMetric: 28.5441, val_loss: 29.3221, val_MinusLogProbMetric: 29.3221

Epoch 532: val_loss did not improve from 28.46277
196/196 - 49s - loss: 28.5441 - MinusLogProbMetric: 28.5441 - val_loss: 29.3221 - val_MinusLogProbMetric: 29.3221 - lr: 0.0010 - 49s/epoch - 252ms/step
Epoch 533/1000
2023-09-29 11:10:11.035 
Epoch 533/1000 
	 loss: 28.6093, MinusLogProbMetric: 28.6093, val_loss: 29.0437, val_MinusLogProbMetric: 29.0437

Epoch 533: val_loss did not improve from 28.46277
196/196 - 47s - loss: 28.6093 - MinusLogProbMetric: 28.6093 - val_loss: 29.0437 - val_MinusLogProbMetric: 29.0437 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 534/1000
2023-09-29 11:10:59.967 
Epoch 534/1000 
	 loss: 28.6581, MinusLogProbMetric: 28.6581, val_loss: 28.9912, val_MinusLogProbMetric: 28.9912

Epoch 534: val_loss did not improve from 28.46277
196/196 - 49s - loss: 28.6581 - MinusLogProbMetric: 28.6581 - val_loss: 28.9912 - val_MinusLogProbMetric: 28.9912 - lr: 0.0010 - 49s/epoch - 250ms/step
Epoch 535/1000
2023-09-29 11:11:46.586 
Epoch 535/1000 
	 loss: 28.5734, MinusLogProbMetric: 28.5734, val_loss: 28.9059, val_MinusLogProbMetric: 28.9059

Epoch 535: val_loss did not improve from 28.46277
196/196 - 47s - loss: 28.5734 - MinusLogProbMetric: 28.5734 - val_loss: 28.9059 - val_MinusLogProbMetric: 28.9059 - lr: 0.0010 - 47s/epoch - 238ms/step
Epoch 536/1000
2023-09-29 11:12:35.358 
Epoch 536/1000 
	 loss: 28.7615, MinusLogProbMetric: 28.7615, val_loss: 28.9845, val_MinusLogProbMetric: 28.9845

Epoch 536: val_loss did not improve from 28.46277
196/196 - 49s - loss: 28.7615 - MinusLogProbMetric: 28.7615 - val_loss: 28.9845 - val_MinusLogProbMetric: 28.9845 - lr: 0.0010 - 49s/epoch - 249ms/step
Epoch 537/1000
2023-09-29 11:13:21.564 
Epoch 537/1000 
	 loss: 28.5457, MinusLogProbMetric: 28.5457, val_loss: 28.7640, val_MinusLogProbMetric: 28.7640

Epoch 537: val_loss did not improve from 28.46277
196/196 - 46s - loss: 28.5457 - MinusLogProbMetric: 28.5457 - val_loss: 28.7640 - val_MinusLogProbMetric: 28.7640 - lr: 0.0010 - 46s/epoch - 236ms/step
Epoch 538/1000
2023-09-29 11:14:07.683 
Epoch 538/1000 
	 loss: 28.5792, MinusLogProbMetric: 28.5792, val_loss: 28.4573, val_MinusLogProbMetric: 28.4573

Epoch 538: val_loss improved from 28.46277 to 28.45726, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 48s - loss: 28.5792 - MinusLogProbMetric: 28.5792 - val_loss: 28.4573 - val_MinusLogProbMetric: 28.4573 - lr: 0.0010 - 48s/epoch - 244ms/step
Epoch 539/1000
2023-09-29 11:15:01.221 
Epoch 539/1000 
	 loss: 28.5747, MinusLogProbMetric: 28.5747, val_loss: 28.7787, val_MinusLogProbMetric: 28.7787

Epoch 539: val_loss did not improve from 28.45726
196/196 - 52s - loss: 28.5747 - MinusLogProbMetric: 28.5747 - val_loss: 28.7787 - val_MinusLogProbMetric: 28.7787 - lr: 0.0010 - 52s/epoch - 265ms/step
Epoch 540/1000
2023-09-29 11:15:49.542 
Epoch 540/1000 
	 loss: 28.5800, MinusLogProbMetric: 28.5800, val_loss: 28.9311, val_MinusLogProbMetric: 28.9311

Epoch 540: val_loss did not improve from 28.45726
196/196 - 48s - loss: 28.5800 - MinusLogProbMetric: 28.5800 - val_loss: 28.9311 - val_MinusLogProbMetric: 28.9311 - lr: 0.0010 - 48s/epoch - 246ms/step
Epoch 541/1000
2023-09-29 11:16:37.482 
Epoch 541/1000 
	 loss: 28.4785, MinusLogProbMetric: 28.4785, val_loss: 28.8881, val_MinusLogProbMetric: 28.8881

Epoch 541: val_loss did not improve from 28.45726
196/196 - 48s - loss: 28.4785 - MinusLogProbMetric: 28.4785 - val_loss: 28.8881 - val_MinusLogProbMetric: 28.8881 - lr: 0.0010 - 48s/epoch - 245ms/step
Epoch 542/1000
2023-09-29 11:17:24.615 
Epoch 542/1000 
	 loss: 28.5243, MinusLogProbMetric: 28.5243, val_loss: 28.5276, val_MinusLogProbMetric: 28.5276

Epoch 542: val_loss did not improve from 28.45726
196/196 - 47s - loss: 28.5243 - MinusLogProbMetric: 28.5243 - val_loss: 28.5276 - val_MinusLogProbMetric: 28.5276 - lr: 0.0010 - 47s/epoch - 240ms/step
Epoch 543/1000
2023-09-29 11:18:11.813 
Epoch 543/1000 
	 loss: 28.7268, MinusLogProbMetric: 28.7268, val_loss: 28.9134, val_MinusLogProbMetric: 28.9134

Epoch 543: val_loss did not improve from 28.45726
196/196 - 47s - loss: 28.7268 - MinusLogProbMetric: 28.7268 - val_loss: 28.9134 - val_MinusLogProbMetric: 28.9134 - lr: 0.0010 - 47s/epoch - 241ms/step
Epoch 544/1000
2023-09-29 11:19:01.580 
Epoch 544/1000 
	 loss: 28.5227, MinusLogProbMetric: 28.5227, val_loss: 28.7941, val_MinusLogProbMetric: 28.7941

Epoch 544: val_loss did not improve from 28.45726
196/196 - 50s - loss: 28.5227 - MinusLogProbMetric: 28.5227 - val_loss: 28.7941 - val_MinusLogProbMetric: 28.7941 - lr: 0.0010 - 50s/epoch - 254ms/step
Epoch 545/1000
2023-09-29 11:19:49.737 
Epoch 545/1000 
	 loss: 28.4616, MinusLogProbMetric: 28.4616, val_loss: 29.5440, val_MinusLogProbMetric: 29.5440

Epoch 545: val_loss did not improve from 28.45726
196/196 - 48s - loss: 28.4616 - MinusLogProbMetric: 28.4616 - val_loss: 29.5440 - val_MinusLogProbMetric: 29.5440 - lr: 0.0010 - 48s/epoch - 246ms/step
Epoch 546/1000
2023-09-29 11:20:38.291 
Epoch 546/1000 
	 loss: 28.6534, MinusLogProbMetric: 28.6534, val_loss: 28.6796, val_MinusLogProbMetric: 28.6796

Epoch 546: val_loss did not improve from 28.45726
196/196 - 49s - loss: 28.6534 - MinusLogProbMetric: 28.6534 - val_loss: 28.6796 - val_MinusLogProbMetric: 28.6796 - lr: 0.0010 - 49s/epoch - 248ms/step
Epoch 547/1000
2023-09-29 11:21:27.412 
Epoch 547/1000 
	 loss: 28.5107, MinusLogProbMetric: 28.5107, val_loss: 28.5755, val_MinusLogProbMetric: 28.5755

Epoch 547: val_loss did not improve from 28.45726
196/196 - 49s - loss: 28.5107 - MinusLogProbMetric: 28.5107 - val_loss: 28.5755 - val_MinusLogProbMetric: 28.5755 - lr: 0.0010 - 49s/epoch - 251ms/step
Epoch 548/1000
2023-09-29 11:22:15.449 
Epoch 548/1000 
	 loss: 28.5713, MinusLogProbMetric: 28.5713, val_loss: 28.7484, val_MinusLogProbMetric: 28.7484

Epoch 548: val_loss did not improve from 28.45726
196/196 - 48s - loss: 28.5713 - MinusLogProbMetric: 28.5713 - val_loss: 28.7484 - val_MinusLogProbMetric: 28.7484 - lr: 0.0010 - 48s/epoch - 245ms/step
Epoch 549/1000
2023-09-29 11:23:02.925 
Epoch 549/1000 
	 loss: 28.4984, MinusLogProbMetric: 28.4984, val_loss: 28.5399, val_MinusLogProbMetric: 28.5399

Epoch 549: val_loss did not improve from 28.45726
196/196 - 47s - loss: 28.4984 - MinusLogProbMetric: 28.4984 - val_loss: 28.5399 - val_MinusLogProbMetric: 28.5399 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 550/1000
2023-09-29 11:23:52.563 
Epoch 550/1000 
	 loss: 28.5660, MinusLogProbMetric: 28.5660, val_loss: 28.6560, val_MinusLogProbMetric: 28.6560

Epoch 550: val_loss did not improve from 28.45726
196/196 - 50s - loss: 28.5660 - MinusLogProbMetric: 28.5660 - val_loss: 28.6560 - val_MinusLogProbMetric: 28.6560 - lr: 0.0010 - 50s/epoch - 253ms/step
Epoch 551/1000
2023-09-29 11:24:39.498 
Epoch 551/1000 
	 loss: 28.5269, MinusLogProbMetric: 28.5269, val_loss: 28.8443, val_MinusLogProbMetric: 28.8443

Epoch 551: val_loss did not improve from 28.45726
196/196 - 47s - loss: 28.5269 - MinusLogProbMetric: 28.5269 - val_loss: 28.8443 - val_MinusLogProbMetric: 28.8443 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 552/1000
2023-09-29 11:25:26.844 
Epoch 552/1000 
	 loss: 28.5400, MinusLogProbMetric: 28.5400, val_loss: 28.8060, val_MinusLogProbMetric: 28.8060

Epoch 552: val_loss did not improve from 28.45726
196/196 - 47s - loss: 28.5400 - MinusLogProbMetric: 28.5400 - val_loss: 28.8060 - val_MinusLogProbMetric: 28.8060 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 553/1000
2023-09-29 11:26:16.500 
Epoch 553/1000 
	 loss: 28.6108, MinusLogProbMetric: 28.6108, val_loss: 29.2421, val_MinusLogProbMetric: 29.2421

Epoch 553: val_loss did not improve from 28.45726
196/196 - 50s - loss: 28.6108 - MinusLogProbMetric: 28.6108 - val_loss: 29.2421 - val_MinusLogProbMetric: 29.2421 - lr: 0.0010 - 50s/epoch - 253ms/step
Epoch 554/1000
2023-09-29 11:27:07.478 
Epoch 554/1000 
	 loss: 28.4687, MinusLogProbMetric: 28.4687, val_loss: 29.2875, val_MinusLogProbMetric: 29.2875

Epoch 554: val_loss did not improve from 28.45726
196/196 - 51s - loss: 28.4687 - MinusLogProbMetric: 28.4687 - val_loss: 29.2875 - val_MinusLogProbMetric: 29.2875 - lr: 0.0010 - 51s/epoch - 260ms/step
Epoch 555/1000
2023-09-29 11:27:58.027 
Epoch 555/1000 
	 loss: 28.6091, MinusLogProbMetric: 28.6091, val_loss: 28.7560, val_MinusLogProbMetric: 28.7560

Epoch 555: val_loss did not improve from 28.45726
196/196 - 51s - loss: 28.6091 - MinusLogProbMetric: 28.6091 - val_loss: 28.7560 - val_MinusLogProbMetric: 28.7560 - lr: 0.0010 - 51s/epoch - 258ms/step
Epoch 556/1000
2023-09-29 11:28:48.033 
Epoch 556/1000 
	 loss: 28.4870, MinusLogProbMetric: 28.4870, val_loss: 29.1039, val_MinusLogProbMetric: 29.1039

Epoch 556: val_loss did not improve from 28.45726
196/196 - 50s - loss: 28.4870 - MinusLogProbMetric: 28.4870 - val_loss: 29.1039 - val_MinusLogProbMetric: 29.1039 - lr: 0.0010 - 50s/epoch - 255ms/step
Epoch 557/1000
2023-09-29 11:29:37.430 
Epoch 557/1000 
	 loss: 28.5118, MinusLogProbMetric: 28.5118, val_loss: 28.5556, val_MinusLogProbMetric: 28.5556

Epoch 557: val_loss did not improve from 28.45726
196/196 - 49s - loss: 28.5118 - MinusLogProbMetric: 28.5118 - val_loss: 28.5556 - val_MinusLogProbMetric: 28.5556 - lr: 0.0010 - 49s/epoch - 252ms/step
Epoch 558/1000
2023-09-29 11:30:25.936 
Epoch 558/1000 
	 loss: 28.4958, MinusLogProbMetric: 28.4958, val_loss: 28.6896, val_MinusLogProbMetric: 28.6896

Epoch 558: val_loss did not improve from 28.45726
196/196 - 48s - loss: 28.4958 - MinusLogProbMetric: 28.4958 - val_loss: 28.6896 - val_MinusLogProbMetric: 28.6896 - lr: 0.0010 - 48s/epoch - 247ms/step
Epoch 559/1000
2023-09-29 11:31:14.999 
Epoch 559/1000 
	 loss: 28.5033, MinusLogProbMetric: 28.5033, val_loss: 28.8744, val_MinusLogProbMetric: 28.8744

Epoch 559: val_loss did not improve from 28.45726
196/196 - 49s - loss: 28.5033 - MinusLogProbMetric: 28.5033 - val_loss: 28.8744 - val_MinusLogProbMetric: 28.8744 - lr: 0.0010 - 49s/epoch - 250ms/step
Epoch 560/1000
2023-09-29 11:32:01.699 
Epoch 560/1000 
	 loss: 28.4829, MinusLogProbMetric: 28.4829, val_loss: 30.9222, val_MinusLogProbMetric: 30.9222

Epoch 560: val_loss did not improve from 28.45726
196/196 - 47s - loss: 28.4829 - MinusLogProbMetric: 28.4829 - val_loss: 30.9222 - val_MinusLogProbMetric: 30.9222 - lr: 0.0010 - 47s/epoch - 238ms/step
Epoch 561/1000
2023-09-29 11:32:49.296 
Epoch 561/1000 
	 loss: 28.6098, MinusLogProbMetric: 28.6098, val_loss: 28.7244, val_MinusLogProbMetric: 28.7244

Epoch 561: val_loss did not improve from 28.45726
196/196 - 48s - loss: 28.6098 - MinusLogProbMetric: 28.6098 - val_loss: 28.7244 - val_MinusLogProbMetric: 28.7244 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 562/1000
2023-09-29 11:33:35.808 
Epoch 562/1000 
	 loss: 28.5352, MinusLogProbMetric: 28.5352, val_loss: 28.6886, val_MinusLogProbMetric: 28.6886

Epoch 562: val_loss did not improve from 28.45726
196/196 - 47s - loss: 28.5352 - MinusLogProbMetric: 28.5352 - val_loss: 28.6886 - val_MinusLogProbMetric: 28.6886 - lr: 0.0010 - 47s/epoch - 237ms/step
Epoch 563/1000
2023-09-29 11:34:24.225 
Epoch 563/1000 
	 loss: 28.5059, MinusLogProbMetric: 28.5059, val_loss: 28.9108, val_MinusLogProbMetric: 28.9108

Epoch 563: val_loss did not improve from 28.45726
196/196 - 48s - loss: 28.5059 - MinusLogProbMetric: 28.5059 - val_loss: 28.9108 - val_MinusLogProbMetric: 28.9108 - lr: 0.0010 - 48s/epoch - 247ms/step
Epoch 564/1000
2023-09-29 11:35:11.642 
Epoch 564/1000 
	 loss: 28.4679, MinusLogProbMetric: 28.4679, val_loss: 28.8657, val_MinusLogProbMetric: 28.8657

Epoch 564: val_loss did not improve from 28.45726
196/196 - 47s - loss: 28.4679 - MinusLogProbMetric: 28.4679 - val_loss: 28.8657 - val_MinusLogProbMetric: 28.8657 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 565/1000
2023-09-29 11:35:59.000 
Epoch 565/1000 
	 loss: 28.5390, MinusLogProbMetric: 28.5390, val_loss: 29.8769, val_MinusLogProbMetric: 29.8769

Epoch 565: val_loss did not improve from 28.45726
196/196 - 47s - loss: 28.5390 - MinusLogProbMetric: 28.5390 - val_loss: 29.8769 - val_MinusLogProbMetric: 29.8769 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 566/1000
2023-09-29 11:36:49.551 
Epoch 566/1000 
	 loss: 28.5650, MinusLogProbMetric: 28.5650, val_loss: 32.5671, val_MinusLogProbMetric: 32.5671

Epoch 566: val_loss did not improve from 28.45726
196/196 - 51s - loss: 28.5650 - MinusLogProbMetric: 28.5650 - val_loss: 32.5671 - val_MinusLogProbMetric: 32.5671 - lr: 0.0010 - 51s/epoch - 258ms/step
Epoch 567/1000
2023-09-29 11:37:40.144 
Epoch 567/1000 
	 loss: 28.6930, MinusLogProbMetric: 28.6930, val_loss: 28.9908, val_MinusLogProbMetric: 28.9908

Epoch 567: val_loss did not improve from 28.45726
196/196 - 51s - loss: 28.6930 - MinusLogProbMetric: 28.6930 - val_loss: 28.9908 - val_MinusLogProbMetric: 28.9908 - lr: 0.0010 - 51s/epoch - 258ms/step
Epoch 568/1000
2023-09-29 11:38:28.743 
Epoch 568/1000 
	 loss: 28.4766, MinusLogProbMetric: 28.4766, val_loss: 28.5625, val_MinusLogProbMetric: 28.5625

Epoch 568: val_loss did not improve from 28.45726
196/196 - 49s - loss: 28.4766 - MinusLogProbMetric: 28.4766 - val_loss: 28.5625 - val_MinusLogProbMetric: 28.5625 - lr: 0.0010 - 49s/epoch - 248ms/step
Epoch 569/1000
2023-09-29 11:39:18.120 
Epoch 569/1000 
	 loss: 28.5833, MinusLogProbMetric: 28.5833, val_loss: 28.9656, val_MinusLogProbMetric: 28.9656

Epoch 569: val_loss did not improve from 28.45726
196/196 - 49s - loss: 28.5833 - MinusLogProbMetric: 28.5833 - val_loss: 28.9656 - val_MinusLogProbMetric: 28.9656 - lr: 0.0010 - 49s/epoch - 252ms/step
Epoch 570/1000
2023-09-29 11:40:05.567 
Epoch 570/1000 
	 loss: 28.4383, MinusLogProbMetric: 28.4383, val_loss: 28.5965, val_MinusLogProbMetric: 28.5965

Epoch 570: val_loss did not improve from 28.45726
196/196 - 47s - loss: 28.4383 - MinusLogProbMetric: 28.4383 - val_loss: 28.5965 - val_MinusLogProbMetric: 28.5965 - lr: 0.0010 - 47s/epoch - 242ms/step
Epoch 571/1000
2023-09-29 11:40:53.486 
Epoch 571/1000 
	 loss: 28.4971, MinusLogProbMetric: 28.4971, val_loss: 28.7927, val_MinusLogProbMetric: 28.7927

Epoch 571: val_loss did not improve from 28.45726
196/196 - 48s - loss: 28.4971 - MinusLogProbMetric: 28.4971 - val_loss: 28.7927 - val_MinusLogProbMetric: 28.7927 - lr: 0.0010 - 48s/epoch - 244ms/step
Epoch 572/1000
2023-09-29 11:41:42.636 
Epoch 572/1000 
	 loss: 28.6017, MinusLogProbMetric: 28.6017, val_loss: 30.4834, val_MinusLogProbMetric: 30.4834

Epoch 572: val_loss did not improve from 28.45726
196/196 - 49s - loss: 28.6017 - MinusLogProbMetric: 28.6017 - val_loss: 30.4834 - val_MinusLogProbMetric: 30.4834 - lr: 0.0010 - 49s/epoch - 251ms/step
Epoch 573/1000
2023-09-29 11:42:33.624 
Epoch 573/1000 
	 loss: 28.5375, MinusLogProbMetric: 28.5375, val_loss: 28.6787, val_MinusLogProbMetric: 28.6787

Epoch 573: val_loss did not improve from 28.45726
196/196 - 51s - loss: 28.5375 - MinusLogProbMetric: 28.5375 - val_loss: 28.6787 - val_MinusLogProbMetric: 28.6787 - lr: 0.0010 - 51s/epoch - 260ms/step
Epoch 574/1000
2023-09-29 11:43:24.313 
Epoch 574/1000 
	 loss: 28.5083, MinusLogProbMetric: 28.5083, val_loss: 28.9869, val_MinusLogProbMetric: 28.9869

Epoch 574: val_loss did not improve from 28.45726
196/196 - 51s - loss: 28.5083 - MinusLogProbMetric: 28.5083 - val_loss: 28.9869 - val_MinusLogProbMetric: 28.9869 - lr: 0.0010 - 51s/epoch - 259ms/step
Epoch 575/1000
2023-09-29 11:44:14.596 
Epoch 575/1000 
	 loss: 28.6024, MinusLogProbMetric: 28.6024, val_loss: 28.6053, val_MinusLogProbMetric: 28.6053

Epoch 575: val_loss did not improve from 28.45726
196/196 - 50s - loss: 28.6024 - MinusLogProbMetric: 28.6024 - val_loss: 28.6053 - val_MinusLogProbMetric: 28.6053 - lr: 0.0010 - 50s/epoch - 256ms/step
Epoch 576/1000
2023-09-29 11:45:03.127 
Epoch 576/1000 
	 loss: 28.5423, MinusLogProbMetric: 28.5423, val_loss: 28.9742, val_MinusLogProbMetric: 28.9742

Epoch 576: val_loss did not improve from 28.45726
196/196 - 49s - loss: 28.5423 - MinusLogProbMetric: 28.5423 - val_loss: 28.9742 - val_MinusLogProbMetric: 28.9742 - lr: 0.0010 - 49s/epoch - 248ms/step
Epoch 577/1000
2023-09-29 11:45:49.152 
Epoch 577/1000 
	 loss: 28.5231, MinusLogProbMetric: 28.5231, val_loss: 29.0344, val_MinusLogProbMetric: 29.0344

Epoch 577: val_loss did not improve from 28.45726
196/196 - 46s - loss: 28.5231 - MinusLogProbMetric: 28.5231 - val_loss: 29.0344 - val_MinusLogProbMetric: 29.0344 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 578/1000
2023-09-29 11:46:38.193 
Epoch 578/1000 
	 loss: 28.5129, MinusLogProbMetric: 28.5129, val_loss: 28.8865, val_MinusLogProbMetric: 28.8865

Epoch 578: val_loss did not improve from 28.45726
196/196 - 49s - loss: 28.5129 - MinusLogProbMetric: 28.5129 - val_loss: 28.8865 - val_MinusLogProbMetric: 28.8865 - lr: 0.0010 - 49s/epoch - 250ms/step
Epoch 579/1000
2023-09-29 11:47:25.048 
Epoch 579/1000 
	 loss: 28.4913, MinusLogProbMetric: 28.4913, val_loss: 29.9887, val_MinusLogProbMetric: 29.9887

Epoch 579: val_loss did not improve from 28.45726
196/196 - 47s - loss: 28.4913 - MinusLogProbMetric: 28.4913 - val_loss: 29.9887 - val_MinusLogProbMetric: 29.9887 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 580/1000
2023-09-29 11:48:14.418 
Epoch 580/1000 
	 loss: 28.7277, MinusLogProbMetric: 28.7277, val_loss: 29.1369, val_MinusLogProbMetric: 29.1369

Epoch 580: val_loss did not improve from 28.45726
196/196 - 49s - loss: 28.7277 - MinusLogProbMetric: 28.7277 - val_loss: 29.1369 - val_MinusLogProbMetric: 29.1369 - lr: 0.0010 - 49s/epoch - 252ms/step
Epoch 581/1000
2023-09-29 11:49:03.210 
Epoch 581/1000 
	 loss: 28.5366, MinusLogProbMetric: 28.5366, val_loss: 28.4853, val_MinusLogProbMetric: 28.4853

Epoch 581: val_loss did not improve from 28.45726
196/196 - 49s - loss: 28.5366 - MinusLogProbMetric: 28.5366 - val_loss: 28.4853 - val_MinusLogProbMetric: 28.4853 - lr: 0.0010 - 49s/epoch - 249ms/step
Epoch 582/1000
2023-09-29 11:49:49.341 
Epoch 582/1000 
	 loss: 28.4873, MinusLogProbMetric: 28.4873, val_loss: 29.5565, val_MinusLogProbMetric: 29.5565

Epoch 582: val_loss did not improve from 28.45726
196/196 - 46s - loss: 28.4873 - MinusLogProbMetric: 28.4873 - val_loss: 29.5565 - val_MinusLogProbMetric: 29.5565 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 583/1000
2023-09-29 11:50:35.819 
Epoch 583/1000 
	 loss: 28.7136, MinusLogProbMetric: 28.7136, val_loss: 28.8533, val_MinusLogProbMetric: 28.8533

Epoch 583: val_loss did not improve from 28.45726
196/196 - 46s - loss: 28.7136 - MinusLogProbMetric: 28.7136 - val_loss: 28.8533 - val_MinusLogProbMetric: 28.8533 - lr: 0.0010 - 46s/epoch - 237ms/step
Epoch 584/1000
2023-09-29 11:51:22.264 
Epoch 584/1000 
	 loss: 28.5187, MinusLogProbMetric: 28.5187, val_loss: 28.7886, val_MinusLogProbMetric: 28.7886

Epoch 584: val_loss did not improve from 28.45726
196/196 - 46s - loss: 28.5187 - MinusLogProbMetric: 28.5187 - val_loss: 28.7886 - val_MinusLogProbMetric: 28.7886 - lr: 0.0010 - 46s/epoch - 237ms/step
Epoch 585/1000
2023-09-29 11:52:10.225 
Epoch 585/1000 
	 loss: 28.5043, MinusLogProbMetric: 28.5043, val_loss: 28.9268, val_MinusLogProbMetric: 28.9268

Epoch 585: val_loss did not improve from 28.45726
196/196 - 48s - loss: 28.5043 - MinusLogProbMetric: 28.5043 - val_loss: 28.9268 - val_MinusLogProbMetric: 28.9268 - lr: 0.0010 - 48s/epoch - 245ms/step
Epoch 586/1000
2023-09-29 11:52:56.744 
Epoch 586/1000 
	 loss: 28.5512, MinusLogProbMetric: 28.5512, val_loss: 28.6442, val_MinusLogProbMetric: 28.6442

Epoch 586: val_loss did not improve from 28.45726
196/196 - 47s - loss: 28.5512 - MinusLogProbMetric: 28.5512 - val_loss: 28.6442 - val_MinusLogProbMetric: 28.6442 - lr: 0.0010 - 47s/epoch - 237ms/step
Epoch 587/1000
2023-09-29 11:53:42.842 
Epoch 587/1000 
	 loss: 28.4748, MinusLogProbMetric: 28.4748, val_loss: 28.8461, val_MinusLogProbMetric: 28.8461

Epoch 587: val_loss did not improve from 28.45726
196/196 - 46s - loss: 28.4748 - MinusLogProbMetric: 28.4748 - val_loss: 28.8461 - val_MinusLogProbMetric: 28.8461 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 588/1000
2023-09-29 11:54:30.593 
Epoch 588/1000 
	 loss: 28.4949, MinusLogProbMetric: 28.4949, val_loss: 29.1575, val_MinusLogProbMetric: 29.1575

Epoch 588: val_loss did not improve from 28.45726
196/196 - 48s - loss: 28.4949 - MinusLogProbMetric: 28.4949 - val_loss: 29.1575 - val_MinusLogProbMetric: 29.1575 - lr: 0.0010 - 48s/epoch - 244ms/step
Epoch 589/1000
2023-09-29 11:55:17.782 
Epoch 589/1000 
	 loss: 28.0184, MinusLogProbMetric: 28.0184, val_loss: 28.3350, val_MinusLogProbMetric: 28.3350

Epoch 589: val_loss improved from 28.45726 to 28.33497, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 48s - loss: 28.0184 - MinusLogProbMetric: 28.0184 - val_loss: 28.3350 - val_MinusLogProbMetric: 28.3350 - lr: 5.0000e-04 - 48s/epoch - 245ms/step
Epoch 590/1000
2023-09-29 11:56:05.243 
Epoch 590/1000 
	 loss: 27.9854, MinusLogProbMetric: 27.9854, val_loss: 28.2960, val_MinusLogProbMetric: 28.2960

Epoch 590: val_loss improved from 28.33497 to 28.29595, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 47s - loss: 27.9854 - MinusLogProbMetric: 27.9854 - val_loss: 28.2960 - val_MinusLogProbMetric: 28.2960 - lr: 5.0000e-04 - 47s/epoch - 242ms/step
Epoch 591/1000
2023-09-29 11:56:53.568 
Epoch 591/1000 
	 loss: 28.0791, MinusLogProbMetric: 28.0791, val_loss: 28.2286, val_MinusLogProbMetric: 28.2286

Epoch 591: val_loss improved from 28.29595 to 28.22859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 48s - loss: 28.0791 - MinusLogProbMetric: 28.0791 - val_loss: 28.2286 - val_MinusLogProbMetric: 28.2286 - lr: 5.0000e-04 - 48s/epoch - 246ms/step
Epoch 592/1000
2023-09-29 11:57:41.603 
Epoch 592/1000 
	 loss: 28.0152, MinusLogProbMetric: 28.0152, val_loss: 28.2912, val_MinusLogProbMetric: 28.2912

Epoch 592: val_loss did not improve from 28.22859
196/196 - 47s - loss: 28.0152 - MinusLogProbMetric: 28.0152 - val_loss: 28.2912 - val_MinusLogProbMetric: 28.2912 - lr: 5.0000e-04 - 47s/epoch - 241ms/step
Epoch 593/1000
2023-09-29 11:58:29.969 
Epoch 593/1000 
	 loss: 27.9750, MinusLogProbMetric: 27.9750, val_loss: 28.5130, val_MinusLogProbMetric: 28.5130

Epoch 593: val_loss did not improve from 28.22859
196/196 - 48s - loss: 27.9750 - MinusLogProbMetric: 27.9750 - val_loss: 28.5130 - val_MinusLogProbMetric: 28.5130 - lr: 5.0000e-04 - 48s/epoch - 247ms/step
Epoch 594/1000
2023-09-29 11:59:20.274 
Epoch 594/1000 
	 loss: 28.0127, MinusLogProbMetric: 28.0127, val_loss: 28.3080, val_MinusLogProbMetric: 28.3080

Epoch 594: val_loss did not improve from 28.22859
196/196 - 50s - loss: 28.0127 - MinusLogProbMetric: 28.0127 - val_loss: 28.3080 - val_MinusLogProbMetric: 28.3080 - lr: 5.0000e-04 - 50s/epoch - 257ms/step
Epoch 595/1000
2023-09-29 12:00:10.949 
Epoch 595/1000 
	 loss: 28.0196, MinusLogProbMetric: 28.0196, val_loss: 28.2770, val_MinusLogProbMetric: 28.2770

Epoch 595: val_loss did not improve from 28.22859
196/196 - 51s - loss: 28.0196 - MinusLogProbMetric: 28.0196 - val_loss: 28.2770 - val_MinusLogProbMetric: 28.2770 - lr: 5.0000e-04 - 51s/epoch - 259ms/step
Epoch 596/1000
2023-09-29 12:00:58.229 
Epoch 596/1000 
	 loss: 27.9859, MinusLogProbMetric: 27.9859, val_loss: 28.4329, val_MinusLogProbMetric: 28.4329

Epoch 596: val_loss did not improve from 28.22859
196/196 - 47s - loss: 27.9859 - MinusLogProbMetric: 27.9859 - val_loss: 28.4329 - val_MinusLogProbMetric: 28.4329 - lr: 5.0000e-04 - 47s/epoch - 241ms/step
Epoch 597/1000
2023-09-29 12:01:46.164 
Epoch 597/1000 
	 loss: 28.0430, MinusLogProbMetric: 28.0430, val_loss: 28.1985, val_MinusLogProbMetric: 28.1985

Epoch 597: val_loss improved from 28.22859 to 28.19850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 49s - loss: 28.0430 - MinusLogProbMetric: 28.0430 - val_loss: 28.1985 - val_MinusLogProbMetric: 28.1985 - lr: 5.0000e-04 - 49s/epoch - 248ms/step
Epoch 598/1000
2023-09-29 12:02:34.968 
Epoch 598/1000 
	 loss: 28.1828, MinusLogProbMetric: 28.1828, val_loss: 28.3994, val_MinusLogProbMetric: 28.3994

Epoch 598: val_loss did not improve from 28.19850
196/196 - 48s - loss: 28.1828 - MinusLogProbMetric: 28.1828 - val_loss: 28.3994 - val_MinusLogProbMetric: 28.3994 - lr: 5.0000e-04 - 48s/epoch - 246ms/step
Epoch 599/1000
2023-09-29 12:03:21.960 
Epoch 599/1000 
	 loss: 28.0021, MinusLogProbMetric: 28.0021, val_loss: 28.1929, val_MinusLogProbMetric: 28.1929

Epoch 599: val_loss improved from 28.19850 to 28.19289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 48s - loss: 28.0021 - MinusLogProbMetric: 28.0021 - val_loss: 28.1929 - val_MinusLogProbMetric: 28.1929 - lr: 5.0000e-04 - 48s/epoch - 243ms/step
Epoch 600/1000
2023-09-29 12:04:09.599 
Epoch 600/1000 
	 loss: 28.0256, MinusLogProbMetric: 28.0256, val_loss: 28.2358, val_MinusLogProbMetric: 28.2358

Epoch 600: val_loss did not improve from 28.19289
196/196 - 47s - loss: 28.0256 - MinusLogProbMetric: 28.0256 - val_loss: 28.2358 - val_MinusLogProbMetric: 28.2358 - lr: 5.0000e-04 - 47s/epoch - 240ms/step
Epoch 601/1000
2023-09-29 12:04:56.503 
Epoch 601/1000 
	 loss: 28.0650, MinusLogProbMetric: 28.0650, val_loss: 28.7802, val_MinusLogProbMetric: 28.7802

Epoch 601: val_loss did not improve from 28.19289
196/196 - 47s - loss: 28.0650 - MinusLogProbMetric: 28.0650 - val_loss: 28.7802 - val_MinusLogProbMetric: 28.7802 - lr: 5.0000e-04 - 47s/epoch - 239ms/step
Epoch 602/1000
2023-09-29 12:05:45.101 
Epoch 602/1000 
	 loss: 28.0581, MinusLogProbMetric: 28.0581, val_loss: 28.2656, val_MinusLogProbMetric: 28.2656

Epoch 602: val_loss did not improve from 28.19289
196/196 - 49s - loss: 28.0581 - MinusLogProbMetric: 28.0581 - val_loss: 28.2656 - val_MinusLogProbMetric: 28.2656 - lr: 5.0000e-04 - 49s/epoch - 248ms/step
Epoch 603/1000
2023-09-29 12:06:33.177 
Epoch 603/1000 
	 loss: 27.9678, MinusLogProbMetric: 27.9678, val_loss: 28.3953, val_MinusLogProbMetric: 28.3953

Epoch 603: val_loss did not improve from 28.19289
196/196 - 48s - loss: 27.9678 - MinusLogProbMetric: 27.9678 - val_loss: 28.3953 - val_MinusLogProbMetric: 28.3953 - lr: 5.0000e-04 - 48s/epoch - 245ms/step
Epoch 604/1000
2023-09-29 12:07:20.766 
Epoch 604/1000 
	 loss: 28.0184, MinusLogProbMetric: 28.0184, val_loss: 28.2551, val_MinusLogProbMetric: 28.2551

Epoch 604: val_loss did not improve from 28.19289
196/196 - 48s - loss: 28.0184 - MinusLogProbMetric: 28.0184 - val_loss: 28.2551 - val_MinusLogProbMetric: 28.2551 - lr: 5.0000e-04 - 48s/epoch - 243ms/step
Epoch 605/1000
2023-09-29 12:08:07.275 
Epoch 605/1000 
	 loss: 28.0055, MinusLogProbMetric: 28.0055, val_loss: 28.1464, val_MinusLogProbMetric: 28.1464

Epoch 605: val_loss improved from 28.19289 to 28.14640, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 47s - loss: 28.0055 - MinusLogProbMetric: 28.0055 - val_loss: 28.1464 - val_MinusLogProbMetric: 28.1464 - lr: 5.0000e-04 - 47s/epoch - 241ms/step
Epoch 606/1000
2023-09-29 12:08:54.391 
Epoch 606/1000 
	 loss: 28.0155, MinusLogProbMetric: 28.0155, val_loss: 28.1542, val_MinusLogProbMetric: 28.1542

Epoch 606: val_loss did not improve from 28.14640
196/196 - 46s - loss: 28.0155 - MinusLogProbMetric: 28.0155 - val_loss: 28.1542 - val_MinusLogProbMetric: 28.1542 - lr: 5.0000e-04 - 46s/epoch - 237ms/step
Epoch 607/1000
2023-09-29 12:09:42.972 
Epoch 607/1000 
	 loss: 28.0098, MinusLogProbMetric: 28.0098, val_loss: 28.4738, val_MinusLogProbMetric: 28.4738

Epoch 607: val_loss did not improve from 28.14640
196/196 - 49s - loss: 28.0098 - MinusLogProbMetric: 28.0098 - val_loss: 28.4738 - val_MinusLogProbMetric: 28.4738 - lr: 5.0000e-04 - 49s/epoch - 248ms/step
Epoch 608/1000
2023-09-29 12:10:29.769 
Epoch 608/1000 
	 loss: 28.0114, MinusLogProbMetric: 28.0114, val_loss: 28.2833, val_MinusLogProbMetric: 28.2833

Epoch 608: val_loss did not improve from 28.14640
196/196 - 47s - loss: 28.0114 - MinusLogProbMetric: 28.0114 - val_loss: 28.2833 - val_MinusLogProbMetric: 28.2833 - lr: 5.0000e-04 - 47s/epoch - 239ms/step
Epoch 609/1000
2023-09-29 12:11:18.533 
Epoch 609/1000 
	 loss: 27.9927, MinusLogProbMetric: 27.9927, val_loss: 28.2303, val_MinusLogProbMetric: 28.2303

Epoch 609: val_loss did not improve from 28.14640
196/196 - 49s - loss: 27.9927 - MinusLogProbMetric: 27.9927 - val_loss: 28.2303 - val_MinusLogProbMetric: 28.2303 - lr: 5.0000e-04 - 49s/epoch - 249ms/step
Epoch 610/1000
2023-09-29 12:12:06.985 
Epoch 610/1000 
	 loss: 27.9865, MinusLogProbMetric: 27.9865, val_loss: 28.3294, val_MinusLogProbMetric: 28.3294

Epoch 610: val_loss did not improve from 28.14640
196/196 - 48s - loss: 27.9865 - MinusLogProbMetric: 27.9865 - val_loss: 28.3294 - val_MinusLogProbMetric: 28.3294 - lr: 5.0000e-04 - 48s/epoch - 247ms/step
Epoch 611/1000
2023-09-29 12:12:54.162 
Epoch 611/1000 
	 loss: 28.0509, MinusLogProbMetric: 28.0509, val_loss: 28.1862, val_MinusLogProbMetric: 28.1862

Epoch 611: val_loss did not improve from 28.14640
196/196 - 47s - loss: 28.0509 - MinusLogProbMetric: 28.0509 - val_loss: 28.1862 - val_MinusLogProbMetric: 28.1862 - lr: 5.0000e-04 - 47s/epoch - 241ms/step
Epoch 612/1000
2023-09-29 12:13:41.889 
Epoch 612/1000 
	 loss: 27.9905, MinusLogProbMetric: 27.9905, val_loss: 28.3611, val_MinusLogProbMetric: 28.3611

Epoch 612: val_loss did not improve from 28.14640
196/196 - 48s - loss: 27.9905 - MinusLogProbMetric: 27.9905 - val_loss: 28.3611 - val_MinusLogProbMetric: 28.3611 - lr: 5.0000e-04 - 48s/epoch - 243ms/step
Epoch 613/1000
2023-09-29 12:14:29.081 
Epoch 613/1000 
	 loss: 28.0176, MinusLogProbMetric: 28.0176, val_loss: 28.1623, val_MinusLogProbMetric: 28.1623

Epoch 613: val_loss did not improve from 28.14640
196/196 - 47s - loss: 28.0176 - MinusLogProbMetric: 28.0176 - val_loss: 28.1623 - val_MinusLogProbMetric: 28.1623 - lr: 5.0000e-04 - 47s/epoch - 241ms/step
Epoch 614/1000
2023-09-29 12:15:15.130 
Epoch 614/1000 
	 loss: 27.9811, MinusLogProbMetric: 27.9811, val_loss: 28.3416, val_MinusLogProbMetric: 28.3416

Epoch 614: val_loss did not improve from 28.14640
196/196 - 46s - loss: 27.9811 - MinusLogProbMetric: 27.9811 - val_loss: 28.3416 - val_MinusLogProbMetric: 28.3416 - lr: 5.0000e-04 - 46s/epoch - 235ms/step
Epoch 615/1000
2023-09-29 12:16:02.052 
Epoch 615/1000 
	 loss: 27.9984, MinusLogProbMetric: 27.9984, val_loss: 28.2657, val_MinusLogProbMetric: 28.2657

Epoch 615: val_loss did not improve from 28.14640
196/196 - 47s - loss: 27.9984 - MinusLogProbMetric: 27.9984 - val_loss: 28.2657 - val_MinusLogProbMetric: 28.2657 - lr: 5.0000e-04 - 47s/epoch - 239ms/step
Epoch 616/1000
2023-09-29 12:16:46.131 
Epoch 616/1000 
	 loss: 28.0293, MinusLogProbMetric: 28.0293, val_loss: 28.2073, val_MinusLogProbMetric: 28.2073

Epoch 616: val_loss did not improve from 28.14640
196/196 - 44s - loss: 28.0293 - MinusLogProbMetric: 28.0293 - val_loss: 28.2073 - val_MinusLogProbMetric: 28.2073 - lr: 5.0000e-04 - 44s/epoch - 225ms/step
Epoch 617/1000
2023-09-29 12:17:32.039 
Epoch 617/1000 
	 loss: 28.0944, MinusLogProbMetric: 28.0944, val_loss: 28.3277, val_MinusLogProbMetric: 28.3277

Epoch 617: val_loss did not improve from 28.14640
196/196 - 46s - loss: 28.0944 - MinusLogProbMetric: 28.0944 - val_loss: 28.3277 - val_MinusLogProbMetric: 28.3277 - lr: 5.0000e-04 - 46s/epoch - 234ms/step
Epoch 618/1000
2023-09-29 12:18:18.500 
Epoch 618/1000 
	 loss: 28.0193, MinusLogProbMetric: 28.0193, val_loss: 28.1610, val_MinusLogProbMetric: 28.1610

Epoch 618: val_loss did not improve from 28.14640
196/196 - 46s - loss: 28.0193 - MinusLogProbMetric: 28.0193 - val_loss: 28.1610 - val_MinusLogProbMetric: 28.1610 - lr: 5.0000e-04 - 46s/epoch - 237ms/step
Epoch 619/1000
2023-09-29 12:19:06.678 
Epoch 619/1000 
	 loss: 27.9994, MinusLogProbMetric: 27.9994, val_loss: 28.3293, val_MinusLogProbMetric: 28.3293

Epoch 619: val_loss did not improve from 28.14640
196/196 - 48s - loss: 27.9994 - MinusLogProbMetric: 27.9994 - val_loss: 28.3293 - val_MinusLogProbMetric: 28.3293 - lr: 5.0000e-04 - 48s/epoch - 246ms/step
Epoch 620/1000
2023-09-29 12:19:52.123 
Epoch 620/1000 
	 loss: 28.0131, MinusLogProbMetric: 28.0131, val_loss: 28.3850, val_MinusLogProbMetric: 28.3850

Epoch 620: val_loss did not improve from 28.14640
196/196 - 45s - loss: 28.0131 - MinusLogProbMetric: 28.0131 - val_loss: 28.3850 - val_MinusLogProbMetric: 28.3850 - lr: 5.0000e-04 - 45s/epoch - 232ms/step
Epoch 621/1000
2023-09-29 12:20:39.844 
Epoch 621/1000 
	 loss: 28.0614, MinusLogProbMetric: 28.0614, val_loss: 28.3686, val_MinusLogProbMetric: 28.3686

Epoch 621: val_loss did not improve from 28.14640
196/196 - 48s - loss: 28.0614 - MinusLogProbMetric: 28.0614 - val_loss: 28.3686 - val_MinusLogProbMetric: 28.3686 - lr: 5.0000e-04 - 48s/epoch - 243ms/step
Epoch 622/1000
2023-09-29 12:21:27.059 
Epoch 622/1000 
	 loss: 28.0364, MinusLogProbMetric: 28.0364, val_loss: 28.2487, val_MinusLogProbMetric: 28.2487

Epoch 622: val_loss did not improve from 28.14640
196/196 - 47s - loss: 28.0364 - MinusLogProbMetric: 28.0364 - val_loss: 28.2487 - val_MinusLogProbMetric: 28.2487 - lr: 5.0000e-04 - 47s/epoch - 241ms/step
Epoch 623/1000
2023-09-29 12:22:15.841 
Epoch 623/1000 
	 loss: 27.9975, MinusLogProbMetric: 27.9975, val_loss: 28.2556, val_MinusLogProbMetric: 28.2556

Epoch 623: val_loss did not improve from 28.14640
196/196 - 49s - loss: 27.9975 - MinusLogProbMetric: 27.9975 - val_loss: 28.2556 - val_MinusLogProbMetric: 28.2556 - lr: 5.0000e-04 - 49s/epoch - 249ms/step
Epoch 624/1000
2023-09-29 12:23:04.392 
Epoch 624/1000 
	 loss: 28.0035, MinusLogProbMetric: 28.0035, val_loss: 28.2207, val_MinusLogProbMetric: 28.2207

Epoch 624: val_loss did not improve from 28.14640
196/196 - 49s - loss: 28.0035 - MinusLogProbMetric: 28.0035 - val_loss: 28.2207 - val_MinusLogProbMetric: 28.2207 - lr: 5.0000e-04 - 49s/epoch - 248ms/step
Epoch 625/1000
2023-09-29 12:23:53.676 
Epoch 625/1000 
	 loss: 28.0281, MinusLogProbMetric: 28.0281, val_loss: 28.4570, val_MinusLogProbMetric: 28.4570

Epoch 625: val_loss did not improve from 28.14640
196/196 - 49s - loss: 28.0281 - MinusLogProbMetric: 28.0281 - val_loss: 28.4570 - val_MinusLogProbMetric: 28.4570 - lr: 5.0000e-04 - 49s/epoch - 251ms/step
Epoch 626/1000
2023-09-29 12:24:48.486 
Epoch 626/1000 
	 loss: 28.0344, MinusLogProbMetric: 28.0344, val_loss: 28.2374, val_MinusLogProbMetric: 28.2374

Epoch 626: val_loss did not improve from 28.14640
196/196 - 55s - loss: 28.0344 - MinusLogProbMetric: 28.0344 - val_loss: 28.2374 - val_MinusLogProbMetric: 28.2374 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 627/1000
2023-09-29 12:25:43.320 
Epoch 627/1000 
	 loss: 28.0202, MinusLogProbMetric: 28.0202, val_loss: 28.2690, val_MinusLogProbMetric: 28.2690

Epoch 627: val_loss did not improve from 28.14640
196/196 - 55s - loss: 28.0202 - MinusLogProbMetric: 28.0202 - val_loss: 28.2690 - val_MinusLogProbMetric: 28.2690 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 628/1000
2023-09-29 12:26:38.296 
Epoch 628/1000 
	 loss: 27.9723, MinusLogProbMetric: 27.9723, val_loss: 28.1648, val_MinusLogProbMetric: 28.1648

Epoch 628: val_loss did not improve from 28.14640
196/196 - 55s - loss: 27.9723 - MinusLogProbMetric: 27.9723 - val_loss: 28.1648 - val_MinusLogProbMetric: 28.1648 - lr: 5.0000e-04 - 55s/epoch - 280ms/step
Epoch 629/1000
2023-09-29 12:27:33.403 
Epoch 629/1000 
	 loss: 27.9951, MinusLogProbMetric: 27.9951, val_loss: 28.1641, val_MinusLogProbMetric: 28.1641

Epoch 629: val_loss did not improve from 28.14640
196/196 - 55s - loss: 27.9951 - MinusLogProbMetric: 27.9951 - val_loss: 28.1641 - val_MinusLogProbMetric: 28.1641 - lr: 5.0000e-04 - 55s/epoch - 281ms/step
Epoch 630/1000
2023-09-29 12:28:29.053 
Epoch 630/1000 
	 loss: 28.0110, MinusLogProbMetric: 28.0110, val_loss: 28.4506, val_MinusLogProbMetric: 28.4506

Epoch 630: val_loss did not improve from 28.14640
196/196 - 56s - loss: 28.0110 - MinusLogProbMetric: 28.0110 - val_loss: 28.4506 - val_MinusLogProbMetric: 28.4506 - lr: 5.0000e-04 - 56s/epoch - 284ms/step
Epoch 631/1000
2023-09-29 12:29:24.348 
Epoch 631/1000 
	 loss: 28.0402, MinusLogProbMetric: 28.0402, val_loss: 28.1590, val_MinusLogProbMetric: 28.1590

Epoch 631: val_loss did not improve from 28.14640
196/196 - 55s - loss: 28.0402 - MinusLogProbMetric: 28.0402 - val_loss: 28.1590 - val_MinusLogProbMetric: 28.1590 - lr: 5.0000e-04 - 55s/epoch - 282ms/step
Epoch 632/1000
2023-09-29 12:30:19.618 
Epoch 632/1000 
	 loss: 27.9898, MinusLogProbMetric: 27.9898, val_loss: 28.3062, val_MinusLogProbMetric: 28.3062

Epoch 632: val_loss did not improve from 28.14640
196/196 - 55s - loss: 27.9898 - MinusLogProbMetric: 27.9898 - val_loss: 28.3062 - val_MinusLogProbMetric: 28.3062 - lr: 5.0000e-04 - 55s/epoch - 282ms/step
Epoch 633/1000
2023-09-29 12:31:14.128 
Epoch 633/1000 
	 loss: 28.0007, MinusLogProbMetric: 28.0007, val_loss: 28.2818, val_MinusLogProbMetric: 28.2818

Epoch 633: val_loss did not improve from 28.14640
196/196 - 55s - loss: 28.0007 - MinusLogProbMetric: 28.0007 - val_loss: 28.2818 - val_MinusLogProbMetric: 28.2818 - lr: 5.0000e-04 - 55s/epoch - 278ms/step
Epoch 634/1000
2023-09-29 12:32:06.664 
Epoch 634/1000 
	 loss: 28.0632, MinusLogProbMetric: 28.0632, val_loss: 28.2755, val_MinusLogProbMetric: 28.2755

Epoch 634: val_loss did not improve from 28.14640
196/196 - 53s - loss: 28.0632 - MinusLogProbMetric: 28.0632 - val_loss: 28.2755 - val_MinusLogProbMetric: 28.2755 - lr: 5.0000e-04 - 53s/epoch - 268ms/step
Epoch 635/1000
2023-09-29 12:32:59.090 
Epoch 635/1000 
	 loss: 28.1422, MinusLogProbMetric: 28.1422, val_loss: 28.3731, val_MinusLogProbMetric: 28.3731

Epoch 635: val_loss did not improve from 28.14640
196/196 - 52s - loss: 28.1422 - MinusLogProbMetric: 28.1422 - val_loss: 28.3731 - val_MinusLogProbMetric: 28.3731 - lr: 5.0000e-04 - 52s/epoch - 267ms/step
Epoch 636/1000
2023-09-29 12:33:49.117 
Epoch 636/1000 
	 loss: 27.9476, MinusLogProbMetric: 27.9476, val_loss: 28.4612, val_MinusLogProbMetric: 28.4612

Epoch 636: val_loss did not improve from 28.14640
196/196 - 50s - loss: 27.9476 - MinusLogProbMetric: 27.9476 - val_loss: 28.4612 - val_MinusLogProbMetric: 28.4612 - lr: 5.0000e-04 - 50s/epoch - 255ms/step
Epoch 637/1000
2023-09-29 12:34:41.260 
Epoch 637/1000 
	 loss: 27.9750, MinusLogProbMetric: 27.9750, val_loss: 28.1368, val_MinusLogProbMetric: 28.1368

Epoch 637: val_loss improved from 28.14640 to 28.13677, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 53s - loss: 27.9750 - MinusLogProbMetric: 27.9750 - val_loss: 28.1368 - val_MinusLogProbMetric: 28.1368 - lr: 5.0000e-04 - 53s/epoch - 269ms/step
Epoch 638/1000
2023-09-29 12:35:33.989 
Epoch 638/1000 
	 loss: 27.9864, MinusLogProbMetric: 27.9864, val_loss: 28.2742, val_MinusLogProbMetric: 28.2742

Epoch 638: val_loss did not improve from 28.13677
196/196 - 52s - loss: 27.9864 - MinusLogProbMetric: 27.9864 - val_loss: 28.2742 - val_MinusLogProbMetric: 28.2742 - lr: 5.0000e-04 - 52s/epoch - 266ms/step
Epoch 639/1000
2023-09-29 12:36:29.278 
Epoch 639/1000 
	 loss: 28.1655, MinusLogProbMetric: 28.1655, val_loss: 28.1605, val_MinusLogProbMetric: 28.1605

Epoch 639: val_loss did not improve from 28.13677
196/196 - 55s - loss: 28.1655 - MinusLogProbMetric: 28.1655 - val_loss: 28.1605 - val_MinusLogProbMetric: 28.1605 - lr: 5.0000e-04 - 55s/epoch - 282ms/step
Epoch 640/1000
2023-09-29 12:37:23.915 
Epoch 640/1000 
	 loss: 27.9374, MinusLogProbMetric: 27.9374, val_loss: 28.2112, val_MinusLogProbMetric: 28.2112

Epoch 640: val_loss did not improve from 28.13677
196/196 - 55s - loss: 27.9374 - MinusLogProbMetric: 27.9374 - val_loss: 28.2112 - val_MinusLogProbMetric: 28.2112 - lr: 5.0000e-04 - 55s/epoch - 279ms/step
Epoch 641/1000
2023-09-29 12:38:19.308 
Epoch 641/1000 
	 loss: 27.9744, MinusLogProbMetric: 27.9744, val_loss: 28.8699, val_MinusLogProbMetric: 28.8699

Epoch 641: val_loss did not improve from 28.13677
196/196 - 55s - loss: 27.9744 - MinusLogProbMetric: 27.9744 - val_loss: 28.8699 - val_MinusLogProbMetric: 28.8699 - lr: 5.0000e-04 - 55s/epoch - 283ms/step
Epoch 642/1000
2023-09-29 12:39:12.455 
Epoch 642/1000 
	 loss: 28.0084, MinusLogProbMetric: 28.0084, val_loss: 28.1941, val_MinusLogProbMetric: 28.1941

Epoch 642: val_loss did not improve from 28.13677
196/196 - 53s - loss: 28.0084 - MinusLogProbMetric: 28.0084 - val_loss: 28.1941 - val_MinusLogProbMetric: 28.1941 - lr: 5.0000e-04 - 53s/epoch - 271ms/step
Epoch 643/1000
2023-09-29 12:40:07.792 
Epoch 643/1000 
	 loss: 28.0120, MinusLogProbMetric: 28.0120, val_loss: 28.0932, val_MinusLogProbMetric: 28.0932

Epoch 643: val_loss improved from 28.13677 to 28.09322, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 57s - loss: 28.0120 - MinusLogProbMetric: 28.0120 - val_loss: 28.0932 - val_MinusLogProbMetric: 28.0932 - lr: 5.0000e-04 - 57s/epoch - 289ms/step
Epoch 644/1000
2023-09-29 12:41:04.620 
Epoch 644/1000 
	 loss: 27.9858, MinusLogProbMetric: 27.9858, val_loss: 28.2868, val_MinusLogProbMetric: 28.2868

Epoch 644: val_loss did not improve from 28.09322
196/196 - 55s - loss: 27.9858 - MinusLogProbMetric: 27.9858 - val_loss: 28.2868 - val_MinusLogProbMetric: 28.2868 - lr: 5.0000e-04 - 55s/epoch - 283ms/step
Epoch 645/1000
2023-09-29 12:42:01.213 
Epoch 645/1000 
	 loss: 28.0044, MinusLogProbMetric: 28.0044, val_loss: 28.2930, val_MinusLogProbMetric: 28.2930

Epoch 645: val_loss did not improve from 28.09322
196/196 - 57s - loss: 28.0044 - MinusLogProbMetric: 28.0044 - val_loss: 28.2930 - val_MinusLogProbMetric: 28.2930 - lr: 5.0000e-04 - 57s/epoch - 289ms/step
Epoch 646/1000
2023-09-29 12:42:56.462 
Epoch 646/1000 
	 loss: 28.0252, MinusLogProbMetric: 28.0252, val_loss: 28.1698, val_MinusLogProbMetric: 28.1698

Epoch 646: val_loss did not improve from 28.09322
196/196 - 55s - loss: 28.0252 - MinusLogProbMetric: 28.0252 - val_loss: 28.1698 - val_MinusLogProbMetric: 28.1698 - lr: 5.0000e-04 - 55s/epoch - 282ms/step
Epoch 647/1000
2023-09-29 12:43:53.093 
Epoch 647/1000 
	 loss: 27.9790, MinusLogProbMetric: 27.9790, val_loss: 28.2806, val_MinusLogProbMetric: 28.2806

Epoch 647: val_loss did not improve from 28.09322
196/196 - 57s - loss: 27.9790 - MinusLogProbMetric: 27.9790 - val_loss: 28.2806 - val_MinusLogProbMetric: 28.2806 - lr: 5.0000e-04 - 57s/epoch - 289ms/step
Epoch 648/1000
2023-09-29 12:44:48.434 
Epoch 648/1000 
	 loss: 27.9746, MinusLogProbMetric: 27.9746, val_loss: 28.2535, val_MinusLogProbMetric: 28.2535

Epoch 648: val_loss did not improve from 28.09322
196/196 - 55s - loss: 27.9746 - MinusLogProbMetric: 27.9746 - val_loss: 28.2535 - val_MinusLogProbMetric: 28.2535 - lr: 5.0000e-04 - 55s/epoch - 282ms/step
Epoch 649/1000
2023-09-29 12:45:43.712 
Epoch 649/1000 
	 loss: 27.9791, MinusLogProbMetric: 27.9791, val_loss: 28.1703, val_MinusLogProbMetric: 28.1703

Epoch 649: val_loss did not improve from 28.09322
196/196 - 55s - loss: 27.9791 - MinusLogProbMetric: 27.9791 - val_loss: 28.1703 - val_MinusLogProbMetric: 28.1703 - lr: 5.0000e-04 - 55s/epoch - 282ms/step
Epoch 650/1000
2023-09-29 12:46:40.527 
Epoch 650/1000 
	 loss: 27.9830, MinusLogProbMetric: 27.9830, val_loss: 28.1602, val_MinusLogProbMetric: 28.1602

Epoch 650: val_loss did not improve from 28.09322
196/196 - 57s - loss: 27.9830 - MinusLogProbMetric: 27.9830 - val_loss: 28.1602 - val_MinusLogProbMetric: 28.1602 - lr: 5.0000e-04 - 57s/epoch - 290ms/step
Epoch 651/1000
2023-09-29 12:47:36.950 
Epoch 651/1000 
	 loss: 28.0652, MinusLogProbMetric: 28.0652, val_loss: 28.4332, val_MinusLogProbMetric: 28.4332

Epoch 651: val_loss did not improve from 28.09322
196/196 - 56s - loss: 28.0652 - MinusLogProbMetric: 28.0652 - val_loss: 28.4332 - val_MinusLogProbMetric: 28.4332 - lr: 5.0000e-04 - 56s/epoch - 288ms/step
Epoch 652/1000
2023-09-29 12:48:33.111 
Epoch 652/1000 
	 loss: 27.9794, MinusLogProbMetric: 27.9794, val_loss: 28.3364, val_MinusLogProbMetric: 28.3364

Epoch 652: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9794 - MinusLogProbMetric: 27.9794 - val_loss: 28.3364 - val_MinusLogProbMetric: 28.3364 - lr: 5.0000e-04 - 56s/epoch - 286ms/step
Epoch 653/1000
2023-09-29 12:49:29.698 
Epoch 653/1000 
	 loss: 27.9565, MinusLogProbMetric: 27.9565, val_loss: 28.4118, val_MinusLogProbMetric: 28.4118

Epoch 653: val_loss did not improve from 28.09322
196/196 - 57s - loss: 27.9565 - MinusLogProbMetric: 27.9565 - val_loss: 28.4118 - val_MinusLogProbMetric: 28.4118 - lr: 5.0000e-04 - 57s/epoch - 289ms/step
Epoch 654/1000
2023-09-29 12:50:25.937 
Epoch 654/1000 
	 loss: 27.9630, MinusLogProbMetric: 27.9630, val_loss: 28.1705, val_MinusLogProbMetric: 28.1705

Epoch 654: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9630 - MinusLogProbMetric: 27.9630 - val_loss: 28.1705 - val_MinusLogProbMetric: 28.1705 - lr: 5.0000e-04 - 56s/epoch - 287ms/step
Epoch 655/1000
2023-09-29 12:51:22.824 
Epoch 655/1000 
	 loss: 27.9955, MinusLogProbMetric: 27.9955, val_loss: 28.2110, val_MinusLogProbMetric: 28.2110

Epoch 655: val_loss did not improve from 28.09322
196/196 - 57s - loss: 27.9955 - MinusLogProbMetric: 27.9955 - val_loss: 28.2110 - val_MinusLogProbMetric: 28.2110 - lr: 5.0000e-04 - 57s/epoch - 290ms/step
Epoch 656/1000
2023-09-29 12:52:19.920 
Epoch 656/1000 
	 loss: 28.0135, MinusLogProbMetric: 28.0135, val_loss: 28.2462, val_MinusLogProbMetric: 28.2462

Epoch 656: val_loss did not improve from 28.09322
196/196 - 57s - loss: 28.0135 - MinusLogProbMetric: 28.0135 - val_loss: 28.2462 - val_MinusLogProbMetric: 28.2462 - lr: 5.0000e-04 - 57s/epoch - 291ms/step
Epoch 657/1000
2023-09-29 12:53:15.874 
Epoch 657/1000 
	 loss: 27.9424, MinusLogProbMetric: 27.9424, val_loss: 28.2450, val_MinusLogProbMetric: 28.2450

Epoch 657: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9424 - MinusLogProbMetric: 27.9424 - val_loss: 28.2450 - val_MinusLogProbMetric: 28.2450 - lr: 5.0000e-04 - 56s/epoch - 285ms/step
Epoch 658/1000
2023-09-29 12:54:11.656 
Epoch 658/1000 
	 loss: 27.9899, MinusLogProbMetric: 27.9899, val_loss: 28.3721, val_MinusLogProbMetric: 28.3721

Epoch 658: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9899 - MinusLogProbMetric: 27.9899 - val_loss: 28.3721 - val_MinusLogProbMetric: 28.3721 - lr: 5.0000e-04 - 56s/epoch - 285ms/step
Epoch 659/1000
2023-09-29 12:55:07.959 
Epoch 659/1000 
	 loss: 27.9993, MinusLogProbMetric: 27.9993, val_loss: 28.3516, val_MinusLogProbMetric: 28.3516

Epoch 659: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9993 - MinusLogProbMetric: 27.9993 - val_loss: 28.3516 - val_MinusLogProbMetric: 28.3516 - lr: 5.0000e-04 - 56s/epoch - 287ms/step
Epoch 660/1000
2023-09-29 12:56:03.557 
Epoch 660/1000 
	 loss: 28.0194, MinusLogProbMetric: 28.0194, val_loss: 28.1699, val_MinusLogProbMetric: 28.1699

Epoch 660: val_loss did not improve from 28.09322
196/196 - 56s - loss: 28.0194 - MinusLogProbMetric: 28.0194 - val_loss: 28.1699 - val_MinusLogProbMetric: 28.1699 - lr: 5.0000e-04 - 56s/epoch - 284ms/step
Epoch 661/1000
2023-09-29 12:56:59.594 
Epoch 661/1000 
	 loss: 27.9346, MinusLogProbMetric: 27.9346, val_loss: 28.1503, val_MinusLogProbMetric: 28.1503

Epoch 661: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9346 - MinusLogProbMetric: 27.9346 - val_loss: 28.1503 - val_MinusLogProbMetric: 28.1503 - lr: 5.0000e-04 - 56s/epoch - 286ms/step
Epoch 662/1000
2023-09-29 12:57:55.764 
Epoch 662/1000 
	 loss: 27.9637, MinusLogProbMetric: 27.9637, val_loss: 28.2367, val_MinusLogProbMetric: 28.2367

Epoch 662: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9637 - MinusLogProbMetric: 27.9637 - val_loss: 28.2367 - val_MinusLogProbMetric: 28.2367 - lr: 5.0000e-04 - 56s/epoch - 287ms/step
Epoch 663/1000
2023-09-29 12:58:51.540 
Epoch 663/1000 
	 loss: 28.0214, MinusLogProbMetric: 28.0214, val_loss: 28.1483, val_MinusLogProbMetric: 28.1483

Epoch 663: val_loss did not improve from 28.09322
196/196 - 56s - loss: 28.0214 - MinusLogProbMetric: 28.0214 - val_loss: 28.1483 - val_MinusLogProbMetric: 28.1483 - lr: 5.0000e-04 - 56s/epoch - 285ms/step
Epoch 664/1000
2023-09-29 12:59:47.682 
Epoch 664/1000 
	 loss: 27.9875, MinusLogProbMetric: 27.9875, val_loss: 28.4363, val_MinusLogProbMetric: 28.4363

Epoch 664: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9875 - MinusLogProbMetric: 27.9875 - val_loss: 28.4363 - val_MinusLogProbMetric: 28.4363 - lr: 5.0000e-04 - 56s/epoch - 286ms/step
Epoch 665/1000
2023-09-29 13:00:44.167 
Epoch 665/1000 
	 loss: 27.9858, MinusLogProbMetric: 27.9858, val_loss: 28.2104, val_MinusLogProbMetric: 28.2104

Epoch 665: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9858 - MinusLogProbMetric: 27.9858 - val_loss: 28.2104 - val_MinusLogProbMetric: 28.2104 - lr: 5.0000e-04 - 56s/epoch - 288ms/step
Epoch 666/1000
2023-09-29 13:01:40.297 
Epoch 666/1000 
	 loss: 28.0245, MinusLogProbMetric: 28.0245, val_loss: 28.2773, val_MinusLogProbMetric: 28.2773

Epoch 666: val_loss did not improve from 28.09322
196/196 - 56s - loss: 28.0245 - MinusLogProbMetric: 28.0245 - val_loss: 28.2773 - val_MinusLogProbMetric: 28.2773 - lr: 5.0000e-04 - 56s/epoch - 286ms/step
Epoch 667/1000
2023-09-29 13:02:36.608 
Epoch 667/1000 
	 loss: 27.9926, MinusLogProbMetric: 27.9926, val_loss: 28.2172, val_MinusLogProbMetric: 28.2172

Epoch 667: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9926 - MinusLogProbMetric: 27.9926 - val_loss: 28.2172 - val_MinusLogProbMetric: 28.2172 - lr: 5.0000e-04 - 56s/epoch - 287ms/step
Epoch 668/1000
2023-09-29 13:03:32.721 
Epoch 668/1000 
	 loss: 28.0009, MinusLogProbMetric: 28.0009, val_loss: 28.4654, val_MinusLogProbMetric: 28.4654

Epoch 668: val_loss did not improve from 28.09322
196/196 - 56s - loss: 28.0009 - MinusLogProbMetric: 28.0009 - val_loss: 28.4654 - val_MinusLogProbMetric: 28.4654 - lr: 5.0000e-04 - 56s/epoch - 286ms/step
Epoch 669/1000
2023-09-29 13:04:29.170 
Epoch 669/1000 
	 loss: 27.9918, MinusLogProbMetric: 27.9918, val_loss: 28.2350, val_MinusLogProbMetric: 28.2350

Epoch 669: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9918 - MinusLogProbMetric: 27.9918 - val_loss: 28.2350 - val_MinusLogProbMetric: 28.2350 - lr: 5.0000e-04 - 56s/epoch - 288ms/step
Epoch 670/1000
2023-09-29 13:05:27.071 
Epoch 670/1000 
	 loss: 27.9934, MinusLogProbMetric: 27.9934, val_loss: 28.2549, val_MinusLogProbMetric: 28.2549

Epoch 670: val_loss did not improve from 28.09322
196/196 - 58s - loss: 27.9934 - MinusLogProbMetric: 27.9934 - val_loss: 28.2549 - val_MinusLogProbMetric: 28.2549 - lr: 5.0000e-04 - 58s/epoch - 295ms/step
Epoch 671/1000
2023-09-29 13:06:23.177 
Epoch 671/1000 
	 loss: 27.9760, MinusLogProbMetric: 27.9760, val_loss: 28.1640, val_MinusLogProbMetric: 28.1640

Epoch 671: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9760 - MinusLogProbMetric: 27.9760 - val_loss: 28.1640 - val_MinusLogProbMetric: 28.1640 - lr: 5.0000e-04 - 56s/epoch - 286ms/step
Epoch 672/1000
2023-09-29 13:07:21.159 
Epoch 672/1000 
	 loss: 27.9440, MinusLogProbMetric: 27.9440, val_loss: 28.3302, val_MinusLogProbMetric: 28.3302

Epoch 672: val_loss did not improve from 28.09322
196/196 - 58s - loss: 27.9440 - MinusLogProbMetric: 27.9440 - val_loss: 28.3302 - val_MinusLogProbMetric: 28.3302 - lr: 5.0000e-04 - 58s/epoch - 296ms/step
Epoch 673/1000
2023-09-29 13:08:17.873 
Epoch 673/1000 
	 loss: 28.0115, MinusLogProbMetric: 28.0115, val_loss: 28.1681, val_MinusLogProbMetric: 28.1681

Epoch 673: val_loss did not improve from 28.09322
196/196 - 57s - loss: 28.0115 - MinusLogProbMetric: 28.0115 - val_loss: 28.1681 - val_MinusLogProbMetric: 28.1681 - lr: 5.0000e-04 - 57s/epoch - 289ms/step
Epoch 674/1000
2023-09-29 13:09:13.955 
Epoch 674/1000 
	 loss: 27.9688, MinusLogProbMetric: 27.9688, val_loss: 28.3082, val_MinusLogProbMetric: 28.3082

Epoch 674: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9688 - MinusLogProbMetric: 27.9688 - val_loss: 28.3082 - val_MinusLogProbMetric: 28.3082 - lr: 5.0000e-04 - 56s/epoch - 286ms/step
Epoch 675/1000
2023-09-29 13:10:11.061 
Epoch 675/1000 
	 loss: 27.9728, MinusLogProbMetric: 27.9728, val_loss: 28.2150, val_MinusLogProbMetric: 28.2150

Epoch 675: val_loss did not improve from 28.09322
196/196 - 57s - loss: 27.9728 - MinusLogProbMetric: 27.9728 - val_loss: 28.2150 - val_MinusLogProbMetric: 28.2150 - lr: 5.0000e-04 - 57s/epoch - 291ms/step
Epoch 676/1000
2023-09-29 13:11:06.462 
Epoch 676/1000 
	 loss: 28.0066, MinusLogProbMetric: 28.0066, val_loss: 28.2517, val_MinusLogProbMetric: 28.2517

Epoch 676: val_loss did not improve from 28.09322
196/196 - 55s - loss: 28.0066 - MinusLogProbMetric: 28.0066 - val_loss: 28.2517 - val_MinusLogProbMetric: 28.2517 - lr: 5.0000e-04 - 55s/epoch - 283ms/step
Epoch 677/1000
2023-09-29 13:12:03.467 
Epoch 677/1000 
	 loss: 27.9825, MinusLogProbMetric: 27.9825, val_loss: 28.2706, val_MinusLogProbMetric: 28.2706

Epoch 677: val_loss did not improve from 28.09322
196/196 - 57s - loss: 27.9825 - MinusLogProbMetric: 27.9825 - val_loss: 28.2706 - val_MinusLogProbMetric: 28.2706 - lr: 5.0000e-04 - 57s/epoch - 291ms/step
Epoch 678/1000
2023-09-29 13:12:59.306 
Epoch 678/1000 
	 loss: 28.0096, MinusLogProbMetric: 28.0096, val_loss: 28.5587, val_MinusLogProbMetric: 28.5587

Epoch 678: val_loss did not improve from 28.09322
196/196 - 56s - loss: 28.0096 - MinusLogProbMetric: 28.0096 - val_loss: 28.5587 - val_MinusLogProbMetric: 28.5587 - lr: 5.0000e-04 - 56s/epoch - 285ms/step
Epoch 679/1000
2023-09-29 13:13:55.164 
Epoch 679/1000 
	 loss: 27.9775, MinusLogProbMetric: 27.9775, val_loss: 28.1672, val_MinusLogProbMetric: 28.1672

Epoch 679: val_loss did not improve from 28.09322
196/196 - 56s - loss: 27.9775 - MinusLogProbMetric: 27.9775 - val_loss: 28.1672 - val_MinusLogProbMetric: 28.1672 - lr: 5.0000e-04 - 56s/epoch - 285ms/step
Epoch 680/1000
2023-09-29 13:14:51.290 
Epoch 680/1000 
	 loss: 27.9573, MinusLogProbMetric: 27.9573, val_loss: 28.0700, val_MinusLogProbMetric: 28.0700

Epoch 680: val_loss improved from 28.09322 to 28.07002, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 57s - loss: 27.9573 - MinusLogProbMetric: 27.9573 - val_loss: 28.0700 - val_MinusLogProbMetric: 28.0700 - lr: 5.0000e-04 - 57s/epoch - 292ms/step
Epoch 681/1000
2023-09-29 13:15:48.821 
Epoch 681/1000 
	 loss: 27.9594, MinusLogProbMetric: 27.9594, val_loss: 28.5977, val_MinusLogProbMetric: 28.5977

Epoch 681: val_loss did not improve from 28.07002
196/196 - 56s - loss: 27.9594 - MinusLogProbMetric: 27.9594 - val_loss: 28.5977 - val_MinusLogProbMetric: 28.5977 - lr: 5.0000e-04 - 56s/epoch - 288ms/step
Epoch 682/1000
2023-09-29 13:16:45.244 
Epoch 682/1000 
	 loss: 27.9498, MinusLogProbMetric: 27.9498, val_loss: 28.2380, val_MinusLogProbMetric: 28.2380

Epoch 682: val_loss did not improve from 28.07002
196/196 - 56s - loss: 27.9498 - MinusLogProbMetric: 27.9498 - val_loss: 28.2380 - val_MinusLogProbMetric: 28.2380 - lr: 5.0000e-04 - 56s/epoch - 288ms/step
Epoch 683/1000
2023-09-29 13:17:41.817 
Epoch 683/1000 
	 loss: 28.0345, MinusLogProbMetric: 28.0345, val_loss: 28.2131, val_MinusLogProbMetric: 28.2131

Epoch 683: val_loss did not improve from 28.07002
196/196 - 57s - loss: 28.0345 - MinusLogProbMetric: 28.0345 - val_loss: 28.2131 - val_MinusLogProbMetric: 28.2131 - lr: 5.0000e-04 - 57s/epoch - 289ms/step
Epoch 684/1000
2023-09-29 13:18:38.420 
Epoch 684/1000 
	 loss: 27.9559, MinusLogProbMetric: 27.9559, val_loss: 28.2313, val_MinusLogProbMetric: 28.2313

Epoch 684: val_loss did not improve from 28.07002
196/196 - 57s - loss: 27.9559 - MinusLogProbMetric: 27.9559 - val_loss: 28.2313 - val_MinusLogProbMetric: 28.2313 - lr: 5.0000e-04 - 57s/epoch - 289ms/step
Epoch 685/1000
2023-09-29 13:19:35.087 
Epoch 685/1000 
	 loss: 27.9833, MinusLogProbMetric: 27.9833, val_loss: 28.3394, val_MinusLogProbMetric: 28.3394

Epoch 685: val_loss did not improve from 28.07002
196/196 - 57s - loss: 27.9833 - MinusLogProbMetric: 27.9833 - val_loss: 28.3394 - val_MinusLogProbMetric: 28.3394 - lr: 5.0000e-04 - 57s/epoch - 289ms/step
Epoch 686/1000
2023-09-29 13:20:31.272 
Epoch 686/1000 
	 loss: 28.0407, MinusLogProbMetric: 28.0407, val_loss: 28.2119, val_MinusLogProbMetric: 28.2119

Epoch 686: val_loss did not improve from 28.07002
196/196 - 56s - loss: 28.0407 - MinusLogProbMetric: 28.0407 - val_loss: 28.2119 - val_MinusLogProbMetric: 28.2119 - lr: 5.0000e-04 - 56s/epoch - 287ms/step
Epoch 687/1000
2023-09-29 13:21:28.001 
Epoch 687/1000 
	 loss: 27.9952, MinusLogProbMetric: 27.9952, val_loss: 28.1193, val_MinusLogProbMetric: 28.1193

Epoch 687: val_loss did not improve from 28.07002
196/196 - 57s - loss: 27.9952 - MinusLogProbMetric: 27.9952 - val_loss: 28.1193 - val_MinusLogProbMetric: 28.1193 - lr: 5.0000e-04 - 57s/epoch - 289ms/step
Epoch 688/1000
2023-09-29 13:22:24.418 
Epoch 688/1000 
	 loss: 27.9743, MinusLogProbMetric: 27.9743, val_loss: 28.1646, val_MinusLogProbMetric: 28.1646

Epoch 688: val_loss did not improve from 28.07002
196/196 - 56s - loss: 27.9743 - MinusLogProbMetric: 27.9743 - val_loss: 28.1646 - val_MinusLogProbMetric: 28.1646 - lr: 5.0000e-04 - 56s/epoch - 288ms/step
Epoch 689/1000
2023-09-29 13:23:21.371 
Epoch 689/1000 
	 loss: 27.9904, MinusLogProbMetric: 27.9904, val_loss: 28.1869, val_MinusLogProbMetric: 28.1869

Epoch 689: val_loss did not improve from 28.07002
196/196 - 57s - loss: 27.9904 - MinusLogProbMetric: 27.9904 - val_loss: 28.1869 - val_MinusLogProbMetric: 28.1869 - lr: 5.0000e-04 - 57s/epoch - 291ms/step
Epoch 690/1000
2023-09-29 13:24:17.598 
Epoch 690/1000 
	 loss: 28.0048, MinusLogProbMetric: 28.0048, val_loss: 28.2549, val_MinusLogProbMetric: 28.2549

Epoch 690: val_loss did not improve from 28.07002
196/196 - 56s - loss: 28.0048 - MinusLogProbMetric: 28.0048 - val_loss: 28.2549 - val_MinusLogProbMetric: 28.2549 - lr: 5.0000e-04 - 56s/epoch - 287ms/step
Epoch 691/1000
2023-09-29 13:25:14.278 
Epoch 691/1000 
	 loss: 28.0310, MinusLogProbMetric: 28.0310, val_loss: 28.3318, val_MinusLogProbMetric: 28.3318

Epoch 691: val_loss did not improve from 28.07002
196/196 - 57s - loss: 28.0310 - MinusLogProbMetric: 28.0310 - val_loss: 28.3318 - val_MinusLogProbMetric: 28.3318 - lr: 5.0000e-04 - 57s/epoch - 289ms/step
Epoch 692/1000
2023-09-29 13:26:10.784 
Epoch 692/1000 
	 loss: 28.0043, MinusLogProbMetric: 28.0043, val_loss: 28.2010, val_MinusLogProbMetric: 28.2010

Epoch 692: val_loss did not improve from 28.07002
196/196 - 56s - loss: 28.0043 - MinusLogProbMetric: 28.0043 - val_loss: 28.2010 - val_MinusLogProbMetric: 28.2010 - lr: 5.0000e-04 - 56s/epoch - 288ms/step
Epoch 693/1000
2023-09-29 13:27:06.862 
Epoch 693/1000 
	 loss: 27.9859, MinusLogProbMetric: 27.9859, val_loss: 28.1767, val_MinusLogProbMetric: 28.1767

Epoch 693: val_loss did not improve from 28.07002
196/196 - 56s - loss: 27.9859 - MinusLogProbMetric: 27.9859 - val_loss: 28.1767 - val_MinusLogProbMetric: 28.1767 - lr: 5.0000e-04 - 56s/epoch - 286ms/step
Epoch 694/1000
2023-09-29 13:28:03.215 
Epoch 694/1000 
	 loss: 27.9313, MinusLogProbMetric: 27.9313, val_loss: 29.1291, val_MinusLogProbMetric: 29.1291

Epoch 694: val_loss did not improve from 28.07002
196/196 - 56s - loss: 27.9313 - MinusLogProbMetric: 27.9313 - val_loss: 29.1291 - val_MinusLogProbMetric: 29.1291 - lr: 5.0000e-04 - 56s/epoch - 287ms/step
Epoch 695/1000
2023-09-29 13:29:00.036 
Epoch 695/1000 
	 loss: 28.0168, MinusLogProbMetric: 28.0168, val_loss: 28.1540, val_MinusLogProbMetric: 28.1540

Epoch 695: val_loss did not improve from 28.07002
196/196 - 57s - loss: 28.0168 - MinusLogProbMetric: 28.0168 - val_loss: 28.1540 - val_MinusLogProbMetric: 28.1540 - lr: 5.0000e-04 - 57s/epoch - 290ms/step
Epoch 696/1000
2023-09-29 13:29:56.515 
Epoch 696/1000 
	 loss: 28.0174, MinusLogProbMetric: 28.0174, val_loss: 28.1829, val_MinusLogProbMetric: 28.1829

Epoch 696: val_loss did not improve from 28.07002
196/196 - 56s - loss: 28.0174 - MinusLogProbMetric: 28.0174 - val_loss: 28.1829 - val_MinusLogProbMetric: 28.1829 - lr: 5.0000e-04 - 56s/epoch - 288ms/step
Epoch 697/1000
2023-09-29 13:30:53.087 
Epoch 697/1000 
	 loss: 28.0011, MinusLogProbMetric: 28.0011, val_loss: 28.1082, val_MinusLogProbMetric: 28.1082

Epoch 697: val_loss did not improve from 28.07002
196/196 - 57s - loss: 28.0011 - MinusLogProbMetric: 28.0011 - val_loss: 28.1082 - val_MinusLogProbMetric: 28.1082 - lr: 5.0000e-04 - 57s/epoch - 289ms/step
Epoch 698/1000
2023-09-29 13:31:49.427 
Epoch 698/1000 
	 loss: 27.9614, MinusLogProbMetric: 27.9614, val_loss: 28.1594, val_MinusLogProbMetric: 28.1594

Epoch 698: val_loss did not improve from 28.07002
196/196 - 56s - loss: 27.9614 - MinusLogProbMetric: 27.9614 - val_loss: 28.1594 - val_MinusLogProbMetric: 28.1594 - lr: 5.0000e-04 - 56s/epoch - 287ms/step
Epoch 699/1000
2023-09-29 13:32:46.064 
Epoch 699/1000 
	 loss: 27.9802, MinusLogProbMetric: 27.9802, val_loss: 28.1841, val_MinusLogProbMetric: 28.1841

Epoch 699: val_loss did not improve from 28.07002
196/196 - 57s - loss: 27.9802 - MinusLogProbMetric: 27.9802 - val_loss: 28.1841 - val_MinusLogProbMetric: 28.1841 - lr: 5.0000e-04 - 57s/epoch - 289ms/step
Epoch 700/1000
2023-09-29 13:33:42.089 
Epoch 700/1000 
	 loss: 27.9986, MinusLogProbMetric: 27.9986, val_loss: 28.1333, val_MinusLogProbMetric: 28.1333

Epoch 700: val_loss did not improve from 28.07002
196/196 - 56s - loss: 27.9986 - MinusLogProbMetric: 27.9986 - val_loss: 28.1333 - val_MinusLogProbMetric: 28.1333 - lr: 5.0000e-04 - 56s/epoch - 286ms/step
Epoch 701/1000
2023-09-29 13:34:38.877 
Epoch 701/1000 
	 loss: 27.9430, MinusLogProbMetric: 27.9430, val_loss: 28.3013, val_MinusLogProbMetric: 28.3013

Epoch 701: val_loss did not improve from 28.07002
196/196 - 57s - loss: 27.9430 - MinusLogProbMetric: 27.9430 - val_loss: 28.3013 - val_MinusLogProbMetric: 28.3013 - lr: 5.0000e-04 - 57s/epoch - 290ms/step
Epoch 702/1000
2023-09-29 13:35:34.505 
Epoch 702/1000 
	 loss: 28.0007, MinusLogProbMetric: 28.0007, val_loss: 28.2149, val_MinusLogProbMetric: 28.2149

Epoch 702: val_loss did not improve from 28.07002
196/196 - 56s - loss: 28.0007 - MinusLogProbMetric: 28.0007 - val_loss: 28.2149 - val_MinusLogProbMetric: 28.2149 - lr: 5.0000e-04 - 56s/epoch - 284ms/step
Epoch 703/1000
2023-09-29 13:36:30.997 
Epoch 703/1000 
	 loss: 27.9241, MinusLogProbMetric: 27.9241, val_loss: 28.1363, val_MinusLogProbMetric: 28.1363

Epoch 703: val_loss did not improve from 28.07002
196/196 - 56s - loss: 27.9241 - MinusLogProbMetric: 27.9241 - val_loss: 28.1363 - val_MinusLogProbMetric: 28.1363 - lr: 5.0000e-04 - 56s/epoch - 288ms/step
Epoch 704/1000
2023-09-29 13:37:27.393 
Epoch 704/1000 
	 loss: 27.9969, MinusLogProbMetric: 27.9969, val_loss: 28.1817, val_MinusLogProbMetric: 28.1817

Epoch 704: val_loss did not improve from 28.07002
196/196 - 56s - loss: 27.9969 - MinusLogProbMetric: 27.9969 - val_loss: 28.1817 - val_MinusLogProbMetric: 28.1817 - lr: 5.0000e-04 - 56s/epoch - 288ms/step
Epoch 705/1000
2023-09-29 13:38:24.177 
Epoch 705/1000 
	 loss: 27.9535, MinusLogProbMetric: 27.9535, val_loss: 28.1123, val_MinusLogProbMetric: 28.1123

Epoch 705: val_loss did not improve from 28.07002
196/196 - 57s - loss: 27.9535 - MinusLogProbMetric: 27.9535 - val_loss: 28.1123 - val_MinusLogProbMetric: 28.1123 - lr: 5.0000e-04 - 57s/epoch - 290ms/step
Epoch 706/1000
2023-09-29 13:39:19.564 
Epoch 706/1000 
	 loss: 27.9983, MinusLogProbMetric: 27.9983, val_loss: 28.2285, val_MinusLogProbMetric: 28.2285

Epoch 706: val_loss did not improve from 28.07002
196/196 - 55s - loss: 27.9983 - MinusLogProbMetric: 27.9983 - val_loss: 28.2285 - val_MinusLogProbMetric: 28.2285 - lr: 5.0000e-04 - 55s/epoch - 283ms/step
Epoch 707/1000
2023-09-29 13:40:09.945 
Epoch 707/1000 
	 loss: 28.0031, MinusLogProbMetric: 28.0031, val_loss: 28.4014, val_MinusLogProbMetric: 28.4014

Epoch 707: val_loss did not improve from 28.07002
196/196 - 50s - loss: 28.0031 - MinusLogProbMetric: 28.0031 - val_loss: 28.4014 - val_MinusLogProbMetric: 28.4014 - lr: 5.0000e-04 - 50s/epoch - 257ms/step
Epoch 708/1000
2023-09-29 13:40:57.802 
Epoch 708/1000 
	 loss: 27.9519, MinusLogProbMetric: 27.9519, val_loss: 28.1589, val_MinusLogProbMetric: 28.1589

Epoch 708: val_loss did not improve from 28.07002
196/196 - 48s - loss: 27.9519 - MinusLogProbMetric: 27.9519 - val_loss: 28.1589 - val_MinusLogProbMetric: 28.1589 - lr: 5.0000e-04 - 48s/epoch - 244ms/step
Epoch 709/1000
2023-09-29 13:41:47.436 
Epoch 709/1000 
	 loss: 27.9738, MinusLogProbMetric: 27.9738, val_loss: 28.2978, val_MinusLogProbMetric: 28.2978

Epoch 709: val_loss did not improve from 28.07002
196/196 - 50s - loss: 27.9738 - MinusLogProbMetric: 27.9738 - val_loss: 28.2978 - val_MinusLogProbMetric: 28.2978 - lr: 5.0000e-04 - 50s/epoch - 253ms/step
Epoch 710/1000
2023-09-29 13:42:37.006 
Epoch 710/1000 
	 loss: 28.0091, MinusLogProbMetric: 28.0091, val_loss: 28.1390, val_MinusLogProbMetric: 28.1390

Epoch 710: val_loss did not improve from 28.07002
196/196 - 50s - loss: 28.0091 - MinusLogProbMetric: 28.0091 - val_loss: 28.1390 - val_MinusLogProbMetric: 28.1390 - lr: 5.0000e-04 - 50s/epoch - 253ms/step
Epoch 711/1000
2023-09-29 13:43:26.565 
Epoch 711/1000 
	 loss: 27.9775, MinusLogProbMetric: 27.9775, val_loss: 28.5407, val_MinusLogProbMetric: 28.5407

Epoch 711: val_loss did not improve from 28.07002
196/196 - 50s - loss: 27.9775 - MinusLogProbMetric: 27.9775 - val_loss: 28.5407 - val_MinusLogProbMetric: 28.5407 - lr: 5.0000e-04 - 50s/epoch - 253ms/step
Epoch 712/1000
2023-09-29 13:44:15.527 
Epoch 712/1000 
	 loss: 28.0024, MinusLogProbMetric: 28.0024, val_loss: 28.2485, val_MinusLogProbMetric: 28.2485

Epoch 712: val_loss did not improve from 28.07002
196/196 - 49s - loss: 28.0024 - MinusLogProbMetric: 28.0024 - val_loss: 28.2485 - val_MinusLogProbMetric: 28.2485 - lr: 5.0000e-04 - 49s/epoch - 250ms/step
Epoch 713/1000
2023-09-29 13:45:05.662 
Epoch 713/1000 
	 loss: 27.9636, MinusLogProbMetric: 27.9636, val_loss: 28.1661, val_MinusLogProbMetric: 28.1661

Epoch 713: val_loss did not improve from 28.07002
196/196 - 50s - loss: 27.9636 - MinusLogProbMetric: 27.9636 - val_loss: 28.1661 - val_MinusLogProbMetric: 28.1661 - lr: 5.0000e-04 - 50s/epoch - 256ms/step
Epoch 714/1000
2023-09-29 13:45:56.735 
Epoch 714/1000 
	 loss: 27.9557, MinusLogProbMetric: 27.9557, val_loss: 28.1184, val_MinusLogProbMetric: 28.1184

Epoch 714: val_loss did not improve from 28.07002
196/196 - 51s - loss: 27.9557 - MinusLogProbMetric: 27.9557 - val_loss: 28.1184 - val_MinusLogProbMetric: 28.1184 - lr: 5.0000e-04 - 51s/epoch - 261ms/step
Epoch 715/1000
2023-09-29 13:46:49.431 
Epoch 715/1000 
	 loss: 27.9326, MinusLogProbMetric: 27.9326, val_loss: 28.1671, val_MinusLogProbMetric: 28.1671

Epoch 715: val_loss did not improve from 28.07002
196/196 - 53s - loss: 27.9326 - MinusLogProbMetric: 27.9326 - val_loss: 28.1671 - val_MinusLogProbMetric: 28.1671 - lr: 5.0000e-04 - 53s/epoch - 269ms/step
Epoch 716/1000
2023-09-29 13:47:43.170 
Epoch 716/1000 
	 loss: 27.9298, MinusLogProbMetric: 27.9298, val_loss: 28.2514, val_MinusLogProbMetric: 28.2514

Epoch 716: val_loss did not improve from 28.07002
196/196 - 54s - loss: 27.9298 - MinusLogProbMetric: 27.9298 - val_loss: 28.2514 - val_MinusLogProbMetric: 28.2514 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 717/1000
2023-09-29 13:48:38.371 
Epoch 717/1000 
	 loss: 27.9850, MinusLogProbMetric: 27.9850, val_loss: 28.1466, val_MinusLogProbMetric: 28.1466

Epoch 717: val_loss did not improve from 28.07002
196/196 - 55s - loss: 27.9850 - MinusLogProbMetric: 27.9850 - val_loss: 28.1466 - val_MinusLogProbMetric: 28.1466 - lr: 5.0000e-04 - 55s/epoch - 282ms/step
Epoch 718/1000
2023-09-29 13:49:31.209 
Epoch 718/1000 
	 loss: 27.9382, MinusLogProbMetric: 27.9382, val_loss: 28.1369, val_MinusLogProbMetric: 28.1369

Epoch 718: val_loss did not improve from 28.07002
196/196 - 53s - loss: 27.9382 - MinusLogProbMetric: 27.9382 - val_loss: 28.1369 - val_MinusLogProbMetric: 28.1369 - lr: 5.0000e-04 - 53s/epoch - 270ms/step
Epoch 719/1000
2023-09-29 13:50:24.933 
Epoch 719/1000 
	 loss: 27.9642, MinusLogProbMetric: 27.9642, val_loss: 28.1992, val_MinusLogProbMetric: 28.1992

Epoch 719: val_loss did not improve from 28.07002
196/196 - 54s - loss: 27.9642 - MinusLogProbMetric: 27.9642 - val_loss: 28.1992 - val_MinusLogProbMetric: 28.1992 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 720/1000
2023-09-29 13:51:18.076 
Epoch 720/1000 
	 loss: 27.9311, MinusLogProbMetric: 27.9311, val_loss: 28.2137, val_MinusLogProbMetric: 28.2137

Epoch 720: val_loss did not improve from 28.07002
196/196 - 53s - loss: 27.9311 - MinusLogProbMetric: 27.9311 - val_loss: 28.2137 - val_MinusLogProbMetric: 28.2137 - lr: 5.0000e-04 - 53s/epoch - 272ms/step
Epoch 721/1000
2023-09-29 13:52:10.497 
Epoch 721/1000 
	 loss: 27.9556, MinusLogProbMetric: 27.9556, val_loss: 28.1346, val_MinusLogProbMetric: 28.1346

Epoch 721: val_loss did not improve from 28.07002
196/196 - 52s - loss: 27.9556 - MinusLogProbMetric: 27.9556 - val_loss: 28.1346 - val_MinusLogProbMetric: 28.1346 - lr: 5.0000e-04 - 52s/epoch - 266ms/step
Epoch 722/1000
2023-09-29 13:53:01.850 
Epoch 722/1000 
	 loss: 28.0156, MinusLogProbMetric: 28.0156, val_loss: 28.5899, val_MinusLogProbMetric: 28.5899

Epoch 722: val_loss did not improve from 28.07002
196/196 - 51s - loss: 28.0156 - MinusLogProbMetric: 28.0156 - val_loss: 28.5899 - val_MinusLogProbMetric: 28.5899 - lr: 5.0000e-04 - 51s/epoch - 262ms/step
Epoch 723/1000
2023-09-29 13:53:54.391 
Epoch 723/1000 
	 loss: 27.9641, MinusLogProbMetric: 27.9641, val_loss: 28.2177, val_MinusLogProbMetric: 28.2177

Epoch 723: val_loss did not improve from 28.07002
196/196 - 53s - loss: 27.9641 - MinusLogProbMetric: 27.9641 - val_loss: 28.2177 - val_MinusLogProbMetric: 28.2177 - lr: 5.0000e-04 - 53s/epoch - 268ms/step
Epoch 724/1000
2023-09-29 13:54:48.749 
Epoch 724/1000 
	 loss: 27.9592, MinusLogProbMetric: 27.9592, val_loss: 28.3484, val_MinusLogProbMetric: 28.3484

Epoch 724: val_loss did not improve from 28.07002
196/196 - 54s - loss: 27.9592 - MinusLogProbMetric: 27.9592 - val_loss: 28.3484 - val_MinusLogProbMetric: 28.3484 - lr: 5.0000e-04 - 54s/epoch - 277ms/step
Epoch 725/1000
2023-09-29 13:55:43.159 
Epoch 725/1000 
	 loss: 27.9488, MinusLogProbMetric: 27.9488, val_loss: 28.1621, val_MinusLogProbMetric: 28.1621

Epoch 725: val_loss did not improve from 28.07002
196/196 - 54s - loss: 27.9488 - MinusLogProbMetric: 27.9488 - val_loss: 28.1621 - val_MinusLogProbMetric: 28.1621 - lr: 5.0000e-04 - 54s/epoch - 278ms/step
Epoch 726/1000
2023-09-29 13:56:37.240 
Epoch 726/1000 
	 loss: 27.9601, MinusLogProbMetric: 27.9601, val_loss: 28.1323, val_MinusLogProbMetric: 28.1323

Epoch 726: val_loss did not improve from 28.07002
196/196 - 54s - loss: 27.9601 - MinusLogProbMetric: 27.9601 - val_loss: 28.1323 - val_MinusLogProbMetric: 28.1323 - lr: 5.0000e-04 - 54s/epoch - 276ms/step
Epoch 727/1000
2023-09-29 13:57:31.196 
Epoch 727/1000 
	 loss: 27.9419, MinusLogProbMetric: 27.9419, val_loss: 28.1349, val_MinusLogProbMetric: 28.1349

Epoch 727: val_loss did not improve from 28.07002
196/196 - 54s - loss: 27.9419 - MinusLogProbMetric: 27.9419 - val_loss: 28.1349 - val_MinusLogProbMetric: 28.1349 - lr: 5.0000e-04 - 54s/epoch - 275ms/step
Epoch 728/1000
2023-09-29 13:58:25.598 
Epoch 728/1000 
	 loss: 28.0826, MinusLogProbMetric: 28.0826, val_loss: 28.2068, val_MinusLogProbMetric: 28.2068

Epoch 728: val_loss did not improve from 28.07002
196/196 - 54s - loss: 28.0826 - MinusLogProbMetric: 28.0826 - val_loss: 28.2068 - val_MinusLogProbMetric: 28.2068 - lr: 5.0000e-04 - 54s/epoch - 278ms/step
Epoch 729/1000
2023-09-29 13:59:19.277 
Epoch 729/1000 
	 loss: 27.9212, MinusLogProbMetric: 27.9212, val_loss: 28.3972, val_MinusLogProbMetric: 28.3972

Epoch 729: val_loss did not improve from 28.07002
196/196 - 54s - loss: 27.9212 - MinusLogProbMetric: 27.9212 - val_loss: 28.3972 - val_MinusLogProbMetric: 28.3972 - lr: 5.0000e-04 - 54s/epoch - 274ms/step
Epoch 730/1000
2023-09-29 14:00:12.471 
Epoch 730/1000 
	 loss: 27.9431, MinusLogProbMetric: 27.9431, val_loss: 28.3013, val_MinusLogProbMetric: 28.3013

Epoch 730: val_loss did not improve from 28.07002
196/196 - 53s - loss: 27.9431 - MinusLogProbMetric: 27.9431 - val_loss: 28.3013 - val_MinusLogProbMetric: 28.3013 - lr: 5.0000e-04 - 53s/epoch - 271ms/step
Epoch 731/1000
2023-09-29 14:01:05.064 
Epoch 731/1000 
	 loss: 27.7631, MinusLogProbMetric: 27.7631, val_loss: 27.9879, val_MinusLogProbMetric: 27.9879

Epoch 731: val_loss improved from 28.07002 to 27.98791, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 53s - loss: 27.7631 - MinusLogProbMetric: 27.7631 - val_loss: 27.9879 - val_MinusLogProbMetric: 27.9879 - lr: 2.5000e-04 - 53s/epoch - 272ms/step
Epoch 732/1000
2023-09-29 14:01:58.052 
Epoch 732/1000 
	 loss: 27.7729, MinusLogProbMetric: 27.7729, val_loss: 28.0312, val_MinusLogProbMetric: 28.0312

Epoch 732: val_loss did not improve from 27.98791
196/196 - 52s - loss: 27.7729 - MinusLogProbMetric: 27.7729 - val_loss: 28.0312 - val_MinusLogProbMetric: 28.0312 - lr: 2.5000e-04 - 52s/epoch - 267ms/step
Epoch 733/1000
2023-09-29 14:02:50.502 
Epoch 733/1000 
	 loss: 27.7389, MinusLogProbMetric: 27.7389, val_loss: 28.0484, val_MinusLogProbMetric: 28.0484

Epoch 733: val_loss did not improve from 27.98791
196/196 - 52s - loss: 27.7389 - MinusLogProbMetric: 27.7389 - val_loss: 28.0484 - val_MinusLogProbMetric: 28.0484 - lr: 2.5000e-04 - 52s/epoch - 268ms/step
Epoch 734/1000
2023-09-29 14:03:42.420 
Epoch 734/1000 
	 loss: 27.7498, MinusLogProbMetric: 27.7498, val_loss: 27.9766, val_MinusLogProbMetric: 27.9766

Epoch 734: val_loss improved from 27.98791 to 27.97662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 53s - loss: 27.7498 - MinusLogProbMetric: 27.7498 - val_loss: 27.9766 - val_MinusLogProbMetric: 27.9766 - lr: 2.5000e-04 - 53s/epoch - 268ms/step
Epoch 735/1000
2023-09-29 14:04:38.235 
Epoch 735/1000 
	 loss: 27.7486, MinusLogProbMetric: 27.7486, val_loss: 27.9988, val_MinusLogProbMetric: 27.9988

Epoch 735: val_loss did not improve from 27.97662
196/196 - 55s - loss: 27.7486 - MinusLogProbMetric: 27.7486 - val_loss: 27.9988 - val_MinusLogProbMetric: 27.9988 - lr: 2.5000e-04 - 55s/epoch - 281ms/step
Epoch 736/1000
2023-09-29 14:05:31.759 
Epoch 736/1000 
	 loss: 27.7293, MinusLogProbMetric: 27.7293, val_loss: 28.0838, val_MinusLogProbMetric: 28.0838

Epoch 736: val_loss did not improve from 27.97662
196/196 - 54s - loss: 27.7293 - MinusLogProbMetric: 27.7293 - val_loss: 28.0838 - val_MinusLogProbMetric: 28.0838 - lr: 2.5000e-04 - 54s/epoch - 273ms/step
Epoch 737/1000
2023-09-29 14:06:23.751 
Epoch 737/1000 
	 loss: 27.7468, MinusLogProbMetric: 27.7468, val_loss: 28.1991, val_MinusLogProbMetric: 28.1991

Epoch 737: val_loss did not improve from 27.97662
196/196 - 52s - loss: 27.7468 - MinusLogProbMetric: 27.7468 - val_loss: 28.1991 - val_MinusLogProbMetric: 28.1991 - lr: 2.5000e-04 - 52s/epoch - 265ms/step
Epoch 738/1000
2023-09-29 14:07:12.335 
Epoch 738/1000 
	 loss: 27.7552, MinusLogProbMetric: 27.7552, val_loss: 28.0213, val_MinusLogProbMetric: 28.0213

Epoch 738: val_loss did not improve from 27.97662
196/196 - 49s - loss: 27.7552 - MinusLogProbMetric: 27.7552 - val_loss: 28.0213 - val_MinusLogProbMetric: 28.0213 - lr: 2.5000e-04 - 49s/epoch - 248ms/step
Epoch 739/1000
2023-09-29 14:07:54.407 
Epoch 739/1000 
	 loss: 27.7953, MinusLogProbMetric: 27.7953, val_loss: 27.9959, val_MinusLogProbMetric: 27.9959

Epoch 739: val_loss did not improve from 27.97662
196/196 - 42s - loss: 27.7953 - MinusLogProbMetric: 27.7953 - val_loss: 27.9959 - val_MinusLogProbMetric: 27.9959 - lr: 2.5000e-04 - 42s/epoch - 215ms/step
Epoch 740/1000
2023-09-29 14:08:45.204 
Epoch 740/1000 
	 loss: 27.7317, MinusLogProbMetric: 27.7317, val_loss: 28.0196, val_MinusLogProbMetric: 28.0196

Epoch 740: val_loss did not improve from 27.97662
196/196 - 51s - loss: 27.7317 - MinusLogProbMetric: 27.7317 - val_loss: 28.0196 - val_MinusLogProbMetric: 28.0196 - lr: 2.5000e-04 - 51s/epoch - 259ms/step
Epoch 741/1000
2023-09-29 14:09:36.071 
Epoch 741/1000 
	 loss: 27.7551, MinusLogProbMetric: 27.7551, val_loss: 28.0263, val_MinusLogProbMetric: 28.0263

Epoch 741: val_loss did not improve from 27.97662
196/196 - 51s - loss: 27.7551 - MinusLogProbMetric: 27.7551 - val_loss: 28.0263 - val_MinusLogProbMetric: 28.0263 - lr: 2.5000e-04 - 51s/epoch - 259ms/step
Epoch 742/1000
2023-09-29 14:10:28.676 
Epoch 742/1000 
	 loss: 27.7741, MinusLogProbMetric: 27.7741, val_loss: 28.0618, val_MinusLogProbMetric: 28.0618

Epoch 742: val_loss did not improve from 27.97662
196/196 - 53s - loss: 27.7741 - MinusLogProbMetric: 27.7741 - val_loss: 28.0618 - val_MinusLogProbMetric: 28.0618 - lr: 2.5000e-04 - 53s/epoch - 268ms/step
Epoch 743/1000
2023-09-29 14:11:18.802 
Epoch 743/1000 
	 loss: 27.7495, MinusLogProbMetric: 27.7495, val_loss: 28.0273, val_MinusLogProbMetric: 28.0273

Epoch 743: val_loss did not improve from 27.97662
196/196 - 50s - loss: 27.7495 - MinusLogProbMetric: 27.7495 - val_loss: 28.0273 - val_MinusLogProbMetric: 28.0273 - lr: 2.5000e-04 - 50s/epoch - 256ms/step
Epoch 744/1000
2023-09-29 14:12:08.113 
Epoch 744/1000 
	 loss: 27.7664, MinusLogProbMetric: 27.7664, val_loss: 28.0538, val_MinusLogProbMetric: 28.0538

Epoch 744: val_loss did not improve from 27.97662
196/196 - 49s - loss: 27.7664 - MinusLogProbMetric: 27.7664 - val_loss: 28.0538 - val_MinusLogProbMetric: 28.0538 - lr: 2.5000e-04 - 49s/epoch - 252ms/step
Epoch 745/1000
2023-09-29 14:12:59.638 
Epoch 745/1000 
	 loss: 27.7653, MinusLogProbMetric: 27.7653, val_loss: 27.9867, val_MinusLogProbMetric: 27.9867

Epoch 745: val_loss did not improve from 27.97662
196/196 - 52s - loss: 27.7653 - MinusLogProbMetric: 27.7653 - val_loss: 27.9867 - val_MinusLogProbMetric: 27.9867 - lr: 2.5000e-04 - 52s/epoch - 263ms/step
Epoch 746/1000
2023-09-29 14:13:49.457 
Epoch 746/1000 
	 loss: 27.7812, MinusLogProbMetric: 27.7812, val_loss: 28.0421, val_MinusLogProbMetric: 28.0421

Epoch 746: val_loss did not improve from 27.97662
196/196 - 50s - loss: 27.7812 - MinusLogProbMetric: 27.7812 - val_loss: 28.0421 - val_MinusLogProbMetric: 28.0421 - lr: 2.5000e-04 - 50s/epoch - 254ms/step
Epoch 747/1000
2023-09-29 14:14:38.404 
Epoch 747/1000 
	 loss: 27.7493, MinusLogProbMetric: 27.7493, val_loss: 28.1687, val_MinusLogProbMetric: 28.1687

Epoch 747: val_loss did not improve from 27.97662
196/196 - 49s - loss: 27.7493 - MinusLogProbMetric: 27.7493 - val_loss: 28.1687 - val_MinusLogProbMetric: 28.1687 - lr: 2.5000e-04 - 49s/epoch - 250ms/step
Epoch 748/1000
2023-09-29 14:15:27.018 
Epoch 748/1000 
	 loss: 27.7618, MinusLogProbMetric: 27.7618, val_loss: 28.0099, val_MinusLogProbMetric: 28.0099

Epoch 748: val_loss did not improve from 27.97662
196/196 - 49s - loss: 27.7618 - MinusLogProbMetric: 27.7618 - val_loss: 28.0099 - val_MinusLogProbMetric: 28.0099 - lr: 2.5000e-04 - 49s/epoch - 248ms/step
Epoch 749/1000
2023-09-29 14:16:20.935 
Epoch 749/1000 
	 loss: 27.7659, MinusLogProbMetric: 27.7659, val_loss: 28.0907, val_MinusLogProbMetric: 28.0907

Epoch 749: val_loss did not improve from 27.97662
196/196 - 54s - loss: 27.7659 - MinusLogProbMetric: 27.7659 - val_loss: 28.0907 - val_MinusLogProbMetric: 28.0907 - lr: 2.5000e-04 - 54s/epoch - 275ms/step
Epoch 750/1000
2023-09-29 14:17:12.555 
Epoch 750/1000 
	 loss: 27.7735, MinusLogProbMetric: 27.7735, val_loss: 28.0160, val_MinusLogProbMetric: 28.0160

Epoch 750: val_loss did not improve from 27.97662
196/196 - 52s - loss: 27.7735 - MinusLogProbMetric: 27.7735 - val_loss: 28.0160 - val_MinusLogProbMetric: 28.0160 - lr: 2.5000e-04 - 52s/epoch - 263ms/step
Epoch 751/1000
2023-09-29 14:18:04.830 
Epoch 751/1000 
	 loss: 27.7736, MinusLogProbMetric: 27.7736, val_loss: 28.0835, val_MinusLogProbMetric: 28.0835

Epoch 751: val_loss did not improve from 27.97662
196/196 - 52s - loss: 27.7736 - MinusLogProbMetric: 27.7736 - val_loss: 28.0835 - val_MinusLogProbMetric: 28.0835 - lr: 2.5000e-04 - 52s/epoch - 267ms/step
Epoch 752/1000
2023-09-29 14:18:54.980 
Epoch 752/1000 
	 loss: 27.7886, MinusLogProbMetric: 27.7886, val_loss: 27.9930, val_MinusLogProbMetric: 27.9930

Epoch 752: val_loss did not improve from 27.97662
196/196 - 50s - loss: 27.7886 - MinusLogProbMetric: 27.7886 - val_loss: 27.9930 - val_MinusLogProbMetric: 27.9930 - lr: 2.5000e-04 - 50s/epoch - 256ms/step
Epoch 753/1000
2023-09-29 14:19:44.190 
Epoch 753/1000 
	 loss: 27.7698, MinusLogProbMetric: 27.7698, val_loss: 28.0459, val_MinusLogProbMetric: 28.0459

Epoch 753: val_loss did not improve from 27.97662
196/196 - 49s - loss: 27.7698 - MinusLogProbMetric: 27.7698 - val_loss: 28.0459 - val_MinusLogProbMetric: 28.0459 - lr: 2.5000e-04 - 49s/epoch - 251ms/step
Epoch 754/1000
2023-09-29 14:20:33.109 
Epoch 754/1000 
	 loss: 27.7531, MinusLogProbMetric: 27.7531, val_loss: 27.9591, val_MinusLogProbMetric: 27.9591

Epoch 754: val_loss improved from 27.97662 to 27.95911, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 50s - loss: 27.7531 - MinusLogProbMetric: 27.7531 - val_loss: 27.9591 - val_MinusLogProbMetric: 27.9591 - lr: 2.5000e-04 - 50s/epoch - 255ms/step
Epoch 755/1000
2023-09-29 14:21:25.732 
Epoch 755/1000 
	 loss: 27.7546, MinusLogProbMetric: 27.7546, val_loss: 28.0574, val_MinusLogProbMetric: 28.0574

Epoch 755: val_loss did not improve from 27.95911
196/196 - 52s - loss: 27.7546 - MinusLogProbMetric: 27.7546 - val_loss: 28.0574 - val_MinusLogProbMetric: 28.0574 - lr: 2.5000e-04 - 52s/epoch - 263ms/step
Epoch 756/1000
2023-09-29 14:22:17.210 
Epoch 756/1000 
	 loss: 27.7334, MinusLogProbMetric: 27.7334, val_loss: 27.9663, val_MinusLogProbMetric: 27.9663

Epoch 756: val_loss did not improve from 27.95911
196/196 - 51s - loss: 27.7334 - MinusLogProbMetric: 27.7334 - val_loss: 27.9663 - val_MinusLogProbMetric: 27.9663 - lr: 2.5000e-04 - 51s/epoch - 263ms/step
Epoch 757/1000
2023-09-29 14:23:07.831 
Epoch 757/1000 
	 loss: 27.7434, MinusLogProbMetric: 27.7434, val_loss: 28.1110, val_MinusLogProbMetric: 28.1110

Epoch 757: val_loss did not improve from 27.95911
196/196 - 51s - loss: 27.7434 - MinusLogProbMetric: 27.7434 - val_loss: 28.1110 - val_MinusLogProbMetric: 28.1110 - lr: 2.5000e-04 - 51s/epoch - 258ms/step
Epoch 758/1000
2023-09-29 14:23:57.842 
Epoch 758/1000 
	 loss: 27.7424, MinusLogProbMetric: 27.7424, val_loss: 28.0355, val_MinusLogProbMetric: 28.0355

Epoch 758: val_loss did not improve from 27.95911
196/196 - 50s - loss: 27.7424 - MinusLogProbMetric: 27.7424 - val_loss: 28.0355 - val_MinusLogProbMetric: 28.0355 - lr: 2.5000e-04 - 50s/epoch - 255ms/step
Epoch 759/1000
2023-09-29 14:24:47.135 
Epoch 759/1000 
	 loss: 27.7362, MinusLogProbMetric: 27.7362, val_loss: 27.9808, val_MinusLogProbMetric: 27.9808

Epoch 759: val_loss did not improve from 27.95911
196/196 - 49s - loss: 27.7362 - MinusLogProbMetric: 27.7362 - val_loss: 27.9808 - val_MinusLogProbMetric: 27.9808 - lr: 2.5000e-04 - 49s/epoch - 251ms/step
Epoch 760/1000
2023-09-29 14:25:36.726 
Epoch 760/1000 
	 loss: 27.7245, MinusLogProbMetric: 27.7245, val_loss: 28.0110, val_MinusLogProbMetric: 28.0110

Epoch 760: val_loss did not improve from 27.95911
196/196 - 50s - loss: 27.7245 - MinusLogProbMetric: 27.7245 - val_loss: 28.0110 - val_MinusLogProbMetric: 28.0110 - lr: 2.5000e-04 - 50s/epoch - 253ms/step
Epoch 761/1000
2023-09-29 14:26:29.495 
Epoch 761/1000 
	 loss: 27.7375, MinusLogProbMetric: 27.7375, val_loss: 27.9571, val_MinusLogProbMetric: 27.9571

Epoch 761: val_loss improved from 27.95911 to 27.95712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 54s - loss: 27.7375 - MinusLogProbMetric: 27.7375 - val_loss: 27.9571 - val_MinusLogProbMetric: 27.9571 - lr: 2.5000e-04 - 54s/epoch - 274ms/step
Epoch 762/1000
2023-09-29 14:27:24.176 
Epoch 762/1000 
	 loss: 27.7181, MinusLogProbMetric: 27.7181, val_loss: 28.0136, val_MinusLogProbMetric: 28.0136

Epoch 762: val_loss did not improve from 27.95712
196/196 - 54s - loss: 27.7181 - MinusLogProbMetric: 27.7181 - val_loss: 28.0136 - val_MinusLogProbMetric: 28.0136 - lr: 2.5000e-04 - 54s/epoch - 274ms/step
Epoch 763/1000
2023-09-29 14:28:14.849 
Epoch 763/1000 
	 loss: 27.7428, MinusLogProbMetric: 27.7428, val_loss: 28.0039, val_MinusLogProbMetric: 28.0039

Epoch 763: val_loss did not improve from 27.95712
196/196 - 51s - loss: 27.7428 - MinusLogProbMetric: 27.7428 - val_loss: 28.0039 - val_MinusLogProbMetric: 28.0039 - lr: 2.5000e-04 - 51s/epoch - 258ms/step
Epoch 764/1000
2023-09-29 14:29:06.655 
Epoch 764/1000 
	 loss: 27.7189, MinusLogProbMetric: 27.7189, val_loss: 28.0040, val_MinusLogProbMetric: 28.0040

Epoch 764: val_loss did not improve from 27.95712
196/196 - 52s - loss: 27.7189 - MinusLogProbMetric: 27.7189 - val_loss: 28.0040 - val_MinusLogProbMetric: 28.0040 - lr: 2.5000e-04 - 52s/epoch - 264ms/step
Epoch 765/1000
2023-09-29 14:29:59.868 
Epoch 765/1000 
	 loss: 27.7319, MinusLogProbMetric: 27.7319, val_loss: 28.2626, val_MinusLogProbMetric: 28.2626

Epoch 765: val_loss did not improve from 27.95712
196/196 - 53s - loss: 27.7319 - MinusLogProbMetric: 27.7319 - val_loss: 28.2626 - val_MinusLogProbMetric: 28.2626 - lr: 2.5000e-04 - 53s/epoch - 271ms/step
Epoch 766/1000
2023-09-29 14:30:51.526 
Epoch 766/1000 
	 loss: 27.7610, MinusLogProbMetric: 27.7610, val_loss: 27.9588, val_MinusLogProbMetric: 27.9588

Epoch 766: val_loss did not improve from 27.95712
196/196 - 52s - loss: 27.7610 - MinusLogProbMetric: 27.7610 - val_loss: 27.9588 - val_MinusLogProbMetric: 27.9588 - lr: 2.5000e-04 - 52s/epoch - 264ms/step
Epoch 767/1000
2023-09-29 14:31:40.949 
Epoch 767/1000 
	 loss: 27.7744, MinusLogProbMetric: 27.7744, val_loss: 28.0181, val_MinusLogProbMetric: 28.0181

Epoch 767: val_loss did not improve from 27.95712
196/196 - 49s - loss: 27.7744 - MinusLogProbMetric: 27.7744 - val_loss: 28.0181 - val_MinusLogProbMetric: 28.0181 - lr: 2.5000e-04 - 49s/epoch - 252ms/step
Epoch 768/1000
2023-09-29 14:32:32.471 
Epoch 768/1000 
	 loss: 27.7655, MinusLogProbMetric: 27.7655, val_loss: 27.9741, val_MinusLogProbMetric: 27.9741

Epoch 768: val_loss did not improve from 27.95712
196/196 - 52s - loss: 27.7655 - MinusLogProbMetric: 27.7655 - val_loss: 27.9741 - val_MinusLogProbMetric: 27.9741 - lr: 2.5000e-04 - 52s/epoch - 263ms/step
Epoch 769/1000
2023-09-29 14:33:23.902 
Epoch 769/1000 
	 loss: 27.7646, MinusLogProbMetric: 27.7646, val_loss: 27.9553, val_MinusLogProbMetric: 27.9553

Epoch 769: val_loss improved from 27.95712 to 27.95526, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 52s - loss: 27.7646 - MinusLogProbMetric: 27.7646 - val_loss: 27.9553 - val_MinusLogProbMetric: 27.9553 - lr: 2.5000e-04 - 52s/epoch - 265ms/step
Epoch 770/1000
2023-09-29 14:34:15.483 
Epoch 770/1000 
	 loss: 27.7435, MinusLogProbMetric: 27.7435, val_loss: 28.0278, val_MinusLogProbMetric: 28.0278

Epoch 770: val_loss did not improve from 27.95526
196/196 - 51s - loss: 27.7435 - MinusLogProbMetric: 27.7435 - val_loss: 28.0278 - val_MinusLogProbMetric: 28.0278 - lr: 2.5000e-04 - 51s/epoch - 260ms/step
Epoch 771/1000
2023-09-29 14:35:07.707 
Epoch 771/1000 
	 loss: 27.7503, MinusLogProbMetric: 27.7503, val_loss: 28.1721, val_MinusLogProbMetric: 28.1721

Epoch 771: val_loss did not improve from 27.95526
196/196 - 52s - loss: 27.7503 - MinusLogProbMetric: 27.7503 - val_loss: 28.1721 - val_MinusLogProbMetric: 28.1721 - lr: 2.5000e-04 - 52s/epoch - 266ms/step
Epoch 772/1000
2023-09-29 14:35:56.405 
Epoch 772/1000 
	 loss: 27.7565, MinusLogProbMetric: 27.7565, val_loss: 27.9857, val_MinusLogProbMetric: 27.9857

Epoch 772: val_loss did not improve from 27.95526
196/196 - 49s - loss: 27.7565 - MinusLogProbMetric: 27.7565 - val_loss: 27.9857 - val_MinusLogProbMetric: 27.9857 - lr: 2.5000e-04 - 49s/epoch - 248ms/step
Epoch 773/1000
2023-09-29 14:36:45.668 
Epoch 773/1000 
	 loss: 27.7501, MinusLogProbMetric: 27.7501, val_loss: 28.0110, val_MinusLogProbMetric: 28.0110

Epoch 773: val_loss did not improve from 27.95526
196/196 - 49s - loss: 27.7501 - MinusLogProbMetric: 27.7501 - val_loss: 28.0110 - val_MinusLogProbMetric: 28.0110 - lr: 2.5000e-04 - 49s/epoch - 251ms/step
Epoch 774/1000
2023-09-29 14:37:35.797 
Epoch 774/1000 
	 loss: 27.7665, MinusLogProbMetric: 27.7665, val_loss: 28.2347, val_MinusLogProbMetric: 28.2347

Epoch 774: val_loss did not improve from 27.95526
196/196 - 50s - loss: 27.7665 - MinusLogProbMetric: 27.7665 - val_loss: 28.2347 - val_MinusLogProbMetric: 28.2347 - lr: 2.5000e-04 - 50s/epoch - 256ms/step
Epoch 775/1000
2023-09-29 14:38:26.736 
Epoch 775/1000 
	 loss: 27.7598, MinusLogProbMetric: 27.7598, val_loss: 28.0594, val_MinusLogProbMetric: 28.0594

Epoch 775: val_loss did not improve from 27.95526
196/196 - 51s - loss: 27.7598 - MinusLogProbMetric: 27.7598 - val_loss: 28.0594 - val_MinusLogProbMetric: 28.0594 - lr: 2.5000e-04 - 51s/epoch - 260ms/step
Epoch 776/1000
2023-09-29 14:39:17.195 
Epoch 776/1000 
	 loss: 27.7461, MinusLogProbMetric: 27.7461, val_loss: 28.0464, val_MinusLogProbMetric: 28.0464

Epoch 776: val_loss did not improve from 27.95526
196/196 - 50s - loss: 27.7461 - MinusLogProbMetric: 27.7461 - val_loss: 28.0464 - val_MinusLogProbMetric: 28.0464 - lr: 2.5000e-04 - 50s/epoch - 257ms/step
Epoch 777/1000
2023-09-29 14:40:07.981 
Epoch 777/1000 
	 loss: 27.7744, MinusLogProbMetric: 27.7744, val_loss: 27.9770, val_MinusLogProbMetric: 27.9770

Epoch 777: val_loss did not improve from 27.95526
196/196 - 51s - loss: 27.7744 - MinusLogProbMetric: 27.7744 - val_loss: 27.9770 - val_MinusLogProbMetric: 27.9770 - lr: 2.5000e-04 - 51s/epoch - 259ms/step
Epoch 778/1000
2023-09-29 14:40:59.125 
Epoch 778/1000 
	 loss: 27.7375, MinusLogProbMetric: 27.7375, val_loss: 28.0850, val_MinusLogProbMetric: 28.0850

Epoch 778: val_loss did not improve from 27.95526
196/196 - 51s - loss: 27.7375 - MinusLogProbMetric: 27.7375 - val_loss: 28.0850 - val_MinusLogProbMetric: 28.0850 - lr: 2.5000e-04 - 51s/epoch - 261ms/step
Epoch 779/1000
2023-09-29 14:41:49.781 
Epoch 779/1000 
	 loss: 27.7529, MinusLogProbMetric: 27.7529, val_loss: 28.1780, val_MinusLogProbMetric: 28.1780

Epoch 779: val_loss did not improve from 27.95526
196/196 - 51s - loss: 27.7529 - MinusLogProbMetric: 27.7529 - val_loss: 28.1780 - val_MinusLogProbMetric: 28.1780 - lr: 2.5000e-04 - 51s/epoch - 258ms/step
Epoch 780/1000
2023-09-29 14:42:39.613 
Epoch 780/1000 
	 loss: 27.7421, MinusLogProbMetric: 27.7421, val_loss: 28.3154, val_MinusLogProbMetric: 28.3154

Epoch 780: val_loss did not improve from 27.95526
196/196 - 50s - loss: 27.7421 - MinusLogProbMetric: 27.7421 - val_loss: 28.3154 - val_MinusLogProbMetric: 28.3154 - lr: 2.5000e-04 - 50s/epoch - 254ms/step
Epoch 781/1000
2023-09-29 14:43:30.034 
Epoch 781/1000 
	 loss: 27.7783, MinusLogProbMetric: 27.7783, val_loss: 27.9720, val_MinusLogProbMetric: 27.9720

Epoch 781: val_loss did not improve from 27.95526
196/196 - 50s - loss: 27.7783 - MinusLogProbMetric: 27.7783 - val_loss: 27.9720 - val_MinusLogProbMetric: 27.9720 - lr: 2.5000e-04 - 50s/epoch - 257ms/step
Epoch 782/1000
2023-09-29 14:44:20.444 
Epoch 782/1000 
	 loss: 27.7673, MinusLogProbMetric: 27.7673, val_loss: 28.0526, val_MinusLogProbMetric: 28.0526

Epoch 782: val_loss did not improve from 27.95526
196/196 - 50s - loss: 27.7673 - MinusLogProbMetric: 27.7673 - val_loss: 28.0526 - val_MinusLogProbMetric: 28.0526 - lr: 2.5000e-04 - 50s/epoch - 257ms/step
Epoch 783/1000
2023-09-29 14:45:12.605 
Epoch 783/1000 
	 loss: 27.7401, MinusLogProbMetric: 27.7401, val_loss: 28.0047, val_MinusLogProbMetric: 28.0047

Epoch 783: val_loss did not improve from 27.95526
196/196 - 52s - loss: 27.7401 - MinusLogProbMetric: 27.7401 - val_loss: 28.0047 - val_MinusLogProbMetric: 28.0047 - lr: 2.5000e-04 - 52s/epoch - 266ms/step
Epoch 784/1000
2023-09-29 14:46:04.250 
Epoch 784/1000 
	 loss: 27.7437, MinusLogProbMetric: 27.7437, val_loss: 27.9878, val_MinusLogProbMetric: 27.9878

Epoch 784: val_loss did not improve from 27.95526
196/196 - 52s - loss: 27.7437 - MinusLogProbMetric: 27.7437 - val_loss: 27.9878 - val_MinusLogProbMetric: 27.9878 - lr: 2.5000e-04 - 52s/epoch - 263ms/step
Epoch 785/1000
2023-09-29 14:46:54.534 
Epoch 785/1000 
	 loss: 27.7735, MinusLogProbMetric: 27.7735, val_loss: 27.9637, val_MinusLogProbMetric: 27.9637

Epoch 785: val_loss did not improve from 27.95526
196/196 - 50s - loss: 27.7735 - MinusLogProbMetric: 27.7735 - val_loss: 27.9637 - val_MinusLogProbMetric: 27.9637 - lr: 2.5000e-04 - 50s/epoch - 257ms/step
Epoch 786/1000
2023-09-29 14:47:45.992 
Epoch 786/1000 
	 loss: 27.7331, MinusLogProbMetric: 27.7331, val_loss: 28.0591, val_MinusLogProbMetric: 28.0591

Epoch 786: val_loss did not improve from 27.95526
196/196 - 51s - loss: 27.7331 - MinusLogProbMetric: 27.7331 - val_loss: 28.0591 - val_MinusLogProbMetric: 28.0591 - lr: 2.5000e-04 - 51s/epoch - 263ms/step
Epoch 787/1000
2023-09-29 14:48:36.204 
Epoch 787/1000 
	 loss: 27.7458, MinusLogProbMetric: 27.7458, val_loss: 28.0528, val_MinusLogProbMetric: 28.0528

Epoch 787: val_loss did not improve from 27.95526
196/196 - 50s - loss: 27.7458 - MinusLogProbMetric: 27.7458 - val_loss: 28.0528 - val_MinusLogProbMetric: 28.0528 - lr: 2.5000e-04 - 50s/epoch - 256ms/step
Epoch 788/1000
2023-09-29 14:49:28.049 
Epoch 788/1000 
	 loss: 27.7477, MinusLogProbMetric: 27.7477, val_loss: 28.0324, val_MinusLogProbMetric: 28.0324

Epoch 788: val_loss did not improve from 27.95526
196/196 - 52s - loss: 27.7477 - MinusLogProbMetric: 27.7477 - val_loss: 28.0324 - val_MinusLogProbMetric: 28.0324 - lr: 2.5000e-04 - 52s/epoch - 264ms/step
Epoch 789/1000
2023-09-29 14:50:19.217 
Epoch 789/1000 
	 loss: 27.7446, MinusLogProbMetric: 27.7446, val_loss: 27.9796, val_MinusLogProbMetric: 27.9796

Epoch 789: val_loss did not improve from 27.95526
196/196 - 51s - loss: 27.7446 - MinusLogProbMetric: 27.7446 - val_loss: 27.9796 - val_MinusLogProbMetric: 27.9796 - lr: 2.5000e-04 - 51s/epoch - 261ms/step
Epoch 790/1000
2023-09-29 14:51:07.302 
Epoch 790/1000 
	 loss: 27.7070, MinusLogProbMetric: 27.7070, val_loss: 28.2780, val_MinusLogProbMetric: 28.2780

Epoch 790: val_loss did not improve from 27.95526
196/196 - 48s - loss: 27.7070 - MinusLogProbMetric: 27.7070 - val_loss: 28.2780 - val_MinusLogProbMetric: 28.2780 - lr: 2.5000e-04 - 48s/epoch - 245ms/step
Epoch 791/1000
2023-09-29 14:51:57.818 
Epoch 791/1000 
	 loss: 27.7311, MinusLogProbMetric: 27.7311, val_loss: 27.9485, val_MinusLogProbMetric: 27.9485

Epoch 791: val_loss improved from 27.95526 to 27.94850, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 51s - loss: 27.7311 - MinusLogProbMetric: 27.7311 - val_loss: 27.9485 - val_MinusLogProbMetric: 27.9485 - lr: 2.5000e-04 - 51s/epoch - 262ms/step
Epoch 792/1000
2023-09-29 14:52:50.722 
Epoch 792/1000 
	 loss: 27.7270, MinusLogProbMetric: 27.7270, val_loss: 28.0070, val_MinusLogProbMetric: 28.0070

Epoch 792: val_loss did not improve from 27.94850
196/196 - 52s - loss: 27.7270 - MinusLogProbMetric: 27.7270 - val_loss: 28.0070 - val_MinusLogProbMetric: 28.0070 - lr: 2.5000e-04 - 52s/epoch - 265ms/step
Epoch 793/1000
2023-09-29 14:53:41.539 
Epoch 793/1000 
	 loss: 27.7509, MinusLogProbMetric: 27.7509, val_loss: 27.9559, val_MinusLogProbMetric: 27.9559

Epoch 793: val_loss did not improve from 27.94850
196/196 - 51s - loss: 27.7509 - MinusLogProbMetric: 27.7509 - val_loss: 27.9559 - val_MinusLogProbMetric: 27.9559 - lr: 2.5000e-04 - 51s/epoch - 259ms/step
Epoch 794/1000
2023-09-29 14:54:31.364 
Epoch 794/1000 
	 loss: 27.7571, MinusLogProbMetric: 27.7571, val_loss: 28.0898, val_MinusLogProbMetric: 28.0898

Epoch 794: val_loss did not improve from 27.94850
196/196 - 50s - loss: 27.7571 - MinusLogProbMetric: 27.7571 - val_loss: 28.0898 - val_MinusLogProbMetric: 28.0898 - lr: 2.5000e-04 - 50s/epoch - 254ms/step
Epoch 795/1000
2023-09-29 14:55:22.332 
Epoch 795/1000 
	 loss: 27.7367, MinusLogProbMetric: 27.7367, val_loss: 28.1059, val_MinusLogProbMetric: 28.1059

Epoch 795: val_loss did not improve from 27.94850
196/196 - 51s - loss: 27.7367 - MinusLogProbMetric: 27.7367 - val_loss: 28.1059 - val_MinusLogProbMetric: 28.1059 - lr: 2.5000e-04 - 51s/epoch - 260ms/step
Epoch 796/1000
2023-09-29 14:56:13.437 
Epoch 796/1000 
	 loss: 27.7621, MinusLogProbMetric: 27.7621, val_loss: 28.0298, val_MinusLogProbMetric: 28.0298

Epoch 796: val_loss did not improve from 27.94850
196/196 - 51s - loss: 27.7621 - MinusLogProbMetric: 27.7621 - val_loss: 28.0298 - val_MinusLogProbMetric: 28.0298 - lr: 2.5000e-04 - 51s/epoch - 261ms/step
Epoch 797/1000
2023-09-29 14:57:04.715 
Epoch 797/1000 
	 loss: 27.7614, MinusLogProbMetric: 27.7614, val_loss: 28.2205, val_MinusLogProbMetric: 28.2205

Epoch 797: val_loss did not improve from 27.94850
196/196 - 51s - loss: 27.7614 - MinusLogProbMetric: 27.7614 - val_loss: 28.2205 - val_MinusLogProbMetric: 28.2205 - lr: 2.5000e-04 - 51s/epoch - 262ms/step
Epoch 798/1000
2023-09-29 14:57:53.634 
Epoch 798/1000 
	 loss: 27.7431, MinusLogProbMetric: 27.7431, val_loss: 28.0417, val_MinusLogProbMetric: 28.0417

Epoch 798: val_loss did not improve from 27.94850
196/196 - 49s - loss: 27.7431 - MinusLogProbMetric: 27.7431 - val_loss: 28.0417 - val_MinusLogProbMetric: 28.0417 - lr: 2.5000e-04 - 49s/epoch - 250ms/step
Epoch 799/1000
2023-09-29 14:58:44.763 
Epoch 799/1000 
	 loss: 27.7443, MinusLogProbMetric: 27.7443, val_loss: 27.9973, val_MinusLogProbMetric: 27.9973

Epoch 799: val_loss did not improve from 27.94850
196/196 - 51s - loss: 27.7443 - MinusLogProbMetric: 27.7443 - val_loss: 27.9973 - val_MinusLogProbMetric: 27.9973 - lr: 2.5000e-04 - 51s/epoch - 261ms/step
Epoch 800/1000
2023-09-29 14:59:39.820 
Epoch 800/1000 
	 loss: 27.7144, MinusLogProbMetric: 27.7144, val_loss: 28.0514, val_MinusLogProbMetric: 28.0514

Epoch 800: val_loss did not improve from 27.94850
196/196 - 55s - loss: 27.7144 - MinusLogProbMetric: 27.7144 - val_loss: 28.0514 - val_MinusLogProbMetric: 28.0514 - lr: 2.5000e-04 - 55s/epoch - 281ms/step
Epoch 801/1000
2023-09-29 15:00:32.163 
Epoch 801/1000 
	 loss: 27.7194, MinusLogProbMetric: 27.7194, val_loss: 27.9647, val_MinusLogProbMetric: 27.9647

Epoch 801: val_loss did not improve from 27.94850
196/196 - 52s - loss: 27.7194 - MinusLogProbMetric: 27.7194 - val_loss: 27.9647 - val_MinusLogProbMetric: 27.9647 - lr: 2.5000e-04 - 52s/epoch - 267ms/step
Epoch 802/1000
2023-09-29 15:01:23.097 
Epoch 802/1000 
	 loss: 27.7119, MinusLogProbMetric: 27.7119, val_loss: 27.9969, val_MinusLogProbMetric: 27.9969

Epoch 802: val_loss did not improve from 27.94850
196/196 - 51s - loss: 27.7119 - MinusLogProbMetric: 27.7119 - val_loss: 27.9969 - val_MinusLogProbMetric: 27.9969 - lr: 2.5000e-04 - 51s/epoch - 260ms/step
Epoch 803/1000
2023-09-29 15:02:13.760 
Epoch 803/1000 
	 loss: 27.7521, MinusLogProbMetric: 27.7521, val_loss: 28.0326, val_MinusLogProbMetric: 28.0326

Epoch 803: val_loss did not improve from 27.94850
196/196 - 51s - loss: 27.7521 - MinusLogProbMetric: 27.7521 - val_loss: 28.0326 - val_MinusLogProbMetric: 28.0326 - lr: 2.5000e-04 - 51s/epoch - 258ms/step
Epoch 804/1000
2023-09-29 15:03:06.161 
Epoch 804/1000 
	 loss: 27.7366, MinusLogProbMetric: 27.7366, val_loss: 28.1321, val_MinusLogProbMetric: 28.1321

Epoch 804: val_loss did not improve from 27.94850
196/196 - 52s - loss: 27.7366 - MinusLogProbMetric: 27.7366 - val_loss: 28.1321 - val_MinusLogProbMetric: 28.1321 - lr: 2.5000e-04 - 52s/epoch - 267ms/step
Epoch 805/1000
2023-09-29 15:03:59.466 
Epoch 805/1000 
	 loss: 27.7273, MinusLogProbMetric: 27.7273, val_loss: 28.2988, val_MinusLogProbMetric: 28.2988

Epoch 805: val_loss did not improve from 27.94850
196/196 - 53s - loss: 27.7273 - MinusLogProbMetric: 27.7273 - val_loss: 28.2988 - val_MinusLogProbMetric: 28.2988 - lr: 2.5000e-04 - 53s/epoch - 272ms/step
Epoch 806/1000
2023-09-29 15:04:52.520 
Epoch 806/1000 
	 loss: 27.7541, MinusLogProbMetric: 27.7541, val_loss: 27.9540, val_MinusLogProbMetric: 27.9540

Epoch 806: val_loss did not improve from 27.94850
196/196 - 53s - loss: 27.7541 - MinusLogProbMetric: 27.7541 - val_loss: 27.9540 - val_MinusLogProbMetric: 27.9540 - lr: 2.5000e-04 - 53s/epoch - 271ms/step
Epoch 807/1000
2023-09-29 15:05:45.664 
Epoch 807/1000 
	 loss: 27.7507, MinusLogProbMetric: 27.7507, val_loss: 27.9641, val_MinusLogProbMetric: 27.9641

Epoch 807: val_loss did not improve from 27.94850
196/196 - 53s - loss: 27.7507 - MinusLogProbMetric: 27.7507 - val_loss: 27.9641 - val_MinusLogProbMetric: 27.9641 - lr: 2.5000e-04 - 53s/epoch - 271ms/step
Epoch 808/1000
2023-09-29 15:06:39.195 
Epoch 808/1000 
	 loss: 27.7432, MinusLogProbMetric: 27.7432, val_loss: 27.9605, val_MinusLogProbMetric: 27.9605

Epoch 808: val_loss did not improve from 27.94850
196/196 - 54s - loss: 27.7432 - MinusLogProbMetric: 27.7432 - val_loss: 27.9605 - val_MinusLogProbMetric: 27.9605 - lr: 2.5000e-04 - 54s/epoch - 273ms/step
Epoch 809/1000
2023-09-29 15:07:30.255 
Epoch 809/1000 
	 loss: 27.7394, MinusLogProbMetric: 27.7394, val_loss: 28.0285, val_MinusLogProbMetric: 28.0285

Epoch 809: val_loss did not improve from 27.94850
196/196 - 51s - loss: 27.7394 - MinusLogProbMetric: 27.7394 - val_loss: 28.0285 - val_MinusLogProbMetric: 28.0285 - lr: 2.5000e-04 - 51s/epoch - 260ms/step
Epoch 810/1000
2023-09-29 15:08:22.007 
Epoch 810/1000 
	 loss: 27.7615, MinusLogProbMetric: 27.7615, val_loss: 27.9697, val_MinusLogProbMetric: 27.9697

Epoch 810: val_loss did not improve from 27.94850
196/196 - 52s - loss: 27.7615 - MinusLogProbMetric: 27.7615 - val_loss: 27.9697 - val_MinusLogProbMetric: 27.9697 - lr: 2.5000e-04 - 52s/epoch - 264ms/step
Epoch 811/1000
2023-09-29 15:09:15.513 
Epoch 811/1000 
	 loss: 27.7527, MinusLogProbMetric: 27.7527, val_loss: 28.2053, val_MinusLogProbMetric: 28.2053

Epoch 811: val_loss did not improve from 27.94850
196/196 - 53s - loss: 27.7527 - MinusLogProbMetric: 27.7527 - val_loss: 28.2053 - val_MinusLogProbMetric: 28.2053 - lr: 2.5000e-04 - 53s/epoch - 273ms/step
Epoch 812/1000
2023-09-29 15:10:07.990 
Epoch 812/1000 
	 loss: 27.7591, MinusLogProbMetric: 27.7591, val_loss: 28.1151, val_MinusLogProbMetric: 28.1151

Epoch 812: val_loss did not improve from 27.94850
196/196 - 52s - loss: 27.7591 - MinusLogProbMetric: 27.7591 - val_loss: 28.1151 - val_MinusLogProbMetric: 28.1151 - lr: 2.5000e-04 - 52s/epoch - 268ms/step
Epoch 813/1000
2023-09-29 15:10:59.503 
Epoch 813/1000 
	 loss: 27.7381, MinusLogProbMetric: 27.7381, val_loss: 28.1234, val_MinusLogProbMetric: 28.1234

Epoch 813: val_loss did not improve from 27.94850
196/196 - 52s - loss: 27.7381 - MinusLogProbMetric: 27.7381 - val_loss: 28.1234 - val_MinusLogProbMetric: 28.1234 - lr: 2.5000e-04 - 52s/epoch - 263ms/step
Epoch 814/1000
2023-09-29 15:11:52.420 
Epoch 814/1000 
	 loss: 27.7377, MinusLogProbMetric: 27.7377, val_loss: 27.9871, val_MinusLogProbMetric: 27.9871

Epoch 814: val_loss did not improve from 27.94850
196/196 - 53s - loss: 27.7377 - MinusLogProbMetric: 27.7377 - val_loss: 27.9871 - val_MinusLogProbMetric: 27.9871 - lr: 2.5000e-04 - 53s/epoch - 270ms/step
Epoch 815/1000
2023-09-29 15:12:44.809 
Epoch 815/1000 
	 loss: 27.7226, MinusLogProbMetric: 27.7226, val_loss: 28.0283, val_MinusLogProbMetric: 28.0283

Epoch 815: val_loss did not improve from 27.94850
196/196 - 52s - loss: 27.7226 - MinusLogProbMetric: 27.7226 - val_loss: 28.0283 - val_MinusLogProbMetric: 28.0283 - lr: 2.5000e-04 - 52s/epoch - 267ms/step
Epoch 816/1000
2023-09-29 15:13:37.456 
Epoch 816/1000 
	 loss: 27.7018, MinusLogProbMetric: 27.7018, val_loss: 28.1107, val_MinusLogProbMetric: 28.1107

Epoch 816: val_loss did not improve from 27.94850
196/196 - 53s - loss: 27.7018 - MinusLogProbMetric: 27.7018 - val_loss: 28.1107 - val_MinusLogProbMetric: 28.1107 - lr: 2.5000e-04 - 53s/epoch - 269ms/step
Epoch 817/1000
2023-09-29 15:14:30.238 
Epoch 817/1000 
	 loss: 27.7228, MinusLogProbMetric: 27.7228, val_loss: 27.9622, val_MinusLogProbMetric: 27.9622

Epoch 817: val_loss did not improve from 27.94850
196/196 - 53s - loss: 27.7228 - MinusLogProbMetric: 27.7228 - val_loss: 27.9622 - val_MinusLogProbMetric: 27.9622 - lr: 2.5000e-04 - 53s/epoch - 269ms/step
Epoch 818/1000
2023-09-29 15:15:21.168 
Epoch 818/1000 
	 loss: 27.7281, MinusLogProbMetric: 27.7281, val_loss: 27.9634, val_MinusLogProbMetric: 27.9634

Epoch 818: val_loss did not improve from 27.94850
196/196 - 51s - loss: 27.7281 - MinusLogProbMetric: 27.7281 - val_loss: 27.9634 - val_MinusLogProbMetric: 27.9634 - lr: 2.5000e-04 - 51s/epoch - 260ms/step
Epoch 819/1000
2023-09-29 15:16:13.118 
Epoch 819/1000 
	 loss: 27.7091, MinusLogProbMetric: 27.7091, val_loss: 28.0194, val_MinusLogProbMetric: 28.0194

Epoch 819: val_loss did not improve from 27.94850
196/196 - 52s - loss: 27.7091 - MinusLogProbMetric: 27.7091 - val_loss: 28.0194 - val_MinusLogProbMetric: 28.0194 - lr: 2.5000e-04 - 52s/epoch - 265ms/step
Epoch 820/1000
2023-09-29 15:17:07.492 
Epoch 820/1000 
	 loss: 27.7133, MinusLogProbMetric: 27.7133, val_loss: 27.9668, val_MinusLogProbMetric: 27.9668

Epoch 820: val_loss did not improve from 27.94850
196/196 - 54s - loss: 27.7133 - MinusLogProbMetric: 27.7133 - val_loss: 27.9668 - val_MinusLogProbMetric: 27.9668 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 821/1000
2023-09-29 15:18:02.700 
Epoch 821/1000 
	 loss: 27.7270, MinusLogProbMetric: 27.7270, val_loss: 27.9715, val_MinusLogProbMetric: 27.9715

Epoch 821: val_loss did not improve from 27.94850
196/196 - 55s - loss: 27.7270 - MinusLogProbMetric: 27.7270 - val_loss: 27.9715 - val_MinusLogProbMetric: 27.9715 - lr: 2.5000e-04 - 55s/epoch - 282ms/step
Epoch 822/1000
2023-09-29 15:18:56.644 
Epoch 822/1000 
	 loss: 27.7257, MinusLogProbMetric: 27.7257, val_loss: 28.1237, val_MinusLogProbMetric: 28.1237

Epoch 822: val_loss did not improve from 27.94850
196/196 - 54s - loss: 27.7257 - MinusLogProbMetric: 27.7257 - val_loss: 28.1237 - val_MinusLogProbMetric: 28.1237 - lr: 2.5000e-04 - 54s/epoch - 275ms/step
Epoch 823/1000
2023-09-29 15:19:53.720 
Epoch 823/1000 
	 loss: 27.7415, MinusLogProbMetric: 27.7415, val_loss: 27.9912, val_MinusLogProbMetric: 27.9912

Epoch 823: val_loss did not improve from 27.94850
196/196 - 57s - loss: 27.7415 - MinusLogProbMetric: 27.7415 - val_loss: 27.9912 - val_MinusLogProbMetric: 27.9912 - lr: 2.5000e-04 - 57s/epoch - 291ms/step
Epoch 824/1000
2023-09-29 15:20:46.880 
Epoch 824/1000 
	 loss: 27.7605, MinusLogProbMetric: 27.7605, val_loss: 27.9950, val_MinusLogProbMetric: 27.9950

Epoch 824: val_loss did not improve from 27.94850
196/196 - 53s - loss: 27.7605 - MinusLogProbMetric: 27.7605 - val_loss: 27.9950 - val_MinusLogProbMetric: 27.9950 - lr: 2.5000e-04 - 53s/epoch - 271ms/step
Epoch 825/1000
2023-09-29 15:21:42.893 
Epoch 825/1000 
	 loss: 27.7302, MinusLogProbMetric: 27.7302, val_loss: 28.1418, val_MinusLogProbMetric: 28.1418

Epoch 825: val_loss did not improve from 27.94850
196/196 - 56s - loss: 27.7302 - MinusLogProbMetric: 27.7302 - val_loss: 28.1418 - val_MinusLogProbMetric: 28.1418 - lr: 2.5000e-04 - 56s/epoch - 286ms/step
Epoch 826/1000
2023-09-29 15:22:36.225 
Epoch 826/1000 
	 loss: 27.7455, MinusLogProbMetric: 27.7455, val_loss: 28.0029, val_MinusLogProbMetric: 28.0029

Epoch 826: val_loss did not improve from 27.94850
196/196 - 53s - loss: 27.7455 - MinusLogProbMetric: 27.7455 - val_loss: 28.0029 - val_MinusLogProbMetric: 28.0029 - lr: 2.5000e-04 - 53s/epoch - 272ms/step
Epoch 827/1000
2023-09-29 15:23:30.831 
Epoch 827/1000 
	 loss: 27.7245, MinusLogProbMetric: 27.7245, val_loss: 27.9928, val_MinusLogProbMetric: 27.9928

Epoch 827: val_loss did not improve from 27.94850
196/196 - 55s - loss: 27.7245 - MinusLogProbMetric: 27.7245 - val_loss: 27.9928 - val_MinusLogProbMetric: 27.9928 - lr: 2.5000e-04 - 55s/epoch - 279ms/step
Epoch 828/1000
2023-09-29 15:24:24.038 
Epoch 828/1000 
	 loss: 27.7284, MinusLogProbMetric: 27.7284, val_loss: 28.0248, val_MinusLogProbMetric: 28.0248

Epoch 828: val_loss did not improve from 27.94850
196/196 - 53s - loss: 27.7284 - MinusLogProbMetric: 27.7284 - val_loss: 28.0248 - val_MinusLogProbMetric: 28.0248 - lr: 2.5000e-04 - 53s/epoch - 271ms/step
Epoch 829/1000
2023-09-29 15:25:17.071 
Epoch 829/1000 
	 loss: 27.7264, MinusLogProbMetric: 27.7264, val_loss: 28.3730, val_MinusLogProbMetric: 28.3730

Epoch 829: val_loss did not improve from 27.94850
196/196 - 53s - loss: 27.7264 - MinusLogProbMetric: 27.7264 - val_loss: 28.3730 - val_MinusLogProbMetric: 28.3730 - lr: 2.5000e-04 - 53s/epoch - 271ms/step
Epoch 830/1000
2023-09-29 15:26:12.330 
Epoch 830/1000 
	 loss: 27.7289, MinusLogProbMetric: 27.7289, val_loss: 27.9422, val_MinusLogProbMetric: 27.9422

Epoch 830: val_loss improved from 27.94850 to 27.94218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 56s - loss: 27.7289 - MinusLogProbMetric: 27.7289 - val_loss: 27.9422 - val_MinusLogProbMetric: 27.9422 - lr: 2.5000e-04 - 56s/epoch - 286ms/step
Epoch 831/1000
2023-09-29 15:27:06.254 
Epoch 831/1000 
	 loss: 27.7165, MinusLogProbMetric: 27.7165, val_loss: 28.1257, val_MinusLogProbMetric: 28.1257

Epoch 831: val_loss did not improve from 27.94218
196/196 - 53s - loss: 27.7165 - MinusLogProbMetric: 27.7165 - val_loss: 28.1257 - val_MinusLogProbMetric: 28.1257 - lr: 2.5000e-04 - 53s/epoch - 271ms/step
Epoch 832/1000
2023-09-29 15:28:01.417 
Epoch 832/1000 
	 loss: 27.7324, MinusLogProbMetric: 27.7324, val_loss: 28.1793, val_MinusLogProbMetric: 28.1793

Epoch 832: val_loss did not improve from 27.94218
196/196 - 55s - loss: 27.7324 - MinusLogProbMetric: 27.7324 - val_loss: 28.1793 - val_MinusLogProbMetric: 28.1793 - lr: 2.5000e-04 - 55s/epoch - 281ms/step
Epoch 833/1000
2023-09-29 15:28:56.870 
Epoch 833/1000 
	 loss: 27.7372, MinusLogProbMetric: 27.7372, val_loss: 27.9451, val_MinusLogProbMetric: 27.9451

Epoch 833: val_loss did not improve from 27.94218
196/196 - 55s - loss: 27.7372 - MinusLogProbMetric: 27.7372 - val_loss: 27.9451 - val_MinusLogProbMetric: 27.9451 - lr: 2.5000e-04 - 55s/epoch - 283ms/step
Epoch 834/1000
2023-09-29 15:29:50.218 
Epoch 834/1000 
	 loss: 27.7222, MinusLogProbMetric: 27.7222, val_loss: 27.9879, val_MinusLogProbMetric: 27.9879

Epoch 834: val_loss did not improve from 27.94218
196/196 - 53s - loss: 27.7222 - MinusLogProbMetric: 27.7222 - val_loss: 27.9879 - val_MinusLogProbMetric: 27.9879 - lr: 2.5000e-04 - 53s/epoch - 272ms/step
Epoch 835/1000
2023-09-29 15:30:47.936 
Epoch 835/1000 
	 loss: 27.7359, MinusLogProbMetric: 27.7359, val_loss: 28.1778, val_MinusLogProbMetric: 28.1778

Epoch 835: val_loss did not improve from 27.94218
196/196 - 58s - loss: 27.7359 - MinusLogProbMetric: 27.7359 - val_loss: 28.1778 - val_MinusLogProbMetric: 28.1778 - lr: 2.5000e-04 - 58s/epoch - 294ms/step
Epoch 836/1000
2023-09-29 15:31:42.069 
Epoch 836/1000 
	 loss: 27.7914, MinusLogProbMetric: 27.7914, val_loss: 28.0884, val_MinusLogProbMetric: 28.0884

Epoch 836: val_loss did not improve from 27.94218
196/196 - 54s - loss: 27.7914 - MinusLogProbMetric: 27.7914 - val_loss: 28.0884 - val_MinusLogProbMetric: 28.0884 - lr: 2.5000e-04 - 54s/epoch - 276ms/step
Epoch 837/1000
2023-09-29 15:32:38.005 
Epoch 837/1000 
	 loss: 27.7178, MinusLogProbMetric: 27.7178, val_loss: 27.9608, val_MinusLogProbMetric: 27.9608

Epoch 837: val_loss did not improve from 27.94218
196/196 - 56s - loss: 27.7178 - MinusLogProbMetric: 27.7178 - val_loss: 27.9608 - val_MinusLogProbMetric: 27.9608 - lr: 2.5000e-04 - 56s/epoch - 285ms/step
Epoch 838/1000
2023-09-29 15:33:33.440 
Epoch 838/1000 
	 loss: 27.7412, MinusLogProbMetric: 27.7412, val_loss: 27.9856, val_MinusLogProbMetric: 27.9856

Epoch 838: val_loss did not improve from 27.94218
196/196 - 55s - loss: 27.7412 - MinusLogProbMetric: 27.7412 - val_loss: 27.9856 - val_MinusLogProbMetric: 27.9856 - lr: 2.5000e-04 - 55s/epoch - 283ms/step
Epoch 839/1000
2023-09-29 15:34:29.001 
Epoch 839/1000 
	 loss: 27.7211, MinusLogProbMetric: 27.7211, val_loss: 27.9696, val_MinusLogProbMetric: 27.9696

Epoch 839: val_loss did not improve from 27.94218
196/196 - 56s - loss: 27.7211 - MinusLogProbMetric: 27.7211 - val_loss: 27.9696 - val_MinusLogProbMetric: 27.9696 - lr: 2.5000e-04 - 56s/epoch - 283ms/step
Epoch 840/1000
2023-09-29 15:35:23.655 
Epoch 840/1000 
	 loss: 27.7446, MinusLogProbMetric: 27.7446, val_loss: 28.0056, val_MinusLogProbMetric: 28.0056

Epoch 840: val_loss did not improve from 27.94218
196/196 - 55s - loss: 27.7446 - MinusLogProbMetric: 27.7446 - val_loss: 28.0056 - val_MinusLogProbMetric: 28.0056 - lr: 2.5000e-04 - 55s/epoch - 279ms/step
Epoch 841/1000
2023-09-29 15:36:21.357 
Epoch 841/1000 
	 loss: 27.7198, MinusLogProbMetric: 27.7198, val_loss: 27.9772, val_MinusLogProbMetric: 27.9772

Epoch 841: val_loss did not improve from 27.94218
196/196 - 58s - loss: 27.7198 - MinusLogProbMetric: 27.7198 - val_loss: 27.9772 - val_MinusLogProbMetric: 27.9772 - lr: 2.5000e-04 - 58s/epoch - 294ms/step
Epoch 842/1000
2023-09-29 15:37:15.884 
Epoch 842/1000 
	 loss: 27.7189, MinusLogProbMetric: 27.7189, val_loss: 28.2149, val_MinusLogProbMetric: 28.2149

Epoch 842: val_loss did not improve from 27.94218
196/196 - 55s - loss: 27.7189 - MinusLogProbMetric: 27.7189 - val_loss: 28.2149 - val_MinusLogProbMetric: 28.2149 - lr: 2.5000e-04 - 55s/epoch - 278ms/step
Epoch 843/1000
2023-09-29 15:38:10.197 
Epoch 843/1000 
	 loss: 27.7542, MinusLogProbMetric: 27.7542, val_loss: 28.3866, val_MinusLogProbMetric: 28.3866

Epoch 843: val_loss did not improve from 27.94218
196/196 - 54s - loss: 27.7542 - MinusLogProbMetric: 27.7542 - val_loss: 28.3866 - val_MinusLogProbMetric: 28.3866 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 844/1000
2023-09-29 15:39:05.715 
Epoch 844/1000 
	 loss: 27.7313, MinusLogProbMetric: 27.7313, val_loss: 27.9835, val_MinusLogProbMetric: 27.9835

Epoch 844: val_loss did not improve from 27.94218
196/196 - 55s - loss: 27.7313 - MinusLogProbMetric: 27.7313 - val_loss: 27.9835 - val_MinusLogProbMetric: 27.9835 - lr: 2.5000e-04 - 55s/epoch - 283ms/step
Epoch 845/1000
2023-09-29 15:40:01.136 
Epoch 845/1000 
	 loss: 27.7333, MinusLogProbMetric: 27.7333, val_loss: 28.0671, val_MinusLogProbMetric: 28.0671

Epoch 845: val_loss did not improve from 27.94218
196/196 - 55s - loss: 27.7333 - MinusLogProbMetric: 27.7333 - val_loss: 28.0671 - val_MinusLogProbMetric: 28.0671 - lr: 2.5000e-04 - 55s/epoch - 283ms/step
Epoch 846/1000
2023-09-29 15:40:56.483 
Epoch 846/1000 
	 loss: 27.7413, MinusLogProbMetric: 27.7413, val_loss: 28.0972, val_MinusLogProbMetric: 28.0972

Epoch 846: val_loss did not improve from 27.94218
196/196 - 55s - loss: 27.7413 - MinusLogProbMetric: 27.7413 - val_loss: 28.0972 - val_MinusLogProbMetric: 28.0972 - lr: 2.5000e-04 - 55s/epoch - 282ms/step
Epoch 847/1000
2023-09-29 15:41:51.719 
Epoch 847/1000 
	 loss: 27.7424, MinusLogProbMetric: 27.7424, val_loss: 28.1584, val_MinusLogProbMetric: 28.1584

Epoch 847: val_loss did not improve from 27.94218
196/196 - 55s - loss: 27.7424 - MinusLogProbMetric: 27.7424 - val_loss: 28.1584 - val_MinusLogProbMetric: 28.1584 - lr: 2.5000e-04 - 55s/epoch - 282ms/step
Epoch 848/1000
2023-09-29 15:42:46.718 
Epoch 848/1000 
	 loss: 27.7219, MinusLogProbMetric: 27.7219, val_loss: 27.9986, val_MinusLogProbMetric: 27.9986

Epoch 848: val_loss did not improve from 27.94218
196/196 - 55s - loss: 27.7219 - MinusLogProbMetric: 27.7219 - val_loss: 27.9986 - val_MinusLogProbMetric: 27.9986 - lr: 2.5000e-04 - 55s/epoch - 281ms/step
Epoch 849/1000
2023-09-29 15:43:41.987 
Epoch 849/1000 
	 loss: 27.7530, MinusLogProbMetric: 27.7530, val_loss: 28.0810, val_MinusLogProbMetric: 28.0810

Epoch 849: val_loss did not improve from 27.94218
196/196 - 55s - loss: 27.7530 - MinusLogProbMetric: 27.7530 - val_loss: 28.0810 - val_MinusLogProbMetric: 28.0810 - lr: 2.5000e-04 - 55s/epoch - 282ms/step
Epoch 850/1000
2023-09-29 15:44:36.141 
Epoch 850/1000 
	 loss: 27.7626, MinusLogProbMetric: 27.7626, val_loss: 28.0412, val_MinusLogProbMetric: 28.0412

Epoch 850: val_loss did not improve from 27.94218
196/196 - 54s - loss: 27.7626 - MinusLogProbMetric: 27.7626 - val_loss: 28.0412 - val_MinusLogProbMetric: 28.0412 - lr: 2.5000e-04 - 54s/epoch - 276ms/step
Epoch 851/1000
2023-09-29 15:45:31.172 
Epoch 851/1000 
	 loss: 27.7372, MinusLogProbMetric: 27.7372, val_loss: 27.9705, val_MinusLogProbMetric: 27.9705

Epoch 851: val_loss did not improve from 27.94218
196/196 - 55s - loss: 27.7372 - MinusLogProbMetric: 27.7372 - val_loss: 27.9705 - val_MinusLogProbMetric: 27.9705 - lr: 2.5000e-04 - 55s/epoch - 281ms/step
Epoch 852/1000
2023-09-29 15:46:25.555 
Epoch 852/1000 
	 loss: 27.7504, MinusLogProbMetric: 27.7504, val_loss: 27.9788, val_MinusLogProbMetric: 27.9788

Epoch 852: val_loss did not improve from 27.94218
196/196 - 54s - loss: 27.7504 - MinusLogProbMetric: 27.7504 - val_loss: 27.9788 - val_MinusLogProbMetric: 27.9788 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 853/1000
2023-09-29 15:47:18.884 
Epoch 853/1000 
	 loss: 27.7689, MinusLogProbMetric: 27.7689, val_loss: 28.1470, val_MinusLogProbMetric: 28.1470

Epoch 853: val_loss did not improve from 27.94218
196/196 - 53s - loss: 27.7689 - MinusLogProbMetric: 27.7689 - val_loss: 28.1470 - val_MinusLogProbMetric: 28.1470 - lr: 2.5000e-04 - 53s/epoch - 272ms/step
Epoch 854/1000
2023-09-29 15:48:12.898 
Epoch 854/1000 
	 loss: 27.7521, MinusLogProbMetric: 27.7521, val_loss: 28.0806, val_MinusLogProbMetric: 28.0806

Epoch 854: val_loss did not improve from 27.94218
196/196 - 54s - loss: 27.7521 - MinusLogProbMetric: 27.7521 - val_loss: 28.0806 - val_MinusLogProbMetric: 28.0806 - lr: 2.5000e-04 - 54s/epoch - 276ms/step
Epoch 855/1000
2023-09-29 15:49:07.260 
Epoch 855/1000 
	 loss: 27.7469, MinusLogProbMetric: 27.7469, val_loss: 27.9866, val_MinusLogProbMetric: 27.9866

Epoch 855: val_loss did not improve from 27.94218
196/196 - 54s - loss: 27.7469 - MinusLogProbMetric: 27.7469 - val_loss: 27.9866 - val_MinusLogProbMetric: 27.9866 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 856/1000
2023-09-29 15:50:01.665 
Epoch 856/1000 
	 loss: 27.7361, MinusLogProbMetric: 27.7361, val_loss: 28.0522, val_MinusLogProbMetric: 28.0522

Epoch 856: val_loss did not improve from 27.94218
196/196 - 54s - loss: 27.7361 - MinusLogProbMetric: 27.7361 - val_loss: 28.0522 - val_MinusLogProbMetric: 28.0522 - lr: 2.5000e-04 - 54s/epoch - 278ms/step
Epoch 857/1000
2023-09-29 15:50:55.938 
Epoch 857/1000 
	 loss: 27.7359, MinusLogProbMetric: 27.7359, val_loss: 27.9612, val_MinusLogProbMetric: 27.9612

Epoch 857: val_loss did not improve from 27.94218
196/196 - 54s - loss: 27.7359 - MinusLogProbMetric: 27.7359 - val_loss: 27.9612 - val_MinusLogProbMetric: 27.9612 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 858/1000
2023-09-29 15:51:50.200 
Epoch 858/1000 
	 loss: 27.7311, MinusLogProbMetric: 27.7311, val_loss: 27.9340, val_MinusLogProbMetric: 27.9340

Epoch 858: val_loss improved from 27.94218 to 27.93398, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 27.7311 - MinusLogProbMetric: 27.7311 - val_loss: 27.9340 - val_MinusLogProbMetric: 27.9340 - lr: 2.5000e-04 - 55s/epoch - 281ms/step
Epoch 859/1000
2023-09-29 15:52:44.870 
Epoch 859/1000 
	 loss: 27.7388, MinusLogProbMetric: 27.7388, val_loss: 28.0263, val_MinusLogProbMetric: 28.0263

Epoch 859: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7388 - MinusLogProbMetric: 27.7388 - val_loss: 28.0263 - val_MinusLogProbMetric: 28.0263 - lr: 2.5000e-04 - 54s/epoch - 275ms/step
Epoch 860/1000
2023-09-29 15:53:39.078 
Epoch 860/1000 
	 loss: 27.7801, MinusLogProbMetric: 27.7801, val_loss: 27.9939, val_MinusLogProbMetric: 27.9939

Epoch 860: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7801 - MinusLogProbMetric: 27.7801 - val_loss: 27.9939 - val_MinusLogProbMetric: 27.9939 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 861/1000
2023-09-29 15:54:33.415 
Epoch 861/1000 
	 loss: 27.7292, MinusLogProbMetric: 27.7292, val_loss: 27.9890, val_MinusLogProbMetric: 27.9890

Epoch 861: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7292 - MinusLogProbMetric: 27.7292 - val_loss: 27.9890 - val_MinusLogProbMetric: 27.9890 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 862/1000
2023-09-29 15:55:27.675 
Epoch 862/1000 
	 loss: 27.7626, MinusLogProbMetric: 27.7626, val_loss: 28.0019, val_MinusLogProbMetric: 28.0019

Epoch 862: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7626 - MinusLogProbMetric: 27.7626 - val_loss: 28.0019 - val_MinusLogProbMetric: 28.0019 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 863/1000
2023-09-29 15:56:22.092 
Epoch 863/1000 
	 loss: 27.7143, MinusLogProbMetric: 27.7143, val_loss: 28.0328, val_MinusLogProbMetric: 28.0328

Epoch 863: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7143 - MinusLogProbMetric: 27.7143 - val_loss: 28.0328 - val_MinusLogProbMetric: 28.0328 - lr: 2.5000e-04 - 54s/epoch - 278ms/step
Epoch 864/1000
2023-09-29 15:57:15.359 
Epoch 864/1000 
	 loss: 27.7525, MinusLogProbMetric: 27.7525, val_loss: 27.9730, val_MinusLogProbMetric: 27.9730

Epoch 864: val_loss did not improve from 27.93398
196/196 - 53s - loss: 27.7525 - MinusLogProbMetric: 27.7525 - val_loss: 27.9730 - val_MinusLogProbMetric: 27.9730 - lr: 2.5000e-04 - 53s/epoch - 272ms/step
Epoch 865/1000
2023-09-29 15:58:09.303 
Epoch 865/1000 
	 loss: 27.7440, MinusLogProbMetric: 27.7440, val_loss: 27.9565, val_MinusLogProbMetric: 27.9565

Epoch 865: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7440 - MinusLogProbMetric: 27.7440 - val_loss: 27.9565 - val_MinusLogProbMetric: 27.9565 - lr: 2.5000e-04 - 54s/epoch - 275ms/step
Epoch 866/1000
2023-09-29 15:59:03.602 
Epoch 866/1000 
	 loss: 27.7312, MinusLogProbMetric: 27.7312, val_loss: 27.9830, val_MinusLogProbMetric: 27.9830

Epoch 866: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7312 - MinusLogProbMetric: 27.7312 - val_loss: 27.9830 - val_MinusLogProbMetric: 27.9830 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 867/1000
2023-09-29 16:00:00.071 
Epoch 867/1000 
	 loss: 27.7364, MinusLogProbMetric: 27.7364, val_loss: 27.9813, val_MinusLogProbMetric: 27.9813

Epoch 867: val_loss did not improve from 27.93398
196/196 - 56s - loss: 27.7364 - MinusLogProbMetric: 27.7364 - val_loss: 27.9813 - val_MinusLogProbMetric: 27.9813 - lr: 2.5000e-04 - 56s/epoch - 288ms/step
Epoch 868/1000
2023-09-29 16:00:55.567 
Epoch 868/1000 
	 loss: 27.7428, MinusLogProbMetric: 27.7428, val_loss: 27.9868, val_MinusLogProbMetric: 27.9868

Epoch 868: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.7428 - MinusLogProbMetric: 27.7428 - val_loss: 27.9868 - val_MinusLogProbMetric: 27.9868 - lr: 2.5000e-04 - 55s/epoch - 283ms/step
Epoch 869/1000
2023-09-29 16:01:52.846 
Epoch 869/1000 
	 loss: 27.7491, MinusLogProbMetric: 27.7491, val_loss: 28.1742, val_MinusLogProbMetric: 28.1742

Epoch 869: val_loss did not improve from 27.93398
196/196 - 57s - loss: 27.7491 - MinusLogProbMetric: 27.7491 - val_loss: 28.1742 - val_MinusLogProbMetric: 28.1742 - lr: 2.5000e-04 - 57s/epoch - 292ms/step
Epoch 870/1000
2023-09-29 16:02:46.367 
Epoch 870/1000 
	 loss: 27.7677, MinusLogProbMetric: 27.7677, val_loss: 28.0095, val_MinusLogProbMetric: 28.0095

Epoch 870: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7677 - MinusLogProbMetric: 27.7677 - val_loss: 28.0095 - val_MinusLogProbMetric: 28.0095 - lr: 2.5000e-04 - 54s/epoch - 273ms/step
Epoch 871/1000
2023-09-29 16:03:43.256 
Epoch 871/1000 
	 loss: 27.7918, MinusLogProbMetric: 27.7918, val_loss: 28.0395, val_MinusLogProbMetric: 28.0395

Epoch 871: val_loss did not improve from 27.93398
196/196 - 57s - loss: 27.7918 - MinusLogProbMetric: 27.7918 - val_loss: 28.0395 - val_MinusLogProbMetric: 28.0395 - lr: 2.5000e-04 - 57s/epoch - 290ms/step
Epoch 872/1000
2023-09-29 16:04:37.728 
Epoch 872/1000 
	 loss: 27.7343, MinusLogProbMetric: 27.7343, val_loss: 27.9451, val_MinusLogProbMetric: 27.9451

Epoch 872: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7343 - MinusLogProbMetric: 27.7343 - val_loss: 27.9451 - val_MinusLogProbMetric: 27.9451 - lr: 2.5000e-04 - 54s/epoch - 278ms/step
Epoch 873/1000
2023-09-29 16:05:32.737 
Epoch 873/1000 
	 loss: 27.7354, MinusLogProbMetric: 27.7354, val_loss: 27.9527, val_MinusLogProbMetric: 27.9527

Epoch 873: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.7354 - MinusLogProbMetric: 27.7354 - val_loss: 27.9527 - val_MinusLogProbMetric: 27.9527 - lr: 2.5000e-04 - 55s/epoch - 281ms/step
Epoch 874/1000
2023-09-29 16:06:29.916 
Epoch 874/1000 
	 loss: 27.7283, MinusLogProbMetric: 27.7283, val_loss: 27.9617, val_MinusLogProbMetric: 27.9617

Epoch 874: val_loss did not improve from 27.93398
196/196 - 57s - loss: 27.7283 - MinusLogProbMetric: 27.7283 - val_loss: 27.9617 - val_MinusLogProbMetric: 27.9617 - lr: 2.5000e-04 - 57s/epoch - 292ms/step
Epoch 875/1000
2023-09-29 16:07:24.235 
Epoch 875/1000 
	 loss: 27.7371, MinusLogProbMetric: 27.7371, val_loss: 28.0774, val_MinusLogProbMetric: 28.0774

Epoch 875: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7371 - MinusLogProbMetric: 27.7371 - val_loss: 28.0774 - val_MinusLogProbMetric: 28.0774 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 876/1000
2023-09-29 16:08:18.919 
Epoch 876/1000 
	 loss: 27.7145, MinusLogProbMetric: 27.7145, val_loss: 28.0282, val_MinusLogProbMetric: 28.0282

Epoch 876: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.7145 - MinusLogProbMetric: 27.7145 - val_loss: 28.0282 - val_MinusLogProbMetric: 28.0282 - lr: 2.5000e-04 - 55s/epoch - 279ms/step
Epoch 877/1000
2023-09-29 16:09:13.705 
Epoch 877/1000 
	 loss: 27.7030, MinusLogProbMetric: 27.7030, val_loss: 27.9826, val_MinusLogProbMetric: 27.9826

Epoch 877: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.7030 - MinusLogProbMetric: 27.7030 - val_loss: 27.9826 - val_MinusLogProbMetric: 27.9826 - lr: 2.5000e-04 - 55s/epoch - 280ms/step
Epoch 878/1000
2023-09-29 16:10:08.382 
Epoch 878/1000 
	 loss: 27.6904, MinusLogProbMetric: 27.6904, val_loss: 27.9830, val_MinusLogProbMetric: 27.9830

Epoch 878: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.6904 - MinusLogProbMetric: 27.6904 - val_loss: 27.9830 - val_MinusLogProbMetric: 27.9830 - lr: 2.5000e-04 - 55s/epoch - 279ms/step
Epoch 879/1000
2023-09-29 16:11:03.461 
Epoch 879/1000 
	 loss: 27.7430, MinusLogProbMetric: 27.7430, val_loss: 28.0081, val_MinusLogProbMetric: 28.0081

Epoch 879: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.7430 - MinusLogProbMetric: 27.7430 - val_loss: 28.0081 - val_MinusLogProbMetric: 28.0081 - lr: 2.5000e-04 - 55s/epoch - 281ms/step
Epoch 880/1000
2023-09-29 16:11:58.238 
Epoch 880/1000 
	 loss: 27.7338, MinusLogProbMetric: 27.7338, val_loss: 27.9426, val_MinusLogProbMetric: 27.9426

Epoch 880: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.7338 - MinusLogProbMetric: 27.7338 - val_loss: 27.9426 - val_MinusLogProbMetric: 27.9426 - lr: 2.5000e-04 - 55s/epoch - 279ms/step
Epoch 881/1000
2023-09-29 16:12:52.783 
Epoch 881/1000 
	 loss: 27.7216, MinusLogProbMetric: 27.7216, val_loss: 28.0291, val_MinusLogProbMetric: 28.0291

Epoch 881: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.7216 - MinusLogProbMetric: 27.7216 - val_loss: 28.0291 - val_MinusLogProbMetric: 28.0291 - lr: 2.5000e-04 - 55s/epoch - 278ms/step
Epoch 882/1000
2023-09-29 16:13:47.084 
Epoch 882/1000 
	 loss: 27.7243, MinusLogProbMetric: 27.7243, val_loss: 27.9357, val_MinusLogProbMetric: 27.9357

Epoch 882: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7243 - MinusLogProbMetric: 27.7243 - val_loss: 27.9357 - val_MinusLogProbMetric: 27.9357 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 883/1000
2023-09-29 16:14:43.434 
Epoch 883/1000 
	 loss: 27.7264, MinusLogProbMetric: 27.7264, val_loss: 27.9955, val_MinusLogProbMetric: 27.9955

Epoch 883: val_loss did not improve from 27.93398
196/196 - 56s - loss: 27.7264 - MinusLogProbMetric: 27.7264 - val_loss: 27.9955 - val_MinusLogProbMetric: 27.9955 - lr: 2.5000e-04 - 56s/epoch - 287ms/step
Epoch 884/1000
2023-09-29 16:15:38.018 
Epoch 884/1000 
	 loss: 27.7407, MinusLogProbMetric: 27.7407, val_loss: 28.1232, val_MinusLogProbMetric: 28.1232

Epoch 884: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.7407 - MinusLogProbMetric: 27.7407 - val_loss: 28.1232 - val_MinusLogProbMetric: 28.1232 - lr: 2.5000e-04 - 55s/epoch - 278ms/step
Epoch 885/1000
2023-09-29 16:16:33.890 
Epoch 885/1000 
	 loss: 27.7258, MinusLogProbMetric: 27.7258, val_loss: 28.0310, val_MinusLogProbMetric: 28.0310

Epoch 885: val_loss did not improve from 27.93398
196/196 - 56s - loss: 27.7258 - MinusLogProbMetric: 27.7258 - val_loss: 28.0310 - val_MinusLogProbMetric: 28.0310 - lr: 2.5000e-04 - 56s/epoch - 285ms/step
Epoch 886/1000
2023-09-29 16:17:27.331 
Epoch 886/1000 
	 loss: 27.7254, MinusLogProbMetric: 27.7254, val_loss: 28.0345, val_MinusLogProbMetric: 28.0345

Epoch 886: val_loss did not improve from 27.93398
196/196 - 53s - loss: 27.7254 - MinusLogProbMetric: 27.7254 - val_loss: 28.0345 - val_MinusLogProbMetric: 28.0345 - lr: 2.5000e-04 - 53s/epoch - 273ms/step
Epoch 887/1000
2023-09-29 16:18:23.338 
Epoch 887/1000 
	 loss: 27.7160, MinusLogProbMetric: 27.7160, val_loss: 28.0185, val_MinusLogProbMetric: 28.0185

Epoch 887: val_loss did not improve from 27.93398
196/196 - 56s - loss: 27.7160 - MinusLogProbMetric: 27.7160 - val_loss: 28.0185 - val_MinusLogProbMetric: 28.0185 - lr: 2.5000e-04 - 56s/epoch - 286ms/step
Epoch 888/1000
2023-09-29 16:19:19.081 
Epoch 888/1000 
	 loss: 27.7232, MinusLogProbMetric: 27.7232, val_loss: 28.0031, val_MinusLogProbMetric: 28.0031

Epoch 888: val_loss did not improve from 27.93398
196/196 - 56s - loss: 27.7232 - MinusLogProbMetric: 27.7232 - val_loss: 28.0031 - val_MinusLogProbMetric: 28.0031 - lr: 2.5000e-04 - 56s/epoch - 284ms/step
Epoch 889/1000
2023-09-29 16:20:13.405 
Epoch 889/1000 
	 loss: 27.7349, MinusLogProbMetric: 27.7349, val_loss: 28.0727, val_MinusLogProbMetric: 28.0727

Epoch 889: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7349 - MinusLogProbMetric: 27.7349 - val_loss: 28.0727 - val_MinusLogProbMetric: 28.0727 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 890/1000
2023-09-29 16:21:09.178 
Epoch 890/1000 
	 loss: 27.7083, MinusLogProbMetric: 27.7083, val_loss: 27.9509, val_MinusLogProbMetric: 27.9509

Epoch 890: val_loss did not improve from 27.93398
196/196 - 56s - loss: 27.7083 - MinusLogProbMetric: 27.7083 - val_loss: 27.9509 - val_MinusLogProbMetric: 27.9509 - lr: 2.5000e-04 - 56s/epoch - 285ms/step
Epoch 891/1000
2023-09-29 16:22:03.302 
Epoch 891/1000 
	 loss: 27.7171, MinusLogProbMetric: 27.7171, val_loss: 28.0927, val_MinusLogProbMetric: 28.0927

Epoch 891: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7171 - MinusLogProbMetric: 27.7171 - val_loss: 28.0927 - val_MinusLogProbMetric: 28.0927 - lr: 2.5000e-04 - 54s/epoch - 276ms/step
Epoch 892/1000
2023-09-29 16:22:56.981 
Epoch 892/1000 
	 loss: 27.7371, MinusLogProbMetric: 27.7371, val_loss: 28.0670, val_MinusLogProbMetric: 28.0670

Epoch 892: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7371 - MinusLogProbMetric: 27.7371 - val_loss: 28.0670 - val_MinusLogProbMetric: 28.0670 - lr: 2.5000e-04 - 54s/epoch - 274ms/step
Epoch 893/1000
2023-09-29 16:23:53.920 
Epoch 893/1000 
	 loss: 27.7055, MinusLogProbMetric: 27.7055, val_loss: 27.9779, val_MinusLogProbMetric: 27.9779

Epoch 893: val_loss did not improve from 27.93398
196/196 - 57s - loss: 27.7055 - MinusLogProbMetric: 27.7055 - val_loss: 27.9779 - val_MinusLogProbMetric: 27.9779 - lr: 2.5000e-04 - 57s/epoch - 290ms/step
Epoch 894/1000
2023-09-29 16:24:48.228 
Epoch 894/1000 
	 loss: 27.7135, MinusLogProbMetric: 27.7135, val_loss: 27.9682, val_MinusLogProbMetric: 27.9682

Epoch 894: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7135 - MinusLogProbMetric: 27.7135 - val_loss: 27.9682 - val_MinusLogProbMetric: 27.9682 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 895/1000
2023-09-29 16:25:42.498 
Epoch 895/1000 
	 loss: 27.7251, MinusLogProbMetric: 27.7251, val_loss: 28.2683, val_MinusLogProbMetric: 28.2683

Epoch 895: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7251 - MinusLogProbMetric: 27.7251 - val_loss: 28.2683 - val_MinusLogProbMetric: 28.2683 - lr: 2.5000e-04 - 54s/epoch - 277ms/step
Epoch 896/1000
2023-09-29 16:26:39.934 
Epoch 896/1000 
	 loss: 27.7078, MinusLogProbMetric: 27.7078, val_loss: 28.0156, val_MinusLogProbMetric: 28.0156

Epoch 896: val_loss did not improve from 27.93398
196/196 - 57s - loss: 27.7078 - MinusLogProbMetric: 27.7078 - val_loss: 28.0156 - val_MinusLogProbMetric: 28.0156 - lr: 2.5000e-04 - 57s/epoch - 293ms/step
Epoch 897/1000
2023-09-29 16:27:34.540 
Epoch 897/1000 
	 loss: 27.7441, MinusLogProbMetric: 27.7441, val_loss: 28.0292, val_MinusLogProbMetric: 28.0292

Epoch 897: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.7441 - MinusLogProbMetric: 27.7441 - val_loss: 28.0292 - val_MinusLogProbMetric: 28.0292 - lr: 2.5000e-04 - 55s/epoch - 279ms/step
Epoch 898/1000
2023-09-29 16:28:28.955 
Epoch 898/1000 
	 loss: 27.6947, MinusLogProbMetric: 27.6947, val_loss: 28.0267, val_MinusLogProbMetric: 28.0267

Epoch 898: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.6947 - MinusLogProbMetric: 27.6947 - val_loss: 28.0267 - val_MinusLogProbMetric: 28.0267 - lr: 2.5000e-04 - 54s/epoch - 278ms/step
Epoch 899/1000
2023-09-29 16:29:23.779 
Epoch 899/1000 
	 loss: 27.7168, MinusLogProbMetric: 27.7168, val_loss: 27.9558, val_MinusLogProbMetric: 27.9558

Epoch 899: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.7168 - MinusLogProbMetric: 27.7168 - val_loss: 27.9558 - val_MinusLogProbMetric: 27.9558 - lr: 2.5000e-04 - 55s/epoch - 280ms/step
Epoch 900/1000
2023-09-29 16:30:18.500 
Epoch 900/1000 
	 loss: 27.6949, MinusLogProbMetric: 27.6949, val_loss: 28.0084, val_MinusLogProbMetric: 28.0084

Epoch 900: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.6949 - MinusLogProbMetric: 27.6949 - val_loss: 28.0084 - val_MinusLogProbMetric: 28.0084 - lr: 2.5000e-04 - 55s/epoch - 279ms/step
Epoch 901/1000
2023-09-29 16:31:13.188 
Epoch 901/1000 
	 loss: 27.6998, MinusLogProbMetric: 27.6998, val_loss: 28.0210, val_MinusLogProbMetric: 28.0210

Epoch 901: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.6998 - MinusLogProbMetric: 27.6998 - val_loss: 28.0210 - val_MinusLogProbMetric: 28.0210 - lr: 2.5000e-04 - 55s/epoch - 279ms/step
Epoch 902/1000
2023-09-29 16:32:06.374 
Epoch 902/1000 
	 loss: 27.7135, MinusLogProbMetric: 27.7135, val_loss: 27.9620, val_MinusLogProbMetric: 27.9620

Epoch 902: val_loss did not improve from 27.93398
196/196 - 53s - loss: 27.7135 - MinusLogProbMetric: 27.7135 - val_loss: 27.9620 - val_MinusLogProbMetric: 27.9620 - lr: 2.5000e-04 - 53s/epoch - 271ms/step
Epoch 903/1000
2023-09-29 16:33:00.920 
Epoch 903/1000 
	 loss: 27.7147, MinusLogProbMetric: 27.7147, val_loss: 27.9569, val_MinusLogProbMetric: 27.9569

Epoch 903: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.7147 - MinusLogProbMetric: 27.7147 - val_loss: 27.9569 - val_MinusLogProbMetric: 27.9569 - lr: 2.5000e-04 - 55s/epoch - 278ms/step
Epoch 904/1000
2023-09-29 16:33:55.775 
Epoch 904/1000 
	 loss: 27.7137, MinusLogProbMetric: 27.7137, val_loss: 27.9711, val_MinusLogProbMetric: 27.9711

Epoch 904: val_loss did not improve from 27.93398
196/196 - 55s - loss: 27.7137 - MinusLogProbMetric: 27.7137 - val_loss: 27.9711 - val_MinusLogProbMetric: 27.9711 - lr: 2.5000e-04 - 55s/epoch - 280ms/step
Epoch 905/1000
2023-09-29 16:34:48.792 
Epoch 905/1000 
	 loss: 27.7060, MinusLogProbMetric: 27.7060, val_loss: 28.1314, val_MinusLogProbMetric: 28.1314

Epoch 905: val_loss did not improve from 27.93398
196/196 - 53s - loss: 27.7060 - MinusLogProbMetric: 27.7060 - val_loss: 28.1314 - val_MinusLogProbMetric: 28.1314 - lr: 2.5000e-04 - 53s/epoch - 270ms/step
Epoch 906/1000
2023-09-29 16:35:42.456 
Epoch 906/1000 
	 loss: 27.7870, MinusLogProbMetric: 27.7870, val_loss: 28.1235, val_MinusLogProbMetric: 28.1235

Epoch 906: val_loss did not improve from 27.93398
196/196 - 54s - loss: 27.7870 - MinusLogProbMetric: 27.7870 - val_loss: 28.1235 - val_MinusLogProbMetric: 28.1235 - lr: 2.5000e-04 - 54s/epoch - 274ms/step
Epoch 907/1000
2023-09-29 16:36:38.969 
Epoch 907/1000 
	 loss: 27.7418, MinusLogProbMetric: 27.7418, val_loss: 27.9609, val_MinusLogProbMetric: 27.9609

Epoch 907: val_loss did not improve from 27.93398
196/196 - 57s - loss: 27.7418 - MinusLogProbMetric: 27.7418 - val_loss: 27.9609 - val_MinusLogProbMetric: 27.9609 - lr: 2.5000e-04 - 57s/epoch - 288ms/step
Epoch 908/1000
2023-09-29 16:37:31.923 
Epoch 908/1000 
	 loss: 27.7138, MinusLogProbMetric: 27.7138, val_loss: 27.9924, val_MinusLogProbMetric: 27.9924

Epoch 908: val_loss did not improve from 27.93398
196/196 - 53s - loss: 27.7138 - MinusLogProbMetric: 27.7138 - val_loss: 27.9924 - val_MinusLogProbMetric: 27.9924 - lr: 2.5000e-04 - 53s/epoch - 270ms/step
Epoch 909/1000
2023-09-29 16:38:25.953 
Epoch 909/1000 
	 loss: 27.6309, MinusLogProbMetric: 27.6309, val_loss: 27.9194, val_MinusLogProbMetric: 27.9194

Epoch 909: val_loss improved from 27.93398 to 27.91940, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 27.6309 - MinusLogProbMetric: 27.6309 - val_loss: 27.9194 - val_MinusLogProbMetric: 27.9194 - lr: 1.2500e-04 - 55s/epoch - 279ms/step
Epoch 910/1000
2023-09-29 16:39:20.824 
Epoch 910/1000 
	 loss: 27.6385, MinusLogProbMetric: 27.6385, val_loss: 27.9121, val_MinusLogProbMetric: 27.9121

Epoch 910: val_loss improved from 27.91940 to 27.91209, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 55s - loss: 27.6385 - MinusLogProbMetric: 27.6385 - val_loss: 27.9121 - val_MinusLogProbMetric: 27.9121 - lr: 1.2500e-04 - 55s/epoch - 281ms/step
Epoch 911/1000
2023-09-29 16:40:16.304 
Epoch 911/1000 
	 loss: 27.6322, MinusLogProbMetric: 27.6322, val_loss: 27.8894, val_MinusLogProbMetric: 27.8894

Epoch 911: val_loss improved from 27.91209 to 27.88943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 56s - loss: 27.6322 - MinusLogProbMetric: 27.6322 - val_loss: 27.8894 - val_MinusLogProbMetric: 27.8894 - lr: 1.2500e-04 - 56s/epoch - 283ms/step
Epoch 912/1000
2023-09-29 16:41:11.867 
Epoch 912/1000 
	 loss: 27.6281, MinusLogProbMetric: 27.6281, val_loss: 27.9021, val_MinusLogProbMetric: 27.9021

Epoch 912: val_loss did not improve from 27.88943
196/196 - 55s - loss: 27.6281 - MinusLogProbMetric: 27.6281 - val_loss: 27.9021 - val_MinusLogProbMetric: 27.9021 - lr: 1.2500e-04 - 55s/epoch - 279ms/step
Epoch 913/1000
2023-09-29 16:42:07.276 
Epoch 913/1000 
	 loss: 27.6329, MinusLogProbMetric: 27.6329, val_loss: 27.8891, val_MinusLogProbMetric: 27.8891

Epoch 913: val_loss improved from 27.88943 to 27.88906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 56s - loss: 27.6329 - MinusLogProbMetric: 27.6329 - val_loss: 27.8891 - val_MinusLogProbMetric: 27.8891 - lr: 1.2500e-04 - 56s/epoch - 286ms/step
Epoch 914/1000
2023-09-29 16:43:01.520 
Epoch 914/1000 
	 loss: 27.6323, MinusLogProbMetric: 27.6323, val_loss: 27.9258, val_MinusLogProbMetric: 27.9258

Epoch 914: val_loss did not improve from 27.88906
196/196 - 54s - loss: 27.6323 - MinusLogProbMetric: 27.6323 - val_loss: 27.9258 - val_MinusLogProbMetric: 27.9258 - lr: 1.2500e-04 - 54s/epoch - 273ms/step
Epoch 915/1000
2023-09-29 16:43:55.122 
Epoch 915/1000 
	 loss: 27.6279, MinusLogProbMetric: 27.6279, val_loss: 27.9219, val_MinusLogProbMetric: 27.9219

Epoch 915: val_loss did not improve from 27.88906
196/196 - 54s - loss: 27.6279 - MinusLogProbMetric: 27.6279 - val_loss: 27.9219 - val_MinusLogProbMetric: 27.9219 - lr: 1.2500e-04 - 54s/epoch - 273ms/step
Epoch 916/1000
2023-09-29 16:44:48.532 
Epoch 916/1000 
	 loss: 27.6345, MinusLogProbMetric: 27.6345, val_loss: 27.9700, val_MinusLogProbMetric: 27.9700

Epoch 916: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6345 - MinusLogProbMetric: 27.6345 - val_loss: 27.9700 - val_MinusLogProbMetric: 27.9700 - lr: 1.2500e-04 - 53s/epoch - 272ms/step
Epoch 917/1000
2023-09-29 16:45:42.240 
Epoch 917/1000 
	 loss: 27.6399, MinusLogProbMetric: 27.6399, val_loss: 27.9008, val_MinusLogProbMetric: 27.9008

Epoch 917: val_loss did not improve from 27.88906
196/196 - 54s - loss: 27.6399 - MinusLogProbMetric: 27.6399 - val_loss: 27.9008 - val_MinusLogProbMetric: 27.9008 - lr: 1.2500e-04 - 54s/epoch - 274ms/step
Epoch 918/1000
2023-09-29 16:46:35.570 
Epoch 918/1000 
	 loss: 27.6439, MinusLogProbMetric: 27.6439, val_loss: 27.8951, val_MinusLogProbMetric: 27.8951

Epoch 918: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6439 - MinusLogProbMetric: 27.6439 - val_loss: 27.8951 - val_MinusLogProbMetric: 27.8951 - lr: 1.2500e-04 - 53s/epoch - 272ms/step
Epoch 919/1000
2023-09-29 16:47:28.568 
Epoch 919/1000 
	 loss: 27.6422, MinusLogProbMetric: 27.6422, val_loss: 27.8983, val_MinusLogProbMetric: 27.8983

Epoch 919: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6422 - MinusLogProbMetric: 27.6422 - val_loss: 27.8983 - val_MinusLogProbMetric: 27.8983 - lr: 1.2500e-04 - 53s/epoch - 270ms/step
Epoch 920/1000
2023-09-29 16:48:22.023 
Epoch 920/1000 
	 loss: 27.6328, MinusLogProbMetric: 27.6328, val_loss: 27.9260, val_MinusLogProbMetric: 27.9260

Epoch 920: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6328 - MinusLogProbMetric: 27.6328 - val_loss: 27.9260 - val_MinusLogProbMetric: 27.9260 - lr: 1.2500e-04 - 53s/epoch - 273ms/step
Epoch 921/1000
2023-09-29 16:49:13.924 
Epoch 921/1000 
	 loss: 27.6328, MinusLogProbMetric: 27.6328, val_loss: 27.9181, val_MinusLogProbMetric: 27.9181

Epoch 921: val_loss did not improve from 27.88906
196/196 - 52s - loss: 27.6328 - MinusLogProbMetric: 27.6328 - val_loss: 27.9181 - val_MinusLogProbMetric: 27.9181 - lr: 1.2500e-04 - 52s/epoch - 265ms/step
Epoch 922/1000
2023-09-29 16:50:07.192 
Epoch 922/1000 
	 loss: 27.6424, MinusLogProbMetric: 27.6424, val_loss: 27.9048, val_MinusLogProbMetric: 27.9048

Epoch 922: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6424 - MinusLogProbMetric: 27.6424 - val_loss: 27.9048 - val_MinusLogProbMetric: 27.9048 - lr: 1.2500e-04 - 53s/epoch - 272ms/step
Epoch 923/1000
2023-09-29 16:51:01.099 
Epoch 923/1000 
	 loss: 27.6343, MinusLogProbMetric: 27.6343, val_loss: 27.9126, val_MinusLogProbMetric: 27.9126

Epoch 923: val_loss did not improve from 27.88906
196/196 - 54s - loss: 27.6343 - MinusLogProbMetric: 27.6343 - val_loss: 27.9126 - val_MinusLogProbMetric: 27.9126 - lr: 1.2500e-04 - 54s/epoch - 275ms/step
Epoch 924/1000
2023-09-29 16:51:54.853 
Epoch 924/1000 
	 loss: 27.6473, MinusLogProbMetric: 27.6473, val_loss: 27.9156, val_MinusLogProbMetric: 27.9156

Epoch 924: val_loss did not improve from 27.88906
196/196 - 54s - loss: 27.6473 - MinusLogProbMetric: 27.6473 - val_loss: 27.9156 - val_MinusLogProbMetric: 27.9156 - lr: 1.2500e-04 - 54s/epoch - 274ms/step
Epoch 925/1000
2023-09-29 16:52:48.621 
Epoch 925/1000 
	 loss: 27.6447, MinusLogProbMetric: 27.6447, val_loss: 27.9288, val_MinusLogProbMetric: 27.9288

Epoch 925: val_loss did not improve from 27.88906
196/196 - 54s - loss: 27.6447 - MinusLogProbMetric: 27.6447 - val_loss: 27.9288 - val_MinusLogProbMetric: 27.9288 - lr: 1.2500e-04 - 54s/epoch - 274ms/step
Epoch 926/1000
2023-09-29 16:53:41.321 
Epoch 926/1000 
	 loss: 27.6340, MinusLogProbMetric: 27.6340, val_loss: 27.9325, val_MinusLogProbMetric: 27.9325

Epoch 926: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6340 - MinusLogProbMetric: 27.6340 - val_loss: 27.9325 - val_MinusLogProbMetric: 27.9325 - lr: 1.2500e-04 - 53s/epoch - 269ms/step
Epoch 927/1000
2023-09-29 16:54:36.799 
Epoch 927/1000 
	 loss: 27.6380, MinusLogProbMetric: 27.6380, val_loss: 27.9436, val_MinusLogProbMetric: 27.9436

Epoch 927: val_loss did not improve from 27.88906
196/196 - 55s - loss: 27.6380 - MinusLogProbMetric: 27.6380 - val_loss: 27.9436 - val_MinusLogProbMetric: 27.9436 - lr: 1.2500e-04 - 55s/epoch - 283ms/step
Epoch 928/1000
2023-09-29 16:55:30.585 
Epoch 928/1000 
	 loss: 27.6281, MinusLogProbMetric: 27.6281, val_loss: 27.9264, val_MinusLogProbMetric: 27.9264

Epoch 928: val_loss did not improve from 27.88906
196/196 - 54s - loss: 27.6281 - MinusLogProbMetric: 27.6281 - val_loss: 27.9264 - val_MinusLogProbMetric: 27.9264 - lr: 1.2500e-04 - 54s/epoch - 274ms/step
Epoch 929/1000
2023-09-29 16:56:24.181 
Epoch 929/1000 
	 loss: 27.6469, MinusLogProbMetric: 27.6469, val_loss: 27.9260, val_MinusLogProbMetric: 27.9260

Epoch 929: val_loss did not improve from 27.88906
196/196 - 54s - loss: 27.6469 - MinusLogProbMetric: 27.6469 - val_loss: 27.9260 - val_MinusLogProbMetric: 27.9260 - lr: 1.2500e-04 - 54s/epoch - 273ms/step
Epoch 930/1000
2023-09-29 16:57:17.602 
Epoch 930/1000 
	 loss: 27.6438, MinusLogProbMetric: 27.6438, val_loss: 27.9130, val_MinusLogProbMetric: 27.9130

Epoch 930: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6438 - MinusLogProbMetric: 27.6438 - val_loss: 27.9130 - val_MinusLogProbMetric: 27.9130 - lr: 1.2500e-04 - 53s/epoch - 273ms/step
Epoch 931/1000
2023-09-29 16:58:09.701 
Epoch 931/1000 
	 loss: 27.6313, MinusLogProbMetric: 27.6313, val_loss: 27.8914, val_MinusLogProbMetric: 27.8914

Epoch 931: val_loss did not improve from 27.88906
196/196 - 52s - loss: 27.6313 - MinusLogProbMetric: 27.6313 - val_loss: 27.8914 - val_MinusLogProbMetric: 27.8914 - lr: 1.2500e-04 - 52s/epoch - 266ms/step
Epoch 932/1000
2023-09-29 16:59:02.224 
Epoch 932/1000 
	 loss: 27.6261, MinusLogProbMetric: 27.6261, val_loss: 27.9102, val_MinusLogProbMetric: 27.9102

Epoch 932: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6261 - MinusLogProbMetric: 27.6261 - val_loss: 27.9102 - val_MinusLogProbMetric: 27.9102 - lr: 1.2500e-04 - 53s/epoch - 268ms/step
Epoch 933/1000
2023-09-29 16:59:57.399 
Epoch 933/1000 
	 loss: 27.6369, MinusLogProbMetric: 27.6369, val_loss: 27.9232, val_MinusLogProbMetric: 27.9232

Epoch 933: val_loss did not improve from 27.88906
196/196 - 55s - loss: 27.6369 - MinusLogProbMetric: 27.6369 - val_loss: 27.9232 - val_MinusLogProbMetric: 27.9232 - lr: 1.2500e-04 - 55s/epoch - 281ms/step
Epoch 934/1000
2023-09-29 17:00:50.887 
Epoch 934/1000 
	 loss: 27.6308, MinusLogProbMetric: 27.6308, val_loss: 27.9613, val_MinusLogProbMetric: 27.9613

Epoch 934: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6308 - MinusLogProbMetric: 27.6308 - val_loss: 27.9613 - val_MinusLogProbMetric: 27.9613 - lr: 1.2500e-04 - 53s/epoch - 273ms/step
Epoch 935/1000
2023-09-29 17:01:43.037 
Epoch 935/1000 
	 loss: 27.6385, MinusLogProbMetric: 27.6385, val_loss: 27.9224, val_MinusLogProbMetric: 27.9224

Epoch 935: val_loss did not improve from 27.88906
196/196 - 52s - loss: 27.6385 - MinusLogProbMetric: 27.6385 - val_loss: 27.9224 - val_MinusLogProbMetric: 27.9224 - lr: 1.2500e-04 - 52s/epoch - 266ms/step
Epoch 936/1000
2023-09-29 17:02:35.156 
Epoch 936/1000 
	 loss: 27.6602, MinusLogProbMetric: 27.6602, val_loss: 27.8917, val_MinusLogProbMetric: 27.8917

Epoch 936: val_loss did not improve from 27.88906
196/196 - 52s - loss: 27.6602 - MinusLogProbMetric: 27.6602 - val_loss: 27.8917 - val_MinusLogProbMetric: 27.8917 - lr: 1.2500e-04 - 52s/epoch - 266ms/step
Epoch 937/1000
2023-09-29 17:03:28.210 
Epoch 937/1000 
	 loss: 27.6621, MinusLogProbMetric: 27.6621, val_loss: 27.9139, val_MinusLogProbMetric: 27.9139

Epoch 937: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6621 - MinusLogProbMetric: 27.6621 - val_loss: 27.9139 - val_MinusLogProbMetric: 27.9139 - lr: 1.2500e-04 - 53s/epoch - 271ms/step
Epoch 938/1000
2023-09-29 17:04:21.547 
Epoch 938/1000 
	 loss: 27.6571, MinusLogProbMetric: 27.6571, val_loss: 27.9039, val_MinusLogProbMetric: 27.9039

Epoch 938: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6571 - MinusLogProbMetric: 27.6571 - val_loss: 27.9039 - val_MinusLogProbMetric: 27.9039 - lr: 1.2500e-04 - 53s/epoch - 272ms/step
Epoch 939/1000
2023-09-29 17:05:14.120 
Epoch 939/1000 
	 loss: 27.6634, MinusLogProbMetric: 27.6634, val_loss: 27.9275, val_MinusLogProbMetric: 27.9275

Epoch 939: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6634 - MinusLogProbMetric: 27.6634 - val_loss: 27.9275 - val_MinusLogProbMetric: 27.9275 - lr: 1.2500e-04 - 53s/epoch - 268ms/step
Epoch 940/1000
2023-09-29 17:06:07.390 
Epoch 940/1000 
	 loss: 27.6365, MinusLogProbMetric: 27.6365, val_loss: 27.9615, val_MinusLogProbMetric: 27.9615

Epoch 940: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6365 - MinusLogProbMetric: 27.6365 - val_loss: 27.9615 - val_MinusLogProbMetric: 27.9615 - lr: 1.2500e-04 - 53s/epoch - 272ms/step
Epoch 941/1000
2023-09-29 17:07:00.369 
Epoch 941/1000 
	 loss: 27.6531, MinusLogProbMetric: 27.6531, val_loss: 27.9137, val_MinusLogProbMetric: 27.9137

Epoch 941: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6531 - MinusLogProbMetric: 27.6531 - val_loss: 27.9137 - val_MinusLogProbMetric: 27.9137 - lr: 1.2500e-04 - 53s/epoch - 270ms/step
Epoch 942/1000
2023-09-29 17:07:53.396 
Epoch 942/1000 
	 loss: 27.6423, MinusLogProbMetric: 27.6423, val_loss: 27.9007, val_MinusLogProbMetric: 27.9007

Epoch 942: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6423 - MinusLogProbMetric: 27.6423 - val_loss: 27.9007 - val_MinusLogProbMetric: 27.9007 - lr: 1.2500e-04 - 53s/epoch - 271ms/step
Epoch 943/1000
2023-09-29 17:08:45.942 
Epoch 943/1000 
	 loss: 27.6690, MinusLogProbMetric: 27.6690, val_loss: 27.8975, val_MinusLogProbMetric: 27.8975

Epoch 943: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6690 - MinusLogProbMetric: 27.6690 - val_loss: 27.8975 - val_MinusLogProbMetric: 27.8975 - lr: 1.2500e-04 - 53s/epoch - 268ms/step
Epoch 944/1000
2023-09-29 17:09:38.795 
Epoch 944/1000 
	 loss: 27.6419, MinusLogProbMetric: 27.6419, val_loss: 27.8965, val_MinusLogProbMetric: 27.8965

Epoch 944: val_loss did not improve from 27.88906
196/196 - 53s - loss: 27.6419 - MinusLogProbMetric: 27.6419 - val_loss: 27.8965 - val_MinusLogProbMetric: 27.8965 - lr: 1.2500e-04 - 53s/epoch - 270ms/step
Epoch 945/1000
2023-09-29 17:10:31.153 
Epoch 945/1000 
	 loss: 27.6573, MinusLogProbMetric: 27.6573, val_loss: 27.9370, val_MinusLogProbMetric: 27.9370

Epoch 945: val_loss did not improve from 27.88906
196/196 - 52s - loss: 27.6573 - MinusLogProbMetric: 27.6573 - val_loss: 27.9370 - val_MinusLogProbMetric: 27.9370 - lr: 1.2500e-04 - 52s/epoch - 267ms/step
Epoch 946/1000
2023-09-29 17:11:23.565 
Epoch 946/1000 
	 loss: 27.6382, MinusLogProbMetric: 27.6382, val_loss: 27.9334, val_MinusLogProbMetric: 27.9334

Epoch 946: val_loss did not improve from 27.88906
196/196 - 52s - loss: 27.6382 - MinusLogProbMetric: 27.6382 - val_loss: 27.9334 - val_MinusLogProbMetric: 27.9334 - lr: 1.2500e-04 - 52s/epoch - 267ms/step
Epoch 947/1000
2023-09-29 17:12:12.048 
Epoch 947/1000 
	 loss: 27.6464, MinusLogProbMetric: 27.6464, val_loss: 27.9256, val_MinusLogProbMetric: 27.9256

Epoch 947: val_loss did not improve from 27.88906
196/196 - 48s - loss: 27.6464 - MinusLogProbMetric: 27.6464 - val_loss: 27.9256 - val_MinusLogProbMetric: 27.9256 - lr: 1.2500e-04 - 48s/epoch - 247ms/step
Epoch 948/1000
2023-09-29 17:12:59.636 
Epoch 948/1000 
	 loss: 27.6502, MinusLogProbMetric: 27.6502, val_loss: 27.8974, val_MinusLogProbMetric: 27.8974

Epoch 948: val_loss did not improve from 27.88906
196/196 - 48s - loss: 27.6502 - MinusLogProbMetric: 27.6502 - val_loss: 27.8974 - val_MinusLogProbMetric: 27.8974 - lr: 1.2500e-04 - 48s/epoch - 243ms/step
Epoch 949/1000
2023-09-29 17:13:44.894 
Epoch 949/1000 
	 loss: 27.6583, MinusLogProbMetric: 27.6583, val_loss: 27.8930, val_MinusLogProbMetric: 27.8930

Epoch 949: val_loss did not improve from 27.88906
196/196 - 45s - loss: 27.6583 - MinusLogProbMetric: 27.6583 - val_loss: 27.8930 - val_MinusLogProbMetric: 27.8930 - lr: 1.2500e-04 - 45s/epoch - 231ms/step
Epoch 950/1000
2023-09-29 17:14:35.324 
Epoch 950/1000 
	 loss: 27.6385, MinusLogProbMetric: 27.6385, val_loss: 27.8959, val_MinusLogProbMetric: 27.8959

Epoch 950: val_loss did not improve from 27.88906
196/196 - 50s - loss: 27.6385 - MinusLogProbMetric: 27.6385 - val_loss: 27.8959 - val_MinusLogProbMetric: 27.8959 - lr: 1.2500e-04 - 50s/epoch - 257ms/step
Epoch 951/1000
2023-09-29 17:15:24.816 
Epoch 951/1000 
	 loss: 27.6665, MinusLogProbMetric: 27.6665, val_loss: 27.9166, val_MinusLogProbMetric: 27.9166

Epoch 951: val_loss did not improve from 27.88906
196/196 - 49s - loss: 27.6665 - MinusLogProbMetric: 27.6665 - val_loss: 27.9166 - val_MinusLogProbMetric: 27.9166 - lr: 1.2500e-04 - 49s/epoch - 252ms/step
Epoch 952/1000
2023-09-29 17:16:12.275 
Epoch 952/1000 
	 loss: 27.6513, MinusLogProbMetric: 27.6513, val_loss: 27.9887, val_MinusLogProbMetric: 27.9887

Epoch 952: val_loss did not improve from 27.88906
196/196 - 47s - loss: 27.6513 - MinusLogProbMetric: 27.6513 - val_loss: 27.9887 - val_MinusLogProbMetric: 27.9887 - lr: 1.2500e-04 - 47s/epoch - 242ms/step
Epoch 953/1000
2023-09-29 17:17:00.314 
Epoch 953/1000 
	 loss: 27.6469, MinusLogProbMetric: 27.6469, val_loss: 28.0053, val_MinusLogProbMetric: 28.0053

Epoch 953: val_loss did not improve from 27.88906
196/196 - 48s - loss: 27.6469 - MinusLogProbMetric: 27.6469 - val_loss: 28.0053 - val_MinusLogProbMetric: 28.0053 - lr: 1.2500e-04 - 48s/epoch - 245ms/step
Epoch 954/1000
2023-09-29 17:17:51.050 
Epoch 954/1000 
	 loss: 27.6399, MinusLogProbMetric: 27.6399, val_loss: 28.0026, val_MinusLogProbMetric: 28.0026

Epoch 954: val_loss did not improve from 27.88906
196/196 - 51s - loss: 27.6399 - MinusLogProbMetric: 27.6399 - val_loss: 28.0026 - val_MinusLogProbMetric: 28.0026 - lr: 1.2500e-04 - 51s/epoch - 259ms/step
Epoch 955/1000
2023-09-29 17:18:37.904 
Epoch 955/1000 
	 loss: 27.6564, MinusLogProbMetric: 27.6564, val_loss: 27.9115, val_MinusLogProbMetric: 27.9115

Epoch 955: val_loss did not improve from 27.88906
196/196 - 47s - loss: 27.6564 - MinusLogProbMetric: 27.6564 - val_loss: 27.9115 - val_MinusLogProbMetric: 27.9115 - lr: 1.2500e-04 - 47s/epoch - 239ms/step
Epoch 956/1000
2023-09-29 17:19:24.624 
Epoch 956/1000 
	 loss: 27.6297, MinusLogProbMetric: 27.6297, val_loss: 27.9298, val_MinusLogProbMetric: 27.9298

Epoch 956: val_loss did not improve from 27.88906
196/196 - 47s - loss: 27.6297 - MinusLogProbMetric: 27.6297 - val_loss: 27.9298 - val_MinusLogProbMetric: 27.9298 - lr: 1.2500e-04 - 47s/epoch - 238ms/step
Epoch 957/1000
2023-09-29 17:20:14.725 
Epoch 957/1000 
	 loss: 27.6385, MinusLogProbMetric: 27.6385, val_loss: 27.8957, val_MinusLogProbMetric: 27.8957

Epoch 957: val_loss did not improve from 27.88906
196/196 - 50s - loss: 27.6385 - MinusLogProbMetric: 27.6385 - val_loss: 27.8957 - val_MinusLogProbMetric: 27.8957 - lr: 1.2500e-04 - 50s/epoch - 256ms/step
Epoch 958/1000
2023-09-29 17:21:03.530 
Epoch 958/1000 
	 loss: 27.6513, MinusLogProbMetric: 27.6513, val_loss: 27.9022, val_MinusLogProbMetric: 27.9022

Epoch 958: val_loss did not improve from 27.88906
196/196 - 49s - loss: 27.6513 - MinusLogProbMetric: 27.6513 - val_loss: 27.9022 - val_MinusLogProbMetric: 27.9022 - lr: 1.2500e-04 - 49s/epoch - 249ms/step
Epoch 959/1000
2023-09-29 17:21:50.431 
Epoch 959/1000 
	 loss: 27.6477, MinusLogProbMetric: 27.6477, val_loss: 28.1770, val_MinusLogProbMetric: 28.1770

Epoch 959: val_loss did not improve from 27.88906
196/196 - 47s - loss: 27.6477 - MinusLogProbMetric: 27.6477 - val_loss: 28.1770 - val_MinusLogProbMetric: 28.1770 - lr: 1.2500e-04 - 47s/epoch - 239ms/step
Epoch 960/1000
2023-09-29 17:22:37.034 
Epoch 960/1000 
	 loss: 27.6551, MinusLogProbMetric: 27.6551, val_loss: 27.9279, val_MinusLogProbMetric: 27.9279

Epoch 960: val_loss did not improve from 27.88906
196/196 - 47s - loss: 27.6551 - MinusLogProbMetric: 27.6551 - val_loss: 27.9279 - val_MinusLogProbMetric: 27.9279 - lr: 1.2500e-04 - 47s/epoch - 238ms/step
Epoch 961/1000
2023-09-29 17:23:25.168 
Epoch 961/1000 
	 loss: 27.6335, MinusLogProbMetric: 27.6335, val_loss: 27.9803, val_MinusLogProbMetric: 27.9803

Epoch 961: val_loss did not improve from 27.88906
196/196 - 48s - loss: 27.6335 - MinusLogProbMetric: 27.6335 - val_loss: 27.9803 - val_MinusLogProbMetric: 27.9803 - lr: 1.2500e-04 - 48s/epoch - 246ms/step
Epoch 962/1000
2023-09-29 17:24:05.753 
Epoch 962/1000 
	 loss: 27.6429, MinusLogProbMetric: 27.6429, val_loss: 28.0715, val_MinusLogProbMetric: 28.0715

Epoch 962: val_loss did not improve from 27.88906
196/196 - 41s - loss: 27.6429 - MinusLogProbMetric: 27.6429 - val_loss: 28.0715 - val_MinusLogProbMetric: 28.0715 - lr: 1.2500e-04 - 41s/epoch - 207ms/step
Epoch 963/1000
2023-09-29 17:24:51.263 
Epoch 963/1000 
	 loss: 27.6672, MinusLogProbMetric: 27.6672, val_loss: 27.9372, val_MinusLogProbMetric: 27.9372

Epoch 963: val_loss did not improve from 27.88906
196/196 - 46s - loss: 27.6672 - MinusLogProbMetric: 27.6672 - val_loss: 27.9372 - val_MinusLogProbMetric: 27.9372 - lr: 1.2500e-04 - 46s/epoch - 232ms/step
Epoch 964/1000
2023-09-29 17:25:42.764 
Epoch 964/1000 
	 loss: 27.5959, MinusLogProbMetric: 27.5959, val_loss: 27.8743, val_MinusLogProbMetric: 27.8743

Epoch 964: val_loss improved from 27.88906 to 27.87426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 52s - loss: 27.5959 - MinusLogProbMetric: 27.5959 - val_loss: 27.8743 - val_MinusLogProbMetric: 27.8743 - lr: 6.2500e-05 - 52s/epoch - 267ms/step
Epoch 965/1000
2023-09-29 17:26:31.600 
Epoch 965/1000 
	 loss: 27.5983, MinusLogProbMetric: 27.5983, val_loss: 27.9667, val_MinusLogProbMetric: 27.9667

Epoch 965: val_loss did not improve from 27.87426
196/196 - 48s - loss: 27.5983 - MinusLogProbMetric: 27.5983 - val_loss: 27.9667 - val_MinusLogProbMetric: 27.9667 - lr: 6.2500e-05 - 48s/epoch - 245ms/step
Epoch 966/1000
2023-09-29 17:27:20.582 
Epoch 966/1000 
	 loss: 27.6030, MinusLogProbMetric: 27.6030, val_loss: 27.8749, val_MinusLogProbMetric: 27.8749

Epoch 966: val_loss did not improve from 27.87426
196/196 - 49s - loss: 27.6030 - MinusLogProbMetric: 27.6030 - val_loss: 27.8749 - val_MinusLogProbMetric: 27.8749 - lr: 6.2500e-05 - 49s/epoch - 250ms/step
Epoch 967/1000
2023-09-29 17:28:08.728 
Epoch 967/1000 
	 loss: 27.5963, MinusLogProbMetric: 27.5963, val_loss: 27.8748, val_MinusLogProbMetric: 27.8748

Epoch 967: val_loss did not improve from 27.87426
196/196 - 48s - loss: 27.5963 - MinusLogProbMetric: 27.5963 - val_loss: 27.8748 - val_MinusLogProbMetric: 27.8748 - lr: 6.2500e-05 - 48s/epoch - 246ms/step
Epoch 968/1000
2023-09-29 17:28:56.595 
Epoch 968/1000 
	 loss: 27.5963, MinusLogProbMetric: 27.5963, val_loss: 27.8662, val_MinusLogProbMetric: 27.8662

Epoch 968: val_loss improved from 27.87426 to 27.86623, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 48s - loss: 27.5963 - MinusLogProbMetric: 27.5963 - val_loss: 27.8662 - val_MinusLogProbMetric: 27.8662 - lr: 6.2500e-05 - 48s/epoch - 247ms/step
Epoch 969/1000
2023-09-29 17:29:46.783 
Epoch 969/1000 
	 loss: 27.5976, MinusLogProbMetric: 27.5976, val_loss: 27.9870, val_MinusLogProbMetric: 27.9870

Epoch 969: val_loss did not improve from 27.86623
196/196 - 50s - loss: 27.5976 - MinusLogProbMetric: 27.5976 - val_loss: 27.9870 - val_MinusLogProbMetric: 27.9870 - lr: 6.2500e-05 - 50s/epoch - 253ms/step
Epoch 970/1000
2023-09-29 17:30:38.884 
Epoch 970/1000 
	 loss: 27.6018, MinusLogProbMetric: 27.6018, val_loss: 27.8702, val_MinusLogProbMetric: 27.8702

Epoch 970: val_loss did not improve from 27.86623
196/196 - 52s - loss: 27.6018 - MinusLogProbMetric: 27.6018 - val_loss: 27.8702 - val_MinusLogProbMetric: 27.8702 - lr: 6.2500e-05 - 52s/epoch - 266ms/step
Epoch 971/1000
2023-09-29 17:31:28.850 
Epoch 971/1000 
	 loss: 27.5933, MinusLogProbMetric: 27.5933, val_loss: 27.8822, val_MinusLogProbMetric: 27.8822

Epoch 971: val_loss did not improve from 27.86623
196/196 - 50s - loss: 27.5933 - MinusLogProbMetric: 27.5933 - val_loss: 27.8822 - val_MinusLogProbMetric: 27.8822 - lr: 6.2500e-05 - 50s/epoch - 255ms/step
Epoch 972/1000
2023-09-29 17:32:19.396 
Epoch 972/1000 
	 loss: 27.5924, MinusLogProbMetric: 27.5924, val_loss: 27.8952, val_MinusLogProbMetric: 27.8952

Epoch 972: val_loss did not improve from 27.86623
196/196 - 51s - loss: 27.5924 - MinusLogProbMetric: 27.5924 - val_loss: 27.8952 - val_MinusLogProbMetric: 27.8952 - lr: 6.2500e-05 - 51s/epoch - 258ms/step
Epoch 973/1000
2023-09-29 17:33:07.681 
Epoch 973/1000 
	 loss: 27.5938, MinusLogProbMetric: 27.5938, val_loss: 27.8726, val_MinusLogProbMetric: 27.8726

Epoch 973: val_loss did not improve from 27.86623
196/196 - 48s - loss: 27.5938 - MinusLogProbMetric: 27.5938 - val_loss: 27.8726 - val_MinusLogProbMetric: 27.8726 - lr: 6.2500e-05 - 48s/epoch - 246ms/step
Epoch 974/1000
2023-09-29 17:33:57.824 
Epoch 974/1000 
	 loss: 27.5876, MinusLogProbMetric: 27.5876, val_loss: 27.8765, val_MinusLogProbMetric: 27.8765

Epoch 974: val_loss did not improve from 27.86623
196/196 - 50s - loss: 27.5876 - MinusLogProbMetric: 27.5876 - val_loss: 27.8765 - val_MinusLogProbMetric: 27.8765 - lr: 6.2500e-05 - 50s/epoch - 256ms/step
Epoch 975/1000
2023-09-29 17:34:45.781 
Epoch 975/1000 
	 loss: 27.5900, MinusLogProbMetric: 27.5900, val_loss: 27.8728, val_MinusLogProbMetric: 27.8728

Epoch 975: val_loss did not improve from 27.86623
196/196 - 48s - loss: 27.5900 - MinusLogProbMetric: 27.5900 - val_loss: 27.8728 - val_MinusLogProbMetric: 27.8728 - lr: 6.2500e-05 - 48s/epoch - 245ms/step
Epoch 976/1000
2023-09-29 17:35:35.204 
Epoch 976/1000 
	 loss: 27.5913, MinusLogProbMetric: 27.5913, val_loss: 27.8690, val_MinusLogProbMetric: 27.8690

Epoch 976: val_loss did not improve from 27.86623
196/196 - 49s - loss: 27.5913 - MinusLogProbMetric: 27.5913 - val_loss: 27.8690 - val_MinusLogProbMetric: 27.8690 - lr: 6.2500e-05 - 49s/epoch - 252ms/step
Epoch 977/1000
2023-09-29 17:36:23.839 
Epoch 977/1000 
	 loss: 27.5919, MinusLogProbMetric: 27.5919, val_loss: 27.8745, val_MinusLogProbMetric: 27.8745

Epoch 977: val_loss did not improve from 27.86623
196/196 - 49s - loss: 27.5919 - MinusLogProbMetric: 27.5919 - val_loss: 27.8745 - val_MinusLogProbMetric: 27.8745 - lr: 6.2500e-05 - 49s/epoch - 248ms/step
Epoch 978/1000
2023-09-29 17:37:14.528 
Epoch 978/1000 
	 loss: 27.5901, MinusLogProbMetric: 27.5901, val_loss: 27.8830, val_MinusLogProbMetric: 27.8830

Epoch 978: val_loss did not improve from 27.86623
196/196 - 51s - loss: 27.5901 - MinusLogProbMetric: 27.5901 - val_loss: 27.8830 - val_MinusLogProbMetric: 27.8830 - lr: 6.2500e-05 - 51s/epoch - 259ms/step
Epoch 979/1000
2023-09-29 17:38:07.106 
Epoch 979/1000 
	 loss: 27.5896, MinusLogProbMetric: 27.5896, val_loss: 27.8709, val_MinusLogProbMetric: 27.8709

Epoch 979: val_loss did not improve from 27.86623
196/196 - 53s - loss: 27.5896 - MinusLogProbMetric: 27.5896 - val_loss: 27.8709 - val_MinusLogProbMetric: 27.8709 - lr: 6.2500e-05 - 53s/epoch - 268ms/step
Epoch 980/1000
2023-09-29 17:38:55.870 
Epoch 980/1000 
	 loss: 27.5908, MinusLogProbMetric: 27.5908, val_loss: 27.8603, val_MinusLogProbMetric: 27.8603

Epoch 980: val_loss improved from 27.86623 to 27.86025, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_331/weights/best_weights.h5
196/196 - 49s - loss: 27.5908 - MinusLogProbMetric: 27.5908 - val_loss: 27.8603 - val_MinusLogProbMetric: 27.8603 - lr: 6.2500e-05 - 49s/epoch - 252ms/step
Epoch 981/1000
2023-09-29 17:39:48.621 
Epoch 981/1000 
	 loss: 27.5901, MinusLogProbMetric: 27.5901, val_loss: 27.8994, val_MinusLogProbMetric: 27.8994

Epoch 981: val_loss did not improve from 27.86025
196/196 - 52s - loss: 27.5901 - MinusLogProbMetric: 27.5901 - val_loss: 27.8994 - val_MinusLogProbMetric: 27.8994 - lr: 6.2500e-05 - 52s/epoch - 266ms/step
Epoch 982/1000
2023-09-29 17:40:39.518 
Epoch 982/1000 
	 loss: 27.5901, MinusLogProbMetric: 27.5901, val_loss: 27.8701, val_MinusLogProbMetric: 27.8701

Epoch 982: val_loss did not improve from 27.86025
196/196 - 51s - loss: 27.5901 - MinusLogProbMetric: 27.5901 - val_loss: 27.8701 - val_MinusLogProbMetric: 27.8701 - lr: 6.2500e-05 - 51s/epoch - 260ms/step
Epoch 983/1000
2023-09-29 17:41:30.777 
Epoch 983/1000 
	 loss: 27.5889, MinusLogProbMetric: 27.5889, val_loss: 27.8708, val_MinusLogProbMetric: 27.8708

Epoch 983: val_loss did not improve from 27.86025
196/196 - 51s - loss: 27.5889 - MinusLogProbMetric: 27.5889 - val_loss: 27.8708 - val_MinusLogProbMetric: 27.8708 - lr: 6.2500e-05 - 51s/epoch - 261ms/step
Epoch 984/1000
2023-09-29 17:42:20.187 
Epoch 984/1000 
	 loss: 27.5934, MinusLogProbMetric: 27.5934, val_loss: 27.8749, val_MinusLogProbMetric: 27.8749

Epoch 984: val_loss did not improve from 27.86025
196/196 - 49s - loss: 27.5934 - MinusLogProbMetric: 27.5934 - val_loss: 27.8749 - val_MinusLogProbMetric: 27.8749 - lr: 6.2500e-05 - 49s/epoch - 252ms/step
Epoch 985/1000
2023-09-29 17:43:08.716 
Epoch 985/1000 
	 loss: 27.5922, MinusLogProbMetric: 27.5922, val_loss: 27.8946, val_MinusLogProbMetric: 27.8946

Epoch 985: val_loss did not improve from 27.86025
196/196 - 49s - loss: 27.5922 - MinusLogProbMetric: 27.5922 - val_loss: 27.8946 - val_MinusLogProbMetric: 27.8946 - lr: 6.2500e-05 - 49s/epoch - 248ms/step
Epoch 986/1000
2023-09-29 17:43:58.624 
Epoch 986/1000 
	 loss: 27.5914, MinusLogProbMetric: 27.5914, val_loss: 27.8712, val_MinusLogProbMetric: 27.8712

Epoch 986: val_loss did not improve from 27.86025
196/196 - 50s - loss: 27.5914 - MinusLogProbMetric: 27.5914 - val_loss: 27.8712 - val_MinusLogProbMetric: 27.8712 - lr: 6.2500e-05 - 50s/epoch - 255ms/step
Epoch 987/1000
2023-09-29 17:44:46.379 
Epoch 987/1000 
	 loss: 27.5922, MinusLogProbMetric: 27.5922, val_loss: 27.8710, val_MinusLogProbMetric: 27.8710

Epoch 987: val_loss did not improve from 27.86025
196/196 - 48s - loss: 27.5922 - MinusLogProbMetric: 27.5922 - val_loss: 27.8710 - val_MinusLogProbMetric: 27.8710 - lr: 6.2500e-05 - 48s/epoch - 244ms/step
Epoch 988/1000
2023-09-29 17:45:35.967 
Epoch 988/1000 
	 loss: 27.5909, MinusLogProbMetric: 27.5909, val_loss: 27.9112, val_MinusLogProbMetric: 27.9112

Epoch 988: val_loss did not improve from 27.86025
196/196 - 50s - loss: 27.5909 - MinusLogProbMetric: 27.5909 - val_loss: 27.9112 - val_MinusLogProbMetric: 27.9112 - lr: 6.2500e-05 - 50s/epoch - 253ms/step
Epoch 989/1000
2023-09-29 17:46:26.088 
Epoch 989/1000 
	 loss: 27.5928, MinusLogProbMetric: 27.5928, val_loss: 27.8839, val_MinusLogProbMetric: 27.8839

Epoch 989: val_loss did not improve from 27.86025
196/196 - 50s - loss: 27.5928 - MinusLogProbMetric: 27.5928 - val_loss: 27.8839 - val_MinusLogProbMetric: 27.8839 - lr: 6.2500e-05 - 50s/epoch - 256ms/step
Epoch 990/1000
2023-09-29 17:47:16.958 
Epoch 990/1000 
	 loss: 27.5929, MinusLogProbMetric: 27.5929, val_loss: 27.8661, val_MinusLogProbMetric: 27.8661

Epoch 990: val_loss did not improve from 27.86025
196/196 - 51s - loss: 27.5929 - MinusLogProbMetric: 27.5929 - val_loss: 27.8661 - val_MinusLogProbMetric: 27.8661 - lr: 6.2500e-05 - 51s/epoch - 260ms/step
Epoch 991/1000
2023-09-29 17:48:06.933 
Epoch 991/1000 
	 loss: 27.5898, MinusLogProbMetric: 27.5898, val_loss: 27.8756, val_MinusLogProbMetric: 27.8756

Epoch 991: val_loss did not improve from 27.86025
196/196 - 50s - loss: 27.5898 - MinusLogProbMetric: 27.5898 - val_loss: 27.8756 - val_MinusLogProbMetric: 27.8756 - lr: 6.2500e-05 - 50s/epoch - 255ms/step
Epoch 992/1000
2023-09-29 17:48:57.031 
Epoch 992/1000 
	 loss: 27.5919, MinusLogProbMetric: 27.5919, val_loss: 27.8761, val_MinusLogProbMetric: 27.8761

Epoch 992: val_loss did not improve from 27.86025
196/196 - 50s - loss: 27.5919 - MinusLogProbMetric: 27.5919 - val_loss: 27.8761 - val_MinusLogProbMetric: 27.8761 - lr: 6.2500e-05 - 50s/epoch - 256ms/step
Epoch 993/1000
2023-09-29 17:49:46.218 
Epoch 993/1000 
	 loss: 27.5938, MinusLogProbMetric: 27.5938, val_loss: 27.9297, val_MinusLogProbMetric: 27.9297

Epoch 993: val_loss did not improve from 27.86025
196/196 - 49s - loss: 27.5938 - MinusLogProbMetric: 27.5938 - val_loss: 27.9297 - val_MinusLogProbMetric: 27.9297 - lr: 6.2500e-05 - 49s/epoch - 251ms/step
Epoch 994/1000
2023-09-29 17:50:33.317 
Epoch 994/1000 
	 loss: 27.5925, MinusLogProbMetric: 27.5925, val_loss: 27.8736, val_MinusLogProbMetric: 27.8736

Epoch 994: val_loss did not improve from 27.86025
196/196 - 47s - loss: 27.5925 - MinusLogProbMetric: 27.5925 - val_loss: 27.8736 - val_MinusLogProbMetric: 27.8736 - lr: 6.2500e-05 - 47s/epoch - 240ms/step
Epoch 995/1000
2023-09-29 17:51:20.872 
Epoch 995/1000 
	 loss: 27.5940, MinusLogProbMetric: 27.5940, val_loss: 27.8681, val_MinusLogProbMetric: 27.8681

Epoch 995: val_loss did not improve from 27.86025
196/196 - 48s - loss: 27.5940 - MinusLogProbMetric: 27.5940 - val_loss: 27.8681 - val_MinusLogProbMetric: 27.8681 - lr: 6.2500e-05 - 48s/epoch - 243ms/step
Epoch 996/1000
2023-09-29 17:52:09.939 
Epoch 996/1000 
	 loss: 27.5933, MinusLogProbMetric: 27.5933, val_loss: 27.9165, val_MinusLogProbMetric: 27.9165

Epoch 996: val_loss did not improve from 27.86025
196/196 - 49s - loss: 27.5933 - MinusLogProbMetric: 27.5933 - val_loss: 27.9165 - val_MinusLogProbMetric: 27.9165 - lr: 6.2500e-05 - 49s/epoch - 250ms/step
Epoch 997/1000
2023-09-29 17:52:58.758 
Epoch 997/1000 
	 loss: 27.5924, MinusLogProbMetric: 27.5924, val_loss: 27.8670, val_MinusLogProbMetric: 27.8670

Epoch 997: val_loss did not improve from 27.86025
196/196 - 49s - loss: 27.5924 - MinusLogProbMetric: 27.5924 - val_loss: 27.8670 - val_MinusLogProbMetric: 27.8670 - lr: 6.2500e-05 - 49s/epoch - 249ms/step
Epoch 998/1000
2023-09-29 17:53:47.358 
Epoch 998/1000 
	 loss: 27.5907, MinusLogProbMetric: 27.5907, val_loss: 27.8971, val_MinusLogProbMetric: 27.8971

Epoch 998: val_loss did not improve from 27.86025
196/196 - 49s - loss: 27.5907 - MinusLogProbMetric: 27.5907 - val_loss: 27.8971 - val_MinusLogProbMetric: 27.8971 - lr: 6.2500e-05 - 49s/epoch - 248ms/step
Epoch 999/1000
2023-09-29 17:54:37.151 
Epoch 999/1000 
	 loss: 27.5927, MinusLogProbMetric: 27.5927, val_loss: 27.8770, val_MinusLogProbMetric: 27.8770

Epoch 999: val_loss did not improve from 27.86025
196/196 - 50s - loss: 27.5927 - MinusLogProbMetric: 27.5927 - val_loss: 27.8770 - val_MinusLogProbMetric: 27.8770 - lr: 6.2500e-05 - 50s/epoch - 254ms/step
Epoch 1000/1000
2023-09-29 17:55:24.658 
Epoch 1000/1000 
	 loss: 27.5980, MinusLogProbMetric: 27.5980, val_loss: 27.8682, val_MinusLogProbMetric: 27.8682

Epoch 1000: val_loss did not improve from 27.86025
196/196 - 48s - loss: 27.5980 - MinusLogProbMetric: 27.5980 - val_loss: 27.8682 - val_MinusLogProbMetric: 27.8682 - lr: 6.2500e-05 - 48s/epoch - 242ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
LR metric calculation completed in 61.792005875962786 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 27.987197143025696 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 24.853028937941417 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 22.045998604036868 seconds.
Training succeeded with seed 187.
Model trained in 51987.09 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 138.09 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 138.54 s.
===========
Run 331/720 done in 52131.80 s.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

===========
Generating train data for run 343.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_292"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_293 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f8cbdf0d1e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8cbd7eb520>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8cbd7eb520>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8cbd666dd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8cbd6e0880>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8cbd6e0df0>, <keras.callbacks.ModelCheckpoint object at 0x7f8cbd6e0eb0>, <keras.callbacks.EarlyStopping object at 0x7f8cbd6e1120>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8cbd6e1150>, <keras.callbacks.TerminateOnNaN object at 0x7f8cbd6e0d90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-09-29 17:57:54.268629
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:00:50.664 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 176s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 176s/epoch - 899ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 343.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_303"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_304 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f8d6c1bb880>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f9228776050>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f9228776050>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8dee8f7f40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8dee852170>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8dee8526e0>, <keras.callbacks.ModelCheckpoint object at 0x7f8dee8527a0>, <keras.callbacks.EarlyStopping object at 0x7f8dee852a10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8dee852a40>, <keras.callbacks.TerminateOnNaN object at 0x7f8dee852680>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-09-29 18:00:59.546201
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:03:44.051 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 164s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 164s/epoch - 838ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 343.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_314"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_315 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f8dbca56950>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8cc492a020>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8cc492a020>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f90d87e3460>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f90d87e2560>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f90d87e1cc0>, <keras.callbacks.ModelCheckpoint object at 0x7f90d87e1e70>, <keras.callbacks.EarlyStopping object at 0x7f90d87e1990>, <keras.callbacks.ReduceLROnPlateau object at 0x7f90d87e19c0>, <keras.callbacks.TerminateOnNaN object at 0x7f90d87e1ff0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-09-29 18:04:02.712370
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:06:47.577 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 165s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 165s/epoch - 840ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 343.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_325"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_326 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f8d9966fbb0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f92083b3eb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f92083b3eb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8d99fb61d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8d996267d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8d99626d40>, <keras.callbacks.ModelCheckpoint object at 0x7f8d99626e00>, <keras.callbacks.EarlyStopping object at 0x7f8d99627070>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8d996270a0>, <keras.callbacks.TerminateOnNaN object at 0x7f8d99626ce0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-09-29 18:06:58.000042
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:09:54.632 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 176s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 176s/epoch - 900ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 343.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_336"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_337 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f9519c185e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8d79594af0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8d79594af0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f914c49c8e0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f9519c7b1c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f9519c78280>, <keras.callbacks.ModelCheckpoint object at 0x7f9519c7bd00>, <keras.callbacks.EarlyStopping object at 0x7f9519c7b2b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f9519c7aef0>, <keras.callbacks.TerminateOnNaN object at 0x7f9519c79ed0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-09-29 18:10:05.831953
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:12:52.390 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 166s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 166s/epoch - 849ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.115226337448558e-06.
===========
Generating train data for run 343.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_347"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_348 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f8dbcda7160>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8d2199f370>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8d2199f370>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8e85c9bb80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8e85cf8a30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8e85cf8fa0>, <keras.callbacks.ModelCheckpoint object at 0x7f8e85cf9060>, <keras.callbacks.EarlyStopping object at 0x7f8e85cf92d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8e85cf9300>, <keras.callbacks.TerminateOnNaN object at 0x7f8e85cf8f40>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-09-29 18:13:04.708762
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:16:02.879 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 178s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 178s/epoch - 908ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.3717421124828526e-06.
===========
Generating train data for run 343.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_358"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_359 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f8ea8bf7fd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f9620787a90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f9620787a90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8d99315db0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8ea8b9b820>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8ea8b9bd90>, <keras.callbacks.ModelCheckpoint object at 0x7f8ea8b9be50>, <keras.callbacks.EarlyStopping object at 0x7f8ea8b9bfd0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8ea8b9bee0>, <keras.callbacks.TerminateOnNaN object at 0x7f8ea8b9bf10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-09-29 18:16:15.639637
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:19:00.167 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 164s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 164s/epoch - 838ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 4.572473708276175e-07.
===========
Generating train data for run 343.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_369"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_370 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f8cc5a84e20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8b3fce23e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8b3fce23e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8b3ffa7ac0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8de9413ca0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8de946c250>, <keras.callbacks.ModelCheckpoint object at 0x7f8de946c310>, <keras.callbacks.EarlyStopping object at 0x7f8de946c580>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8de946c5b0>, <keras.callbacks.TerminateOnNaN object at 0x7f8de946c1f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-09-29 18:19:08.974744
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:22:04.991 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 176s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 176s/epoch - 896ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.524157902758725e-07.
===========
Generating train data for run 343.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_380"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_381 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f8e1e691db0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8e851af0d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8e851af0d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8b6437a170>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8e47b479a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8e47b47f10>, <keras.callbacks.ModelCheckpoint object at 0x7f8e47b47fd0>, <keras.callbacks.EarlyStopping object at 0x7f8e47b47ee0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8e47b47eb0>, <keras.callbacks.TerminateOnNaN object at 0x7f8d6dbe8280>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-09-29 18:22:15.902133
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:25:18.936 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 183s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 183s/epoch - 932ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.0805263425290834e-08.
===========
Generating train data for run 343.
===========
Train data generated in 0.44 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_391"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_392 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f8f050ef9a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8d113ef5b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8d113ef5b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8f06574c70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f919854cb80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f919854c910>, <keras.callbacks.ModelCheckpoint object at 0x7f919854c730>, <keras.callbacks.EarlyStopping object at 0x7f919854f280>, <keras.callbacks.ReduceLROnPlateau object at 0x7f919854dba0>, <keras.callbacks.TerminateOnNaN object at 0x7f919854c190>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-09-29 18:25:32.348973
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:28:14.056 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 162s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 162s/epoch - 824ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.6935087808430278e-08.
===========
Generating train data for run 343.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_343/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_343
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_402"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_403 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  1817280   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,817,280
Trainable params: 1,817,280
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f8de8ff5b40>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8de857c6a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8de857c6a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8de85d3df0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8de84563e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_343/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8de8456950>, <keras.callbacks.ModelCheckpoint object at 0x7f8de8456a10>, <keras.callbacks.EarlyStopping object at 0x7f8de8456c80>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8de8456cb0>, <keras.callbacks.TerminateOnNaN object at 0x7f8de84568f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_343/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 343/720 with hyperparameters:
timestamp = 2023-09-29 18:28:27.884085
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1817280
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:31:31.307 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 7608.4907, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 183s - loss: nan - MinusLogProbMetric: 7608.4907 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 183s/epoch - 934ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 5.645029269476759e-09.
===========
Run 343/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 637, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 313, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 344.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_344
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_413"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_414 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f904c2eff70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8d2d3959c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8d2d3959c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f922855fa60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8fb452ed70>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8e7ef80190>, <keras.callbacks.ModelCheckpoint object at 0x7f8ecbc35150>, <keras.callbacks.EarlyStopping object at 0x7f8e7ef80490>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8ecbc355d0>, <keras.callbacks.TerminateOnNaN object at 0x7f8e7ef807c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_344/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 344/720 with hyperparameters:
timestamp = 2023-09-29 18:31:43.713089
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 4: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:34:29.432 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6692.6997, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 166s - loss: nan - MinusLogProbMetric: 6692.6997 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 166s/epoch - 845ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0003333333333333333.
===========
Generating train data for run 344.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_344
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_424"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_425 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f8b74387220>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8b7c4c79d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8b7c4c79d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8da4a7cb20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8e7fd85840>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8e7fd85e70>, <keras.callbacks.ModelCheckpoint object at 0x7f8e7fd86bf0>, <keras.callbacks.EarlyStopping object at 0x7f8e7fd860e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8e7fd87190>, <keras.callbacks.TerminateOnNaN object at 0x7f8e7fd86770>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_344/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 344/720 with hyperparameters:
timestamp = 2023-09-29 18:34:41.823777
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 64: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:38:27.982 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 4496.5200, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 226s - loss: nan - MinusLogProbMetric: 4496.5200 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 226s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 0.0001111111111111111.
===========
Generating train data for run 344.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_344
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_435"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_436 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f8ec8d93ee0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f906431ff40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f906431ff40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8ecb7e7f40>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8ec8ddf6d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8ec8ddfc40>, <keras.callbacks.ModelCheckpoint object at 0x7f8ec8ddfd00>, <keras.callbacks.EarlyStopping object at 0x7f8ec8ddff70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8ec8ddffa0>, <keras.callbacks.TerminateOnNaN object at 0x7f8ec8ddfbe0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_344/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 344/720 with hyperparameters:
timestamp = 2023-09-29 18:38:41.083600
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 179: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:43:34.930 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 4087.3960, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 293s - loss: nan - MinusLogProbMetric: 4087.3960 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 293s/epoch - 1s/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 3.703703703703703e-05.
===========
Generating train data for run 344.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_344
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_446"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_447 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f8ea87633d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8e85e8e050>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8e85e8e050>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8ec8d68bb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8cac7cbb50>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8cac7cbf70>, <keras.callbacks.ModelCheckpoint object at 0x7f8e06e981c0>, <keras.callbacks.EarlyStopping object at 0x7f8e06e98430>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8e06e98460>, <keras.callbacks.TerminateOnNaN object at 0x7f8e06e980a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_344/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 344/720 with hyperparameters:
timestamp = 2023-09-29 18:43:49.228735
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-09-29 18:49:11.641 
Epoch 1/1000 
	 loss: 5266.4741, MinusLogProbMetric: 5266.4741, val_loss: 3699.1863, val_MinusLogProbMetric: 3699.1863

Epoch 1: val_loss improved from inf to 3699.18628, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 323s - loss: 5266.4741 - MinusLogProbMetric: 5266.4741 - val_loss: 3699.1863 - val_MinusLogProbMetric: 3699.1863 - lr: 3.7037e-05 - 323s/epoch - 2s/step
Epoch 2/1000
2023-09-29 18:51:30.263 
Epoch 2/1000 
	 loss: 2711.4397, MinusLogProbMetric: 2711.4397, val_loss: 1970.9619, val_MinusLogProbMetric: 1970.9619

Epoch 2: val_loss improved from 3699.18628 to 1970.96191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 138s - loss: 2711.4397 - MinusLogProbMetric: 2711.4397 - val_loss: 1970.9619 - val_MinusLogProbMetric: 1970.9619 - lr: 3.7037e-05 - 138s/epoch - 706ms/step
Epoch 3/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 20: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-09-29 18:51:51.522 
Epoch 3/1000 
	 loss: nan, MinusLogProbMetric: 1946.7078, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 3: val_loss did not improve from 1970.96191
196/196 - 20s - loss: nan - MinusLogProbMetric: 1946.7078 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 20s/epoch - 101ms/step
The loss history contains NaN values.
Training failed: trying again with seed 326159 and lr 1.2345679012345677e-05.
===========
Generating train data for run 344.
===========
Train data generated in 0.40 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 377}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_344/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_344
self.data_kwargs: {'seed': 377}
self.x_data: [[ 5.465355   7.122426   6.507682  ...  1.2216942  7.396178   1.4448237]
 [ 1.8683796  2.8760538  9.147447  ...  4.980697  -1.1366768  2.8016024]
 [ 5.6175413  8.642618   5.6734285 ... -0.9440901  7.0561414  1.3082856]
 ...
 [ 6.145307   2.8273122  6.191003  ...  2.964365   6.077158   2.5509653]
 [ 6.5595512  2.759307   6.0778823 ...  3.4800506  4.3951674  2.1941729]
 [ 5.586527   7.3216014  6.2487698 ...  1.292631   5.797888   1.2879843]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_457"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_458 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  4278720   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,278,720
Trainable params: 4,278,720
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7f9208114190>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8b3fa722f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8b3fa722f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8eab109780>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8dd4104cd0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8dd41053f0>, <keras.callbacks.ModelCheckpoint object at 0x7f8dd4105c60>, <keras.callbacks.EarlyStopping object at 0x7f8dd4104d90>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8dd4105150>, <keras.callbacks.TerminateOnNaN object at 0x7f8dd4105b10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[6.6859813 , 2.7837048 , 6.187682  , ..., 3.267298  , 3.312391  ,
        1.7728554 ],
       [5.5053306 , 9.035645  , 6.1751018 , ..., 0.30946255, 7.146924  ,
        1.2154098 ],
       [6.3542504 , 3.2344346 , 6.1280007 , ..., 3.6475017 , 5.5144544 ,
        1.9890704 ],
       ...,
       [6.5301805 , 2.924721  , 6.1604037 , ..., 3.1045055 , 2.0357747 ,
        2.124177  ],
       [6.8180447 , 2.9883018 , 6.104944  , ..., 2.4949408 , 4.7612505 ,
        1.865407  ],
       [5.243594  , 7.4783473 , 5.798233  , ..., 1.133655  , 6.5394554 ,
        1.371142  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
Found and loaded existing weights.
No history found. Generating new history.
===============
Running 344/720 with hyperparameters:
timestamp = 2023-09-29 18:52:08.525370
ndims = 64
seed_train = 377
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4278720
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 5.465355    7.122426    6.507682    5.369255    4.1136785   6.196197
  4.259451    8.702779    9.329599    4.155665    7.5761204   5.312057
  5.67009     9.304932    0.8909831   1.4673554  -0.03015512  8.003835
  8.704047    8.985184    9.595006    8.103893    4.62255     8.186032
  0.5885266   6.325719    0.92288995  9.5184145   6.6715336   3.3791308
  2.4301429   8.523114    4.619198    5.3705144   0.3660334   5.918427
  6.2292395   7.1572037   9.551745    6.67638     3.2475867   4.406022
  7.1114645   0.65166384  6.9876056   6.6356955   2.3022606   1.2872343
  3.532573    3.8445692   6.0782557   4.5467997   9.491786    0.11065191
  3.0255654   1.2933161   6.592196    2.150623    4.2476006   4.39
  1.805124    1.2216942   7.396178    1.4448237 ]
Epoch 1/1000
2023-09-29 18:57:12.473 
Epoch 1/1000 
	 loss: 1656.4012, MinusLogProbMetric: 1656.4012, val_loss: 1695.0824, val_MinusLogProbMetric: 1695.0824

Epoch 1: val_loss improved from inf to 1695.08240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 305s - loss: 1656.4012 - MinusLogProbMetric: 1656.4012 - val_loss: 1695.0824 - val_MinusLogProbMetric: 1695.0824 - lr: 1.2346e-05 - 305s/epoch - 2s/step
Epoch 2/1000
2023-09-29 18:59:15.057 
Epoch 2/1000 
	 loss: 1467.5336, MinusLogProbMetric: 1467.5336, val_loss: 1253.2443, val_MinusLogProbMetric: 1253.2443

Epoch 2: val_loss improved from 1695.08240 to 1253.24426, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 122s - loss: 1467.5336 - MinusLogProbMetric: 1467.5336 - val_loss: 1253.2443 - val_MinusLogProbMetric: 1253.2443 - lr: 1.2346e-05 - 122s/epoch - 625ms/step
Epoch 3/1000
2023-09-29 19:01:03.512 
Epoch 3/1000 
	 loss: 1195.5885, MinusLogProbMetric: 1195.5885, val_loss: 1127.4304, val_MinusLogProbMetric: 1127.4304

Epoch 3: val_loss improved from 1253.24426 to 1127.43042, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 1195.5885 - MinusLogProbMetric: 1195.5885 - val_loss: 1127.4304 - val_MinusLogProbMetric: 1127.4304 - lr: 1.2346e-05 - 109s/epoch - 554ms/step
Epoch 4/1000
2023-09-29 19:02:50.057 
Epoch 4/1000 
	 loss: 1029.7483, MinusLogProbMetric: 1029.7483, val_loss: 921.3776, val_MinusLogProbMetric: 921.3776

Epoch 4: val_loss improved from 1127.43042 to 921.37756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 1029.7483 - MinusLogProbMetric: 1029.7483 - val_loss: 921.3776 - val_MinusLogProbMetric: 921.3776 - lr: 1.2346e-05 - 107s/epoch - 544ms/step
Epoch 5/1000
2023-09-29 19:04:41.198 
Epoch 5/1000 
	 loss: 962.6918, MinusLogProbMetric: 962.6918, val_loss: 1064.3777, val_MinusLogProbMetric: 1064.3777

Epoch 5: val_loss did not improve from 921.37756
196/196 - 110s - loss: 962.6918 - MinusLogProbMetric: 962.6918 - val_loss: 1064.3777 - val_MinusLogProbMetric: 1064.3777 - lr: 1.2346e-05 - 110s/epoch - 559ms/step
Epoch 6/1000
2023-09-29 19:06:26.662 
Epoch 6/1000 
	 loss: 930.2531, MinusLogProbMetric: 930.2531, val_loss: 862.5429, val_MinusLogProbMetric: 862.5429

Epoch 6: val_loss improved from 921.37756 to 862.54291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 930.2531 - MinusLogProbMetric: 930.2531 - val_loss: 862.5429 - val_MinusLogProbMetric: 862.5429 - lr: 1.2346e-05 - 107s/epoch - 544ms/step
Epoch 7/1000
2023-09-29 19:08:13.844 
Epoch 7/1000 
	 loss: 790.0508, MinusLogProbMetric: 790.0508, val_loss: 749.8470, val_MinusLogProbMetric: 749.8470

Epoch 7: val_loss improved from 862.54291 to 749.84705, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 108s - loss: 790.0508 - MinusLogProbMetric: 790.0508 - val_loss: 749.8470 - val_MinusLogProbMetric: 749.8470 - lr: 1.2346e-05 - 108s/epoch - 550ms/step
Epoch 8/1000
2023-09-29 19:10:05.883 
Epoch 8/1000 
	 loss: 709.2399, MinusLogProbMetric: 709.2399, val_loss: 684.3957, val_MinusLogProbMetric: 684.3957

Epoch 8: val_loss improved from 749.84705 to 684.39569, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 709.2399 - MinusLogProbMetric: 709.2399 - val_loss: 684.3957 - val_MinusLogProbMetric: 684.3957 - lr: 1.2346e-05 - 111s/epoch - 569ms/step
Epoch 9/1000
2023-09-29 19:11:56.095 
Epoch 9/1000 
	 loss: 663.9795, MinusLogProbMetric: 663.9795, val_loss: 647.5158, val_MinusLogProbMetric: 647.5158

Epoch 9: val_loss improved from 684.39569 to 647.51581, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 663.9795 - MinusLogProbMetric: 663.9795 - val_loss: 647.5158 - val_MinusLogProbMetric: 647.5158 - lr: 1.2346e-05 - 110s/epoch - 562ms/step
Epoch 10/1000
2023-09-29 19:13:46.569 
Epoch 10/1000 
	 loss: 629.7095, MinusLogProbMetric: 629.7095, val_loss: 614.4185, val_MinusLogProbMetric: 614.4185

Epoch 10: val_loss improved from 647.51581 to 614.41846, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 629.7095 - MinusLogProbMetric: 629.7095 - val_loss: 614.4185 - val_MinusLogProbMetric: 614.4185 - lr: 1.2346e-05 - 110s/epoch - 563ms/step
Epoch 11/1000
2023-09-29 19:15:37.560 
Epoch 11/1000 
	 loss: 611.7699, MinusLogProbMetric: 611.7699, val_loss: 590.8200, val_MinusLogProbMetric: 590.8200

Epoch 11: val_loss improved from 614.41846 to 590.82001, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 611.7699 - MinusLogProbMetric: 611.7699 - val_loss: 590.8200 - val_MinusLogProbMetric: 590.8200 - lr: 1.2346e-05 - 111s/epoch - 566ms/step
Epoch 12/1000
2023-09-29 19:17:21.268 
Epoch 12/1000 
	 loss: 630.1681, MinusLogProbMetric: 630.1681, val_loss: 635.2715, val_MinusLogProbMetric: 635.2715

Epoch 12: val_loss did not improve from 590.82001
196/196 - 102s - loss: 630.1681 - MinusLogProbMetric: 630.1681 - val_loss: 635.2715 - val_MinusLogProbMetric: 635.2715 - lr: 1.2346e-05 - 102s/epoch - 522ms/step
Epoch 13/1000
2023-09-29 19:19:08.840 
Epoch 13/1000 
	 loss: 610.4318, MinusLogProbMetric: 610.4318, val_loss: 593.6746, val_MinusLogProbMetric: 593.6746

Epoch 13: val_loss did not improve from 590.82001
196/196 - 108s - loss: 610.4318 - MinusLogProbMetric: 610.4318 - val_loss: 593.6746 - val_MinusLogProbMetric: 593.6746 - lr: 1.2346e-05 - 108s/epoch - 549ms/step
Epoch 14/1000
2023-09-29 19:20:56.148 
Epoch 14/1000 
	 loss: 583.3365, MinusLogProbMetric: 583.3365, val_loss: 570.2438, val_MinusLogProbMetric: 570.2438

Epoch 14: val_loss improved from 590.82001 to 570.24377, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 583.3365 - MinusLogProbMetric: 583.3365 - val_loss: 570.2438 - val_MinusLogProbMetric: 570.2438 - lr: 1.2346e-05 - 109s/epoch - 554ms/step
Epoch 15/1000
2023-09-29 19:22:43.579 
Epoch 15/1000 
	 loss: 555.9979, MinusLogProbMetric: 555.9979, val_loss: 545.2130, val_MinusLogProbMetric: 545.2130

Epoch 15: val_loss improved from 570.24377 to 545.21295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 555.9979 - MinusLogProbMetric: 555.9979 - val_loss: 545.2130 - val_MinusLogProbMetric: 545.2130 - lr: 1.2346e-05 - 107s/epoch - 548ms/step
Epoch 16/1000
2023-09-29 19:24:30.928 
Epoch 16/1000 
	 loss: 536.9120, MinusLogProbMetric: 536.9120, val_loss: 528.8453, val_MinusLogProbMetric: 528.8453

Epoch 16: val_loss improved from 545.21295 to 528.84534, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 108s - loss: 536.9120 - MinusLogProbMetric: 536.9120 - val_loss: 528.8453 - val_MinusLogProbMetric: 528.8453 - lr: 1.2346e-05 - 108s/epoch - 550ms/step
Epoch 17/1000
2023-09-29 19:26:20.292 
Epoch 17/1000 
	 loss: 522.5161, MinusLogProbMetric: 522.5161, val_loss: 517.3892, val_MinusLogProbMetric: 517.3892

Epoch 17: val_loss improved from 528.84534 to 517.38922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 522.5161 - MinusLogProbMetric: 522.5161 - val_loss: 517.3892 - val_MinusLogProbMetric: 517.3892 - lr: 1.2346e-05 - 109s/epoch - 556ms/step
Epoch 18/1000
2023-09-29 19:28:07.684 
Epoch 18/1000 
	 loss: 511.4229, MinusLogProbMetric: 511.4229, val_loss: 506.7471, val_MinusLogProbMetric: 506.7471

Epoch 18: val_loss improved from 517.38922 to 506.74710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 511.4229 - MinusLogProbMetric: 511.4229 - val_loss: 506.7471 - val_MinusLogProbMetric: 506.7471 - lr: 1.2346e-05 - 107s/epoch - 547ms/step
Epoch 19/1000
2023-09-29 19:29:50.752 
Epoch 19/1000 
	 loss: 499.1082, MinusLogProbMetric: 499.1082, val_loss: 493.0524, val_MinusLogProbMetric: 493.0524

Epoch 19: val_loss improved from 506.74710 to 493.05237, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 103s - loss: 499.1082 - MinusLogProbMetric: 499.1082 - val_loss: 493.0524 - val_MinusLogProbMetric: 493.0524 - lr: 1.2346e-05 - 103s/epoch - 527ms/step
Epoch 20/1000
2023-09-29 19:31:33.631 
Epoch 20/1000 
	 loss: 486.9119, MinusLogProbMetric: 486.9119, val_loss: 481.4516, val_MinusLogProbMetric: 481.4516

Epoch 20: val_loss improved from 493.05237 to 481.45157, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 103s - loss: 486.9119 - MinusLogProbMetric: 486.9119 - val_loss: 481.4516 - val_MinusLogProbMetric: 481.4516 - lr: 1.2346e-05 - 103s/epoch - 525ms/step
Epoch 21/1000
2023-09-29 19:33:18.681 
Epoch 21/1000 
	 loss: 475.1896, MinusLogProbMetric: 475.1896, val_loss: 470.9780, val_MinusLogProbMetric: 470.9780

Epoch 21: val_loss improved from 481.45157 to 470.97800, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 106s - loss: 475.1896 - MinusLogProbMetric: 475.1896 - val_loss: 470.9780 - val_MinusLogProbMetric: 470.9780 - lr: 1.2346e-05 - 106s/epoch - 539ms/step
Epoch 22/1000
2023-09-29 19:35:05.126 
Epoch 22/1000 
	 loss: 466.7509, MinusLogProbMetric: 466.7509, val_loss: 486.9639, val_MinusLogProbMetric: 486.9639

Epoch 22: val_loss did not improve from 470.97800
196/196 - 105s - loss: 466.7509 - MinusLogProbMetric: 466.7509 - val_loss: 486.9639 - val_MinusLogProbMetric: 486.9639 - lr: 1.2346e-05 - 105s/epoch - 534ms/step
Epoch 23/1000
2023-09-29 19:36:54.186 
Epoch 23/1000 
	 loss: 532.8893, MinusLogProbMetric: 532.8893, val_loss: 529.4490, val_MinusLogProbMetric: 529.4490

Epoch 23: val_loss did not improve from 470.97800
196/196 - 109s - loss: 532.8893 - MinusLogProbMetric: 532.8893 - val_loss: 529.4490 - val_MinusLogProbMetric: 529.4490 - lr: 1.2346e-05 - 109s/epoch - 556ms/step
Epoch 24/1000
2023-09-29 19:38:39.658 
Epoch 24/1000 
	 loss: 460.7249, MinusLogProbMetric: 460.7249, val_loss: 435.6297, val_MinusLogProbMetric: 435.6297

Epoch 24: val_loss improved from 470.97800 to 435.62970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 460.7249 - MinusLogProbMetric: 460.7249 - val_loss: 435.6297 - val_MinusLogProbMetric: 435.6297 - lr: 1.2346e-05 - 107s/epoch - 544ms/step
Epoch 25/1000
2023-09-29 19:40:26.241 
Epoch 25/1000 
	 loss: 424.3080, MinusLogProbMetric: 424.3080, val_loss: 420.1816, val_MinusLogProbMetric: 420.1816

Epoch 25: val_loss improved from 435.62970 to 420.18158, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 424.3080 - MinusLogProbMetric: 424.3080 - val_loss: 420.1816 - val_MinusLogProbMetric: 420.1816 - lr: 1.2346e-05 - 107s/epoch - 544ms/step
Epoch 26/1000
2023-09-29 19:42:14.750 
Epoch 26/1000 
	 loss: 413.9437, MinusLogProbMetric: 413.9437, val_loss: 408.1560, val_MinusLogProbMetric: 408.1560

Epoch 26: val_loss improved from 420.18158 to 408.15598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 108s - loss: 413.9437 - MinusLogProbMetric: 413.9437 - val_loss: 408.1560 - val_MinusLogProbMetric: 408.1560 - lr: 1.2346e-05 - 108s/epoch - 553ms/step
Epoch 27/1000
2023-09-29 19:44:00.384 
Epoch 27/1000 
	 loss: 477.3504, MinusLogProbMetric: 477.3504, val_loss: 478.4440, val_MinusLogProbMetric: 478.4440

Epoch 27: val_loss did not improve from 408.15598
196/196 - 104s - loss: 477.3504 - MinusLogProbMetric: 477.3504 - val_loss: 478.4440 - val_MinusLogProbMetric: 478.4440 - lr: 1.2346e-05 - 104s/epoch - 533ms/step
Epoch 28/1000
2023-09-29 19:45:39.621 
Epoch 28/1000 
	 loss: 494.3333, MinusLogProbMetric: 494.3333, val_loss: 490.9928, val_MinusLogProbMetric: 490.9928

Epoch 28: val_loss did not improve from 408.15598
196/196 - 99s - loss: 494.3333 - MinusLogProbMetric: 494.3333 - val_loss: 490.9928 - val_MinusLogProbMetric: 490.9928 - lr: 1.2346e-05 - 99s/epoch - 506ms/step
Epoch 29/1000
2023-09-29 19:47:24.353 
Epoch 29/1000 
	 loss: 480.6357, MinusLogProbMetric: 480.6357, val_loss: 474.2988, val_MinusLogProbMetric: 474.2988

Epoch 29: val_loss did not improve from 408.15598
196/196 - 105s - loss: 480.6357 - MinusLogProbMetric: 480.6357 - val_loss: 474.2988 - val_MinusLogProbMetric: 474.2988 - lr: 1.2346e-05 - 105s/epoch - 534ms/step
Epoch 30/1000
2023-09-29 19:49:09.926 
Epoch 30/1000 
	 loss: 468.8802, MinusLogProbMetric: 468.8802, val_loss: 464.9313, val_MinusLogProbMetric: 464.9313

Epoch 30: val_loss did not improve from 408.15598
196/196 - 106s - loss: 468.8802 - MinusLogProbMetric: 468.8802 - val_loss: 464.9313 - val_MinusLogProbMetric: 464.9313 - lr: 1.2346e-05 - 106s/epoch - 539ms/step
Epoch 31/1000
2023-09-29 19:50:54.970 
Epoch 31/1000 
	 loss: 460.8414, MinusLogProbMetric: 460.8414, val_loss: 458.1315, val_MinusLogProbMetric: 458.1315

Epoch 31: val_loss did not improve from 408.15598
196/196 - 105s - loss: 460.8414 - MinusLogProbMetric: 460.8414 - val_loss: 458.1315 - val_MinusLogProbMetric: 458.1315 - lr: 1.2346e-05 - 105s/epoch - 536ms/step
Epoch 32/1000
2023-09-29 19:52:40.949 
Epoch 32/1000 
	 loss: 454.0936, MinusLogProbMetric: 454.0936, val_loss: 452.5515, val_MinusLogProbMetric: 452.5515

Epoch 32: val_loss did not improve from 408.15598
196/196 - 106s - loss: 454.0936 - MinusLogProbMetric: 454.0936 - val_loss: 452.5515 - val_MinusLogProbMetric: 452.5515 - lr: 1.2346e-05 - 106s/epoch - 541ms/step
Epoch 33/1000
2023-09-29 19:54:25.678 
Epoch 33/1000 
	 loss: 448.4676, MinusLogProbMetric: 448.4676, val_loss: 446.0545, val_MinusLogProbMetric: 446.0545

Epoch 33: val_loss did not improve from 408.15598
196/196 - 105s - loss: 448.4676 - MinusLogProbMetric: 448.4676 - val_loss: 446.0545 - val_MinusLogProbMetric: 446.0545 - lr: 1.2346e-05 - 105s/epoch - 534ms/step
Epoch 34/1000
2023-09-29 19:56:07.235 
Epoch 34/1000 
	 loss: 443.1247, MinusLogProbMetric: 443.1247, val_loss: 441.6571, val_MinusLogProbMetric: 441.6571

Epoch 34: val_loss did not improve from 408.15598
196/196 - 102s - loss: 443.1247 - MinusLogProbMetric: 443.1247 - val_loss: 441.6571 - val_MinusLogProbMetric: 441.6571 - lr: 1.2346e-05 - 102s/epoch - 518ms/step
Epoch 35/1000
2023-09-29 19:57:47.969 
Epoch 35/1000 
	 loss: 439.1081, MinusLogProbMetric: 439.1081, val_loss: 437.4060, val_MinusLogProbMetric: 437.4060

Epoch 35: val_loss did not improve from 408.15598
196/196 - 101s - loss: 439.1081 - MinusLogProbMetric: 439.1081 - val_loss: 437.4060 - val_MinusLogProbMetric: 437.4060 - lr: 1.2346e-05 - 101s/epoch - 514ms/step
Epoch 36/1000
2023-09-29 19:59:26.211 
Epoch 36/1000 
	 loss: 434.9457, MinusLogProbMetric: 434.9457, val_loss: 433.2561, val_MinusLogProbMetric: 433.2561

Epoch 36: val_loss did not improve from 408.15598
196/196 - 98s - loss: 434.9457 - MinusLogProbMetric: 434.9457 - val_loss: 433.2561 - val_MinusLogProbMetric: 433.2561 - lr: 1.2346e-05 - 98s/epoch - 501ms/step
Epoch 37/1000
2023-09-29 20:01:06.803 
Epoch 37/1000 
	 loss: 430.9338, MinusLogProbMetric: 430.9338, val_loss: 429.7601, val_MinusLogProbMetric: 429.7601

Epoch 37: val_loss did not improve from 408.15598
196/196 - 101s - loss: 430.9338 - MinusLogProbMetric: 430.9338 - val_loss: 429.7601 - val_MinusLogProbMetric: 429.7601 - lr: 1.2346e-05 - 101s/epoch - 513ms/step
Epoch 38/1000
2023-09-29 20:02:45.457 
Epoch 38/1000 
	 loss: 425.9929, MinusLogProbMetric: 425.9929, val_loss: 423.8068, val_MinusLogProbMetric: 423.8068

Epoch 38: val_loss did not improve from 408.15598
196/196 - 99s - loss: 425.9929 - MinusLogProbMetric: 425.9929 - val_loss: 423.8068 - val_MinusLogProbMetric: 423.8068 - lr: 1.2346e-05 - 99s/epoch - 503ms/step
Epoch 39/1000
2023-09-29 20:04:27.450 
Epoch 39/1000 
	 loss: 420.8472, MinusLogProbMetric: 420.8472, val_loss: 418.8781, val_MinusLogProbMetric: 418.8781

Epoch 39: val_loss did not improve from 408.15598
196/196 - 102s - loss: 420.8472 - MinusLogProbMetric: 420.8472 - val_loss: 418.8781 - val_MinusLogProbMetric: 418.8781 - lr: 1.2346e-05 - 102s/epoch - 520ms/step
Epoch 40/1000
2023-09-29 20:06:06.918 
Epoch 40/1000 
	 loss: 416.4713, MinusLogProbMetric: 416.4713, val_loss: 415.6083, val_MinusLogProbMetric: 415.6083

Epoch 40: val_loss did not improve from 408.15598
196/196 - 99s - loss: 416.4713 - MinusLogProbMetric: 416.4713 - val_loss: 415.6083 - val_MinusLogProbMetric: 415.6083 - lr: 1.2346e-05 - 99s/epoch - 507ms/step
Epoch 41/1000
2023-09-29 20:07:51.223 
Epoch 41/1000 
	 loss: 399.5338, MinusLogProbMetric: 399.5338, val_loss: 384.6876, val_MinusLogProbMetric: 384.6876

Epoch 41: val_loss improved from 408.15598 to 384.68759, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 105s - loss: 399.5338 - MinusLogProbMetric: 399.5338 - val_loss: 384.6876 - val_MinusLogProbMetric: 384.6876 - lr: 1.2346e-05 - 105s/epoch - 538ms/step
Epoch 42/1000
2023-09-29 20:09:32.880 
Epoch 42/1000 
	 loss: 382.8525, MinusLogProbMetric: 382.8525, val_loss: 381.7281, val_MinusLogProbMetric: 381.7281

Epoch 42: val_loss improved from 384.68759 to 381.72809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 102s - loss: 382.8525 - MinusLogProbMetric: 382.8525 - val_loss: 381.7281 - val_MinusLogProbMetric: 381.7281 - lr: 1.2346e-05 - 102s/epoch - 518ms/step
Epoch 43/1000
2023-09-29 20:11:11.098 
Epoch 43/1000 
	 loss: 402.6129, MinusLogProbMetric: 402.6129, val_loss: 381.4973, val_MinusLogProbMetric: 381.4973

Epoch 43: val_loss improved from 381.72809 to 381.49731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 98s - loss: 402.6129 - MinusLogProbMetric: 402.6129 - val_loss: 381.4973 - val_MinusLogProbMetric: 381.4973 - lr: 1.2346e-05 - 98s/epoch - 502ms/step
Epoch 44/1000
2023-09-29 20:12:58.453 
Epoch 44/1000 
	 loss: 377.6752, MinusLogProbMetric: 377.6752, val_loss: 374.2556, val_MinusLogProbMetric: 374.2556

Epoch 44: val_loss improved from 381.49731 to 374.25565, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 108s - loss: 377.6752 - MinusLogProbMetric: 377.6752 - val_loss: 374.2556 - val_MinusLogProbMetric: 374.2556 - lr: 1.2346e-05 - 108s/epoch - 550ms/step
Epoch 45/1000
2023-09-29 20:14:39.934 
Epoch 45/1000 
	 loss: 372.1211, MinusLogProbMetric: 372.1211, val_loss: 370.4010, val_MinusLogProbMetric: 370.4010

Epoch 45: val_loss improved from 374.25565 to 370.40103, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 101s - loss: 372.1211 - MinusLogProbMetric: 372.1211 - val_loss: 370.4010 - val_MinusLogProbMetric: 370.4010 - lr: 1.2346e-05 - 101s/epoch - 516ms/step
Epoch 46/1000
2023-09-29 20:16:27.106 
Epoch 46/1000 
	 loss: 368.5359, MinusLogProbMetric: 368.5359, val_loss: 367.5225, val_MinusLogProbMetric: 367.5225

Epoch 46: val_loss improved from 370.40103 to 367.52252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 368.5359 - MinusLogProbMetric: 368.5359 - val_loss: 367.5225 - val_MinusLogProbMetric: 367.5225 - lr: 1.2346e-05 - 107s/epoch - 546ms/step
Epoch 47/1000
2023-09-29 20:18:07.438 
Epoch 47/1000 
	 loss: 367.7588, MinusLogProbMetric: 367.7588, val_loss: 397.3828, val_MinusLogProbMetric: 397.3828

Epoch 47: val_loss did not improve from 367.52252
196/196 - 99s - loss: 367.7588 - MinusLogProbMetric: 367.7588 - val_loss: 397.3828 - val_MinusLogProbMetric: 397.3828 - lr: 1.2346e-05 - 99s/epoch - 507ms/step
Epoch 48/1000
2023-09-29 20:19:46.210 
Epoch 48/1000 
	 loss: 386.0949, MinusLogProbMetric: 386.0949, val_loss: 378.1288, val_MinusLogProbMetric: 378.1288

Epoch 48: val_loss did not improve from 367.52252
196/196 - 99s - loss: 386.0949 - MinusLogProbMetric: 386.0949 - val_loss: 378.1288 - val_MinusLogProbMetric: 378.1288 - lr: 1.2346e-05 - 99s/epoch - 504ms/step
Epoch 49/1000
2023-09-29 20:21:28.216 
Epoch 49/1000 
	 loss: 375.3571, MinusLogProbMetric: 375.3571, val_loss: 373.4039, val_MinusLogProbMetric: 373.4039

Epoch 49: val_loss did not improve from 367.52252
196/196 - 102s - loss: 375.3571 - MinusLogProbMetric: 375.3571 - val_loss: 373.4039 - val_MinusLogProbMetric: 373.4039 - lr: 1.2346e-05 - 102s/epoch - 520ms/step
Epoch 50/1000
2023-09-29 20:23:09.149 
Epoch 50/1000 
	 loss: 367.8680, MinusLogProbMetric: 367.8680, val_loss: 363.7576, val_MinusLogProbMetric: 363.7576

Epoch 50: val_loss improved from 367.52252 to 363.75760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 102s - loss: 367.8680 - MinusLogProbMetric: 367.8680 - val_loss: 363.7576 - val_MinusLogProbMetric: 363.7576 - lr: 1.2346e-05 - 102s/epoch - 520ms/step
Epoch 51/1000
2023-09-29 20:24:49.380 
Epoch 51/1000 
	 loss: 360.5870, MinusLogProbMetric: 360.5870, val_loss: 357.5145, val_MinusLogProbMetric: 357.5145

Epoch 51: val_loss improved from 363.75760 to 357.51453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 100s - loss: 360.5870 - MinusLogProbMetric: 360.5870 - val_loss: 357.5145 - val_MinusLogProbMetric: 357.5145 - lr: 1.2346e-05 - 100s/epoch - 513ms/step
Epoch 52/1000
2023-09-29 20:26:31.350 
Epoch 52/1000 
	 loss: 356.3460, MinusLogProbMetric: 356.3460, val_loss: 355.6026, val_MinusLogProbMetric: 355.6026

Epoch 52: val_loss improved from 357.51453 to 355.60260, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 102s - loss: 356.3460 - MinusLogProbMetric: 356.3460 - val_loss: 355.6026 - val_MinusLogProbMetric: 355.6026 - lr: 1.2346e-05 - 102s/epoch - 520ms/step
Epoch 53/1000
2023-09-29 20:28:09.643 
Epoch 53/1000 
	 loss: 354.8828, MinusLogProbMetric: 354.8828, val_loss: 353.5991, val_MinusLogProbMetric: 353.5991

Epoch 53: val_loss improved from 355.60260 to 353.59906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 98s - loss: 354.8828 - MinusLogProbMetric: 354.8828 - val_loss: 353.5991 - val_MinusLogProbMetric: 353.5991 - lr: 1.2346e-05 - 98s/epoch - 502ms/step
Epoch 54/1000
2023-09-29 20:29:54.163 
Epoch 54/1000 
	 loss: 352.1449, MinusLogProbMetric: 352.1449, val_loss: 350.7611, val_MinusLogProbMetric: 350.7611

Epoch 54: val_loss improved from 353.59906 to 350.76111, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 104s - loss: 352.1449 - MinusLogProbMetric: 352.1449 - val_loss: 350.7611 - val_MinusLogProbMetric: 350.7611 - lr: 1.2346e-05 - 104s/epoch - 533ms/step
Epoch 55/1000
2023-09-29 20:31:33.920 
Epoch 55/1000 
	 loss: 349.8716, MinusLogProbMetric: 349.8716, val_loss: 348.8742, val_MinusLogProbMetric: 348.8742

Epoch 55: val_loss improved from 350.76111 to 348.87421, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 100s - loss: 349.8716 - MinusLogProbMetric: 349.8716 - val_loss: 348.8742 - val_MinusLogProbMetric: 348.8742 - lr: 1.2346e-05 - 100s/epoch - 509ms/step
Epoch 56/1000
2023-09-29 20:33:17.567 
Epoch 56/1000 
	 loss: 362.2557, MinusLogProbMetric: 362.2557, val_loss: 351.3199, val_MinusLogProbMetric: 351.3199

Epoch 56: val_loss did not improve from 348.87421
196/196 - 103s - loss: 362.2557 - MinusLogProbMetric: 362.2557 - val_loss: 351.3199 - val_MinusLogProbMetric: 351.3199 - lr: 1.2346e-05 - 103s/epoch - 523ms/step
Epoch 57/1000
2023-09-29 20:34:56.542 
Epoch 57/1000 
	 loss: 344.3509, MinusLogProbMetric: 344.3509, val_loss: 342.3249, val_MinusLogProbMetric: 342.3249

Epoch 57: val_loss improved from 348.87421 to 342.32492, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 100s - loss: 344.3509 - MinusLogProbMetric: 344.3509 - val_loss: 342.3249 - val_MinusLogProbMetric: 342.3249 - lr: 1.2346e-05 - 100s/epoch - 512ms/step
Epoch 58/1000
2023-09-29 20:36:36.643 
Epoch 58/1000 
	 loss: 340.4734, MinusLogProbMetric: 340.4734, val_loss: 339.4886, val_MinusLogProbMetric: 339.4886

Epoch 58: val_loss improved from 342.32492 to 339.48859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 100s - loss: 340.4734 - MinusLogProbMetric: 340.4734 - val_loss: 339.4886 - val_MinusLogProbMetric: 339.4886 - lr: 1.2346e-05 - 100s/epoch - 508ms/step
Epoch 59/1000
2023-09-29 20:38:21.742 
Epoch 59/1000 
	 loss: 337.7130, MinusLogProbMetric: 337.7130, val_loss: 336.7864, val_MinusLogProbMetric: 336.7864

Epoch 59: val_loss improved from 339.48859 to 336.78638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 106s - loss: 337.7130 - MinusLogProbMetric: 337.7130 - val_loss: 336.7864 - val_MinusLogProbMetric: 336.7864 - lr: 1.2346e-05 - 106s/epoch - 539ms/step
Epoch 60/1000
2023-09-29 20:40:04.567 
Epoch 60/1000 
	 loss: 443.8916, MinusLogProbMetric: 443.8916, val_loss: 409.3899, val_MinusLogProbMetric: 409.3899

Epoch 60: val_loss did not improve from 336.78638
196/196 - 101s - loss: 443.8916 - MinusLogProbMetric: 443.8916 - val_loss: 409.3899 - val_MinusLogProbMetric: 409.3899 - lr: 1.2346e-05 - 101s/epoch - 518ms/step
Epoch 61/1000
2023-09-29 20:41:46.217 
Epoch 61/1000 
	 loss: 396.2627, MinusLogProbMetric: 396.2627, val_loss: 390.2632, val_MinusLogProbMetric: 390.2632

Epoch 61: val_loss did not improve from 336.78638
196/196 - 102s - loss: 396.2627 - MinusLogProbMetric: 396.2627 - val_loss: 390.2632 - val_MinusLogProbMetric: 390.2632 - lr: 1.2346e-05 - 102s/epoch - 519ms/step
Epoch 62/1000
2023-09-29 20:43:28.933 
Epoch 62/1000 
	 loss: 382.9911, MinusLogProbMetric: 382.9911, val_loss: 379.4302, val_MinusLogProbMetric: 379.4302

Epoch 62: val_loss did not improve from 336.78638
196/196 - 103s - loss: 382.9911 - MinusLogProbMetric: 382.9911 - val_loss: 379.4302 - val_MinusLogProbMetric: 379.4302 - lr: 1.2346e-05 - 103s/epoch - 524ms/step
Epoch 63/1000
2023-09-29 20:45:08.852 
Epoch 63/1000 
	 loss: 374.4554, MinusLogProbMetric: 374.4554, val_loss: 369.9527, val_MinusLogProbMetric: 369.9527

Epoch 63: val_loss did not improve from 336.78638
196/196 - 100s - loss: 374.4554 - MinusLogProbMetric: 374.4554 - val_loss: 369.9527 - val_MinusLogProbMetric: 369.9527 - lr: 1.2346e-05 - 100s/epoch - 510ms/step
Epoch 64/1000
2023-09-29 20:46:48.063 
Epoch 64/1000 
	 loss: 366.0823, MinusLogProbMetric: 366.0823, val_loss: 365.0674, val_MinusLogProbMetric: 365.0674

Epoch 64: val_loss did not improve from 336.78638
196/196 - 99s - loss: 366.0823 - MinusLogProbMetric: 366.0823 - val_loss: 365.0674 - val_MinusLogProbMetric: 365.0674 - lr: 1.2346e-05 - 99s/epoch - 506ms/step
Epoch 65/1000
2023-09-29 20:48:34.081 
Epoch 65/1000 
	 loss: 360.7227, MinusLogProbMetric: 360.7227, val_loss: 358.5475, val_MinusLogProbMetric: 358.5475

Epoch 65: val_loss did not improve from 336.78638
196/196 - 106s - loss: 360.7227 - MinusLogProbMetric: 360.7227 - val_loss: 358.5475 - val_MinusLogProbMetric: 358.5475 - lr: 1.2346e-05 - 106s/epoch - 541ms/step
Epoch 66/1000
2023-09-29 20:50:13.520 
Epoch 66/1000 
	 loss: 354.6189, MinusLogProbMetric: 354.6189, val_loss: 352.6310, val_MinusLogProbMetric: 352.6310

Epoch 66: val_loss did not improve from 336.78638
196/196 - 99s - loss: 354.6189 - MinusLogProbMetric: 354.6189 - val_loss: 352.6310 - val_MinusLogProbMetric: 352.6310 - lr: 1.2346e-05 - 99s/epoch - 507ms/step
Epoch 67/1000
2023-09-29 20:51:53.392 
Epoch 67/1000 
	 loss: 350.2852, MinusLogProbMetric: 350.2852, val_loss: 348.7409, val_MinusLogProbMetric: 348.7409

Epoch 67: val_loss did not improve from 336.78638
196/196 - 100s - loss: 350.2852 - MinusLogProbMetric: 350.2852 - val_loss: 348.7409 - val_MinusLogProbMetric: 348.7409 - lr: 1.2346e-05 - 100s/epoch - 510ms/step
Epoch 68/1000
2023-09-29 20:53:35.790 
Epoch 68/1000 
	 loss: 346.0386, MinusLogProbMetric: 346.0386, val_loss: 344.3912, val_MinusLogProbMetric: 344.3912

Epoch 68: val_loss did not improve from 336.78638
196/196 - 102s - loss: 346.0386 - MinusLogProbMetric: 346.0386 - val_loss: 344.3912 - val_MinusLogProbMetric: 344.3912 - lr: 1.2346e-05 - 102s/epoch - 522ms/step
Epoch 69/1000
2023-09-29 20:55:17.210 
Epoch 69/1000 
	 loss: 341.1969, MinusLogProbMetric: 341.1969, val_loss: 339.5097, val_MinusLogProbMetric: 339.5097

Epoch 69: val_loss did not improve from 336.78638
196/196 - 101s - loss: 341.1969 - MinusLogProbMetric: 341.1969 - val_loss: 339.5097 - val_MinusLogProbMetric: 339.5097 - lr: 1.2346e-05 - 101s/epoch - 517ms/step
Epoch 70/1000
2023-09-29 20:56:57.876 
Epoch 70/1000 
	 loss: 339.3823, MinusLogProbMetric: 339.3823, val_loss: 337.4368, val_MinusLogProbMetric: 337.4368

Epoch 70: val_loss did not improve from 336.78638
196/196 - 101s - loss: 339.3823 - MinusLogProbMetric: 339.3823 - val_loss: 337.4368 - val_MinusLogProbMetric: 337.4368 - lr: 1.2346e-05 - 101s/epoch - 514ms/step
Epoch 71/1000
2023-09-29 20:58:41.164 
Epoch 71/1000 
	 loss: 334.9469, MinusLogProbMetric: 334.9469, val_loss: 334.1058, val_MinusLogProbMetric: 334.1058

Epoch 71: val_loss improved from 336.78638 to 334.10583, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 105s - loss: 334.9469 - MinusLogProbMetric: 334.9469 - val_loss: 334.1058 - val_MinusLogProbMetric: 334.1058 - lr: 1.2346e-05 - 105s/epoch - 535ms/step
Epoch 72/1000
2023-09-29 21:00:30.951 
Epoch 72/1000 
	 loss: 332.4060, MinusLogProbMetric: 332.4060, val_loss: 332.0094, val_MinusLogProbMetric: 332.0094

Epoch 72: val_loss improved from 334.10583 to 332.00937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 332.4060 - MinusLogProbMetric: 332.4060 - val_loss: 332.0094 - val_MinusLogProbMetric: 332.0094 - lr: 1.2346e-05 - 109s/epoch - 558ms/step
Epoch 73/1000
2023-09-29 21:02:23.259 
Epoch 73/1000 
	 loss: 329.9461, MinusLogProbMetric: 329.9461, val_loss: 329.4349, val_MinusLogProbMetric: 329.4349

Epoch 73: val_loss improved from 332.00937 to 329.43491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 112s - loss: 329.9461 - MinusLogProbMetric: 329.9461 - val_loss: 329.4349 - val_MinusLogProbMetric: 329.4349 - lr: 1.2346e-05 - 112s/epoch - 574ms/step
Epoch 74/1000
2023-09-29 21:04:08.087 
Epoch 74/1000 
	 loss: 327.6559, MinusLogProbMetric: 327.6559, val_loss: 327.6646, val_MinusLogProbMetric: 327.6646

Epoch 74: val_loss improved from 329.43491 to 327.66458, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 104s - loss: 327.6559 - MinusLogProbMetric: 327.6559 - val_loss: 327.6646 - val_MinusLogProbMetric: 327.6646 - lr: 1.2346e-05 - 104s/epoch - 533ms/step
Epoch 75/1000
2023-09-29 21:05:57.444 
Epoch 75/1000 
	 loss: 325.2710, MinusLogProbMetric: 325.2710, val_loss: 324.7347, val_MinusLogProbMetric: 324.7347

Epoch 75: val_loss improved from 327.66458 to 324.73468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 325.2710 - MinusLogProbMetric: 325.2710 - val_loss: 324.7347 - val_MinusLogProbMetric: 324.7347 - lr: 1.2346e-05 - 110s/epoch - 559ms/step
Epoch 76/1000
2023-09-29 21:07:41.766 
Epoch 76/1000 
	 loss: 322.9543, MinusLogProbMetric: 322.9543, val_loss: 322.4776, val_MinusLogProbMetric: 322.4776

Epoch 76: val_loss improved from 324.73468 to 322.47763, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 104s - loss: 322.9543 - MinusLogProbMetric: 322.9543 - val_loss: 322.4776 - val_MinusLogProbMetric: 322.4776 - lr: 1.2346e-05 - 104s/epoch - 532ms/step
Epoch 77/1000
2023-09-29 21:09:28.189 
Epoch 77/1000 
	 loss: 320.5533, MinusLogProbMetric: 320.5533, val_loss: 320.8371, val_MinusLogProbMetric: 320.8371

Epoch 77: val_loss improved from 322.47763 to 320.83710, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 106s - loss: 320.5533 - MinusLogProbMetric: 320.5533 - val_loss: 320.8371 - val_MinusLogProbMetric: 320.8371 - lr: 1.2346e-05 - 106s/epoch - 541ms/step
Epoch 78/1000
2023-09-29 21:11:08.389 
Epoch 78/1000 
	 loss: 319.3297, MinusLogProbMetric: 319.3297, val_loss: 318.9060, val_MinusLogProbMetric: 318.9060

Epoch 78: val_loss improved from 320.83710 to 318.90598, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 100s - loss: 319.3297 - MinusLogProbMetric: 319.3297 - val_loss: 318.9060 - val_MinusLogProbMetric: 318.9060 - lr: 1.2346e-05 - 100s/epoch - 512ms/step
Epoch 79/1000
2023-09-29 21:12:55.218 
Epoch 79/1000 
	 loss: 317.1704, MinusLogProbMetric: 317.1704, val_loss: 316.6525, val_MinusLogProbMetric: 316.6525

Epoch 79: val_loss improved from 318.90598 to 316.65250, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 317.1704 - MinusLogProbMetric: 317.1704 - val_loss: 316.6525 - val_MinusLogProbMetric: 316.6525 - lr: 1.2346e-05 - 107s/epoch - 545ms/step
Epoch 80/1000
2023-09-29 21:14:44.511 
Epoch 80/1000 
	 loss: 315.0225, MinusLogProbMetric: 315.0225, val_loss: 315.4168, val_MinusLogProbMetric: 315.4168

Epoch 80: val_loss improved from 316.65250 to 315.41681, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 315.0225 - MinusLogProbMetric: 315.0225 - val_loss: 315.4168 - val_MinusLogProbMetric: 315.4168 - lr: 1.2346e-05 - 110s/epoch - 559ms/step
Epoch 81/1000
2023-09-29 21:16:35.708 
Epoch 81/1000 
	 loss: 313.2887, MinusLogProbMetric: 313.2887, val_loss: 313.0364, val_MinusLogProbMetric: 313.0364

Epoch 81: val_loss improved from 315.41681 to 313.03638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 313.2887 - MinusLogProbMetric: 313.2887 - val_loss: 313.0364 - val_MinusLogProbMetric: 313.0364 - lr: 1.2346e-05 - 111s/epoch - 565ms/step
Epoch 82/1000
2023-09-29 21:18:21.872 
Epoch 82/1000 
	 loss: 317.0847, MinusLogProbMetric: 317.0847, val_loss: 316.7706, val_MinusLogProbMetric: 316.7706

Epoch 82: val_loss did not improve from 313.03638
196/196 - 105s - loss: 317.0847 - MinusLogProbMetric: 317.0847 - val_loss: 316.7706 - val_MinusLogProbMetric: 316.7706 - lr: 1.2346e-05 - 105s/epoch - 536ms/step
Epoch 83/1000
2023-09-29 21:20:08.912 
Epoch 83/1000 
	 loss: 313.4372, MinusLogProbMetric: 313.4372, val_loss: 311.3567, val_MinusLogProbMetric: 311.3567

Epoch 83: val_loss improved from 313.03638 to 311.35669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 108s - loss: 313.4372 - MinusLogProbMetric: 313.4372 - val_loss: 311.3567 - val_MinusLogProbMetric: 311.3567 - lr: 1.2346e-05 - 108s/epoch - 553ms/step
Epoch 84/1000
2023-09-29 21:21:35.437 
Epoch 84/1000 
	 loss: 309.4203, MinusLogProbMetric: 309.4203, val_loss: 308.7596, val_MinusLogProbMetric: 308.7596

Epoch 84: val_loss improved from 311.35669 to 308.75955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 87s - loss: 309.4203 - MinusLogProbMetric: 309.4203 - val_loss: 308.7596 - val_MinusLogProbMetric: 308.7596 - lr: 1.2346e-05 - 87s/epoch - 443ms/step
Epoch 85/1000
2023-09-29 21:23:11.739 
Epoch 85/1000 
	 loss: 307.9808, MinusLogProbMetric: 307.9808, val_loss: 307.0133, val_MinusLogProbMetric: 307.0133

Epoch 85: val_loss improved from 308.75955 to 307.01331, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 96s - loss: 307.9808 - MinusLogProbMetric: 307.9808 - val_loss: 307.0133 - val_MinusLogProbMetric: 307.0133 - lr: 1.2346e-05 - 96s/epoch - 491ms/step
Epoch 86/1000
2023-09-29 21:25:00.383 
Epoch 86/1000 
	 loss: 305.8494, MinusLogProbMetric: 305.8494, val_loss: 306.1375, val_MinusLogProbMetric: 306.1375

Epoch 86: val_loss improved from 307.01331 to 306.13751, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 108s - loss: 305.8494 - MinusLogProbMetric: 305.8494 - val_loss: 306.1375 - val_MinusLogProbMetric: 306.1375 - lr: 1.2346e-05 - 108s/epoch - 552ms/step
Epoch 87/1000
2023-09-29 21:26:48.958 
Epoch 87/1000 
	 loss: 304.4457, MinusLogProbMetric: 304.4457, val_loss: 304.0944, val_MinusLogProbMetric: 304.0944

Epoch 87: val_loss improved from 306.13751 to 304.09439, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 304.4457 - MinusLogProbMetric: 304.4457 - val_loss: 304.0944 - val_MinusLogProbMetric: 304.0944 - lr: 1.2346e-05 - 109s/epoch - 556ms/step
Epoch 88/1000
2023-09-29 21:28:37.717 
Epoch 88/1000 
	 loss: 303.4693, MinusLogProbMetric: 303.4693, val_loss: 303.3912, val_MinusLogProbMetric: 303.3912

Epoch 88: val_loss improved from 304.09439 to 303.39120, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 303.4693 - MinusLogProbMetric: 303.4693 - val_loss: 303.3912 - val_MinusLogProbMetric: 303.3912 - lr: 1.2346e-05 - 109s/epoch - 554ms/step
Epoch 89/1000
2023-09-29 21:30:31.353 
Epoch 89/1000 
	 loss: 301.8640, MinusLogProbMetric: 301.8640, val_loss: 301.7400, val_MinusLogProbMetric: 301.7400

Epoch 89: val_loss improved from 303.39120 to 301.73999, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 113s - loss: 301.8640 - MinusLogProbMetric: 301.8640 - val_loss: 301.7400 - val_MinusLogProbMetric: 301.7400 - lr: 1.2346e-05 - 113s/epoch - 579ms/step
Epoch 90/1000
2023-09-29 21:32:22.635 
Epoch 90/1000 
	 loss: 300.2441, MinusLogProbMetric: 300.2441, val_loss: 299.9700, val_MinusLogProbMetric: 299.9700

Epoch 90: val_loss improved from 301.73999 to 299.97003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 300.2441 - MinusLogProbMetric: 300.2441 - val_loss: 299.9700 - val_MinusLogProbMetric: 299.9700 - lr: 1.2346e-05 - 111s/epoch - 568ms/step
Epoch 91/1000
2023-09-29 21:34:12.436 
Epoch 91/1000 
	 loss: 298.7728, MinusLogProbMetric: 298.7728, val_loss: 299.1013, val_MinusLogProbMetric: 299.1013

Epoch 91: val_loss improved from 299.97003 to 299.10126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 298.7728 - MinusLogProbMetric: 298.7728 - val_loss: 299.1013 - val_MinusLogProbMetric: 299.1013 - lr: 1.2346e-05 - 110s/epoch - 561ms/step
Epoch 92/1000
2023-09-29 21:36:00.974 
Epoch 92/1000 
	 loss: 297.2980, MinusLogProbMetric: 297.2980, val_loss: 297.7168, val_MinusLogProbMetric: 297.7168

Epoch 92: val_loss improved from 299.10126 to 297.71680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 297.2980 - MinusLogProbMetric: 297.2980 - val_loss: 297.7168 - val_MinusLogProbMetric: 297.7168 - lr: 1.2346e-05 - 109s/epoch - 554ms/step
Epoch 93/1000
2023-09-29 21:37:50.298 
Epoch 93/1000 
	 loss: 295.9649, MinusLogProbMetric: 295.9649, val_loss: 295.9782, val_MinusLogProbMetric: 295.9782

Epoch 93: val_loss improved from 297.71680 to 295.97824, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 295.9649 - MinusLogProbMetric: 295.9649 - val_loss: 295.9782 - val_MinusLogProbMetric: 295.9782 - lr: 1.2346e-05 - 109s/epoch - 557ms/step
Epoch 94/1000
2023-09-29 21:39:36.119 
Epoch 94/1000 
	 loss: 294.5588, MinusLogProbMetric: 294.5588, val_loss: 294.5084, val_MinusLogProbMetric: 294.5084

Epoch 94: val_loss improved from 295.97824 to 294.50842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 106s - loss: 294.5588 - MinusLogProbMetric: 294.5588 - val_loss: 294.5084 - val_MinusLogProbMetric: 294.5084 - lr: 1.2346e-05 - 106s/epoch - 541ms/step
Epoch 95/1000
2023-09-29 21:41:27.789 
Epoch 95/1000 
	 loss: 293.2168, MinusLogProbMetric: 293.2168, val_loss: 293.1496, val_MinusLogProbMetric: 293.1496

Epoch 95: val_loss improved from 294.50842 to 293.14957, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 293.2168 - MinusLogProbMetric: 293.2168 - val_loss: 293.1496 - val_MinusLogProbMetric: 293.1496 - lr: 1.2346e-05 - 111s/epoch - 568ms/step
Epoch 96/1000
2023-09-29 21:43:16.835 
Epoch 96/1000 
	 loss: 292.0354, MinusLogProbMetric: 292.0354, val_loss: 293.4741, val_MinusLogProbMetric: 293.4741

Epoch 96: val_loss did not improve from 293.14957
196/196 - 108s - loss: 292.0354 - MinusLogProbMetric: 292.0354 - val_loss: 293.4741 - val_MinusLogProbMetric: 293.4741 - lr: 1.2346e-05 - 108s/epoch - 550ms/step
Epoch 97/1000
2023-09-29 21:45:04.603 
Epoch 97/1000 
	 loss: 290.7523, MinusLogProbMetric: 290.7523, val_loss: 291.0658, val_MinusLogProbMetric: 291.0658

Epoch 97: val_loss improved from 293.14957 to 291.06577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 290.7523 - MinusLogProbMetric: 290.7523 - val_loss: 291.0658 - val_MinusLogProbMetric: 291.0658 - lr: 1.2346e-05 - 109s/epoch - 557ms/step
Epoch 98/1000
2023-09-29 21:46:55.646 
Epoch 98/1000 
	 loss: 289.5047, MinusLogProbMetric: 289.5047, val_loss: 290.0270, val_MinusLogProbMetric: 290.0270

Epoch 98: val_loss improved from 291.06577 to 290.02698, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 289.5047 - MinusLogProbMetric: 289.5047 - val_loss: 290.0270 - val_MinusLogProbMetric: 290.0270 - lr: 1.2346e-05 - 111s/epoch - 568ms/step
Epoch 99/1000
2023-09-29 21:48:44.368 
Epoch 99/1000 
	 loss: 288.6488, MinusLogProbMetric: 288.6488, val_loss: 288.7659, val_MinusLogProbMetric: 288.7659

Epoch 99: val_loss improved from 290.02698 to 288.76590, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 108s - loss: 288.6488 - MinusLogProbMetric: 288.6488 - val_loss: 288.7659 - val_MinusLogProbMetric: 288.7659 - lr: 1.2346e-05 - 108s/epoch - 553ms/step
Epoch 100/1000
2023-09-29 21:50:34.587 
Epoch 100/1000 
	 loss: 287.1835, MinusLogProbMetric: 287.1835, val_loss: 287.2622, val_MinusLogProbMetric: 287.2622

Epoch 100: val_loss improved from 288.76590 to 287.26218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 287.1835 - MinusLogProbMetric: 287.1835 - val_loss: 287.2622 - val_MinusLogProbMetric: 287.2622 - lr: 1.2346e-05 - 110s/epoch - 563ms/step
Epoch 101/1000
2023-09-29 21:52:19.891 
Epoch 101/1000 
	 loss: 286.0905, MinusLogProbMetric: 286.0905, val_loss: 285.9527, val_MinusLogProbMetric: 285.9527

Epoch 101: val_loss improved from 287.26218 to 285.95273, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 105s - loss: 286.0905 - MinusLogProbMetric: 286.0905 - val_loss: 285.9527 - val_MinusLogProbMetric: 285.9527 - lr: 1.2346e-05 - 105s/epoch - 538ms/step
Epoch 102/1000
2023-09-29 21:54:10.938 
Epoch 102/1000 
	 loss: 285.4536, MinusLogProbMetric: 285.4536, val_loss: 290.9926, val_MinusLogProbMetric: 290.9926

Epoch 102: val_loss did not improve from 285.95273
196/196 - 109s - loss: 285.4536 - MinusLogProbMetric: 285.4536 - val_loss: 290.9926 - val_MinusLogProbMetric: 290.9926 - lr: 1.2346e-05 - 109s/epoch - 559ms/step
Epoch 103/1000
2023-09-29 21:56:01.199 
Epoch 103/1000 
	 loss: 284.6263, MinusLogProbMetric: 284.6263, val_loss: 284.0026, val_MinusLogProbMetric: 284.0026

Epoch 103: val_loss improved from 285.95273 to 284.00259, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 284.6263 - MinusLogProbMetric: 284.6263 - val_loss: 284.0026 - val_MinusLogProbMetric: 284.0026 - lr: 1.2346e-05 - 111s/epoch - 569ms/step
Epoch 104/1000
2023-09-29 21:57:50.988 
Epoch 104/1000 
	 loss: 282.7913, MinusLogProbMetric: 282.7913, val_loss: 282.8484, val_MinusLogProbMetric: 282.8484

Epoch 104: val_loss improved from 284.00259 to 282.84842, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 282.7913 - MinusLogProbMetric: 282.7913 - val_loss: 282.8484 - val_MinusLogProbMetric: 282.8484 - lr: 1.2346e-05 - 110s/epoch - 560ms/step
Epoch 105/1000
2023-09-29 21:59:41.810 
Epoch 105/1000 
	 loss: 281.6184, MinusLogProbMetric: 281.6184, val_loss: 281.7583, val_MinusLogProbMetric: 281.7583

Epoch 105: val_loss improved from 282.84842 to 281.75827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 281.6184 - MinusLogProbMetric: 281.6184 - val_loss: 281.7583 - val_MinusLogProbMetric: 281.7583 - lr: 1.2346e-05 - 111s/epoch - 565ms/step
Epoch 106/1000
2023-09-29 22:01:33.642 
Epoch 106/1000 
	 loss: 280.3889, MinusLogProbMetric: 280.3889, val_loss: 280.4385, val_MinusLogProbMetric: 280.4385

Epoch 106: val_loss improved from 281.75827 to 280.43848, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 112s - loss: 280.3889 - MinusLogProbMetric: 280.3889 - val_loss: 280.4385 - val_MinusLogProbMetric: 280.4385 - lr: 1.2346e-05 - 112s/epoch - 571ms/step
Epoch 107/1000
2023-09-29 22:03:20.866 
Epoch 107/1000 
	 loss: 279.2786, MinusLogProbMetric: 279.2786, val_loss: 279.2430, val_MinusLogProbMetric: 279.2430

Epoch 107: val_loss improved from 280.43848 to 279.24301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 279.2786 - MinusLogProbMetric: 279.2786 - val_loss: 279.2430 - val_MinusLogProbMetric: 279.2430 - lr: 1.2346e-05 - 107s/epoch - 548ms/step
Epoch 108/1000
2023-09-29 22:05:11.408 
Epoch 108/1000 
	 loss: 278.1744, MinusLogProbMetric: 278.1744, val_loss: 278.2504, val_MinusLogProbMetric: 278.2504

Epoch 108: val_loss improved from 279.24301 to 278.25037, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 278.1744 - MinusLogProbMetric: 278.1744 - val_loss: 278.2504 - val_MinusLogProbMetric: 278.2504 - lr: 1.2346e-05 - 111s/epoch - 564ms/step
Epoch 109/1000
2023-09-29 22:07:04.539 
Epoch 109/1000 
	 loss: 277.1314, MinusLogProbMetric: 277.1314, val_loss: 277.3313, val_MinusLogProbMetric: 277.3313

Epoch 109: val_loss improved from 278.25037 to 277.33130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 113s - loss: 277.1314 - MinusLogProbMetric: 277.1314 - val_loss: 277.3313 - val_MinusLogProbMetric: 277.3313 - lr: 1.2346e-05 - 113s/epoch - 575ms/step
Epoch 110/1000
2023-09-29 22:08:53.637 
Epoch 110/1000 
	 loss: 276.2379, MinusLogProbMetric: 276.2379, val_loss: 276.9450, val_MinusLogProbMetric: 276.9450

Epoch 110: val_loss improved from 277.33130 to 276.94504, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 276.2379 - MinusLogProbMetric: 276.2379 - val_loss: 276.9450 - val_MinusLogProbMetric: 276.9450 - lr: 1.2346e-05 - 109s/epoch - 557ms/step
Epoch 111/1000
2023-09-29 22:10:42.416 
Epoch 111/1000 
	 loss: 275.8936, MinusLogProbMetric: 275.8936, val_loss: 275.6277, val_MinusLogProbMetric: 275.6277

Epoch 111: val_loss improved from 276.94504 to 275.62769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 275.8936 - MinusLogProbMetric: 275.8936 - val_loss: 275.6277 - val_MinusLogProbMetric: 275.6277 - lr: 1.2346e-05 - 109s/epoch - 556ms/step
Epoch 112/1000
2023-09-29 22:12:33.224 
Epoch 112/1000 
	 loss: 274.1265, MinusLogProbMetric: 274.1265, val_loss: 274.4462, val_MinusLogProbMetric: 274.4462

Epoch 112: val_loss improved from 275.62769 to 274.44620, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 274.1265 - MinusLogProbMetric: 274.1265 - val_loss: 274.4462 - val_MinusLogProbMetric: 274.4462 - lr: 1.2346e-05 - 111s/epoch - 565ms/step
Epoch 113/1000
2023-09-29 22:14:25.343 
Epoch 113/1000 
	 loss: 273.1351, MinusLogProbMetric: 273.1351, val_loss: 273.5249, val_MinusLogProbMetric: 273.5249

Epoch 113: val_loss improved from 274.44620 to 273.52490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 112s - loss: 273.1351 - MinusLogProbMetric: 273.1351 - val_loss: 273.5249 - val_MinusLogProbMetric: 273.5249 - lr: 1.2346e-05 - 112s/epoch - 574ms/step
Epoch 114/1000
2023-09-29 22:16:18.833 
Epoch 114/1000 
	 loss: 273.0063, MinusLogProbMetric: 273.0063, val_loss: 272.7910, val_MinusLogProbMetric: 272.7910

Epoch 114: val_loss improved from 273.52490 to 272.79095, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 113s - loss: 273.0063 - MinusLogProbMetric: 273.0063 - val_loss: 272.7910 - val_MinusLogProbMetric: 272.7910 - lr: 1.2346e-05 - 113s/epoch - 577ms/step
Epoch 115/1000
2023-09-29 22:18:08.896 
Epoch 115/1000 
	 loss: 271.3851, MinusLogProbMetric: 271.3851, val_loss: 271.2716, val_MinusLogProbMetric: 271.2716

Epoch 115: val_loss improved from 272.79095 to 271.27161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 271.3851 - MinusLogProbMetric: 271.3851 - val_loss: 271.2716 - val_MinusLogProbMetric: 271.2716 - lr: 1.2346e-05 - 110s/epoch - 561ms/step
Epoch 116/1000
2023-09-29 22:19:58.653 
Epoch 116/1000 
	 loss: 270.2225, MinusLogProbMetric: 270.2225, val_loss: 270.7014, val_MinusLogProbMetric: 270.7014

Epoch 116: val_loss improved from 271.27161 to 270.70145, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 270.2225 - MinusLogProbMetric: 270.2225 - val_loss: 270.7014 - val_MinusLogProbMetric: 270.7014 - lr: 1.2346e-05 - 110s/epoch - 561ms/step
Epoch 117/1000
2023-09-29 22:21:47.060 
Epoch 117/1000 
	 loss: 269.2199, MinusLogProbMetric: 269.2199, val_loss: 269.2383, val_MinusLogProbMetric: 269.2383

Epoch 117: val_loss improved from 270.70145 to 269.23825, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 108s - loss: 269.2199 - MinusLogProbMetric: 269.2199 - val_loss: 269.2383 - val_MinusLogProbMetric: 269.2383 - lr: 1.2346e-05 - 108s/epoch - 552ms/step
Epoch 118/1000
2023-09-29 22:23:39.980 
Epoch 118/1000 
	 loss: 268.2227, MinusLogProbMetric: 268.2227, val_loss: 268.4501, val_MinusLogProbMetric: 268.4501

Epoch 118: val_loss improved from 269.23825 to 268.45010, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 113s - loss: 268.2227 - MinusLogProbMetric: 268.2227 - val_loss: 268.4501 - val_MinusLogProbMetric: 268.4501 - lr: 1.2346e-05 - 113s/epoch - 577ms/step
Epoch 119/1000
2023-09-29 22:25:35.870 
Epoch 119/1000 
	 loss: 267.4085, MinusLogProbMetric: 267.4085, val_loss: 267.7875, val_MinusLogProbMetric: 267.7875

Epoch 119: val_loss improved from 268.45010 to 267.78751, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 116s - loss: 267.4085 - MinusLogProbMetric: 267.4085 - val_loss: 267.7875 - val_MinusLogProbMetric: 267.7875 - lr: 1.2346e-05 - 116s/epoch - 590ms/step
Epoch 120/1000
2023-09-29 22:27:25.639 
Epoch 120/1000 
	 loss: 266.4864, MinusLogProbMetric: 266.4864, val_loss: 266.5965, val_MinusLogProbMetric: 266.5965

Epoch 120: val_loss improved from 267.78751 to 266.59650, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 266.4864 - MinusLogProbMetric: 266.4864 - val_loss: 266.5965 - val_MinusLogProbMetric: 266.5965 - lr: 1.2346e-05 - 110s/epoch - 562ms/step
Epoch 121/1000
2023-09-29 22:29:13.705 
Epoch 121/1000 
	 loss: 265.9920, MinusLogProbMetric: 265.9920, val_loss: 266.2300, val_MinusLogProbMetric: 266.2300

Epoch 121: val_loss improved from 266.59650 to 266.23004, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 108s - loss: 265.9920 - MinusLogProbMetric: 265.9920 - val_loss: 266.2300 - val_MinusLogProbMetric: 266.2300 - lr: 1.2346e-05 - 108s/epoch - 549ms/step
Epoch 122/1000
2023-09-29 22:31:07.233 
Epoch 122/1000 
	 loss: 266.6745, MinusLogProbMetric: 266.6745, val_loss: 266.9718, val_MinusLogProbMetric: 266.9718

Epoch 122: val_loss did not improve from 266.23004
196/196 - 112s - loss: 266.6745 - MinusLogProbMetric: 266.6745 - val_loss: 266.9718 - val_MinusLogProbMetric: 266.9718 - lr: 1.2346e-05 - 112s/epoch - 574ms/step
Epoch 123/1000
2023-09-29 22:32:54.692 
Epoch 123/1000 
	 loss: 265.3298, MinusLogProbMetric: 265.3298, val_loss: 265.1863, val_MinusLogProbMetric: 265.1863

Epoch 123: val_loss improved from 266.23004 to 265.18631, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 265.3298 - MinusLogProbMetric: 265.3298 - val_loss: 265.1863 - val_MinusLogProbMetric: 265.1863 - lr: 1.2346e-05 - 109s/epoch - 554ms/step
Epoch 124/1000
2023-09-29 22:34:41.574 
Epoch 124/1000 
	 loss: 263.7237, MinusLogProbMetric: 263.7237, val_loss: 264.0760, val_MinusLogProbMetric: 264.0760

Epoch 124: val_loss improved from 265.18631 to 264.07605, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 263.7237 - MinusLogProbMetric: 263.7237 - val_loss: 264.0760 - val_MinusLogProbMetric: 264.0760 - lr: 1.2346e-05 - 107s/epoch - 547ms/step
Epoch 125/1000
2023-09-29 22:36:32.143 
Epoch 125/1000 
	 loss: 262.5465, MinusLogProbMetric: 262.5465, val_loss: 262.7112, val_MinusLogProbMetric: 262.7112

Epoch 125: val_loss improved from 264.07605 to 262.71118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 262.5465 - MinusLogProbMetric: 262.5465 - val_loss: 262.7112 - val_MinusLogProbMetric: 262.7112 - lr: 1.2346e-05 - 110s/epoch - 563ms/step
Epoch 126/1000
2023-09-29 22:38:22.690 
Epoch 126/1000 
	 loss: 284.5572, MinusLogProbMetric: 284.5572, val_loss: 298.2598, val_MinusLogProbMetric: 298.2598

Epoch 126: val_loss did not improve from 262.71118
196/196 - 109s - loss: 284.5572 - MinusLogProbMetric: 284.5572 - val_loss: 298.2598 - val_MinusLogProbMetric: 298.2598 - lr: 1.2346e-05 - 109s/epoch - 557ms/step
Epoch 127/1000
2023-09-29 22:40:12.157 
Epoch 127/1000 
	 loss: 275.4131, MinusLogProbMetric: 275.4131, val_loss: 269.2002, val_MinusLogProbMetric: 269.2002

Epoch 127: val_loss did not improve from 262.71118
196/196 - 109s - loss: 275.4131 - MinusLogProbMetric: 275.4131 - val_loss: 269.2002 - val_MinusLogProbMetric: 269.2002 - lr: 1.2346e-05 - 109s/epoch - 558ms/step
Epoch 128/1000
2023-09-29 22:41:56.665 
Epoch 128/1000 
	 loss: 266.1499, MinusLogProbMetric: 266.1499, val_loss: 264.8403, val_MinusLogProbMetric: 264.8403

Epoch 128: val_loss did not improve from 262.71118
196/196 - 104s - loss: 266.1499 - MinusLogProbMetric: 266.1499 - val_loss: 264.8403 - val_MinusLogProbMetric: 264.8403 - lr: 1.2346e-05 - 104s/epoch - 533ms/step
Epoch 129/1000
2023-09-29 22:43:43.256 
Epoch 129/1000 
	 loss: 262.9388, MinusLogProbMetric: 262.9388, val_loss: 262.6771, val_MinusLogProbMetric: 262.6771

Epoch 129: val_loss improved from 262.71118 to 262.67709, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 108s - loss: 262.9388 - MinusLogProbMetric: 262.9388 - val_loss: 262.6771 - val_MinusLogProbMetric: 262.6771 - lr: 1.2346e-05 - 108s/epoch - 551ms/step
Epoch 130/1000
2023-09-29 22:45:34.609 
Epoch 130/1000 
	 loss: 261.0691, MinusLogProbMetric: 261.0691, val_loss: 260.8384, val_MinusLogProbMetric: 260.8384

Epoch 130: val_loss improved from 262.67709 to 260.83838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 261.0691 - MinusLogProbMetric: 261.0691 - val_loss: 260.8384 - val_MinusLogProbMetric: 260.8384 - lr: 1.2346e-05 - 111s/epoch - 568ms/step
Epoch 131/1000
2023-09-29 22:47:25.950 
Epoch 131/1000 
	 loss: 259.6528, MinusLogProbMetric: 259.6528, val_loss: 259.7491, val_MinusLogProbMetric: 259.7491

Epoch 131: val_loss improved from 260.83838 to 259.74915, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 259.6528 - MinusLogProbMetric: 259.6528 - val_loss: 259.7491 - val_MinusLogProbMetric: 259.7491 - lr: 1.2346e-05 - 111s/epoch - 567ms/step
Epoch 132/1000
2023-09-29 22:49:14.538 
Epoch 132/1000 
	 loss: 258.4806, MinusLogProbMetric: 258.4806, val_loss: 258.6194, val_MinusLogProbMetric: 258.6194

Epoch 132: val_loss improved from 259.74915 to 258.61935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 258.4806 - MinusLogProbMetric: 258.4806 - val_loss: 258.6194 - val_MinusLogProbMetric: 258.6194 - lr: 1.2346e-05 - 109s/epoch - 555ms/step
Epoch 133/1000
2023-09-29 22:51:05.934 
Epoch 133/1000 
	 loss: 261.8502, MinusLogProbMetric: 261.8502, val_loss: 264.6394, val_MinusLogProbMetric: 264.6394

Epoch 133: val_loss did not improve from 258.61935
196/196 - 110s - loss: 261.8502 - MinusLogProbMetric: 261.8502 - val_loss: 264.6394 - val_MinusLogProbMetric: 264.6394 - lr: 1.2346e-05 - 110s/epoch - 561ms/step
Epoch 134/1000
2023-09-29 22:52:54.483 
Epoch 134/1000 
	 loss: 261.1516, MinusLogProbMetric: 261.1516, val_loss: 257.9576, val_MinusLogProbMetric: 257.9576

Epoch 134: val_loss improved from 258.61935 to 257.95755, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 261.1516 - MinusLogProbMetric: 261.1516 - val_loss: 257.9576 - val_MinusLogProbMetric: 257.9576 - lr: 1.2346e-05 - 110s/epoch - 560ms/step
Epoch 135/1000
2023-09-29 22:54:43.116 
Epoch 135/1000 
	 loss: 256.2359, MinusLogProbMetric: 256.2359, val_loss: 256.5013, val_MinusLogProbMetric: 256.5013

Epoch 135: val_loss improved from 257.95755 to 256.50128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 256.2359 - MinusLogProbMetric: 256.2359 - val_loss: 256.5013 - val_MinusLogProbMetric: 256.5013 - lr: 1.2346e-05 - 109s/epoch - 555ms/step
Epoch 136/1000
2023-09-29 22:56:36.575 
Epoch 136/1000 
	 loss: 255.3080, MinusLogProbMetric: 255.3080, val_loss: 255.5377, val_MinusLogProbMetric: 255.5377

Epoch 136: val_loss improved from 256.50128 to 255.53769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 113s - loss: 255.3080 - MinusLogProbMetric: 255.3080 - val_loss: 255.5377 - val_MinusLogProbMetric: 255.5377 - lr: 1.2346e-05 - 113s/epoch - 578ms/step
Epoch 137/1000
2023-09-29 22:58:25.553 
Epoch 137/1000 
	 loss: 254.2731, MinusLogProbMetric: 254.2731, val_loss: 254.6942, val_MinusLogProbMetric: 254.6942

Epoch 137: val_loss improved from 255.53769 to 254.69423, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 254.2731 - MinusLogProbMetric: 254.2731 - val_loss: 254.6942 - val_MinusLogProbMetric: 254.6942 - lr: 1.2346e-05 - 109s/epoch - 556ms/step
Epoch 138/1000
2023-09-29 23:00:18.496 
Epoch 138/1000 
	 loss: 253.6897, MinusLogProbMetric: 253.6897, val_loss: 254.5349, val_MinusLogProbMetric: 254.5349

Epoch 138: val_loss improved from 254.69423 to 254.53491, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 113s - loss: 253.6897 - MinusLogProbMetric: 253.6897 - val_loss: 254.5349 - val_MinusLogProbMetric: 254.5349 - lr: 1.2346e-05 - 113s/epoch - 577ms/step
Epoch 139/1000
2023-09-29 23:02:09.052 
Epoch 139/1000 
	 loss: 252.9917, MinusLogProbMetric: 252.9917, val_loss: 253.1652, val_MinusLogProbMetric: 253.1652

Epoch 139: val_loss improved from 254.53491 to 253.16516, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 252.9917 - MinusLogProbMetric: 252.9917 - val_loss: 253.1652 - val_MinusLogProbMetric: 253.1652 - lr: 1.2346e-05 - 111s/epoch - 564ms/step
Epoch 140/1000
2023-09-29 23:04:00.394 
Epoch 140/1000 
	 loss: 252.1038, MinusLogProbMetric: 252.1038, val_loss: 252.4978, val_MinusLogProbMetric: 252.4978

Epoch 140: val_loss improved from 253.16516 to 252.49779, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 252.1038 - MinusLogProbMetric: 252.1038 - val_loss: 252.4978 - val_MinusLogProbMetric: 252.4978 - lr: 1.2346e-05 - 111s/epoch - 567ms/step
Epoch 141/1000
2023-09-29 23:05:45.764 
Epoch 141/1000 
	 loss: 251.4073, MinusLogProbMetric: 251.4073, val_loss: 253.3548, val_MinusLogProbMetric: 253.3548

Epoch 141: val_loss did not improve from 252.49779
196/196 - 104s - loss: 251.4073 - MinusLogProbMetric: 251.4073 - val_loss: 253.3548 - val_MinusLogProbMetric: 253.3548 - lr: 1.2346e-05 - 104s/epoch - 532ms/step
Epoch 142/1000
2023-09-29 23:07:36.096 
Epoch 142/1000 
	 loss: 251.4977, MinusLogProbMetric: 251.4977, val_loss: 251.3927, val_MinusLogProbMetric: 251.3927

Epoch 142: val_loss improved from 252.49779 to 251.39275, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 251.4977 - MinusLogProbMetric: 251.4977 - val_loss: 251.3927 - val_MinusLogProbMetric: 251.3927 - lr: 1.2346e-05 - 111s/epoch - 568ms/step
Epoch 143/1000
2023-09-29 23:09:24.227 
Epoch 143/1000 
	 loss: 250.2588, MinusLogProbMetric: 250.2588, val_loss: 250.4046, val_MinusLogProbMetric: 250.4046

Epoch 143: val_loss improved from 251.39275 to 250.40460, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 108s - loss: 250.2588 - MinusLogProbMetric: 250.2588 - val_loss: 250.4046 - val_MinusLogProbMetric: 250.4046 - lr: 1.2346e-05 - 108s/epoch - 553ms/step
Epoch 144/1000
2023-09-29 23:11:15.322 
Epoch 144/1000 
	 loss: 249.3968, MinusLogProbMetric: 249.3968, val_loss: 250.2113, val_MinusLogProbMetric: 250.2113

Epoch 144: val_loss improved from 250.40460 to 250.21135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 249.3968 - MinusLogProbMetric: 249.3968 - val_loss: 250.2113 - val_MinusLogProbMetric: 250.2113 - lr: 1.2346e-05 - 111s/epoch - 565ms/step
Epoch 145/1000
2023-09-29 23:13:07.537 
Epoch 145/1000 
	 loss: 248.7522, MinusLogProbMetric: 248.7522, val_loss: 248.8766, val_MinusLogProbMetric: 248.8766

Epoch 145: val_loss improved from 250.21135 to 248.87663, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 112s - loss: 248.7522 - MinusLogProbMetric: 248.7522 - val_loss: 248.8766 - val_MinusLogProbMetric: 248.8766 - lr: 1.2346e-05 - 112s/epoch - 573ms/step
Epoch 146/1000
2023-09-29 23:14:54.727 
Epoch 146/1000 
	 loss: 248.0084, MinusLogProbMetric: 248.0084, val_loss: 248.3289, val_MinusLogProbMetric: 248.3289

Epoch 146: val_loss improved from 248.87663 to 248.32892, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 248.0084 - MinusLogProbMetric: 248.0084 - val_loss: 248.3289 - val_MinusLogProbMetric: 248.3289 - lr: 1.2346e-05 - 107s/epoch - 548ms/step
Epoch 147/1000
2023-09-29 23:16:43.246 
Epoch 147/1000 
	 loss: 247.3022, MinusLogProbMetric: 247.3022, val_loss: 247.5146, val_MinusLogProbMetric: 247.5146

Epoch 147: val_loss improved from 248.32892 to 247.51460, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 247.3022 - MinusLogProbMetric: 247.3022 - val_loss: 247.5146 - val_MinusLogProbMetric: 247.5146 - lr: 1.2346e-05 - 109s/epoch - 554ms/step
Epoch 148/1000
2023-09-29 23:18:36.907 
Epoch 148/1000 
	 loss: 246.5961, MinusLogProbMetric: 246.5961, val_loss: 247.1528, val_MinusLogProbMetric: 247.1528

Epoch 148: val_loss improved from 247.51460 to 247.15283, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 114s - loss: 246.5961 - MinusLogProbMetric: 246.5961 - val_loss: 247.1528 - val_MinusLogProbMetric: 247.1528 - lr: 1.2346e-05 - 114s/epoch - 579ms/step
Epoch 149/1000
2023-09-29 23:20:27.925 
Epoch 149/1000 
	 loss: 245.9488, MinusLogProbMetric: 245.9488, val_loss: 246.4139, val_MinusLogProbMetric: 246.4139

Epoch 149: val_loss improved from 247.15283 to 246.41394, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 245.9488 - MinusLogProbMetric: 245.9488 - val_loss: 246.4139 - val_MinusLogProbMetric: 246.4139 - lr: 1.2346e-05 - 111s/epoch - 566ms/step
Epoch 150/1000
2023-09-29 23:22:19.351 
Epoch 150/1000 
	 loss: 245.4471, MinusLogProbMetric: 245.4471, val_loss: 246.1503, val_MinusLogProbMetric: 246.1503

Epoch 150: val_loss improved from 246.41394 to 246.15028, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 245.4471 - MinusLogProbMetric: 245.4471 - val_loss: 246.1503 - val_MinusLogProbMetric: 246.1503 - lr: 1.2346e-05 - 111s/epoch - 569ms/step
Epoch 151/1000
2023-09-29 23:24:05.819 
Epoch 151/1000 
	 loss: 244.7535, MinusLogProbMetric: 244.7535, val_loss: 244.8730, val_MinusLogProbMetric: 244.8730

Epoch 151: val_loss improved from 246.15028 to 244.87305, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 106s - loss: 244.7535 - MinusLogProbMetric: 244.7535 - val_loss: 244.8730 - val_MinusLogProbMetric: 244.8730 - lr: 1.2346e-05 - 106s/epoch - 543ms/step
Epoch 152/1000
2023-09-29 23:26:00.246 
Epoch 152/1000 
	 loss: 244.1167, MinusLogProbMetric: 244.1167, val_loss: 244.2290, val_MinusLogProbMetric: 244.2290

Epoch 152: val_loss improved from 244.87305 to 244.22897, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 115s - loss: 244.1167 - MinusLogProbMetric: 244.1167 - val_loss: 244.2290 - val_MinusLogProbMetric: 244.2290 - lr: 1.2346e-05 - 115s/epoch - 585ms/step
Epoch 153/1000
2023-09-29 23:27:50.222 
Epoch 153/1000 
	 loss: 243.4841, MinusLogProbMetric: 243.4841, val_loss: 243.7734, val_MinusLogProbMetric: 243.7734

Epoch 153: val_loss improved from 244.22897 to 243.77344, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 243.4841 - MinusLogProbMetric: 243.4841 - val_loss: 243.7734 - val_MinusLogProbMetric: 243.7734 - lr: 1.2346e-05 - 110s/epoch - 562ms/step
Epoch 154/1000
2023-09-29 23:29:41.260 
Epoch 154/1000 
	 loss: 242.8835, MinusLogProbMetric: 242.8835, val_loss: 243.3483, val_MinusLogProbMetric: 243.3483

Epoch 154: val_loss improved from 243.77344 to 243.34827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 242.8835 - MinusLogProbMetric: 242.8835 - val_loss: 243.3483 - val_MinusLogProbMetric: 243.3483 - lr: 1.2346e-05 - 111s/epoch - 567ms/step
Epoch 155/1000
2023-09-29 23:31:36.940 
Epoch 155/1000 
	 loss: 242.2042, MinusLogProbMetric: 242.2042, val_loss: 242.5988, val_MinusLogProbMetric: 242.5988

Epoch 155: val_loss improved from 243.34827 to 242.59885, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 115s - loss: 242.2042 - MinusLogProbMetric: 242.2042 - val_loss: 242.5988 - val_MinusLogProbMetric: 242.5988 - lr: 1.2346e-05 - 115s/epoch - 588ms/step
Epoch 156/1000
2023-09-29 23:33:27.923 
Epoch 156/1000 
	 loss: 241.6743, MinusLogProbMetric: 241.6743, val_loss: 242.6131, val_MinusLogProbMetric: 242.6131

Epoch 156: val_loss did not improve from 242.59885
196/196 - 110s - loss: 241.6743 - MinusLogProbMetric: 241.6743 - val_loss: 242.6131 - val_MinusLogProbMetric: 242.6131 - lr: 1.2346e-05 - 110s/epoch - 560ms/step
Epoch 157/1000
2023-09-29 23:35:16.881 
Epoch 157/1000 
	 loss: 241.1529, MinusLogProbMetric: 241.1529, val_loss: 241.5750, val_MinusLogProbMetric: 241.5750

Epoch 157: val_loss improved from 242.59885 to 241.57495, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 241.1529 - MinusLogProbMetric: 241.1529 - val_loss: 241.5750 - val_MinusLogProbMetric: 241.5750 - lr: 1.2346e-05 - 110s/epoch - 563ms/step
Epoch 158/1000
2023-09-29 23:37:03.649 
Epoch 158/1000 
	 loss: 240.5417, MinusLogProbMetric: 240.5417, val_loss: 240.8820, val_MinusLogProbMetric: 240.8820

Epoch 158: val_loss improved from 241.57495 to 240.88203, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 240.5417 - MinusLogProbMetric: 240.5417 - val_loss: 240.8820 - val_MinusLogProbMetric: 240.8820 - lr: 1.2346e-05 - 107s/epoch - 545ms/step
Epoch 159/1000
2023-09-29 23:38:56.948 
Epoch 159/1000 
	 loss: 240.0124, MinusLogProbMetric: 240.0124, val_loss: 240.6225, val_MinusLogProbMetric: 240.6225

Epoch 159: val_loss improved from 240.88203 to 240.62251, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 113s - loss: 240.0124 - MinusLogProbMetric: 240.0124 - val_loss: 240.6225 - val_MinusLogProbMetric: 240.6225 - lr: 1.2346e-05 - 113s/epoch - 579ms/step
Epoch 160/1000
2023-09-29 23:40:49.515 
Epoch 160/1000 
	 loss: 239.4366, MinusLogProbMetric: 239.4366, val_loss: 240.1211, val_MinusLogProbMetric: 240.1211

Epoch 160: val_loss improved from 240.62251 to 240.12114, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 112s - loss: 239.4366 - MinusLogProbMetric: 239.4366 - val_loss: 240.1211 - val_MinusLogProbMetric: 240.1211 - lr: 1.2346e-05 - 112s/epoch - 574ms/step
Epoch 161/1000
2023-09-29 23:42:43.330 
Epoch 161/1000 
	 loss: 238.9373, MinusLogProbMetric: 238.9373, val_loss: 239.4491, val_MinusLogProbMetric: 239.4491

Epoch 161: val_loss improved from 240.12114 to 239.44907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 114s - loss: 238.9373 - MinusLogProbMetric: 238.9373 - val_loss: 239.4491 - val_MinusLogProbMetric: 239.4491 - lr: 1.2346e-05 - 114s/epoch - 581ms/step
Epoch 162/1000
2023-09-29 23:44:34.575 
Epoch 162/1000 
	 loss: 238.4553, MinusLogProbMetric: 238.4553, val_loss: 239.1955, val_MinusLogProbMetric: 239.1955

Epoch 162: val_loss improved from 239.44907 to 239.19547, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 238.4553 - MinusLogProbMetric: 238.4553 - val_loss: 239.1955 - val_MinusLogProbMetric: 239.1955 - lr: 1.2346e-05 - 111s/epoch - 567ms/step
Epoch 163/1000
2023-09-29 23:46:20.540 
Epoch 163/1000 
	 loss: 238.0357, MinusLogProbMetric: 238.0357, val_loss: 238.0674, val_MinusLogProbMetric: 238.0674

Epoch 163: val_loss improved from 239.19547 to 238.06743, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 106s - loss: 238.0357 - MinusLogProbMetric: 238.0357 - val_loss: 238.0674 - val_MinusLogProbMetric: 238.0674 - lr: 1.2346e-05 - 106s/epoch - 540ms/step
Epoch 164/1000
2023-09-29 23:48:12.449 
Epoch 164/1000 
	 loss: 237.3949, MinusLogProbMetric: 237.3949, val_loss: 237.8112, val_MinusLogProbMetric: 237.8112

Epoch 164: val_loss improved from 238.06743 to 237.81120, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 112s - loss: 237.3949 - MinusLogProbMetric: 237.3949 - val_loss: 237.8112 - val_MinusLogProbMetric: 237.8112 - lr: 1.2346e-05 - 112s/epoch - 573ms/step
Epoch 165/1000
2023-09-29 23:50:02.986 
Epoch 165/1000 
	 loss: 337.6944, MinusLogProbMetric: 337.6944, val_loss: 332.4787, val_MinusLogProbMetric: 332.4787

Epoch 165: val_loss did not improve from 237.81120
196/196 - 109s - loss: 337.6944 - MinusLogProbMetric: 337.6944 - val_loss: 332.4787 - val_MinusLogProbMetric: 332.4787 - lr: 1.2346e-05 - 109s/epoch - 555ms/step
Epoch 166/1000
2023-09-29 23:51:52.103 
Epoch 166/1000 
	 loss: 313.9960, MinusLogProbMetric: 313.9960, val_loss: 302.4491, val_MinusLogProbMetric: 302.4491

Epoch 166: val_loss did not improve from 237.81120
196/196 - 109s - loss: 313.9960 - MinusLogProbMetric: 313.9960 - val_loss: 302.4491 - val_MinusLogProbMetric: 302.4491 - lr: 1.2346e-05 - 109s/epoch - 557ms/step
Epoch 167/1000
2023-09-29 23:53:40.706 
Epoch 167/1000 
	 loss: 295.3853, MinusLogProbMetric: 295.3853, val_loss: 290.7716, val_MinusLogProbMetric: 290.7716

Epoch 167: val_loss did not improve from 237.81120
196/196 - 109s - loss: 295.3853 - MinusLogProbMetric: 295.3853 - val_loss: 290.7716 - val_MinusLogProbMetric: 290.7716 - lr: 1.2346e-05 - 109s/epoch - 554ms/step
Epoch 168/1000
2023-09-29 23:55:30.132 
Epoch 168/1000 
	 loss: 286.3835, MinusLogProbMetric: 286.3835, val_loss: 283.5555, val_MinusLogProbMetric: 283.5555

Epoch 168: val_loss did not improve from 237.81120
196/196 - 109s - loss: 286.3835 - MinusLogProbMetric: 286.3835 - val_loss: 283.5555 - val_MinusLogProbMetric: 283.5555 - lr: 1.2346e-05 - 109s/epoch - 558ms/step
Epoch 169/1000
2023-09-29 23:57:14.742 
Epoch 169/1000 
	 loss: 279.3314, MinusLogProbMetric: 279.3314, val_loss: 276.8671, val_MinusLogProbMetric: 276.8671

Epoch 169: val_loss did not improve from 237.81120
196/196 - 105s - loss: 279.3314 - MinusLogProbMetric: 279.3314 - val_loss: 276.8671 - val_MinusLogProbMetric: 276.8671 - lr: 1.2346e-05 - 105s/epoch - 534ms/step
Epoch 170/1000
2023-09-29 23:59:04.018 
Epoch 170/1000 
	 loss: 273.2021, MinusLogProbMetric: 273.2021, val_loss: 271.7145, val_MinusLogProbMetric: 271.7145

Epoch 170: val_loss did not improve from 237.81120
196/196 - 109s - loss: 273.2021 - MinusLogProbMetric: 273.2021 - val_loss: 271.7145 - val_MinusLogProbMetric: 271.7145 - lr: 1.2346e-05 - 109s/epoch - 557ms/step
Epoch 171/1000
2023-09-30 00:00:50.849 
Epoch 171/1000 
	 loss: 268.6889, MinusLogProbMetric: 268.6889, val_loss: 267.6885, val_MinusLogProbMetric: 267.6885

Epoch 171: val_loss did not improve from 237.81120
196/196 - 107s - loss: 268.6889 - MinusLogProbMetric: 268.6889 - val_loss: 267.6885 - val_MinusLogProbMetric: 267.6885 - lr: 1.2346e-05 - 107s/epoch - 545ms/step
Epoch 172/1000
2023-09-30 00:02:31.576 
Epoch 172/1000 
	 loss: 265.0772, MinusLogProbMetric: 265.0772, val_loss: 264.1484, val_MinusLogProbMetric: 264.1484

Epoch 172: val_loss did not improve from 237.81120
196/196 - 101s - loss: 265.0772 - MinusLogProbMetric: 265.0772 - val_loss: 264.1484 - val_MinusLogProbMetric: 264.1484 - lr: 1.2346e-05 - 101s/epoch - 514ms/step
Epoch 173/1000
2023-09-30 00:04:13.321 
Epoch 173/1000 
	 loss: 261.7709, MinusLogProbMetric: 261.7709, val_loss: 260.9447, val_MinusLogProbMetric: 260.9447

Epoch 173: val_loss did not improve from 237.81120
196/196 - 102s - loss: 261.7709 - MinusLogProbMetric: 261.7709 - val_loss: 260.9447 - val_MinusLogProbMetric: 260.9447 - lr: 1.2346e-05 - 102s/epoch - 519ms/step
Epoch 174/1000
2023-09-30 00:05:51.094 
Epoch 174/1000 
	 loss: 258.9829, MinusLogProbMetric: 258.9829, val_loss: 258.4348, val_MinusLogProbMetric: 258.4348

Epoch 174: val_loss did not improve from 237.81120
196/196 - 98s - loss: 258.9829 - MinusLogProbMetric: 258.9829 - val_loss: 258.4348 - val_MinusLogProbMetric: 258.4348 - lr: 1.2346e-05 - 98s/epoch - 499ms/step
Epoch 175/1000
2023-09-30 00:07:34.064 
Epoch 175/1000 
	 loss: 256.3240, MinusLogProbMetric: 256.3240, val_loss: 255.5916, val_MinusLogProbMetric: 255.5916

Epoch 175: val_loss did not improve from 237.81120
196/196 - 103s - loss: 256.3240 - MinusLogProbMetric: 256.3240 - val_loss: 255.5916 - val_MinusLogProbMetric: 255.5916 - lr: 1.2346e-05 - 103s/epoch - 525ms/step
Epoch 176/1000
2023-09-30 00:09:22.062 
Epoch 176/1000 
	 loss: 254.0861, MinusLogProbMetric: 254.0861, val_loss: 254.1011, val_MinusLogProbMetric: 254.1011

Epoch 176: val_loss did not improve from 237.81120
196/196 - 108s - loss: 254.0861 - MinusLogProbMetric: 254.0861 - val_loss: 254.1011 - val_MinusLogProbMetric: 254.1011 - lr: 1.2346e-05 - 108s/epoch - 551ms/step
Epoch 177/1000
2023-09-30 00:11:08.694 
Epoch 177/1000 
	 loss: 252.8111, MinusLogProbMetric: 252.8111, val_loss: 252.4402, val_MinusLogProbMetric: 252.4402

Epoch 177: val_loss did not improve from 237.81120
196/196 - 107s - loss: 252.8111 - MinusLogProbMetric: 252.8111 - val_loss: 252.4402 - val_MinusLogProbMetric: 252.4402 - lr: 1.2346e-05 - 107s/epoch - 544ms/step
Epoch 178/1000
2023-09-30 00:12:53.600 
Epoch 178/1000 
	 loss: 250.9535, MinusLogProbMetric: 250.9535, val_loss: 250.5376, val_MinusLogProbMetric: 250.5376

Epoch 178: val_loss did not improve from 237.81120
196/196 - 105s - loss: 250.9535 - MinusLogProbMetric: 250.9535 - val_loss: 250.5376 - val_MinusLogProbMetric: 250.5376 - lr: 1.2346e-05 - 105s/epoch - 535ms/step
Epoch 179/1000
2023-09-30 00:14:39.673 
Epoch 179/1000 
	 loss: 248.8738, MinusLogProbMetric: 248.8738, val_loss: 248.9059, val_MinusLogProbMetric: 248.9059

Epoch 179: val_loss did not improve from 237.81120
196/196 - 106s - loss: 248.8738 - MinusLogProbMetric: 248.8738 - val_loss: 248.9059 - val_MinusLogProbMetric: 248.9059 - lr: 1.2346e-05 - 106s/epoch - 541ms/step
Epoch 180/1000
2023-09-30 00:16:18.936 
Epoch 180/1000 
	 loss: 247.4109, MinusLogProbMetric: 247.4109, val_loss: 247.2032, val_MinusLogProbMetric: 247.2032

Epoch 180: val_loss did not improve from 237.81120
196/196 - 99s - loss: 247.4109 - MinusLogProbMetric: 247.4109 - val_loss: 247.2032 - val_MinusLogProbMetric: 247.2032 - lr: 1.2346e-05 - 99s/epoch - 506ms/step
Epoch 181/1000
2023-09-30 00:18:03.529 
Epoch 181/1000 
	 loss: 245.7193, MinusLogProbMetric: 245.7193, val_loss: 245.3813, val_MinusLogProbMetric: 245.3813

Epoch 181: val_loss did not improve from 237.81120
196/196 - 105s - loss: 245.7193 - MinusLogProbMetric: 245.7193 - val_loss: 245.3813 - val_MinusLogProbMetric: 245.3813 - lr: 1.2346e-05 - 105s/epoch - 534ms/step
Epoch 182/1000
2023-09-30 00:19:48.779 
Epoch 182/1000 
	 loss: 244.2410, MinusLogProbMetric: 244.2410, val_loss: 245.9139, val_MinusLogProbMetric: 245.9139

Epoch 182: val_loss did not improve from 237.81120
196/196 - 105s - loss: 244.2410 - MinusLogProbMetric: 244.2410 - val_loss: 245.9139 - val_MinusLogProbMetric: 245.9139 - lr: 1.2346e-05 - 105s/epoch - 537ms/step
Epoch 183/1000
2023-09-30 00:21:37.090 
Epoch 183/1000 
	 loss: 244.5282, MinusLogProbMetric: 244.5282, val_loss: 245.4967, val_MinusLogProbMetric: 245.4967

Epoch 183: val_loss did not improve from 237.81120
196/196 - 108s - loss: 244.5282 - MinusLogProbMetric: 244.5282 - val_loss: 245.4967 - val_MinusLogProbMetric: 245.4967 - lr: 1.2346e-05 - 108s/epoch - 553ms/step
Epoch 184/1000
2023-09-30 00:23:17.583 
Epoch 184/1000 
	 loss: 243.5371, MinusLogProbMetric: 243.5371, val_loss: 243.3667, val_MinusLogProbMetric: 243.3667

Epoch 184: val_loss did not improve from 237.81120
196/196 - 100s - loss: 243.5371 - MinusLogProbMetric: 243.5371 - val_loss: 243.3667 - val_MinusLogProbMetric: 243.3667 - lr: 1.2346e-05 - 100s/epoch - 513ms/step
Epoch 185/1000
2023-09-30 00:25:00.278 
Epoch 185/1000 
	 loss: 241.8400, MinusLogProbMetric: 241.8400, val_loss: 241.6993, val_MinusLogProbMetric: 241.6993

Epoch 185: val_loss did not improve from 237.81120
196/196 - 103s - loss: 241.8400 - MinusLogProbMetric: 241.8400 - val_loss: 241.6993 - val_MinusLogProbMetric: 241.6993 - lr: 1.2346e-05 - 103s/epoch - 524ms/step
Epoch 186/1000
2023-09-30 00:26:40.905 
Epoch 186/1000 
	 loss: 240.9991, MinusLogProbMetric: 240.9991, val_loss: 241.0232, val_MinusLogProbMetric: 241.0232

Epoch 186: val_loss did not improve from 237.81120
196/196 - 101s - loss: 240.9991 - MinusLogProbMetric: 240.9991 - val_loss: 241.0232 - val_MinusLogProbMetric: 241.0232 - lr: 1.2346e-05 - 101s/epoch - 513ms/step
Epoch 187/1000
2023-09-30 00:28:28.133 
Epoch 187/1000 
	 loss: 239.8232, MinusLogProbMetric: 239.8232, val_loss: 240.0058, val_MinusLogProbMetric: 240.0058

Epoch 187: val_loss did not improve from 237.81120
196/196 - 107s - loss: 239.8232 - MinusLogProbMetric: 239.8232 - val_loss: 240.0058 - val_MinusLogProbMetric: 240.0058 - lr: 1.2346e-05 - 107s/epoch - 547ms/step
Epoch 188/1000
2023-09-30 00:30:10.996 
Epoch 188/1000 
	 loss: 238.9976, MinusLogProbMetric: 238.9976, val_loss: 239.4505, val_MinusLogProbMetric: 239.4505

Epoch 188: val_loss did not improve from 237.81120
196/196 - 103s - loss: 238.9976 - MinusLogProbMetric: 238.9976 - val_loss: 239.4505 - val_MinusLogProbMetric: 239.4505 - lr: 1.2346e-05 - 103s/epoch - 525ms/step
Epoch 189/1000
2023-09-30 00:32:03.153 
Epoch 189/1000 
	 loss: 237.9483, MinusLogProbMetric: 237.9483, val_loss: 238.6476, val_MinusLogProbMetric: 238.6476

Epoch 189: val_loss did not improve from 237.81120
196/196 - 112s - loss: 237.9483 - MinusLogProbMetric: 237.9483 - val_loss: 238.6476 - val_MinusLogProbMetric: 238.6476 - lr: 1.2346e-05 - 112s/epoch - 572ms/step
Epoch 190/1000
2023-09-30 00:33:43.953 
Epoch 190/1000 
	 loss: 237.0368, MinusLogProbMetric: 237.0368, val_loss: 236.9399, val_MinusLogProbMetric: 236.9399

Epoch 190: val_loss improved from 237.81120 to 236.93991, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 102s - loss: 237.0368 - MinusLogProbMetric: 237.0368 - val_loss: 236.9399 - val_MinusLogProbMetric: 236.9399 - lr: 1.2346e-05 - 102s/epoch - 523ms/step
Epoch 191/1000
2023-09-30 00:35:31.466 
Epoch 191/1000 
	 loss: 243.3475, MinusLogProbMetric: 243.3475, val_loss: 238.7742, val_MinusLogProbMetric: 238.7742

Epoch 191: val_loss did not improve from 236.93991
196/196 - 106s - loss: 243.3475 - MinusLogProbMetric: 243.3475 - val_loss: 238.7742 - val_MinusLogProbMetric: 238.7742 - lr: 1.2346e-05 - 106s/epoch - 540ms/step
Epoch 192/1000
2023-09-30 00:37:14.942 
Epoch 192/1000 
	 loss: 237.2905, MinusLogProbMetric: 237.2905, val_loss: 237.4118, val_MinusLogProbMetric: 237.4118

Epoch 192: val_loss did not improve from 236.93991
196/196 - 103s - loss: 237.2905 - MinusLogProbMetric: 237.2905 - val_loss: 237.4118 - val_MinusLogProbMetric: 237.4118 - lr: 1.2346e-05 - 103s/epoch - 528ms/step
Epoch 193/1000
2023-09-30 00:39:00.326 
Epoch 193/1000 
	 loss: 236.0410, MinusLogProbMetric: 236.0410, val_loss: 236.1564, val_MinusLogProbMetric: 236.1564

Epoch 193: val_loss improved from 236.93991 to 236.15636, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 106s - loss: 236.0410 - MinusLogProbMetric: 236.0410 - val_loss: 236.1564 - val_MinusLogProbMetric: 236.1564 - lr: 1.2346e-05 - 106s/epoch - 543ms/step
Epoch 194/1000
2023-09-30 00:40:45.853 
Epoch 194/1000 
	 loss: 234.9823, MinusLogProbMetric: 234.9823, val_loss: 236.8564, val_MinusLogProbMetric: 236.8564

Epoch 194: val_loss did not improve from 236.15636
196/196 - 104s - loss: 234.9823 - MinusLogProbMetric: 234.9823 - val_loss: 236.8564 - val_MinusLogProbMetric: 236.8564 - lr: 1.2346e-05 - 104s/epoch - 533ms/step
Epoch 195/1000
2023-09-30 00:42:28.608 
Epoch 195/1000 
	 loss: 236.2743, MinusLogProbMetric: 236.2743, val_loss: 235.9834, val_MinusLogProbMetric: 235.9834

Epoch 195: val_loss improved from 236.15636 to 235.98337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 104s - loss: 236.2743 - MinusLogProbMetric: 236.2743 - val_loss: 235.9834 - val_MinusLogProbMetric: 235.9834 - lr: 1.2346e-05 - 104s/epoch - 529ms/step
Epoch 196/1000
2023-09-30 00:44:14.073 
Epoch 196/1000 
	 loss: 234.9492, MinusLogProbMetric: 234.9492, val_loss: 234.5316, val_MinusLogProbMetric: 234.5316

Epoch 196: val_loss improved from 235.98337 to 234.53160, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 106s - loss: 234.9492 - MinusLogProbMetric: 234.9492 - val_loss: 234.5316 - val_MinusLogProbMetric: 234.5316 - lr: 1.2346e-05 - 106s/epoch - 539ms/step
Epoch 197/1000
2023-09-30 00:46:01.011 
Epoch 197/1000 
	 loss: 233.9468, MinusLogProbMetric: 233.9468, val_loss: 233.7575, val_MinusLogProbMetric: 233.7575

Epoch 197: val_loss improved from 234.53160 to 233.75746, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 233.9468 - MinusLogProbMetric: 233.9468 - val_loss: 233.7575 - val_MinusLogProbMetric: 233.7575 - lr: 1.2346e-05 - 107s/epoch - 548ms/step
Epoch 198/1000
2023-09-30 00:47:42.259 
Epoch 198/1000 
	 loss: 233.1939, MinusLogProbMetric: 233.1939, val_loss: 233.0035, val_MinusLogProbMetric: 233.0035

Epoch 198: val_loss improved from 233.75746 to 233.00351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 101s - loss: 233.1939 - MinusLogProbMetric: 233.1939 - val_loss: 233.0035 - val_MinusLogProbMetric: 233.0035 - lr: 1.2346e-05 - 101s/epoch - 517ms/step
Epoch 199/1000
2023-09-30 00:49:29.144 
Epoch 199/1000 
	 loss: 232.3290, MinusLogProbMetric: 232.3290, val_loss: 232.6535, val_MinusLogProbMetric: 232.6535

Epoch 199: val_loss improved from 233.00351 to 232.65353, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 232.3290 - MinusLogProbMetric: 232.3290 - val_loss: 232.6535 - val_MinusLogProbMetric: 232.6535 - lr: 1.2346e-05 - 107s/epoch - 544ms/step
Epoch 200/1000
2023-09-30 00:51:11.233 
Epoch 200/1000 
	 loss: 231.5632, MinusLogProbMetric: 231.5632, val_loss: 231.6013, val_MinusLogProbMetric: 231.6013

Epoch 200: val_loss improved from 232.65353 to 231.60127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 102s - loss: 231.5632 - MinusLogProbMetric: 231.5632 - val_loss: 231.6013 - val_MinusLogProbMetric: 231.6013 - lr: 1.2346e-05 - 102s/epoch - 521ms/step
Epoch 201/1000
2023-09-30 00:52:51.553 
Epoch 201/1000 
	 loss: 231.2150, MinusLogProbMetric: 231.2150, val_loss: 231.7608, val_MinusLogProbMetric: 231.7608

Epoch 201: val_loss did not improve from 231.60127
196/196 - 99s - loss: 231.2150 - MinusLogProbMetric: 231.2150 - val_loss: 231.7608 - val_MinusLogProbMetric: 231.7608 - lr: 1.2346e-05 - 99s/epoch - 504ms/step
Epoch 202/1000
2023-09-30 00:54:34.774 
Epoch 202/1000 
	 loss: 231.0439, MinusLogProbMetric: 231.0439, val_loss: 230.9924, val_MinusLogProbMetric: 230.9924

Epoch 202: val_loss improved from 231.60127 to 230.99243, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 105s - loss: 231.0439 - MinusLogProbMetric: 231.0439 - val_loss: 230.9924 - val_MinusLogProbMetric: 230.9924 - lr: 1.2346e-05 - 105s/epoch - 534ms/step
Epoch 203/1000
2023-09-30 00:56:18.846 
Epoch 203/1000 
	 loss: 230.3123, MinusLogProbMetric: 230.3123, val_loss: 230.2529, val_MinusLogProbMetric: 230.2529

Epoch 203: val_loss improved from 230.99243 to 230.25288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 104s - loss: 230.3123 - MinusLogProbMetric: 230.3123 - val_loss: 230.2529 - val_MinusLogProbMetric: 230.2529 - lr: 1.2346e-05 - 104s/epoch - 530ms/step
Epoch 204/1000
2023-09-30 00:58:02.964 
Epoch 204/1000 
	 loss: 229.8560, MinusLogProbMetric: 229.8560, val_loss: 229.5285, val_MinusLogProbMetric: 229.5285

Epoch 204: val_loss improved from 230.25288 to 229.52855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 104s - loss: 229.8560 - MinusLogProbMetric: 229.8560 - val_loss: 229.5285 - val_MinusLogProbMetric: 229.5285 - lr: 1.2346e-05 - 104s/epoch - 531ms/step
Epoch 205/1000
2023-09-30 00:59:41.592 
Epoch 205/1000 
	 loss: 228.3357, MinusLogProbMetric: 228.3357, val_loss: 228.4014, val_MinusLogProbMetric: 228.4014

Epoch 205: val_loss improved from 229.52855 to 228.40140, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 99s - loss: 228.3357 - MinusLogProbMetric: 228.3357 - val_loss: 228.4014 - val_MinusLogProbMetric: 228.4014 - lr: 1.2346e-05 - 99s/epoch - 503ms/step
Epoch 206/1000
2023-09-30 01:01:22.599 
Epoch 206/1000 
	 loss: 227.4976, MinusLogProbMetric: 227.4976, val_loss: 227.4767, val_MinusLogProbMetric: 227.4767

Epoch 206: val_loss improved from 228.40140 to 227.47672, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 103s - loss: 227.4976 - MinusLogProbMetric: 227.4976 - val_loss: 227.4767 - val_MinusLogProbMetric: 227.4767 - lr: 1.2346e-05 - 103s/epoch - 524ms/step
Epoch 207/1000
2023-09-30 01:03:11.025 
Epoch 207/1000 
	 loss: 226.8661, MinusLogProbMetric: 226.8661, val_loss: 227.1645, val_MinusLogProbMetric: 227.1645

Epoch 207: val_loss improved from 227.47672 to 227.16451, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 226.8661 - MinusLogProbMetric: 226.8661 - val_loss: 227.1645 - val_MinusLogProbMetric: 227.1645 - lr: 1.2346e-05 - 107s/epoch - 545ms/step
Epoch 208/1000
2023-09-30 01:04:56.716 
Epoch 208/1000 
	 loss: 226.0930, MinusLogProbMetric: 226.0930, val_loss: 226.9045, val_MinusLogProbMetric: 226.9045

Epoch 208: val_loss improved from 227.16451 to 226.90448, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 106s - loss: 226.0930 - MinusLogProbMetric: 226.0930 - val_loss: 226.9045 - val_MinusLogProbMetric: 226.9045 - lr: 1.2346e-05 - 106s/epoch - 539ms/step
Epoch 209/1000
2023-09-30 01:06:37.300 
Epoch 209/1000 
	 loss: 225.6721, MinusLogProbMetric: 225.6721, val_loss: 225.9780, val_MinusLogProbMetric: 225.9780

Epoch 209: val_loss improved from 226.90448 to 225.97801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 101s - loss: 225.6721 - MinusLogProbMetric: 225.6721 - val_loss: 225.9780 - val_MinusLogProbMetric: 225.9780 - lr: 1.2346e-05 - 101s/epoch - 514ms/step
Epoch 210/1000
2023-09-30 01:08:25.503 
Epoch 210/1000 
	 loss: 225.0915, MinusLogProbMetric: 225.0915, val_loss: 225.1078, val_MinusLogProbMetric: 225.1078

Epoch 210: val_loss improved from 225.97801 to 225.10777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 225.0915 - MinusLogProbMetric: 225.0915 - val_loss: 225.1078 - val_MinusLogProbMetric: 225.1078 - lr: 1.2346e-05 - 109s/epoch - 554ms/step
Epoch 211/1000
2023-09-30 01:10:17.100 
Epoch 211/1000 
	 loss: 224.6226, MinusLogProbMetric: 224.6226, val_loss: 224.9585, val_MinusLogProbMetric: 224.9585

Epoch 211: val_loss improved from 225.10777 to 224.95851, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 224.6226 - MinusLogProbMetric: 224.6226 - val_loss: 224.9585 - val_MinusLogProbMetric: 224.9585 - lr: 1.2346e-05 - 111s/epoch - 567ms/step
Epoch 212/1000
2023-09-30 01:12:09.570 
Epoch 212/1000 
	 loss: 224.1418, MinusLogProbMetric: 224.1418, val_loss: 224.6892, val_MinusLogProbMetric: 224.6892

Epoch 212: val_loss improved from 224.95851 to 224.68916, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 112s - loss: 224.1418 - MinusLogProbMetric: 224.1418 - val_loss: 224.6892 - val_MinusLogProbMetric: 224.6892 - lr: 1.2346e-05 - 112s/epoch - 574ms/step
Epoch 213/1000
2023-09-30 01:13:59.480 
Epoch 213/1000 
	 loss: 223.6681, MinusLogProbMetric: 223.6681, val_loss: 223.8267, val_MinusLogProbMetric: 223.8267

Epoch 213: val_loss improved from 224.68916 to 223.82671, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 223.6681 - MinusLogProbMetric: 223.6681 - val_loss: 223.8267 - val_MinusLogProbMetric: 223.8267 - lr: 1.2346e-05 - 110s/epoch - 564ms/step
Epoch 214/1000
2023-09-30 01:15:51.341 
Epoch 214/1000 
	 loss: 223.1711, MinusLogProbMetric: 223.1711, val_loss: 223.3596, val_MinusLogProbMetric: 223.3596

Epoch 214: val_loss improved from 223.82671 to 223.35963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 112s - loss: 223.1711 - MinusLogProbMetric: 223.1711 - val_loss: 223.3596 - val_MinusLogProbMetric: 223.3596 - lr: 1.2346e-05 - 112s/epoch - 570ms/step
Epoch 215/1000
2023-09-30 01:17:42.390 
Epoch 215/1000 
	 loss: 222.6167, MinusLogProbMetric: 222.6167, val_loss: 223.1223, val_MinusLogProbMetric: 223.1223

Epoch 215: val_loss improved from 223.35963 to 223.12231, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 222.6167 - MinusLogProbMetric: 222.6167 - val_loss: 223.1223 - val_MinusLogProbMetric: 223.1223 - lr: 1.2346e-05 - 111s/epoch - 565ms/step
Epoch 216/1000
2023-09-30 01:19:30.196 
Epoch 216/1000 
	 loss: 222.1801, MinusLogProbMetric: 222.1801, val_loss: 222.4034, val_MinusLogProbMetric: 222.4034

Epoch 216: val_loss improved from 223.12231 to 222.40337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 108s - loss: 222.1801 - MinusLogProbMetric: 222.1801 - val_loss: 222.4034 - val_MinusLogProbMetric: 222.4034 - lr: 1.2346e-05 - 108s/epoch - 553ms/step
Epoch 217/1000
2023-09-30 01:21:24.009 
Epoch 217/1000 
	 loss: 221.7724, MinusLogProbMetric: 221.7724, val_loss: 221.8758, val_MinusLogProbMetric: 221.8758

Epoch 217: val_loss improved from 222.40337 to 221.87576, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 113s - loss: 221.7724 - MinusLogProbMetric: 221.7724 - val_loss: 221.8758 - val_MinusLogProbMetric: 221.8758 - lr: 1.2346e-05 - 113s/epoch - 576ms/step
Epoch 218/1000
2023-09-30 01:23:18.313 
Epoch 218/1000 
	 loss: 221.3451, MinusLogProbMetric: 221.3451, val_loss: 221.6246, val_MinusLogProbMetric: 221.6246

Epoch 218: val_loss improved from 221.87576 to 221.62459, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 115s - loss: 221.3451 - MinusLogProbMetric: 221.3451 - val_loss: 221.6246 - val_MinusLogProbMetric: 221.6246 - lr: 1.2346e-05 - 115s/epoch - 584ms/step
Epoch 219/1000
2023-09-30 01:25:07.647 
Epoch 219/1000 
	 loss: 220.7652, MinusLogProbMetric: 220.7652, val_loss: 221.2995, val_MinusLogProbMetric: 221.2995

Epoch 219: val_loss improved from 221.62459 to 221.29948, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 220.7652 - MinusLogProbMetric: 220.7652 - val_loss: 221.2995 - val_MinusLogProbMetric: 221.2995 - lr: 1.2346e-05 - 109s/epoch - 558ms/step
Epoch 220/1000
2023-09-30 01:26:57.680 
Epoch 220/1000 
	 loss: 220.5276, MinusLogProbMetric: 220.5276, val_loss: 220.7695, val_MinusLogProbMetric: 220.7695

Epoch 220: val_loss improved from 221.29948 to 220.76955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 220.5276 - MinusLogProbMetric: 220.5276 - val_loss: 220.7695 - val_MinusLogProbMetric: 220.7695 - lr: 1.2346e-05 - 110s/epoch - 563ms/step
Epoch 221/1000
2023-09-30 01:28:46.437 
Epoch 221/1000 
	 loss: 220.0286, MinusLogProbMetric: 220.0286, val_loss: 220.6571, val_MinusLogProbMetric: 220.6571

Epoch 221: val_loss improved from 220.76955 to 220.65712, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 220.0286 - MinusLogProbMetric: 220.0286 - val_loss: 220.6571 - val_MinusLogProbMetric: 220.6571 - lr: 1.2346e-05 - 109s/epoch - 555ms/step
Epoch 222/1000
2023-09-30 01:30:39.368 
Epoch 222/1000 
	 loss: 219.5776, MinusLogProbMetric: 219.5776, val_loss: 220.3268, val_MinusLogProbMetric: 220.3268

Epoch 222: val_loss improved from 220.65712 to 220.32680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 113s - loss: 219.5776 - MinusLogProbMetric: 219.5776 - val_loss: 220.3268 - val_MinusLogProbMetric: 220.3268 - lr: 1.2346e-05 - 113s/epoch - 578ms/step
Epoch 223/1000
2023-09-30 01:32:28.563 
Epoch 223/1000 
	 loss: 219.1670, MinusLogProbMetric: 219.1670, val_loss: 219.8559, val_MinusLogProbMetric: 219.8559

Epoch 223: val_loss improved from 220.32680 to 219.85585, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 219.1670 - MinusLogProbMetric: 219.1670 - val_loss: 219.8559 - val_MinusLogProbMetric: 219.8559 - lr: 1.2346e-05 - 109s/epoch - 556ms/step
Epoch 224/1000
2023-09-30 01:34:25.474 
Epoch 224/1000 
	 loss: 218.5959, MinusLogProbMetric: 218.5959, val_loss: 218.9463, val_MinusLogProbMetric: 218.9463

Epoch 224: val_loss improved from 219.85585 to 218.94630, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 117s - loss: 218.5959 - MinusLogProbMetric: 218.5959 - val_loss: 218.9463 - val_MinusLogProbMetric: 218.9463 - lr: 1.2346e-05 - 117s/epoch - 596ms/step
Epoch 225/1000
2023-09-30 01:36:19.520 
Epoch 225/1000 
	 loss: 218.3656, MinusLogProbMetric: 218.3656, val_loss: 219.3399, val_MinusLogProbMetric: 219.3399

Epoch 225: val_loss did not improve from 218.94630
196/196 - 112s - loss: 218.3656 - MinusLogProbMetric: 218.3656 - val_loss: 219.3399 - val_MinusLogProbMetric: 219.3399 - lr: 1.2346e-05 - 112s/epoch - 572ms/step
Epoch 226/1000
2023-09-30 01:38:08.661 
Epoch 226/1000 
	 loss: 218.2465, MinusLogProbMetric: 218.2465, val_loss: 218.8593, val_MinusLogProbMetric: 218.8593

Epoch 226: val_loss improved from 218.94630 to 218.85934, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 218.2465 - MinusLogProbMetric: 218.2465 - val_loss: 218.8593 - val_MinusLogProbMetric: 218.8593 - lr: 1.2346e-05 - 111s/epoch - 566ms/step
Epoch 227/1000
2023-09-30 01:40:05.584 
Epoch 227/1000 
	 loss: 217.6857, MinusLogProbMetric: 217.6857, val_loss: 218.0372, val_MinusLogProbMetric: 218.0372

Epoch 227: val_loss improved from 218.85934 to 218.03722, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 117s - loss: 217.6857 - MinusLogProbMetric: 217.6857 - val_loss: 218.0372 - val_MinusLogProbMetric: 218.0372 - lr: 1.2346e-05 - 117s/epoch - 599ms/step
Epoch 228/1000
2023-09-30 01:41:58.784 
Epoch 228/1000 
	 loss: 217.3103, MinusLogProbMetric: 217.3103, val_loss: 217.8815, val_MinusLogProbMetric: 217.8815

Epoch 228: val_loss improved from 218.03722 to 217.88153, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 114s - loss: 217.3103 - MinusLogProbMetric: 217.3103 - val_loss: 217.8815 - val_MinusLogProbMetric: 217.8815 - lr: 1.2346e-05 - 114s/epoch - 580ms/step
Epoch 229/1000
2023-09-30 01:43:52.718 
Epoch 229/1000 
	 loss: 216.8271, MinusLogProbMetric: 216.8271, val_loss: 217.2913, val_MinusLogProbMetric: 217.2913

Epoch 229: val_loss improved from 217.88153 to 217.29131, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 113s - loss: 216.8271 - MinusLogProbMetric: 216.8271 - val_loss: 217.2913 - val_MinusLogProbMetric: 217.2913 - lr: 1.2346e-05 - 113s/epoch - 577ms/step
Epoch 230/1000
2023-09-30 01:45:47.308 
Epoch 230/1000 
	 loss: 216.4292, MinusLogProbMetric: 216.4292, val_loss: 216.8077, val_MinusLogProbMetric: 216.8077

Epoch 230: val_loss improved from 217.29131 to 216.80769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 114s - loss: 216.4292 - MinusLogProbMetric: 216.4292 - val_loss: 216.8077 - val_MinusLogProbMetric: 216.8077 - lr: 1.2346e-05 - 114s/epoch - 582ms/step
Epoch 231/1000
2023-09-30 01:47:40.306 
Epoch 231/1000 
	 loss: 216.0383, MinusLogProbMetric: 216.0383, val_loss: 216.9300, val_MinusLogProbMetric: 216.9300

Epoch 231: val_loss did not improve from 216.80769
196/196 - 111s - loss: 216.0383 - MinusLogProbMetric: 216.0383 - val_loss: 216.9300 - val_MinusLogProbMetric: 216.9300 - lr: 1.2346e-05 - 111s/epoch - 569ms/step
Epoch 232/1000
2023-09-30 01:49:28.813 
Epoch 232/1000 
	 loss: 215.8325, MinusLogProbMetric: 215.8325, val_loss: 216.4457, val_MinusLogProbMetric: 216.4457

Epoch 232: val_loss improved from 216.80769 to 216.44571, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 215.8325 - MinusLogProbMetric: 215.8325 - val_loss: 216.4457 - val_MinusLogProbMetric: 216.4457 - lr: 1.2346e-05 - 111s/epoch - 564ms/step
Epoch 233/1000
2023-09-30 01:51:25.537 
Epoch 233/1000 
	 loss: 215.4113, MinusLogProbMetric: 215.4113, val_loss: 215.9815, val_MinusLogProbMetric: 215.9815

Epoch 233: val_loss improved from 216.44571 to 215.98146, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 116s - loss: 215.4113 - MinusLogProbMetric: 215.4113 - val_loss: 215.9815 - val_MinusLogProbMetric: 215.9815 - lr: 1.2346e-05 - 116s/epoch - 592ms/step
Epoch 234/1000
2023-09-30 01:53:14.709 
Epoch 234/1000 
	 loss: 214.7098, MinusLogProbMetric: 214.7098, val_loss: 214.8792, val_MinusLogProbMetric: 214.8792

Epoch 234: val_loss improved from 215.98146 to 214.87918, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 214.7098 - MinusLogProbMetric: 214.7098 - val_loss: 214.8792 - val_MinusLogProbMetric: 214.8792 - lr: 1.2346e-05 - 109s/epoch - 558ms/step
Epoch 235/1000
2023-09-30 01:55:10.515 
Epoch 235/1000 
	 loss: 214.3213, MinusLogProbMetric: 214.3213, val_loss: 214.6681, val_MinusLogProbMetric: 214.6681

Epoch 235: val_loss improved from 214.87918 to 214.66814, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 116s - loss: 214.3213 - MinusLogProbMetric: 214.3213 - val_loss: 214.6681 - val_MinusLogProbMetric: 214.6681 - lr: 1.2346e-05 - 116s/epoch - 589ms/step
Epoch 236/1000
2023-09-30 01:56:59.255 
Epoch 236/1000 
	 loss: 213.9157, MinusLogProbMetric: 213.9157, val_loss: 214.3956, val_MinusLogProbMetric: 214.3956

Epoch 236: val_loss improved from 214.66814 to 214.39563, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 213.9157 - MinusLogProbMetric: 213.9157 - val_loss: 214.3956 - val_MinusLogProbMetric: 214.3956 - lr: 1.2346e-05 - 109s/epoch - 555ms/step
Epoch 237/1000
2023-09-30 01:58:50.912 
Epoch 237/1000 
	 loss: 213.5222, MinusLogProbMetric: 213.5222, val_loss: 213.8641, val_MinusLogProbMetric: 213.8641

Epoch 237: val_loss improved from 214.39563 to 213.86411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 112s - loss: 213.5222 - MinusLogProbMetric: 213.5222 - val_loss: 213.8641 - val_MinusLogProbMetric: 213.8641 - lr: 1.2346e-05 - 112s/epoch - 570ms/step
Epoch 238/1000
2023-09-30 02:00:39.986 
Epoch 238/1000 
	 loss: 213.1362, MinusLogProbMetric: 213.1362, val_loss: 213.4619, val_MinusLogProbMetric: 213.4619

Epoch 238: val_loss improved from 213.86411 to 213.46190, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 213.1362 - MinusLogProbMetric: 213.1362 - val_loss: 213.4619 - val_MinusLogProbMetric: 213.4619 - lr: 1.2346e-05 - 110s/epoch - 560ms/step
Epoch 239/1000
2023-09-30 02:02:33.871 
Epoch 239/1000 
	 loss: 212.8122, MinusLogProbMetric: 212.8122, val_loss: 213.6195, val_MinusLogProbMetric: 213.6195

Epoch 239: val_loss did not improve from 213.46190
196/196 - 112s - loss: 212.8122 - MinusLogProbMetric: 212.8122 - val_loss: 213.6195 - val_MinusLogProbMetric: 213.6195 - lr: 1.2346e-05 - 112s/epoch - 570ms/step
Epoch 240/1000
2023-09-30 02:04:19.212 
Epoch 240/1000 
	 loss: 212.5702, MinusLogProbMetric: 212.5702, val_loss: 213.0171, val_MinusLogProbMetric: 213.0171

Epoch 240: val_loss improved from 213.46190 to 213.01714, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 212.5702 - MinusLogProbMetric: 212.5702 - val_loss: 213.0171 - val_MinusLogProbMetric: 213.0171 - lr: 1.2346e-05 - 107s/epoch - 546ms/step
Epoch 241/1000
2023-09-30 02:06:08.754 
Epoch 241/1000 
	 loss: 212.3026, MinusLogProbMetric: 212.3026, val_loss: 212.4370, val_MinusLogProbMetric: 212.4370

Epoch 241: val_loss improved from 213.01714 to 212.43701, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 212.3026 - MinusLogProbMetric: 212.3026 - val_loss: 212.4370 - val_MinusLogProbMetric: 212.4370 - lr: 1.2346e-05 - 110s/epoch - 559ms/step
Epoch 242/1000
2023-09-30 02:08:00.282 
Epoch 242/1000 
	 loss: 211.8857, MinusLogProbMetric: 211.8857, val_loss: 212.1070, val_MinusLogProbMetric: 212.1070

Epoch 242: val_loss improved from 212.43701 to 212.10704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 112s - loss: 211.8857 - MinusLogProbMetric: 211.8857 - val_loss: 212.1070 - val_MinusLogProbMetric: 212.1070 - lr: 1.2346e-05 - 112s/epoch - 570ms/step
Epoch 243/1000
2023-09-30 02:09:45.949 
Epoch 243/1000 
	 loss: 211.4070, MinusLogProbMetric: 211.4070, val_loss: 212.4291, val_MinusLogProbMetric: 212.4291

Epoch 243: val_loss did not improve from 212.10704
196/196 - 104s - loss: 211.4070 - MinusLogProbMetric: 211.4070 - val_loss: 212.4291 - val_MinusLogProbMetric: 212.4291 - lr: 1.2346e-05 - 104s/epoch - 530ms/step
Epoch 244/1000
2023-09-30 02:11:32.071 
Epoch 244/1000 
	 loss: 211.3370, MinusLogProbMetric: 211.3370, val_loss: 212.0457, val_MinusLogProbMetric: 212.0457

Epoch 244: val_loss improved from 212.10704 to 212.04573, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 211.3370 - MinusLogProbMetric: 211.3370 - val_loss: 212.0457 - val_MinusLogProbMetric: 212.0457 - lr: 1.2346e-05 - 107s/epoch - 548ms/step
Epoch 245/1000
2023-09-30 02:13:22.177 
Epoch 245/1000 
	 loss: 210.9525, MinusLogProbMetric: 210.9525, val_loss: 211.4512, val_MinusLogProbMetric: 211.4512

Epoch 245: val_loss improved from 212.04573 to 211.45119, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 210.9525 - MinusLogProbMetric: 210.9525 - val_loss: 211.4512 - val_MinusLogProbMetric: 211.4512 - lr: 1.2346e-05 - 111s/epoch - 566ms/step
Epoch 246/1000
2023-09-30 02:15:11.760 
Epoch 246/1000 
	 loss: 210.5449, MinusLogProbMetric: 210.5449, val_loss: 211.3018, val_MinusLogProbMetric: 211.3018

Epoch 246: val_loss improved from 211.45119 to 211.30183, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 210.5449 - MinusLogProbMetric: 210.5449 - val_loss: 211.3018 - val_MinusLogProbMetric: 211.3018 - lr: 1.2346e-05 - 110s/epoch - 560ms/step
Epoch 247/1000
2023-09-30 02:17:05.819 
Epoch 247/1000 
	 loss: 210.5807, MinusLogProbMetric: 210.5807, val_loss: 211.0217, val_MinusLogProbMetric: 211.0217

Epoch 247: val_loss improved from 211.30183 to 211.02167, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 113s - loss: 210.5807 - MinusLogProbMetric: 210.5807 - val_loss: 211.0217 - val_MinusLogProbMetric: 211.0217 - lr: 1.2346e-05 - 113s/epoch - 578ms/step
Epoch 248/1000
2023-09-30 02:18:54.403 
Epoch 248/1000 
	 loss: 209.8972, MinusLogProbMetric: 209.8972, val_loss: 210.6496, val_MinusLogProbMetric: 210.6496

Epoch 248: val_loss improved from 211.02167 to 210.64963, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 209.8972 - MinusLogProbMetric: 209.8972 - val_loss: 210.6496 - val_MinusLogProbMetric: 210.6496 - lr: 1.2346e-05 - 109s/epoch - 554ms/step
Epoch 249/1000
2023-09-30 02:20:39.942 
Epoch 249/1000 
	 loss: 209.6015, MinusLogProbMetric: 209.6015, val_loss: 210.9659, val_MinusLogProbMetric: 210.9659

Epoch 249: val_loss did not improve from 210.64963
196/196 - 104s - loss: 209.6015 - MinusLogProbMetric: 209.6015 - val_loss: 210.9659 - val_MinusLogProbMetric: 210.9659 - lr: 1.2346e-05 - 104s/epoch - 531ms/step
Epoch 250/1000
2023-09-30 02:22:26.003 
Epoch 250/1000 
	 loss: 209.3481, MinusLogProbMetric: 209.3481, val_loss: 209.6325, val_MinusLogProbMetric: 209.6325

Epoch 250: val_loss improved from 210.64963 to 209.63248, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 209.3481 - MinusLogProbMetric: 209.3481 - val_loss: 209.6325 - val_MinusLogProbMetric: 209.6325 - lr: 1.2346e-05 - 107s/epoch - 547ms/step
Epoch 251/1000
2023-09-30 02:24:16.573 
Epoch 251/1000 
	 loss: 208.7172, MinusLogProbMetric: 208.7172, val_loss: 209.3832, val_MinusLogProbMetric: 209.3832

Epoch 251: val_loss improved from 209.63248 to 209.38316, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 208.7172 - MinusLogProbMetric: 208.7172 - val_loss: 209.3832 - val_MinusLogProbMetric: 209.3832 - lr: 1.2346e-05 - 111s/epoch - 564ms/step
Epoch 252/1000
2023-09-30 02:26:06.053 
Epoch 252/1000 
	 loss: 208.4562, MinusLogProbMetric: 208.4562, val_loss: 209.4155, val_MinusLogProbMetric: 209.4155

Epoch 252: val_loss did not improve from 209.38316
196/196 - 108s - loss: 208.4562 - MinusLogProbMetric: 208.4562 - val_loss: 209.4155 - val_MinusLogProbMetric: 209.4155 - lr: 1.2346e-05 - 108s/epoch - 553ms/step
Epoch 253/1000
2023-09-30 02:27:57.744 
Epoch 253/1000 
	 loss: 208.3363, MinusLogProbMetric: 208.3363, val_loss: 208.9469, val_MinusLogProbMetric: 208.9469

Epoch 253: val_loss improved from 209.38316 to 208.94695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 114s - loss: 208.3363 - MinusLogProbMetric: 208.3363 - val_loss: 208.9469 - val_MinusLogProbMetric: 208.9469 - lr: 1.2346e-05 - 114s/epoch - 580ms/step
Epoch 254/1000
2023-09-30 02:29:48.996 
Epoch 254/1000 
	 loss: 208.0823, MinusLogProbMetric: 208.0823, val_loss: 209.3015, val_MinusLogProbMetric: 209.3015

Epoch 254: val_loss did not improve from 208.94695
196/196 - 109s - loss: 208.0823 - MinusLogProbMetric: 208.0823 - val_loss: 209.3015 - val_MinusLogProbMetric: 209.3015 - lr: 1.2346e-05 - 109s/epoch - 557ms/step
Epoch 255/1000
2023-09-30 02:31:34.029 
Epoch 255/1000 
	 loss: 207.7968, MinusLogProbMetric: 207.7968, val_loss: 208.2940, val_MinusLogProbMetric: 208.2940

Epoch 255: val_loss improved from 208.94695 to 208.29402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 207.7968 - MinusLogProbMetric: 207.7968 - val_loss: 208.2940 - val_MinusLogProbMetric: 208.2940 - lr: 1.2346e-05 - 107s/epoch - 544ms/step
Epoch 256/1000
2023-09-30 02:33:22.227 
Epoch 256/1000 
	 loss: 207.6697, MinusLogProbMetric: 207.6697, val_loss: 209.1166, val_MinusLogProbMetric: 209.1166

Epoch 256: val_loss did not improve from 208.29402
196/196 - 107s - loss: 207.6697 - MinusLogProbMetric: 207.6697 - val_loss: 209.1166 - val_MinusLogProbMetric: 209.1166 - lr: 1.2346e-05 - 107s/epoch - 543ms/step
Epoch 257/1000
2023-09-30 02:35:12.688 
Epoch 257/1000 
	 loss: 207.3531, MinusLogProbMetric: 207.3531, val_loss: 207.7390, val_MinusLogProbMetric: 207.7390

Epoch 257: val_loss improved from 208.29402 to 207.73900, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 112s - loss: 207.3531 - MinusLogProbMetric: 207.3531 - val_loss: 207.7390 - val_MinusLogProbMetric: 207.7390 - lr: 1.2346e-05 - 112s/epoch - 571ms/step
Epoch 258/1000
2023-09-30 02:37:00.123 
Epoch 258/1000 
	 loss: 206.8930, MinusLogProbMetric: 206.8930, val_loss: 207.3457, val_MinusLogProbMetric: 207.3457

Epoch 258: val_loss improved from 207.73900 to 207.34567, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 206.8930 - MinusLogProbMetric: 206.8930 - val_loss: 207.3457 - val_MinusLogProbMetric: 207.3457 - lr: 1.2346e-05 - 107s/epoch - 548ms/step
Epoch 259/1000
2023-09-30 02:38:49.216 
Epoch 259/1000 
	 loss: 206.8118, MinusLogProbMetric: 206.8118, val_loss: 207.0111, val_MinusLogProbMetric: 207.0111

Epoch 259: val_loss improved from 207.34567 to 207.01109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 109s - loss: 206.8118 - MinusLogProbMetric: 206.8118 - val_loss: 207.0111 - val_MinusLogProbMetric: 207.0111 - lr: 1.2346e-05 - 109s/epoch - 557ms/step
Epoch 260/1000
2023-09-30 02:40:41.234 
Epoch 260/1000 
	 loss: 206.2391, MinusLogProbMetric: 206.2391, val_loss: 206.8333, val_MinusLogProbMetric: 206.8333

Epoch 260: val_loss improved from 207.01109 to 206.83334, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 112s - loss: 206.2391 - MinusLogProbMetric: 206.2391 - val_loss: 206.8333 - val_MinusLogProbMetric: 206.8333 - lr: 1.2346e-05 - 112s/epoch - 573ms/step
Epoch 261/1000
2023-09-30 02:42:34.276 
Epoch 261/1000 
	 loss: 206.1131, MinusLogProbMetric: 206.1131, val_loss: 206.4448, val_MinusLogProbMetric: 206.4448

Epoch 261: val_loss improved from 206.83334 to 206.44476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 113s - loss: 206.1131 - MinusLogProbMetric: 206.1131 - val_loss: 206.4448 - val_MinusLogProbMetric: 206.4448 - lr: 1.2346e-05 - 113s/epoch - 576ms/step
Epoch 262/1000
2023-09-30 02:44:23.967 
Epoch 262/1000 
	 loss: 205.7381, MinusLogProbMetric: 205.7381, val_loss: 206.1272, val_MinusLogProbMetric: 206.1272

Epoch 262: val_loss improved from 206.44476 to 206.12715, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 110s - loss: 205.7381 - MinusLogProbMetric: 205.7381 - val_loss: 206.1272 - val_MinusLogProbMetric: 206.1272 - lr: 1.2346e-05 - 110s/epoch - 563ms/step
Epoch 263/1000
2023-09-30 02:46:15.921 
Epoch 263/1000 
	 loss: 205.5678, MinusLogProbMetric: 205.5678, val_loss: 205.7295, val_MinusLogProbMetric: 205.7295

Epoch 263: val_loss improved from 206.12715 to 205.72952, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 111s - loss: 205.5678 - MinusLogProbMetric: 205.5678 - val_loss: 205.7295 - val_MinusLogProbMetric: 205.7295 - lr: 1.2346e-05 - 111s/epoch - 566ms/step
Epoch 264/1000
2023-09-30 02:48:03.965 
Epoch 264/1000 
	 loss: 205.1117, MinusLogProbMetric: 205.1117, val_loss: 205.5919, val_MinusLogProbMetric: 205.5919

Epoch 264: val_loss improved from 205.72952 to 205.59189, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 108s - loss: 205.1117 - MinusLogProbMetric: 205.1117 - val_loss: 205.5919 - val_MinusLogProbMetric: 205.5919 - lr: 1.2346e-05 - 108s/epoch - 552ms/step
Epoch 265/1000
2023-09-30 02:49:50.103 
Epoch 265/1000 
	 loss: 204.8147, MinusLogProbMetric: 204.8147, val_loss: 205.1746, val_MinusLogProbMetric: 205.1746

Epoch 265: val_loss improved from 205.59189 to 205.17458, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 106s - loss: 204.8147 - MinusLogProbMetric: 204.8147 - val_loss: 205.1746 - val_MinusLogProbMetric: 205.1746 - lr: 1.2346e-05 - 106s/epoch - 541ms/step
Epoch 266/1000
2023-09-30 02:51:35.819 
Epoch 266/1000 
	 loss: 204.5629, MinusLogProbMetric: 204.5629, val_loss: 204.7558, val_MinusLogProbMetric: 204.7558

Epoch 266: val_loss improved from 205.17458 to 204.75583, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 106s - loss: 204.5629 - MinusLogProbMetric: 204.5629 - val_loss: 204.7558 - val_MinusLogProbMetric: 204.7558 - lr: 1.2346e-05 - 106s/epoch - 539ms/step
Epoch 267/1000
2023-09-30 02:53:19.745 
Epoch 267/1000 
	 loss: 204.2937, MinusLogProbMetric: 204.2937, val_loss: 204.7127, val_MinusLogProbMetric: 204.7127

Epoch 267: val_loss improved from 204.75583 to 204.71272, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 104s - loss: 204.2937 - MinusLogProbMetric: 204.2937 - val_loss: 204.7127 - val_MinusLogProbMetric: 204.7127 - lr: 1.2346e-05 - 104s/epoch - 530ms/step
Epoch 268/1000
2023-09-30 02:55:02.062 
Epoch 268/1000 
	 loss: 204.1194, MinusLogProbMetric: 204.1194, val_loss: 206.5415, val_MinusLogProbMetric: 206.5415

Epoch 268: val_loss did not improve from 204.71272
196/196 - 101s - loss: 204.1194 - MinusLogProbMetric: 204.1194 - val_loss: 206.5415 - val_MinusLogProbMetric: 206.5415 - lr: 1.2346e-05 - 101s/epoch - 515ms/step
Epoch 269/1000
2023-09-30 02:56:45.113 
Epoch 269/1000 
	 loss: 204.7239, MinusLogProbMetric: 204.7239, val_loss: 204.9855, val_MinusLogProbMetric: 204.9855

Epoch 269: val_loss did not improve from 204.71272
196/196 - 103s - loss: 204.7239 - MinusLogProbMetric: 204.7239 - val_loss: 204.9855 - val_MinusLogProbMetric: 204.9855 - lr: 1.2346e-05 - 103s/epoch - 526ms/step
Epoch 270/1000
2023-09-30 02:58:25.534 
Epoch 270/1000 
	 loss: 204.0537, MinusLogProbMetric: 204.0537, val_loss: 204.1868, val_MinusLogProbMetric: 204.1868

Epoch 270: val_loss improved from 204.71272 to 204.18680, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 102s - loss: 204.0537 - MinusLogProbMetric: 204.0537 - val_loss: 204.1868 - val_MinusLogProbMetric: 204.1868 - lr: 1.2346e-05 - 102s/epoch - 519ms/step
Epoch 271/1000
2023-09-30 03:00:11.233 
Epoch 271/1000 
	 loss: 203.5020, MinusLogProbMetric: 203.5020, val_loss: 204.1985, val_MinusLogProbMetric: 204.1985

Epoch 271: val_loss did not improve from 204.18680
196/196 - 104s - loss: 203.5020 - MinusLogProbMetric: 203.5020 - val_loss: 204.1985 - val_MinusLogProbMetric: 204.1985 - lr: 1.2346e-05 - 104s/epoch - 533ms/step
Epoch 272/1000
2023-09-30 03:01:53.446 
Epoch 272/1000 
	 loss: 203.2878, MinusLogProbMetric: 203.2878, val_loss: 203.7609, val_MinusLogProbMetric: 203.7609

Epoch 272: val_loss improved from 204.18680 to 203.76089, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 104s - loss: 203.2878 - MinusLogProbMetric: 203.2878 - val_loss: 203.7609 - val_MinusLogProbMetric: 203.7609 - lr: 1.2346e-05 - 104s/epoch - 530ms/step
Epoch 273/1000
2023-09-30 03:03:38.932 
Epoch 273/1000 
	 loss: 203.0876, MinusLogProbMetric: 203.0876, val_loss: 203.6028, val_MinusLogProbMetric: 203.6028

Epoch 273: val_loss improved from 203.76089 to 203.60284, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 105s - loss: 203.0876 - MinusLogProbMetric: 203.0876 - val_loss: 203.6028 - val_MinusLogProbMetric: 203.6028 - lr: 1.2346e-05 - 105s/epoch - 538ms/step
Epoch 274/1000
2023-09-30 03:05:19.889 
Epoch 274/1000 
	 loss: 202.4962, MinusLogProbMetric: 202.4962, val_loss: 203.3309, val_MinusLogProbMetric: 203.3309

Epoch 274: val_loss improved from 203.60284 to 203.33089, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 101s - loss: 202.4962 - MinusLogProbMetric: 202.4962 - val_loss: 203.3309 - val_MinusLogProbMetric: 203.3309 - lr: 1.2346e-05 - 101s/epoch - 513ms/step
Epoch 275/1000
2023-09-30 03:07:06.360 
Epoch 275/1000 
	 loss: 202.2845, MinusLogProbMetric: 202.2845, val_loss: 203.1238, val_MinusLogProbMetric: 203.1238

Epoch 275: val_loss improved from 203.33089 to 203.12379, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 202.2845 - MinusLogProbMetric: 202.2845 - val_loss: 203.1238 - val_MinusLogProbMetric: 203.1238 - lr: 1.2346e-05 - 107s/epoch - 544ms/step
Epoch 276/1000
2023-09-30 03:08:52.970 
Epoch 276/1000 
	 loss: 202.1011, MinusLogProbMetric: 202.1011, val_loss: 203.0883, val_MinusLogProbMetric: 203.0883

Epoch 276: val_loss improved from 203.12379 to 203.08829, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 202.1011 - MinusLogProbMetric: 202.1011 - val_loss: 203.0883 - val_MinusLogProbMetric: 203.0883 - lr: 1.2346e-05 - 107s/epoch - 545ms/step
Epoch 277/1000
2023-09-30 03:10:37.931 
Epoch 277/1000 
	 loss: 201.8650, MinusLogProbMetric: 201.8650, val_loss: 202.3571, val_MinusLogProbMetric: 202.3571

Epoch 277: val_loss improved from 203.08829 to 202.35713, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 105s - loss: 201.8650 - MinusLogProbMetric: 201.8650 - val_loss: 202.3571 - val_MinusLogProbMetric: 202.3571 - lr: 1.2346e-05 - 105s/epoch - 534ms/step
Epoch 278/1000
2023-09-30 03:12:18.860 
Epoch 278/1000 
	 loss: 201.5510, MinusLogProbMetric: 201.5510, val_loss: 202.5867, val_MinusLogProbMetric: 202.5867

Epoch 278: val_loss did not improve from 202.35713
196/196 - 100s - loss: 201.5510 - MinusLogProbMetric: 201.5510 - val_loss: 202.5867 - val_MinusLogProbMetric: 202.5867 - lr: 1.2346e-05 - 100s/epoch - 508ms/step
Epoch 279/1000
2023-09-30 03:14:03.301 
Epoch 279/1000 
	 loss: 201.9991, MinusLogProbMetric: 201.9991, val_loss: 209.1181, val_MinusLogProbMetric: 209.1181

Epoch 279: val_loss did not improve from 202.35713
196/196 - 104s - loss: 201.9991 - MinusLogProbMetric: 201.9991 - val_loss: 209.1181 - val_MinusLogProbMetric: 209.1181 - lr: 1.2346e-05 - 104s/epoch - 533ms/step
Epoch 280/1000
2023-09-30 03:15:45.454 
Epoch 280/1000 
	 loss: 203.0425, MinusLogProbMetric: 203.0425, val_loss: 202.9873, val_MinusLogProbMetric: 202.9873

Epoch 280: val_loss did not improve from 202.35713
196/196 - 102s - loss: 203.0425 - MinusLogProbMetric: 203.0425 - val_loss: 202.9873 - val_MinusLogProbMetric: 202.9873 - lr: 1.2346e-05 - 102s/epoch - 521ms/step
Epoch 281/1000
2023-09-30 03:17:26.463 
Epoch 281/1000 
	 loss: 202.0729, MinusLogProbMetric: 202.0729, val_loss: 202.3076, val_MinusLogProbMetric: 202.3076

Epoch 281: val_loss improved from 202.35713 to 202.30765, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 102s - loss: 202.0729 - MinusLogProbMetric: 202.0729 - val_loss: 202.3076 - val_MinusLogProbMetric: 202.3076 - lr: 1.2346e-05 - 102s/epoch - 522ms/step
Epoch 282/1000
2023-09-30 03:19:04.404 
Epoch 282/1000 
	 loss: 201.2603, MinusLogProbMetric: 201.2603, val_loss: 201.4609, val_MinusLogProbMetric: 201.4609

Epoch 282: val_loss improved from 202.30765 to 201.46088, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 98s - loss: 201.2603 - MinusLogProbMetric: 201.2603 - val_loss: 201.4609 - val_MinusLogProbMetric: 201.4609 - lr: 1.2346e-05 - 98s/epoch - 501ms/step
Epoch 283/1000
2023-09-30 03:20:51.452 
Epoch 283/1000 
	 loss: 200.6718, MinusLogProbMetric: 200.6718, val_loss: 200.8619, val_MinusLogProbMetric: 200.8619

Epoch 283: val_loss improved from 201.46088 to 200.86188, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_344/weights/best_weights.h5
196/196 - 107s - loss: 200.6718 - MinusLogProbMetric: 200.6718 - val_loss: 200.8619 - val_MinusLogProbMetric: 200.8619 - lr: 1.2346e-05 - 107s/epoch - 544ms/step
Epoch 284/1000
2023-09-30 03:22:34.446 
Epoch 284/1000 
	 loss: 200.4253, MinusLogProbMetric: 200.4253, val_loss: 202.3611, val_MinusLogProbMetric: 202.3611

Epoch 284: val_loss did not improve from 200.86188
196/196 - 102s - loss: 200.4253 - MinusLogProbMetric: 200.4253 - val_loss: 202.3611 - val_MinusLogProbMetric: 202.3611 - lr: 1.2346e-05 - 102s/epoch - 519ms/step
Epoch 285/1000
2023-09-30 03:24:13.956 
Epoch 285/1000 
	 loss: 200.4345, MinusLogProbMetric: 200.4345, val_loss: 201.1196, val_MinusLogProbMetric: 201.1196

Epoch 285: val_loss did not improve from 200.86188
196/196 - 100s - loss: 200.4345 - MinusLogProbMetric: 200.4345 - val_loss: 201.1196 - val_MinusLogProbMetric: 201.1196 - lr: 1.2346e-05 - 100s/epoch - 508ms/step
Epoch 286/1000
2023-09-30 03:25:57.062 
Epoch 286/1000 
	 loss: 585.3844, MinusLogProbMetric: 585.3844, val_loss: 514.8156, val_MinusLogProbMetric: 514.8156

Epoch 286: val_loss did not improve from 200.86188
196/196 - 103s - loss: 585.3844 - MinusLogProbMetric: 585.3844 - val_loss: 514.8156 - val_MinusLogProbMetric: 514.8156 - lr: 1.2346e-05 - 103s/epoch - 526ms/step
Epoch 287/1000
2023-09-30 03:27:36.663 
Epoch 287/1000 
	 loss: 483.6658, MinusLogProbMetric: 483.6658, val_loss: 463.1722, val_MinusLogProbMetric: 463.1722

Epoch 287: val_loss did not improve from 200.86188
196/196 - 100s - loss: 483.6658 - MinusLogProbMetric: 483.6658 - val_loss: 463.1722 - val_MinusLogProbMetric: 463.1722 - lr: 1.2346e-05 - 100s/epoch - 508ms/step
Epoch 288/1000
2023-09-30 03:29:17.934 
Epoch 288/1000 
	 loss: 450.1680, MinusLogProbMetric: 450.1680, val_loss: 441.4255, val_MinusLogProbMetric: 441.4255

Epoch 288: val_loss did not improve from 200.86188
196/196 - 101s - loss: 450.1680 - MinusLogProbMetric: 450.1680 - val_loss: 441.4255 - val_MinusLogProbMetric: 441.4255 - lr: 1.2346e-05 - 101s/epoch - 517ms/step
Epoch 289/1000
2023-09-30 03:30:59.980 
Epoch 289/1000 
	 loss: 433.5052, MinusLogProbMetric: 433.5052, val_loss: 426.9370, val_MinusLogProbMetric: 426.9370

Epoch 289: val_loss did not improve from 200.86188
196/196 - 102s - loss: 433.5052 - MinusLogProbMetric: 433.5052 - val_loss: 426.9370 - val_MinusLogProbMetric: 426.9370 - lr: 1.2346e-05 - 102s/epoch - 521ms/step
Epoch 290/1000
2023-09-30 03:32:41.747 
Epoch 290/1000 
	 loss: 421.1709, MinusLogProbMetric: 421.1709, val_loss: 416.6769, val_MinusLogProbMetric: 416.6769

Epoch 290: val_loss did not improve from 200.86188
196/196 - 102s - loss: 421.1709 - MinusLogProbMetric: 421.1709 - val_loss: 416.6769 - val_MinusLogProbMetric: 416.6769 - lr: 1.2346e-05 - 102s/epoch - 519ms/step
Epoch 291/1000
2023-09-30 03:34:23.861 
Epoch 291/1000 
	 loss: 412.2673, MinusLogProbMetric: 412.2673, val_loss: 408.5837, val_MinusLogProbMetric: 408.5837

Epoch 291: val_loss did not improve from 200.86188
196/196 - 102s - loss: 412.2673 - MinusLogProbMetric: 412.2673 - val_loss: 408.5837 - val_MinusLogProbMetric: 408.5837 - lr: 1.2346e-05 - 102s/epoch - 521ms/step
Epoch 292/1000
2023-09-30 03:36:07.955 
Epoch 292/1000 
	 loss: 404.6851, MinusLogProbMetric: 404.6851, val_loss: 399.9658, val_MinusLogProbMetric: 399.9658

Epoch 292: val_loss did not improve from 200.86188
196/196 - 104s - loss: 404.6851 - MinusLogProbMetric: 404.6851 - val_loss: 399.9658 - val_MinusLogProbMetric: 399.9658 - lr: 1.2346e-05 - 104s/epoch - 531ms/step
Epoch 293/1000
2023-09-30 03:37:48.844 
Epoch 293/1000 
	 loss: 372.3643, MinusLogProbMetric: 372.3643, val_loss: 362.7896, val_MinusLogProbMetric: 362.7896

Epoch 293: val_loss did not improve from 200.86188
196/196 - 101s - loss: 372.3643 - MinusLogProbMetric: 372.3643 - val_loss: 362.7896 - val_MinusLogProbMetric: 362.7896 - lr: 1.2346e-05 - 101s/epoch - 515ms/step
Epoch 294/1000
2023-09-30 03:39:26.925 
Epoch 294/1000 
	 loss: 359.3813, MinusLogProbMetric: 359.3813, val_loss: 356.1110, val_MinusLogProbMetric: 356.1110

Epoch 294: val_loss did not improve from 200.86188
196/196 - 98s - loss: 359.3813 - MinusLogProbMetric: 359.3813 - val_loss: 356.1110 - val_MinusLogProbMetric: 356.1110 - lr: 1.2346e-05 - 98s/epoch - 500ms/step
Epoch 295/1000
2023-09-30 03:41:12.677 
Epoch 295/1000 
	 loss: 353.4586, MinusLogProbMetric: 353.4586, val_loss: 350.9567, val_MinusLogProbMetric: 350.9567

Epoch 295: val_loss did not improve from 200.86188
196/196 - 106s - loss: 353.4586 - MinusLogProbMetric: 353.4586 - val_loss: 350.9567 - val_MinusLogProbMetric: 350.9567 - lr: 1.2346e-05 - 106s/epoch - 540ms/step
Epoch 296/1000
2023-09-30 03:42:55.508 
Epoch 296/1000 
	 loss: 348.9359, MinusLogProbMetric: 348.9359, val_loss: 346.9560, val_MinusLogProbMetric: 346.9560

Epoch 296: val_loss did not improve from 200.86188
196/196 - 103s - loss: 348.9359 - MinusLogProbMetric: 348.9359 - val_loss: 346.9560 - val_MinusLogProbMetric: 346.9560 - lr: 1.2346e-05 - 103s/epoch - 525ms/step
Epoch 297/1000
2023-09-30 03:44:42.654 
Epoch 297/1000 
	 loss: 345.3812, MinusLogProbMetric: 345.3812, val_loss: 343.5425, val_MinusLogProbMetric: 343.5425

Epoch 297: val_loss did not improve from 200.86188
196/196 - 107s - loss: 345.3812 - MinusLogProbMetric: 345.3812 - val_loss: 343.5425 - val_MinusLogProbMetric: 343.5425 - lr: 1.2346e-05 - 107s/epoch - 547ms/step
Epoch 298/1000
2023-09-30 03:46:29.397 
Epoch 298/1000 
	 loss: 342.6494, MinusLogProbMetric: 342.6494, val_loss: 341.0113, val_MinusLogProbMetric: 341.0113

Epoch 298: val_loss did not improve from 200.86188
196/196 - 107s - loss: 342.6494 - MinusLogProbMetric: 342.6494 - val_loss: 341.0113 - val_MinusLogProbMetric: 341.0113 - lr: 1.2346e-05 - 107s/epoch - 545ms/step
Epoch 299/1000
2023-09-30 03:48:15.906 
Epoch 299/1000 
	 loss: 340.7350, MinusLogProbMetric: 340.7350, val_loss: 340.7077, val_MinusLogProbMetric: 340.7077

Epoch 299: val_loss did not improve from 200.86188
196/196 - 107s - loss: 340.7350 - MinusLogProbMetric: 340.7350 - val_loss: 340.7077 - val_MinusLogProbMetric: 340.7077 - lr: 1.2346e-05 - 107s/epoch - 543ms/step
Epoch 300/1000
2023-09-30 03:50:01.983 
Epoch 300/1000 
	 loss: 339.5186, MinusLogProbMetric: 339.5186, val_loss: 338.2278, val_MinusLogProbMetric: 338.2278

Epoch 300: val_loss did not improve from 200.86188
196/196 - 106s - loss: 339.5186 - MinusLogProbMetric: 339.5186 - val_loss: 338.2278 - val_MinusLogProbMetric: 338.2278 - lr: 1.2346e-05 - 106s/epoch - 541ms/step
Epoch 301/1000
2023-09-30 03:51:48.527 
Epoch 301/1000 
	 loss: 669.7172, MinusLogProbMetric: 669.7172, val_loss: 484.0495, val_MinusLogProbMetric: 484.0495

Epoch 301: val_loss did not improve from 200.86188
196/196 - 107s - loss: 669.7172 - MinusLogProbMetric: 669.7172 - val_loss: 484.0495 - val_MinusLogProbMetric: 484.0495 - lr: 1.2346e-05 - 107s/epoch - 544ms/step
Epoch 302/1000
2023-09-30 03:53:35.482 
Epoch 302/1000 
	 loss: 462.8992, MinusLogProbMetric: 462.8992, val_loss: 446.7177, val_MinusLogProbMetric: 446.7177

Epoch 302: val_loss did not improve from 200.86188
196/196 - 107s - loss: 462.8992 - MinusLogProbMetric: 462.8992 - val_loss: 446.7177 - val_MinusLogProbMetric: 446.7177 - lr: 1.2346e-05 - 107s/epoch - 546ms/step
Epoch 303/1000
2023-09-30 03:55:19.921 
Epoch 303/1000 
	 loss: 436.0446, MinusLogProbMetric: 436.0446, val_loss: 426.5550, val_MinusLogProbMetric: 426.5550

Epoch 303: val_loss did not improve from 200.86188
196/196 - 104s - loss: 436.0446 - MinusLogProbMetric: 436.0446 - val_loss: 426.5550 - val_MinusLogProbMetric: 426.5550 - lr: 1.2346e-05 - 104s/epoch - 533ms/step
Epoch 304/1000
2023-09-30 03:57:04.973 
Epoch 304/1000 
	 loss: 418.8082, MinusLogProbMetric: 418.8082, val_loss: 411.3418, val_MinusLogProbMetric: 411.3418

Epoch 304: val_loss did not improve from 200.86188
196/196 - 105s - loss: 418.8082 - MinusLogProbMetric: 418.8082 - val_loss: 411.3418 - val_MinusLogProbMetric: 411.3418 - lr: 1.2346e-05 - 105s/epoch - 536ms/step
Epoch 305/1000
2023-09-30 03:58:51.610 
Epoch 305/1000 
	 loss: 405.2212, MinusLogProbMetric: 405.2212, val_loss: 398.9125, val_MinusLogProbMetric: 398.9125

Epoch 305: val_loss did not improve from 200.86188
196/196 - 107s - loss: 405.2212 - MinusLogProbMetric: 405.2212 - val_loss: 398.9125 - val_MinusLogProbMetric: 398.9125 - lr: 1.2346e-05 - 107s/epoch - 544ms/step
Epoch 306/1000
2023-09-30 04:00:38.904 
Epoch 306/1000 
	 loss: 394.2159, MinusLogProbMetric: 394.2159, val_loss: 389.4483, val_MinusLogProbMetric: 389.4483

Epoch 306: val_loss did not improve from 200.86188
196/196 - 107s - loss: 394.2159 - MinusLogProbMetric: 394.2159 - val_loss: 389.4483 - val_MinusLogProbMetric: 389.4483 - lr: 1.2346e-05 - 107s/epoch - 547ms/step
Epoch 307/1000
2023-09-30 04:02:25.043 
Epoch 307/1000 
	 loss: 384.5567, MinusLogProbMetric: 384.5567, val_loss: 380.4861, val_MinusLogProbMetric: 380.4861

Epoch 307: val_loss did not improve from 200.86188
196/196 - 106s - loss: 384.5567 - MinusLogProbMetric: 384.5567 - val_loss: 380.4861 - val_MinusLogProbMetric: 380.4861 - lr: 1.2346e-05 - 106s/epoch - 541ms/step
Epoch 308/1000
2023-09-30 04:04:13.557 
Epoch 308/1000 
	 loss: 377.2122, MinusLogProbMetric: 377.2122, val_loss: 374.2437, val_MinusLogProbMetric: 374.2437

Epoch 308: val_loss did not improve from 200.86188
196/196 - 109s - loss: 377.2122 - MinusLogProbMetric: 377.2122 - val_loss: 374.2437 - val_MinusLogProbMetric: 374.2437 - lr: 1.2346e-05 - 109s/epoch - 554ms/step
Epoch 309/1000
2023-09-30 04:05:59.943 
Epoch 309/1000 
	 loss: 371.5738, MinusLogProbMetric: 371.5738, val_loss: 368.9152, val_MinusLogProbMetric: 368.9152

Epoch 309: val_loss did not improve from 200.86188
196/196 - 106s - loss: 371.5738 - MinusLogProbMetric: 371.5738 - val_loss: 368.9152 - val_MinusLogProbMetric: 368.9152 - lr: 1.2346e-05 - 106s/epoch - 543ms/step
Epoch 310/1000
2023-09-30 04:07:47.142 
Epoch 310/1000 
	 loss: 366.3725, MinusLogProbMetric: 366.3725, val_loss: 364.1156, val_MinusLogProbMetric: 364.1156

Epoch 310: val_loss did not improve from 200.86188
196/196 - 107s - loss: 366.3725 - MinusLogProbMetric: 366.3725 - val_loss: 364.1156 - val_MinusLogProbMetric: 364.1156 - lr: 1.2346e-05 - 107s/epoch - 547ms/step
Epoch 311/1000
2023-09-30 04:09:34.068 
Epoch 311/1000 
	 loss: 362.2038, MinusLogProbMetric: 362.2038, val_loss: 360.0195, val_MinusLogProbMetric: 360.0195

Epoch 311: val_loss did not improve from 200.86188
196/196 - 107s - loss: 362.2038 - MinusLogProbMetric: 362.2038 - val_loss: 360.0195 - val_MinusLogProbMetric: 360.0195 - lr: 1.2346e-05 - 107s/epoch - 546ms/step
Epoch 312/1000
2023-09-30 04:11:01.841 
Epoch 312/1000 
	 loss: 359.7692, MinusLogProbMetric: 359.7692, val_loss: 357.2194, val_MinusLogProbMetric: 357.2194

Epoch 312: val_loss did not improve from 200.86188
196/196 - 88s - loss: 359.7692 - MinusLogProbMetric: 359.7692 - val_loss: 357.2194 - val_MinusLogProbMetric: 357.2194 - lr: 1.2346e-05 - 88s/epoch - 448ms/step
Epoch 313/1000
2023-09-30 04:12:31.464 
Epoch 313/1000 
	 loss: 355.1639, MinusLogProbMetric: 355.1639, val_loss: 353.3291, val_MinusLogProbMetric: 353.3291

Epoch 313: val_loss did not improve from 200.86188
196/196 - 90s - loss: 355.1639 - MinusLogProbMetric: 355.1639 - val_loss: 353.3291 - val_MinusLogProbMetric: 353.3291 - lr: 1.2346e-05 - 90s/epoch - 457ms/step
Epoch 314/1000
2023-09-30 04:14:17.162 
Epoch 314/1000 
	 loss: 351.3712, MinusLogProbMetric: 351.3712, val_loss: 349.9067, val_MinusLogProbMetric: 349.9067

Epoch 314: val_loss did not improve from 200.86188
196/196 - 106s - loss: 351.3712 - MinusLogProbMetric: 351.3712 - val_loss: 349.9067 - val_MinusLogProbMetric: 349.9067 - lr: 1.2346e-05 - 106s/epoch - 539ms/step
Epoch 315/1000
2023-09-30 04:16:04.688 
Epoch 315/1000 
	 loss: 348.4174, MinusLogProbMetric: 348.4174, val_loss: 347.0467, val_MinusLogProbMetric: 347.0467

Epoch 315: val_loss did not improve from 200.86188
196/196 - 108s - loss: 348.4174 - MinusLogProbMetric: 348.4174 - val_loss: 347.0467 - val_MinusLogProbMetric: 347.0467 - lr: 1.2346e-05 - 108s/epoch - 549ms/step
Epoch 316/1000
2023-09-30 04:17:51.097 
Epoch 316/1000 
	 loss: 346.0688, MinusLogProbMetric: 346.0688, val_loss: 344.6822, val_MinusLogProbMetric: 344.6822

Epoch 316: val_loss did not improve from 200.86188
196/196 - 106s - loss: 346.0688 - MinusLogProbMetric: 346.0688 - val_loss: 344.6822 - val_MinusLogProbMetric: 344.6822 - lr: 1.2346e-05 - 106s/epoch - 543ms/step
Epoch 317/1000
2023-09-30 04:19:35.777 
Epoch 317/1000 
	 loss: 343.1333, MinusLogProbMetric: 343.1333, val_loss: 342.2499, val_MinusLogProbMetric: 342.2499

Epoch 317: val_loss did not improve from 200.86188
196/196 - 105s - loss: 343.1333 - MinusLogProbMetric: 343.1333 - val_loss: 342.2499 - val_MinusLogProbMetric: 342.2499 - lr: 1.2346e-05 - 105s/epoch - 534ms/step
Epoch 318/1000
2023-09-30 04:21:20.617 
Epoch 318/1000 
	 loss: 340.9143, MinusLogProbMetric: 340.9143, val_loss: 339.9455, val_MinusLogProbMetric: 339.9455

Epoch 318: val_loss did not improve from 200.86188
196/196 - 105s - loss: 340.9143 - MinusLogProbMetric: 340.9143 - val_loss: 339.9455 - val_MinusLogProbMetric: 339.9455 - lr: 1.2346e-05 - 105s/epoch - 535ms/step
Epoch 319/1000
2023-09-30 04:23:05.243 
Epoch 319/1000 
	 loss: 524.1668, MinusLogProbMetric: 524.1668, val_loss: 431.0299, val_MinusLogProbMetric: 431.0299

Epoch 319: val_loss did not improve from 200.86188
196/196 - 105s - loss: 524.1668 - MinusLogProbMetric: 524.1668 - val_loss: 431.0299 - val_MinusLogProbMetric: 431.0299 - lr: 1.2346e-05 - 105s/epoch - 534ms/step
Epoch 320/1000
2023-09-30 04:24:49.185 
Epoch 320/1000 
	 loss: 414.6279, MinusLogProbMetric: 414.6279, val_loss: 411.6874, val_MinusLogProbMetric: 411.6874

Epoch 320: val_loss did not improve from 200.86188
196/196 - 104s - loss: 414.6279 - MinusLogProbMetric: 414.6279 - val_loss: 411.6874 - val_MinusLogProbMetric: 411.6874 - lr: 1.2346e-05 - 104s/epoch - 530ms/step
Epoch 321/1000
2023-09-30 04:26:34.199 
Epoch 321/1000 
	 loss: 395.8594, MinusLogProbMetric: 395.8594, val_loss: 386.4817, val_MinusLogProbMetric: 386.4817

Epoch 321: val_loss did not improve from 200.86188
196/196 - 105s - loss: 395.8594 - MinusLogProbMetric: 395.8594 - val_loss: 386.4817 - val_MinusLogProbMetric: 386.4817 - lr: 1.2346e-05 - 105s/epoch - 536ms/step
Epoch 322/1000
2023-09-30 04:28:20.656 
Epoch 322/1000 
	 loss: 382.5305, MinusLogProbMetric: 382.5305, val_loss: 379.1901, val_MinusLogProbMetric: 379.1901

Epoch 322: val_loss did not improve from 200.86188
196/196 - 106s - loss: 382.5305 - MinusLogProbMetric: 382.5305 - val_loss: 379.1901 - val_MinusLogProbMetric: 379.1901 - lr: 1.2346e-05 - 106s/epoch - 543ms/step
Epoch 323/1000
2023-09-30 04:30:06.864 
Epoch 323/1000 
	 loss: 374.5478, MinusLogProbMetric: 374.5478, val_loss: 372.3596, val_MinusLogProbMetric: 372.3596

Epoch 323: val_loss did not improve from 200.86188
196/196 - 106s - loss: 374.5478 - MinusLogProbMetric: 374.5478 - val_loss: 372.3596 - val_MinusLogProbMetric: 372.3596 - lr: 1.2346e-05 - 106s/epoch - 542ms/step
Epoch 324/1000
2023-09-30 04:31:54.248 
Epoch 324/1000 
	 loss: 369.4461, MinusLogProbMetric: 369.4461, val_loss: 362.9999, val_MinusLogProbMetric: 362.9999

Epoch 324: val_loss did not improve from 200.86188
196/196 - 107s - loss: 369.4461 - MinusLogProbMetric: 369.4461 - val_loss: 362.9999 - val_MinusLogProbMetric: 362.9999 - lr: 1.2346e-05 - 107s/epoch - 548ms/step
Epoch 325/1000
2023-09-30 04:33:37.552 
Epoch 325/1000 
	 loss: 363.4096, MinusLogProbMetric: 363.4096, val_loss: 357.2352, val_MinusLogProbMetric: 357.2352

Epoch 325: val_loss did not improve from 200.86188
196/196 - 103s - loss: 363.4096 - MinusLogProbMetric: 363.4096 - val_loss: 357.2352 - val_MinusLogProbMetric: 357.2352 - lr: 1.2346e-05 - 103s/epoch - 527ms/step
Epoch 326/1000
2023-09-30 04:35:23.094 
Epoch 326/1000 
	 loss: 353.1144, MinusLogProbMetric: 353.1144, val_loss: 352.3335, val_MinusLogProbMetric: 352.3335

Epoch 326: val_loss did not improve from 200.86188
196/196 - 106s - loss: 353.1144 - MinusLogProbMetric: 353.1144 - val_loss: 352.3335 - val_MinusLogProbMetric: 352.3335 - lr: 1.2346e-05 - 106s/epoch - 538ms/step
Epoch 327/1000
2023-09-30 04:37:10.289 
Epoch 327/1000 
	 loss: 349.0340, MinusLogProbMetric: 349.0340, val_loss: 349.6812, val_MinusLogProbMetric: 349.6812

Epoch 327: val_loss did not improve from 200.86188
196/196 - 107s - loss: 349.0340 - MinusLogProbMetric: 349.0340 - val_loss: 349.6812 - val_MinusLogProbMetric: 349.6812 - lr: 1.2346e-05 - 107s/epoch - 547ms/step
Epoch 328/1000
2023-09-30 04:38:55.530 
Epoch 328/1000 
	 loss: 350.4408, MinusLogProbMetric: 350.4408, val_loss: 347.9837, val_MinusLogProbMetric: 347.9837

Epoch 328: val_loss did not improve from 200.86188
196/196 - 105s - loss: 350.4408 - MinusLogProbMetric: 350.4408 - val_loss: 347.9837 - val_MinusLogProbMetric: 347.9837 - lr: 1.2346e-05 - 105s/epoch - 537ms/step
Epoch 329/1000
2023-09-30 04:40:41.339 
Epoch 329/1000 
	 loss: 339.0171, MinusLogProbMetric: 339.0171, val_loss: 333.9645, val_MinusLogProbMetric: 333.9645

Epoch 329: val_loss did not improve from 200.86188
196/196 - 106s - loss: 339.0171 - MinusLogProbMetric: 339.0171 - val_loss: 333.9645 - val_MinusLogProbMetric: 333.9645 - lr: 1.2346e-05 - 106s/epoch - 540ms/step
Epoch 330/1000
2023-09-30 04:42:27.022 
Epoch 330/1000 
	 loss: 331.8255, MinusLogProbMetric: 331.8255, val_loss: 330.2761, val_MinusLogProbMetric: 330.2761

Epoch 330: val_loss did not improve from 200.86188
196/196 - 106s - loss: 331.8255 - MinusLogProbMetric: 331.8255 - val_loss: 330.2761 - val_MinusLogProbMetric: 330.2761 - lr: 1.2346e-05 - 106s/epoch - 539ms/step
Epoch 331/1000
2023-09-30 04:44:11.966 
Epoch 331/1000 
	 loss: 329.0141, MinusLogProbMetric: 329.0141, val_loss: 327.8567, val_MinusLogProbMetric: 327.8567

Epoch 331: val_loss did not improve from 200.86188
196/196 - 105s - loss: 329.0141 - MinusLogProbMetric: 329.0141 - val_loss: 327.8567 - val_MinusLogProbMetric: 327.8567 - lr: 1.2346e-05 - 105s/epoch - 535ms/step
Epoch 332/1000
2023-09-30 04:45:55.627 
Epoch 332/1000 
	 loss: 326.5053, MinusLogProbMetric: 326.5053, val_loss: 325.3959, val_MinusLogProbMetric: 325.3959

Epoch 332: val_loss did not improve from 200.86188
196/196 - 104s - loss: 326.5053 - MinusLogProbMetric: 326.5053 - val_loss: 325.3959 - val_MinusLogProbMetric: 325.3959 - lr: 1.2346e-05 - 104s/epoch - 529ms/step
Epoch 333/1000
2023-09-30 04:47:40.867 
Epoch 333/1000 
	 loss: 324.5416, MinusLogProbMetric: 324.5416, val_loss: 323.6498, val_MinusLogProbMetric: 323.6498

Epoch 333: val_loss did not improve from 200.86188
196/196 - 105s - loss: 324.5416 - MinusLogProbMetric: 324.5416 - val_loss: 323.6498 - val_MinusLogProbMetric: 323.6498 - lr: 1.2346e-05 - 105s/epoch - 537ms/step
Epoch 334/1000
2023-09-30 04:49:25.109 
Epoch 334/1000 
	 loss: 322.9370, MinusLogProbMetric: 322.9370, val_loss: 322.6692, val_MinusLogProbMetric: 322.6692

Epoch 334: val_loss did not improve from 200.86188
196/196 - 104s - loss: 322.9370 - MinusLogProbMetric: 322.9370 - val_loss: 322.6692 - val_MinusLogProbMetric: 322.6692 - lr: 6.1728e-06 - 104s/epoch - 532ms/step
Epoch 335/1000
2023-09-30 04:51:08.817 
Epoch 335/1000 
	 loss: 322.2793, MinusLogProbMetric: 322.2793, val_loss: 321.8490, val_MinusLogProbMetric: 321.8490

Epoch 335: val_loss did not improve from 200.86188
196/196 - 104s - loss: 322.2793 - MinusLogProbMetric: 322.2793 - val_loss: 321.8490 - val_MinusLogProbMetric: 321.8490 - lr: 6.1728e-06 - 104s/epoch - 529ms/step
Epoch 336/1000
2023-09-30 04:52:52.102 
Epoch 336/1000 
	 loss: 321.5035, MinusLogProbMetric: 321.5035, val_loss: 321.0817, val_MinusLogProbMetric: 321.0817

Epoch 336: val_loss did not improve from 200.86188
196/196 - 103s - loss: 321.5035 - MinusLogProbMetric: 321.5035 - val_loss: 321.0817 - val_MinusLogProbMetric: 321.0817 - lr: 6.1728e-06 - 103s/epoch - 527ms/step
Epoch 337/1000
2023-09-30 04:54:36.918 
Epoch 337/1000 
	 loss: 320.6298, MinusLogProbMetric: 320.6298, val_loss: 320.3370, val_MinusLogProbMetric: 320.3370

Epoch 337: val_loss did not improve from 200.86188
196/196 - 105s - loss: 320.6298 - MinusLogProbMetric: 320.6298 - val_loss: 320.3370 - val_MinusLogProbMetric: 320.3370 - lr: 6.1728e-06 - 105s/epoch - 535ms/step
Epoch 338/1000
2023-09-30 04:56:21.332 
Epoch 338/1000 
	 loss: 319.7851, MinusLogProbMetric: 319.7851, val_loss: 319.3864, val_MinusLogProbMetric: 319.3864

Epoch 338: val_loss did not improve from 200.86188
196/196 - 104s - loss: 319.7851 - MinusLogProbMetric: 319.7851 - val_loss: 319.3864 - val_MinusLogProbMetric: 319.3864 - lr: 6.1728e-06 - 104s/epoch - 533ms/step
Epoch 339/1000
2023-09-30 04:58:07.040 
Epoch 339/1000 
	 loss: 319.1690, MinusLogProbMetric: 319.1690, val_loss: 318.7553, val_MinusLogProbMetric: 318.7553

Epoch 339: val_loss did not improve from 200.86188
196/196 - 106s - loss: 319.1690 - MinusLogProbMetric: 319.1690 - val_loss: 318.7553 - val_MinusLogProbMetric: 318.7553 - lr: 6.1728e-06 - 106s/epoch - 539ms/step
Epoch 340/1000
2023-09-30 04:59:51.909 
Epoch 340/1000 
	 loss: 318.3606, MinusLogProbMetric: 318.3606, val_loss: 317.7273, val_MinusLogProbMetric: 317.7273

Epoch 340: val_loss did not improve from 200.86188
196/196 - 105s - loss: 318.3606 - MinusLogProbMetric: 318.3606 - val_loss: 317.7273 - val_MinusLogProbMetric: 317.7273 - lr: 6.1728e-06 - 105s/epoch - 535ms/step
Epoch 341/1000
2023-09-30 05:01:37.120 
Epoch 341/1000 
	 loss: 317.4763, MinusLogProbMetric: 317.4763, val_loss: 317.0998, val_MinusLogProbMetric: 317.0998

Epoch 341: val_loss did not improve from 200.86188
196/196 - 105s - loss: 317.4763 - MinusLogProbMetric: 317.4763 - val_loss: 317.0998 - val_MinusLogProbMetric: 317.0998 - lr: 6.1728e-06 - 105s/epoch - 537ms/step
Epoch 342/1000
2023-09-30 05:03:19.550 
Epoch 342/1000 
	 loss: 316.7867, MinusLogProbMetric: 316.7867, val_loss: 316.5491, val_MinusLogProbMetric: 316.5491

Epoch 342: val_loss did not improve from 200.86188
196/196 - 102s - loss: 316.7867 - MinusLogProbMetric: 316.7867 - val_loss: 316.5491 - val_MinusLogProbMetric: 316.5491 - lr: 6.1728e-06 - 102s/epoch - 523ms/step
Epoch 343/1000
2023-09-30 05:05:03.782 
Epoch 343/1000 
	 loss: 316.1985, MinusLogProbMetric: 316.1985, val_loss: 315.9413, val_MinusLogProbMetric: 315.9413

Epoch 343: val_loss did not improve from 200.86188
196/196 - 104s - loss: 316.1985 - MinusLogProbMetric: 316.1985 - val_loss: 315.9413 - val_MinusLogProbMetric: 315.9413 - lr: 6.1728e-06 - 104s/epoch - 532ms/step
Epoch 344/1000
2023-09-30 05:06:47.383 
Epoch 344/1000 
	 loss: 315.5193, MinusLogProbMetric: 315.5193, val_loss: 315.1176, val_MinusLogProbMetric: 315.1176

Epoch 344: val_loss did not improve from 200.86188
196/196 - 104s - loss: 315.5193 - MinusLogProbMetric: 315.5193 - val_loss: 315.1176 - val_MinusLogProbMetric: 315.1176 - lr: 6.1728e-06 - 104s/epoch - 529ms/step
Epoch 345/1000
2023-09-30 05:08:33.827 
Epoch 345/1000 
	 loss: 314.9687, MinusLogProbMetric: 314.9687, val_loss: 314.6799, val_MinusLogProbMetric: 314.6799

Epoch 345: val_loss did not improve from 200.86188
196/196 - 106s - loss: 314.9687 - MinusLogProbMetric: 314.9687 - val_loss: 314.6799 - val_MinusLogProbMetric: 314.6799 - lr: 6.1728e-06 - 106s/epoch - 543ms/step
Epoch 346/1000
2023-09-30 05:10:18.698 
Epoch 346/1000 
	 loss: 314.3752, MinusLogProbMetric: 314.3752, val_loss: 313.9689, val_MinusLogProbMetric: 313.9689

Epoch 346: val_loss did not improve from 200.86188
196/196 - 105s - loss: 314.3752 - MinusLogProbMetric: 314.3752 - val_loss: 313.9689 - val_MinusLogProbMetric: 313.9689 - lr: 6.1728e-06 - 105s/epoch - 535ms/step
Epoch 347/1000
2023-09-30 05:12:03.626 
Epoch 347/1000 
	 loss: 313.8795, MinusLogProbMetric: 313.8795, val_loss: 313.7455, val_MinusLogProbMetric: 313.7455

Epoch 347: val_loss did not improve from 200.86188
196/196 - 105s - loss: 313.8795 - MinusLogProbMetric: 313.8795 - val_loss: 313.7455 - val_MinusLogProbMetric: 313.7455 - lr: 6.1728e-06 - 105s/epoch - 535ms/step
Epoch 348/1000
2023-09-30 05:13:48.051 
Epoch 348/1000 
	 loss: 313.3930, MinusLogProbMetric: 313.3930, val_loss: 313.0213, val_MinusLogProbMetric: 313.0213

Epoch 348: val_loss did not improve from 200.86188
196/196 - 104s - loss: 313.3930 - MinusLogProbMetric: 313.3930 - val_loss: 313.0213 - val_MinusLogProbMetric: 313.0213 - lr: 6.1728e-06 - 104s/epoch - 533ms/step
Epoch 349/1000
2023-09-30 05:15:30.236 
Epoch 349/1000 
	 loss: 312.9124, MinusLogProbMetric: 312.9124, val_loss: 312.4313, val_MinusLogProbMetric: 312.4313

Epoch 349: val_loss did not improve from 200.86188
196/196 - 102s - loss: 312.9124 - MinusLogProbMetric: 312.9124 - val_loss: 312.4313 - val_MinusLogProbMetric: 312.4313 - lr: 6.1728e-06 - 102s/epoch - 521ms/step
Epoch 350/1000
2023-09-30 05:17:14.184 
Epoch 350/1000 
	 loss: 312.1821, MinusLogProbMetric: 312.1821, val_loss: 311.8780, val_MinusLogProbMetric: 311.8780

Epoch 350: val_loss did not improve from 200.86188
196/196 - 104s - loss: 312.1821 - MinusLogProbMetric: 312.1821 - val_loss: 311.8780 - val_MinusLogProbMetric: 311.8780 - lr: 6.1728e-06 - 104s/epoch - 530ms/step
Epoch 351/1000
2023-09-30 05:19:00.039 
Epoch 351/1000 
	 loss: 311.6944, MinusLogProbMetric: 311.6944, val_loss: 311.5758, val_MinusLogProbMetric: 311.5758

Epoch 351: val_loss did not improve from 200.86188
196/196 - 106s - loss: 311.6944 - MinusLogProbMetric: 311.6944 - val_loss: 311.5758 - val_MinusLogProbMetric: 311.5758 - lr: 6.1728e-06 - 106s/epoch - 540ms/step
Epoch 352/1000
2023-09-30 05:20:45.745 
Epoch 352/1000 
	 loss: 311.2737, MinusLogProbMetric: 311.2737, val_loss: 311.0108, val_MinusLogProbMetric: 311.0108

Epoch 352: val_loss did not improve from 200.86188
196/196 - 106s - loss: 311.2737 - MinusLogProbMetric: 311.2737 - val_loss: 311.0108 - val_MinusLogProbMetric: 311.0108 - lr: 6.1728e-06 - 106s/epoch - 540ms/step
Epoch 353/1000
2023-09-30 05:22:32.127 
Epoch 353/1000 
	 loss: 310.7650, MinusLogProbMetric: 310.7650, val_loss: 310.6432, val_MinusLogProbMetric: 310.6432

Epoch 353: val_loss did not improve from 200.86188
196/196 - 106s - loss: 310.7650 - MinusLogProbMetric: 310.7650 - val_loss: 310.6432 - val_MinusLogProbMetric: 310.6432 - lr: 6.1728e-06 - 106s/epoch - 542ms/step
Epoch 354/1000
2023-09-30 05:24:14.129 
Epoch 354/1000 
	 loss: 310.4918, MinusLogProbMetric: 310.4918, val_loss: 310.8050, val_MinusLogProbMetric: 310.8050

Epoch 354: val_loss did not improve from 200.86188
196/196 - 102s - loss: 310.4918 - MinusLogProbMetric: 310.4918 - val_loss: 310.8050 - val_MinusLogProbMetric: 310.8050 - lr: 6.1728e-06 - 102s/epoch - 520ms/step
Epoch 355/1000
2023-09-30 05:25:52.833 
Epoch 355/1000 
	 loss: 309.9710, MinusLogProbMetric: 309.9710, val_loss: 309.5526, val_MinusLogProbMetric: 309.5526

Epoch 355: val_loss did not improve from 200.86188
196/196 - 99s - loss: 309.9710 - MinusLogProbMetric: 309.9710 - val_loss: 309.5526 - val_MinusLogProbMetric: 309.5526 - lr: 6.1728e-06 - 99s/epoch - 504ms/step
Epoch 356/1000
2023-09-30 05:27:34.812 
Epoch 356/1000 
	 loss: 309.4107, MinusLogProbMetric: 309.4107, val_loss: 309.1752, val_MinusLogProbMetric: 309.1752

Epoch 356: val_loss did not improve from 200.86188
196/196 - 102s - loss: 309.4107 - MinusLogProbMetric: 309.4107 - val_loss: 309.1752 - val_MinusLogProbMetric: 309.1752 - lr: 6.1728e-06 - 102s/epoch - 520ms/step
Epoch 357/1000
2023-09-30 05:29:15.320 
Epoch 357/1000 
	 loss: 308.9831, MinusLogProbMetric: 308.9831, val_loss: 308.7609, val_MinusLogProbMetric: 308.7609

Epoch 357: val_loss did not improve from 200.86188
196/196 - 101s - loss: 308.9831 - MinusLogProbMetric: 308.9831 - val_loss: 308.7609 - val_MinusLogProbMetric: 308.7609 - lr: 6.1728e-06 - 101s/epoch - 513ms/step
Epoch 358/1000
2023-09-30 05:30:56.791 
Epoch 358/1000 
	 loss: 309.0165, MinusLogProbMetric: 309.0165, val_loss: 308.9636, val_MinusLogProbMetric: 308.9636

Epoch 358: val_loss did not improve from 200.86188
196/196 - 101s - loss: 309.0165 - MinusLogProbMetric: 309.0165 - val_loss: 308.9636 - val_MinusLogProbMetric: 308.9636 - lr: 6.1728e-06 - 101s/epoch - 518ms/step
Epoch 359/1000
2023-09-30 05:32:35.312 
Epoch 359/1000 
	 loss: 308.2194, MinusLogProbMetric: 308.2194, val_loss: 307.8408, val_MinusLogProbMetric: 307.8408

Epoch 359: val_loss did not improve from 200.86188
196/196 - 99s - loss: 308.2194 - MinusLogProbMetric: 308.2194 - val_loss: 307.8408 - val_MinusLogProbMetric: 307.8408 - lr: 6.1728e-06 - 99s/epoch - 503ms/step
Epoch 360/1000
2023-09-30 05:34:13.486 
Epoch 360/1000 
	 loss: 307.6130, MinusLogProbMetric: 307.6130, val_loss: 307.5238, val_MinusLogProbMetric: 307.5238

Epoch 360: val_loss did not improve from 200.86188
196/196 - 98s - loss: 307.6130 - MinusLogProbMetric: 307.6130 - val_loss: 307.5238 - val_MinusLogProbMetric: 307.5238 - lr: 6.1728e-06 - 98s/epoch - 501ms/step
Epoch 361/1000
2023-09-30 05:35:56.746 
Epoch 361/1000 
	 loss: 307.2032, MinusLogProbMetric: 307.2032, val_loss: 306.9556, val_MinusLogProbMetric: 306.9556

Epoch 361: val_loss did not improve from 200.86188
196/196 - 103s - loss: 307.2032 - MinusLogProbMetric: 307.2032 - val_loss: 306.9556 - val_MinusLogProbMetric: 306.9556 - lr: 6.1728e-06 - 103s/epoch - 527ms/step
Epoch 362/1000
2023-09-30 05:37:38.796 
Epoch 362/1000 
	 loss: 306.9202, MinusLogProbMetric: 306.9202, val_loss: 306.8367, val_MinusLogProbMetric: 306.8367

Epoch 362: val_loss did not improve from 200.86188
196/196 - 102s - loss: 306.9202 - MinusLogProbMetric: 306.9202 - val_loss: 306.8367 - val_MinusLogProbMetric: 306.8367 - lr: 6.1728e-06 - 102s/epoch - 521ms/step
Epoch 363/1000
2023-09-30 05:39:19.336 
Epoch 363/1000 
	 loss: 306.4245, MinusLogProbMetric: 306.4245, val_loss: 306.2160, val_MinusLogProbMetric: 306.2160

Epoch 363: val_loss did not improve from 200.86188
196/196 - 101s - loss: 306.4245 - MinusLogProbMetric: 306.4245 - val_loss: 306.2160 - val_MinusLogProbMetric: 306.2160 - lr: 6.1728e-06 - 101s/epoch - 513ms/step
Epoch 364/1000
2023-09-30 05:40:59.462 
Epoch 364/1000 
	 loss: 306.0150, MinusLogProbMetric: 306.0150, val_loss: 306.4153, val_MinusLogProbMetric: 306.4153

Epoch 364: val_loss did not improve from 200.86188
196/196 - 100s - loss: 306.0150 - MinusLogProbMetric: 306.0150 - val_loss: 306.4153 - val_MinusLogProbMetric: 306.4153 - lr: 6.1728e-06 - 100s/epoch - 511ms/step
Epoch 365/1000
2023-09-30 05:42:39.465 
Epoch 365/1000 
	 loss: 305.7703, MinusLogProbMetric: 305.7703, val_loss: 305.6964, val_MinusLogProbMetric: 305.6964

Epoch 365: val_loss did not improve from 200.86188
196/196 - 100s - loss: 305.7703 - MinusLogProbMetric: 305.7703 - val_loss: 305.6964 - val_MinusLogProbMetric: 305.6964 - lr: 6.1728e-06 - 100s/epoch - 510ms/step
Epoch 366/1000
2023-09-30 05:44:18.665 
Epoch 366/1000 
	 loss: 305.7585, MinusLogProbMetric: 305.7585, val_loss: 305.5105, val_MinusLogProbMetric: 305.5105

Epoch 366: val_loss did not improve from 200.86188
196/196 - 99s - loss: 305.7585 - MinusLogProbMetric: 305.7585 - val_loss: 305.5105 - val_MinusLogProbMetric: 305.5105 - lr: 6.1728e-06 - 99s/epoch - 506ms/step
Epoch 367/1000
2023-09-30 05:46:01.156 
Epoch 367/1000 
	 loss: 305.3263, MinusLogProbMetric: 305.3263, val_loss: 305.2004, val_MinusLogProbMetric: 305.2004

Epoch 367: val_loss did not improve from 200.86188
196/196 - 102s - loss: 305.3263 - MinusLogProbMetric: 305.3263 - val_loss: 305.2004 - val_MinusLogProbMetric: 305.2004 - lr: 6.1728e-06 - 102s/epoch - 523ms/step
Epoch 368/1000
2023-09-30 05:47:41.239 
Epoch 368/1000 
	 loss: 304.9770, MinusLogProbMetric: 304.9770, val_loss: 304.5837, val_MinusLogProbMetric: 304.5837

Epoch 368: val_loss did not improve from 200.86188
196/196 - 100s - loss: 304.9770 - MinusLogProbMetric: 304.9770 - val_loss: 304.5837 - val_MinusLogProbMetric: 304.5837 - lr: 6.1728e-06 - 100s/epoch - 511ms/step
Epoch 369/1000
2023-09-30 05:49:23.460 
Epoch 369/1000 
	 loss: 304.2784, MinusLogProbMetric: 304.2784, val_loss: 304.5219, val_MinusLogProbMetric: 304.5219

Epoch 369: val_loss did not improve from 200.86188
196/196 - 102s - loss: 304.2784 - MinusLogProbMetric: 304.2784 - val_loss: 304.5219 - val_MinusLogProbMetric: 304.5219 - lr: 6.1728e-06 - 102s/epoch - 521ms/step
Epoch 370/1000
2023-09-30 05:51:02.771 
Epoch 370/1000 
	 loss: 304.0346, MinusLogProbMetric: 304.0346, val_loss: 303.8982, val_MinusLogProbMetric: 303.8982

Epoch 370: val_loss did not improve from 200.86188
196/196 - 99s - loss: 304.0346 - MinusLogProbMetric: 304.0346 - val_loss: 303.8982 - val_MinusLogProbMetric: 303.8982 - lr: 6.1728e-06 - 99s/epoch - 507ms/step
Epoch 371/1000
2023-09-30 05:52:40.969 
Epoch 371/1000 
	 loss: 303.4944, MinusLogProbMetric: 303.4944, val_loss: 303.3242, val_MinusLogProbMetric: 303.3242

Epoch 371: val_loss did not improve from 200.86188
196/196 - 98s - loss: 303.4944 - MinusLogProbMetric: 303.4944 - val_loss: 303.3242 - val_MinusLogProbMetric: 303.3242 - lr: 6.1728e-06 - 98s/epoch - 501ms/step
Epoch 372/1000
2023-09-30 05:54:22.353 
Epoch 372/1000 
	 loss: 302.9201, MinusLogProbMetric: 302.9201, val_loss: 302.7604, val_MinusLogProbMetric: 302.7604

Epoch 372: val_loss did not improve from 200.86188
196/196 - 101s - loss: 302.9201 - MinusLogProbMetric: 302.9201 - val_loss: 302.7604 - val_MinusLogProbMetric: 302.7604 - lr: 6.1728e-06 - 101s/epoch - 517ms/step
Epoch 373/1000
2023-09-30 05:56:00.832 
Epoch 373/1000 
	 loss: 302.5196, MinusLogProbMetric: 302.5196, val_loss: 302.4372, val_MinusLogProbMetric: 302.4372

Epoch 373: val_loss did not improve from 200.86188
196/196 - 98s - loss: 302.5196 - MinusLogProbMetric: 302.5196 - val_loss: 302.4372 - val_MinusLogProbMetric: 302.4372 - lr: 6.1728e-06 - 98s/epoch - 502ms/step
Epoch 374/1000
2023-09-30 05:57:39.608 
Epoch 374/1000 
	 loss: 302.0786, MinusLogProbMetric: 302.0786, val_loss: 301.9695, val_MinusLogProbMetric: 301.9695

Epoch 374: val_loss did not improve from 200.86188
196/196 - 99s - loss: 302.0786 - MinusLogProbMetric: 302.0786 - val_loss: 301.9695 - val_MinusLogProbMetric: 301.9695 - lr: 6.1728e-06 - 99s/epoch - 504ms/step
Epoch 375/1000
2023-09-30 05:59:18.462 
Epoch 375/1000 
	 loss: 301.8936, MinusLogProbMetric: 301.8936, val_loss: 301.8623, val_MinusLogProbMetric: 301.8623

Epoch 375: val_loss did not improve from 200.86188
196/196 - 99s - loss: 301.8936 - MinusLogProbMetric: 301.8936 - val_loss: 301.8623 - val_MinusLogProbMetric: 301.8623 - lr: 6.1728e-06 - 99s/epoch - 504ms/step
Epoch 376/1000
2023-09-30 06:00:56.987 
Epoch 376/1000 
	 loss: 301.4718, MinusLogProbMetric: 301.4718, val_loss: 301.2989, val_MinusLogProbMetric: 301.2989

Epoch 376: val_loss did not improve from 200.86188
196/196 - 99s - loss: 301.4718 - MinusLogProbMetric: 301.4718 - val_loss: 301.2989 - val_MinusLogProbMetric: 301.2989 - lr: 6.1728e-06 - 99s/epoch - 503ms/step
Epoch 377/1000
2023-09-30 06:02:38.197 
Epoch 377/1000 
	 loss: 301.0006, MinusLogProbMetric: 301.0006, val_loss: 300.9744, val_MinusLogProbMetric: 300.9744

Epoch 377: val_loss did not improve from 200.86188
196/196 - 101s - loss: 301.0006 - MinusLogProbMetric: 301.0006 - val_loss: 300.9744 - val_MinusLogProbMetric: 300.9744 - lr: 6.1728e-06 - 101s/epoch - 516ms/step
Epoch 378/1000
2023-09-30 06:04:18.776 
Epoch 378/1000 
	 loss: 300.5923, MinusLogProbMetric: 300.5923, val_loss: 300.4777, val_MinusLogProbMetric: 300.4777

Epoch 378: val_loss did not improve from 200.86188
196/196 - 101s - loss: 300.5923 - MinusLogProbMetric: 300.5923 - val_loss: 300.4777 - val_MinusLogProbMetric: 300.4777 - lr: 6.1728e-06 - 101s/epoch - 513ms/step
Epoch 379/1000
2023-09-30 06:06:00.109 
Epoch 379/1000 
	 loss: 306.8907, MinusLogProbMetric: 306.8907, val_loss: 302.7744, val_MinusLogProbMetric: 302.7744

Epoch 379: val_loss did not improve from 200.86188
196/196 - 101s - loss: 306.8907 - MinusLogProbMetric: 306.8907 - val_loss: 302.7744 - val_MinusLogProbMetric: 302.7744 - lr: 6.1728e-06 - 101s/epoch - 517ms/step
Epoch 380/1000
2023-09-30 06:07:41.158 
Epoch 380/1000 
	 loss: 301.8954, MinusLogProbMetric: 301.8954, val_loss: 301.7303, val_MinusLogProbMetric: 301.7303

Epoch 380: val_loss did not improve from 200.86188
196/196 - 101s - loss: 301.8954 - MinusLogProbMetric: 301.8954 - val_loss: 301.7303 - val_MinusLogProbMetric: 301.7303 - lr: 6.1728e-06 - 101s/epoch - 516ms/step
Epoch 381/1000
2023-09-30 06:09:23.702 
Epoch 381/1000 
	 loss: 301.2336, MinusLogProbMetric: 301.2336, val_loss: 301.1388, val_MinusLogProbMetric: 301.1388

Epoch 381: val_loss did not improve from 200.86188
196/196 - 103s - loss: 301.2336 - MinusLogProbMetric: 301.2336 - val_loss: 301.1388 - val_MinusLogProbMetric: 301.1388 - lr: 6.1728e-06 - 103s/epoch - 523ms/step
Epoch 382/1000
2023-09-30 06:11:01.963 
Epoch 382/1000 
	 loss: 300.5943, MinusLogProbMetric: 300.5943, val_loss: 300.4703, val_MinusLogProbMetric: 300.4703

Epoch 382: val_loss did not improve from 200.86188
196/196 - 98s - loss: 300.5943 - MinusLogProbMetric: 300.5943 - val_loss: 300.4703 - val_MinusLogProbMetric: 300.4703 - lr: 6.1728e-06 - 98s/epoch - 501ms/step
Epoch 383/1000
2023-09-30 06:12:40.880 
Epoch 383/1000 
	 loss: 299.9142, MinusLogProbMetric: 299.9142, val_loss: 299.8910, val_MinusLogProbMetric: 299.8910

Epoch 383: val_loss did not improve from 200.86188
Restoring model weights from the end of the best epoch: 283.
196/196 - 102s - loss: 299.9142 - MinusLogProbMetric: 299.9142 - val_loss: 299.8910 - val_MinusLogProbMetric: 299.8910 - lr: 6.1728e-06 - 102s/epoch - 518ms/step
Epoch 383: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.

------------------------------------------
Starting LR metric calculation...
Running TF LR calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
Warning: Removed a fraction of alternative samples due to non-finite log probabilities.
LR metric calculation completed in 133.96897778101265 seconds.

------------------------------------------
Starting KS tests calculation...
Running TF KS tests...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
KS tests calculation completed in 54.5755817029858 seconds.

------------------------------------------
Starting SWD metric calculation...
Running TF SWD calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
SWD metric calculation completed in 50.45656670804601 seconds.

------------------------------------------
Starting FN metric calculation...
Running TF FN calculation...
niter = 10
batch_size = 100000
The dist_1_num tensor is empty. Batches will be generated 'on-the-fly' from dist_1_symb.
The dist_2_num tensor is empty. Batches will be generated 'on-the-fly' from dist_2_symb.
FN metric calculation completed in 48.2648716350086 seconds.
Training succeeded with seed 377.
Model trained in 40835.17 s.

===========
Computing predictions
===========

Computing metrics...
Metrics computed in 289.91 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN.py , Line : 469, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 290.12 s.
===========
Run 344/720 done in 42362.08 s.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

===========
Generating train data for run 346.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 64)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_346/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_346/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[64], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_346/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_346
self.data_kwargs: {'seed': 440}
self.x_data: [[ 2.0547452   3.776408    6.9601746  ...  5.8637457  -0.4179731
   2.747595  ]
 [ 6.405085    2.999549    6.1033454  ...  2.8107233   3.0569289
   2.003619  ]
 [ 2.0047038   4.064143    6.2048492  ...  5.6568336   0.71395487
   2.6926432 ]
 ...
 [ 5.645455    7.0971727   6.202566   ...  0.61970913  6.238492
   1.3539525 ]
 [ 6.475603    3.1151166   6.2248073  ...  2.8990176   3.6724448
   2.289971  ]
 [ 2.2290072   4.101192    7.1413527  ...  6.717821   -2.0046902
   2.3999293 ]]
self.y_data: []
self.ndims: 64
Model defined.
Model: "model_463"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_464 (InputLayer)      [(None, 64)]              0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  1645920   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,645,920
Trainable params: 1,645,920
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7f8de7f8f5e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f8de7af6e00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f8de7af6e00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f8de7c66620>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f8de78bc280>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f8de78bc7f0>, <keras.callbacks.ModelCheckpoint object at 0x7f8de78bc8b0>, <keras.callbacks.EarlyStopping object at 0x7f8de78bcb20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f8de78bcb50>, <keras.callbacks.TerminateOnNaN object at 0x7f8de78bc790>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[5.7541595 , 5.721841  , 7.5400248 , ..., 0.17855006, 7.5787845 ,
        1.3515668 ],
       [5.442636  , 8.331263  , 5.8727856 , ..., 1.0242728 , 8.106918  ,
        1.274803  ],
       [1.849359  , 3.4476702 , 6.8133354 , ..., 6.851224  , 0.8453402 ,
        3.4794679 ],
       ...,
       [2.3177798 , 4.2122316 , 7.105509  , ..., 7.2533946 , 0.7699296 ,
        1.7943734 ],
       [6.3632    , 2.8481505 , 6.2368546 , ..., 3.6055064 , 5.1009455 ,
        2.9910412 ],
       [7.1731195 , 2.729657  , 6.1427665 , ..., 3.274747  , 1.7997704 ,
        2.8153737 ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_346/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 346/720 with hyperparameters:
timestamp = 2023-09-30 06:17:38.407966
ndims = 64
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 1645920
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.0547452   3.776408    6.9601746   1.2163222   8.644804    1.0450234
  9.756204    5.9333344   9.3759165   6.2056174   7.6663713   0.8787037
  2.7781603   1.7675867   3.9287794   0.8286338   3.6572614   4.846633
  0.68292063  6.317236    6.48625     3.843101    5.88724     2.0270782
  5.1297107   7.7579265   2.6054757   6.6726594   1.0543873   7.123391
  3.5526195   2.046467    5.620422   -0.869851    8.648443    0.02500496
  7.4141846   2.6777635   7.6790566   9.76465     2.481489    6.2672224
  5.7068043   4.504637    1.1161722   9.691578    4.0927534   8.503739
  6.9964066   3.1729455   8.086372    4.0078015   8.096493    5.813139
  8.181514    6.97417     7.246071    5.0315676  10.666024    6.2678714
  3.6855178   5.8637457  -0.4179731   2.747595  ]
Epoch 1/1000
2023-09-30 06:19:44.952 
Epoch 1/1000 
	 loss: 498.3464, MinusLogProbMetric: 498.3464, val_loss: 139.9914, val_MinusLogProbMetric: 139.9914

Epoch 1: val_loss improved from inf to 139.99141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 127s - loss: 498.3464 - MinusLogProbMetric: 498.3464 - val_loss: 139.9914 - val_MinusLogProbMetric: 139.9914 - lr: 0.0010 - 127s/epoch - 648ms/step
Epoch 2/1000
2023-09-30 06:20:43.160 
Epoch 2/1000 
	 loss: 94.4905, MinusLogProbMetric: 94.4905, val_loss: 74.6369, val_MinusLogProbMetric: 74.6369

Epoch 2: val_loss improved from 139.99141 to 74.63689, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 58s - loss: 94.4905 - MinusLogProbMetric: 94.4905 - val_loss: 74.6369 - val_MinusLogProbMetric: 74.6369 - lr: 0.0010 - 58s/epoch - 298ms/step
Epoch 3/1000
2023-09-30 06:21:35.535 
Epoch 3/1000 
	 loss: 65.4452, MinusLogProbMetric: 65.4452, val_loss: 57.9232, val_MinusLogProbMetric: 57.9232

Epoch 3: val_loss improved from 74.63689 to 57.92323, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 52s - loss: 65.4452 - MinusLogProbMetric: 65.4452 - val_loss: 57.9232 - val_MinusLogProbMetric: 57.9232 - lr: 0.0010 - 52s/epoch - 264ms/step
Epoch 4/1000
2023-09-30 06:22:19.411 
Epoch 4/1000 
	 loss: 55.3670, MinusLogProbMetric: 55.3670, val_loss: 49.9993, val_MinusLogProbMetric: 49.9993

Epoch 4: val_loss improved from 57.92323 to 49.99926, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 49s - loss: 55.3670 - MinusLogProbMetric: 55.3670 - val_loss: 49.9993 - val_MinusLogProbMetric: 49.9993 - lr: 0.0010 - 49s/epoch - 252ms/step
Epoch 5/1000
2023-09-30 06:23:12.408 
Epoch 5/1000 
	 loss: 49.4329, MinusLogProbMetric: 49.4329, val_loss: 52.8532, val_MinusLogProbMetric: 52.8532

Epoch 5: val_loss did not improve from 49.99926
196/196 - 47s - loss: 49.4329 - MinusLogProbMetric: 49.4329 - val_loss: 52.8532 - val_MinusLogProbMetric: 52.8532 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 6/1000
2023-09-30 06:23:56.590 
Epoch 6/1000 
	 loss: 46.4665, MinusLogProbMetric: 46.4665, val_loss: 43.8877, val_MinusLogProbMetric: 43.8877

Epoch 6: val_loss improved from 49.99926 to 43.88769, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 45s - loss: 46.4665 - MinusLogProbMetric: 46.4665 - val_loss: 43.8877 - val_MinusLogProbMetric: 43.8877 - lr: 0.0010 - 45s/epoch - 228ms/step
Epoch 7/1000
2023-09-30 06:24:41.557 
Epoch 7/1000 
	 loss: 43.9658, MinusLogProbMetric: 43.9658, val_loss: 43.6788, val_MinusLogProbMetric: 43.6788

Epoch 7: val_loss improved from 43.88769 to 43.67885, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 45s - loss: 43.9658 - MinusLogProbMetric: 43.9658 - val_loss: 43.6788 - val_MinusLogProbMetric: 43.6788 - lr: 0.0010 - 45s/epoch - 231ms/step
Epoch 8/1000
2023-09-30 06:25:24.533 
Epoch 8/1000 
	 loss: 42.4328, MinusLogProbMetric: 42.4328, val_loss: 41.2222, val_MinusLogProbMetric: 41.2222

Epoch 8: val_loss improved from 43.67885 to 41.22218, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 43s - loss: 42.4328 - MinusLogProbMetric: 42.4328 - val_loss: 41.2222 - val_MinusLogProbMetric: 41.2222 - lr: 0.0010 - 43s/epoch - 219ms/step
Epoch 9/1000
2023-09-30 06:26:10.639 
Epoch 9/1000 
	 loss: 41.3711, MinusLogProbMetric: 41.3711, val_loss: 42.1052, val_MinusLogProbMetric: 42.1052

Epoch 9: val_loss did not improve from 41.22218
196/196 - 45s - loss: 41.3711 - MinusLogProbMetric: 41.3711 - val_loss: 42.1052 - val_MinusLogProbMetric: 42.1052 - lr: 0.0010 - 45s/epoch - 231ms/step
Epoch 10/1000
2023-09-30 06:26:53.795 
Epoch 10/1000 
	 loss: 40.9136, MinusLogProbMetric: 40.9136, val_loss: 38.4820, val_MinusLogProbMetric: 38.4820

Epoch 10: val_loss improved from 41.22218 to 38.48201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 45s - loss: 40.9136 - MinusLogProbMetric: 40.9136 - val_loss: 38.4820 - val_MinusLogProbMetric: 38.4820 - lr: 0.0010 - 45s/epoch - 230ms/step
Epoch 11/1000
2023-09-30 06:27:37.960 
Epoch 11/1000 
	 loss: 39.4397, MinusLogProbMetric: 39.4397, val_loss: 38.3301, val_MinusLogProbMetric: 38.3301

Epoch 11: val_loss improved from 38.48201 to 38.33013, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 43s - loss: 39.4397 - MinusLogProbMetric: 39.4397 - val_loss: 38.3301 - val_MinusLogProbMetric: 38.3301 - lr: 0.0010 - 43s/epoch - 218ms/step
Epoch 12/1000
2023-09-30 06:28:21.139 
Epoch 12/1000 
	 loss: 38.5274, MinusLogProbMetric: 38.5274, val_loss: 37.6174, val_MinusLogProbMetric: 37.6174

Epoch 12: val_loss improved from 38.33013 to 37.61736, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 43s - loss: 38.5274 - MinusLogProbMetric: 38.5274 - val_loss: 37.6174 - val_MinusLogProbMetric: 37.6174 - lr: 0.0010 - 43s/epoch - 221ms/step
Epoch 13/1000
2023-09-30 06:29:08.468 
Epoch 13/1000 
	 loss: 38.0926, MinusLogProbMetric: 38.0926, val_loss: 37.1613, val_MinusLogProbMetric: 37.1613

Epoch 13: val_loss improved from 37.61736 to 37.16127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 48s - loss: 38.0926 - MinusLogProbMetric: 38.0926 - val_loss: 37.1613 - val_MinusLogProbMetric: 37.1613 - lr: 0.0010 - 48s/epoch - 243ms/step
Epoch 14/1000
2023-09-30 06:29:56.170 
Epoch 14/1000 
	 loss: 37.5044, MinusLogProbMetric: 37.5044, val_loss: 42.6735, val_MinusLogProbMetric: 42.6735

Epoch 14: val_loss did not improve from 37.16127
196/196 - 47s - loss: 37.5044 - MinusLogProbMetric: 37.5044 - val_loss: 42.6735 - val_MinusLogProbMetric: 42.6735 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 15/1000
2023-09-30 06:30:42.463 
Epoch 15/1000 
	 loss: 37.1320, MinusLogProbMetric: 37.1320, val_loss: 36.1073, val_MinusLogProbMetric: 36.1073

Epoch 15: val_loss improved from 37.16127 to 36.10734, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 47s - loss: 37.1320 - MinusLogProbMetric: 37.1320 - val_loss: 36.1073 - val_MinusLogProbMetric: 36.1073 - lr: 0.0010 - 47s/epoch - 239ms/step
Epoch 16/1000
2023-09-30 06:31:29.045 
Epoch 16/1000 
	 loss: 36.3584, MinusLogProbMetric: 36.3584, val_loss: 36.9950, val_MinusLogProbMetric: 36.9950

Epoch 16: val_loss did not improve from 36.10734
196/196 - 46s - loss: 36.3584 - MinusLogProbMetric: 36.3584 - val_loss: 36.9950 - val_MinusLogProbMetric: 36.9950 - lr: 0.0010 - 46s/epoch - 235ms/step
Epoch 17/1000
2023-09-30 06:32:14.539 
Epoch 17/1000 
	 loss: 36.0104, MinusLogProbMetric: 36.0104, val_loss: 37.2041, val_MinusLogProbMetric: 37.2041

Epoch 17: val_loss did not improve from 36.10734
196/196 - 45s - loss: 36.0104 - MinusLogProbMetric: 36.0104 - val_loss: 37.2041 - val_MinusLogProbMetric: 37.2041 - lr: 0.0010 - 45s/epoch - 232ms/step
Epoch 18/1000
2023-09-30 06:32:59.060 
Epoch 18/1000 
	 loss: 35.6355, MinusLogProbMetric: 35.6355, val_loss: 36.1141, val_MinusLogProbMetric: 36.1141

Epoch 18: val_loss did not improve from 36.10734
196/196 - 45s - loss: 35.6355 - MinusLogProbMetric: 35.6355 - val_loss: 36.1141 - val_MinusLogProbMetric: 36.1141 - lr: 0.0010 - 45s/epoch - 227ms/step
Epoch 19/1000
2023-09-30 06:33:45.277 
Epoch 19/1000 
	 loss: 35.5198, MinusLogProbMetric: 35.5198, val_loss: 39.0535, val_MinusLogProbMetric: 39.0535

Epoch 19: val_loss did not improve from 36.10734
196/196 - 46s - loss: 35.5198 - MinusLogProbMetric: 35.5198 - val_loss: 39.0535 - val_MinusLogProbMetric: 39.0535 - lr: 0.0010 - 46s/epoch - 236ms/step
Epoch 20/1000
2023-09-30 06:34:30.567 
Epoch 20/1000 
	 loss: 35.2213, MinusLogProbMetric: 35.2213, val_loss: 36.2159, val_MinusLogProbMetric: 36.2159

Epoch 20: val_loss did not improve from 36.10734
196/196 - 45s - loss: 35.2213 - MinusLogProbMetric: 35.2213 - val_loss: 36.2159 - val_MinusLogProbMetric: 36.2159 - lr: 0.0010 - 45s/epoch - 231ms/step
Epoch 21/1000
2023-09-30 06:35:22.351 
Epoch 21/1000 
	 loss: 34.8933, MinusLogProbMetric: 34.8933, val_loss: 35.3261, val_MinusLogProbMetric: 35.3261

Epoch 21: val_loss improved from 36.10734 to 35.32608, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 53s - loss: 34.8933 - MinusLogProbMetric: 34.8933 - val_loss: 35.3261 - val_MinusLogProbMetric: 35.3261 - lr: 0.0010 - 53s/epoch - 272ms/step
Epoch 22/1000
2023-09-30 06:36:19.343 
Epoch 22/1000 
	 loss: 34.5984, MinusLogProbMetric: 34.5984, val_loss: 35.0515, val_MinusLogProbMetric: 35.0515

Epoch 22: val_loss improved from 35.32608 to 35.05149, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 56s - loss: 34.5984 - MinusLogProbMetric: 34.5984 - val_loss: 35.0515 - val_MinusLogProbMetric: 35.0515 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 23/1000
2023-09-30 06:37:18.398 
Epoch 23/1000 
	 loss: 34.5883, MinusLogProbMetric: 34.5883, val_loss: 34.4844, val_MinusLogProbMetric: 34.4844

Epoch 23: val_loss improved from 35.05149 to 34.48436, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 59s - loss: 34.5883 - MinusLogProbMetric: 34.5883 - val_loss: 34.4844 - val_MinusLogProbMetric: 34.4844 - lr: 0.0010 - 59s/epoch - 303ms/step
Epoch 24/1000
2023-09-30 06:38:19.348 
Epoch 24/1000 
	 loss: 34.1952, MinusLogProbMetric: 34.1952, val_loss: 33.5091, val_MinusLogProbMetric: 33.5091

Epoch 24: val_loss improved from 34.48436 to 33.50908, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 60s - loss: 34.1952 - MinusLogProbMetric: 34.1952 - val_loss: 33.5091 - val_MinusLogProbMetric: 33.5091 - lr: 0.0010 - 60s/epoch - 308ms/step
Epoch 25/1000
2023-09-30 06:39:15.080 
Epoch 25/1000 
	 loss: 33.8248, MinusLogProbMetric: 33.8248, val_loss: 33.5146, val_MinusLogProbMetric: 33.5146

Epoch 25: val_loss did not improve from 33.50908
196/196 - 55s - loss: 33.8248 - MinusLogProbMetric: 33.8248 - val_loss: 33.5146 - val_MinusLogProbMetric: 33.5146 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 26/1000
2023-09-30 06:40:12.406 
Epoch 26/1000 
	 loss: 33.7576, MinusLogProbMetric: 33.7576, val_loss: 33.2307, val_MinusLogProbMetric: 33.2307

Epoch 26: val_loss improved from 33.50908 to 33.23075, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 58s - loss: 33.7576 - MinusLogProbMetric: 33.7576 - val_loss: 33.2307 - val_MinusLogProbMetric: 33.2307 - lr: 0.0010 - 58s/epoch - 296ms/step
Epoch 27/1000
2023-09-30 06:41:06.061 
Epoch 27/1000 
	 loss: 33.6076, MinusLogProbMetric: 33.6076, val_loss: 33.3422, val_MinusLogProbMetric: 33.3422

Epoch 27: val_loss did not improve from 33.23075
196/196 - 53s - loss: 33.6076 - MinusLogProbMetric: 33.6076 - val_loss: 33.3422 - val_MinusLogProbMetric: 33.3422 - lr: 0.0010 - 53s/epoch - 270ms/step
Epoch 28/1000
2023-09-30 06:42:06.001 
Epoch 28/1000 
	 loss: 33.6533, MinusLogProbMetric: 33.6533, val_loss: 33.7514, val_MinusLogProbMetric: 33.7514

Epoch 28: val_loss did not improve from 33.23075
196/196 - 60s - loss: 33.6533 - MinusLogProbMetric: 33.6533 - val_loss: 33.7514 - val_MinusLogProbMetric: 33.7514 - lr: 0.0010 - 60s/epoch - 306ms/step
Epoch 29/1000
2023-09-30 06:43:03.764 
Epoch 29/1000 
	 loss: 33.5395, MinusLogProbMetric: 33.5395, val_loss: 34.8830, val_MinusLogProbMetric: 34.8830

Epoch 29: val_loss did not improve from 33.23075
196/196 - 58s - loss: 33.5395 - MinusLogProbMetric: 33.5395 - val_loss: 34.8830 - val_MinusLogProbMetric: 34.8830 - lr: 0.0010 - 58s/epoch - 295ms/step
Epoch 30/1000
2023-09-30 06:44:00.113 
Epoch 30/1000 
	 loss: 33.1643, MinusLogProbMetric: 33.1643, val_loss: 33.7663, val_MinusLogProbMetric: 33.7663

Epoch 30: val_loss did not improve from 33.23075
196/196 - 56s - loss: 33.1643 - MinusLogProbMetric: 33.1643 - val_loss: 33.7663 - val_MinusLogProbMetric: 33.7663 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 31/1000
2023-09-30 06:44:56.780 
Epoch 31/1000 
	 loss: 33.3445, MinusLogProbMetric: 33.3445, val_loss: 34.6021, val_MinusLogProbMetric: 34.6021

Epoch 31: val_loss did not improve from 33.23075
196/196 - 57s - loss: 33.3445 - MinusLogProbMetric: 33.3445 - val_loss: 34.6021 - val_MinusLogProbMetric: 34.6021 - lr: 0.0010 - 57s/epoch - 289ms/step
Epoch 32/1000
2023-09-30 06:45:57.347 
Epoch 32/1000 
	 loss: 33.1816, MinusLogProbMetric: 33.1816, val_loss: 34.0952, val_MinusLogProbMetric: 34.0952

Epoch 32: val_loss did not improve from 33.23075
196/196 - 61s - loss: 33.1816 - MinusLogProbMetric: 33.1816 - val_loss: 34.0952 - val_MinusLogProbMetric: 34.0952 - lr: 0.0010 - 61s/epoch - 309ms/step
Epoch 33/1000
2023-09-30 06:46:53.896 
Epoch 33/1000 
	 loss: 33.0861, MinusLogProbMetric: 33.0861, val_loss: 33.5932, val_MinusLogProbMetric: 33.5932

Epoch 33: val_loss did not improve from 33.23075
196/196 - 57s - loss: 33.0861 - MinusLogProbMetric: 33.0861 - val_loss: 33.5932 - val_MinusLogProbMetric: 33.5932 - lr: 0.0010 - 57s/epoch - 288ms/step
Epoch 34/1000
2023-09-30 06:47:50.034 
Epoch 34/1000 
	 loss: 32.9740, MinusLogProbMetric: 32.9740, val_loss: 33.1426, val_MinusLogProbMetric: 33.1426

Epoch 34: val_loss improved from 33.23075 to 33.14265, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 57s - loss: 32.9740 - MinusLogProbMetric: 32.9740 - val_loss: 33.1426 - val_MinusLogProbMetric: 33.1426 - lr: 0.0010 - 57s/epoch - 291ms/step
Epoch 35/1000
2023-09-30 06:48:47.175 
Epoch 35/1000 
	 loss: 32.9410, MinusLogProbMetric: 32.9410, val_loss: 34.2212, val_MinusLogProbMetric: 34.2212

Epoch 35: val_loss did not improve from 33.14265
196/196 - 56s - loss: 32.9410 - MinusLogProbMetric: 32.9410 - val_loss: 34.2212 - val_MinusLogProbMetric: 34.2212 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 36/1000
2023-09-30 06:49:43.464 
Epoch 36/1000 
	 loss: 32.6356, MinusLogProbMetric: 32.6356, val_loss: 32.5713, val_MinusLogProbMetric: 32.5713

Epoch 36: val_loss improved from 33.14265 to 32.57127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 57s - loss: 32.6356 - MinusLogProbMetric: 32.6356 - val_loss: 32.5713 - val_MinusLogProbMetric: 32.5713 - lr: 0.0010 - 57s/epoch - 289ms/step
Epoch 37/1000
2023-09-30 06:50:39.268 
Epoch 37/1000 
	 loss: 32.6395, MinusLogProbMetric: 32.6395, val_loss: 33.8094, val_MinusLogProbMetric: 33.8094

Epoch 37: val_loss did not improve from 32.57127
196/196 - 55s - loss: 32.6395 - MinusLogProbMetric: 32.6395 - val_loss: 33.8094 - val_MinusLogProbMetric: 33.8094 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 38/1000
2023-09-30 06:51:34.673 
Epoch 38/1000 
	 loss: 32.4638, MinusLogProbMetric: 32.4638, val_loss: 32.9550, val_MinusLogProbMetric: 32.9550

Epoch 38: val_loss did not improve from 32.57127
196/196 - 55s - loss: 32.4638 - MinusLogProbMetric: 32.4638 - val_loss: 32.9550 - val_MinusLogProbMetric: 32.9550 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 39/1000
2023-09-30 06:52:30.129 
Epoch 39/1000 
	 loss: 32.3133, MinusLogProbMetric: 32.3133, val_loss: 32.1189, val_MinusLogProbMetric: 32.1189

Epoch 39: val_loss improved from 32.57127 to 32.11889, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 57s - loss: 32.3133 - MinusLogProbMetric: 32.3133 - val_loss: 32.1189 - val_MinusLogProbMetric: 32.1189 - lr: 0.0010 - 57s/epoch - 293ms/step
Epoch 40/1000
2023-09-30 06:53:28.386 
Epoch 40/1000 
	 loss: 32.2153, MinusLogProbMetric: 32.2153, val_loss: 32.8592, val_MinusLogProbMetric: 32.8592

Epoch 40: val_loss did not improve from 32.11889
196/196 - 56s - loss: 32.2153 - MinusLogProbMetric: 32.2153 - val_loss: 32.8592 - val_MinusLogProbMetric: 32.8592 - lr: 0.0010 - 56s/epoch - 287ms/step
Epoch 41/1000
2023-09-30 06:54:27.629 
Epoch 41/1000 
	 loss: 32.3335, MinusLogProbMetric: 32.3335, val_loss: 33.5055, val_MinusLogProbMetric: 33.5055

Epoch 41: val_loss did not improve from 32.11889
196/196 - 59s - loss: 32.3335 - MinusLogProbMetric: 32.3335 - val_loss: 33.5055 - val_MinusLogProbMetric: 33.5055 - lr: 0.0010 - 59s/epoch - 302ms/step
Epoch 42/1000
2023-09-30 06:55:22.288 
Epoch 42/1000 
	 loss: 31.9773, MinusLogProbMetric: 31.9773, val_loss: 34.4268, val_MinusLogProbMetric: 34.4268

Epoch 42: val_loss did not improve from 32.11889
196/196 - 55s - loss: 31.9773 - MinusLogProbMetric: 31.9773 - val_loss: 34.4268 - val_MinusLogProbMetric: 34.4268 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 43/1000
2023-09-30 06:56:18.963 
Epoch 43/1000 
	 loss: 31.9036, MinusLogProbMetric: 31.9036, val_loss: 32.0469, val_MinusLogProbMetric: 32.0469

Epoch 43: val_loss improved from 32.11889 to 32.04691, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 58s - loss: 31.9036 - MinusLogProbMetric: 31.9036 - val_loss: 32.0469 - val_MinusLogProbMetric: 32.0469 - lr: 0.0010 - 58s/epoch - 295ms/step
Epoch 44/1000
2023-09-30 06:57:16.775 
Epoch 44/1000 
	 loss: 32.0479, MinusLogProbMetric: 32.0479, val_loss: 32.2036, val_MinusLogProbMetric: 32.2036

Epoch 44: val_loss did not improve from 32.04691
196/196 - 57s - loss: 32.0479 - MinusLogProbMetric: 32.0479 - val_loss: 32.2036 - val_MinusLogProbMetric: 32.2036 - lr: 0.0010 - 57s/epoch - 289ms/step
Epoch 45/1000
2023-09-30 06:58:13.616 
Epoch 45/1000 
	 loss: 31.6730, MinusLogProbMetric: 31.6730, val_loss: 31.7834, val_MinusLogProbMetric: 31.7834

Epoch 45: val_loss improved from 32.04691 to 31.78335, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 58s - loss: 31.6730 - MinusLogProbMetric: 31.6730 - val_loss: 31.7834 - val_MinusLogProbMetric: 31.7834 - lr: 0.0010 - 58s/epoch - 298ms/step
Epoch 46/1000
2023-09-30 06:59:13.416 
Epoch 46/1000 
	 loss: 31.8191, MinusLogProbMetric: 31.8191, val_loss: 32.5624, val_MinusLogProbMetric: 32.5624

Epoch 46: val_loss did not improve from 31.78335
196/196 - 58s - loss: 31.8191 - MinusLogProbMetric: 31.8191 - val_loss: 32.5624 - val_MinusLogProbMetric: 32.5624 - lr: 0.0010 - 58s/epoch - 297ms/step
Epoch 47/1000
2023-09-30 07:00:09.407 
Epoch 47/1000 
	 loss: 31.8825, MinusLogProbMetric: 31.8825, val_loss: 32.6843, val_MinusLogProbMetric: 32.6843

Epoch 47: val_loss did not improve from 31.78335
196/196 - 56s - loss: 31.8825 - MinusLogProbMetric: 31.8825 - val_loss: 32.6843 - val_MinusLogProbMetric: 32.6843 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 48/1000
2023-09-30 07:01:04.212 
Epoch 48/1000 
	 loss: 31.6754, MinusLogProbMetric: 31.6754, val_loss: 32.1740, val_MinusLogProbMetric: 32.1740

Epoch 48: val_loss did not improve from 31.78335
196/196 - 55s - loss: 31.6754 - MinusLogProbMetric: 31.6754 - val_loss: 32.1740 - val_MinusLogProbMetric: 32.1740 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 49/1000
2023-09-30 07:02:03.862 
Epoch 49/1000 
	 loss: 31.6354, MinusLogProbMetric: 31.6354, val_loss: 32.0750, val_MinusLogProbMetric: 32.0750

Epoch 49: val_loss did not improve from 31.78335
196/196 - 60s - loss: 31.6354 - MinusLogProbMetric: 31.6354 - val_loss: 32.0750 - val_MinusLogProbMetric: 32.0750 - lr: 0.0010 - 60s/epoch - 304ms/step
Epoch 50/1000
2023-09-30 07:03:01.830 
Epoch 50/1000 
	 loss: 31.5833, MinusLogProbMetric: 31.5833, val_loss: 31.2258, val_MinusLogProbMetric: 31.2258

Epoch 50: val_loss improved from 31.78335 to 31.22577, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 58s - loss: 31.5833 - MinusLogProbMetric: 31.5833 - val_loss: 31.2258 - val_MinusLogProbMetric: 31.2258 - lr: 0.0010 - 58s/epoch - 297ms/step
Epoch 51/1000
2023-09-30 07:03:59.304 
Epoch 51/1000 
	 loss: 31.4375, MinusLogProbMetric: 31.4375, val_loss: 31.5931, val_MinusLogProbMetric: 31.5931

Epoch 51: val_loss did not improve from 31.22577
196/196 - 57s - loss: 31.4375 - MinusLogProbMetric: 31.4375 - val_loss: 31.5931 - val_MinusLogProbMetric: 31.5931 - lr: 0.0010 - 57s/epoch - 292ms/step
Epoch 52/1000
2023-09-30 07:04:57.951 
Epoch 52/1000 
	 loss: 31.2767, MinusLogProbMetric: 31.2767, val_loss: 30.8969, val_MinusLogProbMetric: 30.8969

Epoch 52: val_loss improved from 31.22577 to 30.89693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 60s - loss: 31.2767 - MinusLogProbMetric: 31.2767 - val_loss: 30.8969 - val_MinusLogProbMetric: 30.8969 - lr: 0.0010 - 60s/epoch - 305ms/step
Epoch 53/1000
2023-09-30 07:05:57.060 
Epoch 53/1000 
	 loss: 31.4513, MinusLogProbMetric: 31.4513, val_loss: 32.4645, val_MinusLogProbMetric: 32.4645

Epoch 53: val_loss did not improve from 30.89693
196/196 - 58s - loss: 31.4513 - MinusLogProbMetric: 31.4513 - val_loss: 32.4645 - val_MinusLogProbMetric: 32.4645 - lr: 0.0010 - 58s/epoch - 296ms/step
Epoch 54/1000
2023-09-30 07:06:55.166 
Epoch 54/1000 
	 loss: 31.2411, MinusLogProbMetric: 31.2411, val_loss: 31.4123, val_MinusLogProbMetric: 31.4123

Epoch 54: val_loss did not improve from 30.89693
196/196 - 58s - loss: 31.2411 - MinusLogProbMetric: 31.2411 - val_loss: 31.4123 - val_MinusLogProbMetric: 31.4123 - lr: 0.0010 - 58s/epoch - 296ms/step
Epoch 55/1000
2023-09-30 07:07:52.898 
Epoch 55/1000 
	 loss: 31.2026, MinusLogProbMetric: 31.2026, val_loss: 31.7348, val_MinusLogProbMetric: 31.7348

Epoch 55: val_loss did not improve from 30.89693
196/196 - 58s - loss: 31.2026 - MinusLogProbMetric: 31.2026 - val_loss: 31.7348 - val_MinusLogProbMetric: 31.7348 - lr: 0.0010 - 58s/epoch - 295ms/step
Epoch 56/1000
2023-09-30 07:08:51.698 
Epoch 56/1000 
	 loss: 31.2674, MinusLogProbMetric: 31.2674, val_loss: 31.3835, val_MinusLogProbMetric: 31.3835

Epoch 56: val_loss did not improve from 30.89693
196/196 - 59s - loss: 31.2674 - MinusLogProbMetric: 31.2674 - val_loss: 31.3835 - val_MinusLogProbMetric: 31.3835 - lr: 0.0010 - 59s/epoch - 300ms/step
Epoch 57/1000
2023-09-30 07:09:48.283 
Epoch 57/1000 
	 loss: 31.1327, MinusLogProbMetric: 31.1327, val_loss: 31.7383, val_MinusLogProbMetric: 31.7383

Epoch 57: val_loss did not improve from 30.89693
196/196 - 57s - loss: 31.1327 - MinusLogProbMetric: 31.1327 - val_loss: 31.7383 - val_MinusLogProbMetric: 31.7383 - lr: 0.0010 - 57s/epoch - 289ms/step
Epoch 58/1000
2023-09-30 07:10:43.952 
Epoch 58/1000 
	 loss: 31.0745, MinusLogProbMetric: 31.0745, val_loss: 33.3622, val_MinusLogProbMetric: 33.3622

Epoch 58: val_loss did not improve from 30.89693
196/196 - 56s - loss: 31.0745 - MinusLogProbMetric: 31.0745 - val_loss: 33.3622 - val_MinusLogProbMetric: 33.3622 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 59/1000
2023-09-30 07:11:40.425 
Epoch 59/1000 
	 loss: 31.0034, MinusLogProbMetric: 31.0034, val_loss: 31.5453, val_MinusLogProbMetric: 31.5453

Epoch 59: val_loss did not improve from 30.89693
196/196 - 57s - loss: 31.0034 - MinusLogProbMetric: 31.0034 - val_loss: 31.5453 - val_MinusLogProbMetric: 31.5453 - lr: 0.0010 - 57s/epoch - 288ms/step
Epoch 60/1000
2023-09-30 07:12:37.712 
Epoch 60/1000 
	 loss: 30.9889, MinusLogProbMetric: 30.9889, val_loss: 31.7341, val_MinusLogProbMetric: 31.7341

Epoch 60: val_loss did not improve from 30.89693
196/196 - 57s - loss: 30.9889 - MinusLogProbMetric: 30.9889 - val_loss: 31.7341 - val_MinusLogProbMetric: 31.7341 - lr: 0.0010 - 57s/epoch - 292ms/step
Epoch 61/1000
2023-09-30 07:13:35.584 
Epoch 61/1000 
	 loss: 31.0460, MinusLogProbMetric: 31.0460, val_loss: 30.5625, val_MinusLogProbMetric: 30.5625

Epoch 61: val_loss improved from 30.89693 to 30.56252, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 59s - loss: 31.0460 - MinusLogProbMetric: 31.0460 - val_loss: 30.5625 - val_MinusLogProbMetric: 30.5625 - lr: 0.0010 - 59s/epoch - 301ms/step
Epoch 62/1000
2023-09-30 07:14:31.670 
Epoch 62/1000 
	 loss: 31.0547, MinusLogProbMetric: 31.0547, val_loss: 30.6200, val_MinusLogProbMetric: 30.6200

Epoch 62: val_loss did not improve from 30.56252
196/196 - 55s - loss: 31.0547 - MinusLogProbMetric: 31.0547 - val_loss: 30.6200 - val_MinusLogProbMetric: 30.6200 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 63/1000
2023-09-30 07:15:31.133 
Epoch 63/1000 
	 loss: 30.7444, MinusLogProbMetric: 30.7444, val_loss: 30.9537, val_MinusLogProbMetric: 30.9537

Epoch 63: val_loss did not improve from 30.56252
196/196 - 59s - loss: 30.7444 - MinusLogProbMetric: 30.7444 - val_loss: 30.9537 - val_MinusLogProbMetric: 30.9537 - lr: 0.0010 - 59s/epoch - 303ms/step
Epoch 64/1000
2023-09-30 07:16:29.585 
Epoch 64/1000 
	 loss: 30.8087, MinusLogProbMetric: 30.8087, val_loss: 31.0909, val_MinusLogProbMetric: 31.0909

Epoch 64: val_loss did not improve from 30.56252
196/196 - 58s - loss: 30.8087 - MinusLogProbMetric: 30.8087 - val_loss: 31.0909 - val_MinusLogProbMetric: 31.0909 - lr: 0.0010 - 58s/epoch - 298ms/step
Epoch 65/1000
2023-09-30 07:17:26.793 
Epoch 65/1000 
	 loss: 30.9688, MinusLogProbMetric: 30.9688, val_loss: 30.6639, val_MinusLogProbMetric: 30.6639

Epoch 65: val_loss did not improve from 30.56252
196/196 - 57s - loss: 30.9688 - MinusLogProbMetric: 30.9688 - val_loss: 30.6639 - val_MinusLogProbMetric: 30.6639 - lr: 0.0010 - 57s/epoch - 292ms/step
Epoch 66/1000
2023-09-30 07:18:24.084 
Epoch 66/1000 
	 loss: 30.7244, MinusLogProbMetric: 30.7244, val_loss: 30.3336, val_MinusLogProbMetric: 30.3336

Epoch 66: val_loss improved from 30.56252 to 30.33357, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 59s - loss: 30.7244 - MinusLogProbMetric: 30.7244 - val_loss: 30.3336 - val_MinusLogProbMetric: 30.3336 - lr: 0.0010 - 59s/epoch - 300ms/step
Epoch 67/1000
2023-09-30 07:19:23.966 
Epoch 67/1000 
	 loss: 30.7805, MinusLogProbMetric: 30.7805, val_loss: 31.8897, val_MinusLogProbMetric: 31.8897

Epoch 67: val_loss did not improve from 30.33357
196/196 - 58s - loss: 30.7805 - MinusLogProbMetric: 30.7805 - val_loss: 31.8897 - val_MinusLogProbMetric: 31.8897 - lr: 0.0010 - 58s/epoch - 298ms/step
Epoch 68/1000
2023-09-30 07:20:21.323 
Epoch 68/1000 
	 loss: 30.8674, MinusLogProbMetric: 30.8674, val_loss: 30.2331, val_MinusLogProbMetric: 30.2331

Epoch 68: val_loss improved from 30.33357 to 30.23308, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 59s - loss: 30.8674 - MinusLogProbMetric: 30.8674 - val_loss: 30.2331 - val_MinusLogProbMetric: 30.2331 - lr: 0.0010 - 59s/epoch - 299ms/step
Epoch 69/1000
2023-09-30 07:21:20.739 
Epoch 69/1000 
	 loss: 30.6130, MinusLogProbMetric: 30.6130, val_loss: 30.3728, val_MinusLogProbMetric: 30.3728

Epoch 69: val_loss did not improve from 30.23308
196/196 - 58s - loss: 30.6130 - MinusLogProbMetric: 30.6130 - val_loss: 30.3728 - val_MinusLogProbMetric: 30.3728 - lr: 0.0010 - 58s/epoch - 296ms/step
Epoch 70/1000
2023-09-30 07:22:17.488 
Epoch 70/1000 
	 loss: 30.6587, MinusLogProbMetric: 30.6587, val_loss: 32.4618, val_MinusLogProbMetric: 32.4618

Epoch 70: val_loss did not improve from 30.23308
196/196 - 57s - loss: 30.6587 - MinusLogProbMetric: 30.6587 - val_loss: 32.4618 - val_MinusLogProbMetric: 32.4618 - lr: 0.0010 - 57s/epoch - 289ms/step
Epoch 71/1000
2023-09-30 07:23:16.388 
Epoch 71/1000 
	 loss: 30.5644, MinusLogProbMetric: 30.5644, val_loss: 30.8584, val_MinusLogProbMetric: 30.8584

Epoch 71: val_loss did not improve from 30.23308
196/196 - 59s - loss: 30.5644 - MinusLogProbMetric: 30.5644 - val_loss: 30.8584 - val_MinusLogProbMetric: 30.8584 - lr: 0.0010 - 59s/epoch - 300ms/step
Epoch 72/1000
2023-09-30 07:24:12.776 
Epoch 72/1000 
	 loss: 30.5638, MinusLogProbMetric: 30.5638, val_loss: 31.5636, val_MinusLogProbMetric: 31.5636

Epoch 72: val_loss did not improve from 30.23308
196/196 - 56s - loss: 30.5638 - MinusLogProbMetric: 30.5638 - val_loss: 31.5636 - val_MinusLogProbMetric: 31.5636 - lr: 0.0010 - 56s/epoch - 288ms/step
Epoch 73/1000
2023-09-30 07:25:09.736 
Epoch 73/1000 
	 loss: 30.5134, MinusLogProbMetric: 30.5134, val_loss: 30.7798, val_MinusLogProbMetric: 30.7798

Epoch 73: val_loss did not improve from 30.23308
196/196 - 57s - loss: 30.5134 - MinusLogProbMetric: 30.5134 - val_loss: 30.7798 - val_MinusLogProbMetric: 30.7798 - lr: 0.0010 - 57s/epoch - 290ms/step
Epoch 74/1000
2023-09-30 07:26:08.786 
Epoch 74/1000 
	 loss: 30.4901, MinusLogProbMetric: 30.4901, val_loss: 30.4901, val_MinusLogProbMetric: 30.4901

Epoch 74: val_loss did not improve from 30.23308
196/196 - 59s - loss: 30.4901 - MinusLogProbMetric: 30.4901 - val_loss: 30.4901 - val_MinusLogProbMetric: 30.4901 - lr: 0.0010 - 59s/epoch - 301ms/step
Epoch 75/1000
2023-09-30 07:27:07.353 
Epoch 75/1000 
	 loss: 30.4020, MinusLogProbMetric: 30.4020, val_loss: 34.4500, val_MinusLogProbMetric: 34.4500

Epoch 75: val_loss did not improve from 30.23308
196/196 - 59s - loss: 30.4020 - MinusLogProbMetric: 30.4020 - val_loss: 34.4500 - val_MinusLogProbMetric: 34.4500 - lr: 0.0010 - 59s/epoch - 299ms/step
Epoch 76/1000
2023-09-30 07:28:06.046 
Epoch 76/1000 
	 loss: 30.6484, MinusLogProbMetric: 30.6484, val_loss: 30.1661, val_MinusLogProbMetric: 30.1661

Epoch 76: val_loss improved from 30.23308 to 30.16614, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 60s - loss: 30.6484 - MinusLogProbMetric: 30.6484 - val_loss: 30.1661 - val_MinusLogProbMetric: 30.1661 - lr: 0.0010 - 60s/epoch - 309ms/step
Epoch 77/1000
2023-09-30 07:29:07.996 
Epoch 77/1000 
	 loss: 30.3712, MinusLogProbMetric: 30.3712, val_loss: 31.7012, val_MinusLogProbMetric: 31.7012

Epoch 77: val_loss did not improve from 30.16614
196/196 - 60s - loss: 30.3712 - MinusLogProbMetric: 30.3712 - val_loss: 31.7012 - val_MinusLogProbMetric: 31.7012 - lr: 0.0010 - 60s/epoch - 307ms/step
Epoch 78/1000
2023-09-30 07:30:06.249 
Epoch 78/1000 
	 loss: 30.3356, MinusLogProbMetric: 30.3356, val_loss: 30.1707, val_MinusLogProbMetric: 30.1707

Epoch 78: val_loss did not improve from 30.16614
196/196 - 58s - loss: 30.3356 - MinusLogProbMetric: 30.3356 - val_loss: 30.1707 - val_MinusLogProbMetric: 30.1707 - lr: 0.0010 - 58s/epoch - 297ms/step
Epoch 79/1000
2023-09-30 07:31:05.276 
Epoch 79/1000 
	 loss: 30.3714, MinusLogProbMetric: 30.3714, val_loss: 31.2153, val_MinusLogProbMetric: 31.2153

Epoch 79: val_loss did not improve from 30.16614
196/196 - 59s - loss: 30.3714 - MinusLogProbMetric: 30.3714 - val_loss: 31.2153 - val_MinusLogProbMetric: 31.2153 - lr: 0.0010 - 59s/epoch - 301ms/step
Epoch 80/1000
2023-09-30 07:32:05.970 
Epoch 80/1000 
	 loss: 30.4471, MinusLogProbMetric: 30.4471, val_loss: 30.4862, val_MinusLogProbMetric: 30.4862

Epoch 80: val_loss did not improve from 30.16614
196/196 - 61s - loss: 30.4471 - MinusLogProbMetric: 30.4471 - val_loss: 30.4862 - val_MinusLogProbMetric: 30.4862 - lr: 0.0010 - 61s/epoch - 309ms/step
Epoch 81/1000
2023-09-30 07:33:05.672 
Epoch 81/1000 
	 loss: 30.2416, MinusLogProbMetric: 30.2416, val_loss: 30.4351, val_MinusLogProbMetric: 30.4351

Epoch 81: val_loss did not improve from 30.16614
196/196 - 60s - loss: 30.2416 - MinusLogProbMetric: 30.2416 - val_loss: 30.4351 - val_MinusLogProbMetric: 30.4351 - lr: 0.0010 - 60s/epoch - 304ms/step
Epoch 82/1000
2023-09-30 07:34:03.975 
Epoch 82/1000 
	 loss: 30.2993, MinusLogProbMetric: 30.2993, val_loss: 30.7629, val_MinusLogProbMetric: 30.7629

Epoch 82: val_loss did not improve from 30.16614
196/196 - 58s - loss: 30.2993 - MinusLogProbMetric: 30.2993 - val_loss: 30.7629 - val_MinusLogProbMetric: 30.7629 - lr: 0.0010 - 58s/epoch - 298ms/step
Epoch 83/1000
2023-09-30 07:35:01.441 
Epoch 83/1000 
	 loss: 30.3604, MinusLogProbMetric: 30.3604, val_loss: 31.1180, val_MinusLogProbMetric: 31.1180

Epoch 83: val_loss did not improve from 30.16614
196/196 - 57s - loss: 30.3604 - MinusLogProbMetric: 30.3604 - val_loss: 31.1180 - val_MinusLogProbMetric: 31.1180 - lr: 0.0010 - 57s/epoch - 293ms/step
Epoch 84/1000
2023-09-30 07:35:59.297 
Epoch 84/1000 
	 loss: 30.3249, MinusLogProbMetric: 30.3249, val_loss: 30.9136, val_MinusLogProbMetric: 30.9136

Epoch 84: val_loss did not improve from 30.16614
196/196 - 58s - loss: 30.3249 - MinusLogProbMetric: 30.3249 - val_loss: 30.9136 - val_MinusLogProbMetric: 30.9136 - lr: 0.0010 - 58s/epoch - 295ms/step
Epoch 85/1000
2023-09-30 07:36:56.345 
Epoch 85/1000 
	 loss: 30.2833, MinusLogProbMetric: 30.2833, val_loss: 30.2916, val_MinusLogProbMetric: 30.2916

Epoch 85: val_loss did not improve from 30.16614
196/196 - 57s - loss: 30.2833 - MinusLogProbMetric: 30.2833 - val_loss: 30.2916 - val_MinusLogProbMetric: 30.2916 - lr: 0.0010 - 57s/epoch - 291ms/step
Epoch 86/1000
2023-09-30 07:37:54.800 
Epoch 86/1000 
	 loss: 30.1400, MinusLogProbMetric: 30.1400, val_loss: 33.2515, val_MinusLogProbMetric: 33.2515

Epoch 86: val_loss did not improve from 30.16614
196/196 - 58s - loss: 30.1400 - MinusLogProbMetric: 30.1400 - val_loss: 33.2515 - val_MinusLogProbMetric: 33.2515 - lr: 0.0010 - 58s/epoch - 298ms/step
Epoch 87/1000
2023-09-30 07:38:53.573 
Epoch 87/1000 
	 loss: 30.3031, MinusLogProbMetric: 30.3031, val_loss: 30.0877, val_MinusLogProbMetric: 30.0877

Epoch 87: val_loss improved from 30.16614 to 30.08772, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 61s - loss: 30.3031 - MinusLogProbMetric: 30.3031 - val_loss: 30.0877 - val_MinusLogProbMetric: 30.0877 - lr: 0.0010 - 61s/epoch - 312ms/step
Epoch 88/1000
2023-09-30 07:39:55.209 
Epoch 88/1000 
	 loss: 30.0980, MinusLogProbMetric: 30.0980, val_loss: 31.0216, val_MinusLogProbMetric: 31.0216

Epoch 88: val_loss did not improve from 30.08772
196/196 - 59s - loss: 30.0980 - MinusLogProbMetric: 30.0980 - val_loss: 31.0216 - val_MinusLogProbMetric: 31.0216 - lr: 0.0010 - 59s/epoch - 302ms/step
Epoch 89/1000
2023-09-30 07:40:52.465 
Epoch 89/1000 
	 loss: 30.1437, MinusLogProbMetric: 30.1437, val_loss: 30.2504, val_MinusLogProbMetric: 30.2504

Epoch 89: val_loss did not improve from 30.08772
196/196 - 57s - loss: 30.1437 - MinusLogProbMetric: 30.1437 - val_loss: 30.2504 - val_MinusLogProbMetric: 30.2504 - lr: 0.0010 - 57s/epoch - 292ms/step
Epoch 90/1000
2023-09-30 07:41:51.790 
Epoch 90/1000 
	 loss: 30.1020, MinusLogProbMetric: 30.1020, val_loss: 30.5365, val_MinusLogProbMetric: 30.5365

Epoch 90: val_loss did not improve from 30.08772
196/196 - 59s - loss: 30.1020 - MinusLogProbMetric: 30.1020 - val_loss: 30.5365 - val_MinusLogProbMetric: 30.5365 - lr: 0.0010 - 59s/epoch - 303ms/step
Epoch 91/1000
2023-09-30 07:42:50.731 
Epoch 91/1000 
	 loss: 30.1117, MinusLogProbMetric: 30.1117, val_loss: 30.2229, val_MinusLogProbMetric: 30.2229

Epoch 91: val_loss did not improve from 30.08772
196/196 - 59s - loss: 30.1117 - MinusLogProbMetric: 30.1117 - val_loss: 30.2229 - val_MinusLogProbMetric: 30.2229 - lr: 0.0010 - 59s/epoch - 301ms/step
Epoch 92/1000
2023-09-30 07:43:50.072 
Epoch 92/1000 
	 loss: 29.9308, MinusLogProbMetric: 29.9308, val_loss: 30.2019, val_MinusLogProbMetric: 30.2019

Epoch 92: val_loss did not improve from 30.08772
196/196 - 59s - loss: 29.9308 - MinusLogProbMetric: 29.9308 - val_loss: 30.2019 - val_MinusLogProbMetric: 30.2019 - lr: 0.0010 - 59s/epoch - 303ms/step
Epoch 93/1000
2023-09-30 07:44:48.959 
Epoch 93/1000 
	 loss: 30.0500, MinusLogProbMetric: 30.0500, val_loss: 32.6089, val_MinusLogProbMetric: 32.6089

Epoch 93: val_loss did not improve from 30.08772
196/196 - 59s - loss: 30.0500 - MinusLogProbMetric: 30.0500 - val_loss: 32.6089 - val_MinusLogProbMetric: 32.6089 - lr: 0.0010 - 59s/epoch - 300ms/step
Epoch 94/1000
2023-09-30 07:45:47.332 
Epoch 94/1000 
	 loss: 30.0069, MinusLogProbMetric: 30.0069, val_loss: 30.1605, val_MinusLogProbMetric: 30.1605

Epoch 94: val_loss did not improve from 30.08772
196/196 - 58s - loss: 30.0069 - MinusLogProbMetric: 30.0069 - val_loss: 30.1605 - val_MinusLogProbMetric: 30.1605 - lr: 0.0010 - 58s/epoch - 298ms/step
Epoch 95/1000
2023-09-30 07:46:44.505 
Epoch 95/1000 
	 loss: 29.9547, MinusLogProbMetric: 29.9547, val_loss: 31.6609, val_MinusLogProbMetric: 31.6609

Epoch 95: val_loss did not improve from 30.08772
196/196 - 57s - loss: 29.9547 - MinusLogProbMetric: 29.9547 - val_loss: 31.6609 - val_MinusLogProbMetric: 31.6609 - lr: 0.0010 - 57s/epoch - 292ms/step
Epoch 96/1000
2023-09-30 07:47:43.382 
Epoch 96/1000 
	 loss: 29.9403, MinusLogProbMetric: 29.9403, val_loss: 29.8376, val_MinusLogProbMetric: 29.8376

Epoch 96: val_loss improved from 30.08772 to 29.83760, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 60s - loss: 29.9403 - MinusLogProbMetric: 29.9403 - val_loss: 29.8376 - val_MinusLogProbMetric: 29.8376 - lr: 0.0010 - 60s/epoch - 305ms/step
Epoch 97/1000
2023-09-30 07:48:41.462 
Epoch 97/1000 
	 loss: 29.9136, MinusLogProbMetric: 29.9136, val_loss: 29.7801, val_MinusLogProbMetric: 29.7801

Epoch 97: val_loss improved from 29.83760 to 29.78009, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 58s - loss: 29.9136 - MinusLogProbMetric: 29.9136 - val_loss: 29.7801 - val_MinusLogProbMetric: 29.7801 - lr: 0.0010 - 58s/epoch - 296ms/step
Epoch 98/1000
2023-09-30 07:49:43.420 
Epoch 98/1000 
	 loss: 29.8949, MinusLogProbMetric: 29.8949, val_loss: 29.8016, val_MinusLogProbMetric: 29.8016

Epoch 98: val_loss did not improve from 29.78009
196/196 - 61s - loss: 29.8949 - MinusLogProbMetric: 29.8949 - val_loss: 29.8016 - val_MinusLogProbMetric: 29.8016 - lr: 0.0010 - 61s/epoch - 311ms/step
Epoch 99/1000
2023-09-30 07:50:41.220 
Epoch 99/1000 
	 loss: 29.7787, MinusLogProbMetric: 29.7787, val_loss: 30.6541, val_MinusLogProbMetric: 30.6541

Epoch 99: val_loss did not improve from 29.78009
196/196 - 58s - loss: 29.7787 - MinusLogProbMetric: 29.7787 - val_loss: 30.6541 - val_MinusLogProbMetric: 30.6541 - lr: 0.0010 - 58s/epoch - 295ms/step
Epoch 100/1000
2023-09-30 07:51:40.899 
Epoch 100/1000 
	 loss: 29.9100, MinusLogProbMetric: 29.9100, val_loss: 30.2949, val_MinusLogProbMetric: 30.2949

Epoch 100: val_loss did not improve from 29.78009
196/196 - 60s - loss: 29.9100 - MinusLogProbMetric: 29.9100 - val_loss: 30.2949 - val_MinusLogProbMetric: 30.2949 - lr: 0.0010 - 60s/epoch - 304ms/step
Epoch 101/1000
2023-09-30 07:52:39.957 
Epoch 101/1000 
	 loss: 29.9593, MinusLogProbMetric: 29.9593, val_loss: 30.2309, val_MinusLogProbMetric: 30.2309

Epoch 101: val_loss did not improve from 29.78009
196/196 - 59s - loss: 29.9593 - MinusLogProbMetric: 29.9593 - val_loss: 30.2309 - val_MinusLogProbMetric: 30.2309 - lr: 0.0010 - 59s/epoch - 301ms/step
Epoch 102/1000
2023-09-30 07:53:34.609 
Epoch 102/1000 
	 loss: 29.7365, MinusLogProbMetric: 29.7365, val_loss: 29.9900, val_MinusLogProbMetric: 29.9900

Epoch 102: val_loss did not improve from 29.78009
196/196 - 55s - loss: 29.7365 - MinusLogProbMetric: 29.7365 - val_loss: 29.9900 - val_MinusLogProbMetric: 29.9900 - lr: 0.0010 - 55s/epoch - 279ms/step
Epoch 103/1000
2023-09-30 07:54:31.437 
Epoch 103/1000 
	 loss: 29.7911, MinusLogProbMetric: 29.7911, val_loss: 30.5069, val_MinusLogProbMetric: 30.5069

Epoch 103: val_loss did not improve from 29.78009
196/196 - 57s - loss: 29.7911 - MinusLogProbMetric: 29.7911 - val_loss: 30.5069 - val_MinusLogProbMetric: 30.5069 - lr: 0.0010 - 57s/epoch - 290ms/step
Epoch 104/1000
2023-09-30 07:55:28.302 
Epoch 104/1000 
	 loss: 29.9010, MinusLogProbMetric: 29.9010, val_loss: 31.0635, val_MinusLogProbMetric: 31.0635

Epoch 104: val_loss did not improve from 29.78009
196/196 - 57s - loss: 29.9010 - MinusLogProbMetric: 29.9010 - val_loss: 31.0635 - val_MinusLogProbMetric: 31.0635 - lr: 0.0010 - 57s/epoch - 290ms/step
Epoch 105/1000
2023-09-30 07:56:24.353 
Epoch 105/1000 
	 loss: 29.8289, MinusLogProbMetric: 29.8289, val_loss: 30.0797, val_MinusLogProbMetric: 30.0797

Epoch 105: val_loss did not improve from 29.78009
196/196 - 56s - loss: 29.8289 - MinusLogProbMetric: 29.8289 - val_loss: 30.0797 - val_MinusLogProbMetric: 30.0797 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 106/1000
2023-09-30 07:57:21.978 
Epoch 106/1000 
	 loss: 29.9885, MinusLogProbMetric: 29.9885, val_loss: 30.1284, val_MinusLogProbMetric: 30.1284

Epoch 106: val_loss did not improve from 29.78009
196/196 - 58s - loss: 29.9885 - MinusLogProbMetric: 29.9885 - val_loss: 30.1284 - val_MinusLogProbMetric: 30.1284 - lr: 0.0010 - 58s/epoch - 294ms/step
Epoch 107/1000
2023-09-30 07:58:17.766 
Epoch 107/1000 
	 loss: 29.7192, MinusLogProbMetric: 29.7192, val_loss: 30.4545, val_MinusLogProbMetric: 30.4545

Epoch 107: val_loss did not improve from 29.78009
196/196 - 56s - loss: 29.7192 - MinusLogProbMetric: 29.7192 - val_loss: 30.4545 - val_MinusLogProbMetric: 30.4545 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 108/1000
2023-09-30 07:59:13.302 
Epoch 108/1000 
	 loss: 29.6835, MinusLogProbMetric: 29.6835, val_loss: 29.9107, val_MinusLogProbMetric: 29.9107

Epoch 108: val_loss did not improve from 29.78009
196/196 - 55s - loss: 29.6835 - MinusLogProbMetric: 29.6835 - val_loss: 29.9107 - val_MinusLogProbMetric: 29.9107 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 109/1000
2023-09-30 08:00:09.184 
Epoch 109/1000 
	 loss: 29.6712, MinusLogProbMetric: 29.6712, val_loss: 32.4378, val_MinusLogProbMetric: 32.4378

Epoch 109: val_loss did not improve from 29.78009
196/196 - 56s - loss: 29.6712 - MinusLogProbMetric: 29.6712 - val_loss: 32.4378 - val_MinusLogProbMetric: 32.4378 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 110/1000
2023-09-30 08:01:03.777 
Epoch 110/1000 
	 loss: 29.7246, MinusLogProbMetric: 29.7246, val_loss: 31.1058, val_MinusLogProbMetric: 31.1058

Epoch 110: val_loss did not improve from 29.78009
196/196 - 55s - loss: 29.7246 - MinusLogProbMetric: 29.7246 - val_loss: 31.1058 - val_MinusLogProbMetric: 31.1058 - lr: 0.0010 - 55s/epoch - 278ms/step
Epoch 111/1000
2023-09-30 08:01:58.824 
Epoch 111/1000 
	 loss: 29.6572, MinusLogProbMetric: 29.6572, val_loss: 29.3380, val_MinusLogProbMetric: 29.3380

Epoch 111: val_loss improved from 29.78009 to 29.33801, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 56s - loss: 29.6572 - MinusLogProbMetric: 29.6572 - val_loss: 29.3380 - val_MinusLogProbMetric: 29.3380 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 112/1000
2023-09-30 08:02:57.413 
Epoch 112/1000 
	 loss: 29.6625, MinusLogProbMetric: 29.6625, val_loss: 30.4411, val_MinusLogProbMetric: 30.4411

Epoch 112: val_loss did not improve from 29.33801
196/196 - 58s - loss: 29.6625 - MinusLogProbMetric: 29.6625 - val_loss: 30.4411 - val_MinusLogProbMetric: 30.4411 - lr: 0.0010 - 58s/epoch - 295ms/step
Epoch 113/1000
2023-09-30 08:03:53.160 
Epoch 113/1000 
	 loss: 29.6252, MinusLogProbMetric: 29.6252, val_loss: 30.3230, val_MinusLogProbMetric: 30.3230

Epoch 113: val_loss did not improve from 29.33801
196/196 - 56s - loss: 29.6252 - MinusLogProbMetric: 29.6252 - val_loss: 30.3230 - val_MinusLogProbMetric: 30.3230 - lr: 0.0010 - 56s/epoch - 284ms/step
Epoch 114/1000
2023-09-30 08:04:49.526 
Epoch 114/1000 
	 loss: 29.6494, MinusLogProbMetric: 29.6494, val_loss: 30.0378, val_MinusLogProbMetric: 30.0378

Epoch 114: val_loss did not improve from 29.33801
196/196 - 56s - loss: 29.6494 - MinusLogProbMetric: 29.6494 - val_loss: 30.0378 - val_MinusLogProbMetric: 30.0378 - lr: 0.0010 - 56s/epoch - 288ms/step
Epoch 115/1000
2023-09-30 08:05:45.682 
Epoch 115/1000 
	 loss: 29.6629, MinusLogProbMetric: 29.6629, val_loss: 30.5472, val_MinusLogProbMetric: 30.5472

Epoch 115: val_loss did not improve from 29.33801
196/196 - 56s - loss: 29.6629 - MinusLogProbMetric: 29.6629 - val_loss: 30.5472 - val_MinusLogProbMetric: 30.5472 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 116/1000
2023-09-30 08:06:41.686 
Epoch 116/1000 
	 loss: 29.5944, MinusLogProbMetric: 29.5944, val_loss: 30.9308, val_MinusLogProbMetric: 30.9308

Epoch 116: val_loss did not improve from 29.33801
196/196 - 56s - loss: 29.5944 - MinusLogProbMetric: 29.5944 - val_loss: 30.9308 - val_MinusLogProbMetric: 30.9308 - lr: 0.0010 - 56s/epoch - 286ms/step
Epoch 117/1000
2023-09-30 08:07:37.639 
Epoch 117/1000 
	 loss: 29.6262, MinusLogProbMetric: 29.6262, val_loss: 30.2072, val_MinusLogProbMetric: 30.2072

Epoch 117: val_loss did not improve from 29.33801
196/196 - 56s - loss: 29.6262 - MinusLogProbMetric: 29.6262 - val_loss: 30.2072 - val_MinusLogProbMetric: 30.2072 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 118/1000
2023-09-30 08:08:33.466 
Epoch 118/1000 
	 loss: 29.6169, MinusLogProbMetric: 29.6169, val_loss: 29.9273, val_MinusLogProbMetric: 29.9273

Epoch 118: val_loss did not improve from 29.33801
196/196 - 56s - loss: 29.6169 - MinusLogProbMetric: 29.6169 - val_loss: 29.9273 - val_MinusLogProbMetric: 29.9273 - lr: 0.0010 - 56s/epoch - 285ms/step
Epoch 119/1000
2023-09-30 08:09:28.524 
Epoch 119/1000 
	 loss: 29.6319, MinusLogProbMetric: 29.6319, val_loss: 30.8850, val_MinusLogProbMetric: 30.8850

Epoch 119: val_loss did not improve from 29.33801
196/196 - 55s - loss: 29.6319 - MinusLogProbMetric: 29.6319 - val_loss: 30.8850 - val_MinusLogProbMetric: 30.8850 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 120/1000
2023-09-30 08:10:23.452 
Epoch 120/1000 
	 loss: 29.6719, MinusLogProbMetric: 29.6719, val_loss: 29.5103, val_MinusLogProbMetric: 29.5103

Epoch 120: val_loss did not improve from 29.33801
196/196 - 55s - loss: 29.6719 - MinusLogProbMetric: 29.6719 - val_loss: 29.5103 - val_MinusLogProbMetric: 29.5103 - lr: 0.0010 - 55s/epoch - 280ms/step
Epoch 121/1000
2023-09-30 08:11:12.060 
Epoch 121/1000 
	 loss: 29.5261, MinusLogProbMetric: 29.5261, val_loss: 30.1125, val_MinusLogProbMetric: 30.1125

Epoch 121: val_loss did not improve from 29.33801
196/196 - 49s - loss: 29.5261 - MinusLogProbMetric: 29.5261 - val_loss: 30.1125 - val_MinusLogProbMetric: 30.1125 - lr: 0.0010 - 49s/epoch - 248ms/step
Epoch 122/1000
2023-09-30 08:12:07.374 
Epoch 122/1000 
	 loss: 29.5809, MinusLogProbMetric: 29.5809, val_loss: 30.0199, val_MinusLogProbMetric: 30.0199

Epoch 122: val_loss did not improve from 29.33801
196/196 - 55s - loss: 29.5809 - MinusLogProbMetric: 29.5809 - val_loss: 30.0199 - val_MinusLogProbMetric: 30.0199 - lr: 0.0010 - 55s/epoch - 282ms/step
Epoch 123/1000
2023-09-30 08:13:01.586 
Epoch 123/1000 
	 loss: 29.5645, MinusLogProbMetric: 29.5645, val_loss: 29.5544, val_MinusLogProbMetric: 29.5544

Epoch 123: val_loss did not improve from 29.33801
196/196 - 54s - loss: 29.5645 - MinusLogProbMetric: 29.5645 - val_loss: 29.5544 - val_MinusLogProbMetric: 29.5544 - lr: 0.0010 - 54s/epoch - 277ms/step
Epoch 124/1000
2023-09-30 08:13:56.720 
Epoch 124/1000 
	 loss: 29.4720, MinusLogProbMetric: 29.4720, val_loss: 29.3800, val_MinusLogProbMetric: 29.3800

Epoch 124: val_loss did not improve from 29.33801
196/196 - 55s - loss: 29.4720 - MinusLogProbMetric: 29.4720 - val_loss: 29.3800 - val_MinusLogProbMetric: 29.3800 - lr: 0.0010 - 55s/epoch - 281ms/step
Epoch 125/1000
2023-09-30 08:14:52.101 
Epoch 125/1000 
	 loss: 29.6779, MinusLogProbMetric: 29.6779, val_loss: 30.1795, val_MinusLogProbMetric: 30.1795

Epoch 125: val_loss did not improve from 29.33801
196/196 - 55s - loss: 29.6779 - MinusLogProbMetric: 29.6779 - val_loss: 30.1795 - val_MinusLogProbMetric: 30.1795 - lr: 0.0010 - 55s/epoch - 283ms/step
Epoch 126/1000
2023-09-30 08:15:50.212 
Epoch 126/1000 
	 loss: 29.4228, MinusLogProbMetric: 29.4228, val_loss: 29.5494, val_MinusLogProbMetric: 29.5494

Epoch 126: val_loss did not improve from 29.33801
196/196 - 58s - loss: 29.4228 - MinusLogProbMetric: 29.4228 - val_loss: 29.5494 - val_MinusLogProbMetric: 29.5494 - lr: 0.0010 - 58s/epoch - 296ms/step
Epoch 127/1000
2023-09-30 08:16:48.279 
Epoch 127/1000 
	 loss: 29.5803, MinusLogProbMetric: 29.5803, val_loss: 29.8498, val_MinusLogProbMetric: 29.8498

Epoch 127: val_loss did not improve from 29.33801
196/196 - 58s - loss: 29.5803 - MinusLogProbMetric: 29.5803 - val_loss: 29.8498 - val_MinusLogProbMetric: 29.8498 - lr: 0.0010 - 58s/epoch - 296ms/step
Epoch 128/1000
2023-09-30 08:17:49.821 
Epoch 128/1000 
	 loss: 29.4780, MinusLogProbMetric: 29.4780, val_loss: 29.7904, val_MinusLogProbMetric: 29.7904

Epoch 128: val_loss did not improve from 29.33801
196/196 - 62s - loss: 29.4780 - MinusLogProbMetric: 29.4780 - val_loss: 29.7904 - val_MinusLogProbMetric: 29.7904 - lr: 0.0010 - 62s/epoch - 314ms/step
Epoch 129/1000
2023-09-30 08:18:48.187 
Epoch 129/1000 
	 loss: 29.5877, MinusLogProbMetric: 29.5877, val_loss: 30.0089, val_MinusLogProbMetric: 30.0089

Epoch 129: val_loss did not improve from 29.33801
196/196 - 58s - loss: 29.5877 - MinusLogProbMetric: 29.5877 - val_loss: 30.0089 - val_MinusLogProbMetric: 30.0089 - lr: 0.0010 - 58s/epoch - 298ms/step
Epoch 130/1000
2023-09-30 08:19:48.418 
Epoch 130/1000 
	 loss: 29.4244, MinusLogProbMetric: 29.4244, val_loss: 29.3626, val_MinusLogProbMetric: 29.3626

Epoch 130: val_loss did not improve from 29.33801
196/196 - 60s - loss: 29.4244 - MinusLogProbMetric: 29.4244 - val_loss: 29.3626 - val_MinusLogProbMetric: 29.3626 - lr: 0.0010 - 60s/epoch - 307ms/step
Epoch 131/1000
2023-09-30 08:20:49.761 
Epoch 131/1000 
	 loss: 29.5898, MinusLogProbMetric: 29.5898, val_loss: 30.4943, val_MinusLogProbMetric: 30.4943

Epoch 131: val_loss did not improve from 29.33801
196/196 - 61s - loss: 29.5898 - MinusLogProbMetric: 29.5898 - val_loss: 30.4943 - val_MinusLogProbMetric: 30.4943 - lr: 0.0010 - 61s/epoch - 313ms/step
Epoch 132/1000
2023-09-30 08:21:49.108 
Epoch 132/1000 
	 loss: 29.5060, MinusLogProbMetric: 29.5060, val_loss: 29.7587, val_MinusLogProbMetric: 29.7587

Epoch 132: val_loss did not improve from 29.33801
196/196 - 59s - loss: 29.5060 - MinusLogProbMetric: 29.5060 - val_loss: 29.7587 - val_MinusLogProbMetric: 29.7587 - lr: 0.0010 - 59s/epoch - 303ms/step
Epoch 133/1000
2023-09-30 08:22:48.659 
Epoch 133/1000 
	 loss: 29.4237, MinusLogProbMetric: 29.4237, val_loss: 29.7487, val_MinusLogProbMetric: 29.7487

Epoch 133: val_loss did not improve from 29.33801
196/196 - 60s - loss: 29.4237 - MinusLogProbMetric: 29.4237 - val_loss: 29.7487 - val_MinusLogProbMetric: 29.7487 - lr: 0.0010 - 60s/epoch - 304ms/step
Epoch 134/1000
2023-09-30 08:23:46.722 
Epoch 134/1000 
	 loss: 29.3118, MinusLogProbMetric: 29.3118, val_loss: 29.2162, val_MinusLogProbMetric: 29.2162

Epoch 134: val_loss improved from 29.33801 to 29.21622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 60s - loss: 29.3118 - MinusLogProbMetric: 29.3118 - val_loss: 29.2162 - val_MinusLogProbMetric: 29.2162 - lr: 0.0010 - 60s/epoch - 305ms/step
Epoch 135/1000
2023-09-30 08:24:48.256 
Epoch 135/1000 
	 loss: 29.2886, MinusLogProbMetric: 29.2886, val_loss: 29.7026, val_MinusLogProbMetric: 29.7026

Epoch 135: val_loss did not improve from 29.21622
196/196 - 60s - loss: 29.2886 - MinusLogProbMetric: 29.2886 - val_loss: 29.7026 - val_MinusLogProbMetric: 29.7026 - lr: 0.0010 - 60s/epoch - 305ms/step
Epoch 136/1000
2023-09-30 08:25:47.176 
Epoch 136/1000 
	 loss: 29.3943, MinusLogProbMetric: 29.3943, val_loss: 29.7438, val_MinusLogProbMetric: 29.7438

Epoch 136: val_loss did not improve from 29.21622
196/196 - 59s - loss: 29.3943 - MinusLogProbMetric: 29.3943 - val_loss: 29.7438 - val_MinusLogProbMetric: 29.7438 - lr: 0.0010 - 59s/epoch - 301ms/step
Epoch 137/1000
2023-09-30 08:26:47.425 
Epoch 137/1000 
	 loss: 29.5059, MinusLogProbMetric: 29.5059, val_loss: 29.8616, val_MinusLogProbMetric: 29.8616

Epoch 137: val_loss did not improve from 29.21622
196/196 - 60s - loss: 29.5059 - MinusLogProbMetric: 29.5059 - val_loss: 29.8616 - val_MinusLogProbMetric: 29.8616 - lr: 0.0010 - 60s/epoch - 307ms/step
Epoch 138/1000
2023-09-30 08:27:44.634 
Epoch 138/1000 
	 loss: 29.3446, MinusLogProbMetric: 29.3446, val_loss: 30.0798, val_MinusLogProbMetric: 30.0798

Epoch 138: val_loss did not improve from 29.21622
196/196 - 57s - loss: 29.3446 - MinusLogProbMetric: 29.3446 - val_loss: 30.0798 - val_MinusLogProbMetric: 30.0798 - lr: 0.0010 - 57s/epoch - 292ms/step
Epoch 139/1000
2023-09-30 08:28:43.627 
Epoch 139/1000 
	 loss: 29.4517, MinusLogProbMetric: 29.4517, val_loss: 30.8769, val_MinusLogProbMetric: 30.8769

Epoch 139: val_loss did not improve from 29.21622
196/196 - 59s - loss: 29.4517 - MinusLogProbMetric: 29.4517 - val_loss: 30.8769 - val_MinusLogProbMetric: 30.8769 - lr: 0.0010 - 59s/epoch - 301ms/step
Epoch 140/1000
2023-09-30 08:29:43.570 
Epoch 140/1000 
	 loss: 29.4496, MinusLogProbMetric: 29.4496, val_loss: 29.8542, val_MinusLogProbMetric: 29.8542

Epoch 140: val_loss did not improve from 29.21622
196/196 - 60s - loss: 29.4496 - MinusLogProbMetric: 29.4496 - val_loss: 29.8542 - val_MinusLogProbMetric: 29.8542 - lr: 0.0010 - 60s/epoch - 306ms/step
Epoch 141/1000
2023-09-30 08:30:43.044 
Epoch 141/1000 
	 loss: 29.2464, MinusLogProbMetric: 29.2464, val_loss: 29.7384, val_MinusLogProbMetric: 29.7384

Epoch 141: val_loss did not improve from 29.21622
196/196 - 59s - loss: 29.2464 - MinusLogProbMetric: 29.2464 - val_loss: 29.7384 - val_MinusLogProbMetric: 29.7384 - lr: 0.0010 - 59s/epoch - 303ms/step
Epoch 142/1000
2023-09-30 08:31:41.789 
Epoch 142/1000 
	 loss: 29.2796, MinusLogProbMetric: 29.2796, val_loss: 30.4337, val_MinusLogProbMetric: 30.4337

Epoch 142: val_loss did not improve from 29.21622
196/196 - 59s - loss: 29.2796 - MinusLogProbMetric: 29.2796 - val_loss: 30.4337 - val_MinusLogProbMetric: 30.4337 - lr: 0.0010 - 59s/epoch - 300ms/step
Epoch 143/1000
2023-09-30 08:32:40.099 
Epoch 143/1000 
	 loss: 29.2733, MinusLogProbMetric: 29.2733, val_loss: 29.4560, val_MinusLogProbMetric: 29.4560

Epoch 143: val_loss did not improve from 29.21622
196/196 - 58s - loss: 29.2733 - MinusLogProbMetric: 29.2733 - val_loss: 29.4560 - val_MinusLogProbMetric: 29.4560 - lr: 0.0010 - 58s/epoch - 297ms/step
Epoch 144/1000
2023-09-30 08:33:38.617 
Epoch 144/1000 
	 loss: 29.3017, MinusLogProbMetric: 29.3017, val_loss: 29.3697, val_MinusLogProbMetric: 29.3697

Epoch 144: val_loss did not improve from 29.21622
196/196 - 59s - loss: 29.3017 - MinusLogProbMetric: 29.3017 - val_loss: 29.3697 - val_MinusLogProbMetric: 29.3697 - lr: 0.0010 - 59s/epoch - 299ms/step
Epoch 145/1000
2023-09-30 08:34:36.229 
Epoch 145/1000 
	 loss: 29.2571, MinusLogProbMetric: 29.2571, val_loss: 29.1944, val_MinusLogProbMetric: 29.1944

Epoch 145: val_loss improved from 29.21622 to 29.19443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 58s - loss: 29.2571 - MinusLogProbMetric: 29.2571 - val_loss: 29.1944 - val_MinusLogProbMetric: 29.1944 - lr: 0.0010 - 58s/epoch - 297ms/step
Epoch 146/1000
2023-09-30 08:35:35.534 
Epoch 146/1000 
	 loss: 29.2131, MinusLogProbMetric: 29.2131, val_loss: 29.6210, val_MinusLogProbMetric: 29.6210

Epoch 146: val_loss did not improve from 29.19443
196/196 - 59s - loss: 29.2131 - MinusLogProbMetric: 29.2131 - val_loss: 29.6210 - val_MinusLogProbMetric: 29.6210 - lr: 0.0010 - 59s/epoch - 300ms/step
Epoch 147/1000
2023-09-30 08:36:34.466 
Epoch 147/1000 
	 loss: 29.3505, MinusLogProbMetric: 29.3505, val_loss: 29.4112, val_MinusLogProbMetric: 29.4112

Epoch 147: val_loss did not improve from 29.19443
196/196 - 59s - loss: 29.3505 - MinusLogProbMetric: 29.3505 - val_loss: 29.4112 - val_MinusLogProbMetric: 29.4112 - lr: 0.0010 - 59s/epoch - 301ms/step
Epoch 148/1000
2023-09-30 08:37:34.038 
Epoch 148/1000 
	 loss: 29.2110, MinusLogProbMetric: 29.2110, val_loss: 29.1153, val_MinusLogProbMetric: 29.1153

Epoch 148: val_loss improved from 29.19443 to 29.11531, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 61s - loss: 29.2110 - MinusLogProbMetric: 29.2110 - val_loss: 29.1153 - val_MinusLogProbMetric: 29.1153 - lr: 0.0010 - 61s/epoch - 311ms/step
Epoch 149/1000
2023-09-30 08:38:50.351 
Epoch 149/1000 
	 loss: 29.2263, MinusLogProbMetric: 29.2263, val_loss: 29.7577, val_MinusLogProbMetric: 29.7577

Epoch 149: val_loss did not improve from 29.11531
196/196 - 75s - loss: 29.2263 - MinusLogProbMetric: 29.2263 - val_loss: 29.7577 - val_MinusLogProbMetric: 29.7577 - lr: 0.0010 - 75s/epoch - 382ms/step
Epoch 150/1000
2023-09-30 08:39:58.522 
Epoch 150/1000 
	 loss: 29.1730, MinusLogProbMetric: 29.1730, val_loss: 29.9645, val_MinusLogProbMetric: 29.9645

Epoch 150: val_loss did not improve from 29.11531
196/196 - 68s - loss: 29.1730 - MinusLogProbMetric: 29.1730 - val_loss: 29.9645 - val_MinusLogProbMetric: 29.9645 - lr: 0.0010 - 68s/epoch - 348ms/step
Epoch 151/1000
2023-09-30 08:41:12.049 
Epoch 151/1000 
	 loss: 29.1698, MinusLogProbMetric: 29.1698, val_loss: 29.7528, val_MinusLogProbMetric: 29.7528

Epoch 151: val_loss did not improve from 29.11531
196/196 - 74s - loss: 29.1698 - MinusLogProbMetric: 29.1698 - val_loss: 29.7528 - val_MinusLogProbMetric: 29.7528 - lr: 0.0010 - 74s/epoch - 375ms/step
Epoch 152/1000
2023-09-30 08:42:21.162 
Epoch 152/1000 
	 loss: 29.4700, MinusLogProbMetric: 29.4700, val_loss: 29.9909, val_MinusLogProbMetric: 29.9909

Epoch 152: val_loss did not improve from 29.11531
196/196 - 69s - loss: 29.4700 - MinusLogProbMetric: 29.4700 - val_loss: 29.9909 - val_MinusLogProbMetric: 29.9909 - lr: 0.0010 - 69s/epoch - 353ms/step
Epoch 153/1000
2023-09-30 08:43:34.134 
Epoch 153/1000 
	 loss: 29.2286, MinusLogProbMetric: 29.2286, val_loss: 29.9469, val_MinusLogProbMetric: 29.9469

Epoch 153: val_loss did not improve from 29.11531
196/196 - 73s - loss: 29.2286 - MinusLogProbMetric: 29.2286 - val_loss: 29.9469 - val_MinusLogProbMetric: 29.9469 - lr: 0.0010 - 73s/epoch - 372ms/step
Epoch 154/1000
2023-09-30 08:44:51.749 
Epoch 154/1000 
	 loss: 29.1707, MinusLogProbMetric: 29.1707, val_loss: 30.4555, val_MinusLogProbMetric: 30.4555

Epoch 154: val_loss did not improve from 29.11531
196/196 - 78s - loss: 29.1707 - MinusLogProbMetric: 29.1707 - val_loss: 30.4555 - val_MinusLogProbMetric: 30.4555 - lr: 0.0010 - 78s/epoch - 396ms/step
Epoch 155/1000
2023-09-30 08:46:05.444 
Epoch 155/1000 
	 loss: 29.1854, MinusLogProbMetric: 29.1854, val_loss: 29.2598, val_MinusLogProbMetric: 29.2598

Epoch 155: val_loss did not improve from 29.11531
196/196 - 74s - loss: 29.1854 - MinusLogProbMetric: 29.1854 - val_loss: 29.2598 - val_MinusLogProbMetric: 29.2598 - lr: 0.0010 - 74s/epoch - 376ms/step
Epoch 156/1000
2023-09-30 08:47:18.871 
Epoch 156/1000 
	 loss: 29.2681, MinusLogProbMetric: 29.2681, val_loss: 29.9014, val_MinusLogProbMetric: 29.9014

Epoch 156: val_loss did not improve from 29.11531
196/196 - 73s - loss: 29.2681 - MinusLogProbMetric: 29.2681 - val_loss: 29.9014 - val_MinusLogProbMetric: 29.9014 - lr: 0.0010 - 73s/epoch - 374ms/step
Epoch 157/1000
2023-09-30 08:48:33.113 
Epoch 157/1000 
	 loss: 29.0838, MinusLogProbMetric: 29.0838, val_loss: 30.0007, val_MinusLogProbMetric: 30.0007

Epoch 157: val_loss did not improve from 29.11531
196/196 - 74s - loss: 29.0838 - MinusLogProbMetric: 29.0838 - val_loss: 30.0007 - val_MinusLogProbMetric: 30.0007 - lr: 0.0010 - 74s/epoch - 379ms/step
Epoch 158/1000
2023-09-30 08:49:45.935 
Epoch 158/1000 
	 loss: 29.1850, MinusLogProbMetric: 29.1850, val_loss: 29.9139, val_MinusLogProbMetric: 29.9139

Epoch 158: val_loss did not improve from 29.11531
196/196 - 73s - loss: 29.1850 - MinusLogProbMetric: 29.1850 - val_loss: 29.9139 - val_MinusLogProbMetric: 29.9139 - lr: 0.0010 - 73s/epoch - 371ms/step
Epoch 159/1000
2023-09-30 08:51:00.923 
Epoch 159/1000 
	 loss: 29.1154, MinusLogProbMetric: 29.1154, val_loss: 29.0940, val_MinusLogProbMetric: 29.0940

Epoch 159: val_loss improved from 29.11531 to 29.09402, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 76s - loss: 29.1154 - MinusLogProbMetric: 29.1154 - val_loss: 29.0940 - val_MinusLogProbMetric: 29.0940 - lr: 0.0010 - 76s/epoch - 386ms/step
Epoch 160/1000
2023-09-30 08:52:11.813 
Epoch 160/1000 
	 loss: 29.1121, MinusLogProbMetric: 29.1121, val_loss: 29.4784, val_MinusLogProbMetric: 29.4784

Epoch 160: val_loss did not improve from 29.09402
196/196 - 70s - loss: 29.1121 - MinusLogProbMetric: 29.1121 - val_loss: 29.4784 - val_MinusLogProbMetric: 29.4784 - lr: 0.0010 - 70s/epoch - 358ms/step
Epoch 161/1000
2023-09-30 08:53:23.810 
Epoch 161/1000 
	 loss: 29.0936, MinusLogProbMetric: 29.0936, val_loss: 29.3956, val_MinusLogProbMetric: 29.3956

Epoch 161: val_loss did not improve from 29.09402
196/196 - 72s - loss: 29.0936 - MinusLogProbMetric: 29.0936 - val_loss: 29.3956 - val_MinusLogProbMetric: 29.3956 - lr: 0.0010 - 72s/epoch - 367ms/step
Epoch 162/1000
2023-09-30 08:54:35.067 
Epoch 162/1000 
	 loss: 29.0779, MinusLogProbMetric: 29.0779, val_loss: 29.6282, val_MinusLogProbMetric: 29.6282

Epoch 162: val_loss did not improve from 29.09402
196/196 - 71s - loss: 29.0779 - MinusLogProbMetric: 29.0779 - val_loss: 29.6282 - val_MinusLogProbMetric: 29.6282 - lr: 0.0010 - 71s/epoch - 363ms/step
Epoch 163/1000
2023-09-30 08:55:47.700 
Epoch 163/1000 
	 loss: 29.1537, MinusLogProbMetric: 29.1537, val_loss: 29.2181, val_MinusLogProbMetric: 29.2181

Epoch 163: val_loss did not improve from 29.09402
196/196 - 73s - loss: 29.1537 - MinusLogProbMetric: 29.1537 - val_loss: 29.2181 - val_MinusLogProbMetric: 29.2181 - lr: 0.0010 - 73s/epoch - 371ms/step
Epoch 164/1000
2023-09-30 08:56:57.912 
Epoch 164/1000 
	 loss: 29.0657, MinusLogProbMetric: 29.0657, val_loss: 29.5036, val_MinusLogProbMetric: 29.5036

Epoch 164: val_loss did not improve from 29.09402
196/196 - 70s - loss: 29.0657 - MinusLogProbMetric: 29.0657 - val_loss: 29.5036 - val_MinusLogProbMetric: 29.5036 - lr: 0.0010 - 70s/epoch - 358ms/step
Epoch 165/1000
2023-09-30 08:58:12.165 
Epoch 165/1000 
	 loss: 29.0257, MinusLogProbMetric: 29.0257, val_loss: 29.2760, val_MinusLogProbMetric: 29.2760

Epoch 165: val_loss did not improve from 29.09402
196/196 - 74s - loss: 29.0257 - MinusLogProbMetric: 29.0257 - val_loss: 29.2760 - val_MinusLogProbMetric: 29.2760 - lr: 0.0010 - 74s/epoch - 379ms/step
Epoch 166/1000
2023-09-30 08:59:27.208 
Epoch 166/1000 
	 loss: 29.2173, MinusLogProbMetric: 29.2173, val_loss: 30.0049, val_MinusLogProbMetric: 30.0049

Epoch 166: val_loss did not improve from 29.09402
196/196 - 75s - loss: 29.2173 - MinusLogProbMetric: 29.2173 - val_loss: 30.0049 - val_MinusLogProbMetric: 30.0049 - lr: 0.0010 - 75s/epoch - 383ms/step
Epoch 167/1000
2023-09-30 09:00:39.271 
Epoch 167/1000 
	 loss: 29.2123, MinusLogProbMetric: 29.2123, val_loss: 30.6508, val_MinusLogProbMetric: 30.6508

Epoch 167: val_loss did not improve from 29.09402
196/196 - 72s - loss: 29.2123 - MinusLogProbMetric: 29.2123 - val_loss: 30.6508 - val_MinusLogProbMetric: 30.6508 - lr: 0.0010 - 72s/epoch - 368ms/step
Epoch 168/1000
2023-09-30 09:01:51.983 
Epoch 168/1000 
	 loss: 29.0014, MinusLogProbMetric: 29.0014, val_loss: 29.6003, val_MinusLogProbMetric: 29.6003

Epoch 168: val_loss did not improve from 29.09402
196/196 - 73s - loss: 29.0014 - MinusLogProbMetric: 29.0014 - val_loss: 29.6003 - val_MinusLogProbMetric: 29.6003 - lr: 0.0010 - 73s/epoch - 371ms/step
Epoch 169/1000
2023-09-30 09:03:05.434 
Epoch 169/1000 
	 loss: 29.1583, MinusLogProbMetric: 29.1583, val_loss: 29.2008, val_MinusLogProbMetric: 29.2008

Epoch 169: val_loss did not improve from 29.09402
196/196 - 73s - loss: 29.1583 - MinusLogProbMetric: 29.1583 - val_loss: 29.2008 - val_MinusLogProbMetric: 29.2008 - lr: 0.0010 - 73s/epoch - 375ms/step
Epoch 170/1000
2023-09-30 09:04:16.997 
Epoch 170/1000 
	 loss: 28.9546, MinusLogProbMetric: 28.9546, val_loss: 29.4422, val_MinusLogProbMetric: 29.4422

Epoch 170: val_loss did not improve from 29.09402
196/196 - 72s - loss: 28.9546 - MinusLogProbMetric: 28.9546 - val_loss: 29.4422 - val_MinusLogProbMetric: 29.4422 - lr: 0.0010 - 72s/epoch - 365ms/step
Epoch 171/1000
2023-09-30 09:05:29.174 
Epoch 171/1000 
	 loss: 29.0949, MinusLogProbMetric: 29.0949, val_loss: 28.8313, val_MinusLogProbMetric: 28.8313

Epoch 171: val_loss improved from 29.09402 to 28.83129, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 73s - loss: 29.0949 - MinusLogProbMetric: 29.0949 - val_loss: 28.8313 - val_MinusLogProbMetric: 28.8313 - lr: 0.0010 - 73s/epoch - 373ms/step
Epoch 172/1000
2023-09-30 09:06:44.912 
Epoch 172/1000 
	 loss: 28.9475, MinusLogProbMetric: 28.9475, val_loss: 29.3686, val_MinusLogProbMetric: 29.3686

Epoch 172: val_loss did not improve from 28.83129
196/196 - 75s - loss: 28.9475 - MinusLogProbMetric: 28.9475 - val_loss: 29.3686 - val_MinusLogProbMetric: 29.3686 - lr: 0.0010 - 75s/epoch - 381ms/step
Epoch 173/1000
2023-09-30 09:07:56.212 
Epoch 173/1000 
	 loss: 28.9403, MinusLogProbMetric: 28.9403, val_loss: 29.8836, val_MinusLogProbMetric: 29.8836

Epoch 173: val_loss did not improve from 28.83129
196/196 - 71s - loss: 28.9403 - MinusLogProbMetric: 28.9403 - val_loss: 29.8836 - val_MinusLogProbMetric: 29.8836 - lr: 0.0010 - 71s/epoch - 364ms/step
Epoch 174/1000
2023-09-30 09:09:06.592 
Epoch 174/1000 
	 loss: 28.9834, MinusLogProbMetric: 28.9834, val_loss: 29.3343, val_MinusLogProbMetric: 29.3343

Epoch 174: val_loss did not improve from 28.83129
196/196 - 70s - loss: 28.9834 - MinusLogProbMetric: 28.9834 - val_loss: 29.3343 - val_MinusLogProbMetric: 29.3343 - lr: 0.0010 - 70s/epoch - 359ms/step
Epoch 175/1000
2023-09-30 09:10:22.403 
Epoch 175/1000 
	 loss: 28.9624, MinusLogProbMetric: 28.9624, val_loss: 29.1381, val_MinusLogProbMetric: 29.1381

Epoch 175: val_loss did not improve from 28.83129
196/196 - 76s - loss: 28.9624 - MinusLogProbMetric: 28.9624 - val_loss: 29.1381 - val_MinusLogProbMetric: 29.1381 - lr: 0.0010 - 76s/epoch - 387ms/step
Epoch 176/1000
2023-09-30 09:11:38.028 
Epoch 176/1000 
	 loss: 28.9832, MinusLogProbMetric: 28.9832, val_loss: 29.4914, val_MinusLogProbMetric: 29.4914

Epoch 176: val_loss did not improve from 28.83129
196/196 - 76s - loss: 28.9832 - MinusLogProbMetric: 28.9832 - val_loss: 29.4914 - val_MinusLogProbMetric: 29.4914 - lr: 0.0010 - 76s/epoch - 386ms/step
Epoch 177/1000
2023-09-30 09:12:52.955 
Epoch 177/1000 
	 loss: 29.0327, MinusLogProbMetric: 29.0327, val_loss: 29.3537, val_MinusLogProbMetric: 29.3537

Epoch 177: val_loss did not improve from 28.83129
196/196 - 75s - loss: 29.0327 - MinusLogProbMetric: 29.0327 - val_loss: 29.3537 - val_MinusLogProbMetric: 29.3537 - lr: 0.0010 - 75s/epoch - 382ms/step
Epoch 178/1000
2023-09-30 09:14:06.306 
Epoch 178/1000 
	 loss: 28.8913, MinusLogProbMetric: 28.8913, val_loss: 29.4603, val_MinusLogProbMetric: 29.4603

Epoch 178: val_loss did not improve from 28.83129
196/196 - 73s - loss: 28.8913 - MinusLogProbMetric: 28.8913 - val_loss: 29.4603 - val_MinusLogProbMetric: 29.4603 - lr: 0.0010 - 73s/epoch - 374ms/step
Epoch 179/1000
2023-09-30 09:15:18.523 
Epoch 179/1000 
	 loss: 28.9911, MinusLogProbMetric: 28.9911, val_loss: 29.2219, val_MinusLogProbMetric: 29.2219

Epoch 179: val_loss did not improve from 28.83129
196/196 - 72s - loss: 28.9911 - MinusLogProbMetric: 28.9911 - val_loss: 29.2219 - val_MinusLogProbMetric: 29.2219 - lr: 0.0010 - 72s/epoch - 368ms/step
Epoch 180/1000
2023-09-30 09:16:29.739 
Epoch 180/1000 
	 loss: 28.9180, MinusLogProbMetric: 28.9180, val_loss: 29.2943, val_MinusLogProbMetric: 29.2943

Epoch 180: val_loss did not improve from 28.83129
196/196 - 71s - loss: 28.9180 - MinusLogProbMetric: 28.9180 - val_loss: 29.2943 - val_MinusLogProbMetric: 29.2943 - lr: 0.0010 - 71s/epoch - 363ms/step
Epoch 181/1000
2023-09-30 09:17:43.209 
Epoch 181/1000 
	 loss: 29.0472, MinusLogProbMetric: 29.0472, val_loss: 29.1778, val_MinusLogProbMetric: 29.1778

Epoch 181: val_loss did not improve from 28.83129
196/196 - 73s - loss: 29.0472 - MinusLogProbMetric: 29.0472 - val_loss: 29.1778 - val_MinusLogProbMetric: 29.1778 - lr: 0.0010 - 73s/epoch - 375ms/step
Epoch 182/1000
2023-09-30 09:18:56.964 
Epoch 182/1000 
	 loss: 28.8139, MinusLogProbMetric: 28.8139, val_loss: 29.1818, val_MinusLogProbMetric: 29.1818

Epoch 182: val_loss did not improve from 28.83129
196/196 - 74s - loss: 28.8139 - MinusLogProbMetric: 28.8139 - val_loss: 29.1818 - val_MinusLogProbMetric: 29.1818 - lr: 0.0010 - 74s/epoch - 376ms/step
Epoch 183/1000
2023-09-30 09:20:10.721 
Epoch 183/1000 
	 loss: 28.9291, MinusLogProbMetric: 28.9291, val_loss: 28.9592, val_MinusLogProbMetric: 28.9592

Epoch 183: val_loss did not improve from 28.83129
196/196 - 74s - loss: 28.9291 - MinusLogProbMetric: 28.9291 - val_loss: 28.9592 - val_MinusLogProbMetric: 28.9592 - lr: 0.0010 - 74s/epoch - 376ms/step
Epoch 184/1000
2023-09-30 09:21:21.485 
Epoch 184/1000 
	 loss: 29.0946, MinusLogProbMetric: 29.0946, val_loss: 29.0343, val_MinusLogProbMetric: 29.0343

Epoch 184: val_loss did not improve from 28.83129
196/196 - 71s - loss: 29.0946 - MinusLogProbMetric: 29.0946 - val_loss: 29.0343 - val_MinusLogProbMetric: 29.0343 - lr: 0.0010 - 71s/epoch - 361ms/step
Epoch 185/1000
2023-09-30 09:22:34.097 
Epoch 185/1000 
	 loss: 28.8759, MinusLogProbMetric: 28.8759, val_loss: 28.9719, val_MinusLogProbMetric: 28.9719

Epoch 185: val_loss did not improve from 28.83129
196/196 - 73s - loss: 28.8759 - MinusLogProbMetric: 28.8759 - val_loss: 28.9719 - val_MinusLogProbMetric: 28.9719 - lr: 0.0010 - 73s/epoch - 370ms/step
Epoch 186/1000
2023-09-30 09:23:56.419 
Epoch 186/1000 
	 loss: 29.0115, MinusLogProbMetric: 29.0115, val_loss: 29.6593, val_MinusLogProbMetric: 29.6593

Epoch 186: val_loss did not improve from 28.83129
196/196 - 82s - loss: 29.0115 - MinusLogProbMetric: 29.0115 - val_loss: 29.6593 - val_MinusLogProbMetric: 29.6593 - lr: 0.0010 - 82s/epoch - 420ms/step
Epoch 187/1000
2023-09-30 09:25:11.032 
Epoch 187/1000 
	 loss: 28.9259, MinusLogProbMetric: 28.9259, val_loss: 31.6742, val_MinusLogProbMetric: 31.6742

Epoch 187: val_loss did not improve from 28.83129
196/196 - 75s - loss: 28.9259 - MinusLogProbMetric: 28.9259 - val_loss: 31.6742 - val_MinusLogProbMetric: 31.6742 - lr: 0.0010 - 75s/epoch - 381ms/step
Epoch 188/1000
2023-09-30 09:26:25.402 
Epoch 188/1000 
	 loss: 28.9230, MinusLogProbMetric: 28.9230, val_loss: 31.0672, val_MinusLogProbMetric: 31.0672

Epoch 188: val_loss did not improve from 28.83129
196/196 - 74s - loss: 28.9230 - MinusLogProbMetric: 28.9230 - val_loss: 31.0672 - val_MinusLogProbMetric: 31.0672 - lr: 0.0010 - 74s/epoch - 379ms/step
Epoch 189/1000
2023-09-30 09:27:37.673 
Epoch 189/1000 
	 loss: 28.7927, MinusLogProbMetric: 28.7927, val_loss: 29.7314, val_MinusLogProbMetric: 29.7314

Epoch 189: val_loss did not improve from 28.83129
196/196 - 72s - loss: 28.7927 - MinusLogProbMetric: 28.7927 - val_loss: 29.7314 - val_MinusLogProbMetric: 29.7314 - lr: 0.0010 - 72s/epoch - 369ms/step
Epoch 190/1000
2023-09-30 09:28:52.372 
Epoch 190/1000 
	 loss: 28.9077, MinusLogProbMetric: 28.9077, val_loss: 29.7308, val_MinusLogProbMetric: 29.7308

Epoch 190: val_loss did not improve from 28.83129
196/196 - 75s - loss: 28.9077 - MinusLogProbMetric: 28.9077 - val_loss: 29.7308 - val_MinusLogProbMetric: 29.7308 - lr: 0.0010 - 75s/epoch - 381ms/step
Epoch 191/1000
2023-09-30 09:30:03.775 
Epoch 191/1000 
	 loss: 28.8493, MinusLogProbMetric: 28.8493, val_loss: 29.4523, val_MinusLogProbMetric: 29.4523

Epoch 191: val_loss did not improve from 28.83129
196/196 - 71s - loss: 28.8493 - MinusLogProbMetric: 28.8493 - val_loss: 29.4523 - val_MinusLogProbMetric: 29.4523 - lr: 0.0010 - 71s/epoch - 364ms/step
Epoch 192/1000
2023-09-30 09:31:15.934 
Epoch 192/1000 
	 loss: 28.8498, MinusLogProbMetric: 28.8498, val_loss: 29.2297, val_MinusLogProbMetric: 29.2297

Epoch 192: val_loss did not improve from 28.83129
196/196 - 72s - loss: 28.8498 - MinusLogProbMetric: 28.8498 - val_loss: 29.2297 - val_MinusLogProbMetric: 29.2297 - lr: 0.0010 - 72s/epoch - 368ms/step
Epoch 193/1000
2023-09-30 09:32:29.071 
Epoch 193/1000 
	 loss: 28.8292, MinusLogProbMetric: 28.8292, val_loss: 29.0072, val_MinusLogProbMetric: 29.0072

Epoch 193: val_loss did not improve from 28.83129
196/196 - 73s - loss: 28.8292 - MinusLogProbMetric: 28.8292 - val_loss: 29.0072 - val_MinusLogProbMetric: 29.0072 - lr: 0.0010 - 73s/epoch - 373ms/step
Epoch 194/1000
2023-09-30 09:33:41.064 
Epoch 194/1000 
	 loss: 28.8277, MinusLogProbMetric: 28.8277, val_loss: 30.1000, val_MinusLogProbMetric: 30.1000

Epoch 194: val_loss did not improve from 28.83129
196/196 - 72s - loss: 28.8277 - MinusLogProbMetric: 28.8277 - val_loss: 30.1000 - val_MinusLogProbMetric: 30.1000 - lr: 0.0010 - 72s/epoch - 367ms/step
Epoch 195/1000
2023-09-30 09:35:00.203 
Epoch 195/1000 
	 loss: 29.0771, MinusLogProbMetric: 29.0771, val_loss: 29.5205, val_MinusLogProbMetric: 29.5205

Epoch 195: val_loss did not improve from 28.83129
196/196 - 79s - loss: 29.0771 - MinusLogProbMetric: 29.0771 - val_loss: 29.5205 - val_MinusLogProbMetric: 29.5205 - lr: 0.0010 - 79s/epoch - 404ms/step
Epoch 196/1000
2023-09-30 09:36:14.623 
Epoch 196/1000 
	 loss: 28.8468, MinusLogProbMetric: 28.8468, val_loss: 30.2142, val_MinusLogProbMetric: 30.2142

Epoch 196: val_loss did not improve from 28.83129
196/196 - 74s - loss: 28.8468 - MinusLogProbMetric: 28.8468 - val_loss: 30.2142 - val_MinusLogProbMetric: 30.2142 - lr: 0.0010 - 74s/epoch - 380ms/step
Epoch 197/1000
2023-09-30 09:37:29.615 
Epoch 197/1000 
	 loss: 28.9761, MinusLogProbMetric: 28.9761, val_loss: 29.3419, val_MinusLogProbMetric: 29.3419

Epoch 197: val_loss did not improve from 28.83129
196/196 - 75s - loss: 28.9761 - MinusLogProbMetric: 28.9761 - val_loss: 29.3419 - val_MinusLogProbMetric: 29.3419 - lr: 0.0010 - 75s/epoch - 383ms/step
Epoch 198/1000
2023-09-30 09:38:43.592 
Epoch 198/1000 
	 loss: 28.7987, MinusLogProbMetric: 28.7987, val_loss: 29.5304, val_MinusLogProbMetric: 29.5304

Epoch 198: val_loss did not improve from 28.83129
196/196 - 74s - loss: 28.7987 - MinusLogProbMetric: 28.7987 - val_loss: 29.5304 - val_MinusLogProbMetric: 29.5304 - lr: 0.0010 - 74s/epoch - 377ms/step
Epoch 199/1000
2023-09-30 09:39:57.350 
Epoch 199/1000 
	 loss: 28.7917, MinusLogProbMetric: 28.7917, val_loss: 30.2931, val_MinusLogProbMetric: 30.2931

Epoch 199: val_loss did not improve from 28.83129
196/196 - 74s - loss: 28.7917 - MinusLogProbMetric: 28.7917 - val_loss: 30.2931 - val_MinusLogProbMetric: 30.2931 - lr: 0.0010 - 74s/epoch - 376ms/step
Epoch 200/1000
2023-09-30 09:41:11.611 
Epoch 200/1000 
	 loss: 28.9299, MinusLogProbMetric: 28.9299, val_loss: 29.5515, val_MinusLogProbMetric: 29.5515

Epoch 200: val_loss did not improve from 28.83129
196/196 - 74s - loss: 28.9299 - MinusLogProbMetric: 28.9299 - val_loss: 29.5515 - val_MinusLogProbMetric: 29.5515 - lr: 0.0010 - 74s/epoch - 379ms/step
Epoch 201/1000
2023-09-30 09:42:28.095 
Epoch 201/1000 
	 loss: 28.7506, MinusLogProbMetric: 28.7506, val_loss: 29.7122, val_MinusLogProbMetric: 29.7122

Epoch 201: val_loss did not improve from 28.83129
196/196 - 76s - loss: 28.7506 - MinusLogProbMetric: 28.7506 - val_loss: 29.7122 - val_MinusLogProbMetric: 29.7122 - lr: 0.0010 - 76s/epoch - 390ms/step
Epoch 202/1000
2023-09-30 09:43:43.588 
Epoch 202/1000 
	 loss: 28.8689, MinusLogProbMetric: 28.8689, val_loss: 29.0663, val_MinusLogProbMetric: 29.0663

Epoch 202: val_loss did not improve from 28.83129
196/196 - 75s - loss: 28.8689 - MinusLogProbMetric: 28.8689 - val_loss: 29.0663 - val_MinusLogProbMetric: 29.0663 - lr: 0.0010 - 75s/epoch - 385ms/step
Epoch 203/1000
2023-09-30 09:44:56.230 
Epoch 203/1000 
	 loss: 28.9146, MinusLogProbMetric: 28.9146, val_loss: 28.9790, val_MinusLogProbMetric: 28.9790

Epoch 203: val_loss did not improve from 28.83129
196/196 - 73s - loss: 28.9146 - MinusLogProbMetric: 28.9146 - val_loss: 28.9790 - val_MinusLogProbMetric: 28.9790 - lr: 0.0010 - 73s/epoch - 371ms/step
Epoch 204/1000
2023-09-30 09:46:10.405 
Epoch 204/1000 
	 loss: 28.6964, MinusLogProbMetric: 28.6964, val_loss: 29.2023, val_MinusLogProbMetric: 29.2023

Epoch 204: val_loss did not improve from 28.83129
196/196 - 74s - loss: 28.6964 - MinusLogProbMetric: 28.6964 - val_loss: 29.2023 - val_MinusLogProbMetric: 29.2023 - lr: 0.0010 - 74s/epoch - 378ms/step
Epoch 205/1000
2023-09-30 09:47:25.082 
Epoch 205/1000 
	 loss: 28.7713, MinusLogProbMetric: 28.7713, val_loss: 29.5036, val_MinusLogProbMetric: 29.5036

Epoch 205: val_loss did not improve from 28.83129
196/196 - 75s - loss: 28.7713 - MinusLogProbMetric: 28.7713 - val_loss: 29.5036 - val_MinusLogProbMetric: 29.5036 - lr: 0.0010 - 75s/epoch - 381ms/step
Epoch 206/1000
2023-09-30 09:48:34.844 
Epoch 206/1000 
	 loss: 28.8486, MinusLogProbMetric: 28.8486, val_loss: 29.0771, val_MinusLogProbMetric: 29.0771

Epoch 206: val_loss did not improve from 28.83129
196/196 - 70s - loss: 28.8486 - MinusLogProbMetric: 28.8486 - val_loss: 29.0771 - val_MinusLogProbMetric: 29.0771 - lr: 0.0010 - 70s/epoch - 356ms/step
Epoch 207/1000
2023-09-30 09:49:48.471 
Epoch 207/1000 
	 loss: 28.8577, MinusLogProbMetric: 28.8577, val_loss: 29.3030, val_MinusLogProbMetric: 29.3030

Epoch 207: val_loss did not improve from 28.83129
196/196 - 74s - loss: 28.8577 - MinusLogProbMetric: 28.8577 - val_loss: 29.3030 - val_MinusLogProbMetric: 29.3030 - lr: 0.0010 - 74s/epoch - 376ms/step
Epoch 208/1000
2023-09-30 09:51:00.273 
Epoch 208/1000 
	 loss: 28.8674, MinusLogProbMetric: 28.8674, val_loss: 29.7050, val_MinusLogProbMetric: 29.7050

Epoch 208: val_loss did not improve from 28.83129
196/196 - 72s - loss: 28.8674 - MinusLogProbMetric: 28.8674 - val_loss: 29.7050 - val_MinusLogProbMetric: 29.7050 - lr: 0.0010 - 72s/epoch - 366ms/step
Epoch 209/1000
2023-09-30 09:52:11.934 
Epoch 209/1000 
	 loss: 28.6730, MinusLogProbMetric: 28.6730, val_loss: 28.9804, val_MinusLogProbMetric: 28.9804

Epoch 209: val_loss did not improve from 28.83129
196/196 - 72s - loss: 28.6730 - MinusLogProbMetric: 28.6730 - val_loss: 28.9804 - val_MinusLogProbMetric: 28.9804 - lr: 0.0010 - 72s/epoch - 366ms/step
Epoch 210/1000
2023-09-30 09:53:20.854 
Epoch 210/1000 
	 loss: 28.6825, MinusLogProbMetric: 28.6825, val_loss: 29.3785, val_MinusLogProbMetric: 29.3785

Epoch 210: val_loss did not improve from 28.83129
196/196 - 69s - loss: 28.6825 - MinusLogProbMetric: 28.6825 - val_loss: 29.3785 - val_MinusLogProbMetric: 29.3785 - lr: 0.0010 - 69s/epoch - 352ms/step
Epoch 211/1000
2023-09-30 09:54:34.632 
Epoch 211/1000 
	 loss: 28.7825, MinusLogProbMetric: 28.7825, val_loss: 29.5681, val_MinusLogProbMetric: 29.5681

Epoch 211: val_loss did not improve from 28.83129
196/196 - 74s - loss: 28.7825 - MinusLogProbMetric: 28.7825 - val_loss: 29.5681 - val_MinusLogProbMetric: 29.5681 - lr: 0.0010 - 74s/epoch - 376ms/step
Epoch 212/1000
2023-09-30 09:55:52.552 
Epoch 212/1000 
	 loss: 28.8109, MinusLogProbMetric: 28.8109, val_loss: 29.6356, val_MinusLogProbMetric: 29.6356

Epoch 212: val_loss did not improve from 28.83129
196/196 - 78s - loss: 28.8109 - MinusLogProbMetric: 28.8109 - val_loss: 29.6356 - val_MinusLogProbMetric: 29.6356 - lr: 0.0010 - 78s/epoch - 398ms/step
Epoch 213/1000
2023-09-30 09:57:07.410 
Epoch 213/1000 
	 loss: 28.7384, MinusLogProbMetric: 28.7384, val_loss: 28.9650, val_MinusLogProbMetric: 28.9650

Epoch 213: val_loss did not improve from 28.83129
196/196 - 75s - loss: 28.7384 - MinusLogProbMetric: 28.7384 - val_loss: 28.9650 - val_MinusLogProbMetric: 28.9650 - lr: 0.0010 - 75s/epoch - 381ms/step
Epoch 214/1000
2023-09-30 09:58:23.486 
Epoch 214/1000 
	 loss: 28.7455, MinusLogProbMetric: 28.7455, val_loss: 28.9941, val_MinusLogProbMetric: 28.9941

Epoch 214: val_loss did not improve from 28.83129
196/196 - 76s - loss: 28.7455 - MinusLogProbMetric: 28.7455 - val_loss: 28.9941 - val_MinusLogProbMetric: 28.9941 - lr: 0.0010 - 76s/epoch - 388ms/step
Epoch 215/1000
2023-09-30 09:59:35.669 
Epoch 215/1000 
	 loss: 28.6927, MinusLogProbMetric: 28.6927, val_loss: 29.2135, val_MinusLogProbMetric: 29.2135

Epoch 215: val_loss did not improve from 28.83129
196/196 - 72s - loss: 28.6927 - MinusLogProbMetric: 28.6927 - val_loss: 29.2135 - val_MinusLogProbMetric: 29.2135 - lr: 0.0010 - 72s/epoch - 368ms/step
Epoch 216/1000
2023-09-30 10:00:50.704 
Epoch 216/1000 
	 loss: 28.6756, MinusLogProbMetric: 28.6756, val_loss: 29.2185, val_MinusLogProbMetric: 29.2185

Epoch 216: val_loss did not improve from 28.83129
196/196 - 75s - loss: 28.6756 - MinusLogProbMetric: 28.6756 - val_loss: 29.2185 - val_MinusLogProbMetric: 29.2185 - lr: 0.0010 - 75s/epoch - 383ms/step
Epoch 217/1000
2023-09-30 10:02:09.785 
Epoch 217/1000 
	 loss: 28.7552, MinusLogProbMetric: 28.7552, val_loss: 28.9564, val_MinusLogProbMetric: 28.9564

Epoch 217: val_loss did not improve from 28.83129
196/196 - 79s - loss: 28.7552 - MinusLogProbMetric: 28.7552 - val_loss: 28.9564 - val_MinusLogProbMetric: 28.9564 - lr: 0.0010 - 79s/epoch - 403ms/step
Epoch 218/1000
2023-09-30 10:03:23.997 
Epoch 218/1000 
	 loss: 28.7742, MinusLogProbMetric: 28.7742, val_loss: 29.3592, val_MinusLogProbMetric: 29.3592

Epoch 218: val_loss did not improve from 28.83129
196/196 - 74s - loss: 28.7742 - MinusLogProbMetric: 28.7742 - val_loss: 29.3592 - val_MinusLogProbMetric: 29.3592 - lr: 0.0010 - 74s/epoch - 378ms/step
Epoch 219/1000
2023-09-30 10:04:38.417 
Epoch 219/1000 
	 loss: 28.6675, MinusLogProbMetric: 28.6675, val_loss: 29.3126, val_MinusLogProbMetric: 29.3126

Epoch 219: val_loss did not improve from 28.83129
196/196 - 74s - loss: 28.6675 - MinusLogProbMetric: 28.6675 - val_loss: 29.3126 - val_MinusLogProbMetric: 29.3126 - lr: 0.0010 - 74s/epoch - 380ms/step
Epoch 220/1000
2023-09-30 10:05:51.839 
Epoch 220/1000 
	 loss: 28.6916, MinusLogProbMetric: 28.6916, val_loss: 29.6734, val_MinusLogProbMetric: 29.6734

Epoch 220: val_loss did not improve from 28.83129
196/196 - 73s - loss: 28.6916 - MinusLogProbMetric: 28.6916 - val_loss: 29.6734 - val_MinusLogProbMetric: 29.6734 - lr: 0.0010 - 73s/epoch - 374ms/step
Epoch 221/1000
2023-09-30 10:06:50.395 
Epoch 221/1000 
	 loss: 28.7440, MinusLogProbMetric: 28.7440, val_loss: 29.0138, val_MinusLogProbMetric: 29.0138

Epoch 221: val_loss did not improve from 28.83129
196/196 - 59s - loss: 28.7440 - MinusLogProbMetric: 28.7440 - val_loss: 29.0138 - val_MinusLogProbMetric: 29.0138 - lr: 0.0010 - 59s/epoch - 299ms/step
Epoch 222/1000
2023-09-30 10:08:01.692 
Epoch 222/1000 
	 loss: 28.0482, MinusLogProbMetric: 28.0482, val_loss: 28.5278, val_MinusLogProbMetric: 28.5278

Epoch 222: val_loss improved from 28.83129 to 28.52784, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 74s - loss: 28.0482 - MinusLogProbMetric: 28.0482 - val_loss: 28.5278 - val_MinusLogProbMetric: 28.5278 - lr: 5.0000e-04 - 74s/epoch - 380ms/step
Epoch 223/1000
2023-09-30 10:09:15.365 
Epoch 223/1000 
	 loss: 28.0413, MinusLogProbMetric: 28.0413, val_loss: 28.6857, val_MinusLogProbMetric: 28.6857

Epoch 223: val_loss did not improve from 28.52784
196/196 - 71s - loss: 28.0413 - MinusLogProbMetric: 28.0413 - val_loss: 28.6857 - val_MinusLogProbMetric: 28.6857 - lr: 5.0000e-04 - 71s/epoch - 360ms/step
Epoch 224/1000
2023-09-30 10:10:28.387 
Epoch 224/1000 
	 loss: 28.0318, MinusLogProbMetric: 28.0318, val_loss: 28.4198, val_MinusLogProbMetric: 28.4198

Epoch 224: val_loss improved from 28.52784 to 28.41977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 74s - loss: 28.0318 - MinusLogProbMetric: 28.0318 - val_loss: 28.4198 - val_MinusLogProbMetric: 28.4198 - lr: 5.0000e-04 - 74s/epoch - 378ms/step
Epoch 225/1000
2023-09-30 10:11:42.209 
Epoch 225/1000 
	 loss: 28.0452, MinusLogProbMetric: 28.0452, val_loss: 28.7353, val_MinusLogProbMetric: 28.7353

Epoch 225: val_loss did not improve from 28.41977
196/196 - 73s - loss: 28.0452 - MinusLogProbMetric: 28.0452 - val_loss: 28.7353 - val_MinusLogProbMetric: 28.7353 - lr: 5.0000e-04 - 73s/epoch - 371ms/step
Epoch 226/1000
2023-09-30 10:12:54.892 
Epoch 226/1000 
	 loss: 28.0783, MinusLogProbMetric: 28.0783, val_loss: 28.9063, val_MinusLogProbMetric: 28.9063

Epoch 226: val_loss did not improve from 28.41977
196/196 - 73s - loss: 28.0783 - MinusLogProbMetric: 28.0783 - val_loss: 28.9063 - val_MinusLogProbMetric: 28.9063 - lr: 5.0000e-04 - 73s/epoch - 371ms/step
Epoch 227/1000
2023-09-30 10:14:08.775 
Epoch 227/1000 
	 loss: 28.0446, MinusLogProbMetric: 28.0446, val_loss: 28.4053, val_MinusLogProbMetric: 28.4053

Epoch 227: val_loss improved from 28.41977 to 28.40529, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 75s - loss: 28.0446 - MinusLogProbMetric: 28.0446 - val_loss: 28.4053 - val_MinusLogProbMetric: 28.4053 - lr: 5.0000e-04 - 75s/epoch - 383ms/step
Epoch 228/1000
2023-09-30 10:15:21.950 
Epoch 228/1000 
	 loss: 28.0493, MinusLogProbMetric: 28.0493, val_loss: 28.5020, val_MinusLogProbMetric: 28.5020

Epoch 228: val_loss did not improve from 28.40529
196/196 - 72s - loss: 28.0493 - MinusLogProbMetric: 28.0493 - val_loss: 28.5020 - val_MinusLogProbMetric: 28.5020 - lr: 5.0000e-04 - 72s/epoch - 367ms/step
Epoch 229/1000
2023-09-30 10:16:32.248 
Epoch 229/1000 
	 loss: 28.0680, MinusLogProbMetric: 28.0680, val_loss: 28.6279, val_MinusLogProbMetric: 28.6279

Epoch 229: val_loss did not improve from 28.40529
196/196 - 70s - loss: 28.0680 - MinusLogProbMetric: 28.0680 - val_loss: 28.6279 - val_MinusLogProbMetric: 28.6279 - lr: 5.0000e-04 - 70s/epoch - 358ms/step
Epoch 230/1000
2023-09-30 10:17:42.175 
Epoch 230/1000 
	 loss: 28.0193, MinusLogProbMetric: 28.0193, val_loss: 28.6023, val_MinusLogProbMetric: 28.6023

Epoch 230: val_loss did not improve from 28.40529
196/196 - 70s - loss: 28.0193 - MinusLogProbMetric: 28.0193 - val_loss: 28.6023 - val_MinusLogProbMetric: 28.6023 - lr: 5.0000e-04 - 70s/epoch - 357ms/step
Epoch 231/1000
2023-09-30 10:18:51.083 
Epoch 231/1000 
	 loss: 28.0373, MinusLogProbMetric: 28.0373, val_loss: 29.0237, val_MinusLogProbMetric: 29.0237

Epoch 231: val_loss did not improve from 28.40529
196/196 - 69s - loss: 28.0373 - MinusLogProbMetric: 28.0373 - val_loss: 29.0237 - val_MinusLogProbMetric: 29.0237 - lr: 5.0000e-04 - 69s/epoch - 352ms/step
Epoch 232/1000
2023-09-30 10:20:02.940 
Epoch 232/1000 
	 loss: 28.0284, MinusLogProbMetric: 28.0284, val_loss: 28.6055, val_MinusLogProbMetric: 28.6055

Epoch 232: val_loss did not improve from 28.40529
196/196 - 72s - loss: 28.0284 - MinusLogProbMetric: 28.0284 - val_loss: 28.6055 - val_MinusLogProbMetric: 28.6055 - lr: 5.0000e-04 - 72s/epoch - 366ms/step
Epoch 233/1000
2023-09-30 10:21:13.079 
Epoch 233/1000 
	 loss: 28.0053, MinusLogProbMetric: 28.0053, val_loss: 28.8946, val_MinusLogProbMetric: 28.8946

Epoch 233: val_loss did not improve from 28.40529
196/196 - 70s - loss: 28.0053 - MinusLogProbMetric: 28.0053 - val_loss: 28.8946 - val_MinusLogProbMetric: 28.8946 - lr: 5.0000e-04 - 70s/epoch - 358ms/step
Epoch 234/1000
2023-09-30 10:22:25.743 
Epoch 234/1000 
	 loss: 28.0785, MinusLogProbMetric: 28.0785, val_loss: 28.4397, val_MinusLogProbMetric: 28.4397

Epoch 234: val_loss did not improve from 28.40529
196/196 - 73s - loss: 28.0785 - MinusLogProbMetric: 28.0785 - val_loss: 28.4397 - val_MinusLogProbMetric: 28.4397 - lr: 5.0000e-04 - 73s/epoch - 371ms/step
Epoch 235/1000
2023-09-30 10:23:37.809 
Epoch 235/1000 
	 loss: 27.9535, MinusLogProbMetric: 27.9535, val_loss: 28.5682, val_MinusLogProbMetric: 28.5682

Epoch 235: val_loss did not improve from 28.40529
196/196 - 72s - loss: 27.9535 - MinusLogProbMetric: 27.9535 - val_loss: 28.5682 - val_MinusLogProbMetric: 28.5682 - lr: 5.0000e-04 - 72s/epoch - 368ms/step
Epoch 236/1000
2023-09-30 10:24:46.360 
Epoch 236/1000 
	 loss: 28.0224, MinusLogProbMetric: 28.0224, val_loss: 28.8831, val_MinusLogProbMetric: 28.8831

Epoch 236: val_loss did not improve from 28.40529
196/196 - 69s - loss: 28.0224 - MinusLogProbMetric: 28.0224 - val_loss: 28.8831 - val_MinusLogProbMetric: 28.8831 - lr: 5.0000e-04 - 69s/epoch - 350ms/step
Epoch 237/1000
2023-09-30 10:25:58.373 
Epoch 237/1000 
	 loss: 28.0769, MinusLogProbMetric: 28.0769, val_loss: 28.4858, val_MinusLogProbMetric: 28.4858

Epoch 237: val_loss did not improve from 28.40529
196/196 - 72s - loss: 28.0769 - MinusLogProbMetric: 28.0769 - val_loss: 28.4858 - val_MinusLogProbMetric: 28.4858 - lr: 5.0000e-04 - 72s/epoch - 367ms/step
Epoch 238/1000
2023-09-30 10:27:07.273 
Epoch 238/1000 
	 loss: 27.9901, MinusLogProbMetric: 27.9901, val_loss: 28.4287, val_MinusLogProbMetric: 28.4287

Epoch 238: val_loss did not improve from 28.40529
196/196 - 69s - loss: 27.9901 - MinusLogProbMetric: 27.9901 - val_loss: 28.4287 - val_MinusLogProbMetric: 28.4287 - lr: 5.0000e-04 - 69s/epoch - 351ms/step
Epoch 239/1000
2023-09-30 10:28:24.719 
Epoch 239/1000 
	 loss: 28.0343, MinusLogProbMetric: 28.0343, val_loss: 28.6040, val_MinusLogProbMetric: 28.6040

Epoch 239: val_loss did not improve from 28.40529
196/196 - 77s - loss: 28.0343 - MinusLogProbMetric: 28.0343 - val_loss: 28.6040 - val_MinusLogProbMetric: 28.6040 - lr: 5.0000e-04 - 77s/epoch - 395ms/step
Epoch 240/1000
2023-09-30 10:29:34.355 
Epoch 240/1000 
	 loss: 28.0335, MinusLogProbMetric: 28.0335, val_loss: 28.5738, val_MinusLogProbMetric: 28.5738

Epoch 240: val_loss did not improve from 28.40529
196/196 - 70s - loss: 28.0335 - MinusLogProbMetric: 28.0335 - val_loss: 28.5738 - val_MinusLogProbMetric: 28.5738 - lr: 5.0000e-04 - 70s/epoch - 355ms/step
Epoch 241/1000
2023-09-30 10:30:46.012 
Epoch 241/1000 
	 loss: 28.0485, MinusLogProbMetric: 28.0485, val_loss: 28.3932, val_MinusLogProbMetric: 28.3932

Epoch 241: val_loss improved from 28.40529 to 28.39321, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 73s - loss: 28.0485 - MinusLogProbMetric: 28.0485 - val_loss: 28.3932 - val_MinusLogProbMetric: 28.3932 - lr: 5.0000e-04 - 73s/epoch - 374ms/step
Epoch 242/1000
2023-09-30 10:31:58.615 
Epoch 242/1000 
	 loss: 28.0271, MinusLogProbMetric: 28.0271, val_loss: 28.7833, val_MinusLogProbMetric: 28.7833

Epoch 242: val_loss did not improve from 28.39321
196/196 - 71s - loss: 28.0271 - MinusLogProbMetric: 28.0271 - val_loss: 28.7833 - val_MinusLogProbMetric: 28.7833 - lr: 5.0000e-04 - 71s/epoch - 362ms/step
Epoch 243/1000
2023-09-30 10:33:08.269 
Epoch 243/1000 
	 loss: 28.0831, MinusLogProbMetric: 28.0831, val_loss: 28.5056, val_MinusLogProbMetric: 28.5056

Epoch 243: val_loss did not improve from 28.39321
196/196 - 70s - loss: 28.0831 - MinusLogProbMetric: 28.0831 - val_loss: 28.5056 - val_MinusLogProbMetric: 28.5056 - lr: 5.0000e-04 - 70s/epoch - 355ms/step
Epoch 244/1000
2023-09-30 10:34:19.939 
Epoch 244/1000 
	 loss: 27.9861, MinusLogProbMetric: 27.9861, val_loss: 28.4817, val_MinusLogProbMetric: 28.4817

Epoch 244: val_loss did not improve from 28.39321
196/196 - 72s - loss: 27.9861 - MinusLogProbMetric: 27.9861 - val_loss: 28.4817 - val_MinusLogProbMetric: 28.4817 - lr: 5.0000e-04 - 72s/epoch - 366ms/step
Epoch 245/1000
2023-09-30 10:35:29.446 
Epoch 245/1000 
	 loss: 28.0044, MinusLogProbMetric: 28.0044, val_loss: 28.5303, val_MinusLogProbMetric: 28.5303

Epoch 245: val_loss did not improve from 28.39321
196/196 - 69s - loss: 28.0044 - MinusLogProbMetric: 28.0044 - val_loss: 28.5303 - val_MinusLogProbMetric: 28.5303 - lr: 5.0000e-04 - 69s/epoch - 354ms/step
Epoch 246/1000
2023-09-30 10:36:40.334 
Epoch 246/1000 
	 loss: 28.0373, MinusLogProbMetric: 28.0373, val_loss: 28.8048, val_MinusLogProbMetric: 28.8048

Epoch 246: val_loss did not improve from 28.39321
196/196 - 71s - loss: 28.0373 - MinusLogProbMetric: 28.0373 - val_loss: 28.8048 - val_MinusLogProbMetric: 28.8048 - lr: 5.0000e-04 - 71s/epoch - 362ms/step
Epoch 247/1000
2023-09-30 10:37:55.603 
Epoch 247/1000 
	 loss: 28.0285, MinusLogProbMetric: 28.0285, val_loss: 28.8612, val_MinusLogProbMetric: 28.8612

Epoch 247: val_loss did not improve from 28.39321
196/196 - 75s - loss: 28.0285 - MinusLogProbMetric: 28.0285 - val_loss: 28.8612 - val_MinusLogProbMetric: 28.8612 - lr: 5.0000e-04 - 75s/epoch - 384ms/step
Epoch 248/1000
2023-09-30 10:39:04.624 
Epoch 248/1000 
	 loss: 27.9627, MinusLogProbMetric: 27.9627, val_loss: 28.9535, val_MinusLogProbMetric: 28.9535

Epoch 248: val_loss did not improve from 28.39321
196/196 - 69s - loss: 27.9627 - MinusLogProbMetric: 27.9627 - val_loss: 28.9535 - val_MinusLogProbMetric: 28.9535 - lr: 5.0000e-04 - 69s/epoch - 352ms/step
Epoch 249/1000
2023-09-30 10:40:15.207 
Epoch 249/1000 
	 loss: 28.0001, MinusLogProbMetric: 28.0001, val_loss: 28.5720, val_MinusLogProbMetric: 28.5720

Epoch 249: val_loss did not improve from 28.39321
196/196 - 71s - loss: 28.0001 - MinusLogProbMetric: 28.0001 - val_loss: 28.5720 - val_MinusLogProbMetric: 28.5720 - lr: 5.0000e-04 - 71s/epoch - 360ms/step
Epoch 250/1000
2023-09-30 10:41:25.792 
Epoch 250/1000 
	 loss: 28.0407, MinusLogProbMetric: 28.0407, val_loss: 28.4750, val_MinusLogProbMetric: 28.4750

Epoch 250: val_loss did not improve from 28.39321
196/196 - 71s - loss: 28.0407 - MinusLogProbMetric: 28.0407 - val_loss: 28.4750 - val_MinusLogProbMetric: 28.4750 - lr: 5.0000e-04 - 71s/epoch - 360ms/step
Epoch 251/1000
2023-09-30 10:42:38.909 
Epoch 251/1000 
	 loss: 27.9925, MinusLogProbMetric: 27.9925, val_loss: 28.6288, val_MinusLogProbMetric: 28.6288

Epoch 251: val_loss did not improve from 28.39321
196/196 - 73s - loss: 27.9925 - MinusLogProbMetric: 27.9925 - val_loss: 28.6288 - val_MinusLogProbMetric: 28.6288 - lr: 5.0000e-04 - 73s/epoch - 373ms/step
Epoch 252/1000
2023-09-30 10:43:51.452 
Epoch 252/1000 
	 loss: 27.9854, MinusLogProbMetric: 27.9854, val_loss: 28.4181, val_MinusLogProbMetric: 28.4181

Epoch 252: val_loss did not improve from 28.39321
196/196 - 73s - loss: 27.9854 - MinusLogProbMetric: 27.9854 - val_loss: 28.4181 - val_MinusLogProbMetric: 28.4181 - lr: 5.0000e-04 - 73s/epoch - 370ms/step
Epoch 253/1000
2023-09-30 10:45:04.041 
Epoch 253/1000 
	 loss: 27.9521, MinusLogProbMetric: 27.9521, val_loss: 28.3825, val_MinusLogProbMetric: 28.3825

Epoch 253: val_loss improved from 28.39321 to 28.38246, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 74s - loss: 27.9521 - MinusLogProbMetric: 27.9521 - val_loss: 28.3825 - val_MinusLogProbMetric: 28.3825 - lr: 5.0000e-04 - 74s/epoch - 378ms/step
Epoch 254/1000
2023-09-30 10:46:22.509 
Epoch 254/1000 
	 loss: 27.9716, MinusLogProbMetric: 27.9716, val_loss: 28.4141, val_MinusLogProbMetric: 28.4141

Epoch 254: val_loss did not improve from 28.38246
196/196 - 77s - loss: 27.9716 - MinusLogProbMetric: 27.9716 - val_loss: 28.4141 - val_MinusLogProbMetric: 28.4141 - lr: 5.0000e-04 - 77s/epoch - 393ms/step
Epoch 255/1000
2023-09-30 10:47:35.771 
Epoch 255/1000 
	 loss: 28.0120, MinusLogProbMetric: 28.0120, val_loss: 28.4181, val_MinusLogProbMetric: 28.4181

Epoch 255: val_loss did not improve from 28.38246
196/196 - 73s - loss: 28.0120 - MinusLogProbMetric: 28.0120 - val_loss: 28.4181 - val_MinusLogProbMetric: 28.4181 - lr: 5.0000e-04 - 73s/epoch - 374ms/step
Epoch 256/1000
2023-09-30 10:48:45.268 
Epoch 256/1000 
	 loss: 27.9529, MinusLogProbMetric: 27.9529, val_loss: 28.9059, val_MinusLogProbMetric: 28.9059

Epoch 256: val_loss did not improve from 28.38246
196/196 - 69s - loss: 27.9529 - MinusLogProbMetric: 27.9529 - val_loss: 28.9059 - val_MinusLogProbMetric: 28.9059 - lr: 5.0000e-04 - 69s/epoch - 355ms/step
Epoch 257/1000
2023-09-30 10:49:54.785 
Epoch 257/1000 
	 loss: 27.9916, MinusLogProbMetric: 27.9916, val_loss: 28.4761, val_MinusLogProbMetric: 28.4761

Epoch 257: val_loss did not improve from 28.38246
196/196 - 70s - loss: 27.9916 - MinusLogProbMetric: 27.9916 - val_loss: 28.4761 - val_MinusLogProbMetric: 28.4761 - lr: 5.0000e-04 - 70s/epoch - 355ms/step
Epoch 258/1000
2023-09-30 10:51:06.120 
Epoch 258/1000 
	 loss: 27.9949, MinusLogProbMetric: 27.9949, val_loss: 28.5629, val_MinusLogProbMetric: 28.5629

Epoch 258: val_loss did not improve from 28.38246
196/196 - 71s - loss: 27.9949 - MinusLogProbMetric: 27.9949 - val_loss: 28.5629 - val_MinusLogProbMetric: 28.5629 - lr: 5.0000e-04 - 71s/epoch - 364ms/step
Epoch 259/1000
2023-09-30 10:52:16.659 
Epoch 259/1000 
	 loss: 27.9907, MinusLogProbMetric: 27.9907, val_loss: 28.3838, val_MinusLogProbMetric: 28.3838

Epoch 259: val_loss did not improve from 28.38246
196/196 - 71s - loss: 27.9907 - MinusLogProbMetric: 27.9907 - val_loss: 28.3838 - val_MinusLogProbMetric: 28.3838 - lr: 5.0000e-04 - 71s/epoch - 360ms/step
Epoch 260/1000
2023-09-30 10:53:29.354 
Epoch 260/1000 
	 loss: 27.9733, MinusLogProbMetric: 27.9733, val_loss: 28.4922, val_MinusLogProbMetric: 28.4922

Epoch 260: val_loss did not improve from 28.38246
196/196 - 73s - loss: 27.9733 - MinusLogProbMetric: 27.9733 - val_loss: 28.4922 - val_MinusLogProbMetric: 28.4922 - lr: 5.0000e-04 - 73s/epoch - 371ms/step
Epoch 261/1000
2023-09-30 10:54:40.931 
Epoch 261/1000 
	 loss: 27.9812, MinusLogProbMetric: 27.9812, val_loss: 28.4193, val_MinusLogProbMetric: 28.4193

Epoch 261: val_loss did not improve from 28.38246
196/196 - 72s - loss: 27.9812 - MinusLogProbMetric: 27.9812 - val_loss: 28.4193 - val_MinusLogProbMetric: 28.4193 - lr: 5.0000e-04 - 72s/epoch - 365ms/step
Epoch 262/1000
2023-09-30 10:55:51.492 
Epoch 262/1000 
	 loss: 27.9759, MinusLogProbMetric: 27.9759, val_loss: 28.5524, val_MinusLogProbMetric: 28.5524

Epoch 262: val_loss did not improve from 28.38246
196/196 - 71s - loss: 27.9759 - MinusLogProbMetric: 27.9759 - val_loss: 28.5524 - val_MinusLogProbMetric: 28.5524 - lr: 5.0000e-04 - 71s/epoch - 360ms/step
Epoch 263/1000
2023-09-30 10:56:59.854 
Epoch 263/1000 
	 loss: 27.9811, MinusLogProbMetric: 27.9811, val_loss: 28.6602, val_MinusLogProbMetric: 28.6602

Epoch 263: val_loss did not improve from 28.38246
196/196 - 68s - loss: 27.9811 - MinusLogProbMetric: 27.9811 - val_loss: 28.6602 - val_MinusLogProbMetric: 28.6602 - lr: 5.0000e-04 - 68s/epoch - 349ms/step
Epoch 264/1000
2023-09-30 10:58:17.355 
Epoch 264/1000 
	 loss: 28.0166, MinusLogProbMetric: 28.0166, val_loss: 28.5501, val_MinusLogProbMetric: 28.5501

Epoch 264: val_loss did not improve from 28.38246
196/196 - 77s - loss: 28.0166 - MinusLogProbMetric: 28.0166 - val_loss: 28.5501 - val_MinusLogProbMetric: 28.5501 - lr: 5.0000e-04 - 77s/epoch - 395ms/step
Epoch 265/1000
2023-09-30 10:59:28.676 
Epoch 265/1000 
	 loss: 28.0140, MinusLogProbMetric: 28.0140, val_loss: 28.3948, val_MinusLogProbMetric: 28.3948

Epoch 265: val_loss did not improve from 28.38246
196/196 - 71s - loss: 28.0140 - MinusLogProbMetric: 28.0140 - val_loss: 28.3948 - val_MinusLogProbMetric: 28.3948 - lr: 5.0000e-04 - 71s/epoch - 364ms/step
Epoch 266/1000
2023-09-30 11:00:34.524 
Epoch 266/1000 
	 loss: 27.9986, MinusLogProbMetric: 27.9986, val_loss: 28.5012, val_MinusLogProbMetric: 28.5012

Epoch 266: val_loss did not improve from 28.38246
196/196 - 66s - loss: 27.9986 - MinusLogProbMetric: 27.9986 - val_loss: 28.5012 - val_MinusLogProbMetric: 28.5012 - lr: 5.0000e-04 - 66s/epoch - 336ms/step
Epoch 267/1000
2023-09-30 11:01:43.305 
Epoch 267/1000 
	 loss: 27.9739, MinusLogProbMetric: 27.9739, val_loss: 28.6608, val_MinusLogProbMetric: 28.6608

Epoch 267: val_loss did not improve from 28.38246
196/196 - 69s - loss: 27.9739 - MinusLogProbMetric: 27.9739 - val_loss: 28.6608 - val_MinusLogProbMetric: 28.6608 - lr: 5.0000e-04 - 69s/epoch - 351ms/step
Epoch 268/1000
2023-09-30 11:02:55.789 
Epoch 268/1000 
	 loss: 27.9872, MinusLogProbMetric: 27.9872, val_loss: 28.5772, val_MinusLogProbMetric: 28.5772

Epoch 268: val_loss did not improve from 28.38246
196/196 - 72s - loss: 27.9872 - MinusLogProbMetric: 27.9872 - val_loss: 28.5772 - val_MinusLogProbMetric: 28.5772 - lr: 5.0000e-04 - 72s/epoch - 370ms/step
Epoch 269/1000
2023-09-30 11:04:07.984 
Epoch 269/1000 
	 loss: 27.9738, MinusLogProbMetric: 27.9738, val_loss: 28.7263, val_MinusLogProbMetric: 28.7263

Epoch 269: val_loss did not improve from 28.38246
196/196 - 72s - loss: 27.9738 - MinusLogProbMetric: 27.9738 - val_loss: 28.7263 - val_MinusLogProbMetric: 28.7263 - lr: 5.0000e-04 - 72s/epoch - 368ms/step
Epoch 270/1000
2023-09-30 11:05:20.619 
Epoch 270/1000 
	 loss: 27.9766, MinusLogProbMetric: 27.9766, val_loss: 28.5721, val_MinusLogProbMetric: 28.5721

Epoch 270: val_loss did not improve from 28.38246
196/196 - 73s - loss: 27.9766 - MinusLogProbMetric: 27.9766 - val_loss: 28.5721 - val_MinusLogProbMetric: 28.5721 - lr: 5.0000e-04 - 73s/epoch - 371ms/step
Epoch 271/1000
2023-09-30 11:06:29.569 
Epoch 271/1000 
	 loss: 27.9541, MinusLogProbMetric: 27.9541, val_loss: 28.7071, val_MinusLogProbMetric: 28.7071

Epoch 271: val_loss did not improve from 28.38246
196/196 - 69s - loss: 27.9541 - MinusLogProbMetric: 27.9541 - val_loss: 28.7071 - val_MinusLogProbMetric: 28.7071 - lr: 5.0000e-04 - 69s/epoch - 352ms/step
Epoch 272/1000
2023-09-30 11:07:41.866 
Epoch 272/1000 
	 loss: 27.9595, MinusLogProbMetric: 27.9595, val_loss: 28.6415, val_MinusLogProbMetric: 28.6415

Epoch 272: val_loss did not improve from 28.38246
196/196 - 72s - loss: 27.9595 - MinusLogProbMetric: 27.9595 - val_loss: 28.6415 - val_MinusLogProbMetric: 28.6415 - lr: 5.0000e-04 - 72s/epoch - 369ms/step
Epoch 273/1000
2023-09-30 11:08:53.764 
Epoch 273/1000 
	 loss: 28.0130, MinusLogProbMetric: 28.0130, val_loss: 28.3891, val_MinusLogProbMetric: 28.3891

Epoch 273: val_loss did not improve from 28.38246
196/196 - 72s - loss: 28.0130 - MinusLogProbMetric: 28.0130 - val_loss: 28.3891 - val_MinusLogProbMetric: 28.3891 - lr: 5.0000e-04 - 72s/epoch - 367ms/step
Epoch 274/1000
2023-09-30 11:10:03.893 
Epoch 274/1000 
	 loss: 27.9632, MinusLogProbMetric: 27.9632, val_loss: 28.7323, val_MinusLogProbMetric: 28.7323

Epoch 274: val_loss did not improve from 28.38246
196/196 - 70s - loss: 27.9632 - MinusLogProbMetric: 27.9632 - val_loss: 28.7323 - val_MinusLogProbMetric: 28.7323 - lr: 5.0000e-04 - 70s/epoch - 358ms/step
Epoch 275/1000
2023-09-30 11:11:09.653 
Epoch 275/1000 
	 loss: 28.0034, MinusLogProbMetric: 28.0034, val_loss: 28.4507, val_MinusLogProbMetric: 28.4507

Epoch 275: val_loss did not improve from 28.38246
196/196 - 66s - loss: 28.0034 - MinusLogProbMetric: 28.0034 - val_loss: 28.4507 - val_MinusLogProbMetric: 28.4507 - lr: 5.0000e-04 - 66s/epoch - 336ms/step
Epoch 276/1000
2023-09-30 11:12:19.133 
Epoch 276/1000 
	 loss: 27.9359, MinusLogProbMetric: 27.9359, val_loss: 28.6281, val_MinusLogProbMetric: 28.6281

Epoch 276: val_loss did not improve from 28.38246
196/196 - 69s - loss: 27.9359 - MinusLogProbMetric: 27.9359 - val_loss: 28.6281 - val_MinusLogProbMetric: 28.6281 - lr: 5.0000e-04 - 69s/epoch - 354ms/step
Epoch 277/1000
2023-09-30 11:13:29.681 
Epoch 277/1000 
	 loss: 27.9630, MinusLogProbMetric: 27.9630, val_loss: 28.7594, val_MinusLogProbMetric: 28.7594

Epoch 277: val_loss did not improve from 28.38246
196/196 - 71s - loss: 27.9630 - MinusLogProbMetric: 27.9630 - val_loss: 28.7594 - val_MinusLogProbMetric: 28.7594 - lr: 5.0000e-04 - 71s/epoch - 360ms/step
Epoch 278/1000
2023-09-30 11:14:40.181 
Epoch 278/1000 
	 loss: 28.0030, MinusLogProbMetric: 28.0030, val_loss: 28.7394, val_MinusLogProbMetric: 28.7394

Epoch 278: val_loss did not improve from 28.38246
196/196 - 70s - loss: 28.0030 - MinusLogProbMetric: 28.0030 - val_loss: 28.7394 - val_MinusLogProbMetric: 28.7394 - lr: 5.0000e-04 - 70s/epoch - 360ms/step
Epoch 279/1000
2023-09-30 11:15:52.517 
Epoch 279/1000 
	 loss: 27.9400, MinusLogProbMetric: 27.9400, val_loss: 28.4591, val_MinusLogProbMetric: 28.4591

Epoch 279: val_loss did not improve from 28.38246
196/196 - 72s - loss: 27.9400 - MinusLogProbMetric: 27.9400 - val_loss: 28.4591 - val_MinusLogProbMetric: 28.4591 - lr: 5.0000e-04 - 72s/epoch - 369ms/step
Epoch 280/1000
2023-09-30 11:17:03.825 
Epoch 280/1000 
	 loss: 27.9362, MinusLogProbMetric: 27.9362, val_loss: 28.5445, val_MinusLogProbMetric: 28.5445

Epoch 280: val_loss did not improve from 28.38246
196/196 - 71s - loss: 27.9362 - MinusLogProbMetric: 27.9362 - val_loss: 28.5445 - val_MinusLogProbMetric: 28.5445 - lr: 5.0000e-04 - 71s/epoch - 364ms/step
Epoch 281/1000
2023-09-30 11:18:13.585 
Epoch 281/1000 
	 loss: 27.9659, MinusLogProbMetric: 27.9659, val_loss: 28.3172, val_MinusLogProbMetric: 28.3172

Epoch 281: val_loss improved from 28.38246 to 28.31718, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_346/weights/best_weights.h5
196/196 - 72s - loss: 27.9659 - MinusLogProbMetric: 27.9659 - val_loss: 28.3172 - val_MinusLogProbMetric: 28.3172 - lr: 5.0000e-04 - 72s/epoch - 368ms/step
Epoch 282/1000
2023-09-30 11:19:24.451 
Epoch 282/1000 
	 loss: 27.9322, MinusLogProbMetric: 27.9322, val_loss: 28.3925, val_MinusLogProbMetric: 28.3925

Epoch 282: val_loss did not improve from 28.31718
196/196 - 68s - loss: 27.9322 - MinusLogProbMetric: 27.9322 - val_loss: 28.3925 - val_MinusLogProbMetric: 28.3925 - lr: 5.0000e-04 - 68s/epoch - 349ms/step
Epoch 283/1000
2023-09-30 11:20:43.489 
Epoch 283/1000 
	 loss: 27.9914, MinusLogProbMetric: 27.9914, val_loss: 28.4584, val_MinusLogProbMetric: 28.4584

Epoch 283: val_loss did not improve from 28.31718
196/196 - 79s - loss: 27.9914 - MinusLogProbMetric: 27.9914 - val_loss: 28.4584 - val_MinusLogProbMetric: 28.4584 - lr: 5.0000e-04 - 79s/epoch - 403ms/step
Epoch 284/1000
2023-09-30 11:21:55.950 
Epoch 284/1000 
	 loss: 27.9615, MinusLogProbMetric: 27.9615, val_loss: 28.5636, val_MinusLogProbMetric: 28.5636

Epoch 284: val_loss did not improve from 28.31718
196/196 - 72s - loss: 27.9615 - MinusLogProbMetric: 27.9615 - val_loss: 28.5636 - val_MinusLogProbMetric: 28.5636 - lr: 5.0000e-04 - 72s/epoch - 370ms/step
Epoch 285/1000
2023-09-30 11:23:07.061 
Epoch 285/1000 
	 loss: 27.9384, MinusLogProbMetric: 27.9384, val_loss: 28.6148, val_MinusLogProbMetric: 28.6148

Epoch 285: val_loss did not improve from 28.31718
196/196 - 71s - loss: 27.9384 - MinusLogProbMetric: 27.9384 - val_loss: 28.6148 - val_MinusLogProbMetric: 28.6148 - lr: 5.0000e-04 - 71s/epoch - 363ms/step
Epoch 286/1000
2023-09-30 11:24:20.064 
Epoch 286/1000 
	 loss: 27.9257, MinusLogProbMetric: 27.9257, val_loss: 29.0945, val_MinusLogProbMetric: 29.0945

Epoch 286: val_loss did not improve from 28.31718
196/196 - 73s - loss: 27.9257 - MinusLogProbMetric: 27.9257 - val_loss: 29.0945 - val_MinusLogProbMetric: 29.0945 - lr: 5.0000e-04 - 73s/epoch - 372ms/step
Epoch 287/1000
2023-09-30 11:25:32.512 
Epoch 287/1000 
	 loss: 28.0258, MinusLogProbMetric: 28.0258, val_loss: 28.6326, val_MinusLogProbMetric: 28.6326

Epoch 287: val_loss did not improve from 28.31718
196/196 - 72s - loss: 28.0258 - MinusLogProbMetric: 28.0258 - val_loss: 28.6326 - val_MinusLogProbMetric: 28.6326 - lr: 5.0000e-04 - 72s/epoch - 370ms/step
Epoch 288/1000
2023-09-30 11:26:44.733 
Epoch 288/1000 
	 loss: 27.8900, MinusLogProbMetric: 27.8900, val_loss: 28.5692, val_MinusLogProbMetric: 28.5692

Epoch 288: val_loss did not improve from 28.31718
196/196 - 72s - loss: 27.8900 - MinusLogProbMetric: 27.8900 - val_loss: 28.5692 - val_MinusLogProbMetric: 28.5692 - lr: 5.0000e-04 - 72s/epoch - 369ms/step
Epoch 289/1000
2023-09-30 11:27:54.362 
Epoch 289/1000 
	 loss: 27.9985, MinusLogProbMetric: 27.9985, val_loss: 28.3339, val_MinusLogProbMetric: 28.3339

Epoch 289: val_loss did not improve from 28.31718
196/196 - 70s - loss: 27.9985 - MinusLogProbMetric: 27.9985 - val_loss: 28.3339 - val_MinusLogProbMetric: 28.3339 - lr: 5.0000e-04 - 70s/epoch - 355ms/step
Epoch 290/1000
2023-09-30 11:29:07.381 
Epoch 290/1000 
	 loss: 27.9282, MinusLogProbMetric: 27.9282, val_loss: 28.3636, val_MinusLogProbMetric: 28.3636

Epoch 290: val_loss did not improve from 28.31718
196/196 - 73s - loss: 27.9282 - MinusLogProbMetric: 27.9282 - val_loss: 28.3636 - val_MinusLogProbMetric: 28.3636 - lr: 5.0000e-04 - 73s/epoch - 372ms/step
Epoch 291/1000
2023-09-30 11:30:19.766 
Epoch 291/1000 
	 loss: 27.9668, MinusLogProbMetric: 27.9668, val_loss: 28.3711, val_MinusLogProbMetric: 28.3711

Epoch 291: val_loss did not improve from 28.31718
196/196 - 72s - loss: 27.9668 - MinusLogProbMetric: 27.9668 - val_loss: 28.3711 - val_MinusLogProbMetric: 28.3711 - lr: 5.0000e-04 - 72s/epoch - 369ms/step
Epoch 292/1000
2023-09-30 11:31:31.468 
Epoch 292/1000 
	 loss: 27.9231, MinusLogProbMetric: 27.9231, val_loss: 28.4847, val_MinusLogProbMetric: 28.4847

Epoch 292: val_loss did not improve from 28.31718
196/196 - 72s - loss: 27.9231 - MinusLogProbMetric: 27.9231 - val_loss: 28.4847 - val_MinusLogProbMetric: 28.4847 - lr: 5.0000e-04 - 72s/epoch - 366ms/step
Epoch 293/1000
