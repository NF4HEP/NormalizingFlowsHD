2023-10-27 12:56:15.459249: Importing os...
2023-10-27 12:56:15.459325: Importing sys...
2023-10-27 12:56:15.459340: Importing and initializing argparse...
Visible devices: [1]
2023-10-27 12:56:15.478069: Importing timer from timeit...
2023-10-27 12:56:15.478696: Setting env variables for tf import (only device [1] will be available)...
2023-10-27 12:56:15.478741: Importing numpy...
2023-10-27 12:56:15.663151: Importing pandas...
2023-10-27 12:56:15.864150: Importing shutil...
2023-10-27 12:56:15.864176: Importing subprocess...
2023-10-27 12:56:15.864184: Importing tensorflow...
Tensorflow version: 2.12.0
2023-10-27 12:56:18.077182: Importing tensorflow_probability...
Tensorflow probability version: 0.20.1
2023-10-27 12:56:18.475449: Importing textwrap...
2023-10-27 12:56:18.475478: Importing timeit...
2023-10-27 12:56:18.475488: Importing traceback...
2023-10-27 12:56:18.475495: Importing typing...
2023-10-27 12:56:18.475505: Setting tf configs...
2023-10-27 12:56:18.811298: Importing custom module...
Successfully loaded GPU model: NVIDIA A40
2023-10-27 12:56:20.240161: All modues imported successfully.
Directory ../../results/CsplineN_new/ already exists.
Directory ../../results/CsplineN_new/run_1/ already exists.
Skipping it.
===========
Run 1/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_2/ already exists.
Skipping it.
===========
Run 2/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_3/ already exists.
Skipping it.
===========
Run 3/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_4/ already exists.
Skipping it.
===========
Run 4/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_5/ already exists.
Skipping it.
===========
Run 5/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_6/ already exists.
Skipping it.
===========
Run 6/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_7/ already exists.
Skipping it.
===========
Run 7/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_8/ already exists.
Skipping it.
===========
Run 8/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_9/ already exists.
Skipping it.
===========
Run 9/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_10/ already exists.
Skipping it.
===========
Run 10/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_11/ already exists.
Skipping it.
===========
Run 11/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_12/ already exists.
Skipping it.
===========
Run 12/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_13/ already exists.
Skipping it.
===========
Run 13/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_14/ already exists.
Skipping it.
===========
Run 14/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_15/ already exists.
Skipping it.
===========
Run 15/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_16/ already exists.
Skipping it.
===========
Run 16/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_17/ already exists.
Skipping it.
===========
Run 17/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_18/ already exists.
Skipping it.
===========
Run 18/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_19/ already exists.
Skipping it.
===========
Run 19/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_20/ already exists.
Skipping it.
===========
Run 20/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_21/ already exists.
Skipping it.
===========
Run 21/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_22/ already exists.
Skipping it.
===========
Run 22/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_23/ already exists.
Skipping it.
===========
Run 23/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_24/ already exists.
Skipping it.
===========
Run 24/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_25/ already exists.
Skipping it.
===========
Run 25/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_26/ already exists.
Skipping it.
===========
Run 26/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_27/ already exists.
Skipping it.
===========
Run 27/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_28/ already exists.
Skipping it.
===========
Run 28/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_29/ already exists.
Skipping it.
===========
Run 29/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_30/ already exists.
Skipping it.
===========
Run 30/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_31/ already exists.
Skipping it.
===========
Run 31/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_32/ already exists.
Skipping it.
===========
Run 32/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_33/ already exists.
Skipping it.
===========
Run 33/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_34/ already exists.
Skipping it.
===========
Run 34/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_35/ already exists.
Skipping it.
===========
Run 35/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_36/ already exists.
Skipping it.
===========
Run 36/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_37/ already exists.
Skipping it.
===========
Run 37/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_38/ already exists.
Skipping it.
===========
Run 38/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_39/ already exists.
Skipping it.
===========
Run 39/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_40/ already exists.
Skipping it.
===========
Run 40/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_41/ already exists.
Skipping it.
===========
Run 41/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_42/ already exists.
Skipping it.
===========
Run 42/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_43/ already exists.
Skipping it.
===========
Run 43/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_44/ already exists.
Skipping it.
===========
Run 44/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_45/ already exists.
Skipping it.
===========
Run 45/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_46/ already exists.
Skipping it.
===========
Run 46/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_47/ already exists.
Skipping it.
===========
Run 47/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_48/ already exists.
Skipping it.
===========
Run 48/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_49/ already exists.
Skipping it.
===========
Run 49/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_50/ already exists.
Skipping it.
===========
Run 50/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_51/ already exists.
Skipping it.
===========
Run 51/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_52/ already exists.
Skipping it.
===========
Run 52/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_53/ already exists.
Skipping it.
===========
Run 53/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_54/ already exists.
Skipping it.
===========
Run 54/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_55/ already exists.
Skipping it.
===========
Run 55/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_56/ already exists.
Skipping it.
===========
Run 56/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_57/ already exists.
Skipping it.
===========
Run 57/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_58/ already exists.
Skipping it.
===========
Run 58/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_59/ already exists.
Skipping it.
===========
Run 59/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_60/ already exists.
Skipping it.
===========
Run 60/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_61/ already exists.
Skipping it.
===========
Run 61/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_62/ already exists.
Skipping it.
===========
Run 62/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_63/ already exists.
Skipping it.
===========
Run 63/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_64/ already exists.
Skipping it.
===========
Run 64/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_65/ already exists.
Skipping it.
===========
Run 65/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_66/ already exists.
Skipping it.
===========
Run 66/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_67/ already exists.
Skipping it.
===========
Run 67/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_68/ already exists.
Skipping it.
===========
Run 68/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_69/ already exists.
Skipping it.
===========
Run 69/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_70/ already exists.
Skipping it.
===========
Run 70/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_71/ already exists.
Skipping it.
===========
Run 71/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_72/ already exists.
Skipping it.
===========
Run 72/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_73/ already exists.
Skipping it.
===========
Run 73/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_74/ already exists.
Skipping it.
===========
Run 74/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_75/ already exists.
Skipping it.
===========
Run 75/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_76/ already exists.
Skipping it.
===========
Run 76/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_77/ already exists.
Skipping it.
===========
Run 77/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_78/ already exists.
Skipping it.
===========
Run 78/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_79/ already exists.
Skipping it.
===========
Run 79/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_80/ already exists.
Skipping it.
===========
Run 80/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_81/ already exists.
Skipping it.
===========
Run 81/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_82/ already exists.
Skipping it.
===========
Run 82/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_83/ already exists.
Skipping it.
===========
Run 83/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_84/ already exists.
Skipping it.
===========
Run 84/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_85/ already exists.
Skipping it.
===========
Run 85/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_86/ already exists.
Skipping it.
===========
Run 86/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_87/ already exists.
Skipping it.
===========
Run 87/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_88/ already exists.
Skipping it.
===========
Run 88/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_89/ already exists.
Skipping it.
===========
Run 89/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_90/ already exists.
Skipping it.
===========
Run 90/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_91/ already exists.
Skipping it.
===========
Run 91/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_92/ already exists.
Skipping it.
===========
Run 92/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_93/ already exists.
Skipping it.
===========
Run 93/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_94/ already exists.
Skipping it.
===========
Run 94/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_95/ already exists.
Skipping it.
===========
Run 95/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_96/ already exists.
Skipping it.
===========
Run 96/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_97/ already exists.
Skipping it.
===========
Run 97/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_98/ already exists.
Skipping it.
===========
Run 98/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_99/ already exists.
Skipping it.
===========
Run 99/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_100/ already exists.
Skipping it.
===========
Run 100/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_101/ already exists.
Skipping it.
===========
Run 101/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_102/ already exists.
Skipping it.
===========
Run 102/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_103/ already exists.
Skipping it.
===========
Run 103/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_104/ already exists.
Skipping it.
===========
Run 104/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_105/ already exists.
Skipping it.
===========
Run 105/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_106/ already exists.
Skipping it.
===========
Run 106/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_107/ already exists.
Skipping it.
===========
Run 107/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_108/ already exists.
Skipping it.
===========
Run 108/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_109/ already exists.
Skipping it.
===========
Run 109/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_110/ already exists.
Skipping it.
===========
Run 110/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_111/ already exists.
Skipping it.
===========
Run 111/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_112/ already exists.
Skipping it.
===========
Run 112/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_113/ already exists.
Skipping it.
===========
Run 113/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_114/ already exists.
Skipping it.
===========
Run 114/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_115/ already exists.
Skipping it.
===========
Run 115/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_116/ already exists.
Skipping it.
===========
Run 116/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_117/ already exists.
Skipping it.
===========
Run 117/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_118/ already exists.
Skipping it.
===========
Run 118/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_119/ already exists.
Skipping it.
===========
Run 119/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_120/ already exists.
Skipping it.
===========
Run 120/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_121/ already exists.
Skipping it.
===========
Run 121/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_122/ already exists.
Skipping it.
===========
Run 122/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_123/ already exists.
Skipping it.
===========
Run 123/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_124/ already exists.
Skipping it.
===========
Run 124/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_125/ already exists.
Skipping it.
===========
Run 125/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_126/ already exists.
Skipping it.
===========
Run 126/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_127/ already exists.
Skipping it.
===========
Run 127/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_128/ already exists.
Skipping it.
===========
Run 128/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_129/ already exists.
Skipping it.
===========
Run 129/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_130/ already exists.
Skipping it.
===========
Run 130/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_131/ already exists.
Skipping it.
===========
Run 131/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_132/ already exists.
Skipping it.
===========
Run 132/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_133/ already exists.
Skipping it.
===========
Run 133/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_134/ already exists.
Skipping it.
===========
Run 134/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_135/ already exists.
Skipping it.
===========
Run 135/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_136/ already exists.
Skipping it.
===========
Run 136/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_137/ already exists.
Skipping it.
===========
Run 137/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_138/ already exists.
Skipping it.
===========
Run 138/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_139/ already exists.
Skipping it.
===========
Run 139/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_140/ already exists.
Skipping it.
===========
Run 140/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_141/ already exists.
Skipping it.
===========
Run 141/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_142/ already exists.
Skipping it.
===========
Run 142/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_143/ already exists.
Skipping it.
===========
Run 143/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_144/ already exists.
Skipping it.
===========
Run 144/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_145/ already exists.
Skipping it.
===========
Run 145/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_146/ already exists.
Skipping it.
===========
Run 146/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_147/ already exists.
Skipping it.
===========
Run 147/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_148/ already exists.
Skipping it.
===========
Run 148/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_149/ already exists.
Skipping it.
===========
Run 149/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_150/ already exists.
Skipping it.
===========
Run 150/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_151/ already exists.
Skipping it.
===========
Run 151/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_152/ already exists.
Skipping it.
===========
Run 152/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_153/ already exists.
Skipping it.
===========
Run 153/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_154/ already exists.
Skipping it.
===========
Run 154/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_155/ already exists.
Skipping it.
===========
Run 155/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_156/ already exists.
Skipping it.
===========
Run 156/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_157/ already exists.
Skipping it.
===========
Run 157/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_158/ already exists.
Skipping it.
===========
Run 158/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_159/ already exists.
Skipping it.
===========
Run 159/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_160/ already exists.
Skipping it.
===========
Run 160/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_161/ already exists.
Skipping it.
===========
Run 161/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_162/ already exists.
Skipping it.
===========
Run 162/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_163/ already exists.
Skipping it.
===========
Run 163/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_164/ already exists.
Skipping it.
===========
Run 164/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_165/ already exists.
Skipping it.
===========
Run 165/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_166/ already exists.
Skipping it.
===========
Run 166/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_167/ already exists.
Skipping it.
===========
Run 167/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_168/ already exists.
Skipping it.
===========
Run 168/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_169/ already exists.
Skipping it.
===========
Run 169/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_170/ already exists.
Skipping it.
===========
Run 170/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_171/ already exists.
Skipping it.
===========
Run 171/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_172/ already exists.
Skipping it.
===========
Run 172/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_173/ already exists.
Skipping it.
===========
Run 173/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_174/ already exists.
Skipping it.
===========
Run 174/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_175/ already exists.
Skipping it.
===========
Run 175/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_176/ already exists.
Skipping it.
===========
Run 176/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_177/ already exists.
Skipping it.
===========
Run 177/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_178/ already exists.
Skipping it.
===========
Run 178/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_179/ already exists.
Skipping it.
===========
Run 179/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_180/ already exists.
Skipping it.
===========
Run 180/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_181/ already exists.
Skipping it.
===========
Run 181/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_182/ already exists.
Skipping it.
===========
Run 182/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_183/ already exists.
Skipping it.
===========
Run 183/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_184/ already exists.
Skipping it.
===========
Run 184/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_185/ already exists.
Skipping it.
===========
Run 185/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_186/ already exists.
Skipping it.
===========
Run 186/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_187/ already exists.
Skipping it.
===========
Run 187/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_188/ already exists.
Skipping it.
===========
Run 188/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_189/ already exists.
Skipping it.
===========
Run 189/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_190/ already exists.
Skipping it.
===========
Run 190/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_191/ already exists.
Skipping it.
===========
Run 191/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_192/ already exists.
Skipping it.
===========
Run 192/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_193/ already exists.
Skipping it.
===========
Run 193/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_194/ already exists.
Skipping it.
===========
Run 194/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_195/ already exists.
Skipping it.
===========
Run 195/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_196/ already exists.
Skipping it.
===========
Run 196/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_197/ already exists.
Skipping it.
===========
Run 197/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_198/ already exists.
Skipping it.
===========
Run 198/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_199/ already exists.
Skipping it.
===========
Run 199/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_200/ already exists.
Skipping it.
===========
Run 200/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_201/ already exists.
Skipping it.
===========
Run 201/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_202/ already exists.
Skipping it.
===========
Run 202/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_203/ already exists.
Skipping it.
===========
Run 203/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_204/ already exists.
Skipping it.
===========
Run 204/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_205/ already exists.
Skipping it.
===========
Run 205/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_206/ already exists.
Skipping it.
===========
Run 206/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_207/ already exists.
Skipping it.
===========
Run 207/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_208/ already exists.
Skipping it.
===========
Run 208/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_209/ already exists.
Skipping it.
===========
Run 209/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_210/ already exists.
Skipping it.
===========
Run 210/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_211/ already exists.
Skipping it.
===========
Run 211/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_212/ already exists.
Skipping it.
===========
Run 212/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_213/ already exists.
Skipping it.
===========
Run 213/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_214/ already exists.
Skipping it.
===========
Run 214/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_215/ already exists.
Skipping it.
===========
Run 215/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_216/ already exists.
Skipping it.
===========
Run 216/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_217/ already exists.
Skipping it.
===========
Run 217/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_218/ already exists.
Skipping it.
===========
Run 218/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_219/ already exists.
Skipping it.
===========
Run 219/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_220/ already exists.
Skipping it.
===========
Run 220/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_221/ already exists.
Skipping it.
===========
Run 221/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_222/ already exists.
Skipping it.
===========
Run 222/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_223/ already exists.
Skipping it.
===========
Run 223/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_224/ already exists.
Skipping it.
===========
Run 224/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_225/ already exists.
Skipping it.
===========
Run 225/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_226/ already exists.
Skipping it.
===========
Run 226/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_227/ already exists.
Skipping it.
===========
Run 227/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_228/ already exists.
Skipping it.
===========
Run 228/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_229/ already exists.
Skipping it.
===========
Run 229/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_230/ already exists.
Skipping it.
===========
Run 230/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_231/ already exists.
Skipping it.
===========
Run 231/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_232/ already exists.
Skipping it.
===========
Run 232/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_233/ already exists.
Skipping it.
===========
Run 233/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_234/ already exists.
Skipping it.
===========
Run 234/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_235/ already exists.
Skipping it.
===========
Run 235/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_236/ already exists.
Skipping it.
===========
Run 236/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_237/ already exists.
Skipping it.
===========
Run 237/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_238/ already exists.
Skipping it.
===========
Run 238/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_239/ already exists.
Skipping it.
===========
Run 239/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_240/ already exists.
Skipping it.
===========
Run 240/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_241/ already exists.
Skipping it.
===========
Run 241/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_242/ already exists.
Skipping it.
===========
Run 242/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_243/ already exists.
Skipping it.
===========
Run 243/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_244/ already exists.
Skipping it.
===========
Run 244/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_245/ already exists.
Skipping it.
===========
Run 245/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_246/ already exists.
Skipping it.
===========
Run 246/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_247/ already exists.
Skipping it.
===========
Run 247/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_248/ already exists.
Skipping it.
===========
Run 248/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_249/ already exists.
Skipping it.
===========
Run 249/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_250/ already exists.
Skipping it.
===========
Run 250/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_251/ already exists.
Skipping it.
===========
Run 251/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_252/ already exists.
Skipping it.
===========
Run 252/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_253/ already exists.
Skipping it.
===========
Run 253/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_254/ already exists.
Skipping it.
===========
Run 254/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_255/ already exists.
Skipping it.
===========
Run 255/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_256/ already exists.
Skipping it.
===========
Run 256/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_257/ already exists.
Skipping it.
===========
Run 257/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_258/ already exists.
Skipping it.
===========
Run 258/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_259/ already exists.
Skipping it.
===========
Run 259/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_260/ already exists.
Skipping it.
===========
Run 260/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_261/ already exists.
Skipping it.
===========
Run 261/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_262/ already exists.
Skipping it.
===========
Run 262/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_263/ already exists.
Skipping it.
===========
Run 263/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_264/ already exists.
Skipping it.
===========
Run 264/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_265/ already exists.
Skipping it.
===========
Run 265/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_266/ already exists.
Skipping it.
===========
Run 266/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_267/ already exists.
Skipping it.
===========
Run 267/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_268/ already exists.
Skipping it.
===========
Run 268/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_269/ already exists.
Skipping it.
===========
Run 269/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_270/ already exists.
Skipping it.
===========
Run 270/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_271/ already exists.
Skipping it.
===========
Run 271/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_272/ already exists.
Skipping it.
===========
Run 272/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_273/ already exists.
Skipping it.
===========
Run 273/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_274/ already exists.
Skipping it.
===========
Run 274/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_275/ already exists.
Skipping it.
===========
Run 275/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_276/ already exists.
Skipping it.
===========
Run 276/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_277/ already exists.
Skipping it.
===========
Run 277/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_278/ already exists.
Skipping it.
===========
Run 278/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_279/ already exists.
Skipping it.
===========
Run 279/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_280/ already exists.
Skipping it.
===========
Run 280/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_281/ already exists.
Skipping it.
===========
Run 281/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_282/ already exists.
Skipping it.
===========
Run 282/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_283/ already exists.
Skipping it.
===========
Run 283/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_284/ already exists.
Skipping it.
===========
Run 284/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_285/ already exists.
Skipping it.
===========
Run 285/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_286/ already exists.
Skipping it.
===========
Run 286/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_287/ already exists.
Skipping it.
===========
Run 287/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_288/ already exists.
Skipping it.
===========
Run 288/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_289/ already exists.
Skipping it.
===========
Run 289/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_290/ already exists.
Skipping it.
===========
Run 290/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_291/ already exists.
Skipping it.
===========
Run 291/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_292/ already exists.
Skipping it.
===========
Run 292/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_293/ already exists.
Skipping it.
===========
Run 293/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_294/ already exists.
Skipping it.
===========
Run 294/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_295/ already exists.
Skipping it.
===========
Run 295/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_296/ already exists.
Skipping it.
===========
Run 296/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_297/ already exists.
Skipping it.
===========
Run 297/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_298/ already exists.
Skipping it.
===========
Run 298/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_299/ already exists.
Skipping it.
===========
Run 299/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_300/ already exists.
Skipping it.
===========
Run 300/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_301/ already exists.
Skipping it.
===========
Run 301/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_302/ already exists.
Skipping it.
===========
Run 302/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_303/ already exists.
Skipping it.
===========
Run 303/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_304/ already exists.
Skipping it.
===========
Run 304/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_305/ already exists.
Skipping it.
===========
Run 305/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_306/ already exists.
Skipping it.
===========
Run 306/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_307/ already exists.
Skipping it.
===========
Run 307/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_308/ already exists.
Skipping it.
===========
Run 308/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_309/ already exists.
Skipping it.
===========
Run 309/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_310/ already exists.
Skipping it.
===========
Run 310/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_311/ already exists.
Skipping it.
===========
Run 311/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_312/ already exists.
Skipping it.
===========
Run 312/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_313/ already exists.
Skipping it.
===========
Run 313/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_314/ already exists.
Skipping it.
===========
Run 314/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_315/ already exists.
Skipping it.
===========
Run 315/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_316/ already exists.
Skipping it.
===========
Run 316/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_317/ already exists.
Skipping it.
===========
Run 317/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_318/ already exists.
Skipping it.
===========
Run 318/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_319/ already exists.
Skipping it.
===========
Run 319/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_320/ already exists.
Skipping it.
===========
Run 320/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_321/ already exists.
Skipping it.
===========
Run 321/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_322/ already exists.
Skipping it.
===========
Run 322/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_323/ already exists.
Skipping it.
===========
Run 323/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_324/ already exists.
Skipping it.
===========
Run 324/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_325/ already exists.
Skipping it.
===========
Run 325/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_326/ already exists.
Skipping it.
===========
Run 326/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_327/ already exists.
Skipping it.
===========
Run 327/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_328/ already exists.
Skipping it.
===========
Run 328/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_329/ already exists.
Skipping it.
===========
Run 329/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_330/ already exists.
Skipping it.
===========
Run 330/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_331/ already exists.
Skipping it.
===========
Run 331/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_332/ already exists.
Skipping it.
===========
Run 332/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_333/ already exists.
Skipping it.
===========
Run 333/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_334/ already exists.
Skipping it.
===========
Run 334/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_335/ already exists.
Skipping it.
===========
Run 335/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_336/ already exists.
Skipping it.
===========
Run 336/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_337/ already exists.
Skipping it.
===========
Run 337/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_338/ already exists.
Skipping it.
===========
Run 338/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_339/ already exists.
Skipping it.
===========
Run 339/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_340/ already exists.
Skipping it.
===========
Run 340/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_341/ already exists.
Skipping it.
===========
Run 341/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_342/ already exists.
Skipping it.
===========
Run 342/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_343/ already exists.
Skipping it.
===========
Run 343/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_344/ already exists.
Skipping it.
===========
Run 344/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_345/ already exists.
Skipping it.
===========
Run 345/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_346/ already exists.
Skipping it.
===========
Run 346/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_347/ already exists.
Skipping it.
===========
Run 347/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_348/ already exists.
Skipping it.
===========
Run 348/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_349/ already exists.
Skipping it.
===========
Run 349/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_350/ already exists.
Skipping it.
===========
Run 350/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_351/ already exists.
Skipping it.
===========
Run 351/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_352/ already exists.
Skipping it.
===========
Run 352/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_353/ already exists.
Skipping it.
===========
Run 353/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_354/ already exists.
Skipping it.
===========
Run 354/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_355/ already exists.
Skipping it.
===========
Run 355/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_356/ already exists.
Skipping it.
===========
Run 356/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_357/ already exists.
Skipping it.
===========
Run 357/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_358/ already exists.
Skipping it.
===========
Run 358/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_359/ already exists.
Skipping it.
===========
Run 359/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_360/ already exists.
Skipping it.
===========
Run 360/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_361/ already exists.
Skipping it.
===========
Run 361/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_362/ already exists.
Skipping it.
===========
Run 362/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_363/ already exists.
Skipping it.
===========
Run 363/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_364/ already exists.
Skipping it.
===========
Run 364/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_365/ already exists.
Skipping it.
===========
Run 365/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_366/ already exists.
Skipping it.
===========
Run 366/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_367/ already exists.
Skipping it.
===========
Run 367/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_368/ already exists.
Skipping it.
===========
Run 368/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_369/ already exists.
Skipping it.
===========
Run 369/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_370/ already exists.
Skipping it.
===========
Run 370/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_371/ already exists.
Skipping it.
===========
Run 371/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_372/ already exists.
Skipping it.
===========
Run 372/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_373/ already exists.
Skipping it.
===========
Run 373/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_374/ already exists.
Skipping it.
===========
Run 374/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_375/ already exists.
Skipping it.
===========
Run 375/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_376/ already exists.
Skipping it.
===========
Run 376/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_377/ already exists.
Skipping it.
===========
Run 377/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_378/ already exists.
Skipping it.
===========
Run 378/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_379/ already exists.
Skipping it.
===========
Run 379/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_380/ already exists.
Skipping it.
===========
Run 380/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_381/ already exists.
Skipping it.
===========
Run 381/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_382/ already exists.
Skipping it.
===========
Run 382/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_383/ already exists.
Skipping it.
===========
Run 383/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_384/ already exists.
Skipping it.
===========
Run 384/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_385/ already exists.
Skipping it.
===========
Run 385/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_386/ already exists.
Skipping it.
===========
Run 386/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_387/ already exists.
Skipping it.
===========
Run 387/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_388/ already exists.
Skipping it.
===========
Run 388/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_389/ already exists.
Skipping it.
===========
Run 389/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_390/ already exists.
Skipping it.
===========
Run 390/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_391/ already exists.
Skipping it.
===========
Run 391/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_392/ already exists.
Skipping it.
===========
Run 392/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_393/ already exists.
Skipping it.
===========
Run 393/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_394/ already exists.
Skipping it.
===========
Run 394/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_395/ already exists.
Skipping it.
===========
Run 395/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_396/ already exists.
Skipping it.
===========
Run 396/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_397/ already exists.
Skipping it.
===========
Run 397/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_398/ already exists.
Skipping it.
===========
Run 398/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_399/ already exists.
Skipping it.
===========
Run 399/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_400/ already exists.
Skipping it.
===========
Run 400/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_401/ already exists.
Skipping it.
===========
Run 401/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_402/ already exists.
Skipping it.
===========
Run 402/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_403/ already exists.
Skipping it.
===========
Run 403/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_404/ already exists.
Skipping it.
===========
Run 404/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_405/ already exists.
Skipping it.
===========
Run 405/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_406/ already exists.
Skipping it.
===========
Run 406/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_407/ already exists.
Skipping it.
===========
Run 407/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_408/ already exists.
Skipping it.
===========
Run 408/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_409/ already exists.
Skipping it.
===========
Run 409/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_410/ already exists.
Skipping it.
===========
Run 410/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_411/ already exists.
Skipping it.
===========
Run 411/720 already exists. Skipping it.
===========

===========
Generating train data for run 412.
===========
Train data generated in 0.39 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_412/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_412/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_412/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_412
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_5"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 100)]             0         
                                                                 
 log_prob_layer (LogProbLaye  (None,)                  2971950   
 r)                                                              
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer'")
self.model: <keras.engine.functional.Functional object at 0x7f77447ab6d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f77657b4a60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f77657b4a60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f776579b310>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7765799810>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f77658153f0>, <keras.callbacks.ModelCheckpoint object at 0x7f7765814dc0>, <keras.callbacks.EarlyStopping object at 0x7f7765815240>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7765814820>, <keras.callbacks.TerminateOnNaN object at 0x7f77658150f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_412/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 412/720 with hyperparameters:
timestamp = 2023-10-27 12:56:26.620847
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 4: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 12:57:49.294 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 9004.8086, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 82s - loss: nan - MinusLogProbMetric: 9004.8086 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 82s/epoch - 421ms/step
The loss history contains NaN values.
Training failed: trying again with seed 511595 and lr 0.0003333333333333333.
===========
Generating train data for run 412.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_412/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 187}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_412/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_412/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_412
self.data_kwargs: {'seed': 187}
self.x_data: [[ 3.363249    6.7996144   3.7558484  ...  8.487494    9.4672365
   8.406783  ]
 [ 6.429963    1.6688861   7.1802053  ...  2.2553494   0.6794903
   4.4349604 ]
 [ 6.456467    3.3944483   7.3872457  ...  2.453699    0.87060153
   4.491277  ]
 ...
 [ 5.9895535   1.3493791   7.20848    ...  1.7700427   0.7181699
   3.9588675 ]
 [ 3.903314    7.1540666   4.520662   ...  7.9577374   9.63259
  10.554146  ]
 [ 7.108661    3.9697583   7.446551   ...  2.5551188   0.477225
   4.0121694 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_11"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_1 (LogProbLa  (None,)                  2971950   
 yer)                                                            
                                                                 
=================================================================
Total params: 2,971,950
Trainable params: 2,971,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_1/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_1'")
self.model: <keras.engine.functional.Functional object at 0x7f7b03c9d270>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7b03b7f3a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7b03b7f3a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7b04132a70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7b037f4be0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7b037f5150>, <keras.callbacks.ModelCheckpoint object at 0x7f7b037f5210>, <keras.callbacks.EarlyStopping object at 0x7f7b037f5480>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7b037f54b0>, <keras.callbacks.TerminateOnNaN object at 0x7f7b037f50f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.4419913 ,  1.5360464 ,  7.4556723 , ...,  2.6291485 ,
         0.51444   ,  4.109222  ],
       [ 3.153259  ,  6.8511314 ,  3.8978415 , ...,  9.545169  ,
        10.074345  ,  8.361531  ],
       [ 5.988738  ,  7.1984186 ,  6.0094676 , ..., -0.12096485,
         8.301349  , -0.19920124],
       ...,
       [ 6.81505   ,  0.92492115,  7.338936  , ...,  3.3879175 ,
         0.16174722,  3.00487   ],
       [ 2.5060794 ,  7.3628144 ,  3.204593  , ...,  8.143249  ,
         9.353718  , 10.2199135 ],
       [ 3.285063  ,  6.913296  ,  4.502696  , ...,  9.038563  ,
         9.846277  ,  9.374819  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_412/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 412/720 with hyperparameters:
timestamp = 2023-10-27 12:57:54.723947
ndims = 100
seed_train = 187
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2971950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.363249    6.7996144   3.7558484   1.6905175   0.59504455  1.3572187
  8.552164    4.573737    5.622487    8.850269    9.919881    2.442041
  6.4713607   2.4110885   0.582009    6.8260555   3.2736723   4.1556153
  6.284559    9.342276    6.8533273   8.768529    2.7136652   6.8141527
  1.8623005   9.99699     7.2033806   1.5403578   9.54195     7.607635
  2.5313134   2.061085    5.972526    0.7375989   1.931644    4.2606106
  3.528998    5.132032    4.133215    6.4308267   8.643526    0.59724224
  5.238656    0.6318855   5.801837    3.9359124   5.668258    2.8762863
  2.2584295   5.26404     4.043352    9.504596    7.6159754   7.311254
  8.6322365   0.7895291   5.4676733   5.602808   10.294471    4.03677
  2.139928    0.72304964 -1.6175498   8.732431    7.1576815   5.355942
  2.7106154   6.013588   -0.08663982  3.5826845   9.295919    8.744785
  3.5537617   9.7776165   1.4147863   9.605443    9.551717    8.587783
  5.6122823   8.676071    3.0853114   8.1522255   6.511031    0.04184827
  4.384617    1.688982    9.852707    4.786712    4.9854093   6.8219695
  3.8010216   2.489355    7.8810973   2.4671822   5.09907     2.941558
  0.59668285  8.487494    9.4672365   8.406783  ]
Epoch 1/1000
2023-10-27 12:59:56.708 
Epoch 1/1000 
	 loss: 2171.7468, MinusLogProbMetric: 2171.7468, val_loss: 659.6379, val_MinusLogProbMetric: 659.6379

Epoch 1: val_loss improved from inf to 659.63794, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 122s - loss: 2171.7468 - MinusLogProbMetric: 2171.7468 - val_loss: 659.6379 - val_MinusLogProbMetric: 659.6379 - lr: 3.3333e-04 - 122s/epoch - 624ms/step
Epoch 2/1000
2023-10-27 13:00:34.015 
Epoch 2/1000 
	 loss: 602.7699, MinusLogProbMetric: 602.7699, val_loss: 448.1242, val_MinusLogProbMetric: 448.1242

Epoch 2: val_loss improved from 659.63794 to 448.12421, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 602.7699 - MinusLogProbMetric: 602.7699 - val_loss: 448.1242 - val_MinusLogProbMetric: 448.1242 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 3/1000
2023-10-27 13:01:10.924 
Epoch 3/1000 
	 loss: 401.0105, MinusLogProbMetric: 401.0105, val_loss: 353.0610, val_MinusLogProbMetric: 353.0610

Epoch 3: val_loss improved from 448.12421 to 353.06100, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 401.0105 - MinusLogProbMetric: 401.0105 - val_loss: 353.0610 - val_MinusLogProbMetric: 353.0610 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 4/1000
2023-10-27 13:01:51.084 
Epoch 4/1000 
	 loss: 327.1512, MinusLogProbMetric: 327.1512, val_loss: 301.4337, val_MinusLogProbMetric: 301.4337

Epoch 4: val_loss improved from 353.06100 to 301.43365, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 40s - loss: 327.1512 - MinusLogProbMetric: 327.1512 - val_loss: 301.4337 - val_MinusLogProbMetric: 301.4337 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 5/1000
2023-10-27 13:02:28.872 
Epoch 5/1000 
	 loss: 249.5396, MinusLogProbMetric: 249.5396, val_loss: 207.9533, val_MinusLogProbMetric: 207.9533

Epoch 5: val_loss improved from 301.43365 to 207.95328, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 38s - loss: 249.5396 - MinusLogProbMetric: 249.5396 - val_loss: 207.9533 - val_MinusLogProbMetric: 207.9533 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 6/1000
2023-10-27 13:03:05.385 
Epoch 6/1000 
	 loss: 258.8919, MinusLogProbMetric: 258.8919, val_loss: 273.0183, val_MinusLogProbMetric: 273.0183

Epoch 6: val_loss did not improve from 207.95328
196/196 - 36s - loss: 258.8919 - MinusLogProbMetric: 258.8919 - val_loss: 273.0183 - val_MinusLogProbMetric: 273.0183 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 7/1000
2023-10-27 13:03:41.468 
Epoch 7/1000 
	 loss: 223.3143, MinusLogProbMetric: 223.3143, val_loss: 192.1102, val_MinusLogProbMetric: 192.1102

Epoch 7: val_loss improved from 207.95328 to 192.11015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 223.3143 - MinusLogProbMetric: 223.3143 - val_loss: 192.1102 - val_MinusLogProbMetric: 192.1102 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 8/1000
2023-10-27 13:04:19.305 
Epoch 8/1000 
	 loss: 179.8233, MinusLogProbMetric: 179.8233, val_loss: 161.5780, val_MinusLogProbMetric: 161.5780

Epoch 8: val_loss improved from 192.11015 to 161.57805, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 38s - loss: 179.8233 - MinusLogProbMetric: 179.8233 - val_loss: 161.5780 - val_MinusLogProbMetric: 161.5780 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 9/1000
2023-10-27 13:04:56.104 
Epoch 9/1000 
	 loss: 153.4972, MinusLogProbMetric: 153.4972, val_loss: 145.3551, val_MinusLogProbMetric: 145.3551

Epoch 9: val_loss improved from 161.57805 to 145.35506, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 153.4972 - MinusLogProbMetric: 153.4972 - val_loss: 145.3551 - val_MinusLogProbMetric: 145.3551 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 10/1000
2023-10-27 13:05:32.828 
Epoch 10/1000 
	 loss: 146.3669, MinusLogProbMetric: 146.3669, val_loss: 133.5382, val_MinusLogProbMetric: 133.5382

Epoch 10: val_loss improved from 145.35506 to 133.53816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 146.3669 - MinusLogProbMetric: 146.3669 - val_loss: 133.5382 - val_MinusLogProbMetric: 133.5382 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 11/1000
2023-10-27 13:06:09.033 
Epoch 11/1000 
	 loss: 128.1303, MinusLogProbMetric: 128.1303, val_loss: 123.1958, val_MinusLogProbMetric: 123.1958

Epoch 11: val_loss improved from 133.53816 to 123.19578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 36s - loss: 128.1303 - MinusLogProbMetric: 128.1303 - val_loss: 123.1958 - val_MinusLogProbMetric: 123.1958 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 12/1000
2023-10-27 13:06:46.203 
Epoch 12/1000 
	 loss: 119.0447, MinusLogProbMetric: 119.0447, val_loss: 115.9642, val_MinusLogProbMetric: 115.9642

Epoch 12: val_loss improved from 123.19578 to 115.96423, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 119.0447 - MinusLogProbMetric: 119.0447 - val_loss: 115.9642 - val_MinusLogProbMetric: 115.9642 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 13/1000
2023-10-27 13:07:22.255 
Epoch 13/1000 
	 loss: 115.1444, MinusLogProbMetric: 115.1444, val_loss: 112.2448, val_MinusLogProbMetric: 112.2448

Epoch 13: val_loss improved from 115.96423 to 112.24483, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 115.1444 - MinusLogProbMetric: 115.1444 - val_loss: 112.2448 - val_MinusLogProbMetric: 112.2448 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 14/1000
2023-10-27 13:07:59.350 
Epoch 14/1000 
	 loss: 108.1237, MinusLogProbMetric: 108.1237, val_loss: 105.3694, val_MinusLogProbMetric: 105.3694

Epoch 14: val_loss improved from 112.24483 to 105.36943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 108.1237 - MinusLogProbMetric: 108.1237 - val_loss: 105.3694 - val_MinusLogProbMetric: 105.3694 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 15/1000
2023-10-27 13:08:36.148 
Epoch 15/1000 
	 loss: 102.0720, MinusLogProbMetric: 102.0720, val_loss: 101.2278, val_MinusLogProbMetric: 101.2278

Epoch 15: val_loss improved from 105.36943 to 101.22781, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 102.0720 - MinusLogProbMetric: 102.0720 - val_loss: 101.2278 - val_MinusLogProbMetric: 101.2278 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 16/1000
2023-10-27 13:09:12.617 
Epoch 16/1000 
	 loss: 99.4630, MinusLogProbMetric: 99.4630, val_loss: 96.0023, val_MinusLogProbMetric: 96.0023

Epoch 16: val_loss improved from 101.22781 to 96.00235, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 36s - loss: 99.4630 - MinusLogProbMetric: 99.4630 - val_loss: 96.0023 - val_MinusLogProbMetric: 96.0023 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 17/1000
2023-10-27 13:09:49.161 
Epoch 17/1000 
	 loss: 93.8902, MinusLogProbMetric: 93.8902, val_loss: 92.9904, val_MinusLogProbMetric: 92.9904

Epoch 17: val_loss improved from 96.00235 to 92.99041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 93.8902 - MinusLogProbMetric: 93.8902 - val_loss: 92.9904 - val_MinusLogProbMetric: 92.9904 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 18/1000
2023-10-27 13:10:25.536 
Epoch 18/1000 
	 loss: 108.0754, MinusLogProbMetric: 108.0754, val_loss: 99.4452, val_MinusLogProbMetric: 99.4452

Epoch 18: val_loss did not improve from 92.99041
196/196 - 36s - loss: 108.0754 - MinusLogProbMetric: 108.0754 - val_loss: 99.4452 - val_MinusLogProbMetric: 99.4452 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 19/1000
2023-10-27 13:11:01.792 
Epoch 19/1000 
	 loss: 92.3653, MinusLogProbMetric: 92.3653, val_loss: 88.9850, val_MinusLogProbMetric: 88.9850

Epoch 19: val_loss improved from 92.99041 to 88.98499, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 92.3653 - MinusLogProbMetric: 92.3653 - val_loss: 88.9850 - val_MinusLogProbMetric: 88.9850 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 20/1000
2023-10-27 13:11:38.335 
Epoch 20/1000 
	 loss: 87.4367, MinusLogProbMetric: 87.4367, val_loss: 86.7838, val_MinusLogProbMetric: 86.7838

Epoch 20: val_loss improved from 88.98499 to 86.78375, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 87.4367 - MinusLogProbMetric: 87.4367 - val_loss: 86.7838 - val_MinusLogProbMetric: 86.7838 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 21/1000
2023-10-27 13:12:15.411 
Epoch 21/1000 
	 loss: 84.3562, MinusLogProbMetric: 84.3562, val_loss: 83.4962, val_MinusLogProbMetric: 83.4962

Epoch 21: val_loss improved from 86.78375 to 83.49622, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 84.3562 - MinusLogProbMetric: 84.3562 - val_loss: 83.4962 - val_MinusLogProbMetric: 83.4962 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 22/1000
2023-10-27 13:12:52.292 
Epoch 22/1000 
	 loss: 81.9388, MinusLogProbMetric: 81.9388, val_loss: 80.7838, val_MinusLogProbMetric: 80.7838

Epoch 22: val_loss improved from 83.49622 to 80.78380, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 81.9388 - MinusLogProbMetric: 81.9388 - val_loss: 80.7838 - val_MinusLogProbMetric: 80.7838 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 23/1000
2023-10-27 13:13:28.728 
Epoch 23/1000 
	 loss: 79.7310, MinusLogProbMetric: 79.7310, val_loss: 78.6746, val_MinusLogProbMetric: 78.6746

Epoch 23: val_loss improved from 80.78380 to 78.67456, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 36s - loss: 79.7310 - MinusLogProbMetric: 79.7310 - val_loss: 78.6746 - val_MinusLogProbMetric: 78.6746 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 24/1000
2023-10-27 13:14:05.236 
Epoch 24/1000 
	 loss: 77.7991, MinusLogProbMetric: 77.7991, val_loss: 77.1401, val_MinusLogProbMetric: 77.1401

Epoch 24: val_loss improved from 78.67456 to 77.14014, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 77.7991 - MinusLogProbMetric: 77.7991 - val_loss: 77.1401 - val_MinusLogProbMetric: 77.1401 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 25/1000
2023-10-27 13:14:41.807 
Epoch 25/1000 
	 loss: 76.1554, MinusLogProbMetric: 76.1554, val_loss: 75.8429, val_MinusLogProbMetric: 75.8429

Epoch 25: val_loss improved from 77.14014 to 75.84295, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 76.1554 - MinusLogProbMetric: 76.1554 - val_loss: 75.8429 - val_MinusLogProbMetric: 75.8429 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 26/1000
2023-10-27 13:15:18.808 
Epoch 26/1000 
	 loss: 74.7067, MinusLogProbMetric: 74.7067, val_loss: 74.7880, val_MinusLogProbMetric: 74.7880

Epoch 26: val_loss improved from 75.84295 to 74.78803, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 74.7067 - MinusLogProbMetric: 74.7067 - val_loss: 74.7880 - val_MinusLogProbMetric: 74.7880 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 27/1000
2023-10-27 13:15:55.460 
Epoch 27/1000 
	 loss: 73.3787, MinusLogProbMetric: 73.3787, val_loss: 73.7529, val_MinusLogProbMetric: 73.7529

Epoch 27: val_loss improved from 74.78803 to 73.75291, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 73.3787 - MinusLogProbMetric: 73.3787 - val_loss: 73.7529 - val_MinusLogProbMetric: 73.7529 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 28/1000
2023-10-27 13:16:32.094 
Epoch 28/1000 
	 loss: 72.0838, MinusLogProbMetric: 72.0838, val_loss: 72.9049, val_MinusLogProbMetric: 72.9049

Epoch 28: val_loss improved from 73.75291 to 72.90490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 72.0838 - MinusLogProbMetric: 72.0838 - val_loss: 72.9049 - val_MinusLogProbMetric: 72.9049 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 29/1000
2023-10-27 13:17:08.895 
Epoch 29/1000 
	 loss: 71.2247, MinusLogProbMetric: 71.2247, val_loss: 71.5762, val_MinusLogProbMetric: 71.5762

Epoch 29: val_loss improved from 72.90490 to 71.57620, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 71.2247 - MinusLogProbMetric: 71.2247 - val_loss: 71.5762 - val_MinusLogProbMetric: 71.5762 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 30/1000
2023-10-27 13:17:45.485 
Epoch 30/1000 
	 loss: 70.1780, MinusLogProbMetric: 70.1780, val_loss: 70.5386, val_MinusLogProbMetric: 70.5386

Epoch 30: val_loss improved from 71.57620 to 70.53862, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 70.1780 - MinusLogProbMetric: 70.1780 - val_loss: 70.5386 - val_MinusLogProbMetric: 70.5386 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 31/1000
2023-10-27 13:18:21.573 
Epoch 31/1000 
	 loss: 69.0149, MinusLogProbMetric: 69.0149, val_loss: 68.5107, val_MinusLogProbMetric: 68.5107

Epoch 31: val_loss improved from 70.53862 to 68.51066, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 36s - loss: 69.0149 - MinusLogProbMetric: 69.0149 - val_loss: 68.5107 - val_MinusLogProbMetric: 68.5107 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 32/1000
2023-10-27 13:18:57.976 
Epoch 32/1000 
	 loss: 68.4910, MinusLogProbMetric: 68.4910, val_loss: 68.3752, val_MinusLogProbMetric: 68.3752

Epoch 32: val_loss improved from 68.51066 to 68.37517, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 36s - loss: 68.4910 - MinusLogProbMetric: 68.4910 - val_loss: 68.3752 - val_MinusLogProbMetric: 68.3752 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 33/1000
2023-10-27 13:19:36.554 
Epoch 33/1000 
	 loss: 67.6336, MinusLogProbMetric: 67.6336, val_loss: 68.3959, val_MinusLogProbMetric: 68.3959

Epoch 33: val_loss did not improve from 68.37517
196/196 - 38s - loss: 67.6336 - MinusLogProbMetric: 67.6336 - val_loss: 68.3959 - val_MinusLogProbMetric: 68.3959 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 34/1000
2023-10-27 13:20:12.602 
Epoch 34/1000 
	 loss: 66.9113, MinusLogProbMetric: 66.9113, val_loss: 66.7264, val_MinusLogProbMetric: 66.7264

Epoch 34: val_loss improved from 68.37517 to 66.72640, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 66.9113 - MinusLogProbMetric: 66.9113 - val_loss: 66.7264 - val_MinusLogProbMetric: 66.7264 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 35/1000
2023-10-27 13:20:49.500 
Epoch 35/1000 
	 loss: 66.1089, MinusLogProbMetric: 66.1089, val_loss: 66.1804, val_MinusLogProbMetric: 66.1804

Epoch 35: val_loss improved from 66.72640 to 66.18039, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 66.1089 - MinusLogProbMetric: 66.1089 - val_loss: 66.1804 - val_MinusLogProbMetric: 66.1804 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 36/1000
2023-10-27 13:21:26.119 
Epoch 36/1000 
	 loss: 65.5200, MinusLogProbMetric: 65.5200, val_loss: 66.3141, val_MinusLogProbMetric: 66.3141

Epoch 36: val_loss did not improve from 66.18039
196/196 - 36s - loss: 65.5200 - MinusLogProbMetric: 65.5200 - val_loss: 66.3141 - val_MinusLogProbMetric: 66.3141 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 37/1000
2023-10-27 13:22:02.151 
Epoch 37/1000 
	 loss: 65.0731, MinusLogProbMetric: 65.0731, val_loss: 65.1522, val_MinusLogProbMetric: 65.1522

Epoch 37: val_loss improved from 66.18039 to 65.15222, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 65.0731 - MinusLogProbMetric: 65.0731 - val_loss: 65.1522 - val_MinusLogProbMetric: 65.1522 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 38/1000
2023-10-27 13:22:39.172 
Epoch 38/1000 
	 loss: 64.5272, MinusLogProbMetric: 64.5272, val_loss: 65.8691, val_MinusLogProbMetric: 65.8691

Epoch 38: val_loss did not improve from 65.15222
196/196 - 36s - loss: 64.5272 - MinusLogProbMetric: 64.5272 - val_loss: 65.8691 - val_MinusLogProbMetric: 65.8691 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 39/1000
2023-10-27 13:23:15.295 
Epoch 39/1000 
	 loss: 63.8627, MinusLogProbMetric: 63.8627, val_loss: 64.2560, val_MinusLogProbMetric: 64.2560

Epoch 39: val_loss improved from 65.15222 to 64.25595, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 63.8627 - MinusLogProbMetric: 63.8627 - val_loss: 64.2560 - val_MinusLogProbMetric: 64.2560 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 40/1000
2023-10-27 13:23:51.843 
Epoch 40/1000 
	 loss: 63.8807, MinusLogProbMetric: 63.8807, val_loss: 63.8869, val_MinusLogProbMetric: 63.8869

Epoch 40: val_loss improved from 64.25595 to 63.88686, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 63.8807 - MinusLogProbMetric: 63.8807 - val_loss: 63.8869 - val_MinusLogProbMetric: 63.8869 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 41/1000
2023-10-27 13:24:28.774 
Epoch 41/1000 
	 loss: 62.8003, MinusLogProbMetric: 62.8003, val_loss: 62.9264, val_MinusLogProbMetric: 62.9264

Epoch 41: val_loss improved from 63.88686 to 62.92637, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 39s - loss: 62.8003 - MinusLogProbMetric: 62.8003 - val_loss: 62.9264 - val_MinusLogProbMetric: 62.9264 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 42/1000
2023-10-27 13:25:08.016 
Epoch 42/1000 
	 loss: 62.4283, MinusLogProbMetric: 62.4283, val_loss: 62.9508, val_MinusLogProbMetric: 62.9508

Epoch 42: val_loss did not improve from 62.92637
196/196 - 36s - loss: 62.4283 - MinusLogProbMetric: 62.4283 - val_loss: 62.9508 - val_MinusLogProbMetric: 62.9508 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 43/1000
2023-10-27 13:25:43.964 
Epoch 43/1000 
	 loss: 61.9541, MinusLogProbMetric: 61.9541, val_loss: 62.0496, val_MinusLogProbMetric: 62.0496

Epoch 43: val_loss improved from 62.92637 to 62.04962, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 61.9541 - MinusLogProbMetric: 61.9541 - val_loss: 62.0496 - val_MinusLogProbMetric: 62.0496 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 44/1000
2023-10-27 13:26:22.746 
Epoch 44/1000 
	 loss: 61.6674, MinusLogProbMetric: 61.6674, val_loss: 63.6147, val_MinusLogProbMetric: 63.6147

Epoch 44: val_loss did not improve from 62.04962
196/196 - 38s - loss: 61.6674 - MinusLogProbMetric: 61.6674 - val_loss: 63.6147 - val_MinusLogProbMetric: 63.6147 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 45/1000
2023-10-27 13:27:00.021 
Epoch 45/1000 
	 loss: 61.2994, MinusLogProbMetric: 61.2994, val_loss: 61.7595, val_MinusLogProbMetric: 61.7595

Epoch 45: val_loss improved from 62.04962 to 61.75950, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 38s - loss: 61.2994 - MinusLogProbMetric: 61.2994 - val_loss: 61.7595 - val_MinusLogProbMetric: 61.7595 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 46/1000
2023-10-27 13:27:36.750 
Epoch 46/1000 
	 loss: 99.8037, MinusLogProbMetric: 99.8037, val_loss: 398.0334, val_MinusLogProbMetric: 398.0334

Epoch 46: val_loss did not improve from 61.75950
196/196 - 36s - loss: 99.8037 - MinusLogProbMetric: 99.8037 - val_loss: 398.0334 - val_MinusLogProbMetric: 398.0334 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 47/1000
2023-10-27 13:28:12.939 
Epoch 47/1000 
	 loss: 167.8675, MinusLogProbMetric: 167.8675, val_loss: 102.5363, val_MinusLogProbMetric: 102.5363

Epoch 47: val_loss did not improve from 61.75950
196/196 - 36s - loss: 167.8675 - MinusLogProbMetric: 167.8675 - val_loss: 102.5363 - val_MinusLogProbMetric: 102.5363 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 48/1000
2023-10-27 13:28:48.710 
Epoch 48/1000 
	 loss: 92.6259, MinusLogProbMetric: 92.6259, val_loss: 85.3333, val_MinusLogProbMetric: 85.3333

Epoch 48: val_loss did not improve from 61.75950
196/196 - 36s - loss: 92.6259 - MinusLogProbMetric: 92.6259 - val_loss: 85.3333 - val_MinusLogProbMetric: 85.3333 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 49/1000
2023-10-27 13:29:26.124 
Epoch 49/1000 
	 loss: 87.3968, MinusLogProbMetric: 87.3968, val_loss: 86.3444, val_MinusLogProbMetric: 86.3444

Epoch 49: val_loss did not improve from 61.75950
196/196 - 37s - loss: 87.3968 - MinusLogProbMetric: 87.3968 - val_loss: 86.3444 - val_MinusLogProbMetric: 86.3444 - lr: 3.3333e-04 - 37s/epoch - 191ms/step
Epoch 50/1000
2023-10-27 13:30:04.380 
Epoch 50/1000 
	 loss: 98.4081, MinusLogProbMetric: 98.4081, val_loss: 81.6022, val_MinusLogProbMetric: 81.6022

Epoch 50: val_loss did not improve from 61.75950
196/196 - 38s - loss: 98.4081 - MinusLogProbMetric: 98.4081 - val_loss: 81.6022 - val_MinusLogProbMetric: 81.6022 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 51/1000
2023-10-27 13:30:39.963 
Epoch 51/1000 
	 loss: 78.1281, MinusLogProbMetric: 78.1281, val_loss: 76.8237, val_MinusLogProbMetric: 76.8237

Epoch 51: val_loss did not improve from 61.75950
196/196 - 36s - loss: 78.1281 - MinusLogProbMetric: 78.1281 - val_loss: 76.8237 - val_MinusLogProbMetric: 76.8237 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 52/1000
2023-10-27 13:31:15.483 
Epoch 52/1000 
	 loss: 74.1477, MinusLogProbMetric: 74.1477, val_loss: 72.6224, val_MinusLogProbMetric: 72.6224

Epoch 52: val_loss did not improve from 61.75950
196/196 - 36s - loss: 74.1477 - MinusLogProbMetric: 74.1477 - val_loss: 72.6224 - val_MinusLogProbMetric: 72.6224 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 53/1000
2023-10-27 13:31:53.880 
Epoch 53/1000 
	 loss: 70.8464, MinusLogProbMetric: 70.8464, val_loss: 69.9170, val_MinusLogProbMetric: 69.9170

Epoch 53: val_loss did not improve from 61.75950
196/196 - 38s - loss: 70.8464 - MinusLogProbMetric: 70.8464 - val_loss: 69.9170 - val_MinusLogProbMetric: 69.9170 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 54/1000
2023-10-27 13:32:34.252 
Epoch 54/1000 
	 loss: 68.8181, MinusLogProbMetric: 68.8181, val_loss: 68.1568, val_MinusLogProbMetric: 68.1568

Epoch 54: val_loss did not improve from 61.75950
196/196 - 40s - loss: 68.8181 - MinusLogProbMetric: 68.8181 - val_loss: 68.1568 - val_MinusLogProbMetric: 68.1568 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 55/1000
2023-10-27 13:33:06.831 
Epoch 55/1000 
	 loss: 67.5253, MinusLogProbMetric: 67.5253, val_loss: 68.9042, val_MinusLogProbMetric: 68.9042

Epoch 55: val_loss did not improve from 61.75950
196/196 - 33s - loss: 67.5253 - MinusLogProbMetric: 67.5253 - val_loss: 68.9042 - val_MinusLogProbMetric: 68.9042 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 56/1000
2023-10-27 13:33:39.613 
Epoch 56/1000 
	 loss: 66.5301, MinusLogProbMetric: 66.5301, val_loss: 66.1836, val_MinusLogProbMetric: 66.1836

Epoch 56: val_loss did not improve from 61.75950
196/196 - 33s - loss: 66.5301 - MinusLogProbMetric: 66.5301 - val_loss: 66.1836 - val_MinusLogProbMetric: 66.1836 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 57/1000
2023-10-27 13:34:11.740 
Epoch 57/1000 
	 loss: 65.5116, MinusLogProbMetric: 65.5116, val_loss: 65.5523, val_MinusLogProbMetric: 65.5523

Epoch 57: val_loss did not improve from 61.75950
196/196 - 32s - loss: 65.5116 - MinusLogProbMetric: 65.5116 - val_loss: 65.5523 - val_MinusLogProbMetric: 65.5523 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 58/1000
2023-10-27 13:34:47.279 
Epoch 58/1000 
	 loss: 64.7657, MinusLogProbMetric: 64.7657, val_loss: 65.8083, val_MinusLogProbMetric: 65.8083

Epoch 58: val_loss did not improve from 61.75950
196/196 - 36s - loss: 64.7657 - MinusLogProbMetric: 64.7657 - val_loss: 65.8083 - val_MinusLogProbMetric: 65.8083 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 59/1000
2023-10-27 13:35:24.907 
Epoch 59/1000 
	 loss: 64.0983, MinusLogProbMetric: 64.0983, val_loss: 63.9769, val_MinusLogProbMetric: 63.9769

Epoch 59: val_loss did not improve from 61.75950
196/196 - 38s - loss: 64.0983 - MinusLogProbMetric: 64.0983 - val_loss: 63.9769 - val_MinusLogProbMetric: 63.9769 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 60/1000
2023-10-27 13:36:00.554 
Epoch 60/1000 
	 loss: 63.7240, MinusLogProbMetric: 63.7240, val_loss: 65.9035, val_MinusLogProbMetric: 65.9035

Epoch 60: val_loss did not improve from 61.75950
196/196 - 36s - loss: 63.7240 - MinusLogProbMetric: 63.7240 - val_loss: 65.9035 - val_MinusLogProbMetric: 65.9035 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 61/1000
2023-10-27 13:36:35.818 
Epoch 61/1000 
	 loss: 63.1413, MinusLogProbMetric: 63.1413, val_loss: 63.1433, val_MinusLogProbMetric: 63.1433

Epoch 61: val_loss did not improve from 61.75950
196/196 - 35s - loss: 63.1413 - MinusLogProbMetric: 63.1413 - val_loss: 63.1433 - val_MinusLogProbMetric: 63.1433 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 62/1000
2023-10-27 13:37:11.261 
Epoch 62/1000 
	 loss: 62.6747, MinusLogProbMetric: 62.6747, val_loss: 62.5084, val_MinusLogProbMetric: 62.5084

Epoch 62: val_loss did not improve from 61.75950
196/196 - 35s - loss: 62.6747 - MinusLogProbMetric: 62.6747 - val_loss: 62.5084 - val_MinusLogProbMetric: 62.5084 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 63/1000
2023-10-27 13:37:50.825 
Epoch 63/1000 
	 loss: 62.1497, MinusLogProbMetric: 62.1497, val_loss: 62.1916, val_MinusLogProbMetric: 62.1916

Epoch 63: val_loss did not improve from 61.75950
196/196 - 40s - loss: 62.1497 - MinusLogProbMetric: 62.1497 - val_loss: 62.1916 - val_MinusLogProbMetric: 62.1916 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 64/1000
2023-10-27 13:38:31.020 
Epoch 64/1000 
	 loss: 61.7071, MinusLogProbMetric: 61.7071, val_loss: 62.0098, val_MinusLogProbMetric: 62.0098

Epoch 64: val_loss did not improve from 61.75950
196/196 - 40s - loss: 61.7071 - MinusLogProbMetric: 61.7071 - val_loss: 62.0098 - val_MinusLogProbMetric: 62.0098 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 65/1000
2023-10-27 13:39:06.244 
Epoch 65/1000 
	 loss: 61.3771, MinusLogProbMetric: 61.3771, val_loss: 60.9786, val_MinusLogProbMetric: 60.9786

Epoch 65: val_loss improved from 61.75950 to 60.97859, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 36s - loss: 61.3771 - MinusLogProbMetric: 61.3771 - val_loss: 60.9786 - val_MinusLogProbMetric: 60.9786 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 66/1000
2023-10-27 13:39:41.992 
Epoch 66/1000 
	 loss: 60.8072, MinusLogProbMetric: 60.8072, val_loss: 60.5279, val_MinusLogProbMetric: 60.5279

Epoch 66: val_loss improved from 60.97859 to 60.52787, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 36s - loss: 60.8072 - MinusLogProbMetric: 60.8072 - val_loss: 60.5279 - val_MinusLogProbMetric: 60.5279 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 67/1000
2023-10-27 13:40:18.351 
Epoch 67/1000 
	 loss: 60.6444, MinusLogProbMetric: 60.6444, val_loss: 61.0107, val_MinusLogProbMetric: 61.0107

Epoch 67: val_loss did not improve from 60.52787
196/196 - 36s - loss: 60.6444 - MinusLogProbMetric: 60.6444 - val_loss: 61.0107 - val_MinusLogProbMetric: 61.0107 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 68/1000
2023-10-27 13:40:59.662 
Epoch 68/1000 
	 loss: 60.2073, MinusLogProbMetric: 60.2073, val_loss: 60.0981, val_MinusLogProbMetric: 60.0981

Epoch 68: val_loss improved from 60.52787 to 60.09809, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 42s - loss: 60.2073 - MinusLogProbMetric: 60.2073 - val_loss: 60.0981 - val_MinusLogProbMetric: 60.0981 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 69/1000
2023-10-27 13:41:37.444 
Epoch 69/1000 
	 loss: 60.0240, MinusLogProbMetric: 60.0240, val_loss: 60.4713, val_MinusLogProbMetric: 60.4713

Epoch 69: val_loss did not improve from 60.09809
196/196 - 37s - loss: 60.0240 - MinusLogProbMetric: 60.0240 - val_loss: 60.4713 - val_MinusLogProbMetric: 60.4713 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 70/1000
2023-10-27 13:42:13.071 
Epoch 70/1000 
	 loss: 59.9846, MinusLogProbMetric: 59.9846, val_loss: 59.9981, val_MinusLogProbMetric: 59.9981

Epoch 70: val_loss improved from 60.09809 to 59.99807, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 36s - loss: 59.9846 - MinusLogProbMetric: 59.9846 - val_loss: 59.9981 - val_MinusLogProbMetric: 59.9981 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 71/1000
2023-10-27 13:42:49.066 
Epoch 71/1000 
	 loss: 59.4001, MinusLogProbMetric: 59.4001, val_loss: 59.0807, val_MinusLogProbMetric: 59.0807

Epoch 71: val_loss improved from 59.99807 to 59.08070, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 36s - loss: 59.4001 - MinusLogProbMetric: 59.4001 - val_loss: 59.0807 - val_MinusLogProbMetric: 59.0807 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 72/1000
2023-10-27 13:43:25.099 
Epoch 72/1000 
	 loss: 59.0248, MinusLogProbMetric: 59.0248, val_loss: 59.9067, val_MinusLogProbMetric: 59.9067

Epoch 72: val_loss did not improve from 59.08070
196/196 - 35s - loss: 59.0248 - MinusLogProbMetric: 59.0248 - val_loss: 59.9067 - val_MinusLogProbMetric: 59.9067 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 73/1000
2023-10-27 13:44:04.810 
Epoch 73/1000 
	 loss: 58.8411, MinusLogProbMetric: 58.8411, val_loss: 59.4858, val_MinusLogProbMetric: 59.4858

Epoch 73: val_loss did not improve from 59.08070
196/196 - 40s - loss: 58.8411 - MinusLogProbMetric: 58.8411 - val_loss: 59.4858 - val_MinusLogProbMetric: 59.4858 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 74/1000
2023-10-27 13:44:42.779 
Epoch 74/1000 
	 loss: 58.9188, MinusLogProbMetric: 58.9188, val_loss: 58.9416, val_MinusLogProbMetric: 58.9416

Epoch 74: val_loss improved from 59.08070 to 58.94157, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 39s - loss: 58.9188 - MinusLogProbMetric: 58.9188 - val_loss: 58.9416 - val_MinusLogProbMetric: 58.9416 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 75/1000
2023-10-27 13:45:19.474 
Epoch 75/1000 
	 loss: 58.2169, MinusLogProbMetric: 58.2169, val_loss: 60.5321, val_MinusLogProbMetric: 60.5321

Epoch 75: val_loss did not improve from 58.94157
196/196 - 36s - loss: 58.2169 - MinusLogProbMetric: 58.2169 - val_loss: 60.5321 - val_MinusLogProbMetric: 60.5321 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 76/1000
2023-10-27 13:45:57.536 
Epoch 76/1000 
	 loss: 58.2046, MinusLogProbMetric: 58.2046, val_loss: 57.7019, val_MinusLogProbMetric: 57.7019

Epoch 76: val_loss improved from 58.94157 to 57.70185, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 39s - loss: 58.2046 - MinusLogProbMetric: 58.2046 - val_loss: 57.7019 - val_MinusLogProbMetric: 57.7019 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 77/1000
2023-10-27 13:46:34.451 
Epoch 77/1000 
	 loss: 57.6816, MinusLogProbMetric: 57.6816, val_loss: 57.3876, val_MinusLogProbMetric: 57.3876

Epoch 77: val_loss improved from 57.70185 to 57.38756, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 57.6816 - MinusLogProbMetric: 57.6816 - val_loss: 57.3876 - val_MinusLogProbMetric: 57.3876 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 78/1000
2023-10-27 13:47:16.026 
Epoch 78/1000 
	 loss: 57.4951, MinusLogProbMetric: 57.4951, val_loss: 59.0190, val_MinusLogProbMetric: 59.0190

Epoch 78: val_loss did not improve from 57.38756
196/196 - 41s - loss: 57.4951 - MinusLogProbMetric: 57.4951 - val_loss: 59.0190 - val_MinusLogProbMetric: 59.0190 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 79/1000
2023-10-27 13:47:51.756 
Epoch 79/1000 
	 loss: 57.0935, MinusLogProbMetric: 57.0935, val_loss: 56.9843, val_MinusLogProbMetric: 56.9843

Epoch 79: val_loss improved from 57.38756 to 56.98433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 57.0935 - MinusLogProbMetric: 57.0935 - val_loss: 56.9843 - val_MinusLogProbMetric: 56.9843 - lr: 3.3333e-04 - 37s/epoch - 186ms/step
Epoch 80/1000
2023-10-27 13:48:33.518 
Epoch 80/1000 
	 loss: 57.6357, MinusLogProbMetric: 57.6357, val_loss: 57.2767, val_MinusLogProbMetric: 57.2767

Epoch 80: val_loss did not improve from 56.98433
196/196 - 41s - loss: 57.6357 - MinusLogProbMetric: 57.6357 - val_loss: 57.2767 - val_MinusLogProbMetric: 57.2767 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 81/1000
2023-10-27 13:49:08.756 
Epoch 81/1000 
	 loss: 56.8434, MinusLogProbMetric: 56.8434, val_loss: 57.2962, val_MinusLogProbMetric: 57.2962

Epoch 81: val_loss did not improve from 56.98433
196/196 - 35s - loss: 56.8434 - MinusLogProbMetric: 56.8434 - val_loss: 57.2962 - val_MinusLogProbMetric: 57.2962 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 82/1000
2023-10-27 13:49:47.891 
Epoch 82/1000 
	 loss: 56.8253, MinusLogProbMetric: 56.8253, val_loss: 57.5287, val_MinusLogProbMetric: 57.5287

Epoch 82: val_loss did not improve from 56.98433
196/196 - 39s - loss: 56.8253 - MinusLogProbMetric: 56.8253 - val_loss: 57.5287 - val_MinusLogProbMetric: 57.5287 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 83/1000
2023-10-27 13:50:24.216 
Epoch 83/1000 
	 loss: 56.5830, MinusLogProbMetric: 56.5830, val_loss: 57.6631, val_MinusLogProbMetric: 57.6631

Epoch 83: val_loss did not improve from 56.98433
196/196 - 36s - loss: 56.5830 - MinusLogProbMetric: 56.5830 - val_loss: 57.6631 - val_MinusLogProbMetric: 57.6631 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 84/1000
2023-10-27 13:51:01.786 
Epoch 84/1000 
	 loss: 56.3440, MinusLogProbMetric: 56.3440, val_loss: 57.3552, val_MinusLogProbMetric: 57.3552

Epoch 84: val_loss did not improve from 56.98433
196/196 - 38s - loss: 56.3440 - MinusLogProbMetric: 56.3440 - val_loss: 57.3552 - val_MinusLogProbMetric: 57.3552 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 85/1000
2023-10-27 13:51:40.418 
Epoch 85/1000 
	 loss: 56.3041, MinusLogProbMetric: 56.3041, val_loss: 58.4718, val_MinusLogProbMetric: 58.4718

Epoch 85: val_loss did not improve from 56.98433
196/196 - 39s - loss: 56.3041 - MinusLogProbMetric: 56.3041 - val_loss: 58.4718 - val_MinusLogProbMetric: 58.4718 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 86/1000
2023-10-27 13:52:16.892 
Epoch 86/1000 
	 loss: 55.9998, MinusLogProbMetric: 55.9998, val_loss: 56.5242, val_MinusLogProbMetric: 56.5242

Epoch 86: val_loss improved from 56.98433 to 56.52417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 55.9998 - MinusLogProbMetric: 55.9998 - val_loss: 56.5242 - val_MinusLogProbMetric: 56.5242 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 87/1000
2023-10-27 13:52:56.470 
Epoch 87/1000 
	 loss: 55.7789, MinusLogProbMetric: 55.7789, val_loss: 59.8919, val_MinusLogProbMetric: 59.8919

Epoch 87: val_loss did not improve from 56.52417
196/196 - 39s - loss: 55.7789 - MinusLogProbMetric: 55.7789 - val_loss: 59.8919 - val_MinusLogProbMetric: 59.8919 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 88/1000
2023-10-27 13:53:32.147 
Epoch 88/1000 
	 loss: 55.7667, MinusLogProbMetric: 55.7667, val_loss: 55.3954, val_MinusLogProbMetric: 55.3954

Epoch 88: val_loss improved from 56.52417 to 55.39537, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 36s - loss: 55.7667 - MinusLogProbMetric: 55.7667 - val_loss: 55.3954 - val_MinusLogProbMetric: 55.3954 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 89/1000
2023-10-27 13:54:13.059 
Epoch 89/1000 
	 loss: 55.5552, MinusLogProbMetric: 55.5552, val_loss: 55.5810, val_MinusLogProbMetric: 55.5810

Epoch 89: val_loss did not improve from 55.39537
196/196 - 40s - loss: 55.5552 - MinusLogProbMetric: 55.5552 - val_loss: 55.5810 - val_MinusLogProbMetric: 55.5810 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 90/1000
2023-10-27 13:54:48.455 
Epoch 90/1000 
	 loss: 55.4066, MinusLogProbMetric: 55.4066, val_loss: 58.9827, val_MinusLogProbMetric: 58.9827

Epoch 90: val_loss did not improve from 55.39537
196/196 - 35s - loss: 55.4066 - MinusLogProbMetric: 55.4066 - val_loss: 58.9827 - val_MinusLogProbMetric: 58.9827 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 91/1000
2023-10-27 13:55:28.198 
Epoch 91/1000 
	 loss: 55.3141, MinusLogProbMetric: 55.3141, val_loss: 55.7729, val_MinusLogProbMetric: 55.7729

Epoch 91: val_loss did not improve from 55.39537
196/196 - 40s - loss: 55.3141 - MinusLogProbMetric: 55.3141 - val_loss: 55.7729 - val_MinusLogProbMetric: 55.7729 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 92/1000
2023-10-27 13:56:03.576 
Epoch 92/1000 
	 loss: 55.3737, MinusLogProbMetric: 55.3737, val_loss: 55.0924, val_MinusLogProbMetric: 55.0924

Epoch 92: val_loss improved from 55.39537 to 55.09236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 36s - loss: 55.3737 - MinusLogProbMetric: 55.3737 - val_loss: 55.0924 - val_MinusLogProbMetric: 55.0924 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 93/1000
2023-10-27 13:56:40.115 
Epoch 93/1000 
	 loss: 54.6901, MinusLogProbMetric: 54.6901, val_loss: 55.2106, val_MinusLogProbMetric: 55.2106

Epoch 93: val_loss did not improve from 55.09236
196/196 - 36s - loss: 54.6901 - MinusLogProbMetric: 54.6901 - val_loss: 55.2106 - val_MinusLogProbMetric: 55.2106 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 94/1000
2023-10-27 13:57:17.075 
Epoch 94/1000 
	 loss: 54.7412, MinusLogProbMetric: 54.7412, val_loss: 55.2561, val_MinusLogProbMetric: 55.2561

Epoch 94: val_loss did not improve from 55.09236
196/196 - 37s - loss: 54.7412 - MinusLogProbMetric: 54.7412 - val_loss: 55.2561 - val_MinusLogProbMetric: 55.2561 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 95/1000
2023-10-27 13:57:53.219 
Epoch 95/1000 
	 loss: 54.8507, MinusLogProbMetric: 54.8507, val_loss: 55.0541, val_MinusLogProbMetric: 55.0541

Epoch 95: val_loss improved from 55.09236 to 55.05411, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 37s - loss: 54.8507 - MinusLogProbMetric: 54.8507 - val_loss: 55.0541 - val_MinusLogProbMetric: 55.0541 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 96/1000
2023-10-27 13:58:31.512 
Epoch 96/1000 
	 loss: 55.1974, MinusLogProbMetric: 55.1974, val_loss: 54.7847, val_MinusLogProbMetric: 54.7847

Epoch 96: val_loss improved from 55.05411 to 54.78468, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 38s - loss: 55.1974 - MinusLogProbMetric: 55.1974 - val_loss: 54.7847 - val_MinusLogProbMetric: 54.7847 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 97/1000
2023-10-27 13:59:08.408 
Epoch 97/1000 
	 loss: 54.2465, MinusLogProbMetric: 54.2465, val_loss: 56.4165, val_MinusLogProbMetric: 56.4165

Epoch 97: val_loss did not improve from 54.78468
196/196 - 36s - loss: 54.2465 - MinusLogProbMetric: 54.2465 - val_loss: 56.4165 - val_MinusLogProbMetric: 56.4165 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 98/1000
2023-10-27 13:59:47.613 
Epoch 98/1000 
	 loss: 54.3885, MinusLogProbMetric: 54.3885, val_loss: 54.9421, val_MinusLogProbMetric: 54.9421

Epoch 98: val_loss did not improve from 54.78468
196/196 - 39s - loss: 54.3885 - MinusLogProbMetric: 54.3885 - val_loss: 54.9421 - val_MinusLogProbMetric: 54.9421 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 99/1000
2023-10-27 14:00:23.189 
Epoch 99/1000 
	 loss: 54.3756, MinusLogProbMetric: 54.3756, val_loss: 54.7801, val_MinusLogProbMetric: 54.7801

Epoch 99: val_loss improved from 54.78468 to 54.78012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 36s - loss: 54.3756 - MinusLogProbMetric: 54.3756 - val_loss: 54.7801 - val_MinusLogProbMetric: 54.7801 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 100/1000
2023-10-27 14:01:02.906 
Epoch 100/1000 
	 loss: 54.0922, MinusLogProbMetric: 54.0922, val_loss: 54.0422, val_MinusLogProbMetric: 54.0422

Epoch 100: val_loss improved from 54.78012 to 54.04219, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 40s - loss: 54.0922 - MinusLogProbMetric: 54.0922 - val_loss: 54.0422 - val_MinusLogProbMetric: 54.0422 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 101/1000
2023-10-27 14:01:41.673 
Epoch 101/1000 
	 loss: 54.0191, MinusLogProbMetric: 54.0191, val_loss: 59.2256, val_MinusLogProbMetric: 59.2256

Epoch 101: val_loss did not improve from 54.04219
196/196 - 38s - loss: 54.0191 - MinusLogProbMetric: 54.0191 - val_loss: 59.2256 - val_MinusLogProbMetric: 59.2256 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 102/1000
2023-10-27 14:02:18.173 
Epoch 102/1000 
	 loss: 53.9523, MinusLogProbMetric: 53.9523, val_loss: 54.1048, val_MinusLogProbMetric: 54.1048

Epoch 102: val_loss did not improve from 54.04219
196/196 - 36s - loss: 53.9523 - MinusLogProbMetric: 53.9523 - val_loss: 54.1048 - val_MinusLogProbMetric: 54.1048 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 103/1000
2023-10-27 14:02:59.631 
Epoch 103/1000 
	 loss: 53.8549, MinusLogProbMetric: 53.8549, val_loss: 54.4188, val_MinusLogProbMetric: 54.4188

Epoch 103: val_loss did not improve from 54.04219
196/196 - 41s - loss: 53.8549 - MinusLogProbMetric: 53.8549 - val_loss: 54.4188 - val_MinusLogProbMetric: 54.4188 - lr: 3.3333e-04 - 41s/epoch - 212ms/step
Epoch 104/1000
2023-10-27 14:03:41.467 
Epoch 104/1000 
	 loss: 53.7236, MinusLogProbMetric: 53.7236, val_loss: 54.2356, val_MinusLogProbMetric: 54.2356

Epoch 104: val_loss did not improve from 54.04219
196/196 - 42s - loss: 53.7236 - MinusLogProbMetric: 53.7236 - val_loss: 54.2356 - val_MinusLogProbMetric: 54.2356 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 105/1000
2023-10-27 14:04:23.052 
Epoch 105/1000 
	 loss: 53.4356, MinusLogProbMetric: 53.4356, val_loss: 54.2722, val_MinusLogProbMetric: 54.2722

Epoch 105: val_loss did not improve from 54.04219
196/196 - 42s - loss: 53.4356 - MinusLogProbMetric: 53.4356 - val_loss: 54.2722 - val_MinusLogProbMetric: 54.2722 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 106/1000
2023-10-27 14:05:04.528 
Epoch 106/1000 
	 loss: 53.6184, MinusLogProbMetric: 53.6184, val_loss: 53.8030, val_MinusLogProbMetric: 53.8030

Epoch 106: val_loss improved from 54.04219 to 53.80301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 42s - loss: 53.6184 - MinusLogProbMetric: 53.6184 - val_loss: 53.8030 - val_MinusLogProbMetric: 53.8030 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 107/1000
2023-10-27 14:05:46.474 
Epoch 107/1000 
	 loss: 53.2650, MinusLogProbMetric: 53.2650, val_loss: 54.4361, val_MinusLogProbMetric: 54.4361

Epoch 107: val_loss did not improve from 53.80301
196/196 - 41s - loss: 53.2650 - MinusLogProbMetric: 53.2650 - val_loss: 54.4361 - val_MinusLogProbMetric: 54.4361 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 108/1000
2023-10-27 14:06:27.565 
Epoch 108/1000 
	 loss: 53.3666, MinusLogProbMetric: 53.3666, val_loss: 52.5270, val_MinusLogProbMetric: 52.5270

Epoch 108: val_loss improved from 53.80301 to 52.52695, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 42s - loss: 53.3666 - MinusLogProbMetric: 53.3666 - val_loss: 52.5270 - val_MinusLogProbMetric: 52.5270 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 109/1000
2023-10-27 14:07:09.380 
Epoch 109/1000 
	 loss: 53.4655, MinusLogProbMetric: 53.4655, val_loss: 53.6307, val_MinusLogProbMetric: 53.6307

Epoch 109: val_loss did not improve from 52.52695
196/196 - 41s - loss: 53.4655 - MinusLogProbMetric: 53.4655 - val_loss: 53.6307 - val_MinusLogProbMetric: 53.6307 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 110/1000
2023-10-27 14:07:50.917 
Epoch 110/1000 
	 loss: 53.1419, MinusLogProbMetric: 53.1419, val_loss: 53.2972, val_MinusLogProbMetric: 53.2972

Epoch 110: val_loss did not improve from 52.52695
196/196 - 42s - loss: 53.1419 - MinusLogProbMetric: 53.1419 - val_loss: 53.2972 - val_MinusLogProbMetric: 53.2972 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 111/1000
2023-10-27 14:08:32.011 
Epoch 111/1000 
	 loss: 53.1059, MinusLogProbMetric: 53.1059, val_loss: 53.9353, val_MinusLogProbMetric: 53.9353

Epoch 111: val_loss did not improve from 52.52695
196/196 - 41s - loss: 53.1059 - MinusLogProbMetric: 53.1059 - val_loss: 53.9353 - val_MinusLogProbMetric: 53.9353 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 112/1000
2023-10-27 14:09:13.331 
Epoch 112/1000 
	 loss: 53.1500, MinusLogProbMetric: 53.1500, val_loss: 53.5285, val_MinusLogProbMetric: 53.5285

Epoch 112: val_loss did not improve from 52.52695
196/196 - 41s - loss: 53.1500 - MinusLogProbMetric: 53.1500 - val_loss: 53.5285 - val_MinusLogProbMetric: 53.5285 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 113/1000
2023-10-27 14:09:54.147 
Epoch 113/1000 
	 loss: 52.8292, MinusLogProbMetric: 52.8292, val_loss: 52.7501, val_MinusLogProbMetric: 52.7501

Epoch 113: val_loss did not improve from 52.52695
196/196 - 41s - loss: 52.8292 - MinusLogProbMetric: 52.8292 - val_loss: 52.7501 - val_MinusLogProbMetric: 52.7501 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 114/1000
2023-10-27 14:10:35.653 
Epoch 114/1000 
	 loss: 53.1064, MinusLogProbMetric: 53.1064, val_loss: 52.5585, val_MinusLogProbMetric: 52.5585

Epoch 114: val_loss did not improve from 52.52695
196/196 - 42s - loss: 53.1064 - MinusLogProbMetric: 53.1064 - val_loss: 52.5585 - val_MinusLogProbMetric: 52.5585 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 115/1000
2023-10-27 14:11:17.258 
Epoch 115/1000 
	 loss: 52.6232, MinusLogProbMetric: 52.6232, val_loss: 52.2722, val_MinusLogProbMetric: 52.2722

Epoch 115: val_loss improved from 52.52695 to 52.27222, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 42s - loss: 52.6232 - MinusLogProbMetric: 52.6232 - val_loss: 52.2722 - val_MinusLogProbMetric: 52.2722 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 116/1000
2023-10-27 14:11:59.788 
Epoch 116/1000 
	 loss: 52.4914, MinusLogProbMetric: 52.4914, val_loss: 52.8985, val_MinusLogProbMetric: 52.8985

Epoch 116: val_loss did not improve from 52.27222
196/196 - 42s - loss: 52.4914 - MinusLogProbMetric: 52.4914 - val_loss: 52.8985 - val_MinusLogProbMetric: 52.8985 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 117/1000
2023-10-27 14:12:41.179 
Epoch 117/1000 
	 loss: 52.3217, MinusLogProbMetric: 52.3217, val_loss: 52.6792, val_MinusLogProbMetric: 52.6792

Epoch 117: val_loss did not improve from 52.27222
196/196 - 41s - loss: 52.3217 - MinusLogProbMetric: 52.3217 - val_loss: 52.6792 - val_MinusLogProbMetric: 52.6792 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 118/1000
2023-10-27 14:13:22.844 
Epoch 118/1000 
	 loss: 52.4257, MinusLogProbMetric: 52.4257, val_loss: 54.2189, val_MinusLogProbMetric: 54.2189

Epoch 118: val_loss did not improve from 52.27222
196/196 - 42s - loss: 52.4257 - MinusLogProbMetric: 52.4257 - val_loss: 54.2189 - val_MinusLogProbMetric: 54.2189 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 119/1000
2023-10-27 14:14:04.389 
Epoch 119/1000 
	 loss: 52.3754, MinusLogProbMetric: 52.3754, val_loss: 52.2214, val_MinusLogProbMetric: 52.2214

Epoch 119: val_loss improved from 52.27222 to 52.22141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 42s - loss: 52.3754 - MinusLogProbMetric: 52.3754 - val_loss: 52.2214 - val_MinusLogProbMetric: 52.2214 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 120/1000
2023-10-27 14:14:46.726 
Epoch 120/1000 
	 loss: 52.4415, MinusLogProbMetric: 52.4415, val_loss: 53.0791, val_MinusLogProbMetric: 53.0791

Epoch 120: val_loss did not improve from 52.22141
196/196 - 42s - loss: 52.4415 - MinusLogProbMetric: 52.4415 - val_loss: 53.0791 - val_MinusLogProbMetric: 53.0791 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 121/1000
2023-10-27 14:15:28.208 
Epoch 121/1000 
	 loss: 52.1451, MinusLogProbMetric: 52.1451, val_loss: 52.9327, val_MinusLogProbMetric: 52.9327

Epoch 121: val_loss did not improve from 52.22141
196/196 - 41s - loss: 52.1451 - MinusLogProbMetric: 52.1451 - val_loss: 52.9327 - val_MinusLogProbMetric: 52.9327 - lr: 3.3333e-04 - 41s/epoch - 212ms/step
Epoch 122/1000
2023-10-27 14:16:09.662 
Epoch 122/1000 
	 loss: 52.2173, MinusLogProbMetric: 52.2173, val_loss: 53.4583, val_MinusLogProbMetric: 53.4583

Epoch 122: val_loss did not improve from 52.22141
196/196 - 41s - loss: 52.2173 - MinusLogProbMetric: 52.2173 - val_loss: 53.4583 - val_MinusLogProbMetric: 53.4583 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 123/1000
2023-10-27 14:16:51.245 
Epoch 123/1000 
	 loss: 51.9989, MinusLogProbMetric: 51.9989, val_loss: 52.9280, val_MinusLogProbMetric: 52.9280

Epoch 123: val_loss did not improve from 52.22141
196/196 - 42s - loss: 51.9989 - MinusLogProbMetric: 51.9989 - val_loss: 52.9280 - val_MinusLogProbMetric: 52.9280 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 124/1000
2023-10-27 14:17:32.416 
Epoch 124/1000 
	 loss: 51.9911, MinusLogProbMetric: 51.9911, val_loss: 52.2306, val_MinusLogProbMetric: 52.2306

Epoch 124: val_loss did not improve from 52.22141
196/196 - 41s - loss: 51.9911 - MinusLogProbMetric: 51.9911 - val_loss: 52.2306 - val_MinusLogProbMetric: 52.2306 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 125/1000
2023-10-27 14:18:14.102 
Epoch 125/1000 
	 loss: 51.9083, MinusLogProbMetric: 51.9083, val_loss: 53.5338, val_MinusLogProbMetric: 53.5338

Epoch 125: val_loss did not improve from 52.22141
196/196 - 42s - loss: 51.9083 - MinusLogProbMetric: 51.9083 - val_loss: 53.5338 - val_MinusLogProbMetric: 53.5338 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 126/1000
2023-10-27 14:18:55.437 
Epoch 126/1000 
	 loss: 52.0101, MinusLogProbMetric: 52.0101, val_loss: 52.5548, val_MinusLogProbMetric: 52.5548

Epoch 126: val_loss did not improve from 52.22141
196/196 - 41s - loss: 52.0101 - MinusLogProbMetric: 52.0101 - val_loss: 52.5548 - val_MinusLogProbMetric: 52.5548 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 127/1000
2023-10-27 14:19:36.764 
Epoch 127/1000 
	 loss: 51.7538, MinusLogProbMetric: 51.7538, val_loss: 51.9570, val_MinusLogProbMetric: 51.9570

Epoch 127: val_loss improved from 52.22141 to 51.95703, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 42s - loss: 51.7538 - MinusLogProbMetric: 51.7538 - val_loss: 51.9570 - val_MinusLogProbMetric: 51.9570 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 128/1000
2023-10-27 14:20:18.994 
Epoch 128/1000 
	 loss: 51.6916, MinusLogProbMetric: 51.6916, val_loss: 54.3077, val_MinusLogProbMetric: 54.3077

Epoch 128: val_loss did not improve from 51.95703
196/196 - 41s - loss: 51.6916 - MinusLogProbMetric: 51.6916 - val_loss: 54.3077 - val_MinusLogProbMetric: 54.3077 - lr: 3.3333e-04 - 41s/epoch - 212ms/step
Epoch 129/1000
2023-10-27 14:21:00.710 
Epoch 129/1000 
	 loss: 51.6998, MinusLogProbMetric: 51.6998, val_loss: 52.7713, val_MinusLogProbMetric: 52.7713

Epoch 129: val_loss did not improve from 51.95703
196/196 - 42s - loss: 51.6998 - MinusLogProbMetric: 51.6998 - val_loss: 52.7713 - val_MinusLogProbMetric: 52.7713 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 130/1000
2023-10-27 14:21:42.357 
Epoch 130/1000 
	 loss: 52.0184, MinusLogProbMetric: 52.0184, val_loss: 52.1015, val_MinusLogProbMetric: 52.1015

Epoch 130: val_loss did not improve from 51.95703
196/196 - 42s - loss: 52.0184 - MinusLogProbMetric: 52.0184 - val_loss: 52.1015 - val_MinusLogProbMetric: 52.1015 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 131/1000
2023-10-27 14:22:24.302 
Epoch 131/1000 
	 loss: 51.7442, MinusLogProbMetric: 51.7442, val_loss: 51.5819, val_MinusLogProbMetric: 51.5819

Epoch 131: val_loss improved from 51.95703 to 51.58191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 43s - loss: 51.7442 - MinusLogProbMetric: 51.7442 - val_loss: 51.5819 - val_MinusLogProbMetric: 51.5819 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 132/1000
2023-10-27 14:23:06.368 
Epoch 132/1000 
	 loss: 51.4467, MinusLogProbMetric: 51.4467, val_loss: 52.1398, val_MinusLogProbMetric: 52.1398

Epoch 132: val_loss did not improve from 51.58191
196/196 - 41s - loss: 51.4467 - MinusLogProbMetric: 51.4467 - val_loss: 52.1398 - val_MinusLogProbMetric: 52.1398 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 133/1000
2023-10-27 14:23:47.468 
Epoch 133/1000 
	 loss: 51.4840, MinusLogProbMetric: 51.4840, val_loss: 51.9841, val_MinusLogProbMetric: 51.9841

Epoch 133: val_loss did not improve from 51.58191
196/196 - 41s - loss: 51.4840 - MinusLogProbMetric: 51.4840 - val_loss: 51.9841 - val_MinusLogProbMetric: 51.9841 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 134/1000
2023-10-27 14:24:28.906 
Epoch 134/1000 
	 loss: 51.2120, MinusLogProbMetric: 51.2120, val_loss: 53.3826, val_MinusLogProbMetric: 53.3826

Epoch 134: val_loss did not improve from 51.58191
196/196 - 41s - loss: 51.2120 - MinusLogProbMetric: 51.2120 - val_loss: 53.3826 - val_MinusLogProbMetric: 53.3826 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 135/1000
2023-10-27 14:25:10.425 
Epoch 135/1000 
	 loss: 51.4414, MinusLogProbMetric: 51.4414, val_loss: 53.0325, val_MinusLogProbMetric: 53.0325

Epoch 135: val_loss did not improve from 51.58191
196/196 - 42s - loss: 51.4414 - MinusLogProbMetric: 51.4414 - val_loss: 53.0325 - val_MinusLogProbMetric: 53.0325 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 136/1000
2023-10-27 14:25:52.553 
Epoch 136/1000 
	 loss: 51.2919, MinusLogProbMetric: 51.2919, val_loss: 51.3317, val_MinusLogProbMetric: 51.3317

Epoch 136: val_loss improved from 51.58191 to 51.33168, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 43s - loss: 51.2919 - MinusLogProbMetric: 51.2919 - val_loss: 51.3317 - val_MinusLogProbMetric: 51.3317 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 137/1000
2023-10-27 14:26:35.419 
Epoch 137/1000 
	 loss: 51.2362, MinusLogProbMetric: 51.2362, val_loss: 52.7038, val_MinusLogProbMetric: 52.7038

Epoch 137: val_loss did not improve from 51.33168
196/196 - 42s - loss: 51.2362 - MinusLogProbMetric: 51.2362 - val_loss: 52.7038 - val_MinusLogProbMetric: 52.7038 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 138/1000
2023-10-27 14:27:16.978 
Epoch 138/1000 
	 loss: 50.8765, MinusLogProbMetric: 50.8765, val_loss: 52.3192, val_MinusLogProbMetric: 52.3192

Epoch 138: val_loss did not improve from 51.33168
196/196 - 42s - loss: 50.8765 - MinusLogProbMetric: 50.8765 - val_loss: 52.3192 - val_MinusLogProbMetric: 52.3192 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 139/1000
2023-10-27 14:27:58.955 
Epoch 139/1000 
	 loss: 51.2640, MinusLogProbMetric: 51.2640, val_loss: 50.9642, val_MinusLogProbMetric: 50.9642

Epoch 139: val_loss improved from 51.33168 to 50.96416, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 43s - loss: 51.2640 - MinusLogProbMetric: 51.2640 - val_loss: 50.9642 - val_MinusLogProbMetric: 50.9642 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 140/1000
2023-10-27 14:28:42.498 
Epoch 140/1000 
	 loss: 51.2980, MinusLogProbMetric: 51.2980, val_loss: 51.0053, val_MinusLogProbMetric: 51.0053

Epoch 140: val_loss did not improve from 50.96416
196/196 - 43s - loss: 51.2980 - MinusLogProbMetric: 51.2980 - val_loss: 51.0053 - val_MinusLogProbMetric: 51.0053 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 141/1000
2023-10-27 14:29:25.435 
Epoch 141/1000 
	 loss: 51.0741, MinusLogProbMetric: 51.0741, val_loss: 51.4274, val_MinusLogProbMetric: 51.4274

Epoch 141: val_loss did not improve from 50.96416
196/196 - 43s - loss: 51.0741 - MinusLogProbMetric: 51.0741 - val_loss: 51.4274 - val_MinusLogProbMetric: 51.4274 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 142/1000
2023-10-27 14:30:08.861 
Epoch 142/1000 
	 loss: 50.6649, MinusLogProbMetric: 50.6649, val_loss: 51.0584, val_MinusLogProbMetric: 51.0584

Epoch 142: val_loss did not improve from 50.96416
196/196 - 43s - loss: 50.6649 - MinusLogProbMetric: 50.6649 - val_loss: 51.0584 - val_MinusLogProbMetric: 51.0584 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 143/1000
2023-10-27 14:30:51.657 
Epoch 143/1000 
	 loss: 50.7935, MinusLogProbMetric: 50.7935, val_loss: 51.0397, val_MinusLogProbMetric: 51.0397

Epoch 143: val_loss did not improve from 50.96416
196/196 - 43s - loss: 50.7935 - MinusLogProbMetric: 50.7935 - val_loss: 51.0397 - val_MinusLogProbMetric: 51.0397 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 144/1000
2023-10-27 14:31:35.472 
Epoch 144/1000 
	 loss: 50.8983, MinusLogProbMetric: 50.8983, val_loss: 51.9156, val_MinusLogProbMetric: 51.9156

Epoch 144: val_loss did not improve from 50.96416
196/196 - 44s - loss: 50.8983 - MinusLogProbMetric: 50.8983 - val_loss: 51.9156 - val_MinusLogProbMetric: 51.9156 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 145/1000
2023-10-27 14:32:20.005 
Epoch 145/1000 
	 loss: 50.8576, MinusLogProbMetric: 50.8576, val_loss: 50.5113, val_MinusLogProbMetric: 50.5113

Epoch 145: val_loss improved from 50.96416 to 50.51135, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 45s - loss: 50.8576 - MinusLogProbMetric: 50.8576 - val_loss: 50.5113 - val_MinusLogProbMetric: 50.5113 - lr: 3.3333e-04 - 45s/epoch - 232ms/step
Epoch 146/1000
2023-10-27 14:33:05.651 
Epoch 146/1000 
	 loss: 50.5799, MinusLogProbMetric: 50.5799, val_loss: 51.2610, val_MinusLogProbMetric: 51.2610

Epoch 146: val_loss did not improve from 50.51135
196/196 - 45s - loss: 50.5799 - MinusLogProbMetric: 50.5799 - val_loss: 51.2610 - val_MinusLogProbMetric: 51.2610 - lr: 3.3333e-04 - 45s/epoch - 228ms/step
Epoch 147/1000
2023-10-27 14:33:50.377 
Epoch 147/1000 
	 loss: 50.9836, MinusLogProbMetric: 50.9836, val_loss: 51.6939, val_MinusLogProbMetric: 51.6939

Epoch 147: val_loss did not improve from 50.51135
196/196 - 45s - loss: 50.9836 - MinusLogProbMetric: 50.9836 - val_loss: 51.6939 - val_MinusLogProbMetric: 51.6939 - lr: 3.3333e-04 - 45s/epoch - 228ms/step
Epoch 148/1000
2023-10-27 14:34:34.514 
Epoch 148/1000 
	 loss: 50.5847, MinusLogProbMetric: 50.5847, val_loss: 51.1356, val_MinusLogProbMetric: 51.1356

Epoch 148: val_loss did not improve from 50.51135
196/196 - 44s - loss: 50.5847 - MinusLogProbMetric: 50.5847 - val_loss: 51.1356 - val_MinusLogProbMetric: 51.1356 - lr: 3.3333e-04 - 44s/epoch - 225ms/step
Epoch 149/1000
2023-10-27 14:35:19.392 
Epoch 149/1000 
	 loss: 50.5983, MinusLogProbMetric: 50.5983, val_loss: 50.5592, val_MinusLogProbMetric: 50.5592

Epoch 149: val_loss did not improve from 50.51135
196/196 - 45s - loss: 50.5983 - MinusLogProbMetric: 50.5983 - val_loss: 50.5592 - val_MinusLogProbMetric: 50.5592 - lr: 3.3333e-04 - 45s/epoch - 229ms/step
Epoch 150/1000
2023-10-27 14:36:02.430 
Epoch 150/1000 
	 loss: 50.7301, MinusLogProbMetric: 50.7301, val_loss: 50.7275, val_MinusLogProbMetric: 50.7275

Epoch 150: val_loss did not improve from 50.51135
196/196 - 43s - loss: 50.7301 - MinusLogProbMetric: 50.7301 - val_loss: 50.7275 - val_MinusLogProbMetric: 50.7275 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 151/1000
2023-10-27 14:36:45.002 
Epoch 151/1000 
	 loss: 50.4744, MinusLogProbMetric: 50.4744, val_loss: 50.4284, val_MinusLogProbMetric: 50.4284

Epoch 151: val_loss improved from 50.51135 to 50.42841, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 43s - loss: 50.4744 - MinusLogProbMetric: 50.4744 - val_loss: 50.4284 - val_MinusLogProbMetric: 50.4284 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 152/1000
2023-10-27 14:37:27.827 
Epoch 152/1000 
	 loss: 50.6211, MinusLogProbMetric: 50.6211, val_loss: 51.5723, val_MinusLogProbMetric: 51.5723

Epoch 152: val_loss did not improve from 50.42841
196/196 - 42s - loss: 50.6211 - MinusLogProbMetric: 50.6211 - val_loss: 51.5723 - val_MinusLogProbMetric: 51.5723 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 153/1000
2023-10-27 14:38:04.280 
Epoch 153/1000 
	 loss: 50.5257, MinusLogProbMetric: 50.5257, val_loss: 50.8424, val_MinusLogProbMetric: 50.8424

Epoch 153: val_loss did not improve from 50.42841
196/196 - 36s - loss: 50.5257 - MinusLogProbMetric: 50.5257 - val_loss: 50.8424 - val_MinusLogProbMetric: 50.8424 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 154/1000
2023-10-27 14:38:41.907 
Epoch 154/1000 
	 loss: 50.4873, MinusLogProbMetric: 50.4873, val_loss: 52.6599, val_MinusLogProbMetric: 52.6599

Epoch 154: val_loss did not improve from 50.42841
196/196 - 38s - loss: 50.4873 - MinusLogProbMetric: 50.4873 - val_loss: 52.6599 - val_MinusLogProbMetric: 52.6599 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 155/1000
2023-10-27 14:39:24.025 
Epoch 155/1000 
	 loss: 50.6144, MinusLogProbMetric: 50.6144, val_loss: 51.7431, val_MinusLogProbMetric: 51.7431

Epoch 155: val_loss did not improve from 50.42841
196/196 - 42s - loss: 50.6144 - MinusLogProbMetric: 50.6144 - val_loss: 51.7431 - val_MinusLogProbMetric: 51.7431 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 156/1000
2023-10-27 14:40:08.337 
Epoch 156/1000 
	 loss: 50.3810, MinusLogProbMetric: 50.3810, val_loss: 50.1673, val_MinusLogProbMetric: 50.1673

Epoch 156: val_loss improved from 50.42841 to 50.16727, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 45s - loss: 50.3810 - MinusLogProbMetric: 50.3810 - val_loss: 50.1673 - val_MinusLogProbMetric: 50.1673 - lr: 3.3333e-04 - 45s/epoch - 230ms/step
Epoch 157/1000
2023-10-27 14:40:51.554 
Epoch 157/1000 
	 loss: 50.3301, MinusLogProbMetric: 50.3301, val_loss: 50.5337, val_MinusLogProbMetric: 50.5337

Epoch 157: val_loss did not improve from 50.16727
196/196 - 42s - loss: 50.3301 - MinusLogProbMetric: 50.3301 - val_loss: 50.5337 - val_MinusLogProbMetric: 50.5337 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 158/1000
2023-10-27 14:41:34.436 
Epoch 158/1000 
	 loss: 50.0603, MinusLogProbMetric: 50.0603, val_loss: 51.9986, val_MinusLogProbMetric: 51.9986

Epoch 158: val_loss did not improve from 50.16727
196/196 - 43s - loss: 50.0603 - MinusLogProbMetric: 50.0603 - val_loss: 51.9986 - val_MinusLogProbMetric: 51.9986 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 159/1000
2023-10-27 14:42:17.477 
Epoch 159/1000 
	 loss: 50.2803, MinusLogProbMetric: 50.2803, val_loss: 50.5709, val_MinusLogProbMetric: 50.5709

Epoch 159: val_loss did not improve from 50.16727
196/196 - 43s - loss: 50.2803 - MinusLogProbMetric: 50.2803 - val_loss: 50.5709 - val_MinusLogProbMetric: 50.5709 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 160/1000
2023-10-27 14:42:59.380 
Epoch 160/1000 
	 loss: 50.2304, MinusLogProbMetric: 50.2304, val_loss: 49.9626, val_MinusLogProbMetric: 49.9626

Epoch 160: val_loss improved from 50.16727 to 49.96260, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 43s - loss: 50.2304 - MinusLogProbMetric: 50.2304 - val_loss: 49.9626 - val_MinusLogProbMetric: 49.9626 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 161/1000
2023-10-27 14:43:42.664 
Epoch 161/1000 
	 loss: 50.2810, MinusLogProbMetric: 50.2810, val_loss: 51.7330, val_MinusLogProbMetric: 51.7330

Epoch 161: val_loss did not improve from 49.96260
196/196 - 43s - loss: 50.2810 - MinusLogProbMetric: 50.2810 - val_loss: 51.7330 - val_MinusLogProbMetric: 51.7330 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 162/1000
2023-10-27 14:44:25.543 
Epoch 162/1000 
	 loss: 50.0264, MinusLogProbMetric: 50.0264, val_loss: 53.4751, val_MinusLogProbMetric: 53.4751

Epoch 162: val_loss did not improve from 49.96260
196/196 - 43s - loss: 50.0264 - MinusLogProbMetric: 50.0264 - val_loss: 53.4751 - val_MinusLogProbMetric: 53.4751 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 163/1000
2023-10-27 14:45:08.222 
Epoch 163/1000 
	 loss: 49.8865, MinusLogProbMetric: 49.8865, val_loss: 49.5811, val_MinusLogProbMetric: 49.5811

Epoch 163: val_loss improved from 49.96260 to 49.58109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 43s - loss: 49.8865 - MinusLogProbMetric: 49.8865 - val_loss: 49.5811 - val_MinusLogProbMetric: 49.5811 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 164/1000
2023-10-27 14:45:52.394 
Epoch 164/1000 
	 loss: 50.2578, MinusLogProbMetric: 50.2578, val_loss: 49.6093, val_MinusLogProbMetric: 49.6093

Epoch 164: val_loss did not improve from 49.58109
196/196 - 43s - loss: 50.2578 - MinusLogProbMetric: 50.2578 - val_loss: 49.6093 - val_MinusLogProbMetric: 49.6093 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 165/1000
2023-10-27 14:46:34.454 
Epoch 165/1000 
	 loss: 49.8900, MinusLogProbMetric: 49.8900, val_loss: 50.7920, val_MinusLogProbMetric: 50.7920

Epoch 165: val_loss did not improve from 49.58109
196/196 - 42s - loss: 49.8900 - MinusLogProbMetric: 49.8900 - val_loss: 50.7920 - val_MinusLogProbMetric: 50.7920 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 166/1000
2023-10-27 14:47:17.279 
Epoch 166/1000 
	 loss: 50.0470, MinusLogProbMetric: 50.0470, val_loss: 49.4030, val_MinusLogProbMetric: 49.4030

Epoch 166: val_loss improved from 49.58109 to 49.40301, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 44s - loss: 50.0470 - MinusLogProbMetric: 50.0470 - val_loss: 49.4030 - val_MinusLogProbMetric: 49.4030 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 167/1000
2023-10-27 14:48:00.691 
Epoch 167/1000 
	 loss: 49.8611, MinusLogProbMetric: 49.8611, val_loss: 50.8696, val_MinusLogProbMetric: 50.8696

Epoch 167: val_loss did not improve from 49.40301
196/196 - 43s - loss: 49.8611 - MinusLogProbMetric: 49.8611 - val_loss: 50.8696 - val_MinusLogProbMetric: 50.8696 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 168/1000
2023-10-27 14:48:42.674 
Epoch 168/1000 
	 loss: 49.8798, MinusLogProbMetric: 49.8798, val_loss: 49.5357, val_MinusLogProbMetric: 49.5357

Epoch 168: val_loss did not improve from 49.40301
196/196 - 42s - loss: 49.8798 - MinusLogProbMetric: 49.8798 - val_loss: 49.5357 - val_MinusLogProbMetric: 49.5357 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 169/1000
2023-10-27 14:49:25.029 
Epoch 169/1000 
	 loss: 49.8151, MinusLogProbMetric: 49.8151, val_loss: 50.5025, val_MinusLogProbMetric: 50.5025

Epoch 169: val_loss did not improve from 49.40301
196/196 - 42s - loss: 49.8151 - MinusLogProbMetric: 49.8151 - val_loss: 50.5025 - val_MinusLogProbMetric: 50.5025 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 170/1000
2023-10-27 14:50:06.697 
Epoch 170/1000 
	 loss: 49.7335, MinusLogProbMetric: 49.7335, val_loss: 49.7708, val_MinusLogProbMetric: 49.7708

Epoch 170: val_loss did not improve from 49.40301
196/196 - 42s - loss: 49.7335 - MinusLogProbMetric: 49.7335 - val_loss: 49.7708 - val_MinusLogProbMetric: 49.7708 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 171/1000
2023-10-27 14:50:48.869 
Epoch 171/1000 
	 loss: 49.7434, MinusLogProbMetric: 49.7434, val_loss: 56.8494, val_MinusLogProbMetric: 56.8494

Epoch 171: val_loss did not improve from 49.40301
196/196 - 42s - loss: 49.7434 - MinusLogProbMetric: 49.7434 - val_loss: 56.8494 - val_MinusLogProbMetric: 56.8494 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 172/1000
2023-10-27 14:51:31.018 
Epoch 172/1000 
	 loss: 50.3263, MinusLogProbMetric: 50.3263, val_loss: 51.2300, val_MinusLogProbMetric: 51.2300

Epoch 172: val_loss did not improve from 49.40301
196/196 - 42s - loss: 50.3263 - MinusLogProbMetric: 50.3263 - val_loss: 51.2300 - val_MinusLogProbMetric: 51.2300 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 173/1000
2023-10-27 14:52:13.158 
Epoch 173/1000 
	 loss: 49.7061, MinusLogProbMetric: 49.7061, val_loss: 50.2660, val_MinusLogProbMetric: 50.2660

Epoch 173: val_loss did not improve from 49.40301
196/196 - 42s - loss: 49.7061 - MinusLogProbMetric: 49.7061 - val_loss: 50.2660 - val_MinusLogProbMetric: 50.2660 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 174/1000
2023-10-27 14:52:55.019 
Epoch 174/1000 
	 loss: 50.0235, MinusLogProbMetric: 50.0235, val_loss: 54.5424, val_MinusLogProbMetric: 54.5424

Epoch 174: val_loss did not improve from 49.40301
196/196 - 42s - loss: 50.0235 - MinusLogProbMetric: 50.0235 - val_loss: 54.5424 - val_MinusLogProbMetric: 54.5424 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 175/1000
2023-10-27 14:53:37.874 
Epoch 175/1000 
	 loss: 49.5747, MinusLogProbMetric: 49.5747, val_loss: 49.2866, val_MinusLogProbMetric: 49.2866

Epoch 175: val_loss improved from 49.40301 to 49.28657, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 44s - loss: 49.5747 - MinusLogProbMetric: 49.5747 - val_loss: 49.2866 - val_MinusLogProbMetric: 49.2866 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 176/1000
2023-10-27 14:54:21.862 
Epoch 176/1000 
	 loss: 49.5109, MinusLogProbMetric: 49.5109, val_loss: 50.4937, val_MinusLogProbMetric: 50.4937

Epoch 176: val_loss did not improve from 49.28657
196/196 - 43s - loss: 49.5109 - MinusLogProbMetric: 49.5109 - val_loss: 50.4937 - val_MinusLogProbMetric: 50.4937 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 177/1000
2023-10-27 14:55:04.983 
Epoch 177/1000 
	 loss: 49.6549, MinusLogProbMetric: 49.6549, val_loss: 51.2383, val_MinusLogProbMetric: 51.2383

Epoch 177: val_loss did not improve from 49.28657
196/196 - 43s - loss: 49.6549 - MinusLogProbMetric: 49.6549 - val_loss: 51.2383 - val_MinusLogProbMetric: 51.2383 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 178/1000
2023-10-27 14:55:48.313 
Epoch 178/1000 
	 loss: 49.5539, MinusLogProbMetric: 49.5539, val_loss: 49.9979, val_MinusLogProbMetric: 49.9979

Epoch 178: val_loss did not improve from 49.28657
196/196 - 43s - loss: 49.5539 - MinusLogProbMetric: 49.5539 - val_loss: 49.9979 - val_MinusLogProbMetric: 49.9979 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 179/1000
2023-10-27 14:56:31.123 
Epoch 179/1000 
	 loss: 49.3597, MinusLogProbMetric: 49.3597, val_loss: 51.4046, val_MinusLogProbMetric: 51.4046

Epoch 179: val_loss did not improve from 49.28657
196/196 - 43s - loss: 49.3597 - MinusLogProbMetric: 49.3597 - val_loss: 51.4046 - val_MinusLogProbMetric: 51.4046 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 180/1000
2023-10-27 14:57:09.716 
Epoch 180/1000 
	 loss: 49.5417, MinusLogProbMetric: 49.5417, val_loss: 49.8528, val_MinusLogProbMetric: 49.8528

Epoch 180: val_loss did not improve from 49.28657
196/196 - 39s - loss: 49.5417 - MinusLogProbMetric: 49.5417 - val_loss: 49.8528 - val_MinusLogProbMetric: 49.8528 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 181/1000
2023-10-27 14:57:45.884 
Epoch 181/1000 
	 loss: 49.4603, MinusLogProbMetric: 49.4603, val_loss: 49.7645, val_MinusLogProbMetric: 49.7645

Epoch 181: val_loss did not improve from 49.28657
196/196 - 36s - loss: 49.4603 - MinusLogProbMetric: 49.4603 - val_loss: 49.7645 - val_MinusLogProbMetric: 49.7645 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 182/1000
2023-10-27 14:58:26.042 
Epoch 182/1000 
	 loss: 49.3324, MinusLogProbMetric: 49.3324, val_loss: 49.3983, val_MinusLogProbMetric: 49.3983

Epoch 182: val_loss did not improve from 49.28657
196/196 - 40s - loss: 49.3324 - MinusLogProbMetric: 49.3324 - val_loss: 49.3983 - val_MinusLogProbMetric: 49.3983 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 183/1000
2023-10-27 14:59:10.725 
Epoch 183/1000 
	 loss: 49.4663, MinusLogProbMetric: 49.4663, val_loss: 50.2242, val_MinusLogProbMetric: 50.2242

Epoch 183: val_loss did not improve from 49.28657
196/196 - 45s - loss: 49.4663 - MinusLogProbMetric: 49.4663 - val_loss: 50.2242 - val_MinusLogProbMetric: 50.2242 - lr: 3.3333e-04 - 45s/epoch - 228ms/step
Epoch 184/1000
2023-10-27 14:59:49.399 
Epoch 184/1000 
	 loss: 49.2613, MinusLogProbMetric: 49.2613, val_loss: 49.3695, val_MinusLogProbMetric: 49.3695

Epoch 184: val_loss did not improve from 49.28657
196/196 - 39s - loss: 49.2613 - MinusLogProbMetric: 49.2613 - val_loss: 49.3695 - val_MinusLogProbMetric: 49.3695 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 185/1000
2023-10-27 15:00:31.314 
Epoch 185/1000 
	 loss: 49.4027, MinusLogProbMetric: 49.4027, val_loss: 49.5502, val_MinusLogProbMetric: 49.5502

Epoch 185: val_loss did not improve from 49.28657
196/196 - 42s - loss: 49.4027 - MinusLogProbMetric: 49.4027 - val_loss: 49.5502 - val_MinusLogProbMetric: 49.5502 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 186/1000
2023-10-27 15:01:09.484 
Epoch 186/1000 
	 loss: 49.4754, MinusLogProbMetric: 49.4754, val_loss: 50.0812, val_MinusLogProbMetric: 50.0812

Epoch 186: val_loss did not improve from 49.28657
196/196 - 38s - loss: 49.4754 - MinusLogProbMetric: 49.4754 - val_loss: 50.0812 - val_MinusLogProbMetric: 50.0812 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 187/1000
2023-10-27 15:01:53.763 
Epoch 187/1000 
	 loss: 49.2334, MinusLogProbMetric: 49.2334, val_loss: 51.0214, val_MinusLogProbMetric: 51.0214

Epoch 187: val_loss did not improve from 49.28657
196/196 - 44s - loss: 49.2334 - MinusLogProbMetric: 49.2334 - val_loss: 51.0214 - val_MinusLogProbMetric: 51.0214 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 188/1000
2023-10-27 15:02:34.421 
Epoch 188/1000 
	 loss: 49.3853, MinusLogProbMetric: 49.3853, val_loss: 51.2591, val_MinusLogProbMetric: 51.2591

Epoch 188: val_loss did not improve from 49.28657
196/196 - 41s - loss: 49.3853 - MinusLogProbMetric: 49.3853 - val_loss: 51.2591 - val_MinusLogProbMetric: 51.2591 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 189/1000
2023-10-27 15:03:16.196 
Epoch 189/1000 
	 loss: 49.2294, MinusLogProbMetric: 49.2294, val_loss: 48.6639, val_MinusLogProbMetric: 48.6639

Epoch 189: val_loss improved from 49.28657 to 48.66393, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 43s - loss: 49.2294 - MinusLogProbMetric: 49.2294 - val_loss: 48.6639 - val_MinusLogProbMetric: 48.6639 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 190/1000
2023-10-27 15:03:57.205 
Epoch 190/1000 
	 loss: 49.2127, MinusLogProbMetric: 49.2127, val_loss: 50.0877, val_MinusLogProbMetric: 50.0877

Epoch 190: val_loss did not improve from 48.66393
196/196 - 40s - loss: 49.2127 - MinusLogProbMetric: 49.2127 - val_loss: 50.0877 - val_MinusLogProbMetric: 50.0877 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 191/1000
2023-10-27 15:04:40.911 
Epoch 191/1000 
	 loss: 49.1553, MinusLogProbMetric: 49.1553, val_loss: 48.6964, val_MinusLogProbMetric: 48.6964

Epoch 191: val_loss did not improve from 48.66393
196/196 - 44s - loss: 49.1553 - MinusLogProbMetric: 49.1553 - val_loss: 48.6964 - val_MinusLogProbMetric: 48.6964 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 192/1000
2023-10-27 15:05:24.122 
Epoch 192/1000 
	 loss: 49.3680, MinusLogProbMetric: 49.3680, val_loss: 50.0046, val_MinusLogProbMetric: 50.0046

Epoch 192: val_loss did not improve from 48.66393
196/196 - 43s - loss: 49.3680 - MinusLogProbMetric: 49.3680 - val_loss: 50.0046 - val_MinusLogProbMetric: 50.0046 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 193/1000
2023-10-27 15:06:04.083 
Epoch 193/1000 
	 loss: 49.2897, MinusLogProbMetric: 49.2897, val_loss: 48.7631, val_MinusLogProbMetric: 48.7631

Epoch 193: val_loss did not improve from 48.66393
196/196 - 40s - loss: 49.2897 - MinusLogProbMetric: 49.2897 - val_loss: 48.7631 - val_MinusLogProbMetric: 48.7631 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 194/1000
2023-10-27 15:06:44.291 
Epoch 194/1000 
	 loss: 49.6247, MinusLogProbMetric: 49.6247, val_loss: 53.9498, val_MinusLogProbMetric: 53.9498

Epoch 194: val_loss did not improve from 48.66393
196/196 - 40s - loss: 49.6247 - MinusLogProbMetric: 49.6247 - val_loss: 53.9498 - val_MinusLogProbMetric: 53.9498 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 195/1000
2023-10-27 15:07:27.865 
Epoch 195/1000 
	 loss: 50.0224, MinusLogProbMetric: 50.0224, val_loss: 49.5754, val_MinusLogProbMetric: 49.5754

Epoch 195: val_loss did not improve from 48.66393
196/196 - 44s - loss: 50.0224 - MinusLogProbMetric: 50.0224 - val_loss: 49.5754 - val_MinusLogProbMetric: 49.5754 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 196/1000
2023-10-27 15:08:08.948 
Epoch 196/1000 
	 loss: 48.9013, MinusLogProbMetric: 48.9013, val_loss: 49.1859, val_MinusLogProbMetric: 49.1859

Epoch 196: val_loss did not improve from 48.66393
196/196 - 41s - loss: 48.9013 - MinusLogProbMetric: 48.9013 - val_loss: 49.1859 - val_MinusLogProbMetric: 49.1859 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 197/1000
2023-10-27 15:08:47.216 
Epoch 197/1000 
	 loss: 49.2342, MinusLogProbMetric: 49.2342, val_loss: 48.9975, val_MinusLogProbMetric: 48.9975

Epoch 197: val_loss did not improve from 48.66393
196/196 - 38s - loss: 49.2342 - MinusLogProbMetric: 49.2342 - val_loss: 48.9975 - val_MinusLogProbMetric: 48.9975 - lr: 3.3333e-04 - 38s/epoch - 195ms/step
Epoch 198/1000
2023-10-27 15:09:27.246 
Epoch 198/1000 
	 loss: 49.2112, MinusLogProbMetric: 49.2112, val_loss: 49.0463, val_MinusLogProbMetric: 49.0463

Epoch 198: val_loss did not improve from 48.66393
196/196 - 40s - loss: 49.2112 - MinusLogProbMetric: 49.2112 - val_loss: 49.0463 - val_MinusLogProbMetric: 49.0463 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 199/1000
2023-10-27 15:10:10.437 
Epoch 199/1000 
	 loss: 48.9749, MinusLogProbMetric: 48.9749, val_loss: 49.6734, val_MinusLogProbMetric: 49.6734

Epoch 199: val_loss did not improve from 48.66393
196/196 - 43s - loss: 48.9749 - MinusLogProbMetric: 48.9749 - val_loss: 49.6734 - val_MinusLogProbMetric: 49.6734 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 200/1000
2023-10-27 15:10:53.626 
Epoch 200/1000 
	 loss: 49.0835, MinusLogProbMetric: 49.0835, val_loss: 48.4821, val_MinusLogProbMetric: 48.4821

Epoch 200: val_loss improved from 48.66393 to 48.48210, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 44s - loss: 49.0835 - MinusLogProbMetric: 49.0835 - val_loss: 48.4821 - val_MinusLogProbMetric: 48.4821 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 201/1000
2023-10-27 15:11:35.116 
Epoch 201/1000 
	 loss: 50.9885, MinusLogProbMetric: 50.9885, val_loss: 50.4772, val_MinusLogProbMetric: 50.4772

Epoch 201: val_loss did not improve from 48.48210
196/196 - 41s - loss: 50.9885 - MinusLogProbMetric: 50.9885 - val_loss: 50.4772 - val_MinusLogProbMetric: 50.4772 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 202/1000
2023-10-27 15:12:17.825 
Epoch 202/1000 
	 loss: 50.4627, MinusLogProbMetric: 50.4627, val_loss: 51.0493, val_MinusLogProbMetric: 51.0493

Epoch 202: val_loss did not improve from 48.48210
196/196 - 43s - loss: 50.4627 - MinusLogProbMetric: 50.4627 - val_loss: 51.0493 - val_MinusLogProbMetric: 51.0493 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 203/1000
2023-10-27 15:13:00.738 
Epoch 203/1000 
	 loss: 49.3800, MinusLogProbMetric: 49.3800, val_loss: 48.9893, val_MinusLogProbMetric: 48.9893

Epoch 203: val_loss did not improve from 48.48210
196/196 - 43s - loss: 49.3800 - MinusLogProbMetric: 49.3800 - val_loss: 48.9893 - val_MinusLogProbMetric: 48.9893 - lr: 3.3333e-04 - 43s/epoch - 219ms/step
Epoch 204/1000
2023-10-27 15:13:42.169 
Epoch 204/1000 
	 loss: 48.5983, MinusLogProbMetric: 48.5983, val_loss: 49.3098, val_MinusLogProbMetric: 49.3098

Epoch 204: val_loss did not improve from 48.48210
196/196 - 41s - loss: 48.5983 - MinusLogProbMetric: 48.5983 - val_loss: 49.3098 - val_MinusLogProbMetric: 49.3098 - lr: 3.3333e-04 - 41s/epoch - 211ms/step
Epoch 205/1000
2023-10-27 15:14:21.851 
Epoch 205/1000 
	 loss: 48.8194, MinusLogProbMetric: 48.8194, val_loss: 51.3223, val_MinusLogProbMetric: 51.3223

Epoch 205: val_loss did not improve from 48.48210
196/196 - 40s - loss: 48.8194 - MinusLogProbMetric: 48.8194 - val_loss: 51.3223 - val_MinusLogProbMetric: 51.3223 - lr: 3.3333e-04 - 40s/epoch - 202ms/step
Epoch 206/1000
2023-10-27 15:15:03.422 
Epoch 206/1000 
	 loss: 48.8565, MinusLogProbMetric: 48.8565, val_loss: 49.8463, val_MinusLogProbMetric: 49.8463

Epoch 206: val_loss did not improve from 48.48210
196/196 - 42s - loss: 48.8565 - MinusLogProbMetric: 48.8565 - val_loss: 49.8463 - val_MinusLogProbMetric: 49.8463 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 207/1000
2023-10-27 15:15:43.524 
Epoch 207/1000 
	 loss: 48.8638, MinusLogProbMetric: 48.8638, val_loss: 48.7531, val_MinusLogProbMetric: 48.7531

Epoch 207: val_loss did not improve from 48.48210
196/196 - 40s - loss: 48.8638 - MinusLogProbMetric: 48.8638 - val_loss: 48.7531 - val_MinusLogProbMetric: 48.7531 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 208/1000
2023-10-27 15:16:27.241 
Epoch 208/1000 
	 loss: 48.7551, MinusLogProbMetric: 48.7551, val_loss: 49.2028, val_MinusLogProbMetric: 49.2028

Epoch 208: val_loss did not improve from 48.48210
196/196 - 44s - loss: 48.7551 - MinusLogProbMetric: 48.7551 - val_loss: 49.2028 - val_MinusLogProbMetric: 49.2028 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 209/1000
2023-10-27 15:17:06.650 
Epoch 209/1000 
	 loss: 48.6633, MinusLogProbMetric: 48.6633, val_loss: 48.9863, val_MinusLogProbMetric: 48.9863

Epoch 209: val_loss did not improve from 48.48210
196/196 - 39s - loss: 48.6633 - MinusLogProbMetric: 48.6633 - val_loss: 48.9863 - val_MinusLogProbMetric: 48.9863 - lr: 3.3333e-04 - 39s/epoch - 201ms/step
Epoch 210/1000
2023-10-27 15:17:46.648 
Epoch 210/1000 
	 loss: 48.5498, MinusLogProbMetric: 48.5498, val_loss: 48.5860, val_MinusLogProbMetric: 48.5860

Epoch 210: val_loss did not improve from 48.48210
196/196 - 40s - loss: 48.5498 - MinusLogProbMetric: 48.5498 - val_loss: 48.5860 - val_MinusLogProbMetric: 48.5860 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 211/1000
2023-10-27 15:18:26.926 
Epoch 211/1000 
	 loss: 48.6248, MinusLogProbMetric: 48.6248, val_loss: 48.2679, val_MinusLogProbMetric: 48.2679

Epoch 211: val_loss improved from 48.48210 to 48.26786, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 41s - loss: 48.6248 - MinusLogProbMetric: 48.6248 - val_loss: 48.2679 - val_MinusLogProbMetric: 48.2679 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 212/1000
2023-10-27 15:19:11.067 
Epoch 212/1000 
	 loss: 48.7046, MinusLogProbMetric: 48.7046, val_loss: 48.5254, val_MinusLogProbMetric: 48.5254

Epoch 212: val_loss did not improve from 48.26786
196/196 - 43s - loss: 48.7046 - MinusLogProbMetric: 48.7046 - val_loss: 48.5254 - val_MinusLogProbMetric: 48.5254 - lr: 3.3333e-04 - 43s/epoch - 221ms/step
Epoch 213/1000
2023-10-27 15:19:48.695 
Epoch 213/1000 
	 loss: 48.8945, MinusLogProbMetric: 48.8945, val_loss: 49.5789, val_MinusLogProbMetric: 49.5789

Epoch 213: val_loss did not improve from 48.26786
196/196 - 38s - loss: 48.8945 - MinusLogProbMetric: 48.8945 - val_loss: 49.5789 - val_MinusLogProbMetric: 49.5789 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 214/1000
2023-10-27 15:20:27.287 
Epoch 214/1000 
	 loss: 48.7288, MinusLogProbMetric: 48.7288, val_loss: 50.3961, val_MinusLogProbMetric: 50.3961

Epoch 214: val_loss did not improve from 48.26786
196/196 - 39s - loss: 48.7288 - MinusLogProbMetric: 48.7288 - val_loss: 50.3961 - val_MinusLogProbMetric: 50.3961 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 215/1000
2023-10-27 15:21:11.468 
Epoch 215/1000 
	 loss: 48.5853, MinusLogProbMetric: 48.5853, val_loss: 48.2560, val_MinusLogProbMetric: 48.2560

Epoch 215: val_loss improved from 48.26786 to 48.25600, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 45s - loss: 48.5853 - MinusLogProbMetric: 48.5853 - val_loss: 48.2560 - val_MinusLogProbMetric: 48.2560 - lr: 3.3333e-04 - 45s/epoch - 230ms/step
Epoch 216/1000
2023-10-27 15:21:52.773 
Epoch 216/1000 
	 loss: 48.5175, MinusLogProbMetric: 48.5175, val_loss: 48.9891, val_MinusLogProbMetric: 48.9891

Epoch 216: val_loss did not improve from 48.25600
196/196 - 40s - loss: 48.5175 - MinusLogProbMetric: 48.5175 - val_loss: 48.9891 - val_MinusLogProbMetric: 48.9891 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 217/1000
2023-10-27 15:22:31.805 
Epoch 217/1000 
	 loss: 48.3950, MinusLogProbMetric: 48.3950, val_loss: 51.5700, val_MinusLogProbMetric: 51.5700

Epoch 217: val_loss did not improve from 48.25600
196/196 - 39s - loss: 48.3950 - MinusLogProbMetric: 48.3950 - val_loss: 51.5700 - val_MinusLogProbMetric: 51.5700 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 218/1000
2023-10-27 15:23:10.903 
Epoch 218/1000 
	 loss: 48.4658, MinusLogProbMetric: 48.4658, val_loss: 49.4951, val_MinusLogProbMetric: 49.4951

Epoch 218: val_loss did not improve from 48.25600
196/196 - 39s - loss: 48.4658 - MinusLogProbMetric: 48.4658 - val_loss: 49.4951 - val_MinusLogProbMetric: 49.4951 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 219/1000
2023-10-27 15:23:51.950 
Epoch 219/1000 
	 loss: 48.2950, MinusLogProbMetric: 48.2950, val_loss: 48.3610, val_MinusLogProbMetric: 48.3610

Epoch 219: val_loss did not improve from 48.25600
196/196 - 41s - loss: 48.2950 - MinusLogProbMetric: 48.2950 - val_loss: 48.3610 - val_MinusLogProbMetric: 48.3610 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 220/1000
2023-10-27 15:24:34.859 
Epoch 220/1000 
	 loss: 48.7010, MinusLogProbMetric: 48.7010, val_loss: 48.1132, val_MinusLogProbMetric: 48.1132

Epoch 220: val_loss improved from 48.25600 to 48.11319, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 44s - loss: 48.7010 - MinusLogProbMetric: 48.7010 - val_loss: 48.1132 - val_MinusLogProbMetric: 48.1132 - lr: 3.3333e-04 - 44s/epoch - 222ms/step
Epoch 221/1000
2023-10-27 15:25:12.380 
Epoch 221/1000 
	 loss: 48.4277, MinusLogProbMetric: 48.4277, val_loss: 48.9956, val_MinusLogProbMetric: 48.9956

Epoch 221: val_loss did not improve from 48.11319
196/196 - 37s - loss: 48.4277 - MinusLogProbMetric: 48.4277 - val_loss: 48.9956 - val_MinusLogProbMetric: 48.9956 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 222/1000
2023-10-27 15:25:52.228 
Epoch 222/1000 
	 loss: 48.3386, MinusLogProbMetric: 48.3386, val_loss: 49.3825, val_MinusLogProbMetric: 49.3825

Epoch 222: val_loss did not improve from 48.11319
196/196 - 40s - loss: 48.3386 - MinusLogProbMetric: 48.3386 - val_loss: 49.3825 - val_MinusLogProbMetric: 49.3825 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 223/1000
2023-10-27 15:26:35.731 
Epoch 223/1000 
	 loss: 48.5387, MinusLogProbMetric: 48.5387, val_loss: 48.3467, val_MinusLogProbMetric: 48.3467

Epoch 223: val_loss did not improve from 48.11319
196/196 - 43s - loss: 48.5387 - MinusLogProbMetric: 48.5387 - val_loss: 48.3467 - val_MinusLogProbMetric: 48.3467 - lr: 3.3333e-04 - 43s/epoch - 222ms/step
Epoch 224/1000
2023-10-27 15:27:16.804 
Epoch 224/1000 
	 loss: 48.3799, MinusLogProbMetric: 48.3799, val_loss: 49.5089, val_MinusLogProbMetric: 49.5089

Epoch 224: val_loss did not improve from 48.11319
196/196 - 41s - loss: 48.3799 - MinusLogProbMetric: 48.3799 - val_loss: 49.5089 - val_MinusLogProbMetric: 49.5089 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 225/1000
2023-10-27 15:27:55.679 
Epoch 225/1000 
	 loss: 48.3944, MinusLogProbMetric: 48.3944, val_loss: 49.3053, val_MinusLogProbMetric: 49.3053

Epoch 225: val_loss did not improve from 48.11319
196/196 - 39s - loss: 48.3944 - MinusLogProbMetric: 48.3944 - val_loss: 49.3053 - val_MinusLogProbMetric: 49.3053 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 226/1000
2023-10-27 15:28:34.361 
Epoch 226/1000 
	 loss: 48.2016, MinusLogProbMetric: 48.2016, val_loss: 49.9286, val_MinusLogProbMetric: 49.9286

Epoch 226: val_loss did not improve from 48.11319
196/196 - 39s - loss: 48.2016 - MinusLogProbMetric: 48.2016 - val_loss: 49.9286 - val_MinusLogProbMetric: 49.9286 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 227/1000
2023-10-27 15:29:17.097 
Epoch 227/1000 
	 loss: 48.5232, MinusLogProbMetric: 48.5232, val_loss: 48.2999, val_MinusLogProbMetric: 48.2999

Epoch 227: val_loss did not improve from 48.11319
196/196 - 43s - loss: 48.5232 - MinusLogProbMetric: 48.5232 - val_loss: 48.2999 - val_MinusLogProbMetric: 48.2999 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 228/1000
2023-10-27 15:29:58.991 
Epoch 228/1000 
	 loss: 48.3978, MinusLogProbMetric: 48.3978, val_loss: 51.0746, val_MinusLogProbMetric: 51.0746

Epoch 228: val_loss did not improve from 48.11319
196/196 - 42s - loss: 48.3978 - MinusLogProbMetric: 48.3978 - val_loss: 51.0746 - val_MinusLogProbMetric: 51.0746 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 229/1000
2023-10-27 15:30:36.983 
Epoch 229/1000 
	 loss: 48.1287, MinusLogProbMetric: 48.1287, val_loss: 48.7335, val_MinusLogProbMetric: 48.7335

Epoch 229: val_loss did not improve from 48.11319
196/196 - 38s - loss: 48.1287 - MinusLogProbMetric: 48.1287 - val_loss: 48.7335 - val_MinusLogProbMetric: 48.7335 - lr: 3.3333e-04 - 38s/epoch - 194ms/step
Epoch 230/1000
2023-10-27 15:31:17.883 
Epoch 230/1000 
	 loss: 48.4142, MinusLogProbMetric: 48.4142, val_loss: 48.1956, val_MinusLogProbMetric: 48.1956

Epoch 230: val_loss did not improve from 48.11319
196/196 - 41s - loss: 48.4142 - MinusLogProbMetric: 48.4142 - val_loss: 48.1956 - val_MinusLogProbMetric: 48.1956 - lr: 3.3333e-04 - 41s/epoch - 209ms/step
Epoch 231/1000
2023-10-27 15:31:58.677 
Epoch 231/1000 
	 loss: 48.2053, MinusLogProbMetric: 48.2053, val_loss: 49.2946, val_MinusLogProbMetric: 49.2946

Epoch 231: val_loss did not improve from 48.11319
196/196 - 41s - loss: 48.2053 - MinusLogProbMetric: 48.2053 - val_loss: 49.2946 - val_MinusLogProbMetric: 49.2946 - lr: 3.3333e-04 - 41s/epoch - 208ms/step
Epoch 232/1000
2023-10-27 15:32:41.171 
Epoch 232/1000 
	 loss: 48.3900, MinusLogProbMetric: 48.3900, val_loss: 48.2221, val_MinusLogProbMetric: 48.2221

Epoch 232: val_loss did not improve from 48.11319
196/196 - 42s - loss: 48.3900 - MinusLogProbMetric: 48.3900 - val_loss: 48.2221 - val_MinusLogProbMetric: 48.2221 - lr: 3.3333e-04 - 42s/epoch - 217ms/step
Epoch 233/1000
2023-10-27 15:33:20.514 
Epoch 233/1000 
	 loss: 48.1582, MinusLogProbMetric: 48.1582, val_loss: 47.9368, val_MinusLogProbMetric: 47.9368

Epoch 233: val_loss improved from 48.11319 to 47.93681, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 40s - loss: 48.1582 - MinusLogProbMetric: 48.1582 - val_loss: 47.9368 - val_MinusLogProbMetric: 47.9368 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 234/1000
2023-10-27 15:34:00.558 
Epoch 234/1000 
	 loss: 48.1356, MinusLogProbMetric: 48.1356, val_loss: 48.7435, val_MinusLogProbMetric: 48.7435

Epoch 234: val_loss did not improve from 47.93681
196/196 - 39s - loss: 48.1356 - MinusLogProbMetric: 48.1356 - val_loss: 48.7435 - val_MinusLogProbMetric: 48.7435 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 235/1000
2023-10-27 15:34:41.578 
Epoch 235/1000 
	 loss: 47.9724, MinusLogProbMetric: 47.9724, val_loss: 47.7819, val_MinusLogProbMetric: 47.7819

Epoch 235: val_loss improved from 47.93681 to 47.78194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 42s - loss: 47.9724 - MinusLogProbMetric: 47.9724 - val_loss: 47.7819 - val_MinusLogProbMetric: 47.7819 - lr: 3.3333e-04 - 42s/epoch - 213ms/step
Epoch 236/1000
2023-10-27 15:35:26.403 
Epoch 236/1000 
	 loss: 48.3145, MinusLogProbMetric: 48.3145, val_loss: 48.6380, val_MinusLogProbMetric: 48.6380

Epoch 236: val_loss did not improve from 47.78194
196/196 - 44s - loss: 48.3145 - MinusLogProbMetric: 48.3145 - val_loss: 48.6380 - val_MinusLogProbMetric: 48.6380 - lr: 3.3333e-04 - 44s/epoch - 225ms/step
Epoch 237/1000
2023-10-27 15:36:08.647 
Epoch 237/1000 
	 loss: 48.1119, MinusLogProbMetric: 48.1119, val_loss: 49.1491, val_MinusLogProbMetric: 49.1491

Epoch 237: val_loss did not improve from 47.78194
196/196 - 42s - loss: 48.1119 - MinusLogProbMetric: 48.1119 - val_loss: 49.1491 - val_MinusLogProbMetric: 49.1491 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 238/1000
2023-10-27 15:36:47.028 
Epoch 238/1000 
	 loss: 48.3400, MinusLogProbMetric: 48.3400, val_loss: 48.2477, val_MinusLogProbMetric: 48.2477

Epoch 238: val_loss did not improve from 47.78194
196/196 - 38s - loss: 48.3400 - MinusLogProbMetric: 48.3400 - val_loss: 48.2477 - val_MinusLogProbMetric: 48.2477 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 239/1000
2023-10-27 15:37:27.919 
Epoch 239/1000 
	 loss: 47.8848, MinusLogProbMetric: 47.8848, val_loss: 47.4494, val_MinusLogProbMetric: 47.4494

Epoch 239: val_loss improved from 47.78194 to 47.44937, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 42s - loss: 47.8848 - MinusLogProbMetric: 47.8848 - val_loss: 47.4494 - val_MinusLogProbMetric: 47.4494 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 240/1000
2023-10-27 15:38:10.776 
Epoch 240/1000 
	 loss: 48.0186, MinusLogProbMetric: 48.0186, val_loss: 49.0619, val_MinusLogProbMetric: 49.0619

Epoch 240: val_loss did not improve from 47.44937
196/196 - 42s - loss: 48.0186 - MinusLogProbMetric: 48.0186 - val_loss: 49.0619 - val_MinusLogProbMetric: 49.0619 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 241/1000
2023-10-27 15:38:55.020 
Epoch 241/1000 
	 loss: 48.1843, MinusLogProbMetric: 48.1843, val_loss: 49.6118, val_MinusLogProbMetric: 49.6118

Epoch 241: val_loss did not improve from 47.44937
196/196 - 44s - loss: 48.1843 - MinusLogProbMetric: 48.1843 - val_loss: 49.6118 - val_MinusLogProbMetric: 49.6118 - lr: 3.3333e-04 - 44s/epoch - 226ms/step
Epoch 242/1000
2023-10-27 15:39:37.373 
Epoch 242/1000 
	 loss: 47.8661, MinusLogProbMetric: 47.8661, val_loss: 49.6070, val_MinusLogProbMetric: 49.6070

Epoch 242: val_loss did not improve from 47.44937
196/196 - 42s - loss: 47.8661 - MinusLogProbMetric: 47.8661 - val_loss: 49.6070 - val_MinusLogProbMetric: 49.6070 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 243/1000
2023-10-27 15:40:17.078 
Epoch 243/1000 
	 loss: 48.0273, MinusLogProbMetric: 48.0273, val_loss: 47.5821, val_MinusLogProbMetric: 47.5821

Epoch 243: val_loss did not improve from 47.44937
196/196 - 40s - loss: 48.0273 - MinusLogProbMetric: 48.0273 - val_loss: 47.5821 - val_MinusLogProbMetric: 47.5821 - lr: 3.3333e-04 - 40s/epoch - 203ms/step
Epoch 244/1000
2023-10-27 15:40:57.702 
Epoch 244/1000 
	 loss: 48.3119, MinusLogProbMetric: 48.3119, val_loss: 47.5643, val_MinusLogProbMetric: 47.5643

Epoch 244: val_loss did not improve from 47.44937
196/196 - 41s - loss: 48.3119 - MinusLogProbMetric: 48.3119 - val_loss: 47.5643 - val_MinusLogProbMetric: 47.5643 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 245/1000
2023-10-27 15:41:41.632 
Epoch 245/1000 
	 loss: 47.8671, MinusLogProbMetric: 47.8671, val_loss: 47.8696, val_MinusLogProbMetric: 47.8696

Epoch 245: val_loss did not improve from 47.44937
196/196 - 44s - loss: 47.8671 - MinusLogProbMetric: 47.8671 - val_loss: 47.8696 - val_MinusLogProbMetric: 47.8696 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 246/1000
2023-10-27 15:42:21.582 
Epoch 246/1000 
	 loss: 48.0485, MinusLogProbMetric: 48.0485, val_loss: 48.2498, val_MinusLogProbMetric: 48.2498

Epoch 246: val_loss did not improve from 47.44937
196/196 - 40s - loss: 48.0485 - MinusLogProbMetric: 48.0485 - val_loss: 48.2498 - val_MinusLogProbMetric: 48.2498 - lr: 3.3333e-04 - 40s/epoch - 204ms/step
Epoch 247/1000
2023-10-27 15:43:02.199 
Epoch 247/1000 
	 loss: 48.1362, MinusLogProbMetric: 48.1362, val_loss: 49.1095, val_MinusLogProbMetric: 49.1095

Epoch 247: val_loss did not improve from 47.44937
196/196 - 41s - loss: 48.1362 - MinusLogProbMetric: 48.1362 - val_loss: 49.1095 - val_MinusLogProbMetric: 49.1095 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 248/1000
2023-10-27 15:43:44.263 
Epoch 248/1000 
	 loss: 47.8977, MinusLogProbMetric: 47.8977, val_loss: 48.2387, val_MinusLogProbMetric: 48.2387

Epoch 248: val_loss did not improve from 47.44937
196/196 - 42s - loss: 47.8977 - MinusLogProbMetric: 47.8977 - val_loss: 48.2387 - val_MinusLogProbMetric: 48.2387 - lr: 3.3333e-04 - 42s/epoch - 215ms/step
Epoch 249/1000
2023-10-27 15:44:26.507 
Epoch 249/1000 
	 loss: 47.8282, MinusLogProbMetric: 47.8282, val_loss: 48.0873, val_MinusLogProbMetric: 48.0873

Epoch 249: val_loss did not improve from 47.44937
196/196 - 42s - loss: 47.8282 - MinusLogProbMetric: 47.8282 - val_loss: 48.0873 - val_MinusLogProbMetric: 48.0873 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 250/1000
2023-10-27 15:45:10.397 
Epoch 250/1000 
	 loss: 47.8668, MinusLogProbMetric: 47.8668, val_loss: 48.5349, val_MinusLogProbMetric: 48.5349

Epoch 250: val_loss did not improve from 47.44937
196/196 - 44s - loss: 47.8668 - MinusLogProbMetric: 47.8668 - val_loss: 48.5349 - val_MinusLogProbMetric: 48.5349 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 251/1000
2023-10-27 15:45:52.944 
Epoch 251/1000 
	 loss: 47.8216, MinusLogProbMetric: 47.8216, val_loss: 48.0953, val_MinusLogProbMetric: 48.0953

Epoch 251: val_loss did not improve from 47.44937
196/196 - 43s - loss: 47.8216 - MinusLogProbMetric: 47.8216 - val_loss: 48.0953 - val_MinusLogProbMetric: 48.0953 - lr: 3.3333e-04 - 43s/epoch - 217ms/step
Epoch 252/1000
2023-10-27 15:46:35.713 
Epoch 252/1000 
	 loss: 47.9602, MinusLogProbMetric: 47.9602, val_loss: 47.8754, val_MinusLogProbMetric: 47.8754

Epoch 252: val_loss did not improve from 47.44937
196/196 - 43s - loss: 47.9602 - MinusLogProbMetric: 47.9602 - val_loss: 47.8754 - val_MinusLogProbMetric: 47.8754 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 253/1000
2023-10-27 15:47:16.784 
Epoch 253/1000 
	 loss: 47.8294, MinusLogProbMetric: 47.8294, val_loss: 48.3531, val_MinusLogProbMetric: 48.3531

Epoch 253: val_loss did not improve from 47.44937
196/196 - 41s - loss: 47.8294 - MinusLogProbMetric: 47.8294 - val_loss: 48.3531 - val_MinusLogProbMetric: 48.3531 - lr: 3.3333e-04 - 41s/epoch - 210ms/step
Epoch 254/1000
2023-10-27 15:48:01.457 
Epoch 254/1000 
	 loss: 47.8125, MinusLogProbMetric: 47.8125, val_loss: 48.0601, val_MinusLogProbMetric: 48.0601

Epoch 254: val_loss did not improve from 47.44937
196/196 - 45s - loss: 47.8125 - MinusLogProbMetric: 47.8125 - val_loss: 48.0601 - val_MinusLogProbMetric: 48.0601 - lr: 3.3333e-04 - 45s/epoch - 228ms/step
Epoch 255/1000
2023-10-27 15:48:44.209 
Epoch 255/1000 
	 loss: 47.9731, MinusLogProbMetric: 47.9731, val_loss: 48.9822, val_MinusLogProbMetric: 48.9822

Epoch 255: val_loss did not improve from 47.44937
196/196 - 43s - loss: 47.9731 - MinusLogProbMetric: 47.9731 - val_loss: 48.9822 - val_MinusLogProbMetric: 48.9822 - lr: 3.3333e-04 - 43s/epoch - 218ms/step
Epoch 256/1000
2023-10-27 15:49:27.858 
Epoch 256/1000 
	 loss: 47.9500, MinusLogProbMetric: 47.9500, val_loss: 47.7978, val_MinusLogProbMetric: 47.7978

Epoch 256: val_loss did not improve from 47.44937
196/196 - 44s - loss: 47.9500 - MinusLogProbMetric: 47.9500 - val_loss: 47.7978 - val_MinusLogProbMetric: 47.7978 - lr: 3.3333e-04 - 44s/epoch - 223ms/step
Epoch 257/1000
2023-10-27 15:50:09.095 
Epoch 257/1000 
	 loss: 47.8795, MinusLogProbMetric: 47.8795, val_loss: 47.2961, val_MinusLogProbMetric: 47.2961

Epoch 257: val_loss improved from 47.44937 to 47.29615, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 42s - loss: 47.8795 - MinusLogProbMetric: 47.8795 - val_loss: 47.2961 - val_MinusLogProbMetric: 47.2961 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 258/1000
2023-10-27 15:50:52.864 
Epoch 258/1000 
	 loss: 47.8557, MinusLogProbMetric: 47.8557, val_loss: 47.4640, val_MinusLogProbMetric: 47.4640

Epoch 258: val_loss did not improve from 47.29615
196/196 - 43s - loss: 47.8557 - MinusLogProbMetric: 47.8557 - val_loss: 47.4640 - val_MinusLogProbMetric: 47.4640 - lr: 3.3333e-04 - 43s/epoch - 220ms/step
Epoch 259/1000
2023-10-27 15:51:36.808 
Epoch 259/1000 
	 loss: 48.0050, MinusLogProbMetric: 48.0050, val_loss: 48.1608, val_MinusLogProbMetric: 48.1608

Epoch 259: val_loss did not improve from 47.29615
196/196 - 44s - loss: 48.0050 - MinusLogProbMetric: 48.0050 - val_loss: 48.1608 - val_MinusLogProbMetric: 48.1608 - lr: 3.3333e-04 - 44s/epoch - 224ms/step
Epoch 260/1000
2023-10-27 15:52:16.910 
Epoch 260/1000 
	 loss: 47.6056, MinusLogProbMetric: 47.6056, val_loss: 48.2241, val_MinusLogProbMetric: 48.2241

Epoch 260: val_loss did not improve from 47.29615
196/196 - 40s - loss: 47.6056 - MinusLogProbMetric: 47.6056 - val_loss: 48.2241 - val_MinusLogProbMetric: 48.2241 - lr: 3.3333e-04 - 40s/epoch - 205ms/step
Epoch 261/1000
2023-10-27 15:52:55.319 
Epoch 261/1000 
	 loss: 47.6854, MinusLogProbMetric: 47.6854, val_loss: 47.6552, val_MinusLogProbMetric: 47.6552

Epoch 261: val_loss did not improve from 47.29615
196/196 - 38s - loss: 47.6854 - MinusLogProbMetric: 47.6854 - val_loss: 47.6552 - val_MinusLogProbMetric: 47.6552 - lr: 3.3333e-04 - 38s/epoch - 196ms/step
Epoch 262/1000
2023-10-27 15:53:35.959 
Epoch 262/1000 
	 loss: 47.7308, MinusLogProbMetric: 47.7308, val_loss: 48.0062, val_MinusLogProbMetric: 48.0062

Epoch 262: val_loss did not improve from 47.29615
196/196 - 41s - loss: 47.7308 - MinusLogProbMetric: 47.7308 - val_loss: 48.0062 - val_MinusLogProbMetric: 48.0062 - lr: 3.3333e-04 - 41s/epoch - 207ms/step
Epoch 263/1000
2023-10-27 15:54:17.936 
Epoch 263/1000 
	 loss: 47.7192, MinusLogProbMetric: 47.7192, val_loss: 48.0152, val_MinusLogProbMetric: 48.0152

Epoch 263: val_loss did not improve from 47.29615
196/196 - 42s - loss: 47.7192 - MinusLogProbMetric: 47.7192 - val_loss: 48.0152 - val_MinusLogProbMetric: 48.0152 - lr: 3.3333e-04 - 42s/epoch - 214ms/step
Epoch 264/1000
2023-10-27 15:54:55.544 
Epoch 264/1000 
	 loss: 47.7834, MinusLogProbMetric: 47.7834, val_loss: 47.5702, val_MinusLogProbMetric: 47.5702

Epoch 264: val_loss did not improve from 47.29615
196/196 - 38s - loss: 47.7834 - MinusLogProbMetric: 47.7834 - val_loss: 47.5702 - val_MinusLogProbMetric: 47.5702 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 265/1000
2023-10-27 15:55:32.141 
Epoch 265/1000 
	 loss: 48.1215, MinusLogProbMetric: 48.1215, val_loss: 48.0376, val_MinusLogProbMetric: 48.0376

Epoch 265: val_loss did not improve from 47.29615
196/196 - 37s - loss: 48.1215 - MinusLogProbMetric: 48.1215 - val_loss: 48.0376 - val_MinusLogProbMetric: 48.0376 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 266/1000
2023-10-27 15:56:07.774 
Epoch 266/1000 
	 loss: 47.5707, MinusLogProbMetric: 47.5707, val_loss: 48.0699, val_MinusLogProbMetric: 48.0699

Epoch 266: val_loss did not improve from 47.29615
196/196 - 36s - loss: 47.5707 - MinusLogProbMetric: 47.5707 - val_loss: 48.0699 - val_MinusLogProbMetric: 48.0699 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 267/1000
2023-10-27 15:56:50.202 
Epoch 267/1000 
	 loss: 47.5461, MinusLogProbMetric: 47.5461, val_loss: 47.9877, val_MinusLogProbMetric: 47.9877

Epoch 267: val_loss did not improve from 47.29615
196/196 - 42s - loss: 47.5461 - MinusLogProbMetric: 47.5461 - val_loss: 47.9877 - val_MinusLogProbMetric: 47.9877 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 268/1000
2023-10-27 15:57:29.034 
Epoch 268/1000 
	 loss: 47.8562, MinusLogProbMetric: 47.8562, val_loss: 47.3048, val_MinusLogProbMetric: 47.3048

Epoch 268: val_loss did not improve from 47.29615
196/196 - 39s - loss: 47.8562 - MinusLogProbMetric: 47.8562 - val_loss: 47.3048 - val_MinusLogProbMetric: 47.3048 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 269/1000
2023-10-27 15:58:05.846 
Epoch 269/1000 
	 loss: 47.5749, MinusLogProbMetric: 47.5749, val_loss: 47.9638, val_MinusLogProbMetric: 47.9638

Epoch 269: val_loss did not improve from 47.29615
196/196 - 37s - loss: 47.5749 - MinusLogProbMetric: 47.5749 - val_loss: 47.9638 - val_MinusLogProbMetric: 47.9638 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 270/1000
2023-10-27 15:58:41.661 
Epoch 270/1000 
	 loss: 47.4573, MinusLogProbMetric: 47.4573, val_loss: 48.0077, val_MinusLogProbMetric: 48.0077

Epoch 270: val_loss did not improve from 47.29615
196/196 - 36s - loss: 47.4573 - MinusLogProbMetric: 47.4573 - val_loss: 48.0077 - val_MinusLogProbMetric: 48.0077 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 271/1000
2023-10-27 15:59:23.301 
Epoch 271/1000 
	 loss: 47.8547, MinusLogProbMetric: 47.8547, val_loss: 47.5869, val_MinusLogProbMetric: 47.5869

Epoch 271: val_loss did not improve from 47.29615
196/196 - 42s - loss: 47.8547 - MinusLogProbMetric: 47.8547 - val_loss: 47.5869 - val_MinusLogProbMetric: 47.5869 - lr: 3.3333e-04 - 42s/epoch - 212ms/step
Epoch 272/1000
2023-10-27 15:59:58.980 
Epoch 272/1000 
	 loss: 47.6163, MinusLogProbMetric: 47.6163, val_loss: 48.3745, val_MinusLogProbMetric: 48.3745

Epoch 272: val_loss did not improve from 47.29615
196/196 - 36s - loss: 47.6163 - MinusLogProbMetric: 47.6163 - val_loss: 48.3745 - val_MinusLogProbMetric: 48.3745 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 273/1000
2023-10-27 16:00:35.706 
Epoch 273/1000 
	 loss: 48.1168, MinusLogProbMetric: 48.1168, val_loss: 47.8894, val_MinusLogProbMetric: 47.8894

Epoch 273: val_loss did not improve from 47.29615
196/196 - 37s - loss: 48.1168 - MinusLogProbMetric: 48.1168 - val_loss: 47.8894 - val_MinusLogProbMetric: 47.8894 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 274/1000
2023-10-27 16:01:13.218 
Epoch 274/1000 
	 loss: 47.5127, MinusLogProbMetric: 47.5127, val_loss: 49.3529, val_MinusLogProbMetric: 49.3529

Epoch 274: val_loss did not improve from 47.29615
196/196 - 38s - loss: 47.5127 - MinusLogProbMetric: 47.5127 - val_loss: 49.3529 - val_MinusLogProbMetric: 49.3529 - lr: 3.3333e-04 - 38s/epoch - 191ms/step
Epoch 275/1000
2023-10-27 16:01:50.146 
Epoch 275/1000 
	 loss: 47.4895, MinusLogProbMetric: 47.4895, val_loss: 47.7009, val_MinusLogProbMetric: 47.7009

Epoch 275: val_loss did not improve from 47.29615
196/196 - 37s - loss: 47.4895 - MinusLogProbMetric: 47.4895 - val_loss: 47.7009 - val_MinusLogProbMetric: 47.7009 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 276/1000
2023-10-27 16:02:26.553 
Epoch 276/1000 
	 loss: 47.7406, MinusLogProbMetric: 47.7406, val_loss: 49.3070, val_MinusLogProbMetric: 49.3070

Epoch 276: val_loss did not improve from 47.29615
196/196 - 36s - loss: 47.7406 - MinusLogProbMetric: 47.7406 - val_loss: 49.3070 - val_MinusLogProbMetric: 49.3070 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 277/1000
2023-10-27 16:03:06.008 
Epoch 277/1000 
	 loss: 47.5735, MinusLogProbMetric: 47.5735, val_loss: 46.8816, val_MinusLogProbMetric: 46.8816

Epoch 277: val_loss improved from 47.29615 to 46.88164, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_412/weights/best_weights.h5
196/196 - 40s - loss: 47.5735 - MinusLogProbMetric: 47.5735 - val_loss: 46.8816 - val_MinusLogProbMetric: 46.8816 - lr: 3.3333e-04 - 40s/epoch - 206ms/step
Epoch 278/1000
2023-10-27 16:03:45.853 
Epoch 278/1000 
	 loss: 47.5245, MinusLogProbMetric: 47.5245, val_loss: 47.4446, val_MinusLogProbMetric: 47.4446

Epoch 278: val_loss did not improve from 46.88164
196/196 - 39s - loss: 47.5245 - MinusLogProbMetric: 47.5245 - val_loss: 47.4446 - val_MinusLogProbMetric: 47.4446 - lr: 3.3333e-04 - 39s/epoch - 199ms/step
Epoch 279/1000
2023-10-27 16:04:23.106 
Epoch 279/1000 
	 loss: 47.4835, MinusLogProbMetric: 47.4835, val_loss: 49.2946, val_MinusLogProbMetric: 49.2946

Epoch 279: val_loss did not improve from 46.88164
196/196 - 37s - loss: 47.4835 - MinusLogProbMetric: 47.4835 - val_loss: 49.2946 - val_MinusLogProbMetric: 49.2946 - lr: 3.3333e-04 - 37s/epoch - 190ms/step
Epoch 280/1000
2023-10-27 16:04:58.880 
Epoch 280/1000 
	 loss: 48.3028, MinusLogProbMetric: 48.3028, val_loss: 47.4564, val_MinusLogProbMetric: 47.4564

Epoch 280: val_loss did not improve from 46.88164
196/196 - 36s - loss: 48.3028 - MinusLogProbMetric: 48.3028 - val_loss: 47.4564 - val_MinusLogProbMetric: 47.4564 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 281/1000
2023-10-27 16:05:31.899 
Epoch 281/1000 
	 loss: 51.7781, MinusLogProbMetric: 51.7781, val_loss: 53.3029, val_MinusLogProbMetric: 53.3029

Epoch 281: val_loss did not improve from 46.88164
196/196 - 33s - loss: 51.7781 - MinusLogProbMetric: 51.7781 - val_loss: 53.3029 - val_MinusLogProbMetric: 53.3029 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 282/1000
2023-10-27 16:06:04.667 
Epoch 282/1000 
	 loss: 52.2771, MinusLogProbMetric: 52.2771, val_loss: 51.7098, val_MinusLogProbMetric: 51.7098

Epoch 282: val_loss did not improve from 46.88164
196/196 - 33s - loss: 52.2771 - MinusLogProbMetric: 52.2771 - val_loss: 51.7098 - val_MinusLogProbMetric: 51.7098 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 283/1000
2023-10-27 16:06:37.383 
Epoch 283/1000 
	 loss: 51.4204, MinusLogProbMetric: 51.4204, val_loss: 51.6253, val_MinusLogProbMetric: 51.6253

Epoch 283: val_loss did not improve from 46.88164
196/196 - 33s - loss: 51.4204 - MinusLogProbMetric: 51.4204 - val_loss: 51.6253 - val_MinusLogProbMetric: 51.6253 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 284/1000
2023-10-27 16:07:14.313 
Epoch 284/1000 
	 loss: 50.9301, MinusLogProbMetric: 50.9301, val_loss: 51.8362, val_MinusLogProbMetric: 51.8362

Epoch 284: val_loss did not improve from 46.88164
196/196 - 37s - loss: 50.9301 - MinusLogProbMetric: 50.9301 - val_loss: 51.8362 - val_MinusLogProbMetric: 51.8362 - lr: 3.3333e-04 - 37s/epoch - 188ms/step
Epoch 285/1000
2023-10-27 16:07:50.997 
Epoch 285/1000 
	 loss: 50.7299, MinusLogProbMetric: 50.7299, val_loss: 53.0249, val_MinusLogProbMetric: 53.0249

Epoch 285: val_loss did not improve from 46.88164
196/196 - 37s - loss: 50.7299 - MinusLogProbMetric: 50.7299 - val_loss: 53.0249 - val_MinusLogProbMetric: 53.0249 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 286/1000
2023-10-27 16:08:25.834 
Epoch 286/1000 
	 loss: 50.3609, MinusLogProbMetric: 50.3609, val_loss: 54.8184, val_MinusLogProbMetric: 54.8184

Epoch 286: val_loss did not improve from 46.88164
196/196 - 35s - loss: 50.3609 - MinusLogProbMetric: 50.3609 - val_loss: 54.8184 - val_MinusLogProbMetric: 54.8184 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 287/1000
2023-10-27 16:09:00.403 
Epoch 287/1000 
	 loss: 50.1758, MinusLogProbMetric: 50.1758, val_loss: 51.3098, val_MinusLogProbMetric: 51.3098

Epoch 287: val_loss did not improve from 46.88164
196/196 - 35s - loss: 50.1758 - MinusLogProbMetric: 50.1758 - val_loss: 51.3098 - val_MinusLogProbMetric: 51.3098 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 288/1000
2023-10-27 16:09:35.339 
Epoch 288/1000 
	 loss: 49.8615, MinusLogProbMetric: 49.8615, val_loss: 50.0629, val_MinusLogProbMetric: 50.0629

Epoch 288: val_loss did not improve from 46.88164
196/196 - 35s - loss: 49.8615 - MinusLogProbMetric: 49.8615 - val_loss: 50.0629 - val_MinusLogProbMetric: 50.0629 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 289/1000
2023-10-27 16:10:11.921 
Epoch 289/1000 
	 loss: 49.6655, MinusLogProbMetric: 49.6655, val_loss: 50.3303, val_MinusLogProbMetric: 50.3303

Epoch 289: val_loss did not improve from 46.88164
196/196 - 37s - loss: 49.6655 - MinusLogProbMetric: 49.6655 - val_loss: 50.3303 - val_MinusLogProbMetric: 50.3303 - lr: 3.3333e-04 - 37s/epoch - 187ms/step
Epoch 290/1000
2023-10-27 16:10:51.214 
Epoch 290/1000 
	 loss: 49.8498, MinusLogProbMetric: 49.8498, val_loss: 49.6376, val_MinusLogProbMetric: 49.6376

Epoch 290: val_loss did not improve from 46.88164
196/196 - 39s - loss: 49.8498 - MinusLogProbMetric: 49.8498 - val_loss: 49.6376 - val_MinusLogProbMetric: 49.6376 - lr: 3.3333e-04 - 39s/epoch - 200ms/step
Epoch 291/1000
2023-10-27 16:11:24.751 
Epoch 291/1000 
	 loss: 49.3744, MinusLogProbMetric: 49.3744, val_loss: 49.5841, val_MinusLogProbMetric: 49.5841

Epoch 291: val_loss did not improve from 46.88164
196/196 - 34s - loss: 49.3744 - MinusLogProbMetric: 49.3744 - val_loss: 49.5841 - val_MinusLogProbMetric: 49.5841 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 292/1000
2023-10-27 16:11:57.957 
Epoch 292/1000 
	 loss: 49.4366, MinusLogProbMetric: 49.4366, val_loss: 50.8758, val_MinusLogProbMetric: 50.8758

Epoch 292: val_loss did not improve from 46.88164
196/196 - 33s - loss: 49.4366 - MinusLogProbMetric: 49.4366 - val_loss: 50.8758 - val_MinusLogProbMetric: 50.8758 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 293/1000
2023-10-27 16:12:31.597 
Epoch 293/1000 
	 loss: 49.3101, MinusLogProbMetric: 49.3101, val_loss: 49.7133, val_MinusLogProbMetric: 49.7133

Epoch 293: val_loss did not improve from 46.88164
196/196 - 34s - loss: 49.3101 - MinusLogProbMetric: 49.3101 - val_loss: 49.7133 - val_MinusLogProbMetric: 49.7133 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 294/1000
2023-10-27 16:13:10.181 
Epoch 294/1000 
	 loss: 49.0583, MinusLogProbMetric: 49.0583, val_loss: 49.3273, val_MinusLogProbMetric: 49.3273

Epoch 294: val_loss did not improve from 46.88164
196/196 - 39s - loss: 49.0583 - MinusLogProbMetric: 49.0583 - val_loss: 49.3273 - val_MinusLogProbMetric: 49.3273 - lr: 3.3333e-04 - 39s/epoch - 197ms/step
Epoch 295/1000
2023-10-27 16:13:48.945 
Epoch 295/1000 
	 loss: 48.9836, MinusLogProbMetric: 48.9836, val_loss: 49.1125, val_MinusLogProbMetric: 49.1125

Epoch 295: val_loss did not improve from 46.88164
196/196 - 39s - loss: 48.9836 - MinusLogProbMetric: 48.9836 - val_loss: 49.1125 - val_MinusLogProbMetric: 49.1125 - lr: 3.3333e-04 - 39s/epoch - 198ms/step
Epoch 296/1000
2023-10-27 16:14:23.070 
Epoch 296/1000 
	 loss: 48.7413, MinusLogProbMetric: 48.7413, val_loss: 48.6864, val_MinusLogProbMetric: 48.6864

Epoch 296: val_loss did not improve from 46.88164
196/196 - 34s - loss: 48.7413 - MinusLogProbMetric: 48.7413 - val_loss: 48.6864 - val_MinusLogProbMetric: 48.6864 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 297/1000
2023-10-27 16:14:56.596 
Epoch 297/1000 
	 loss: 49.1137, MinusLogProbMetric: 49.1137, val_loss: 48.7181, val_MinusLogProbMetric: 48.7181

Epoch 297: val_loss did not improve from 46.88164
196/196 - 34s - loss: 49.1137 - MinusLogProbMetric: 49.1137 - val_loss: 48.7181 - val_MinusLogProbMetric: 48.7181 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 298/1000
2023-10-27 16:15:30.290 
Epoch 298/1000 
	 loss: 48.3067, MinusLogProbMetric: 48.3067, val_loss: 48.4171, val_MinusLogProbMetric: 48.4171

Epoch 298: val_loss did not improve from 46.88164
196/196 - 34s - loss: 48.3067 - MinusLogProbMetric: 48.3067 - val_loss: 48.4171 - val_MinusLogProbMetric: 48.4171 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 299/1000
2023-10-27 16:16:04.904 
Epoch 299/1000 
	 loss: 48.3593, MinusLogProbMetric: 48.3593, val_loss: 49.3567, val_MinusLogProbMetric: 49.3567

Epoch 299: val_loss did not improve from 46.88164
196/196 - 35s - loss: 48.3593 - MinusLogProbMetric: 48.3593 - val_loss: 49.3567 - val_MinusLogProbMetric: 49.3567 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 300/1000
2023-10-27 16:16:39.104 
Epoch 300/1000 
	 loss: 48.2561, MinusLogProbMetric: 48.2561, val_loss: 48.8919, val_MinusLogProbMetric: 48.8919

Epoch 300: val_loss did not improve from 46.88164
196/196 - 34s - loss: 48.2561 - MinusLogProbMetric: 48.2561 - val_loss: 48.8919 - val_MinusLogProbMetric: 48.8919 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 301/1000
2023-10-27 16:17:14.630 
Epoch 301/1000 
	 loss: 48.3797, MinusLogProbMetric: 48.3797, val_loss: 48.6740, val_MinusLogProbMetric: 48.6740

Epoch 301: val_loss did not improve from 46.88164
196/196 - 36s - loss: 48.3797 - MinusLogProbMetric: 48.3797 - val_loss: 48.6740 - val_MinusLogProbMetric: 48.6740 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 302/1000
2023-10-27 16:17:47.526 
Epoch 302/1000 
	 loss: 64.4031, MinusLogProbMetric: 64.4031, val_loss: 56.5384, val_MinusLogProbMetric: 56.5384

Epoch 302: val_loss did not improve from 46.88164
196/196 - 33s - loss: 64.4031 - MinusLogProbMetric: 64.4031 - val_loss: 56.5384 - val_MinusLogProbMetric: 56.5384 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 303/1000
2023-10-27 16:18:21.227 
Epoch 303/1000 
	 loss: 53.1997, MinusLogProbMetric: 53.1997, val_loss: 52.0612, val_MinusLogProbMetric: 52.0612

Epoch 303: val_loss did not improve from 46.88164
196/196 - 34s - loss: 53.1997 - MinusLogProbMetric: 53.1997 - val_loss: 52.0612 - val_MinusLogProbMetric: 52.0612 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 304/1000
2023-10-27 16:18:55.072 
Epoch 304/1000 
	 loss: 51.9246, MinusLogProbMetric: 51.9246, val_loss: 52.8558, val_MinusLogProbMetric: 52.8558

Epoch 304: val_loss did not improve from 46.88164
196/196 - 34s - loss: 51.9246 - MinusLogProbMetric: 51.9246 - val_loss: 52.8558 - val_MinusLogProbMetric: 52.8558 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 305/1000
2023-10-27 16:19:32.828 
Epoch 305/1000 
	 loss: 51.9008, MinusLogProbMetric: 51.9008, val_loss: 54.9836, val_MinusLogProbMetric: 54.9836

Epoch 305: val_loss did not improve from 46.88164
196/196 - 38s - loss: 51.9008 - MinusLogProbMetric: 51.9008 - val_loss: 54.9836 - val_MinusLogProbMetric: 54.9836 - lr: 3.3333e-04 - 38s/epoch - 193ms/step
Epoch 306/1000
2023-10-27 16:20:09.007 
Epoch 306/1000 
	 loss: 54.1430, MinusLogProbMetric: 54.1430, val_loss: 53.9756, val_MinusLogProbMetric: 53.9756

Epoch 306: val_loss did not improve from 46.88164
196/196 - 36s - loss: 54.1430 - MinusLogProbMetric: 54.1430 - val_loss: 53.9756 - val_MinusLogProbMetric: 53.9756 - lr: 3.3333e-04 - 36s/epoch - 185ms/step
Epoch 307/1000
2023-10-27 16:20:43.152 
Epoch 307/1000 
	 loss: 52.6086, MinusLogProbMetric: 52.6086, val_loss: 53.4245, val_MinusLogProbMetric: 53.4245

Epoch 307: val_loss did not improve from 46.88164
196/196 - 34s - loss: 52.6086 - MinusLogProbMetric: 52.6086 - val_loss: 53.4245 - val_MinusLogProbMetric: 53.4245 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 308/1000
2023-10-27 16:21:17.244 
Epoch 308/1000 
	 loss: 51.6629, MinusLogProbMetric: 51.6629, val_loss: 51.8929, val_MinusLogProbMetric: 51.8929

Epoch 308: val_loss did not improve from 46.88164
196/196 - 34s - loss: 51.6629 - MinusLogProbMetric: 51.6629 - val_loss: 51.8929 - val_MinusLogProbMetric: 51.8929 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 309/1000
2023-10-27 16:21:50.526 
Epoch 309/1000 
	 loss: 51.6726, MinusLogProbMetric: 51.6726, val_loss: 50.6818, val_MinusLogProbMetric: 50.6818

Epoch 309: val_loss did not improve from 46.88164
196/196 - 33s - loss: 51.6726 - MinusLogProbMetric: 51.6726 - val_loss: 50.6818 - val_MinusLogProbMetric: 50.6818 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 310/1000
2023-10-27 16:22:27.635 
Epoch 310/1000 
	 loss: 50.5452, MinusLogProbMetric: 50.5452, val_loss: 50.4871, val_MinusLogProbMetric: 50.4871

Epoch 310: val_loss did not improve from 46.88164
196/196 - 37s - loss: 50.5452 - MinusLogProbMetric: 50.5452 - val_loss: 50.4871 - val_MinusLogProbMetric: 50.4871 - lr: 3.3333e-04 - 37s/epoch - 189ms/step
Epoch 311/1000
2023-10-27 16:23:05.249 
Epoch 311/1000 
	 loss: 50.2058, MinusLogProbMetric: 50.2058, val_loss: 51.0538, val_MinusLogProbMetric: 51.0538

Epoch 311: val_loss did not improve from 46.88164
196/196 - 38s - loss: 50.2058 - MinusLogProbMetric: 50.2058 - val_loss: 51.0538 - val_MinusLogProbMetric: 51.0538 - lr: 3.3333e-04 - 38s/epoch - 192ms/step
Epoch 312/1000
2023-10-27 16:23:38.459 
Epoch 312/1000 
	 loss: 49.6906, MinusLogProbMetric: 49.6906, val_loss: 49.0378, val_MinusLogProbMetric: 49.0378

Epoch 312: val_loss did not improve from 46.88164
196/196 - 33s - loss: 49.6906 - MinusLogProbMetric: 49.6906 - val_loss: 49.0378 - val_MinusLogProbMetric: 49.0378 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 313/1000
2023-10-27 16:24:11.590 
Epoch 313/1000 
	 loss: 51.3013, MinusLogProbMetric: 51.3013, val_loss: 49.9572, val_MinusLogProbMetric: 49.9572

Epoch 313: val_loss did not improve from 46.88164
196/196 - 33s - loss: 51.3013 - MinusLogProbMetric: 51.3013 - val_loss: 49.9572 - val_MinusLogProbMetric: 49.9572 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 314/1000
2023-10-27 16:24:44.722 
Epoch 314/1000 
	 loss: 49.2359, MinusLogProbMetric: 49.2359, val_loss: 49.3529, val_MinusLogProbMetric: 49.3529

Epoch 314: val_loss did not improve from 46.88164
196/196 - 33s - loss: 49.2359 - MinusLogProbMetric: 49.2359 - val_loss: 49.3529 - val_MinusLogProbMetric: 49.3529 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 315/1000
2023-10-27 16:25:19.289 
Epoch 315/1000 
	 loss: 49.3552, MinusLogProbMetric: 49.3552, val_loss: 49.5598, val_MinusLogProbMetric: 49.5598

Epoch 315: val_loss did not improve from 46.88164
196/196 - 35s - loss: 49.3552 - MinusLogProbMetric: 49.3552 - val_loss: 49.5598 - val_MinusLogProbMetric: 49.5598 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 316/1000
2023-10-27 16:25:55.237 
Epoch 316/1000 
	 loss: 49.0784, MinusLogProbMetric: 49.0784, val_loss: 48.5700, val_MinusLogProbMetric: 48.5700

Epoch 316: val_loss did not improve from 46.88164
196/196 - 36s - loss: 49.0784 - MinusLogProbMetric: 49.0784 - val_loss: 48.5700 - val_MinusLogProbMetric: 48.5700 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 317/1000
2023-10-27 16:26:30.140 
Epoch 317/1000 
	 loss: 49.0255, MinusLogProbMetric: 49.0255, val_loss: 50.2466, val_MinusLogProbMetric: 50.2466

Epoch 317: val_loss did not improve from 46.88164
196/196 - 35s - loss: 49.0255 - MinusLogProbMetric: 49.0255 - val_loss: 50.2466 - val_MinusLogProbMetric: 50.2466 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 318/1000
2023-10-27 16:27:03.154 
Epoch 318/1000 
	 loss: 48.8554, MinusLogProbMetric: 48.8554, val_loss: 49.6824, val_MinusLogProbMetric: 49.6824

Epoch 318: val_loss did not improve from 46.88164
196/196 - 33s - loss: 48.8554 - MinusLogProbMetric: 48.8554 - val_loss: 49.6824 - val_MinusLogProbMetric: 49.6824 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 319/1000
2023-10-27 16:27:35.961 
Epoch 319/1000 
	 loss: 54.0498, MinusLogProbMetric: 54.0498, val_loss: 55.3203, val_MinusLogProbMetric: 55.3203

Epoch 319: val_loss did not improve from 46.88164
196/196 - 33s - loss: 54.0498 - MinusLogProbMetric: 54.0498 - val_loss: 55.3203 - val_MinusLogProbMetric: 55.3203 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 320/1000
2023-10-27 16:28:08.639 
Epoch 320/1000 
	 loss: 52.1702, MinusLogProbMetric: 52.1702, val_loss: 50.9116, val_MinusLogProbMetric: 50.9116

Epoch 320: val_loss did not improve from 46.88164
196/196 - 33s - loss: 52.1702 - MinusLogProbMetric: 52.1702 - val_loss: 50.9116 - val_MinusLogProbMetric: 50.9116 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 321/1000
2023-10-27 16:28:41.242 
Epoch 321/1000 
	 loss: 50.4153, MinusLogProbMetric: 50.4153, val_loss: 50.2700, val_MinusLogProbMetric: 50.2700

Epoch 321: val_loss did not improve from 46.88164
196/196 - 33s - loss: 50.4153 - MinusLogProbMetric: 50.4153 - val_loss: 50.2700 - val_MinusLogProbMetric: 50.2700 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 322/1000
2023-10-27 16:29:15.429 
Epoch 322/1000 
	 loss: 49.9617, MinusLogProbMetric: 49.9617, val_loss: 49.6305, val_MinusLogProbMetric: 49.6305

Epoch 322: val_loss did not improve from 46.88164
196/196 - 34s - loss: 49.9617 - MinusLogProbMetric: 49.9617 - val_loss: 49.6305 - val_MinusLogProbMetric: 49.6305 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 323/1000
2023-10-27 16:29:49.859 
Epoch 323/1000 
	 loss: 49.9570, MinusLogProbMetric: 49.9570, val_loss: 49.7457, val_MinusLogProbMetric: 49.7457

Epoch 323: val_loss did not improve from 46.88164
196/196 - 34s - loss: 49.9570 - MinusLogProbMetric: 49.9570 - val_loss: 49.7457 - val_MinusLogProbMetric: 49.7457 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 324/1000
2023-10-27 16:30:22.985 
Epoch 324/1000 
	 loss: 50.0808, MinusLogProbMetric: 50.0808, val_loss: 50.1311, val_MinusLogProbMetric: 50.1311

Epoch 324: val_loss did not improve from 46.88164
196/196 - 33s - loss: 50.0808 - MinusLogProbMetric: 50.0808 - val_loss: 50.1311 - val_MinusLogProbMetric: 50.1311 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 325/1000
2023-10-27 16:30:57.373 
Epoch 325/1000 
	 loss: 49.4232, MinusLogProbMetric: 49.4232, val_loss: 50.0811, val_MinusLogProbMetric: 50.0811

Epoch 325: val_loss did not improve from 46.88164
196/196 - 34s - loss: 49.4232 - MinusLogProbMetric: 49.4232 - val_loss: 50.0811 - val_MinusLogProbMetric: 50.0811 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 326/1000
2023-10-27 16:31:39.618 
Epoch 326/1000 
	 loss: 49.2194, MinusLogProbMetric: 49.2194, val_loss: 49.4195, val_MinusLogProbMetric: 49.4195

Epoch 326: val_loss did not improve from 46.88164
196/196 - 42s - loss: 49.2194 - MinusLogProbMetric: 49.2194 - val_loss: 49.4195 - val_MinusLogProbMetric: 49.4195 - lr: 3.3333e-04 - 42s/epoch - 216ms/step
Epoch 327/1000
2023-10-27 16:32:13.390 
Epoch 327/1000 
	 loss: 49.1119, MinusLogProbMetric: 49.1119, val_loss: 49.8364, val_MinusLogProbMetric: 49.8364

Epoch 327: val_loss did not improve from 46.88164
196/196 - 34s - loss: 49.1119 - MinusLogProbMetric: 49.1119 - val_loss: 49.8364 - val_MinusLogProbMetric: 49.8364 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 328/1000
2023-10-27 16:32:46.242 
Epoch 328/1000 
	 loss: 47.8178, MinusLogProbMetric: 47.8178, val_loss: 48.0547, val_MinusLogProbMetric: 48.0547

Epoch 328: val_loss did not improve from 46.88164
196/196 - 33s - loss: 47.8178 - MinusLogProbMetric: 47.8178 - val_loss: 48.0547 - val_MinusLogProbMetric: 48.0547 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 329/1000
2023-10-27 16:33:19.058 
Epoch 329/1000 
	 loss: 47.6543, MinusLogProbMetric: 47.6543, val_loss: 47.9259, val_MinusLogProbMetric: 47.9259

Epoch 329: val_loss did not improve from 46.88164
196/196 - 33s - loss: 47.6543 - MinusLogProbMetric: 47.6543 - val_loss: 47.9259 - val_MinusLogProbMetric: 47.9259 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 330/1000
2023-10-27 16:33:52.761 
Epoch 330/1000 
	 loss: 47.6665, MinusLogProbMetric: 47.6665, val_loss: 48.0441, val_MinusLogProbMetric: 48.0441

Epoch 330: val_loss did not improve from 46.88164
196/196 - 34s - loss: 47.6665 - MinusLogProbMetric: 47.6665 - val_loss: 48.0441 - val_MinusLogProbMetric: 48.0441 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 331/1000
2023-10-27 16:34:30.414 
Epoch 331/1000 
	 loss: 47.6472, MinusLogProbMetric: 47.6472, val_loss: 47.9075, val_MinusLogProbMetric: 47.9075

Epoch 331: val_loss did not improve from 46.88164
196/196 - 38s - loss: 47.6472 - MinusLogProbMetric: 47.6472 - val_loss: 47.9075 - val_MinusLogProbMetric: 47.9075 - lr: 1.6667e-04 - 38s/epoch - 192ms/step
Epoch 332/1000
2023-10-27 16:35:10.206 
Epoch 332/1000 
	 loss: 47.6502, MinusLogProbMetric: 47.6502, val_loss: 48.6519, val_MinusLogProbMetric: 48.6519

Epoch 332: val_loss did not improve from 46.88164
196/196 - 40s - loss: 47.6502 - MinusLogProbMetric: 47.6502 - val_loss: 48.6519 - val_MinusLogProbMetric: 48.6519 - lr: 1.6667e-04 - 40s/epoch - 203ms/step
Epoch 333/1000
2023-10-27 16:35:51.666 
Epoch 333/1000 
	 loss: 47.6555, MinusLogProbMetric: 47.6555, val_loss: 48.3088, val_MinusLogProbMetric: 48.3088

Epoch 333: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.6555 - MinusLogProbMetric: 47.6555 - val_loss: 48.3088 - val_MinusLogProbMetric: 48.3088 - lr: 1.6667e-04 - 41s/epoch - 212ms/step
Epoch 334/1000
2023-10-27 16:36:32.612 
Epoch 334/1000 
	 loss: 47.6025, MinusLogProbMetric: 47.6025, val_loss: 48.3344, val_MinusLogProbMetric: 48.3344

Epoch 334: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.6025 - MinusLogProbMetric: 47.6025 - val_loss: 48.3344 - val_MinusLogProbMetric: 48.3344 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 335/1000
2023-10-27 16:37:13.559 
Epoch 335/1000 
	 loss: 47.5172, MinusLogProbMetric: 47.5172, val_loss: 48.7956, val_MinusLogProbMetric: 48.7956

Epoch 335: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.5172 - MinusLogProbMetric: 47.5172 - val_loss: 48.7956 - val_MinusLogProbMetric: 48.7956 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 336/1000
2023-10-27 16:37:54.260 
Epoch 336/1000 
	 loss: 47.5114, MinusLogProbMetric: 47.5114, val_loss: 48.5277, val_MinusLogProbMetric: 48.5277

Epoch 336: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.5114 - MinusLogProbMetric: 47.5114 - val_loss: 48.5277 - val_MinusLogProbMetric: 48.5277 - lr: 1.6667e-04 - 41s/epoch - 208ms/step
Epoch 337/1000
2023-10-27 16:38:35.025 
Epoch 337/1000 
	 loss: 47.4472, MinusLogProbMetric: 47.4472, val_loss: 47.7253, val_MinusLogProbMetric: 47.7253

Epoch 337: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.4472 - MinusLogProbMetric: 47.4472 - val_loss: 47.7253 - val_MinusLogProbMetric: 47.7253 - lr: 1.6667e-04 - 41s/epoch - 208ms/step
Epoch 338/1000
2023-10-27 16:39:16.460 
Epoch 338/1000 
	 loss: 47.4211, MinusLogProbMetric: 47.4211, val_loss: 48.2962, val_MinusLogProbMetric: 48.2962

Epoch 338: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.4211 - MinusLogProbMetric: 47.4211 - val_loss: 48.2962 - val_MinusLogProbMetric: 48.2962 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 339/1000
2023-10-27 16:39:57.160 
Epoch 339/1000 
	 loss: 47.4804, MinusLogProbMetric: 47.4804, val_loss: 47.7624, val_MinusLogProbMetric: 47.7624

Epoch 339: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.4804 - MinusLogProbMetric: 47.4804 - val_loss: 47.7624 - val_MinusLogProbMetric: 47.7624 - lr: 1.6667e-04 - 41s/epoch - 208ms/step
Epoch 340/1000
2023-10-27 16:40:37.714 
Epoch 340/1000 
	 loss: 47.4348, MinusLogProbMetric: 47.4348, val_loss: 48.7807, val_MinusLogProbMetric: 48.7807

Epoch 340: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.4348 - MinusLogProbMetric: 47.4348 - val_loss: 48.7807 - val_MinusLogProbMetric: 48.7807 - lr: 1.6667e-04 - 41s/epoch - 207ms/step
Epoch 341/1000
2023-10-27 16:41:18.961 
Epoch 341/1000 
	 loss: 47.3256, MinusLogProbMetric: 47.3256, val_loss: 48.4952, val_MinusLogProbMetric: 48.4952

Epoch 341: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.3256 - MinusLogProbMetric: 47.3256 - val_loss: 48.4952 - val_MinusLogProbMetric: 48.4952 - lr: 1.6667e-04 - 41s/epoch - 210ms/step
Epoch 342/1000
2023-10-27 16:41:58.197 
Epoch 342/1000 
	 loss: 47.3258, MinusLogProbMetric: 47.3258, val_loss: 47.4267, val_MinusLogProbMetric: 47.4267

Epoch 342: val_loss did not improve from 46.88164
196/196 - 39s - loss: 47.3258 - MinusLogProbMetric: 47.3258 - val_loss: 47.4267 - val_MinusLogProbMetric: 47.4267 - lr: 1.6667e-04 - 39s/epoch - 200ms/step
Epoch 343/1000
2023-10-27 16:42:31.137 
Epoch 343/1000 
	 loss: 47.2646, MinusLogProbMetric: 47.2646, val_loss: 47.7065, val_MinusLogProbMetric: 47.7065

Epoch 343: val_loss did not improve from 46.88164
196/196 - 33s - loss: 47.2646 - MinusLogProbMetric: 47.2646 - val_loss: 47.7065 - val_MinusLogProbMetric: 47.7065 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 344/1000
2023-10-27 16:43:04.101 
Epoch 344/1000 
	 loss: 47.2982, MinusLogProbMetric: 47.2982, val_loss: 47.7418, val_MinusLogProbMetric: 47.7418

Epoch 344: val_loss did not improve from 46.88164
196/196 - 33s - loss: 47.2982 - MinusLogProbMetric: 47.2982 - val_loss: 47.7418 - val_MinusLogProbMetric: 47.7418 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 345/1000
2023-10-27 16:43:37.123 
Epoch 345/1000 
	 loss: 47.2374, MinusLogProbMetric: 47.2374, val_loss: 47.4586, val_MinusLogProbMetric: 47.4586

Epoch 345: val_loss did not improve from 46.88164
196/196 - 33s - loss: 47.2374 - MinusLogProbMetric: 47.2374 - val_loss: 47.4586 - val_MinusLogProbMetric: 47.4586 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 346/1000
2023-10-27 16:44:11.776 
Epoch 346/1000 
	 loss: 47.1372, MinusLogProbMetric: 47.1372, val_loss: 47.8297, val_MinusLogProbMetric: 47.8297

Epoch 346: val_loss did not improve from 46.88164
196/196 - 35s - loss: 47.1372 - MinusLogProbMetric: 47.1372 - val_loss: 47.8297 - val_MinusLogProbMetric: 47.8297 - lr: 1.6667e-04 - 35s/epoch - 177ms/step
Epoch 347/1000
2023-10-27 16:44:53.211 
Epoch 347/1000 
	 loss: 47.2138, MinusLogProbMetric: 47.2138, val_loss: 47.5906, val_MinusLogProbMetric: 47.5906

Epoch 347: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.2138 - MinusLogProbMetric: 47.2138 - val_loss: 47.5906 - val_MinusLogProbMetric: 47.5906 - lr: 1.6667e-04 - 41s/epoch - 211ms/step
Epoch 348/1000
2023-10-27 16:45:34.422 
Epoch 348/1000 
	 loss: 47.2615, MinusLogProbMetric: 47.2615, val_loss: 48.2886, val_MinusLogProbMetric: 48.2886

Epoch 348: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.2615 - MinusLogProbMetric: 47.2615 - val_loss: 48.2886 - val_MinusLogProbMetric: 48.2886 - lr: 1.6667e-04 - 41s/epoch - 210ms/step
Epoch 349/1000
2023-10-27 16:46:15.090 
Epoch 349/1000 
	 loss: 47.2531, MinusLogProbMetric: 47.2531, val_loss: 47.3427, val_MinusLogProbMetric: 47.3427

Epoch 349: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.2531 - MinusLogProbMetric: 47.2531 - val_loss: 47.3427 - val_MinusLogProbMetric: 47.3427 - lr: 1.6667e-04 - 41s/epoch - 207ms/step
Epoch 350/1000
2023-10-27 16:46:55.967 
Epoch 350/1000 
	 loss: 47.0057, MinusLogProbMetric: 47.0057, val_loss: 47.7319, val_MinusLogProbMetric: 47.7319

Epoch 350: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.0057 - MinusLogProbMetric: 47.0057 - val_loss: 47.7319 - val_MinusLogProbMetric: 47.7319 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 351/1000
2023-10-27 16:47:36.855 
Epoch 351/1000 
	 loss: 47.1468, MinusLogProbMetric: 47.1468, val_loss: 48.4518, val_MinusLogProbMetric: 48.4518

Epoch 351: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.1468 - MinusLogProbMetric: 47.1468 - val_loss: 48.4518 - val_MinusLogProbMetric: 48.4518 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 352/1000
2023-10-27 16:48:18.057 
Epoch 352/1000 
	 loss: 47.0369, MinusLogProbMetric: 47.0369, val_loss: 47.2887, val_MinusLogProbMetric: 47.2887

Epoch 352: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.0369 - MinusLogProbMetric: 47.0369 - val_loss: 47.2887 - val_MinusLogProbMetric: 47.2887 - lr: 1.6667e-04 - 41s/epoch - 210ms/step
Epoch 353/1000
2023-10-27 16:48:58.180 
Epoch 353/1000 
	 loss: 47.0941, MinusLogProbMetric: 47.0941, val_loss: 47.3130, val_MinusLogProbMetric: 47.3130

Epoch 353: val_loss did not improve from 46.88164
196/196 - 40s - loss: 47.0941 - MinusLogProbMetric: 47.0941 - val_loss: 47.3130 - val_MinusLogProbMetric: 47.3130 - lr: 1.6667e-04 - 40s/epoch - 205ms/step
Epoch 354/1000
2023-10-27 16:49:39.134 
Epoch 354/1000 
	 loss: 47.0984, MinusLogProbMetric: 47.0984, val_loss: 47.2612, val_MinusLogProbMetric: 47.2612

Epoch 354: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.0984 - MinusLogProbMetric: 47.0984 - val_loss: 47.2612 - val_MinusLogProbMetric: 47.2612 - lr: 1.6667e-04 - 41s/epoch - 209ms/step
Epoch 355/1000
2023-10-27 16:50:20.602 
Epoch 355/1000 
	 loss: 47.0333, MinusLogProbMetric: 47.0333, val_loss: 47.1763, val_MinusLogProbMetric: 47.1763

Epoch 355: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.0333 - MinusLogProbMetric: 47.0333 - val_loss: 47.1763 - val_MinusLogProbMetric: 47.1763 - lr: 1.6667e-04 - 41s/epoch - 212ms/step
Epoch 356/1000
2023-10-27 16:51:01.195 
Epoch 356/1000 
	 loss: 47.0160, MinusLogProbMetric: 47.0160, val_loss: 47.3260, val_MinusLogProbMetric: 47.3260

Epoch 356: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.0160 - MinusLogProbMetric: 47.0160 - val_loss: 47.3260 - val_MinusLogProbMetric: 47.3260 - lr: 1.6667e-04 - 41s/epoch - 207ms/step
Epoch 357/1000
2023-10-27 16:51:42.832 
Epoch 357/1000 
	 loss: 46.9762, MinusLogProbMetric: 46.9762, val_loss: 47.2528, val_MinusLogProbMetric: 47.2528

Epoch 357: val_loss did not improve from 46.88164
196/196 - 42s - loss: 46.9762 - MinusLogProbMetric: 46.9762 - val_loss: 47.2528 - val_MinusLogProbMetric: 47.2528 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 358/1000
2023-10-27 16:52:23.430 
Epoch 358/1000 
	 loss: 47.0944, MinusLogProbMetric: 47.0944, val_loss: 47.3816, val_MinusLogProbMetric: 47.3816

Epoch 358: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.0944 - MinusLogProbMetric: 47.0944 - val_loss: 47.3816 - val_MinusLogProbMetric: 47.3816 - lr: 1.6667e-04 - 41s/epoch - 207ms/step
Epoch 359/1000
2023-10-27 16:53:05.326 
Epoch 359/1000 
	 loss: 46.9411, MinusLogProbMetric: 46.9411, val_loss: 47.2374, val_MinusLogProbMetric: 47.2374

Epoch 359: val_loss did not improve from 46.88164
196/196 - 42s - loss: 46.9411 - MinusLogProbMetric: 46.9411 - val_loss: 47.2374 - val_MinusLogProbMetric: 47.2374 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 360/1000
2023-10-27 16:53:46.418 
Epoch 360/1000 
	 loss: 47.0044, MinusLogProbMetric: 47.0044, val_loss: 47.3516, val_MinusLogProbMetric: 47.3516

Epoch 360: val_loss did not improve from 46.88164
196/196 - 41s - loss: 47.0044 - MinusLogProbMetric: 47.0044 - val_loss: 47.3516 - val_MinusLogProbMetric: 47.3516 - lr: 1.6667e-04 - 41s/epoch - 210ms/step
Epoch 361/1000
2023-10-27 16:54:26.946 
Epoch 361/1000 
	 loss: 46.9603, MinusLogProbMetric: 46.9603, val_loss: 47.5344, val_MinusLogProbMetric: 47.5344

Epoch 361: val_loss did not improve from 46.88164
196/196 - 41s - loss: 46.9603 - MinusLogProbMetric: 46.9603 - val_loss: 47.5344 - val_MinusLogProbMetric: 47.5344 - lr: 1.6667e-04 - 41s/epoch - 207ms/step
Epoch 362/1000
2023-10-27 16:55:08.521 
Epoch 362/1000 
	 loss: 46.9922, MinusLogProbMetric: 46.9922, val_loss: 47.0530, val_MinusLogProbMetric: 47.0530

Epoch 362: val_loss did not improve from 46.88164
196/196 - 42s - loss: 46.9922 - MinusLogProbMetric: 46.9922 - val_loss: 47.0530 - val_MinusLogProbMetric: 47.0530 - lr: 1.6667e-04 - 42s/epoch - 212ms/step
Epoch 363/1000
2023-10-27 16:55:50.447 
Epoch 363/1000 
	 loss: 46.8386, MinusLogProbMetric: 46.8386, val_loss: 47.1248, val_MinusLogProbMetric: 47.1248

Epoch 363: val_loss did not improve from 46.88164
196/196 - 42s - loss: 46.8386 - MinusLogProbMetric: 46.8386 - val_loss: 47.1248 - val_MinusLogProbMetric: 47.1248 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 364/1000
2023-10-27 16:56:31.537 
Epoch 364/1000 
	 loss: 46.8598, MinusLogProbMetric: 46.8598, val_loss: 47.2682, val_MinusLogProbMetric: 47.2682

Epoch 364: val_loss did not improve from 46.88164
196/196 - 41s - loss: 46.8598 - MinusLogProbMetric: 46.8598 - val_loss: 47.2682 - val_MinusLogProbMetric: 47.2682 - lr: 1.6667e-04 - 41s/epoch - 210ms/step
Epoch 365/1000
2023-10-27 16:57:12.607 
Epoch 365/1000 
	 loss: 46.8686, MinusLogProbMetric: 46.8686, val_loss: 47.8675, val_MinusLogProbMetric: 47.8675

Epoch 365: val_loss did not improve from 46.88164
196/196 - 41s - loss: 46.8686 - MinusLogProbMetric: 46.8686 - val_loss: 47.8675 - val_MinusLogProbMetric: 47.8675 - lr: 1.6667e-04 - 41s/epoch - 210ms/step
Epoch 366/1000
2023-10-27 16:57:53.179 
Epoch 366/1000 
	 loss: 46.9672, MinusLogProbMetric: 46.9672, val_loss: 47.0049, val_MinusLogProbMetric: 47.0049

Epoch 366: val_loss did not improve from 46.88164
196/196 - 41s - loss: 46.9672 - MinusLogProbMetric: 46.9672 - val_loss: 47.0049 - val_MinusLogProbMetric: 47.0049 - lr: 1.6667e-04 - 41s/epoch - 207ms/step
Epoch 367/1000
2023-10-27 16:58:34.247 
Epoch 367/1000 
	 loss: 46.8311, MinusLogProbMetric: 46.8311, val_loss: 46.9412, val_MinusLogProbMetric: 46.9412

Epoch 367: val_loss did not improve from 46.88164
196/196 - 41s - loss: 46.8311 - MinusLogProbMetric: 46.8311 - val_loss: 46.9412 - val_MinusLogProbMetric: 46.9412 - lr: 1.6667e-04 - 41s/epoch - 210ms/step
Epoch 368/1000
2023-10-27 16:59:14.850 
Epoch 368/1000 
	 loss: 46.8217, MinusLogProbMetric: 46.8217, val_loss: 46.9693, val_MinusLogProbMetric: 46.9693

Epoch 368: val_loss did not improve from 46.88164
196/196 - 41s - loss: 46.8217 - MinusLogProbMetric: 46.8217 - val_loss: 46.9693 - val_MinusLogProbMetric: 46.9693 - lr: 1.6667e-04 - 41s/epoch - 207ms/step
Epoch 369/1000
2023-10-27 16:59:55.715 
Epoch 369/1000 
	 loss: 46.8338, MinusLogProbMetric: 46.8338, val_loss: 47.0423, val_MinusLogProbMetric: 47.0423

Epoch 369: val_loss did not improve from 46.88164
196/196 - 41s - loss: 46.8338 - MinusLogProbMetric: 46.8338 - val_loss: 47.0423 - val_MinusLogProbMetric: 47.0423 - lr: 1.6667e-04 - 41s/epoch - 208ms/step
Epoch 370/1000
2023-10-27 17:00:35.984 
Epoch 370/1000 
	 loss: 46.8443, MinusLogProbMetric: 46.8443, val_loss: 47.2040, val_MinusLogProbMetric: 47.2040

Epoch 370: val_loss did not improve from 46.88164
196/196 - 40s - loss: 46.8443 - MinusLogProbMetric: 46.8443 - val_loss: 47.2040 - val_MinusLogProbMetric: 47.2040 - lr: 1.6667e-04 - 40s/epoch - 205ms/step
Epoch 371/1000
2023-10-27 17:01:17.942 
Epoch 371/1000 
	 loss: 46.7706, MinusLogProbMetric: 46.7706, val_loss: 46.9456, val_MinusLogProbMetric: 46.9456

Epoch 371: val_loss did not improve from 46.88164
196/196 - 42s - loss: 46.7706 - MinusLogProbMetric: 46.7706 - val_loss: 46.9456 - val_MinusLogProbMetric: 46.9456 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 372/1000
2023-10-27 17:01:59.901 
Epoch 372/1000 
	 loss: 46.7827, MinusLogProbMetric: 46.7827, val_loss: 47.2727, val_MinusLogProbMetric: 47.2727

Epoch 372: val_loss did not improve from 46.88164
196/196 - 42s - loss: 46.7827 - MinusLogProbMetric: 46.7827 - val_loss: 47.2727 - val_MinusLogProbMetric: 47.2727 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 373/1000
2023-10-27 17:02:41.113 
Epoch 373/1000 
	 loss: 46.7264, MinusLogProbMetric: 46.7264, val_loss: 47.2833, val_MinusLogProbMetric: 47.2833

Epoch 373: val_loss did not improve from 46.88164
196/196 - 41s - loss: 46.7264 - MinusLogProbMetric: 46.7264 - val_loss: 47.2833 - val_MinusLogProbMetric: 47.2833 - lr: 1.6667e-04 - 41s/epoch - 210ms/step
Epoch 374/1000
2023-10-27 17:03:21.790 
Epoch 374/1000 
	 loss: 46.7508, MinusLogProbMetric: 46.7508, val_loss: 47.6764, val_MinusLogProbMetric: 47.6764

Epoch 374: val_loss did not improve from 46.88164
196/196 - 41s - loss: 46.7508 - MinusLogProbMetric: 46.7508 - val_loss: 47.6764 - val_MinusLogProbMetric: 47.6764 - lr: 1.6667e-04 - 41s/epoch - 208ms/step
Epoch 375/1000
2023-10-27 17:04:02.598 
Epoch 375/1000 
	 loss: 46.7890, MinusLogProbMetric: 46.7890, val_loss: 47.8805, val_MinusLogProbMetric: 47.8805

Epoch 375: val_loss did not improve from 46.88164
196/196 - 41s - loss: 46.7890 - MinusLogProbMetric: 46.7890 - val_loss: 47.8805 - val_MinusLogProbMetric: 47.8805 - lr: 1.6667e-04 - 41s/epoch - 208ms/step
Epoch 376/1000
2023-10-27 17:04:44.522 
Epoch 376/1000 
	 loss: 46.6980, MinusLogProbMetric: 46.6980, val_loss: 47.4722, val_MinusLogProbMetric: 47.4722

Epoch 376: val_loss did not improve from 46.88164
196/196 - 42s - loss: 46.6980 - MinusLogProbMetric: 46.6980 - val_loss: 47.4722 - val_MinusLogProbMetric: 47.4722 - lr: 1.6667e-04 - 42s/epoch - 214ms/step
Epoch 377/1000
2023-10-27 17:05:28.019 
Epoch 377/1000 
	 loss: 46.7117, MinusLogProbMetric: 46.7117, val_loss: 47.2075, val_MinusLogProbMetric: 47.2075

Epoch 377: val_loss did not improve from 46.88164
Restoring model weights from the end of the best epoch: 277.
196/196 - 44s - loss: 46.7117 - MinusLogProbMetric: 46.7117 - val_loss: 47.2075 - val_MinusLogProbMetric: 47.2075 - lr: 1.6667e-04 - 44s/epoch - 224ms/step
Epoch 377: early stopping
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 187.
Model trained in 14853.77 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.99 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 1.46 s.
===========
Run 412/720 done in 14948.29 s.
===========

Directory ../../results/CsplineN_new/run_413/ already exists.
Skipping it.
===========
Run 413/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_414/ already exists.
Skipping it.
===========
Run 414/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_415/ already exists.
Skipping it.
===========
Run 415/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_416/ already exists.
Skipping it.
===========
Run 416/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_417/ already exists.
Skipping it.
===========
Run 417/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_418/ already exists.
Skipping it.
===========
Run 418/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_419/ already exists.
Skipping it.
===========
Run 419/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_420/ already exists.
Skipping it.
===========
Run 420/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_421/ already exists.
Skipping it.
===========
Run 421/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_422/ already exists.
Skipping it.
===========
Run 422/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_423/ already exists.
Skipping it.
===========
Run 423/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_424/ already exists.
Skipping it.
===========
Run 424/720 already exists. Skipping it.
===========

===========
Generating train data for run 425.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_425
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_17"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_18 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_2 (LogProbLa  (None,)                  939510    
 yer)                                                            
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_2/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_2'")
self.model: <keras.engine.functional.Functional object at 0x7f755c4b2f80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f75f80c2da0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f75f80c2da0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f752038c280>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7b04ddc220>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7b04ddc790>, <keras.callbacks.ModelCheckpoint object at 0x7f7b04ddc850>, <keras.callbacks.EarlyStopping object at 0x7f7b04ddcac0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7b04ddcaf0>, <keras.callbacks.TerminateOnNaN object at 0x7f7b04ddc730>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_425/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 425/720 with hyperparameters:
timestamp = 2023-10-27 17:05:33.360440
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 17:06:32.477 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10855.1270, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 59s - loss: nan - MinusLogProbMetric: 10855.1270 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 59s/epoch - 301ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 425.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_425
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_23"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_24 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_3 (LogProbLa  (None,)                  939510    
 yer)                                                            
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_3/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_3'")
self.model: <keras.engine.functional.Functional object at 0x7f7ada02b700>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7ad9bd0d00>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7ad9bd0d00>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7ada02bf10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7ad17fc2b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7ad17fc820>, <keras.callbacks.ModelCheckpoint object at 0x7f7ad17fc8e0>, <keras.callbacks.EarlyStopping object at 0x7f7ad17fcb50>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7ad17fcb80>, <keras.callbacks.TerminateOnNaN object at 0x7f7ad17fc7c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_425/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 425/720 with hyperparameters:
timestamp = 2023-10-27 17:06:36.652832
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 17:07:36.311 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10855.1270, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 60s - loss: nan - MinusLogProbMetric: 10855.1270 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 60s/epoch - 304ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0001111111111111111.
===========
Generating train data for run 425.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_425
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_29"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_30 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_4 (LogProbLa  (None,)                  939510    
 yer)                                                            
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_4/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_4'")
self.model: <keras.engine.functional.Functional object at 0x7f75da2c73d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7ae2741060>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7ae2741060>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73a0772680>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73a0694dc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73a0695330>, <keras.callbacks.ModelCheckpoint object at 0x7f73a06953f0>, <keras.callbacks.EarlyStopping object at 0x7f73a0695660>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73a0695690>, <keras.callbacks.TerminateOnNaN object at 0x7f73a06952d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_425/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 425/720 with hyperparameters:
timestamp = 2023-10-27 17:07:40.842731
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 17:08:38.596 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10855.1270, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 58s - loss: nan - MinusLogProbMetric: 10855.1270 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 58s/epoch - 294ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 3.703703703703703e-05.
===========
Generating train data for run 425.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_425
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_35"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_36 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_5 (LogProbLa  (None,)                  939510    
 yer)                                                            
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_5/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_5'")
self.model: <keras.engine.functional.Functional object at 0x7f7ad14d14b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f74029512a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f74029512a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73f07305b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73a8454910>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73a8454e80>, <keras.callbacks.ModelCheckpoint object at 0x7f73a8454f40>, <keras.callbacks.EarlyStopping object at 0x7f73a84551b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73a84551e0>, <keras.callbacks.TerminateOnNaN object at 0x7f73a8454e20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_425/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 425/720 with hyperparameters:
timestamp = 2023-10-27 17:08:43.319879
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:5 out of the last 73896 calls to <function Model.make_train_function.<locals>.train_function at 0x7f72f07441f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 17:09:41.053 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10855.1270, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 58s - loss: nan - MinusLogProbMetric: 10855.1270 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 58s/epoch - 294ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.2345679012345677e-05.
===========
Generating train data for run 425.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_425
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_41"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_42 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_6 (LogProbLa  (None,)                  939510    
 yer)                                                            
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_6/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_6'")
self.model: <keras.engine.functional.Functional object at 0x7f74021dfc70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f76d051f940>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f76d051f940>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73b0798700>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7402116e00>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7402117370>, <keras.callbacks.ModelCheckpoint object at 0x7f7402117430>, <keras.callbacks.EarlyStopping object at 0x7f74021176a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f74021176d0>, <keras.callbacks.TerminateOnNaN object at 0x7f7402117310>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_425/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 425/720 with hyperparameters:
timestamp = 2023-10-27 17:09:45.341638
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
WARNING:tensorflow:6 out of the last 73897 calls to <function Model.make_train_function.<locals>.train_function at 0x7f7b04d901f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 17:10:44.583 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10855.1270, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 59s - loss: nan - MinusLogProbMetric: 10855.1270 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 59s/epoch - 302ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.115226337448558e-06.
===========
Generating train data for run 425.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_425
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_47"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_48 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_7 (LogProbLa  (None,)                  939510    
 yer)                                                            
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_7/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_7'")
self.model: <keras.engine.functional.Functional object at 0x7f7402e47c70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7404858b50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7404858b50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f72f076a590>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7402e12c80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7402e131f0>, <keras.callbacks.ModelCheckpoint object at 0x7f7402e132b0>, <keras.callbacks.EarlyStopping object at 0x7f7402e13520>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7402e13550>, <keras.callbacks.TerminateOnNaN object at 0x7f7402e13190>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_425/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 425/720 with hyperparameters:
timestamp = 2023-10-27 17:10:49.090170
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 17:11:46.134 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10855.1270, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 57s - loss: nan - MinusLogProbMetric: 10855.1270 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 57s/epoch - 290ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.3717421124828526e-06.
===========
Generating train data for run 425.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_425
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_53"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_54 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_8 (LogProbLa  (None,)                  939510    
 yer)                                                            
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_8/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_8'")
self.model: <keras.engine.functional.Functional object at 0x7f73f0193f70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f75547665c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f75547665c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f74026256f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7aa519fc40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7aa51341f0>, <keras.callbacks.ModelCheckpoint object at 0x7f7aa51342b0>, <keras.callbacks.EarlyStopping object at 0x7f7aa5134520>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7aa5134550>, <keras.callbacks.TerminateOnNaN object at 0x7f7aa5134190>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_425/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 425/720 with hyperparameters:
timestamp = 2023-10-27 17:11:50.891568
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 17:12:47.072 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10855.1270, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 10855.1270 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 56s/epoch - 287ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 4.572473708276175e-07.
===========
Generating train data for run 425.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_425
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_59"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_60 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_9 (LogProbLa  (None,)                  939510    
 yer)                                                            
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_9/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_9'")
self.model: <keras.engine.functional.Functional object at 0x7f7aead84e20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7402f1ba90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7402f1ba90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f74022e4190>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f74046c7ac0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f74046c7fd0>, <keras.callbacks.ModelCheckpoint object at 0x7f74046cc130>, <keras.callbacks.EarlyStopping object at 0x7f74046cc3a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f74046cc3d0>, <keras.callbacks.TerminateOnNaN object at 0x7f74046cc040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_425/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 425/720 with hyperparameters:
timestamp = 2023-10-27 17:12:50.967792
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 17:13:49.535 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10855.1270, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 58s - loss: nan - MinusLogProbMetric: 10855.1270 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 58s/epoch - 298ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.524157902758725e-07.
===========
Generating train data for run 425.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_425
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_65"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_66 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_10 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_10/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_10'")
self.model: <keras.engine.functional.Functional object at 0x7f76d0733c10>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7aa4c162c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7aa4c162c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f76d035e4d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73a025df60>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73a025e4d0>, <keras.callbacks.ModelCheckpoint object at 0x7f73a025e590>, <keras.callbacks.EarlyStopping object at 0x7f73a025e800>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73a025e830>, <keras.callbacks.TerminateOnNaN object at 0x7f73a025e470>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_425/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 425/720 with hyperparameters:
timestamp = 2023-10-27 17:13:53.617834
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 17:14:49.402 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10855.1270, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 10855.1270 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 56s/epoch - 284ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.0805263425290834e-08.
===========
Generating train data for run 425.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_425
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_71"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_72 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_11 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_11/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_11'")
self.model: <keras.engine.functional.Functional object at 0x7f7403175000>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7404e1c790>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7404e1c790>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7404e1e1a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7404e1eaa0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7404e1f010>, <keras.callbacks.ModelCheckpoint object at 0x7f7404e1f0d0>, <keras.callbacks.EarlyStopping object at 0x7f7404e1f340>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7404e1f370>, <keras.callbacks.TerminateOnNaN object at 0x7f7404e1efb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_425/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 425/720 with hyperparameters:
timestamp = 2023-10-27 17:14:53.940996
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 17:15:51.582 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10855.1270, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 58s - loss: nan - MinusLogProbMetric: 10855.1270 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 58s/epoch - 294ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 1.6935087808430278e-08.
===========
Generating train data for run 425.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_425/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_425
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_77"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_78 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_12 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_12/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_12'")
self.model: <keras.engine.functional.Functional object at 0x7f7aa4d9c520>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f73a87bd7b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f73a87bd7b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7403283ac0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7401f20be0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_425/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7401f21900>, <keras.callbacks.ModelCheckpoint object at 0x7f7401f21270>, <keras.callbacks.EarlyStopping object at 0x7f7401f21090>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7401f20460>, <keras.callbacks.TerminateOnNaN object at 0x7f7401f213f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_425/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 425/720 with hyperparameters:
timestamp = 2023-10-27 17:15:56.552083
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 17:16:52.715 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10855.1270, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 10855.1270 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 56s/epoch - 286ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 5.645029269476759e-09.
===========
Run 425/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 426.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_426/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_426/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_426/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_426
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_83"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_84 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_13 (LogProbL  (None,)                  2200950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,200,950
Trainable params: 2,200,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_13/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_13'")
self.model: <keras.engine.functional.Functional object at 0x7f7ab8017fd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f740202f2e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f740202f2e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73f3175f00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7ab807b850>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7ab807bdc0>, <keras.callbacks.ModelCheckpoint object at 0x7f7ab807be80>, <keras.callbacks.EarlyStopping object at 0x7f7ab807bf70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7ab807bf10>, <keras.callbacks.TerminateOnNaN object at 0x7f7ab807bd90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_426/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 426/720 with hyperparameters:
timestamp = 2023-10-27 17:16:56.716196
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2200950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 6: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-27 17:17:58.644 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 8782.9189, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 62s - loss: nan - MinusLogProbMetric: 8782.9189 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 62s/epoch - 315ms/step
The loss history contains NaN values.
Training failed: trying again with seed 978294 and lr 0.0003333333333333333.
===========
Generating train data for run 426.
===========
Train data generated in 0.19 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_426/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 440}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_426/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_426/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_426
self.data_kwargs: {'seed': 440}
self.x_data: [[ 6.8427815   2.8418343   7.2225194  ...  3.063788    0.63813645
   4.869683  ]
 [ 3.3568702   7.2389436   3.91875    ...  7.561039   10.192369
  10.237938  ]
 [ 7.2036924   3.0214014   7.4942985  ...  1.8736274   0.49044883
   4.17296   ]
 ...
 [ 2.983732    6.193991    3.466531   ...  9.819353    9.192663
   9.6530285 ]
 [ 3.4107406   6.741781    3.2448382  ...  8.579087    9.408462
  10.291045  ]
 [ 6.991257    1.4824692   7.3291607  ...  2.938532    0.41938597
   3.6897519 ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_89"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_90 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_14 (LogProbL  (None,)                  2200950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,200,950
Trainable params: 2,200,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_14/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_14'")
self.model: <keras.engine.functional.Functional object at 0x7f7aa7037b80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7aaf25f6a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7aaf25f6a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73f1fc8670>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7aa7096b90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7aa7097100>, <keras.callbacks.ModelCheckpoint object at 0x7f7aa70971c0>, <keras.callbacks.EarlyStopping object at 0x7f7aa7097430>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7aa7097460>, <keras.callbacks.TerminateOnNaN object at 0x7f7aa70970a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 6.7833824 ,  5.979762  ,  6.70892   , ...,  0.13252193,
         8.314855  ,  0.295921  ],
       [ 4.9103665 ,  7.8417034 ,  5.917997  , ..., -0.4679658 ,
         8.274182  , -0.08630078],
       [ 3.102833  ,  7.087651  ,  3.269106  , ...,  8.235055  ,
         9.345084  ,  8.34726   ],
       ...,
       [ 7.003491  ,  3.0847974 ,  7.4666314 , ...,  3.2820067 ,
         0.7031764 ,  4.2121606 ],
       [ 2.962933  ,  6.8779483 ,  4.0922813 , ...,  8.617371  ,
         9.531757  ,  8.87614   ],
       [ 3.3107684 ,  7.200947  ,  2.8012362 , ...,  7.512141  ,
         9.795536  ,  8.62886   ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_426/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 426/720 with hyperparameters:
timestamp = 2023-10-27 17:18:02.545256
ndims = 100
seed_train = 440
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2200950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 6.8427815   2.8418343   7.2225194  10.415633    3.4455686   6.14213
  5.753561    6.03471     1.8715446   9.53411     4.4864173   8.820064
  6.9416695   3.2678108   8.356851    3.861458    8.996062    5.821541
  8.838011    6.5295396   8.101426    6.486868    9.985665    6.642628
  2.865735    5.177376   -0.737168    3.008622    6.007168    2.8231366
  6.342085    4.584826    1.233682    1.8677073   6.203567    5.8407316
  6.1723785   6.4982657   6.9730644   4.5782175   8.9432      4.1616006
  4.155563    8.860918    7.7759166   7.1127677   0.07635063  9.676662
  7.146252   10.762262    1.3789698   9.474915    1.3793094   6.1593947
  0.45352787  8.557686    8.017052    5.7029414   5.2734895   0.07348329
  6.616018    4.3992834   6.955278    8.3876505  10.133325    8.174863
 -0.5543385   3.2184148   7.8509903   1.3025492   4.8006268   1.6788661
  1.813379   -0.9351572   7.604723    1.9064149   4.31039     8.632207
  5.5583024   1.267277    1.580614    6.270075    5.8559227   2.6634958
  9.283614    6.655195    5.888442    7.0383253   7.4760857   2.2032177
  4.411108    2.5667205   0.8848109  10.196264    7.578826    4.845884
  1.1469412   3.063788    0.63813645  4.869683  ]
Epoch 1/1000
2023-10-27 17:19:34.064 
Epoch 1/1000 
	 loss: 1979.0640, MinusLogProbMetric: 1979.0640, val_loss: 500.1635, val_MinusLogProbMetric: 500.1635

Epoch 1: val_loss improved from inf to 500.16351, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 92s - loss: 1979.0640 - MinusLogProbMetric: 1979.0640 - val_loss: 500.1635 - val_MinusLogProbMetric: 500.1635 - lr: 3.3333e-04 - 92s/epoch - 469ms/step
Epoch 2/1000
2023-10-27 17:20:09.208 
Epoch 2/1000 
	 loss: 488.4788, MinusLogProbMetric: 488.4788, val_loss: 409.1490, val_MinusLogProbMetric: 409.1490

Epoch 2: val_loss improved from 500.16351 to 409.14899, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 488.4788 - MinusLogProbMetric: 488.4788 - val_loss: 409.1490 - val_MinusLogProbMetric: 409.1490 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 3/1000
2023-10-27 17:20:43.994 
Epoch 3/1000 
	 loss: 363.8456, MinusLogProbMetric: 363.8456, val_loss: 330.6507, val_MinusLogProbMetric: 330.6507

Epoch 3: val_loss improved from 409.14899 to 330.65073, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 363.8456 - MinusLogProbMetric: 363.8456 - val_loss: 330.6507 - val_MinusLogProbMetric: 330.6507 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 4/1000
2023-10-27 17:21:18.305 
Epoch 4/1000 
	 loss: 306.2014, MinusLogProbMetric: 306.2014, val_loss: 273.5529, val_MinusLogProbMetric: 273.5529

Epoch 4: val_loss improved from 330.65073 to 273.55286, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 306.2014 - MinusLogProbMetric: 306.2014 - val_loss: 273.5529 - val_MinusLogProbMetric: 273.5529 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 5/1000
2023-10-27 17:21:53.339 
Epoch 5/1000 
	 loss: 263.8674, MinusLogProbMetric: 263.8674, val_loss: 226.1513, val_MinusLogProbMetric: 226.1513

Epoch 5: val_loss improved from 273.55286 to 226.15126, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 263.8674 - MinusLogProbMetric: 263.8674 - val_loss: 226.1513 - val_MinusLogProbMetric: 226.1513 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 6/1000
2023-10-27 17:22:28.845 
Epoch 6/1000 
	 loss: 224.4533, MinusLogProbMetric: 224.4533, val_loss: 197.6472, val_MinusLogProbMetric: 197.6472

Epoch 6: val_loss improved from 226.15126 to 197.64725, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 36s - loss: 224.4533 - MinusLogProbMetric: 224.4533 - val_loss: 197.6472 - val_MinusLogProbMetric: 197.6472 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 7/1000
2023-10-27 17:23:04.181 
Epoch 7/1000 
	 loss: 201.5864, MinusLogProbMetric: 201.5864, val_loss: 364.3161, val_MinusLogProbMetric: 364.3161

Epoch 7: val_loss did not improve from 197.64725
196/196 - 35s - loss: 201.5864 - MinusLogProbMetric: 201.5864 - val_loss: 364.3161 - val_MinusLogProbMetric: 364.3161 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 8/1000
2023-10-27 17:23:38.465 
Epoch 8/1000 
	 loss: 224.1925, MinusLogProbMetric: 224.1925, val_loss: 193.6225, val_MinusLogProbMetric: 193.6225

Epoch 8: val_loss improved from 197.64725 to 193.62247, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 224.1925 - MinusLogProbMetric: 224.1925 - val_loss: 193.6225 - val_MinusLogProbMetric: 193.6225 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 9/1000
2023-10-27 17:24:14.224 
Epoch 9/1000 
	 loss: 186.5905, MinusLogProbMetric: 186.5905, val_loss: 176.5732, val_MinusLogProbMetric: 176.5732

Epoch 9: val_loss improved from 193.62247 to 176.57318, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 36s - loss: 186.5905 - MinusLogProbMetric: 186.5905 - val_loss: 176.5732 - val_MinusLogProbMetric: 176.5732 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 10/1000
2023-10-27 17:24:49.086 
Epoch 10/1000 
	 loss: 172.1279, MinusLogProbMetric: 172.1279, val_loss: 169.2531, val_MinusLogProbMetric: 169.2531

Epoch 10: val_loss improved from 176.57318 to 169.25311, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 172.1279 - MinusLogProbMetric: 172.1279 - val_loss: 169.2531 - val_MinusLogProbMetric: 169.2531 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 11/1000
2023-10-27 17:25:23.921 
Epoch 11/1000 
	 loss: 163.0290, MinusLogProbMetric: 163.0290, val_loss: 162.0688, val_MinusLogProbMetric: 162.0688

Epoch 11: val_loss improved from 169.25311 to 162.06880, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 163.0290 - MinusLogProbMetric: 163.0290 - val_loss: 162.0688 - val_MinusLogProbMetric: 162.0688 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 12/1000
2023-10-27 17:25:58.471 
Epoch 12/1000 
	 loss: 155.8563, MinusLogProbMetric: 155.8563, val_loss: 152.7030, val_MinusLogProbMetric: 152.7030

Epoch 12: val_loss improved from 162.06880 to 152.70303, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 155.8563 - MinusLogProbMetric: 155.8563 - val_loss: 152.7030 - val_MinusLogProbMetric: 152.7030 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 13/1000
2023-10-27 17:26:33.616 
Epoch 13/1000 
	 loss: 149.6768, MinusLogProbMetric: 149.6768, val_loss: 147.5211, val_MinusLogProbMetric: 147.5211

Epoch 13: val_loss improved from 152.70303 to 147.52109, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 149.6768 - MinusLogProbMetric: 149.6768 - val_loss: 147.5211 - val_MinusLogProbMetric: 147.5211 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 14/1000
2023-10-27 17:27:08.556 
Epoch 14/1000 
	 loss: 153.9252, MinusLogProbMetric: 153.9252, val_loss: 293.7664, val_MinusLogProbMetric: 293.7664

Epoch 14: val_loss did not improve from 147.52109
196/196 - 35s - loss: 153.9252 - MinusLogProbMetric: 153.9252 - val_loss: 293.7664 - val_MinusLogProbMetric: 293.7664 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 15/1000
2023-10-27 17:27:43.113 
Epoch 15/1000 
	 loss: 136.6247, MinusLogProbMetric: 136.6247, val_loss: 114.3848, val_MinusLogProbMetric: 114.3848

Epoch 15: val_loss improved from 147.52109 to 114.38478, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 136.6247 - MinusLogProbMetric: 136.6247 - val_loss: 114.3848 - val_MinusLogProbMetric: 114.3848 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 16/1000
2023-10-27 17:28:17.672 
Epoch 16/1000 
	 loss: 109.5988, MinusLogProbMetric: 109.5988, val_loss: 105.8334, val_MinusLogProbMetric: 105.8334

Epoch 16: val_loss improved from 114.38478 to 105.83337, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 109.5988 - MinusLogProbMetric: 109.5988 - val_loss: 105.8334 - val_MinusLogProbMetric: 105.8334 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 17/1000
2023-10-27 17:28:52.555 
Epoch 17/1000 
	 loss: 104.0333, MinusLogProbMetric: 104.0333, val_loss: 129.0575, val_MinusLogProbMetric: 129.0575

Epoch 17: val_loss did not improve from 105.83337
196/196 - 34s - loss: 104.0333 - MinusLogProbMetric: 104.0333 - val_loss: 129.0575 - val_MinusLogProbMetric: 129.0575 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 18/1000
2023-10-27 17:29:27.073 
Epoch 18/1000 
	 loss: 101.1380, MinusLogProbMetric: 101.1380, val_loss: 98.7918, val_MinusLogProbMetric: 98.7918

Epoch 18: val_loss improved from 105.83337 to 98.79179, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 101.1380 - MinusLogProbMetric: 101.1380 - val_loss: 98.7918 - val_MinusLogProbMetric: 98.7918 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 19/1000
2023-10-27 17:30:02.317 
Epoch 19/1000 
	 loss: 94.5088, MinusLogProbMetric: 94.5088, val_loss: 93.1516, val_MinusLogProbMetric: 93.1516

Epoch 19: val_loss improved from 98.79179 to 93.15165, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 94.5088 - MinusLogProbMetric: 94.5088 - val_loss: 93.1516 - val_MinusLogProbMetric: 93.1516 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 20/1000
2023-10-27 17:30:37.584 
Epoch 20/1000 
	 loss: 91.2987, MinusLogProbMetric: 91.2987, val_loss: 90.7014, val_MinusLogProbMetric: 90.7014

Epoch 20: val_loss improved from 93.15165 to 90.70141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 91.2987 - MinusLogProbMetric: 91.2987 - val_loss: 90.7014 - val_MinusLogProbMetric: 90.7014 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 21/1000
2023-10-27 17:31:13.300 
Epoch 21/1000 
	 loss: 88.6155, MinusLogProbMetric: 88.6155, val_loss: 87.6589, val_MinusLogProbMetric: 87.6589

Epoch 21: val_loss improved from 90.70141 to 87.65890, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 36s - loss: 88.6155 - MinusLogProbMetric: 88.6155 - val_loss: 87.6589 - val_MinusLogProbMetric: 87.6589 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 22/1000
2023-10-27 17:31:48.195 
Epoch 22/1000 
	 loss: 86.1199, MinusLogProbMetric: 86.1199, val_loss: 86.2623, val_MinusLogProbMetric: 86.2623

Epoch 22: val_loss improved from 87.65890 to 86.26234, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 86.1199 - MinusLogProbMetric: 86.1199 - val_loss: 86.2623 - val_MinusLogProbMetric: 86.2623 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 23/1000
2023-10-27 17:32:23.205 
Epoch 23/1000 
	 loss: 87.3254, MinusLogProbMetric: 87.3254, val_loss: 86.2621, val_MinusLogProbMetric: 86.2621

Epoch 23: val_loss improved from 86.26234 to 86.26206, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 87.3254 - MinusLogProbMetric: 87.3254 - val_loss: 86.2621 - val_MinusLogProbMetric: 86.2621 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 24/1000
2023-10-27 17:32:58.986 
Epoch 24/1000 
	 loss: 83.0031, MinusLogProbMetric: 83.0031, val_loss: 82.9926, val_MinusLogProbMetric: 82.9926

Epoch 24: val_loss improved from 86.26206 to 82.99261, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 36s - loss: 83.0031 - MinusLogProbMetric: 83.0031 - val_loss: 82.9926 - val_MinusLogProbMetric: 82.9926 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 25/1000
2023-10-27 17:33:34.484 
Epoch 25/1000 
	 loss: 80.8937, MinusLogProbMetric: 80.8937, val_loss: 80.6443, val_MinusLogProbMetric: 80.6443

Epoch 25: val_loss improved from 82.99261 to 80.64433, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 36s - loss: 80.8937 - MinusLogProbMetric: 80.8937 - val_loss: 80.6443 - val_MinusLogProbMetric: 80.6443 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 26/1000
2023-10-27 17:34:10.221 
Epoch 26/1000 
	 loss: 79.2718, MinusLogProbMetric: 79.2718, val_loss: 78.7638, val_MinusLogProbMetric: 78.7638

Epoch 26: val_loss improved from 80.64433 to 78.76379, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 36s - loss: 79.2718 - MinusLogProbMetric: 79.2718 - val_loss: 78.7638 - val_MinusLogProbMetric: 78.7638 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 27/1000
2023-10-27 17:34:46.228 
Epoch 27/1000 
	 loss: 77.7718, MinusLogProbMetric: 77.7718, val_loss: 77.3278, val_MinusLogProbMetric: 77.3278

Epoch 27: val_loss improved from 78.76379 to 77.32777, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 36s - loss: 77.7718 - MinusLogProbMetric: 77.7718 - val_loss: 77.3278 - val_MinusLogProbMetric: 77.3278 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 28/1000
2023-10-27 17:35:21.260 
Epoch 28/1000 
	 loss: 76.3733, MinusLogProbMetric: 76.3733, val_loss: 76.4815, val_MinusLogProbMetric: 76.4815

Epoch 28: val_loss improved from 77.32777 to 76.48148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 76.3733 - MinusLogProbMetric: 76.3733 - val_loss: 76.4815 - val_MinusLogProbMetric: 76.4815 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 29/1000
2023-10-27 17:35:57.372 
Epoch 29/1000 
	 loss: 74.9457, MinusLogProbMetric: 74.9457, val_loss: 75.3060, val_MinusLogProbMetric: 75.3060

Epoch 29: val_loss improved from 76.48148 to 75.30596, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 36s - loss: 74.9457 - MinusLogProbMetric: 74.9457 - val_loss: 75.3060 - val_MinusLogProbMetric: 75.3060 - lr: 3.3333e-04 - 36s/epoch - 184ms/step
Epoch 30/1000
2023-10-27 17:36:31.873 
Epoch 30/1000 
	 loss: 73.7220, MinusLogProbMetric: 73.7220, val_loss: 73.8049, val_MinusLogProbMetric: 73.8049

Epoch 30: val_loss improved from 75.30596 to 73.80486, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 73.7220 - MinusLogProbMetric: 73.7220 - val_loss: 73.8049 - val_MinusLogProbMetric: 73.8049 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 31/1000
2023-10-27 17:37:06.547 
Epoch 31/1000 
	 loss: 72.6584, MinusLogProbMetric: 72.6584, val_loss: 73.4113, val_MinusLogProbMetric: 73.4113

Epoch 31: val_loss improved from 73.80486 to 73.41130, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 72.6584 - MinusLogProbMetric: 72.6584 - val_loss: 73.4113 - val_MinusLogProbMetric: 73.4113 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 32/1000
2023-10-27 17:37:40.762 
Epoch 32/1000 
	 loss: 71.8692, MinusLogProbMetric: 71.8692, val_loss: 71.7553, val_MinusLogProbMetric: 71.7553

Epoch 32: val_loss improved from 73.41130 to 71.75530, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 71.8692 - MinusLogProbMetric: 71.8692 - val_loss: 71.7553 - val_MinusLogProbMetric: 71.7553 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 33/1000
2023-10-27 17:38:15.342 
Epoch 33/1000 
	 loss: 70.7999, MinusLogProbMetric: 70.7999, val_loss: 70.7119, val_MinusLogProbMetric: 70.7119

Epoch 33: val_loss improved from 71.75530 to 70.71187, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 70.7999 - MinusLogProbMetric: 70.7999 - val_loss: 70.7119 - val_MinusLogProbMetric: 70.7119 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 34/1000
2023-10-27 17:38:46.399 
Epoch 34/1000 
	 loss: 69.8797, MinusLogProbMetric: 69.8797, val_loss: 69.7582, val_MinusLogProbMetric: 69.7582

Epoch 34: val_loss improved from 70.71187 to 69.75816, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 31s - loss: 69.8797 - MinusLogProbMetric: 69.8797 - val_loss: 69.7582 - val_MinusLogProbMetric: 69.7582 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 35/1000
2023-10-27 17:39:16.428 
Epoch 35/1000 
	 loss: 69.1152, MinusLogProbMetric: 69.1152, val_loss: 68.7684, val_MinusLogProbMetric: 68.7684

Epoch 35: val_loss improved from 69.75816 to 68.76838, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 30s - loss: 69.1152 - MinusLogProbMetric: 69.1152 - val_loss: 68.7684 - val_MinusLogProbMetric: 68.7684 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 36/1000
2023-10-27 17:39:46.277 
Epoch 36/1000 
	 loss: 68.1566, MinusLogProbMetric: 68.1566, val_loss: 68.5264, val_MinusLogProbMetric: 68.5264

Epoch 36: val_loss improved from 68.76838 to 68.52641, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 30s - loss: 68.1566 - MinusLogProbMetric: 68.1566 - val_loss: 68.5264 - val_MinusLogProbMetric: 68.5264 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 37/1000
2023-10-27 17:40:16.363 
Epoch 37/1000 
	 loss: 67.2908, MinusLogProbMetric: 67.2908, val_loss: 67.7070, val_MinusLogProbMetric: 67.7070

Epoch 37: val_loss improved from 68.52641 to 67.70698, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 30s - loss: 67.2908 - MinusLogProbMetric: 67.2908 - val_loss: 67.7070 - val_MinusLogProbMetric: 67.7070 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 38/1000
2023-10-27 17:40:48.443 
Epoch 38/1000 
	 loss: 66.8683, MinusLogProbMetric: 66.8683, val_loss: 66.8978, val_MinusLogProbMetric: 66.8978

Epoch 38: val_loss improved from 67.70698 to 66.89780, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 32s - loss: 66.8683 - MinusLogProbMetric: 66.8683 - val_loss: 66.8978 - val_MinusLogProbMetric: 66.8978 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 39/1000
2023-10-27 17:41:23.203 
Epoch 39/1000 
	 loss: 66.2855, MinusLogProbMetric: 66.2855, val_loss: 66.2889, val_MinusLogProbMetric: 66.2889

Epoch 39: val_loss improved from 66.89780 to 66.28893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 66.2855 - MinusLogProbMetric: 66.2855 - val_loss: 66.2889 - val_MinusLogProbMetric: 66.2889 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 40/1000
2023-10-27 17:41:57.430 
Epoch 40/1000 
	 loss: 65.5859, MinusLogProbMetric: 65.5859, val_loss: 65.2672, val_MinusLogProbMetric: 65.2672

Epoch 40: val_loss improved from 66.28893 to 65.26720, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 65.5859 - MinusLogProbMetric: 65.5859 - val_loss: 65.2672 - val_MinusLogProbMetric: 65.2672 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 41/1000
2023-10-27 17:42:31.974 
Epoch 41/1000 
	 loss: 64.7227, MinusLogProbMetric: 64.7227, val_loss: 65.7332, val_MinusLogProbMetric: 65.7332

Epoch 41: val_loss did not improve from 65.26720
196/196 - 34s - loss: 64.7227 - MinusLogProbMetric: 64.7227 - val_loss: 65.7332 - val_MinusLogProbMetric: 65.7332 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 42/1000
2023-10-27 17:43:06.055 
Epoch 42/1000 
	 loss: 64.3050, MinusLogProbMetric: 64.3050, val_loss: 65.2534, val_MinusLogProbMetric: 65.2534

Epoch 42: val_loss improved from 65.26720 to 65.25336, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 64.3050 - MinusLogProbMetric: 64.3050 - val_loss: 65.2534 - val_MinusLogProbMetric: 65.2534 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 43/1000
2023-10-27 17:43:40.969 
Epoch 43/1000 
	 loss: 63.8373, MinusLogProbMetric: 63.8373, val_loss: 64.7516, val_MinusLogProbMetric: 64.7516

Epoch 43: val_loss improved from 65.25336 to 64.75161, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 63.8373 - MinusLogProbMetric: 63.8373 - val_loss: 64.7516 - val_MinusLogProbMetric: 64.7516 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 44/1000
2023-10-27 17:44:15.305 
Epoch 44/1000 
	 loss: 63.3219, MinusLogProbMetric: 63.3219, val_loss: 62.8667, val_MinusLogProbMetric: 62.8667

Epoch 44: val_loss improved from 64.75161 to 62.86673, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 63.3219 - MinusLogProbMetric: 63.3219 - val_loss: 62.8667 - val_MinusLogProbMetric: 62.8667 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 45/1000
2023-10-27 17:44:49.943 
Epoch 45/1000 
	 loss: 62.6651, MinusLogProbMetric: 62.6651, val_loss: 62.7882, val_MinusLogProbMetric: 62.7882

Epoch 45: val_loss improved from 62.86673 to 62.78819, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 62.6651 - MinusLogProbMetric: 62.6651 - val_loss: 62.7882 - val_MinusLogProbMetric: 62.7882 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 46/1000
2023-10-27 17:45:25.813 
Epoch 46/1000 
	 loss: 62.6706, MinusLogProbMetric: 62.6706, val_loss: 63.7982, val_MinusLogProbMetric: 63.7982

Epoch 46: val_loss did not improve from 62.78819
196/196 - 35s - loss: 62.6706 - MinusLogProbMetric: 62.6706 - val_loss: 63.7982 - val_MinusLogProbMetric: 63.7982 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 47/1000
2023-10-27 17:46:00.093 
Epoch 47/1000 
	 loss: 61.6856, MinusLogProbMetric: 61.6856, val_loss: 61.9485, val_MinusLogProbMetric: 61.9485

Epoch 47: val_loss improved from 62.78819 to 61.94855, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 61.6856 - MinusLogProbMetric: 61.6856 - val_loss: 61.9485 - val_MinusLogProbMetric: 61.9485 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 48/1000
2023-10-27 17:46:34.252 
Epoch 48/1000 
	 loss: 62.3211, MinusLogProbMetric: 62.3211, val_loss: 61.7966, val_MinusLogProbMetric: 61.7966

Epoch 48: val_loss improved from 61.94855 to 61.79662, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 62.3211 - MinusLogProbMetric: 62.3211 - val_loss: 61.7966 - val_MinusLogProbMetric: 61.7966 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 49/1000
2023-10-27 17:47:09.084 
Epoch 49/1000 
	 loss: 60.8163, MinusLogProbMetric: 60.8163, val_loss: 61.9267, val_MinusLogProbMetric: 61.9267

Epoch 49: val_loss did not improve from 61.79662
196/196 - 34s - loss: 60.8163 - MinusLogProbMetric: 60.8163 - val_loss: 61.9267 - val_MinusLogProbMetric: 61.9267 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 50/1000
2023-10-27 17:47:42.976 
Epoch 50/1000 
	 loss: 60.8775, MinusLogProbMetric: 60.8775, val_loss: 60.5065, val_MinusLogProbMetric: 60.5065

Epoch 50: val_loss improved from 61.79662 to 60.50654, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 60.8775 - MinusLogProbMetric: 60.8775 - val_loss: 60.5065 - val_MinusLogProbMetric: 60.5065 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 51/1000
2023-10-27 17:48:13.505 
Epoch 51/1000 
	 loss: 60.1180, MinusLogProbMetric: 60.1180, val_loss: 61.4831, val_MinusLogProbMetric: 61.4831

Epoch 51: val_loss did not improve from 60.50654
196/196 - 30s - loss: 60.1180 - MinusLogProbMetric: 60.1180 - val_loss: 61.4831 - val_MinusLogProbMetric: 61.4831 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 52/1000
2023-10-27 17:48:40.915 
Epoch 52/1000 
	 loss: 59.9697, MinusLogProbMetric: 59.9697, val_loss: 59.8711, val_MinusLogProbMetric: 59.8711

Epoch 52: val_loss improved from 60.50654 to 59.87108, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 28s - loss: 59.9697 - MinusLogProbMetric: 59.9697 - val_loss: 59.8711 - val_MinusLogProbMetric: 59.8711 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 53/1000
2023-10-27 17:49:09.693 
Epoch 53/1000 
	 loss: 59.5101, MinusLogProbMetric: 59.5101, val_loss: 59.7006, val_MinusLogProbMetric: 59.7006

Epoch 53: val_loss improved from 59.87108 to 59.70061, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 29s - loss: 59.5101 - MinusLogProbMetric: 59.5101 - val_loss: 59.7006 - val_MinusLogProbMetric: 59.7006 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 54/1000
2023-10-27 17:49:37.472 
Epoch 54/1000 
	 loss: 59.6514, MinusLogProbMetric: 59.6514, val_loss: 60.0431, val_MinusLogProbMetric: 60.0431

Epoch 54: val_loss did not improve from 59.70061
196/196 - 27s - loss: 59.6514 - MinusLogProbMetric: 59.6514 - val_loss: 60.0431 - val_MinusLogProbMetric: 60.0431 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 55/1000
2023-10-27 17:50:08.015 
Epoch 55/1000 
	 loss: 59.1457, MinusLogProbMetric: 59.1457, val_loss: 60.0552, val_MinusLogProbMetric: 60.0552

Epoch 55: val_loss did not improve from 59.70061
196/196 - 31s - loss: 59.1457 - MinusLogProbMetric: 59.1457 - val_loss: 60.0552 - val_MinusLogProbMetric: 60.0552 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 56/1000
2023-10-27 17:50:41.805 
Epoch 56/1000 
	 loss: 58.5306, MinusLogProbMetric: 58.5306, val_loss: 59.6845, val_MinusLogProbMetric: 59.6845

Epoch 56: val_loss improved from 59.70061 to 59.68449, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 58.5306 - MinusLogProbMetric: 58.5306 - val_loss: 59.6845 - val_MinusLogProbMetric: 59.6845 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 57/1000
2023-10-27 17:51:16.104 
Epoch 57/1000 
	 loss: 58.2231, MinusLogProbMetric: 58.2231, val_loss: 58.3434, val_MinusLogProbMetric: 58.3434

Epoch 57: val_loss improved from 59.68449 to 58.34343, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 58.2231 - MinusLogProbMetric: 58.2231 - val_loss: 58.3434 - val_MinusLogProbMetric: 58.3434 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 58/1000
2023-10-27 17:51:50.396 
Epoch 58/1000 
	 loss: 58.5076, MinusLogProbMetric: 58.5076, val_loss: 59.6852, val_MinusLogProbMetric: 59.6852

Epoch 58: val_loss did not improve from 58.34343
196/196 - 34s - loss: 58.5076 - MinusLogProbMetric: 58.5076 - val_loss: 59.6852 - val_MinusLogProbMetric: 59.6852 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 59/1000
2023-10-27 17:52:24.327 
Epoch 59/1000 
	 loss: 57.9878, MinusLogProbMetric: 57.9878, val_loss: 58.6831, val_MinusLogProbMetric: 58.6831

Epoch 59: val_loss did not improve from 58.34343
196/196 - 34s - loss: 57.9878 - MinusLogProbMetric: 57.9878 - val_loss: 58.6831 - val_MinusLogProbMetric: 58.6831 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 60/1000
2023-10-27 17:52:58.278 
Epoch 60/1000 
	 loss: 57.9435, MinusLogProbMetric: 57.9435, val_loss: 57.0567, val_MinusLogProbMetric: 57.0567

Epoch 60: val_loss improved from 58.34343 to 57.05669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 57.9435 - MinusLogProbMetric: 57.9435 - val_loss: 57.0567 - val_MinusLogProbMetric: 57.0567 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 61/1000
2023-10-27 17:53:32.917 
Epoch 61/1000 
	 loss: 58.2555, MinusLogProbMetric: 58.2555, val_loss: 58.6759, val_MinusLogProbMetric: 58.6759

Epoch 61: val_loss did not improve from 57.05669
196/196 - 34s - loss: 58.2555 - MinusLogProbMetric: 58.2555 - val_loss: 58.6759 - val_MinusLogProbMetric: 58.6759 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 62/1000
2023-10-27 17:54:06.900 
Epoch 62/1000 
	 loss: 57.0080, MinusLogProbMetric: 57.0080, val_loss: 56.5318, val_MinusLogProbMetric: 56.5318

Epoch 62: val_loss improved from 57.05669 to 56.53178, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 57.0080 - MinusLogProbMetric: 57.0080 - val_loss: 56.5318 - val_MinusLogProbMetric: 56.5318 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 63/1000
2023-10-27 17:54:41.444 
Epoch 63/1000 
	 loss: 57.0843, MinusLogProbMetric: 57.0843, val_loss: 56.8033, val_MinusLogProbMetric: 56.8033

Epoch 63: val_loss did not improve from 56.53178
196/196 - 34s - loss: 57.0843 - MinusLogProbMetric: 57.0843 - val_loss: 56.8033 - val_MinusLogProbMetric: 56.8033 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 64/1000
2023-10-27 17:55:16.241 
Epoch 64/1000 
	 loss: 56.7129, MinusLogProbMetric: 56.7129, val_loss: 57.2243, val_MinusLogProbMetric: 57.2243

Epoch 64: val_loss did not improve from 56.53178
196/196 - 35s - loss: 56.7129 - MinusLogProbMetric: 56.7129 - val_loss: 57.2243 - val_MinusLogProbMetric: 57.2243 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 65/1000
2023-10-27 17:55:49.572 
Epoch 65/1000 
	 loss: 56.9056, MinusLogProbMetric: 56.9056, val_loss: 57.7291, val_MinusLogProbMetric: 57.7291

Epoch 65: val_loss did not improve from 56.53178
196/196 - 33s - loss: 56.9056 - MinusLogProbMetric: 56.9056 - val_loss: 57.7291 - val_MinusLogProbMetric: 57.7291 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 66/1000
2023-10-27 17:56:24.350 
Epoch 66/1000 
	 loss: 56.1731, MinusLogProbMetric: 56.1731, val_loss: 56.2667, val_MinusLogProbMetric: 56.2667

Epoch 66: val_loss improved from 56.53178 to 56.26667, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 56.1731 - MinusLogProbMetric: 56.1731 - val_loss: 56.2667 - val_MinusLogProbMetric: 56.2667 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 67/1000
2023-10-27 17:56:58.910 
Epoch 67/1000 
	 loss: 56.6547, MinusLogProbMetric: 56.6547, val_loss: 55.4590, val_MinusLogProbMetric: 55.4590

Epoch 67: val_loss improved from 56.26667 to 55.45896, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 56.6547 - MinusLogProbMetric: 56.6547 - val_loss: 55.4590 - val_MinusLogProbMetric: 55.4590 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 68/1000
2023-10-27 17:57:34.240 
Epoch 68/1000 
	 loss: 56.3571, MinusLogProbMetric: 56.3571, val_loss: 59.9120, val_MinusLogProbMetric: 59.9120

Epoch 68: val_loss did not improve from 55.45896
196/196 - 35s - loss: 56.3571 - MinusLogProbMetric: 56.3571 - val_loss: 59.9120 - val_MinusLogProbMetric: 59.9120 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 69/1000
2023-10-27 17:58:09.683 
Epoch 69/1000 
	 loss: 56.1380, MinusLogProbMetric: 56.1380, val_loss: 56.5783, val_MinusLogProbMetric: 56.5783

Epoch 69: val_loss did not improve from 55.45896
196/196 - 35s - loss: 56.1380 - MinusLogProbMetric: 56.1380 - val_loss: 56.5783 - val_MinusLogProbMetric: 56.5783 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 70/1000
2023-10-27 17:58:45.166 
Epoch 70/1000 
	 loss: 55.7244, MinusLogProbMetric: 55.7244, val_loss: 55.7724, val_MinusLogProbMetric: 55.7724

Epoch 70: val_loss did not improve from 55.45896
196/196 - 35s - loss: 55.7244 - MinusLogProbMetric: 55.7244 - val_loss: 55.7724 - val_MinusLogProbMetric: 55.7724 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 71/1000
2023-10-27 17:59:21.032 
Epoch 71/1000 
	 loss: 56.0282, MinusLogProbMetric: 56.0282, val_loss: 55.3813, val_MinusLogProbMetric: 55.3813

Epoch 71: val_loss improved from 55.45896 to 55.38127, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 36s - loss: 56.0282 - MinusLogProbMetric: 56.0282 - val_loss: 55.3813 - val_MinusLogProbMetric: 55.3813 - lr: 3.3333e-04 - 36s/epoch - 186ms/step
Epoch 72/1000
2023-10-27 17:59:55.885 
Epoch 72/1000 
	 loss: 55.3234, MinusLogProbMetric: 55.3234, val_loss: 55.3568, val_MinusLogProbMetric: 55.3568

Epoch 72: val_loss improved from 55.38127 to 55.35675, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 55.3234 - MinusLogProbMetric: 55.3234 - val_loss: 55.3568 - val_MinusLogProbMetric: 55.3568 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 73/1000
2023-10-27 18:00:30.565 
Epoch 73/1000 
	 loss: 97.5507, MinusLogProbMetric: 97.5507, val_loss: 166.6638, val_MinusLogProbMetric: 166.6638

Epoch 73: val_loss did not improve from 55.35675
196/196 - 34s - loss: 97.5507 - MinusLogProbMetric: 97.5507 - val_loss: 166.6638 - val_MinusLogProbMetric: 166.6638 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 74/1000
2023-10-27 18:01:04.833 
Epoch 74/1000 
	 loss: 99.4499, MinusLogProbMetric: 99.4499, val_loss: 81.2262, val_MinusLogProbMetric: 81.2262

Epoch 74: val_loss did not improve from 55.35675
196/196 - 34s - loss: 99.4499 - MinusLogProbMetric: 99.4499 - val_loss: 81.2262 - val_MinusLogProbMetric: 81.2262 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 75/1000
2023-10-27 18:01:39.747 
Epoch 75/1000 
	 loss: 74.2557, MinusLogProbMetric: 74.2557, val_loss: 69.9872, val_MinusLogProbMetric: 69.9872

Epoch 75: val_loss did not improve from 55.35675
196/196 - 35s - loss: 74.2557 - MinusLogProbMetric: 74.2557 - val_loss: 69.9872 - val_MinusLogProbMetric: 69.9872 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 76/1000
2023-10-27 18:02:14.741 
Epoch 76/1000 
	 loss: 66.8394, MinusLogProbMetric: 66.8394, val_loss: 65.1768, val_MinusLogProbMetric: 65.1768

Epoch 76: val_loss did not improve from 55.35675
196/196 - 35s - loss: 66.8394 - MinusLogProbMetric: 66.8394 - val_loss: 65.1768 - val_MinusLogProbMetric: 65.1768 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 77/1000
2023-10-27 18:02:49.065 
Epoch 77/1000 
	 loss: 63.0519, MinusLogProbMetric: 63.0519, val_loss: 62.6116, val_MinusLogProbMetric: 62.6116

Epoch 77: val_loss did not improve from 55.35675
196/196 - 34s - loss: 63.0519 - MinusLogProbMetric: 63.0519 - val_loss: 62.6116 - val_MinusLogProbMetric: 62.6116 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 78/1000
2023-10-27 18:03:23.018 
Epoch 78/1000 
	 loss: 60.8117, MinusLogProbMetric: 60.8117, val_loss: 60.1290, val_MinusLogProbMetric: 60.1290

Epoch 78: val_loss did not improve from 55.35675
196/196 - 34s - loss: 60.8117 - MinusLogProbMetric: 60.8117 - val_loss: 60.1290 - val_MinusLogProbMetric: 60.1290 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 79/1000
2023-10-27 18:03:57.914 
Epoch 79/1000 
	 loss: 59.8972, MinusLogProbMetric: 59.8972, val_loss: 59.5963, val_MinusLogProbMetric: 59.5963

Epoch 79: val_loss did not improve from 55.35675
196/196 - 35s - loss: 59.8972 - MinusLogProbMetric: 59.8972 - val_loss: 59.5963 - val_MinusLogProbMetric: 59.5963 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 80/1000
2023-10-27 18:04:32.315 
Epoch 80/1000 
	 loss: 58.4916, MinusLogProbMetric: 58.4916, val_loss: 58.6065, val_MinusLogProbMetric: 58.6065

Epoch 80: val_loss did not improve from 55.35675
196/196 - 34s - loss: 58.4916 - MinusLogProbMetric: 58.4916 - val_loss: 58.6065 - val_MinusLogProbMetric: 58.6065 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 81/1000
2023-10-27 18:05:05.774 
Epoch 81/1000 
	 loss: 57.6793, MinusLogProbMetric: 57.6793, val_loss: 57.5529, val_MinusLogProbMetric: 57.5529

Epoch 81: val_loss did not improve from 55.35675
196/196 - 33s - loss: 57.6793 - MinusLogProbMetric: 57.6793 - val_loss: 57.5529 - val_MinusLogProbMetric: 57.5529 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 82/1000
2023-10-27 18:05:39.457 
Epoch 82/1000 
	 loss: 57.0185, MinusLogProbMetric: 57.0185, val_loss: 57.2619, val_MinusLogProbMetric: 57.2619

Epoch 82: val_loss did not improve from 55.35675
196/196 - 34s - loss: 57.0185 - MinusLogProbMetric: 57.0185 - val_loss: 57.2619 - val_MinusLogProbMetric: 57.2619 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 83/1000
2023-10-27 18:06:13.616 
Epoch 83/1000 
	 loss: 56.6034, MinusLogProbMetric: 56.6034, val_loss: 57.0963, val_MinusLogProbMetric: 57.0963

Epoch 83: val_loss did not improve from 55.35675
196/196 - 34s - loss: 56.6034 - MinusLogProbMetric: 56.6034 - val_loss: 57.0963 - val_MinusLogProbMetric: 57.0963 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 84/1000
2023-10-27 18:06:49.006 
Epoch 84/1000 
	 loss: 56.2511, MinusLogProbMetric: 56.2511, val_loss: 55.7706, val_MinusLogProbMetric: 55.7706

Epoch 84: val_loss did not improve from 55.35675
196/196 - 35s - loss: 56.2511 - MinusLogProbMetric: 56.2511 - val_loss: 55.7706 - val_MinusLogProbMetric: 55.7706 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 85/1000
2023-10-27 18:07:24.168 
Epoch 85/1000 
	 loss: 55.7773, MinusLogProbMetric: 55.7773, val_loss: 55.7296, val_MinusLogProbMetric: 55.7296

Epoch 85: val_loss did not improve from 55.35675
196/196 - 35s - loss: 55.7773 - MinusLogProbMetric: 55.7773 - val_loss: 55.7296 - val_MinusLogProbMetric: 55.7296 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 86/1000
2023-10-27 18:07:57.805 
Epoch 86/1000 
	 loss: 56.7189, MinusLogProbMetric: 56.7189, val_loss: 56.6555, val_MinusLogProbMetric: 56.6555

Epoch 86: val_loss did not improve from 55.35675
196/196 - 34s - loss: 56.7189 - MinusLogProbMetric: 56.7189 - val_loss: 56.6555 - val_MinusLogProbMetric: 56.6555 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 87/1000
2023-10-27 18:08:32.042 
Epoch 87/1000 
	 loss: 55.0135, MinusLogProbMetric: 55.0135, val_loss: 54.6899, val_MinusLogProbMetric: 54.6899

Epoch 87: val_loss improved from 55.35675 to 54.68989, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 55.0135 - MinusLogProbMetric: 55.0135 - val_loss: 54.6899 - val_MinusLogProbMetric: 54.6899 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 88/1000
2023-10-27 18:09:06.591 
Epoch 88/1000 
	 loss: 54.7755, MinusLogProbMetric: 54.7755, val_loss: 54.2571, val_MinusLogProbMetric: 54.2571

Epoch 88: val_loss improved from 54.68989 to 54.25708, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 54.7755 - MinusLogProbMetric: 54.7755 - val_loss: 54.2571 - val_MinusLogProbMetric: 54.2571 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 89/1000
2023-10-27 18:09:40.915 
Epoch 89/1000 
	 loss: 57.3299, MinusLogProbMetric: 57.3299, val_loss: 55.8259, val_MinusLogProbMetric: 55.8259

Epoch 89: val_loss did not improve from 54.25708
196/196 - 34s - loss: 57.3299 - MinusLogProbMetric: 57.3299 - val_loss: 55.8259 - val_MinusLogProbMetric: 55.8259 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 90/1000
2023-10-27 18:10:14.758 
Epoch 90/1000 
	 loss: 55.3623, MinusLogProbMetric: 55.3623, val_loss: 54.9913, val_MinusLogProbMetric: 54.9913

Epoch 90: val_loss did not improve from 54.25708
196/196 - 34s - loss: 55.3623 - MinusLogProbMetric: 55.3623 - val_loss: 54.9913 - val_MinusLogProbMetric: 54.9913 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 91/1000
2023-10-27 18:10:48.650 
Epoch 91/1000 
	 loss: 54.7941, MinusLogProbMetric: 54.7941, val_loss: 54.8612, val_MinusLogProbMetric: 54.8612

Epoch 91: val_loss did not improve from 54.25708
196/196 - 34s - loss: 54.7941 - MinusLogProbMetric: 54.7941 - val_loss: 54.8612 - val_MinusLogProbMetric: 54.8612 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 92/1000
2023-10-27 18:11:22.663 
Epoch 92/1000 
	 loss: 54.6197, MinusLogProbMetric: 54.6197, val_loss: 54.8637, val_MinusLogProbMetric: 54.8637

Epoch 92: val_loss did not improve from 54.25708
196/196 - 34s - loss: 54.6197 - MinusLogProbMetric: 54.6197 - val_loss: 54.8637 - val_MinusLogProbMetric: 54.8637 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 93/1000
2023-10-27 18:11:56.351 
Epoch 93/1000 
	 loss: 54.3508, MinusLogProbMetric: 54.3508, val_loss: 56.6214, val_MinusLogProbMetric: 56.6214

Epoch 93: val_loss did not improve from 54.25708
196/196 - 34s - loss: 54.3508 - MinusLogProbMetric: 54.3508 - val_loss: 56.6214 - val_MinusLogProbMetric: 56.6214 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 94/1000
2023-10-27 18:12:30.297 
Epoch 94/1000 
	 loss: 54.2844, MinusLogProbMetric: 54.2844, val_loss: 54.9096, val_MinusLogProbMetric: 54.9096

Epoch 94: val_loss did not improve from 54.25708
196/196 - 34s - loss: 54.2844 - MinusLogProbMetric: 54.2844 - val_loss: 54.9096 - val_MinusLogProbMetric: 54.9096 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 95/1000
2023-10-27 18:13:04.147 
Epoch 95/1000 
	 loss: 53.8537, MinusLogProbMetric: 53.8537, val_loss: 54.4854, val_MinusLogProbMetric: 54.4854

Epoch 95: val_loss did not improve from 54.25708
196/196 - 34s - loss: 53.8537 - MinusLogProbMetric: 53.8537 - val_loss: 54.4854 - val_MinusLogProbMetric: 54.4854 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 96/1000
2023-10-27 18:13:37.969 
Epoch 96/1000 
	 loss: 54.0075, MinusLogProbMetric: 54.0075, val_loss: 53.9579, val_MinusLogProbMetric: 53.9579

Epoch 96: val_loss improved from 54.25708 to 53.95788, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 54.0075 - MinusLogProbMetric: 54.0075 - val_loss: 53.9579 - val_MinusLogProbMetric: 53.9579 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 97/1000
2023-10-27 18:14:13.020 
Epoch 97/1000 
	 loss: 53.4473, MinusLogProbMetric: 53.4473, val_loss: 53.5622, val_MinusLogProbMetric: 53.5622

Epoch 97: val_loss improved from 53.95788 to 53.56215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 53.4473 - MinusLogProbMetric: 53.4473 - val_loss: 53.5622 - val_MinusLogProbMetric: 53.5622 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 98/1000
2023-10-27 18:14:47.689 
Epoch 98/1000 
	 loss: 53.3944, MinusLogProbMetric: 53.3944, val_loss: 52.8658, val_MinusLogProbMetric: 52.8658

Epoch 98: val_loss improved from 53.56215 to 52.86578, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 53.3944 - MinusLogProbMetric: 53.3944 - val_loss: 52.8658 - val_MinusLogProbMetric: 52.8658 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 99/1000
2023-10-27 18:15:22.788 
Epoch 99/1000 
	 loss: 53.1068, MinusLogProbMetric: 53.1068, val_loss: 53.1251, val_MinusLogProbMetric: 53.1251

Epoch 99: val_loss did not improve from 52.86578
196/196 - 34s - loss: 53.1068 - MinusLogProbMetric: 53.1068 - val_loss: 53.1251 - val_MinusLogProbMetric: 53.1251 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 100/1000
2023-10-27 18:15:56.533 
Epoch 100/1000 
	 loss: 53.1028, MinusLogProbMetric: 53.1028, val_loss: 53.1585, val_MinusLogProbMetric: 53.1585

Epoch 100: val_loss did not improve from 52.86578
196/196 - 34s - loss: 53.1028 - MinusLogProbMetric: 53.1028 - val_loss: 53.1585 - val_MinusLogProbMetric: 53.1585 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 101/1000
2023-10-27 18:16:30.300 
Epoch 101/1000 
	 loss: 52.8742, MinusLogProbMetric: 52.8742, val_loss: 52.8780, val_MinusLogProbMetric: 52.8780

Epoch 101: val_loss did not improve from 52.86578
196/196 - 34s - loss: 52.8742 - MinusLogProbMetric: 52.8742 - val_loss: 52.8780 - val_MinusLogProbMetric: 52.8780 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 102/1000
2023-10-27 18:17:04.589 
Epoch 102/1000 
	 loss: 52.8855, MinusLogProbMetric: 52.8855, val_loss: 52.5730, val_MinusLogProbMetric: 52.5730

Epoch 102: val_loss improved from 52.86578 to 52.57300, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 52.8855 - MinusLogProbMetric: 52.8855 - val_loss: 52.5730 - val_MinusLogProbMetric: 52.5730 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 103/1000
2023-10-27 18:17:39.372 
Epoch 103/1000 
	 loss: 52.9026, MinusLogProbMetric: 52.9026, val_loss: 54.2880, val_MinusLogProbMetric: 54.2880

Epoch 103: val_loss did not improve from 52.57300
196/196 - 34s - loss: 52.9026 - MinusLogProbMetric: 52.9026 - val_loss: 54.2880 - val_MinusLogProbMetric: 54.2880 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 104/1000
2023-10-27 18:18:13.391 
Epoch 104/1000 
	 loss: 52.6898, MinusLogProbMetric: 52.6898, val_loss: 53.2593, val_MinusLogProbMetric: 53.2593

Epoch 104: val_loss did not improve from 52.57300
196/196 - 34s - loss: 52.6898 - MinusLogProbMetric: 52.6898 - val_loss: 53.2593 - val_MinusLogProbMetric: 53.2593 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 105/1000
2023-10-27 18:18:47.381 
Epoch 105/1000 
	 loss: 52.2666, MinusLogProbMetric: 52.2666, val_loss: 52.8445, val_MinusLogProbMetric: 52.8445

Epoch 105: val_loss did not improve from 52.57300
196/196 - 34s - loss: 52.2666 - MinusLogProbMetric: 52.2666 - val_loss: 52.8445 - val_MinusLogProbMetric: 52.8445 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 106/1000
2023-10-27 18:19:20.743 
Epoch 106/1000 
	 loss: 52.7352, MinusLogProbMetric: 52.7352, val_loss: 58.9412, val_MinusLogProbMetric: 58.9412

Epoch 106: val_loss did not improve from 52.57300
196/196 - 33s - loss: 52.7352 - MinusLogProbMetric: 52.7352 - val_loss: 58.9412 - val_MinusLogProbMetric: 58.9412 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 107/1000
2023-10-27 18:19:54.767 
Epoch 107/1000 
	 loss: 52.8419, MinusLogProbMetric: 52.8419, val_loss: 54.4884, val_MinusLogProbMetric: 54.4884

Epoch 107: val_loss did not improve from 52.57300
196/196 - 34s - loss: 52.8419 - MinusLogProbMetric: 52.8419 - val_loss: 54.4884 - val_MinusLogProbMetric: 54.4884 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 108/1000
2023-10-27 18:20:28.598 
Epoch 108/1000 
	 loss: 52.1581, MinusLogProbMetric: 52.1581, val_loss: 53.3028, val_MinusLogProbMetric: 53.3028

Epoch 108: val_loss did not improve from 52.57300
196/196 - 34s - loss: 52.1581 - MinusLogProbMetric: 52.1581 - val_loss: 53.3028 - val_MinusLogProbMetric: 53.3028 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 109/1000
2023-10-27 18:21:01.813 
Epoch 109/1000 
	 loss: 52.3187, MinusLogProbMetric: 52.3187, val_loss: 51.7643, val_MinusLogProbMetric: 51.7643

Epoch 109: val_loss improved from 52.57300 to 51.76429, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 52.3187 - MinusLogProbMetric: 52.3187 - val_loss: 51.7643 - val_MinusLogProbMetric: 51.7643 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 110/1000
2023-10-27 18:21:36.281 
Epoch 110/1000 
	 loss: 51.9490, MinusLogProbMetric: 51.9490, val_loss: 53.2673, val_MinusLogProbMetric: 53.2673

Epoch 110: val_loss did not improve from 51.76429
196/196 - 34s - loss: 51.9490 - MinusLogProbMetric: 51.9490 - val_loss: 53.2673 - val_MinusLogProbMetric: 53.2673 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 111/1000
2023-10-27 18:22:10.202 
Epoch 111/1000 
	 loss: 51.8733, MinusLogProbMetric: 51.8733, val_loss: 52.9207, val_MinusLogProbMetric: 52.9207

Epoch 111: val_loss did not improve from 51.76429
196/196 - 34s - loss: 51.8733 - MinusLogProbMetric: 51.8733 - val_loss: 52.9207 - val_MinusLogProbMetric: 52.9207 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 112/1000
2023-10-27 18:22:43.969 
Epoch 112/1000 
	 loss: 51.8208, MinusLogProbMetric: 51.8208, val_loss: 62.0703, val_MinusLogProbMetric: 62.0703

Epoch 112: val_loss did not improve from 51.76429
196/196 - 34s - loss: 51.8208 - MinusLogProbMetric: 51.8208 - val_loss: 62.0703 - val_MinusLogProbMetric: 62.0703 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 113/1000
2023-10-27 18:23:17.644 
Epoch 113/1000 
	 loss: 52.6199, MinusLogProbMetric: 52.6199, val_loss: 52.4303, val_MinusLogProbMetric: 52.4303

Epoch 113: val_loss did not improve from 51.76429
196/196 - 34s - loss: 52.6199 - MinusLogProbMetric: 52.6199 - val_loss: 52.4303 - val_MinusLogProbMetric: 52.4303 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 114/1000
2023-10-27 18:23:51.209 
Epoch 114/1000 
	 loss: 51.8373, MinusLogProbMetric: 51.8373, val_loss: 51.5645, val_MinusLogProbMetric: 51.5645

Epoch 114: val_loss improved from 51.76429 to 51.56453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 51.8373 - MinusLogProbMetric: 51.8373 - val_loss: 51.5645 - val_MinusLogProbMetric: 51.5645 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 115/1000
2023-10-27 18:24:25.723 
Epoch 115/1000 
	 loss: 51.6015, MinusLogProbMetric: 51.6015, val_loss: 52.3119, val_MinusLogProbMetric: 52.3119

Epoch 115: val_loss did not improve from 51.56453
196/196 - 34s - loss: 51.6015 - MinusLogProbMetric: 51.6015 - val_loss: 52.3119 - val_MinusLogProbMetric: 52.3119 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 116/1000
2023-10-27 18:24:59.422 
Epoch 116/1000 
	 loss: 51.5746, MinusLogProbMetric: 51.5746, val_loss: 51.9746, val_MinusLogProbMetric: 51.9746

Epoch 116: val_loss did not improve from 51.56453
196/196 - 34s - loss: 51.5746 - MinusLogProbMetric: 51.5746 - val_loss: 51.9746 - val_MinusLogProbMetric: 51.9746 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 117/1000
2023-10-27 18:25:33.495 
Epoch 117/1000 
	 loss: 51.3883, MinusLogProbMetric: 51.3883, val_loss: 52.5113, val_MinusLogProbMetric: 52.5113

Epoch 117: val_loss did not improve from 51.56453
196/196 - 34s - loss: 51.3883 - MinusLogProbMetric: 51.3883 - val_loss: 52.5113 - val_MinusLogProbMetric: 52.5113 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 118/1000
2023-10-27 18:26:07.485 
Epoch 118/1000 
	 loss: 51.4286, MinusLogProbMetric: 51.4286, val_loss: 56.9324, val_MinusLogProbMetric: 56.9324

Epoch 118: val_loss did not improve from 51.56453
196/196 - 34s - loss: 51.4286 - MinusLogProbMetric: 51.4286 - val_loss: 56.9324 - val_MinusLogProbMetric: 56.9324 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 119/1000
2023-10-27 18:26:41.812 
Epoch 119/1000 
	 loss: 51.9227, MinusLogProbMetric: 51.9227, val_loss: 57.5574, val_MinusLogProbMetric: 57.5574

Epoch 119: val_loss did not improve from 51.56453
196/196 - 34s - loss: 51.9227 - MinusLogProbMetric: 51.9227 - val_loss: 57.5574 - val_MinusLogProbMetric: 57.5574 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 120/1000
2023-10-27 18:27:15.697 
Epoch 120/1000 
	 loss: 51.6069, MinusLogProbMetric: 51.6069, val_loss: 56.3842, val_MinusLogProbMetric: 56.3842

Epoch 120: val_loss did not improve from 51.56453
196/196 - 34s - loss: 51.6069 - MinusLogProbMetric: 51.6069 - val_loss: 56.3842 - val_MinusLogProbMetric: 56.3842 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 121/1000
2023-10-27 18:27:49.830 
Epoch 121/1000 
	 loss: 51.6124, MinusLogProbMetric: 51.6124, val_loss: 53.1665, val_MinusLogProbMetric: 53.1665

Epoch 121: val_loss did not improve from 51.56453
196/196 - 34s - loss: 51.6124 - MinusLogProbMetric: 51.6124 - val_loss: 53.1665 - val_MinusLogProbMetric: 53.1665 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 122/1000
2023-10-27 18:28:24.239 
Epoch 122/1000 
	 loss: 51.1906, MinusLogProbMetric: 51.1906, val_loss: 51.6435, val_MinusLogProbMetric: 51.6435

Epoch 122: val_loss did not improve from 51.56453
196/196 - 34s - loss: 51.1906 - MinusLogProbMetric: 51.1906 - val_loss: 51.6435 - val_MinusLogProbMetric: 51.6435 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 123/1000
2023-10-27 18:28:58.426 
Epoch 123/1000 
	 loss: 51.1269, MinusLogProbMetric: 51.1269, val_loss: 55.6935, val_MinusLogProbMetric: 55.6935

Epoch 123: val_loss did not improve from 51.56453
196/196 - 34s - loss: 51.1269 - MinusLogProbMetric: 51.1269 - val_loss: 55.6935 - val_MinusLogProbMetric: 55.6935 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 124/1000
2023-10-27 18:29:32.297 
Epoch 124/1000 
	 loss: 52.4441, MinusLogProbMetric: 52.4441, val_loss: 50.7894, val_MinusLogProbMetric: 50.7894

Epoch 124: val_loss improved from 51.56453 to 50.78943, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 52.4441 - MinusLogProbMetric: 52.4441 - val_loss: 50.7894 - val_MinusLogProbMetric: 50.7894 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 125/1000
2023-10-27 18:30:07.085 
Epoch 125/1000 
	 loss: 50.7543, MinusLogProbMetric: 50.7543, val_loss: 51.3240, val_MinusLogProbMetric: 51.3240

Epoch 125: val_loss did not improve from 50.78943
196/196 - 34s - loss: 50.7543 - MinusLogProbMetric: 50.7543 - val_loss: 51.3240 - val_MinusLogProbMetric: 51.3240 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 126/1000
2023-10-27 18:30:40.583 
Epoch 126/1000 
	 loss: 50.7539, MinusLogProbMetric: 50.7539, val_loss: 50.0797, val_MinusLogProbMetric: 50.0797

Epoch 126: val_loss improved from 50.78943 to 50.07970, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 50.7539 - MinusLogProbMetric: 50.7539 - val_loss: 50.0797 - val_MinusLogProbMetric: 50.0797 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 127/1000
2023-10-27 18:31:14.731 
Epoch 127/1000 
	 loss: 50.6570, MinusLogProbMetric: 50.6570, val_loss: 50.7874, val_MinusLogProbMetric: 50.7874

Epoch 127: val_loss did not improve from 50.07970
196/196 - 34s - loss: 50.6570 - MinusLogProbMetric: 50.6570 - val_loss: 50.7874 - val_MinusLogProbMetric: 50.7874 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 128/1000
2023-10-27 18:31:48.642 
Epoch 128/1000 
	 loss: 50.3232, MinusLogProbMetric: 50.3232, val_loss: 50.9112, val_MinusLogProbMetric: 50.9112

Epoch 128: val_loss did not improve from 50.07970
196/196 - 34s - loss: 50.3232 - MinusLogProbMetric: 50.3232 - val_loss: 50.9112 - val_MinusLogProbMetric: 50.9112 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 129/1000
2023-10-27 18:32:22.668 
Epoch 129/1000 
	 loss: 50.9633, MinusLogProbMetric: 50.9633, val_loss: 50.1041, val_MinusLogProbMetric: 50.1041

Epoch 129: val_loss did not improve from 50.07970
196/196 - 34s - loss: 50.9633 - MinusLogProbMetric: 50.9633 - val_loss: 50.1041 - val_MinusLogProbMetric: 50.1041 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 130/1000
2023-10-27 18:32:56.702 
Epoch 130/1000 
	 loss: 50.9818, MinusLogProbMetric: 50.9818, val_loss: 51.0568, val_MinusLogProbMetric: 51.0568

Epoch 130: val_loss did not improve from 50.07970
196/196 - 34s - loss: 50.9818 - MinusLogProbMetric: 50.9818 - val_loss: 51.0568 - val_MinusLogProbMetric: 51.0568 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 131/1000
2023-10-27 18:33:30.621 
Epoch 131/1000 
	 loss: 50.2661, MinusLogProbMetric: 50.2661, val_loss: 49.6976, val_MinusLogProbMetric: 49.6976

Epoch 131: val_loss improved from 50.07970 to 49.69761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 50.2661 - MinusLogProbMetric: 50.2661 - val_loss: 49.6976 - val_MinusLogProbMetric: 49.6976 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 132/1000
2023-10-27 18:34:04.773 
Epoch 132/1000 
	 loss: 50.9627, MinusLogProbMetric: 50.9627, val_loss: 52.0823, val_MinusLogProbMetric: 52.0823

Epoch 132: val_loss did not improve from 49.69761
196/196 - 34s - loss: 50.9627 - MinusLogProbMetric: 50.9627 - val_loss: 52.0823 - val_MinusLogProbMetric: 52.0823 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 133/1000
2023-10-27 18:34:38.659 
Epoch 133/1000 
	 loss: 50.4497, MinusLogProbMetric: 50.4497, val_loss: 50.6665, val_MinusLogProbMetric: 50.6665

Epoch 133: val_loss did not improve from 49.69761
196/196 - 34s - loss: 50.4497 - MinusLogProbMetric: 50.4497 - val_loss: 50.6665 - val_MinusLogProbMetric: 50.6665 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 134/1000
2023-10-27 18:35:12.753 
Epoch 134/1000 
	 loss: 51.1633, MinusLogProbMetric: 51.1633, val_loss: 51.0280, val_MinusLogProbMetric: 51.0280

Epoch 134: val_loss did not improve from 49.69761
196/196 - 34s - loss: 51.1633 - MinusLogProbMetric: 51.1633 - val_loss: 51.0280 - val_MinusLogProbMetric: 51.0280 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 135/1000
2023-10-27 18:35:46.811 
Epoch 135/1000 
	 loss: 50.2853, MinusLogProbMetric: 50.2853, val_loss: 50.3709, val_MinusLogProbMetric: 50.3709

Epoch 135: val_loss did not improve from 49.69761
196/196 - 34s - loss: 50.2853 - MinusLogProbMetric: 50.2853 - val_loss: 50.3709 - val_MinusLogProbMetric: 50.3709 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 136/1000
2023-10-27 18:36:21.382 
Epoch 136/1000 
	 loss: 50.1956, MinusLogProbMetric: 50.1956, val_loss: 50.0345, val_MinusLogProbMetric: 50.0345

Epoch 136: val_loss did not improve from 49.69761
196/196 - 35s - loss: 50.1956 - MinusLogProbMetric: 50.1956 - val_loss: 50.0345 - val_MinusLogProbMetric: 50.0345 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 137/1000
2023-10-27 18:36:55.421 
Epoch 137/1000 
	 loss: 49.9683, MinusLogProbMetric: 49.9683, val_loss: 50.7296, val_MinusLogProbMetric: 50.7296

Epoch 137: val_loss did not improve from 49.69761
196/196 - 34s - loss: 49.9683 - MinusLogProbMetric: 49.9683 - val_loss: 50.7296 - val_MinusLogProbMetric: 50.7296 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 138/1000
2023-10-27 18:37:29.778 
Epoch 138/1000 
	 loss: 50.4091, MinusLogProbMetric: 50.4091, val_loss: 51.4064, val_MinusLogProbMetric: 51.4064

Epoch 138: val_loss did not improve from 49.69761
196/196 - 34s - loss: 50.4091 - MinusLogProbMetric: 50.4091 - val_loss: 51.4064 - val_MinusLogProbMetric: 51.4064 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 139/1000
2023-10-27 18:38:03.542 
Epoch 139/1000 
	 loss: 50.5314, MinusLogProbMetric: 50.5314, val_loss: 50.9727, val_MinusLogProbMetric: 50.9727

Epoch 139: val_loss did not improve from 49.69761
196/196 - 34s - loss: 50.5314 - MinusLogProbMetric: 50.5314 - val_loss: 50.9727 - val_MinusLogProbMetric: 50.9727 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 140/1000
2023-10-27 18:38:37.481 
Epoch 140/1000 
	 loss: 50.0638, MinusLogProbMetric: 50.0638, val_loss: 49.6152, val_MinusLogProbMetric: 49.6152

Epoch 140: val_loss improved from 49.69761 to 49.61524, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 50.0638 - MinusLogProbMetric: 50.0638 - val_loss: 49.6152 - val_MinusLogProbMetric: 49.6152 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 141/1000
2023-10-27 18:39:11.626 
Epoch 141/1000 
	 loss: 50.0004, MinusLogProbMetric: 50.0004, val_loss: 54.1998, val_MinusLogProbMetric: 54.1998

Epoch 141: val_loss did not improve from 49.61524
196/196 - 34s - loss: 50.0004 - MinusLogProbMetric: 50.0004 - val_loss: 54.1998 - val_MinusLogProbMetric: 54.1998 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 142/1000
2023-10-27 18:39:45.783 
Epoch 142/1000 
	 loss: 50.3833, MinusLogProbMetric: 50.3833, val_loss: 50.0520, val_MinusLogProbMetric: 50.0520

Epoch 142: val_loss did not improve from 49.61524
196/196 - 34s - loss: 50.3833 - MinusLogProbMetric: 50.3833 - val_loss: 50.0520 - val_MinusLogProbMetric: 50.0520 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 143/1000
2023-10-27 18:40:19.667 
Epoch 143/1000 
	 loss: 49.6878, MinusLogProbMetric: 49.6878, val_loss: 50.5003, val_MinusLogProbMetric: 50.5003

Epoch 143: val_loss did not improve from 49.61524
196/196 - 34s - loss: 49.6878 - MinusLogProbMetric: 49.6878 - val_loss: 50.5003 - val_MinusLogProbMetric: 50.5003 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 144/1000
2023-10-27 18:40:53.772 
Epoch 144/1000 
	 loss: 50.3591, MinusLogProbMetric: 50.3591, val_loss: 50.8414, val_MinusLogProbMetric: 50.8414

Epoch 144: val_loss did not improve from 49.61524
196/196 - 34s - loss: 50.3591 - MinusLogProbMetric: 50.3591 - val_loss: 50.8414 - val_MinusLogProbMetric: 50.8414 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 145/1000
2023-10-27 18:41:27.706 
Epoch 145/1000 
	 loss: 49.8952, MinusLogProbMetric: 49.8952, val_loss: 49.3218, val_MinusLogProbMetric: 49.3218

Epoch 145: val_loss improved from 49.61524 to 49.32179, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 49.8952 - MinusLogProbMetric: 49.8952 - val_loss: 49.3218 - val_MinusLogProbMetric: 49.3218 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 146/1000
2023-10-27 18:42:02.095 
Epoch 146/1000 
	 loss: 49.6696, MinusLogProbMetric: 49.6696, val_loss: 51.4375, val_MinusLogProbMetric: 51.4375

Epoch 146: val_loss did not improve from 49.32179
196/196 - 34s - loss: 49.6696 - MinusLogProbMetric: 49.6696 - val_loss: 51.4375 - val_MinusLogProbMetric: 51.4375 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 147/1000
2023-10-27 18:42:35.844 
Epoch 147/1000 
	 loss: 51.3056, MinusLogProbMetric: 51.3056, val_loss: 50.1654, val_MinusLogProbMetric: 50.1654

Epoch 147: val_loss did not improve from 49.32179
196/196 - 34s - loss: 51.3056 - MinusLogProbMetric: 51.3056 - val_loss: 50.1654 - val_MinusLogProbMetric: 50.1654 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 148/1000
2023-10-27 18:43:10.040 
Epoch 148/1000 
	 loss: 49.2451, MinusLogProbMetric: 49.2451, val_loss: 49.8902, val_MinusLogProbMetric: 49.8902

Epoch 148: val_loss did not improve from 49.32179
196/196 - 34s - loss: 49.2451 - MinusLogProbMetric: 49.2451 - val_loss: 49.8902 - val_MinusLogProbMetric: 49.8902 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 149/1000
2023-10-27 18:43:44.312 
Epoch 149/1000 
	 loss: 49.5144, MinusLogProbMetric: 49.5144, val_loss: 54.3878, val_MinusLogProbMetric: 54.3878

Epoch 149: val_loss did not improve from 49.32179
196/196 - 34s - loss: 49.5144 - MinusLogProbMetric: 49.5144 - val_loss: 54.3878 - val_MinusLogProbMetric: 54.3878 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 150/1000
2023-10-27 18:44:18.691 
Epoch 150/1000 
	 loss: 49.8257, MinusLogProbMetric: 49.8257, val_loss: 51.8211, val_MinusLogProbMetric: 51.8211

Epoch 150: val_loss did not improve from 49.32179
196/196 - 34s - loss: 49.8257 - MinusLogProbMetric: 49.8257 - val_loss: 51.8211 - val_MinusLogProbMetric: 51.8211 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 151/1000
2023-10-27 18:44:52.897 
Epoch 151/1000 
	 loss: 50.2918, MinusLogProbMetric: 50.2918, val_loss: 49.7489, val_MinusLogProbMetric: 49.7489

Epoch 151: val_loss did not improve from 49.32179
196/196 - 34s - loss: 50.2918 - MinusLogProbMetric: 50.2918 - val_loss: 49.7489 - val_MinusLogProbMetric: 49.7489 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 152/1000
2023-10-27 18:45:26.385 
Epoch 152/1000 
	 loss: 49.8546, MinusLogProbMetric: 49.8546, val_loss: 50.6570, val_MinusLogProbMetric: 50.6570

Epoch 152: val_loss did not improve from 49.32179
196/196 - 33s - loss: 49.8546 - MinusLogProbMetric: 49.8546 - val_loss: 50.6570 - val_MinusLogProbMetric: 50.6570 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 153/1000
2023-10-27 18:46:00.146 
Epoch 153/1000 
	 loss: 49.5108, MinusLogProbMetric: 49.5108, val_loss: 49.3872, val_MinusLogProbMetric: 49.3872

Epoch 153: val_loss did not improve from 49.32179
196/196 - 34s - loss: 49.5108 - MinusLogProbMetric: 49.5108 - val_loss: 49.3872 - val_MinusLogProbMetric: 49.3872 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 154/1000
2023-10-27 18:46:34.538 
Epoch 154/1000 
	 loss: 49.4643, MinusLogProbMetric: 49.4643, val_loss: 49.8341, val_MinusLogProbMetric: 49.8341

Epoch 154: val_loss did not improve from 49.32179
196/196 - 34s - loss: 49.4643 - MinusLogProbMetric: 49.4643 - val_loss: 49.8341 - val_MinusLogProbMetric: 49.8341 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 155/1000
2023-10-27 18:47:08.378 
Epoch 155/1000 
	 loss: 49.0155, MinusLogProbMetric: 49.0155, val_loss: 49.3174, val_MinusLogProbMetric: 49.3174

Epoch 155: val_loss improved from 49.32179 to 49.31739, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 49.0155 - MinusLogProbMetric: 49.0155 - val_loss: 49.3174 - val_MinusLogProbMetric: 49.3174 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 156/1000
2023-10-27 18:47:42.859 
Epoch 156/1000 
	 loss: 50.6498, MinusLogProbMetric: 50.6498, val_loss: 50.2839, val_MinusLogProbMetric: 50.2839

Epoch 156: val_loss did not improve from 49.31739
196/196 - 34s - loss: 50.6498 - MinusLogProbMetric: 50.6498 - val_loss: 50.2839 - val_MinusLogProbMetric: 50.2839 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 157/1000
2023-10-27 18:48:16.518 
Epoch 157/1000 
	 loss: 49.9410, MinusLogProbMetric: 49.9410, val_loss: 51.0413, val_MinusLogProbMetric: 51.0413

Epoch 157: val_loss did not improve from 49.31739
196/196 - 34s - loss: 49.9410 - MinusLogProbMetric: 49.9410 - val_loss: 51.0413 - val_MinusLogProbMetric: 51.0413 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 158/1000
2023-10-27 18:48:50.474 
Epoch 158/1000 
	 loss: 49.0952, MinusLogProbMetric: 49.0952, val_loss: 49.6748, val_MinusLogProbMetric: 49.6748

Epoch 158: val_loss did not improve from 49.31739
196/196 - 34s - loss: 49.0952 - MinusLogProbMetric: 49.0952 - val_loss: 49.6748 - val_MinusLogProbMetric: 49.6748 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 159/1000
2023-10-27 18:49:24.146 
Epoch 159/1000 
	 loss: 49.2021, MinusLogProbMetric: 49.2021, val_loss: 50.6832, val_MinusLogProbMetric: 50.6832

Epoch 159: val_loss did not improve from 49.31739
196/196 - 34s - loss: 49.2021 - MinusLogProbMetric: 49.2021 - val_loss: 50.6832 - val_MinusLogProbMetric: 50.6832 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 160/1000
2023-10-27 18:49:58.153 
Epoch 160/1000 
	 loss: 48.9961, MinusLogProbMetric: 48.9961, val_loss: 49.2769, val_MinusLogProbMetric: 49.2769

Epoch 160: val_loss improved from 49.31739 to 49.27690, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 48.9961 - MinusLogProbMetric: 48.9961 - val_loss: 49.2769 - val_MinusLogProbMetric: 49.2769 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 161/1000
2023-10-27 18:50:32.787 
Epoch 161/1000 
	 loss: 48.8165, MinusLogProbMetric: 48.8165, val_loss: 51.3675, val_MinusLogProbMetric: 51.3675

Epoch 161: val_loss did not improve from 49.27690
196/196 - 34s - loss: 48.8165 - MinusLogProbMetric: 48.8165 - val_loss: 51.3675 - val_MinusLogProbMetric: 51.3675 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 162/1000
2023-10-27 18:51:07.257 
Epoch 162/1000 
	 loss: 49.8997, MinusLogProbMetric: 49.8997, val_loss: 49.0195, val_MinusLogProbMetric: 49.0195

Epoch 162: val_loss improved from 49.27690 to 49.01949, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 49.8997 - MinusLogProbMetric: 49.8997 - val_loss: 49.0195 - val_MinusLogProbMetric: 49.0195 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 163/1000
2023-10-27 18:51:41.950 
Epoch 163/1000 
	 loss: 49.1043, MinusLogProbMetric: 49.1043, val_loss: 51.1790, val_MinusLogProbMetric: 51.1790

Epoch 163: val_loss did not improve from 49.01949
196/196 - 34s - loss: 49.1043 - MinusLogProbMetric: 49.1043 - val_loss: 51.1790 - val_MinusLogProbMetric: 51.1790 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 164/1000
2023-10-27 18:52:16.369 
Epoch 164/1000 
	 loss: 49.3375, MinusLogProbMetric: 49.3375, val_loss: 49.6643, val_MinusLogProbMetric: 49.6643

Epoch 164: val_loss did not improve from 49.01949
196/196 - 34s - loss: 49.3375 - MinusLogProbMetric: 49.3375 - val_loss: 49.6643 - val_MinusLogProbMetric: 49.6643 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 165/1000
2023-10-27 18:52:50.094 
Epoch 165/1000 
	 loss: 49.1234, MinusLogProbMetric: 49.1234, val_loss: 48.4593, val_MinusLogProbMetric: 48.4593

Epoch 165: val_loss improved from 49.01949 to 48.45935, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 49.1234 - MinusLogProbMetric: 49.1234 - val_loss: 48.4593 - val_MinusLogProbMetric: 48.4593 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 166/1000
2023-10-27 18:53:24.327 
Epoch 166/1000 
	 loss: 48.9859, MinusLogProbMetric: 48.9859, val_loss: 52.8557, val_MinusLogProbMetric: 52.8557

Epoch 166: val_loss did not improve from 48.45935
196/196 - 34s - loss: 48.9859 - MinusLogProbMetric: 48.9859 - val_loss: 52.8557 - val_MinusLogProbMetric: 52.8557 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 167/1000
2023-10-27 18:53:59.660 
Epoch 167/1000 
	 loss: 49.2414, MinusLogProbMetric: 49.2414, val_loss: 49.8043, val_MinusLogProbMetric: 49.8043

Epoch 167: val_loss did not improve from 48.45935
196/196 - 35s - loss: 49.2414 - MinusLogProbMetric: 49.2414 - val_loss: 49.8043 - val_MinusLogProbMetric: 49.8043 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 168/1000
2023-10-27 18:54:34.931 
Epoch 168/1000 
	 loss: 48.7028, MinusLogProbMetric: 48.7028, val_loss: 48.8625, val_MinusLogProbMetric: 48.8625

Epoch 168: val_loss did not improve from 48.45935
196/196 - 35s - loss: 48.7028 - MinusLogProbMetric: 48.7028 - val_loss: 48.8625 - val_MinusLogProbMetric: 48.8625 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 169/1000
2023-10-27 18:55:09.828 
Epoch 169/1000 
	 loss: 49.3557, MinusLogProbMetric: 49.3557, val_loss: 50.3335, val_MinusLogProbMetric: 50.3335

Epoch 169: val_loss did not improve from 48.45935
196/196 - 35s - loss: 49.3557 - MinusLogProbMetric: 49.3557 - val_loss: 50.3335 - val_MinusLogProbMetric: 50.3335 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 170/1000
2023-10-27 18:55:44.376 
Epoch 170/1000 
	 loss: 48.7502, MinusLogProbMetric: 48.7502, val_loss: 48.1170, val_MinusLogProbMetric: 48.1170

Epoch 170: val_loss improved from 48.45935 to 48.11698, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 48.7502 - MinusLogProbMetric: 48.7502 - val_loss: 48.1170 - val_MinusLogProbMetric: 48.1170 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 171/1000
2023-10-27 18:56:19.495 
Epoch 171/1000 
	 loss: 48.8537, MinusLogProbMetric: 48.8537, val_loss: 48.8781, val_MinusLogProbMetric: 48.8781

Epoch 171: val_loss did not improve from 48.11698
196/196 - 35s - loss: 48.8537 - MinusLogProbMetric: 48.8537 - val_loss: 48.8781 - val_MinusLogProbMetric: 48.8781 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 172/1000
2023-10-27 18:56:53.273 
Epoch 172/1000 
	 loss: 49.2281, MinusLogProbMetric: 49.2281, val_loss: 49.3412, val_MinusLogProbMetric: 49.3412

Epoch 172: val_loss did not improve from 48.11698
196/196 - 34s - loss: 49.2281 - MinusLogProbMetric: 49.2281 - val_loss: 49.3412 - val_MinusLogProbMetric: 49.3412 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 173/1000
2023-10-27 18:57:27.065 
Epoch 173/1000 
	 loss: 48.6278, MinusLogProbMetric: 48.6278, val_loss: 49.0318, val_MinusLogProbMetric: 49.0318

Epoch 173: val_loss did not improve from 48.11698
196/196 - 34s - loss: 48.6278 - MinusLogProbMetric: 48.6278 - val_loss: 49.0318 - val_MinusLogProbMetric: 49.0318 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 174/1000
2023-10-27 18:58:01.245 
Epoch 174/1000 
	 loss: 49.8852, MinusLogProbMetric: 49.8852, val_loss: 49.2451, val_MinusLogProbMetric: 49.2451

Epoch 174: val_loss did not improve from 48.11698
196/196 - 34s - loss: 49.8852 - MinusLogProbMetric: 49.8852 - val_loss: 49.2451 - val_MinusLogProbMetric: 49.2451 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 175/1000
2023-10-27 18:58:36.164 
Epoch 175/1000 
	 loss: 48.7437, MinusLogProbMetric: 48.7437, val_loss: 48.9167, val_MinusLogProbMetric: 48.9167

Epoch 175: val_loss did not improve from 48.11698
196/196 - 35s - loss: 48.7437 - MinusLogProbMetric: 48.7437 - val_loss: 48.9167 - val_MinusLogProbMetric: 48.9167 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 176/1000
2023-10-27 18:59:10.823 
Epoch 176/1000 
	 loss: 49.6280, MinusLogProbMetric: 49.6280, val_loss: 50.2336, val_MinusLogProbMetric: 50.2336

Epoch 176: val_loss did not improve from 48.11698
196/196 - 35s - loss: 49.6280 - MinusLogProbMetric: 49.6280 - val_loss: 50.2336 - val_MinusLogProbMetric: 50.2336 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 177/1000
2023-10-27 18:59:45.257 
Epoch 177/1000 
	 loss: 48.7614, MinusLogProbMetric: 48.7614, val_loss: 48.9546, val_MinusLogProbMetric: 48.9546

Epoch 177: val_loss did not improve from 48.11698
196/196 - 34s - loss: 48.7614 - MinusLogProbMetric: 48.7614 - val_loss: 48.9546 - val_MinusLogProbMetric: 48.9546 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 178/1000
2023-10-27 19:00:19.981 
Epoch 178/1000 
	 loss: 49.9359, MinusLogProbMetric: 49.9359, val_loss: 48.4102, val_MinusLogProbMetric: 48.4102

Epoch 178: val_loss did not improve from 48.11698
196/196 - 35s - loss: 49.9359 - MinusLogProbMetric: 49.9359 - val_loss: 48.4102 - val_MinusLogProbMetric: 48.4102 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 179/1000
2023-10-27 19:00:54.404 
Epoch 179/1000 
	 loss: 48.6286, MinusLogProbMetric: 48.6286, val_loss: 48.3824, val_MinusLogProbMetric: 48.3824

Epoch 179: val_loss did not improve from 48.11698
196/196 - 34s - loss: 48.6286 - MinusLogProbMetric: 48.6286 - val_loss: 48.3824 - val_MinusLogProbMetric: 48.3824 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 180/1000
2023-10-27 19:01:27.835 
Epoch 180/1000 
	 loss: 48.9932, MinusLogProbMetric: 48.9932, val_loss: 50.1857, val_MinusLogProbMetric: 50.1857

Epoch 180: val_loss did not improve from 48.11698
196/196 - 33s - loss: 48.9932 - MinusLogProbMetric: 48.9932 - val_loss: 50.1857 - val_MinusLogProbMetric: 50.1857 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 181/1000
2023-10-27 19:02:02.709 
Epoch 181/1000 
	 loss: 48.1503, MinusLogProbMetric: 48.1503, val_loss: 50.5929, val_MinusLogProbMetric: 50.5929

Epoch 181: val_loss did not improve from 48.11698
196/196 - 35s - loss: 48.1503 - MinusLogProbMetric: 48.1503 - val_loss: 50.5929 - val_MinusLogProbMetric: 50.5929 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 182/1000
2023-10-27 19:02:37.032 
Epoch 182/1000 
	 loss: 49.2272, MinusLogProbMetric: 49.2272, val_loss: 49.2616, val_MinusLogProbMetric: 49.2616

Epoch 182: val_loss did not improve from 48.11698
196/196 - 34s - loss: 49.2272 - MinusLogProbMetric: 49.2272 - val_loss: 49.2616 - val_MinusLogProbMetric: 49.2616 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 183/1000
2023-10-27 19:03:11.781 
Epoch 183/1000 
	 loss: 49.1822, MinusLogProbMetric: 49.1822, val_loss: 49.5153, val_MinusLogProbMetric: 49.5153

Epoch 183: val_loss did not improve from 48.11698
196/196 - 35s - loss: 49.1822 - MinusLogProbMetric: 49.1822 - val_loss: 49.5153 - val_MinusLogProbMetric: 49.5153 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 184/1000
2023-10-27 19:03:45.918 
Epoch 184/1000 
	 loss: 48.4774, MinusLogProbMetric: 48.4774, val_loss: 48.3895, val_MinusLogProbMetric: 48.3895

Epoch 184: val_loss did not improve from 48.11698
196/196 - 34s - loss: 48.4774 - MinusLogProbMetric: 48.4774 - val_loss: 48.3895 - val_MinusLogProbMetric: 48.3895 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 185/1000
2023-10-27 19:04:20.379 
Epoch 185/1000 
	 loss: 48.7320, MinusLogProbMetric: 48.7320, val_loss: 48.2168, val_MinusLogProbMetric: 48.2168

Epoch 185: val_loss did not improve from 48.11698
196/196 - 34s - loss: 48.7320 - MinusLogProbMetric: 48.7320 - val_loss: 48.2168 - val_MinusLogProbMetric: 48.2168 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 186/1000
2023-10-27 19:04:54.817 
Epoch 186/1000 
	 loss: 48.5608, MinusLogProbMetric: 48.5608, val_loss: 48.1851, val_MinusLogProbMetric: 48.1851

Epoch 186: val_loss did not improve from 48.11698
196/196 - 34s - loss: 48.5608 - MinusLogProbMetric: 48.5608 - val_loss: 48.1851 - val_MinusLogProbMetric: 48.1851 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 187/1000
2023-10-27 19:05:28.750 
Epoch 187/1000 
	 loss: 49.1393, MinusLogProbMetric: 49.1393, val_loss: 49.8060, val_MinusLogProbMetric: 49.8060

Epoch 187: val_loss did not improve from 48.11698
196/196 - 34s - loss: 49.1393 - MinusLogProbMetric: 49.1393 - val_loss: 49.8060 - val_MinusLogProbMetric: 49.8060 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 188/1000
2023-10-27 19:06:03.598 
Epoch 188/1000 
	 loss: 48.4319, MinusLogProbMetric: 48.4319, val_loss: 49.6996, val_MinusLogProbMetric: 49.6996

Epoch 188: val_loss did not improve from 48.11698
196/196 - 35s - loss: 48.4319 - MinusLogProbMetric: 48.4319 - val_loss: 49.6996 - val_MinusLogProbMetric: 49.6996 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 189/1000
2023-10-27 19:06:37.641 
Epoch 189/1000 
	 loss: 48.2270, MinusLogProbMetric: 48.2270, val_loss: 51.4562, val_MinusLogProbMetric: 51.4562

Epoch 189: val_loss did not improve from 48.11698
196/196 - 34s - loss: 48.2270 - MinusLogProbMetric: 48.2270 - val_loss: 51.4562 - val_MinusLogProbMetric: 51.4562 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 190/1000
2023-10-27 19:07:11.653 
Epoch 190/1000 
	 loss: 48.1990, MinusLogProbMetric: 48.1990, val_loss: 49.8030, val_MinusLogProbMetric: 49.8030

Epoch 190: val_loss did not improve from 48.11698
196/196 - 34s - loss: 48.1990 - MinusLogProbMetric: 48.1990 - val_loss: 49.8030 - val_MinusLogProbMetric: 49.8030 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 191/1000
2023-10-27 19:07:45.625 
Epoch 191/1000 
	 loss: 48.6917, MinusLogProbMetric: 48.6917, val_loss: 51.8839, val_MinusLogProbMetric: 51.8839

Epoch 191: val_loss did not improve from 48.11698
196/196 - 34s - loss: 48.6917 - MinusLogProbMetric: 48.6917 - val_loss: 51.8839 - val_MinusLogProbMetric: 51.8839 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 192/1000
2023-10-27 19:08:19.967 
Epoch 192/1000 
	 loss: 48.5826, MinusLogProbMetric: 48.5826, val_loss: 48.0367, val_MinusLogProbMetric: 48.0367

Epoch 192: val_loss improved from 48.11698 to 48.03670, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 48.5826 - MinusLogProbMetric: 48.5826 - val_loss: 48.0367 - val_MinusLogProbMetric: 48.0367 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 193/1000
2023-10-27 19:08:54.298 
Epoch 193/1000 
	 loss: 48.2245, MinusLogProbMetric: 48.2245, val_loss: 48.5523, val_MinusLogProbMetric: 48.5523

Epoch 193: val_loss did not improve from 48.03670
196/196 - 34s - loss: 48.2245 - MinusLogProbMetric: 48.2245 - val_loss: 48.5523 - val_MinusLogProbMetric: 48.5523 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 194/1000
2023-10-27 19:09:27.639 
Epoch 194/1000 
	 loss: 48.0532, MinusLogProbMetric: 48.0532, val_loss: 51.8511, val_MinusLogProbMetric: 51.8511

Epoch 194: val_loss did not improve from 48.03670
196/196 - 33s - loss: 48.0532 - MinusLogProbMetric: 48.0532 - val_loss: 51.8511 - val_MinusLogProbMetric: 51.8511 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 195/1000
2023-10-27 19:10:01.634 
Epoch 195/1000 
	 loss: 48.1962, MinusLogProbMetric: 48.1962, val_loss: 49.3015, val_MinusLogProbMetric: 49.3015

Epoch 195: val_loss did not improve from 48.03670
196/196 - 34s - loss: 48.1962 - MinusLogProbMetric: 48.1962 - val_loss: 49.3015 - val_MinusLogProbMetric: 49.3015 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 196/1000
2023-10-27 19:10:35.409 
Epoch 196/1000 
	 loss: 48.0570, MinusLogProbMetric: 48.0570, val_loss: 48.3317, val_MinusLogProbMetric: 48.3317

Epoch 196: val_loss did not improve from 48.03670
196/196 - 34s - loss: 48.0570 - MinusLogProbMetric: 48.0570 - val_loss: 48.3317 - val_MinusLogProbMetric: 48.3317 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 197/1000
2023-10-27 19:11:09.252 
Epoch 197/1000 
	 loss: 48.7694, MinusLogProbMetric: 48.7694, val_loss: 48.4980, val_MinusLogProbMetric: 48.4980

Epoch 197: val_loss did not improve from 48.03670
196/196 - 34s - loss: 48.7694 - MinusLogProbMetric: 48.7694 - val_loss: 48.4980 - val_MinusLogProbMetric: 48.4980 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 198/1000
2023-10-27 19:11:44.106 
Epoch 198/1000 
	 loss: 48.3139, MinusLogProbMetric: 48.3139, val_loss: 47.7001, val_MinusLogProbMetric: 47.7001

Epoch 198: val_loss improved from 48.03670 to 47.70012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 48.3139 - MinusLogProbMetric: 48.3139 - val_loss: 47.7001 - val_MinusLogProbMetric: 47.7001 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 199/1000
2023-10-27 19:12:19.222 
Epoch 199/1000 
	 loss: 47.8882, MinusLogProbMetric: 47.8882, val_loss: 47.4445, val_MinusLogProbMetric: 47.4445

Epoch 199: val_loss improved from 47.70012 to 47.44450, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 47.8882 - MinusLogProbMetric: 47.8882 - val_loss: 47.4445 - val_MinusLogProbMetric: 47.4445 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 200/1000
2023-10-27 19:12:54.010 
Epoch 200/1000 
	 loss: 48.3826, MinusLogProbMetric: 48.3826, val_loss: 49.9152, val_MinusLogProbMetric: 49.9152

Epoch 200: val_loss did not improve from 47.44450
196/196 - 34s - loss: 48.3826 - MinusLogProbMetric: 48.3826 - val_loss: 49.9152 - val_MinusLogProbMetric: 49.9152 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 201/1000
2023-10-27 19:13:28.174 
Epoch 201/1000 
	 loss: 47.7594, MinusLogProbMetric: 47.7594, val_loss: 48.3563, val_MinusLogProbMetric: 48.3563

Epoch 201: val_loss did not improve from 47.44450
196/196 - 34s - loss: 47.7594 - MinusLogProbMetric: 47.7594 - val_loss: 48.3563 - val_MinusLogProbMetric: 48.3563 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 202/1000
2023-10-27 19:14:02.506 
Epoch 202/1000 
	 loss: 51.5412, MinusLogProbMetric: 51.5412, val_loss: 49.8914, val_MinusLogProbMetric: 49.8914

Epoch 202: val_loss did not improve from 47.44450
196/196 - 34s - loss: 51.5412 - MinusLogProbMetric: 51.5412 - val_loss: 49.8914 - val_MinusLogProbMetric: 49.8914 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 203/1000
2023-10-27 19:14:35.698 
Epoch 203/1000 
	 loss: 48.1208, MinusLogProbMetric: 48.1208, val_loss: 49.5770, val_MinusLogProbMetric: 49.5770

Epoch 203: val_loss did not improve from 47.44450
196/196 - 33s - loss: 48.1208 - MinusLogProbMetric: 48.1208 - val_loss: 49.5770 - val_MinusLogProbMetric: 49.5770 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 204/1000
2023-10-27 19:15:09.272 
Epoch 204/1000 
	 loss: 51.4258, MinusLogProbMetric: 51.4258, val_loss: 50.5843, val_MinusLogProbMetric: 50.5843

Epoch 204: val_loss did not improve from 47.44450
196/196 - 34s - loss: 51.4258 - MinusLogProbMetric: 51.4258 - val_loss: 50.5843 - val_MinusLogProbMetric: 50.5843 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 205/1000
2023-10-27 19:15:43.642 
Epoch 205/1000 
	 loss: 48.5213, MinusLogProbMetric: 48.5213, val_loss: 48.5026, val_MinusLogProbMetric: 48.5026

Epoch 205: val_loss did not improve from 47.44450
196/196 - 34s - loss: 48.5213 - MinusLogProbMetric: 48.5213 - val_loss: 48.5026 - val_MinusLogProbMetric: 48.5026 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 206/1000
2023-10-27 19:16:18.140 
Epoch 206/1000 
	 loss: 48.2947, MinusLogProbMetric: 48.2947, val_loss: 47.7502, val_MinusLogProbMetric: 47.7502

Epoch 206: val_loss did not improve from 47.44450
196/196 - 34s - loss: 48.2947 - MinusLogProbMetric: 48.2947 - val_loss: 47.7502 - val_MinusLogProbMetric: 47.7502 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 207/1000
2023-10-27 19:16:52.531 
Epoch 207/1000 
	 loss: 47.9533, MinusLogProbMetric: 47.9533, val_loss: 51.4711, val_MinusLogProbMetric: 51.4711

Epoch 207: val_loss did not improve from 47.44450
196/196 - 34s - loss: 47.9533 - MinusLogProbMetric: 47.9533 - val_loss: 51.4711 - val_MinusLogProbMetric: 51.4711 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 208/1000
2023-10-27 19:17:27.095 
Epoch 208/1000 
	 loss: 48.3781, MinusLogProbMetric: 48.3781, val_loss: 48.8751, val_MinusLogProbMetric: 48.8751

Epoch 208: val_loss did not improve from 47.44450
196/196 - 35s - loss: 48.3781 - MinusLogProbMetric: 48.3781 - val_loss: 48.8751 - val_MinusLogProbMetric: 48.8751 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 209/1000
2023-10-27 19:18:00.848 
Epoch 209/1000 
	 loss: 48.7008, MinusLogProbMetric: 48.7008, val_loss: 48.9944, val_MinusLogProbMetric: 48.9944

Epoch 209: val_loss did not improve from 47.44450
196/196 - 34s - loss: 48.7008 - MinusLogProbMetric: 48.7008 - val_loss: 48.9944 - val_MinusLogProbMetric: 48.9944 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 210/1000
2023-10-27 19:18:34.960 
Epoch 210/1000 
	 loss: 48.1676, MinusLogProbMetric: 48.1676, val_loss: 49.1906, val_MinusLogProbMetric: 49.1906

Epoch 210: val_loss did not improve from 47.44450
196/196 - 34s - loss: 48.1676 - MinusLogProbMetric: 48.1676 - val_loss: 49.1906 - val_MinusLogProbMetric: 49.1906 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 211/1000
2023-10-27 19:19:08.342 
Epoch 211/1000 
	 loss: 48.1565, MinusLogProbMetric: 48.1565, val_loss: 47.7852, val_MinusLogProbMetric: 47.7852

Epoch 211: val_loss did not improve from 47.44450
196/196 - 33s - loss: 48.1565 - MinusLogProbMetric: 48.1565 - val_loss: 47.7852 - val_MinusLogProbMetric: 47.7852 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 212/1000
2023-10-27 19:19:42.941 
Epoch 212/1000 
	 loss: 47.7907, MinusLogProbMetric: 47.7907, val_loss: 48.5464, val_MinusLogProbMetric: 48.5464

Epoch 212: val_loss did not improve from 47.44450
196/196 - 35s - loss: 47.7907 - MinusLogProbMetric: 47.7907 - val_loss: 48.5464 - val_MinusLogProbMetric: 48.5464 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 213/1000
2023-10-27 19:20:17.660 
Epoch 213/1000 
	 loss: 47.8926, MinusLogProbMetric: 47.8926, val_loss: 48.4321, val_MinusLogProbMetric: 48.4321

Epoch 213: val_loss did not improve from 47.44450
196/196 - 35s - loss: 47.8926 - MinusLogProbMetric: 47.8926 - val_loss: 48.4321 - val_MinusLogProbMetric: 48.4321 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 214/1000
2023-10-27 19:20:52.241 
Epoch 214/1000 
	 loss: 48.8871, MinusLogProbMetric: 48.8871, val_loss: 51.1010, val_MinusLogProbMetric: 51.1010

Epoch 214: val_loss did not improve from 47.44450
196/196 - 35s - loss: 48.8871 - MinusLogProbMetric: 48.8871 - val_loss: 51.1010 - val_MinusLogProbMetric: 51.1010 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 215/1000
2023-10-27 19:21:26.509 
Epoch 215/1000 
	 loss: 47.3350, MinusLogProbMetric: 47.3350, val_loss: 49.1323, val_MinusLogProbMetric: 49.1323

Epoch 215: val_loss did not improve from 47.44450
196/196 - 34s - loss: 47.3350 - MinusLogProbMetric: 47.3350 - val_loss: 49.1323 - val_MinusLogProbMetric: 49.1323 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 216/1000
2023-10-27 19:22:00.714 
Epoch 216/1000 
	 loss: 47.9102, MinusLogProbMetric: 47.9102, val_loss: 48.3338, val_MinusLogProbMetric: 48.3338

Epoch 216: val_loss did not improve from 47.44450
196/196 - 34s - loss: 47.9102 - MinusLogProbMetric: 47.9102 - val_loss: 48.3338 - val_MinusLogProbMetric: 48.3338 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 217/1000
2023-10-27 19:22:34.682 
Epoch 217/1000 
	 loss: 47.7997, MinusLogProbMetric: 47.7997, val_loss: 51.2987, val_MinusLogProbMetric: 51.2987

Epoch 217: val_loss did not improve from 47.44450
196/196 - 34s - loss: 47.7997 - MinusLogProbMetric: 47.7997 - val_loss: 51.2987 - val_MinusLogProbMetric: 51.2987 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 218/1000
2023-10-27 19:23:08.733 
Epoch 218/1000 
	 loss: 48.6522, MinusLogProbMetric: 48.6522, val_loss: 47.6794, val_MinusLogProbMetric: 47.6794

Epoch 218: val_loss did not improve from 47.44450
196/196 - 34s - loss: 48.6522 - MinusLogProbMetric: 48.6522 - val_loss: 47.6794 - val_MinusLogProbMetric: 47.6794 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 219/1000
2023-10-27 19:23:42.622 
Epoch 219/1000 
	 loss: 47.9699, MinusLogProbMetric: 47.9699, val_loss: 47.8194, val_MinusLogProbMetric: 47.8194

Epoch 219: val_loss did not improve from 47.44450
196/196 - 34s - loss: 47.9699 - MinusLogProbMetric: 47.9699 - val_loss: 47.8194 - val_MinusLogProbMetric: 47.8194 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 220/1000
2023-10-27 19:24:16.566 
Epoch 220/1000 
	 loss: 47.5293, MinusLogProbMetric: 47.5293, val_loss: 47.8739, val_MinusLogProbMetric: 47.8739

Epoch 220: val_loss did not improve from 47.44450
196/196 - 34s - loss: 47.5293 - MinusLogProbMetric: 47.5293 - val_loss: 47.8739 - val_MinusLogProbMetric: 47.8739 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 221/1000
2023-10-27 19:24:50.129 
Epoch 221/1000 
	 loss: 47.7511, MinusLogProbMetric: 47.7511, val_loss: 48.4752, val_MinusLogProbMetric: 48.4752

Epoch 221: val_loss did not improve from 47.44450
196/196 - 34s - loss: 47.7511 - MinusLogProbMetric: 47.7511 - val_loss: 48.4752 - val_MinusLogProbMetric: 48.4752 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 222/1000
2023-10-27 19:25:23.782 
Epoch 222/1000 
	 loss: 48.8608, MinusLogProbMetric: 48.8608, val_loss: 48.3586, val_MinusLogProbMetric: 48.3586

Epoch 222: val_loss did not improve from 47.44450
196/196 - 34s - loss: 48.8608 - MinusLogProbMetric: 48.8608 - val_loss: 48.3586 - val_MinusLogProbMetric: 48.3586 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 223/1000
2023-10-27 19:25:58.169 
Epoch 223/1000 
	 loss: 48.1154, MinusLogProbMetric: 48.1154, val_loss: 48.3606, val_MinusLogProbMetric: 48.3606

Epoch 223: val_loss did not improve from 47.44450
196/196 - 34s - loss: 48.1154 - MinusLogProbMetric: 48.1154 - val_loss: 48.3606 - val_MinusLogProbMetric: 48.3606 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 224/1000
2023-10-27 19:26:32.143 
Epoch 224/1000 
	 loss: 48.8355, MinusLogProbMetric: 48.8355, val_loss: 53.2319, val_MinusLogProbMetric: 53.2319

Epoch 224: val_loss did not improve from 47.44450
196/196 - 34s - loss: 48.8355 - MinusLogProbMetric: 48.8355 - val_loss: 53.2319 - val_MinusLogProbMetric: 53.2319 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 225/1000
2023-10-27 19:27:06.528 
Epoch 225/1000 
	 loss: 47.6963, MinusLogProbMetric: 47.6963, val_loss: 47.5802, val_MinusLogProbMetric: 47.5802

Epoch 225: val_loss did not improve from 47.44450
196/196 - 34s - loss: 47.6963 - MinusLogProbMetric: 47.6963 - val_loss: 47.5802 - val_MinusLogProbMetric: 47.5802 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 226/1000
2023-10-27 19:27:40.943 
Epoch 226/1000 
	 loss: 47.2122, MinusLogProbMetric: 47.2122, val_loss: 49.6726, val_MinusLogProbMetric: 49.6726

Epoch 226: val_loss did not improve from 47.44450
196/196 - 34s - loss: 47.2122 - MinusLogProbMetric: 47.2122 - val_loss: 49.6726 - val_MinusLogProbMetric: 49.6726 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 227/1000
2023-10-27 19:28:15.170 
Epoch 227/1000 
	 loss: 48.5483, MinusLogProbMetric: 48.5483, val_loss: 47.7263, val_MinusLogProbMetric: 47.7263

Epoch 227: val_loss did not improve from 47.44450
196/196 - 34s - loss: 48.5483 - MinusLogProbMetric: 48.5483 - val_loss: 47.7263 - val_MinusLogProbMetric: 47.7263 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 228/1000
2023-10-27 19:28:49.219 
Epoch 228/1000 
	 loss: 47.3398, MinusLogProbMetric: 47.3398, val_loss: 47.5710, val_MinusLogProbMetric: 47.5710

Epoch 228: val_loss did not improve from 47.44450
196/196 - 34s - loss: 47.3398 - MinusLogProbMetric: 47.3398 - val_loss: 47.5710 - val_MinusLogProbMetric: 47.5710 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 229/1000
2023-10-27 19:29:22.949 
Epoch 229/1000 
	 loss: 47.3120, MinusLogProbMetric: 47.3120, val_loss: 48.2293, val_MinusLogProbMetric: 48.2293

Epoch 229: val_loss did not improve from 47.44450
196/196 - 34s - loss: 47.3120 - MinusLogProbMetric: 47.3120 - val_loss: 48.2293 - val_MinusLogProbMetric: 48.2293 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 230/1000
2023-10-27 19:29:57.118 
Epoch 230/1000 
	 loss: 47.8072, MinusLogProbMetric: 47.8072, val_loss: 48.4193, val_MinusLogProbMetric: 48.4193

Epoch 230: val_loss did not improve from 47.44450
196/196 - 34s - loss: 47.8072 - MinusLogProbMetric: 47.8072 - val_loss: 48.4193 - val_MinusLogProbMetric: 48.4193 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 231/1000
2023-10-27 19:30:31.501 
Epoch 231/1000 
	 loss: 48.6839, MinusLogProbMetric: 48.6839, val_loss: 50.7938, val_MinusLogProbMetric: 50.7938

Epoch 231: val_loss did not improve from 47.44450
196/196 - 34s - loss: 48.6839 - MinusLogProbMetric: 48.6839 - val_loss: 50.7938 - val_MinusLogProbMetric: 50.7938 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 232/1000
2023-10-27 19:31:05.707 
Epoch 232/1000 
	 loss: 47.4799, MinusLogProbMetric: 47.4799, val_loss: 46.9706, val_MinusLogProbMetric: 46.9706

Epoch 232: val_loss improved from 47.44450 to 46.97063, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 47.4799 - MinusLogProbMetric: 47.4799 - val_loss: 46.9706 - val_MinusLogProbMetric: 46.9706 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 233/1000
2023-10-27 19:31:39.921 
Epoch 233/1000 
	 loss: 47.7059, MinusLogProbMetric: 47.7059, val_loss: 48.2306, val_MinusLogProbMetric: 48.2306

Epoch 233: val_loss did not improve from 46.97063
196/196 - 34s - loss: 47.7059 - MinusLogProbMetric: 47.7059 - val_loss: 48.2306 - val_MinusLogProbMetric: 48.2306 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 234/1000
2023-10-27 19:32:14.926 
Epoch 234/1000 
	 loss: 47.7367, MinusLogProbMetric: 47.7367, val_loss: 47.4232, val_MinusLogProbMetric: 47.4232

Epoch 234: val_loss did not improve from 46.97063
196/196 - 35s - loss: 47.7367 - MinusLogProbMetric: 47.7367 - val_loss: 47.4232 - val_MinusLogProbMetric: 47.4232 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 235/1000
2023-10-27 19:32:49.260 
Epoch 235/1000 
	 loss: 47.7820, MinusLogProbMetric: 47.7820, val_loss: 106.2723, val_MinusLogProbMetric: 106.2723

Epoch 235: val_loss did not improve from 46.97063
196/196 - 34s - loss: 47.7820 - MinusLogProbMetric: 47.7820 - val_loss: 106.2723 - val_MinusLogProbMetric: 106.2723 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 236/1000
2023-10-27 19:33:23.725 
Epoch 236/1000 
	 loss: 58.9143, MinusLogProbMetric: 58.9143, val_loss: 48.9764, val_MinusLogProbMetric: 48.9764

Epoch 236: val_loss did not improve from 46.97063
196/196 - 34s - loss: 58.9143 - MinusLogProbMetric: 58.9143 - val_loss: 48.9764 - val_MinusLogProbMetric: 48.9764 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 237/1000
2023-10-27 19:33:57.956 
Epoch 237/1000 
	 loss: 48.3160, MinusLogProbMetric: 48.3160, val_loss: 48.9943, val_MinusLogProbMetric: 48.9943

Epoch 237: val_loss did not improve from 46.97063
196/196 - 34s - loss: 48.3160 - MinusLogProbMetric: 48.3160 - val_loss: 48.9943 - val_MinusLogProbMetric: 48.9943 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 238/1000
2023-10-27 19:34:32.124 
Epoch 238/1000 
	 loss: 47.9221, MinusLogProbMetric: 47.9221, val_loss: 48.7731, val_MinusLogProbMetric: 48.7731

Epoch 238: val_loss did not improve from 46.97063
196/196 - 34s - loss: 47.9221 - MinusLogProbMetric: 47.9221 - val_loss: 48.7731 - val_MinusLogProbMetric: 48.7731 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 239/1000
2023-10-27 19:35:06.283 
Epoch 239/1000 
	 loss: 47.5498, MinusLogProbMetric: 47.5498, val_loss: 47.1525, val_MinusLogProbMetric: 47.1525

Epoch 239: val_loss did not improve from 46.97063
196/196 - 34s - loss: 47.5498 - MinusLogProbMetric: 47.5498 - val_loss: 47.1525 - val_MinusLogProbMetric: 47.1525 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 240/1000
2023-10-27 19:35:40.401 
Epoch 240/1000 
	 loss: 47.6031, MinusLogProbMetric: 47.6031, val_loss: 48.1403, val_MinusLogProbMetric: 48.1403

Epoch 240: val_loss did not improve from 46.97063
196/196 - 34s - loss: 47.6031 - MinusLogProbMetric: 47.6031 - val_loss: 48.1403 - val_MinusLogProbMetric: 48.1403 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 241/1000
2023-10-27 19:36:14.168 
Epoch 241/1000 
	 loss: 47.8911, MinusLogProbMetric: 47.8911, val_loss: 46.5322, val_MinusLogProbMetric: 46.5322

Epoch 241: val_loss improved from 46.97063 to 46.53216, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 47.8911 - MinusLogProbMetric: 47.8911 - val_loss: 46.5322 - val_MinusLogProbMetric: 46.5322 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 242/1000
2023-10-27 19:36:49.405 
Epoch 242/1000 
	 loss: 47.0214, MinusLogProbMetric: 47.0214, val_loss: 47.9596, val_MinusLogProbMetric: 47.9596

Epoch 242: val_loss did not improve from 46.53216
196/196 - 35s - loss: 47.0214 - MinusLogProbMetric: 47.0214 - val_loss: 47.9596 - val_MinusLogProbMetric: 47.9596 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 243/1000
2023-10-27 19:37:24.717 
Epoch 243/1000 
	 loss: 47.7660, MinusLogProbMetric: 47.7660, val_loss: 48.1813, val_MinusLogProbMetric: 48.1813

Epoch 243: val_loss did not improve from 46.53216
196/196 - 35s - loss: 47.7660 - MinusLogProbMetric: 47.7660 - val_loss: 48.1813 - val_MinusLogProbMetric: 48.1813 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 244/1000
2023-10-27 19:37:59.700 
Epoch 244/1000 
	 loss: 47.1852, MinusLogProbMetric: 47.1852, val_loss: 48.7712, val_MinusLogProbMetric: 48.7712

Epoch 244: val_loss did not improve from 46.53216
196/196 - 35s - loss: 47.1852 - MinusLogProbMetric: 47.1852 - val_loss: 48.7712 - val_MinusLogProbMetric: 48.7712 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 245/1000
2023-10-27 19:38:33.830 
Epoch 245/1000 
	 loss: 47.0103, MinusLogProbMetric: 47.0103, val_loss: 47.1988, val_MinusLogProbMetric: 47.1988

Epoch 245: val_loss did not improve from 46.53216
196/196 - 34s - loss: 47.0103 - MinusLogProbMetric: 47.0103 - val_loss: 47.1988 - val_MinusLogProbMetric: 47.1988 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 246/1000
2023-10-27 19:39:07.379 
Epoch 246/1000 
	 loss: 47.0932, MinusLogProbMetric: 47.0932, val_loss: 49.6740, val_MinusLogProbMetric: 49.6740

Epoch 246: val_loss did not improve from 46.53216
196/196 - 34s - loss: 47.0932 - MinusLogProbMetric: 47.0932 - val_loss: 49.6740 - val_MinusLogProbMetric: 49.6740 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 247/1000
2023-10-27 19:39:41.433 
Epoch 247/1000 
	 loss: 47.3527, MinusLogProbMetric: 47.3527, val_loss: 48.4306, val_MinusLogProbMetric: 48.4306

Epoch 247: val_loss did not improve from 46.53216
196/196 - 34s - loss: 47.3527 - MinusLogProbMetric: 47.3527 - val_loss: 48.4306 - val_MinusLogProbMetric: 48.4306 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 248/1000
2023-10-27 19:40:15.717 
Epoch 248/1000 
	 loss: 46.9702, MinusLogProbMetric: 46.9702, val_loss: 48.0510, val_MinusLogProbMetric: 48.0510

Epoch 248: val_loss did not improve from 46.53216
196/196 - 34s - loss: 46.9702 - MinusLogProbMetric: 46.9702 - val_loss: 48.0510 - val_MinusLogProbMetric: 48.0510 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 249/1000
2023-10-27 19:40:49.791 
Epoch 249/1000 
	 loss: 47.3660, MinusLogProbMetric: 47.3660, val_loss: 49.6617, val_MinusLogProbMetric: 49.6617

Epoch 249: val_loss did not improve from 46.53216
196/196 - 34s - loss: 47.3660 - MinusLogProbMetric: 47.3660 - val_loss: 49.6617 - val_MinusLogProbMetric: 49.6617 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 250/1000
2023-10-27 19:41:23.934 
Epoch 250/1000 
	 loss: 47.9464, MinusLogProbMetric: 47.9464, val_loss: 50.8552, val_MinusLogProbMetric: 50.8552

Epoch 250: val_loss did not improve from 46.53216
196/196 - 34s - loss: 47.9464 - MinusLogProbMetric: 47.9464 - val_loss: 50.8552 - val_MinusLogProbMetric: 50.8552 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 251/1000
2023-10-27 19:41:58.629 
Epoch 251/1000 
	 loss: 46.7120, MinusLogProbMetric: 46.7120, val_loss: 47.5892, val_MinusLogProbMetric: 47.5892

Epoch 251: val_loss did not improve from 46.53216
196/196 - 35s - loss: 46.7120 - MinusLogProbMetric: 46.7120 - val_loss: 47.5892 - val_MinusLogProbMetric: 47.5892 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 252/1000
2023-10-27 19:42:33.245 
Epoch 252/1000 
	 loss: 46.7210, MinusLogProbMetric: 46.7210, val_loss: 47.4599, val_MinusLogProbMetric: 47.4599

Epoch 252: val_loss did not improve from 46.53216
196/196 - 35s - loss: 46.7210 - MinusLogProbMetric: 46.7210 - val_loss: 47.4599 - val_MinusLogProbMetric: 47.4599 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 253/1000
2023-10-27 19:43:07.307 
Epoch 253/1000 
	 loss: 47.0588, MinusLogProbMetric: 47.0588, val_loss: 48.2580, val_MinusLogProbMetric: 48.2580

Epoch 253: val_loss did not improve from 46.53216
196/196 - 34s - loss: 47.0588 - MinusLogProbMetric: 47.0588 - val_loss: 48.2580 - val_MinusLogProbMetric: 48.2580 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 254/1000
2023-10-27 19:43:41.690 
Epoch 254/1000 
	 loss: 47.0655, MinusLogProbMetric: 47.0655, val_loss: 48.5261, val_MinusLogProbMetric: 48.5261

Epoch 254: val_loss did not improve from 46.53216
196/196 - 34s - loss: 47.0655 - MinusLogProbMetric: 47.0655 - val_loss: 48.5261 - val_MinusLogProbMetric: 48.5261 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 255/1000
2023-10-27 19:44:15.725 
Epoch 255/1000 
	 loss: 47.8248, MinusLogProbMetric: 47.8248, val_loss: 46.8383, val_MinusLogProbMetric: 46.8383

Epoch 255: val_loss did not improve from 46.53216
196/196 - 34s - loss: 47.8248 - MinusLogProbMetric: 47.8248 - val_loss: 46.8383 - val_MinusLogProbMetric: 46.8383 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 256/1000
2023-10-27 19:44:50.589 
Epoch 256/1000 
	 loss: 47.4433, MinusLogProbMetric: 47.4433, val_loss: 46.8455, val_MinusLogProbMetric: 46.8455

Epoch 256: val_loss did not improve from 46.53216
196/196 - 35s - loss: 47.4433 - MinusLogProbMetric: 47.4433 - val_loss: 46.8455 - val_MinusLogProbMetric: 46.8455 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 257/1000
2023-10-27 19:45:25.149 
Epoch 257/1000 
	 loss: 46.7494, MinusLogProbMetric: 46.7494, val_loss: 51.0529, val_MinusLogProbMetric: 51.0529

Epoch 257: val_loss did not improve from 46.53216
196/196 - 35s - loss: 46.7494 - MinusLogProbMetric: 46.7494 - val_loss: 51.0529 - val_MinusLogProbMetric: 51.0529 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 258/1000
2023-10-27 19:46:00.025 
Epoch 258/1000 
	 loss: 46.7458, MinusLogProbMetric: 46.7458, val_loss: 51.3052, val_MinusLogProbMetric: 51.3052

Epoch 258: val_loss did not improve from 46.53216
196/196 - 35s - loss: 46.7458 - MinusLogProbMetric: 46.7458 - val_loss: 51.3052 - val_MinusLogProbMetric: 51.3052 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 259/1000
2023-10-27 19:46:35.003 
Epoch 259/1000 
	 loss: 47.2307, MinusLogProbMetric: 47.2307, val_loss: 47.6984, val_MinusLogProbMetric: 47.6984

Epoch 259: val_loss did not improve from 46.53216
196/196 - 35s - loss: 47.2307 - MinusLogProbMetric: 47.2307 - val_loss: 47.6984 - val_MinusLogProbMetric: 47.6984 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 260/1000
2023-10-27 19:47:09.859 
Epoch 260/1000 
	 loss: 46.9915, MinusLogProbMetric: 46.9915, val_loss: 47.3027, val_MinusLogProbMetric: 47.3027

Epoch 260: val_loss did not improve from 46.53216
196/196 - 35s - loss: 46.9915 - MinusLogProbMetric: 46.9915 - val_loss: 47.3027 - val_MinusLogProbMetric: 47.3027 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 261/1000
2023-10-27 19:47:43.965 
Epoch 261/1000 
	 loss: 46.9031, MinusLogProbMetric: 46.9031, val_loss: 48.6687, val_MinusLogProbMetric: 48.6687

Epoch 261: val_loss did not improve from 46.53216
196/196 - 34s - loss: 46.9031 - MinusLogProbMetric: 46.9031 - val_loss: 48.6687 - val_MinusLogProbMetric: 48.6687 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 262/1000
2023-10-27 19:48:18.568 
Epoch 262/1000 
	 loss: 46.9291, MinusLogProbMetric: 46.9291, val_loss: 46.6119, val_MinusLogProbMetric: 46.6119

Epoch 262: val_loss did not improve from 46.53216
196/196 - 35s - loss: 46.9291 - MinusLogProbMetric: 46.9291 - val_loss: 46.6119 - val_MinusLogProbMetric: 46.6119 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 263/1000
2023-10-27 19:48:53.489 
Epoch 263/1000 
	 loss: 46.8539, MinusLogProbMetric: 46.8539, val_loss: 47.3590, val_MinusLogProbMetric: 47.3590

Epoch 263: val_loss did not improve from 46.53216
196/196 - 35s - loss: 46.8539 - MinusLogProbMetric: 46.8539 - val_loss: 47.3590 - val_MinusLogProbMetric: 47.3590 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 264/1000
2023-10-27 19:49:27.543 
Epoch 264/1000 
	 loss: 47.2569, MinusLogProbMetric: 47.2569, val_loss: 49.8631, val_MinusLogProbMetric: 49.8631

Epoch 264: val_loss did not improve from 46.53216
196/196 - 34s - loss: 47.2569 - MinusLogProbMetric: 47.2569 - val_loss: 49.8631 - val_MinusLogProbMetric: 49.8631 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 265/1000
2023-10-27 19:50:01.256 
Epoch 265/1000 
	 loss: 47.7198, MinusLogProbMetric: 47.7198, val_loss: 49.0330, val_MinusLogProbMetric: 49.0330

Epoch 265: val_loss did not improve from 46.53216
196/196 - 34s - loss: 47.7198 - MinusLogProbMetric: 47.7198 - val_loss: 49.0330 - val_MinusLogProbMetric: 49.0330 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 266/1000
2023-10-27 19:50:35.613 
Epoch 266/1000 
	 loss: 47.4705, MinusLogProbMetric: 47.4705, val_loss: 48.1084, val_MinusLogProbMetric: 48.1084

Epoch 266: val_loss did not improve from 46.53216
196/196 - 34s - loss: 47.4705 - MinusLogProbMetric: 47.4705 - val_loss: 48.1084 - val_MinusLogProbMetric: 48.1084 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 267/1000
2023-10-27 19:51:09.458 
Epoch 267/1000 
	 loss: 46.4534, MinusLogProbMetric: 46.4534, val_loss: 46.2983, val_MinusLogProbMetric: 46.2983

Epoch 267: val_loss improved from 46.53216 to 46.29835, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 46.4534 - MinusLogProbMetric: 46.4534 - val_loss: 46.2983 - val_MinusLogProbMetric: 46.2983 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 268/1000
2023-10-27 19:51:43.939 
Epoch 268/1000 
	 loss: 47.1976, MinusLogProbMetric: 47.1976, val_loss: 47.2218, val_MinusLogProbMetric: 47.2218

Epoch 268: val_loss did not improve from 46.29835
196/196 - 34s - loss: 47.1976 - MinusLogProbMetric: 47.1976 - val_loss: 47.2218 - val_MinusLogProbMetric: 47.2218 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 269/1000
2023-10-27 19:52:17.979 
Epoch 269/1000 
	 loss: 47.1321, MinusLogProbMetric: 47.1321, val_loss: 48.7126, val_MinusLogProbMetric: 48.7126

Epoch 269: val_loss did not improve from 46.29835
196/196 - 34s - loss: 47.1321 - MinusLogProbMetric: 47.1321 - val_loss: 48.7126 - val_MinusLogProbMetric: 48.7126 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 270/1000
2023-10-27 19:52:52.349 
Epoch 270/1000 
	 loss: 47.1969, MinusLogProbMetric: 47.1969, val_loss: 49.6135, val_MinusLogProbMetric: 49.6135

Epoch 270: val_loss did not improve from 46.29835
196/196 - 34s - loss: 47.1969 - MinusLogProbMetric: 47.1969 - val_loss: 49.6135 - val_MinusLogProbMetric: 49.6135 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 271/1000
2023-10-27 19:53:26.200 
Epoch 271/1000 
	 loss: 47.3515, MinusLogProbMetric: 47.3515, val_loss: 52.5443, val_MinusLogProbMetric: 52.5443

Epoch 271: val_loss did not improve from 46.29835
196/196 - 34s - loss: 47.3515 - MinusLogProbMetric: 47.3515 - val_loss: 52.5443 - val_MinusLogProbMetric: 52.5443 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 272/1000
2023-10-27 19:54:01.561 
Epoch 272/1000 
	 loss: 47.0792, MinusLogProbMetric: 47.0792, val_loss: 47.1961, val_MinusLogProbMetric: 47.1961

Epoch 272: val_loss did not improve from 46.29835
196/196 - 35s - loss: 47.0792 - MinusLogProbMetric: 47.0792 - val_loss: 47.1961 - val_MinusLogProbMetric: 47.1961 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 273/1000
2023-10-27 19:54:36.185 
Epoch 273/1000 
	 loss: 47.6552, MinusLogProbMetric: 47.6552, val_loss: 49.3883, val_MinusLogProbMetric: 49.3883

Epoch 273: val_loss did not improve from 46.29835
196/196 - 35s - loss: 47.6552 - MinusLogProbMetric: 47.6552 - val_loss: 49.3883 - val_MinusLogProbMetric: 49.3883 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 274/1000
2023-10-27 19:55:10.550 
Epoch 274/1000 
	 loss: 46.6201, MinusLogProbMetric: 46.6201, val_loss: 47.4550, val_MinusLogProbMetric: 47.4550

Epoch 274: val_loss did not improve from 46.29835
196/196 - 34s - loss: 46.6201 - MinusLogProbMetric: 46.6201 - val_loss: 47.4550 - val_MinusLogProbMetric: 47.4550 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 275/1000
2023-10-27 19:55:42.603 
Epoch 275/1000 
	 loss: 47.1683, MinusLogProbMetric: 47.1683, val_loss: 48.4457, val_MinusLogProbMetric: 48.4457

Epoch 275: val_loss did not improve from 46.29835
196/196 - 32s - loss: 47.1683 - MinusLogProbMetric: 47.1683 - val_loss: 48.4457 - val_MinusLogProbMetric: 48.4457 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 276/1000
2023-10-27 19:56:09.766 
Epoch 276/1000 
	 loss: 46.6765, MinusLogProbMetric: 46.6765, val_loss: 46.6905, val_MinusLogProbMetric: 46.6905

Epoch 276: val_loss did not improve from 46.29835
196/196 - 27s - loss: 46.6765 - MinusLogProbMetric: 46.6765 - val_loss: 46.6905 - val_MinusLogProbMetric: 46.6905 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 277/1000
2023-10-27 19:56:36.955 
Epoch 277/1000 
	 loss: 49.3604, MinusLogProbMetric: 49.3604, val_loss: 50.8714, val_MinusLogProbMetric: 50.8714

Epoch 277: val_loss did not improve from 46.29835
196/196 - 27s - loss: 49.3604 - MinusLogProbMetric: 49.3604 - val_loss: 50.8714 - val_MinusLogProbMetric: 50.8714 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 278/1000
2023-10-27 19:57:04.557 
Epoch 278/1000 
	 loss: 46.7873, MinusLogProbMetric: 46.7873, val_loss: 46.8396, val_MinusLogProbMetric: 46.8396

Epoch 278: val_loss did not improve from 46.29835
196/196 - 28s - loss: 46.7873 - MinusLogProbMetric: 46.7873 - val_loss: 46.8396 - val_MinusLogProbMetric: 46.8396 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 279/1000
2023-10-27 19:57:32.004 
Epoch 279/1000 
	 loss: 47.9757, MinusLogProbMetric: 47.9757, val_loss: 48.0854, val_MinusLogProbMetric: 48.0854

Epoch 279: val_loss did not improve from 46.29835
196/196 - 27s - loss: 47.9757 - MinusLogProbMetric: 47.9757 - val_loss: 48.0854 - val_MinusLogProbMetric: 48.0854 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 280/1000
2023-10-27 19:58:02.007 
Epoch 280/1000 
	 loss: 47.0849, MinusLogProbMetric: 47.0849, val_loss: 50.4965, val_MinusLogProbMetric: 50.4965

Epoch 280: val_loss did not improve from 46.29835
196/196 - 30s - loss: 47.0849 - MinusLogProbMetric: 47.0849 - val_loss: 50.4965 - val_MinusLogProbMetric: 50.4965 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 281/1000
2023-10-27 19:58:36.392 
Epoch 281/1000 
	 loss: 46.7591, MinusLogProbMetric: 46.7591, val_loss: 47.7026, val_MinusLogProbMetric: 47.7026

Epoch 281: val_loss did not improve from 46.29835
196/196 - 34s - loss: 46.7591 - MinusLogProbMetric: 46.7591 - val_loss: 47.7026 - val_MinusLogProbMetric: 47.7026 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 282/1000
2023-10-27 19:59:10.661 
Epoch 282/1000 
	 loss: 47.1952, MinusLogProbMetric: 47.1952, val_loss: 47.4547, val_MinusLogProbMetric: 47.4547

Epoch 282: val_loss did not improve from 46.29835
196/196 - 34s - loss: 47.1952 - MinusLogProbMetric: 47.1952 - val_loss: 47.4547 - val_MinusLogProbMetric: 47.4547 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 283/1000
2023-10-27 19:59:45.498 
Epoch 283/1000 
	 loss: 46.4819, MinusLogProbMetric: 46.4819, val_loss: 46.6554, val_MinusLogProbMetric: 46.6554

Epoch 283: val_loss did not improve from 46.29835
196/196 - 35s - loss: 46.4819 - MinusLogProbMetric: 46.4819 - val_loss: 46.6554 - val_MinusLogProbMetric: 46.6554 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 284/1000
2023-10-27 20:00:19.252 
Epoch 284/1000 
	 loss: 47.0991, MinusLogProbMetric: 47.0991, val_loss: 47.4835, val_MinusLogProbMetric: 47.4835

Epoch 284: val_loss did not improve from 46.29835
196/196 - 34s - loss: 47.0991 - MinusLogProbMetric: 47.0991 - val_loss: 47.4835 - val_MinusLogProbMetric: 47.4835 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 285/1000
2023-10-27 20:00:48.405 
Epoch 285/1000 
	 loss: 46.7562, MinusLogProbMetric: 46.7562, val_loss: 48.1578, val_MinusLogProbMetric: 48.1578

Epoch 285: val_loss did not improve from 46.29835
196/196 - 29s - loss: 46.7562 - MinusLogProbMetric: 46.7562 - val_loss: 48.1578 - val_MinusLogProbMetric: 48.1578 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 286/1000
2023-10-27 20:01:15.845 
Epoch 286/1000 
	 loss: 46.9501, MinusLogProbMetric: 46.9501, val_loss: 50.0756, val_MinusLogProbMetric: 50.0756

Epoch 286: val_loss did not improve from 46.29835
196/196 - 27s - loss: 46.9501 - MinusLogProbMetric: 46.9501 - val_loss: 50.0756 - val_MinusLogProbMetric: 50.0756 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 287/1000
2023-10-27 20:01:43.258 
Epoch 287/1000 
	 loss: 47.3470, MinusLogProbMetric: 47.3470, val_loss: 46.7356, val_MinusLogProbMetric: 46.7356

Epoch 287: val_loss did not improve from 46.29835
196/196 - 27s - loss: 47.3470 - MinusLogProbMetric: 47.3470 - val_loss: 46.7356 - val_MinusLogProbMetric: 46.7356 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 288/1000
2023-10-27 20:02:10.554 
Epoch 288/1000 
	 loss: 46.3765, MinusLogProbMetric: 46.3765, val_loss: 50.7117, val_MinusLogProbMetric: 50.7117

Epoch 288: val_loss did not improve from 46.29835
196/196 - 27s - loss: 46.3765 - MinusLogProbMetric: 46.3765 - val_loss: 50.7117 - val_MinusLogProbMetric: 50.7117 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 289/1000
2023-10-27 20:02:39.091 
Epoch 289/1000 
	 loss: 46.6854, MinusLogProbMetric: 46.6854, val_loss: 48.4038, val_MinusLogProbMetric: 48.4038

Epoch 289: val_loss did not improve from 46.29835
196/196 - 29s - loss: 46.6854 - MinusLogProbMetric: 46.6854 - val_loss: 48.4038 - val_MinusLogProbMetric: 48.4038 - lr: 3.3333e-04 - 29s/epoch - 146ms/step
Epoch 290/1000
2023-10-27 20:03:13.243 
Epoch 290/1000 
	 loss: 46.5512, MinusLogProbMetric: 46.5512, val_loss: 47.1816, val_MinusLogProbMetric: 47.1816

Epoch 290: val_loss did not improve from 46.29835
196/196 - 34s - loss: 46.5512 - MinusLogProbMetric: 46.5512 - val_loss: 47.1816 - val_MinusLogProbMetric: 47.1816 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 291/1000
2023-10-27 20:03:47.496 
Epoch 291/1000 
	 loss: 47.4582, MinusLogProbMetric: 47.4582, val_loss: 49.1150, val_MinusLogProbMetric: 49.1150

Epoch 291: val_loss did not improve from 46.29835
196/196 - 34s - loss: 47.4582 - MinusLogProbMetric: 47.4582 - val_loss: 49.1150 - val_MinusLogProbMetric: 49.1150 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 292/1000
2023-10-27 20:04:21.592 
Epoch 292/1000 
	 loss: 47.1164, MinusLogProbMetric: 47.1164, val_loss: 47.8997, val_MinusLogProbMetric: 47.8997

Epoch 292: val_loss did not improve from 46.29835
196/196 - 34s - loss: 47.1164 - MinusLogProbMetric: 47.1164 - val_loss: 47.8997 - val_MinusLogProbMetric: 47.8997 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 293/1000
2023-10-27 20:04:56.305 
Epoch 293/1000 
	 loss: 46.6597, MinusLogProbMetric: 46.6597, val_loss: 47.5029, val_MinusLogProbMetric: 47.5029

Epoch 293: val_loss did not improve from 46.29835
196/196 - 35s - loss: 46.6597 - MinusLogProbMetric: 46.6597 - val_loss: 47.5029 - val_MinusLogProbMetric: 47.5029 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 294/1000
2023-10-27 20:05:30.680 
Epoch 294/1000 
	 loss: 46.2990, MinusLogProbMetric: 46.2990, val_loss: 46.0689, val_MinusLogProbMetric: 46.0689

Epoch 294: val_loss improved from 46.29835 to 46.06893, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 46.2990 - MinusLogProbMetric: 46.2990 - val_loss: 46.0689 - val_MinusLogProbMetric: 46.0689 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 295/1000
2023-10-27 20:06:05.277 
Epoch 295/1000 
	 loss: 46.2669, MinusLogProbMetric: 46.2669, val_loss: 46.5327, val_MinusLogProbMetric: 46.5327

Epoch 295: val_loss did not improve from 46.06893
196/196 - 34s - loss: 46.2669 - MinusLogProbMetric: 46.2669 - val_loss: 46.5327 - val_MinusLogProbMetric: 46.5327 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 296/1000
2023-10-27 20:06:38.506 
Epoch 296/1000 
	 loss: 46.5157, MinusLogProbMetric: 46.5157, val_loss: 49.0092, val_MinusLogProbMetric: 49.0092

Epoch 296: val_loss did not improve from 46.06893
196/196 - 33s - loss: 46.5157 - MinusLogProbMetric: 46.5157 - val_loss: 49.0092 - val_MinusLogProbMetric: 49.0092 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 297/1000
2023-10-27 20:07:12.423 
Epoch 297/1000 
	 loss: 48.0703, MinusLogProbMetric: 48.0703, val_loss: 48.3508, val_MinusLogProbMetric: 48.3508

Epoch 297: val_loss did not improve from 46.06893
196/196 - 34s - loss: 48.0703 - MinusLogProbMetric: 48.0703 - val_loss: 48.3508 - val_MinusLogProbMetric: 48.3508 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 298/1000
2023-10-27 20:07:46.341 
Epoch 298/1000 
	 loss: 46.5619, MinusLogProbMetric: 46.5619, val_loss: 49.5886, val_MinusLogProbMetric: 49.5886

Epoch 298: val_loss did not improve from 46.06893
196/196 - 34s - loss: 46.5619 - MinusLogProbMetric: 46.5619 - val_loss: 49.5886 - val_MinusLogProbMetric: 49.5886 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 299/1000
2023-10-27 20:08:20.634 
Epoch 299/1000 
	 loss: 46.8812, MinusLogProbMetric: 46.8812, val_loss: 46.4516, val_MinusLogProbMetric: 46.4516

Epoch 299: val_loss did not improve from 46.06893
196/196 - 34s - loss: 46.8812 - MinusLogProbMetric: 46.8812 - val_loss: 46.4516 - val_MinusLogProbMetric: 46.4516 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 300/1000
2023-10-27 20:08:54.723 
Epoch 300/1000 
	 loss: 46.5559, MinusLogProbMetric: 46.5559, val_loss: 46.9071, val_MinusLogProbMetric: 46.9071

Epoch 300: val_loss did not improve from 46.06893
196/196 - 34s - loss: 46.5559 - MinusLogProbMetric: 46.5559 - val_loss: 46.9071 - val_MinusLogProbMetric: 46.9071 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 301/1000
2023-10-27 20:09:28.747 
Epoch 301/1000 
	 loss: 46.3816, MinusLogProbMetric: 46.3816, val_loss: 47.5877, val_MinusLogProbMetric: 47.5877

Epoch 301: val_loss did not improve from 46.06893
196/196 - 34s - loss: 46.3816 - MinusLogProbMetric: 46.3816 - val_loss: 47.5877 - val_MinusLogProbMetric: 47.5877 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 302/1000
2023-10-27 20:10:01.451 
Epoch 302/1000 
	 loss: 46.3730, MinusLogProbMetric: 46.3730, val_loss: 50.3617, val_MinusLogProbMetric: 50.3617

Epoch 302: val_loss did not improve from 46.06893
196/196 - 33s - loss: 46.3730 - MinusLogProbMetric: 46.3730 - val_loss: 50.3617 - val_MinusLogProbMetric: 50.3617 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 303/1000
2023-10-27 20:10:35.532 
Epoch 303/1000 
	 loss: 46.7609, MinusLogProbMetric: 46.7609, val_loss: 57.4930, val_MinusLogProbMetric: 57.4930

Epoch 303: val_loss did not improve from 46.06893
196/196 - 34s - loss: 46.7609 - MinusLogProbMetric: 46.7609 - val_loss: 57.4930 - val_MinusLogProbMetric: 57.4930 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 304/1000
2023-10-27 20:11:09.674 
Epoch 304/1000 
	 loss: 48.2880, MinusLogProbMetric: 48.2880, val_loss: 46.8081, val_MinusLogProbMetric: 46.8081

Epoch 304: val_loss did not improve from 46.06893
196/196 - 34s - loss: 48.2880 - MinusLogProbMetric: 48.2880 - val_loss: 46.8081 - val_MinusLogProbMetric: 46.8081 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 305/1000
2023-10-27 20:11:43.953 
Epoch 305/1000 
	 loss: 46.0509, MinusLogProbMetric: 46.0509, val_loss: 47.1620, val_MinusLogProbMetric: 47.1620

Epoch 305: val_loss did not improve from 46.06893
196/196 - 34s - loss: 46.0509 - MinusLogProbMetric: 46.0509 - val_loss: 47.1620 - val_MinusLogProbMetric: 47.1620 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 306/1000
2023-10-27 20:12:17.934 
Epoch 306/1000 
	 loss: 46.3391, MinusLogProbMetric: 46.3391, val_loss: 46.4010, val_MinusLogProbMetric: 46.4010

Epoch 306: val_loss did not improve from 46.06893
196/196 - 34s - loss: 46.3391 - MinusLogProbMetric: 46.3391 - val_loss: 46.4010 - val_MinusLogProbMetric: 46.4010 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 307/1000
2023-10-27 20:12:52.244 
Epoch 307/1000 
	 loss: 46.2954, MinusLogProbMetric: 46.2954, val_loss: 46.0624, val_MinusLogProbMetric: 46.0624

Epoch 307: val_loss improved from 46.06893 to 46.06244, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 46.2954 - MinusLogProbMetric: 46.2954 - val_loss: 46.0624 - val_MinusLogProbMetric: 46.0624 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 308/1000
2023-10-27 20:13:26.557 
Epoch 308/1000 
	 loss: 46.0631, MinusLogProbMetric: 46.0631, val_loss: 46.1015, val_MinusLogProbMetric: 46.1015

Epoch 308: val_loss did not improve from 46.06244
196/196 - 34s - loss: 46.0631 - MinusLogProbMetric: 46.0631 - val_loss: 46.1015 - val_MinusLogProbMetric: 46.1015 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 309/1000
2023-10-27 20:14:00.368 
Epoch 309/1000 
	 loss: 46.7474, MinusLogProbMetric: 46.7474, val_loss: 46.2936, val_MinusLogProbMetric: 46.2936

Epoch 309: val_loss did not improve from 46.06244
196/196 - 34s - loss: 46.7474 - MinusLogProbMetric: 46.7474 - val_loss: 46.2936 - val_MinusLogProbMetric: 46.2936 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 310/1000
2023-10-27 20:14:34.629 
Epoch 310/1000 
	 loss: 47.1190, MinusLogProbMetric: 47.1190, val_loss: 46.0316, val_MinusLogProbMetric: 46.0316

Epoch 310: val_loss improved from 46.06244 to 46.03156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 47.1190 - MinusLogProbMetric: 47.1190 - val_loss: 46.0316 - val_MinusLogProbMetric: 46.0316 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 311/1000
2023-10-27 20:15:09.281 
Epoch 311/1000 
	 loss: 46.5107, MinusLogProbMetric: 46.5107, val_loss: 46.4672, val_MinusLogProbMetric: 46.4672

Epoch 311: val_loss did not improve from 46.03156
196/196 - 34s - loss: 46.5107 - MinusLogProbMetric: 46.5107 - val_loss: 46.4672 - val_MinusLogProbMetric: 46.4672 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 312/1000
2023-10-27 20:15:43.390 
Epoch 312/1000 
	 loss: 46.1623, MinusLogProbMetric: 46.1623, val_loss: 48.6108, val_MinusLogProbMetric: 48.6108

Epoch 312: val_loss did not improve from 46.03156
196/196 - 34s - loss: 46.1623 - MinusLogProbMetric: 46.1623 - val_loss: 48.6108 - val_MinusLogProbMetric: 48.6108 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 313/1000
2023-10-27 20:16:17.324 
Epoch 313/1000 
	 loss: 46.2912, MinusLogProbMetric: 46.2912, val_loss: 47.3677, val_MinusLogProbMetric: 47.3677

Epoch 313: val_loss did not improve from 46.03156
196/196 - 34s - loss: 46.2912 - MinusLogProbMetric: 46.2912 - val_loss: 47.3677 - val_MinusLogProbMetric: 47.3677 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 314/1000
2023-10-27 20:16:51.556 
Epoch 314/1000 
	 loss: 46.5894, MinusLogProbMetric: 46.5894, val_loss: 46.3817, val_MinusLogProbMetric: 46.3817

Epoch 314: val_loss did not improve from 46.03156
196/196 - 34s - loss: 46.5894 - MinusLogProbMetric: 46.5894 - val_loss: 46.3817 - val_MinusLogProbMetric: 46.3817 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 315/1000
2023-10-27 20:17:26.556 
Epoch 315/1000 
	 loss: 46.2832, MinusLogProbMetric: 46.2832, val_loss: 46.3641, val_MinusLogProbMetric: 46.3641

Epoch 315: val_loss did not improve from 46.03156
196/196 - 35s - loss: 46.2832 - MinusLogProbMetric: 46.2832 - val_loss: 46.3641 - val_MinusLogProbMetric: 46.3641 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 316/1000
2023-10-27 20:18:01.408 
Epoch 316/1000 
	 loss: 45.9056, MinusLogProbMetric: 45.9056, val_loss: 46.2149, val_MinusLogProbMetric: 46.2149

Epoch 316: val_loss did not improve from 46.03156
196/196 - 35s - loss: 45.9056 - MinusLogProbMetric: 45.9056 - val_loss: 46.2149 - val_MinusLogProbMetric: 46.2149 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 317/1000
2023-10-27 20:18:35.521 
Epoch 317/1000 
	 loss: 46.2006, MinusLogProbMetric: 46.2006, val_loss: 46.5541, val_MinusLogProbMetric: 46.5541

Epoch 317: val_loss did not improve from 46.03156
196/196 - 34s - loss: 46.2006 - MinusLogProbMetric: 46.2006 - val_loss: 46.5541 - val_MinusLogProbMetric: 46.5541 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 318/1000
2023-10-27 20:19:10.161 
Epoch 318/1000 
	 loss: 47.4753, MinusLogProbMetric: 47.4753, val_loss: 46.6858, val_MinusLogProbMetric: 46.6858

Epoch 318: val_loss did not improve from 46.03156
196/196 - 35s - loss: 47.4753 - MinusLogProbMetric: 47.4753 - val_loss: 46.6858 - val_MinusLogProbMetric: 46.6858 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 319/1000
2023-10-27 20:19:44.131 
Epoch 319/1000 
	 loss: 45.9993, MinusLogProbMetric: 45.9993, val_loss: 46.4281, val_MinusLogProbMetric: 46.4281

Epoch 319: val_loss did not improve from 46.03156
196/196 - 34s - loss: 45.9993 - MinusLogProbMetric: 45.9993 - val_loss: 46.4281 - val_MinusLogProbMetric: 46.4281 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 320/1000
2023-10-27 20:20:18.665 
Epoch 320/1000 
	 loss: 46.3172, MinusLogProbMetric: 46.3172, val_loss: 46.9810, val_MinusLogProbMetric: 46.9810

Epoch 320: val_loss did not improve from 46.03156
196/196 - 35s - loss: 46.3172 - MinusLogProbMetric: 46.3172 - val_loss: 46.9810 - val_MinusLogProbMetric: 46.9810 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 321/1000
2023-10-27 20:20:52.614 
Epoch 321/1000 
	 loss: 46.5095, MinusLogProbMetric: 46.5095, val_loss: 46.0615, val_MinusLogProbMetric: 46.0615

Epoch 321: val_loss did not improve from 46.03156
196/196 - 34s - loss: 46.5095 - MinusLogProbMetric: 46.5095 - val_loss: 46.0615 - val_MinusLogProbMetric: 46.0615 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 322/1000
2023-10-27 20:21:26.169 
Epoch 322/1000 
	 loss: 46.8138, MinusLogProbMetric: 46.8138, val_loss: 47.2438, val_MinusLogProbMetric: 47.2438

Epoch 322: val_loss did not improve from 46.03156
196/196 - 34s - loss: 46.8138 - MinusLogProbMetric: 46.8138 - val_loss: 47.2438 - val_MinusLogProbMetric: 47.2438 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 323/1000
2023-10-27 20:22:00.240 
Epoch 323/1000 
	 loss: 46.0181, MinusLogProbMetric: 46.0181, val_loss: 47.6654, val_MinusLogProbMetric: 47.6654

Epoch 323: val_loss did not improve from 46.03156
196/196 - 34s - loss: 46.0181 - MinusLogProbMetric: 46.0181 - val_loss: 47.6654 - val_MinusLogProbMetric: 47.6654 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 324/1000
2023-10-27 20:22:34.471 
Epoch 324/1000 
	 loss: 46.4294, MinusLogProbMetric: 46.4294, val_loss: 49.1187, val_MinusLogProbMetric: 49.1187

Epoch 324: val_loss did not improve from 46.03156
196/196 - 34s - loss: 46.4294 - MinusLogProbMetric: 46.4294 - val_loss: 49.1187 - val_MinusLogProbMetric: 49.1187 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 325/1000
2023-10-27 20:23:08.575 
Epoch 325/1000 
	 loss: 46.8134, MinusLogProbMetric: 46.8134, val_loss: 46.6336, val_MinusLogProbMetric: 46.6336

Epoch 325: val_loss did not improve from 46.03156
196/196 - 34s - loss: 46.8134 - MinusLogProbMetric: 46.8134 - val_loss: 46.6336 - val_MinusLogProbMetric: 46.6336 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 326/1000
2023-10-27 20:23:42.815 
Epoch 326/1000 
	 loss: 46.1509, MinusLogProbMetric: 46.1509, val_loss: 46.3577, val_MinusLogProbMetric: 46.3577

Epoch 326: val_loss did not improve from 46.03156
196/196 - 34s - loss: 46.1509 - MinusLogProbMetric: 46.1509 - val_loss: 46.3577 - val_MinusLogProbMetric: 46.3577 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 327/1000
2023-10-27 20:24:16.646 
Epoch 327/1000 
	 loss: 45.7832, MinusLogProbMetric: 45.7832, val_loss: 45.9957, val_MinusLogProbMetric: 45.9957

Epoch 327: val_loss improved from 46.03156 to 45.99574, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 45.7832 - MinusLogProbMetric: 45.7832 - val_loss: 45.9957 - val_MinusLogProbMetric: 45.9957 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 328/1000
2023-10-27 20:24:51.578 
Epoch 328/1000 
	 loss: 45.9975, MinusLogProbMetric: 45.9975, val_loss: 46.2486, val_MinusLogProbMetric: 46.2486

Epoch 328: val_loss did not improve from 45.99574
196/196 - 34s - loss: 45.9975 - MinusLogProbMetric: 45.9975 - val_loss: 46.2486 - val_MinusLogProbMetric: 46.2486 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 329/1000
2023-10-27 20:25:25.763 
Epoch 329/1000 
	 loss: 45.8687, MinusLogProbMetric: 45.8687, val_loss: 46.3942, val_MinusLogProbMetric: 46.3942

Epoch 329: val_loss did not improve from 45.99574
196/196 - 34s - loss: 45.8687 - MinusLogProbMetric: 45.8687 - val_loss: 46.3942 - val_MinusLogProbMetric: 46.3942 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 330/1000
2023-10-27 20:25:59.762 
Epoch 330/1000 
	 loss: 46.7999, MinusLogProbMetric: 46.7999, val_loss: 46.3658, val_MinusLogProbMetric: 46.3658

Epoch 330: val_loss did not improve from 45.99574
196/196 - 34s - loss: 46.7999 - MinusLogProbMetric: 46.7999 - val_loss: 46.3658 - val_MinusLogProbMetric: 46.3658 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 331/1000
2023-10-27 20:26:34.022 
Epoch 331/1000 
	 loss: 47.2661, MinusLogProbMetric: 47.2661, val_loss: 47.7881, val_MinusLogProbMetric: 47.7881

Epoch 331: val_loss did not improve from 45.99574
196/196 - 34s - loss: 47.2661 - MinusLogProbMetric: 47.2661 - val_loss: 47.7881 - val_MinusLogProbMetric: 47.7881 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 332/1000
2023-10-27 20:27:08.711 
Epoch 332/1000 
	 loss: 46.2315, MinusLogProbMetric: 46.2315, val_loss: 47.3153, val_MinusLogProbMetric: 47.3153

Epoch 332: val_loss did not improve from 45.99574
196/196 - 35s - loss: 46.2315 - MinusLogProbMetric: 46.2315 - val_loss: 47.3153 - val_MinusLogProbMetric: 47.3153 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 333/1000
2023-10-27 20:27:42.962 
Epoch 333/1000 
	 loss: 46.2644, MinusLogProbMetric: 46.2644, val_loss: 48.2636, val_MinusLogProbMetric: 48.2636

Epoch 333: val_loss did not improve from 45.99574
196/196 - 34s - loss: 46.2644 - MinusLogProbMetric: 46.2644 - val_loss: 48.2636 - val_MinusLogProbMetric: 48.2636 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 334/1000
2023-10-27 20:28:17.154 
Epoch 334/1000 
	 loss: 46.2287, MinusLogProbMetric: 46.2287, val_loss: 48.8461, val_MinusLogProbMetric: 48.8461

Epoch 334: val_loss did not improve from 45.99574
196/196 - 34s - loss: 46.2287 - MinusLogProbMetric: 46.2287 - val_loss: 48.8461 - val_MinusLogProbMetric: 48.8461 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 335/1000
2023-10-27 20:28:51.381 
Epoch 335/1000 
	 loss: 46.0358, MinusLogProbMetric: 46.0358, val_loss: 47.4797, val_MinusLogProbMetric: 47.4797

Epoch 335: val_loss did not improve from 45.99574
196/196 - 34s - loss: 46.0358 - MinusLogProbMetric: 46.0358 - val_loss: 47.4797 - val_MinusLogProbMetric: 47.4797 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 336/1000
2023-10-27 20:29:26.110 
Epoch 336/1000 
	 loss: 45.9398, MinusLogProbMetric: 45.9398, val_loss: 46.3867, val_MinusLogProbMetric: 46.3867

Epoch 336: val_loss did not improve from 45.99574
196/196 - 35s - loss: 45.9398 - MinusLogProbMetric: 45.9398 - val_loss: 46.3867 - val_MinusLogProbMetric: 46.3867 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 337/1000
2023-10-27 20:30:01.517 
Epoch 337/1000 
	 loss: 46.0697, MinusLogProbMetric: 46.0697, val_loss: 47.1230, val_MinusLogProbMetric: 47.1230

Epoch 337: val_loss did not improve from 45.99574
196/196 - 35s - loss: 46.0697 - MinusLogProbMetric: 46.0697 - val_loss: 47.1230 - val_MinusLogProbMetric: 47.1230 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 338/1000
2023-10-27 20:30:36.963 
Epoch 338/1000 
	 loss: 46.4412, MinusLogProbMetric: 46.4412, val_loss: 49.7774, val_MinusLogProbMetric: 49.7774

Epoch 338: val_loss did not improve from 45.99574
196/196 - 35s - loss: 46.4412 - MinusLogProbMetric: 46.4412 - val_loss: 49.7774 - val_MinusLogProbMetric: 49.7774 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 339/1000
2023-10-27 20:31:10.594 
Epoch 339/1000 
	 loss: 46.4747, MinusLogProbMetric: 46.4747, val_loss: 45.7137, val_MinusLogProbMetric: 45.7137

Epoch 339: val_loss improved from 45.99574 to 45.71367, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 46.4747 - MinusLogProbMetric: 46.4747 - val_loss: 45.7137 - val_MinusLogProbMetric: 45.7137 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 340/1000
2023-10-27 20:31:45.316 
Epoch 340/1000 
	 loss: 46.5916, MinusLogProbMetric: 46.5916, val_loss: 49.0243, val_MinusLogProbMetric: 49.0243

Epoch 340: val_loss did not improve from 45.71367
196/196 - 34s - loss: 46.5916 - MinusLogProbMetric: 46.5916 - val_loss: 49.0243 - val_MinusLogProbMetric: 49.0243 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 341/1000
2023-10-27 20:32:19.588 
Epoch 341/1000 
	 loss: 46.5962, MinusLogProbMetric: 46.5962, val_loss: 47.2432, val_MinusLogProbMetric: 47.2432

Epoch 341: val_loss did not improve from 45.71367
196/196 - 34s - loss: 46.5962 - MinusLogProbMetric: 46.5962 - val_loss: 47.2432 - val_MinusLogProbMetric: 47.2432 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 342/1000
2023-10-27 20:32:54.312 
Epoch 342/1000 
	 loss: 46.0519, MinusLogProbMetric: 46.0519, val_loss: 50.0493, val_MinusLogProbMetric: 50.0493

Epoch 342: val_loss did not improve from 45.71367
196/196 - 35s - loss: 46.0519 - MinusLogProbMetric: 46.0519 - val_loss: 50.0493 - val_MinusLogProbMetric: 50.0493 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 343/1000
2023-10-27 20:33:28.518 
Epoch 343/1000 
	 loss: 46.6234, MinusLogProbMetric: 46.6234, val_loss: 46.3429, val_MinusLogProbMetric: 46.3429

Epoch 343: val_loss did not improve from 45.71367
196/196 - 34s - loss: 46.6234 - MinusLogProbMetric: 46.6234 - val_loss: 46.3429 - val_MinusLogProbMetric: 46.3429 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 344/1000
2023-10-27 20:34:03.042 
Epoch 344/1000 
	 loss: 46.4153, MinusLogProbMetric: 46.4153, val_loss: 47.9227, val_MinusLogProbMetric: 47.9227

Epoch 344: val_loss did not improve from 45.71367
196/196 - 35s - loss: 46.4153 - MinusLogProbMetric: 46.4153 - val_loss: 47.9227 - val_MinusLogProbMetric: 47.9227 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 345/1000
2023-10-27 20:34:37.422 
Epoch 345/1000 
	 loss: 46.0214, MinusLogProbMetric: 46.0214, val_loss: 45.9966, val_MinusLogProbMetric: 45.9966

Epoch 345: val_loss did not improve from 45.71367
196/196 - 34s - loss: 46.0214 - MinusLogProbMetric: 46.0214 - val_loss: 45.9966 - val_MinusLogProbMetric: 45.9966 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 346/1000
2023-10-27 20:35:11.170 
Epoch 346/1000 
	 loss: 46.0116, MinusLogProbMetric: 46.0116, val_loss: 45.9351, val_MinusLogProbMetric: 45.9351

Epoch 346: val_loss did not improve from 45.71367
196/196 - 34s - loss: 46.0116 - MinusLogProbMetric: 46.0116 - val_loss: 45.9351 - val_MinusLogProbMetric: 45.9351 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 347/1000
2023-10-27 20:35:45.481 
Epoch 347/1000 
	 loss: 46.1707, MinusLogProbMetric: 46.1707, val_loss: 48.5642, val_MinusLogProbMetric: 48.5642

Epoch 347: val_loss did not improve from 45.71367
196/196 - 34s - loss: 46.1707 - MinusLogProbMetric: 46.1707 - val_loss: 48.5642 - val_MinusLogProbMetric: 48.5642 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 348/1000
2023-10-27 20:36:20.999 
Epoch 348/1000 
	 loss: 46.1297, MinusLogProbMetric: 46.1297, val_loss: 46.5407, val_MinusLogProbMetric: 46.5407

Epoch 348: val_loss did not improve from 45.71367
196/196 - 36s - loss: 46.1297 - MinusLogProbMetric: 46.1297 - val_loss: 46.5407 - val_MinusLogProbMetric: 46.5407 - lr: 3.3333e-04 - 36s/epoch - 181ms/step
Epoch 349/1000
2023-10-27 20:36:55.642 
Epoch 349/1000 
	 loss: 47.4635, MinusLogProbMetric: 47.4635, val_loss: 46.2420, val_MinusLogProbMetric: 46.2420

Epoch 349: val_loss did not improve from 45.71367
196/196 - 35s - loss: 47.4635 - MinusLogProbMetric: 47.4635 - val_loss: 46.2420 - val_MinusLogProbMetric: 46.2420 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 350/1000
2023-10-27 20:37:30.382 
Epoch 350/1000 
	 loss: 46.8095, MinusLogProbMetric: 46.8095, val_loss: 49.1410, val_MinusLogProbMetric: 49.1410

Epoch 350: val_loss did not improve from 45.71367
196/196 - 35s - loss: 46.8095 - MinusLogProbMetric: 46.8095 - val_loss: 49.1410 - val_MinusLogProbMetric: 49.1410 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 351/1000
2023-10-27 20:38:05.970 
Epoch 351/1000 
	 loss: 45.8733, MinusLogProbMetric: 45.8733, val_loss: 46.1675, val_MinusLogProbMetric: 46.1675

Epoch 351: val_loss did not improve from 45.71367
196/196 - 36s - loss: 45.8733 - MinusLogProbMetric: 45.8733 - val_loss: 46.1675 - val_MinusLogProbMetric: 46.1675 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 352/1000
2023-10-27 20:38:40.657 
Epoch 352/1000 
	 loss: 45.5616, MinusLogProbMetric: 45.5616, val_loss: 46.7499, val_MinusLogProbMetric: 46.7499

Epoch 352: val_loss did not improve from 45.71367
196/196 - 35s - loss: 45.5616 - MinusLogProbMetric: 45.5616 - val_loss: 46.7499 - val_MinusLogProbMetric: 46.7499 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 353/1000
2023-10-27 20:39:15.691 
Epoch 353/1000 
	 loss: 46.1057, MinusLogProbMetric: 46.1057, val_loss: 46.7496, val_MinusLogProbMetric: 46.7496

Epoch 353: val_loss did not improve from 45.71367
196/196 - 35s - loss: 46.1057 - MinusLogProbMetric: 46.1057 - val_loss: 46.7496 - val_MinusLogProbMetric: 46.7496 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 354/1000
2023-10-27 20:39:50.972 
Epoch 354/1000 
	 loss: 46.1364, MinusLogProbMetric: 46.1364, val_loss: 49.2515, val_MinusLogProbMetric: 49.2515

Epoch 354: val_loss did not improve from 45.71367
196/196 - 35s - loss: 46.1364 - MinusLogProbMetric: 46.1364 - val_loss: 49.2515 - val_MinusLogProbMetric: 49.2515 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 355/1000
2023-10-27 20:40:25.732 
Epoch 355/1000 
	 loss: 46.5994, MinusLogProbMetric: 46.5994, val_loss: 54.5939, val_MinusLogProbMetric: 54.5939

Epoch 355: val_loss did not improve from 45.71367
196/196 - 35s - loss: 46.5994 - MinusLogProbMetric: 46.5994 - val_loss: 54.5939 - val_MinusLogProbMetric: 54.5939 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 356/1000
2023-10-27 20:41:00.121 
Epoch 356/1000 
	 loss: 46.4254, MinusLogProbMetric: 46.4254, val_loss: 47.2326, val_MinusLogProbMetric: 47.2326

Epoch 356: val_loss did not improve from 45.71367
196/196 - 34s - loss: 46.4254 - MinusLogProbMetric: 46.4254 - val_loss: 47.2326 - val_MinusLogProbMetric: 47.2326 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 357/1000
2023-10-27 20:41:35.271 
Epoch 357/1000 
	 loss: 45.7317, MinusLogProbMetric: 45.7317, val_loss: 45.6576, val_MinusLogProbMetric: 45.6576

Epoch 357: val_loss improved from 45.71367 to 45.65762, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 36s - loss: 45.7317 - MinusLogProbMetric: 45.7317 - val_loss: 45.6576 - val_MinusLogProbMetric: 45.6576 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 358/1000
2023-10-27 20:42:09.701 
Epoch 358/1000 
	 loss: 45.9459, MinusLogProbMetric: 45.9459, val_loss: 48.2710, val_MinusLogProbMetric: 48.2710

Epoch 358: val_loss did not improve from 45.65762
196/196 - 34s - loss: 45.9459 - MinusLogProbMetric: 45.9459 - val_loss: 48.2710 - val_MinusLogProbMetric: 48.2710 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 359/1000
2023-10-27 20:42:43.911 
Epoch 359/1000 
	 loss: 46.5221, MinusLogProbMetric: 46.5221, val_loss: 45.8188, val_MinusLogProbMetric: 45.8188

Epoch 359: val_loss did not improve from 45.65762
196/196 - 34s - loss: 46.5221 - MinusLogProbMetric: 46.5221 - val_loss: 45.8188 - val_MinusLogProbMetric: 45.8188 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 360/1000
2023-10-27 20:43:17.882 
Epoch 360/1000 
	 loss: 45.6118, MinusLogProbMetric: 45.6118, val_loss: 46.6506, val_MinusLogProbMetric: 46.6506

Epoch 360: val_loss did not improve from 45.65762
196/196 - 34s - loss: 45.6118 - MinusLogProbMetric: 45.6118 - val_loss: 46.6506 - val_MinusLogProbMetric: 46.6506 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 361/1000
2023-10-27 20:43:52.404 
Epoch 361/1000 
	 loss: 46.0009, MinusLogProbMetric: 46.0009, val_loss: 47.3577, val_MinusLogProbMetric: 47.3577

Epoch 361: val_loss did not improve from 45.65762
196/196 - 35s - loss: 46.0009 - MinusLogProbMetric: 46.0009 - val_loss: 47.3577 - val_MinusLogProbMetric: 47.3577 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 362/1000
2023-10-27 20:44:26.923 
Epoch 362/1000 
	 loss: 46.2403, MinusLogProbMetric: 46.2403, val_loss: 45.9595, val_MinusLogProbMetric: 45.9595

Epoch 362: val_loss did not improve from 45.65762
196/196 - 35s - loss: 46.2403 - MinusLogProbMetric: 46.2403 - val_loss: 45.9595 - val_MinusLogProbMetric: 45.9595 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 363/1000
2023-10-27 20:45:01.031 
Epoch 363/1000 
	 loss: 45.5576, MinusLogProbMetric: 45.5576, val_loss: 46.3051, val_MinusLogProbMetric: 46.3051

Epoch 363: val_loss did not improve from 45.65762
196/196 - 34s - loss: 45.5576 - MinusLogProbMetric: 45.5576 - val_loss: 46.3051 - val_MinusLogProbMetric: 46.3051 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 364/1000
2023-10-27 20:45:34.830 
Epoch 364/1000 
	 loss: 46.1636, MinusLogProbMetric: 46.1636, val_loss: 46.1195, val_MinusLogProbMetric: 46.1195

Epoch 364: val_loss did not improve from 45.65762
196/196 - 34s - loss: 46.1636 - MinusLogProbMetric: 46.1636 - val_loss: 46.1195 - val_MinusLogProbMetric: 46.1195 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 365/1000
2023-10-27 20:46:08.557 
Epoch 365/1000 
	 loss: 46.6568, MinusLogProbMetric: 46.6568, val_loss: 45.9337, val_MinusLogProbMetric: 45.9337

Epoch 365: val_loss did not improve from 45.65762
196/196 - 34s - loss: 46.6568 - MinusLogProbMetric: 46.6568 - val_loss: 45.9337 - val_MinusLogProbMetric: 45.9337 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 366/1000
2023-10-27 20:46:42.846 
Epoch 366/1000 
	 loss: 45.3602, MinusLogProbMetric: 45.3602, val_loss: 46.0733, val_MinusLogProbMetric: 46.0733

Epoch 366: val_loss did not improve from 45.65762
196/196 - 34s - loss: 45.3602 - MinusLogProbMetric: 45.3602 - val_loss: 46.0733 - val_MinusLogProbMetric: 46.0733 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 367/1000
2023-10-27 20:47:16.815 
Epoch 367/1000 
	 loss: 46.6866, MinusLogProbMetric: 46.6866, val_loss: 45.5483, val_MinusLogProbMetric: 45.5483

Epoch 367: val_loss improved from 45.65762 to 45.54828, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 46.6866 - MinusLogProbMetric: 46.6866 - val_loss: 45.5483 - val_MinusLogProbMetric: 45.5483 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 368/1000
2023-10-27 20:47:52.119 
Epoch 368/1000 
	 loss: 47.6189, MinusLogProbMetric: 47.6189, val_loss: 46.2124, val_MinusLogProbMetric: 46.2124

Epoch 368: val_loss did not improve from 45.54828
196/196 - 35s - loss: 47.6189 - MinusLogProbMetric: 47.6189 - val_loss: 46.2124 - val_MinusLogProbMetric: 46.2124 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 369/1000
2023-10-27 20:48:26.242 
Epoch 369/1000 
	 loss: 46.2337, MinusLogProbMetric: 46.2337, val_loss: 48.5846, val_MinusLogProbMetric: 48.5846

Epoch 369: val_loss did not improve from 45.54828
196/196 - 34s - loss: 46.2337 - MinusLogProbMetric: 46.2337 - val_loss: 48.5846 - val_MinusLogProbMetric: 48.5846 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 370/1000
2023-10-27 20:49:00.878 
Epoch 370/1000 
	 loss: 46.1352, MinusLogProbMetric: 46.1352, val_loss: 47.1179, val_MinusLogProbMetric: 47.1179

Epoch 370: val_loss did not improve from 45.54828
196/196 - 35s - loss: 46.1352 - MinusLogProbMetric: 46.1352 - val_loss: 47.1179 - val_MinusLogProbMetric: 47.1179 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 371/1000
2023-10-27 20:49:35.376 
Epoch 371/1000 
	 loss: 46.2614, MinusLogProbMetric: 46.2614, val_loss: 47.5927, val_MinusLogProbMetric: 47.5927

Epoch 371: val_loss did not improve from 45.54828
196/196 - 34s - loss: 46.2614 - MinusLogProbMetric: 46.2614 - val_loss: 47.5927 - val_MinusLogProbMetric: 47.5927 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 372/1000
2023-10-27 20:50:09.972 
Epoch 372/1000 
	 loss: 46.0484, MinusLogProbMetric: 46.0484, val_loss: 45.9904, val_MinusLogProbMetric: 45.9904

Epoch 372: val_loss did not improve from 45.54828
196/196 - 35s - loss: 46.0484 - MinusLogProbMetric: 46.0484 - val_loss: 45.9904 - val_MinusLogProbMetric: 45.9904 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 373/1000
2023-10-27 20:50:44.328 
Epoch 373/1000 
	 loss: 45.6643, MinusLogProbMetric: 45.6643, val_loss: 55.2938, val_MinusLogProbMetric: 55.2938

Epoch 373: val_loss did not improve from 45.54828
196/196 - 34s - loss: 45.6643 - MinusLogProbMetric: 45.6643 - val_loss: 55.2938 - val_MinusLogProbMetric: 55.2938 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 374/1000
2023-10-27 20:51:20.002 
Epoch 374/1000 
	 loss: 46.2441, MinusLogProbMetric: 46.2441, val_loss: 52.5109, val_MinusLogProbMetric: 52.5109

Epoch 374: val_loss did not improve from 45.54828
196/196 - 36s - loss: 46.2441 - MinusLogProbMetric: 46.2441 - val_loss: 52.5109 - val_MinusLogProbMetric: 52.5109 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 375/1000
2023-10-27 20:51:54.628 
Epoch 375/1000 
	 loss: 46.9054, MinusLogProbMetric: 46.9054, val_loss: 45.5813, val_MinusLogProbMetric: 45.5813

Epoch 375: val_loss did not improve from 45.54828
196/196 - 35s - loss: 46.9054 - MinusLogProbMetric: 46.9054 - val_loss: 45.5813 - val_MinusLogProbMetric: 45.5813 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 376/1000
2023-10-27 20:52:28.674 
Epoch 376/1000 
	 loss: 46.2468, MinusLogProbMetric: 46.2468, val_loss: 45.2601, val_MinusLogProbMetric: 45.2601

Epoch 376: val_loss improved from 45.54828 to 45.26012, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 46.2468 - MinusLogProbMetric: 46.2468 - val_loss: 45.2601 - val_MinusLogProbMetric: 45.2601 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 377/1000
2023-10-27 20:53:03.219 
Epoch 377/1000 
	 loss: 45.3047, MinusLogProbMetric: 45.3047, val_loss: 47.4109, val_MinusLogProbMetric: 47.4109

Epoch 377: val_loss did not improve from 45.26012
196/196 - 34s - loss: 45.3047 - MinusLogProbMetric: 45.3047 - val_loss: 47.4109 - val_MinusLogProbMetric: 47.4109 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 378/1000
2023-10-27 20:53:37.440 
Epoch 378/1000 
	 loss: 46.1416, MinusLogProbMetric: 46.1416, val_loss: 49.9089, val_MinusLogProbMetric: 49.9089

Epoch 378: val_loss did not improve from 45.26012
196/196 - 34s - loss: 46.1416 - MinusLogProbMetric: 46.1416 - val_loss: 49.9089 - val_MinusLogProbMetric: 49.9089 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 379/1000
2023-10-27 20:54:11.817 
Epoch 379/1000 
	 loss: 45.9735, MinusLogProbMetric: 45.9735, val_loss: 47.8313, val_MinusLogProbMetric: 47.8313

Epoch 379: val_loss did not improve from 45.26012
196/196 - 34s - loss: 45.9735 - MinusLogProbMetric: 45.9735 - val_loss: 47.8313 - val_MinusLogProbMetric: 47.8313 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 380/1000
2023-10-27 20:54:46.146 
Epoch 380/1000 
	 loss: 46.3022, MinusLogProbMetric: 46.3022, val_loss: 46.0931, val_MinusLogProbMetric: 46.0931

Epoch 380: val_loss did not improve from 45.26012
196/196 - 34s - loss: 46.3022 - MinusLogProbMetric: 46.3022 - val_loss: 46.0931 - val_MinusLogProbMetric: 46.0931 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 381/1000
2023-10-27 20:55:20.602 
Epoch 381/1000 
	 loss: 45.7863, MinusLogProbMetric: 45.7863, val_loss: 46.0708, val_MinusLogProbMetric: 46.0708

Epoch 381: val_loss did not improve from 45.26012
196/196 - 34s - loss: 45.7863 - MinusLogProbMetric: 45.7863 - val_loss: 46.0708 - val_MinusLogProbMetric: 46.0708 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 382/1000
2023-10-27 20:55:54.855 
Epoch 382/1000 
	 loss: 46.1634, MinusLogProbMetric: 46.1634, val_loss: 45.5354, val_MinusLogProbMetric: 45.5354

Epoch 382: val_loss did not improve from 45.26012
196/196 - 34s - loss: 46.1634 - MinusLogProbMetric: 46.1634 - val_loss: 45.5354 - val_MinusLogProbMetric: 45.5354 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 383/1000
2023-10-27 20:56:29.225 
Epoch 383/1000 
	 loss: 45.7302, MinusLogProbMetric: 45.7302, val_loss: 45.8894, val_MinusLogProbMetric: 45.8894

Epoch 383: val_loss did not improve from 45.26012
196/196 - 34s - loss: 45.7302 - MinusLogProbMetric: 45.7302 - val_loss: 45.8894 - val_MinusLogProbMetric: 45.8894 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 384/1000
2023-10-27 20:57:04.818 
Epoch 384/1000 
	 loss: 46.1558, MinusLogProbMetric: 46.1558, val_loss: 46.5722, val_MinusLogProbMetric: 46.5722

Epoch 384: val_loss did not improve from 45.26012
196/196 - 36s - loss: 46.1558 - MinusLogProbMetric: 46.1558 - val_loss: 46.5722 - val_MinusLogProbMetric: 46.5722 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 385/1000
2023-10-27 20:57:39.854 
Epoch 385/1000 
	 loss: 45.4151, MinusLogProbMetric: 45.4151, val_loss: 45.7714, val_MinusLogProbMetric: 45.7714

Epoch 385: val_loss did not improve from 45.26012
196/196 - 35s - loss: 45.4151 - MinusLogProbMetric: 45.4151 - val_loss: 45.7714 - val_MinusLogProbMetric: 45.7714 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 386/1000
2023-10-27 20:58:14.099 
Epoch 386/1000 
	 loss: 46.1273, MinusLogProbMetric: 46.1273, val_loss: 49.3960, val_MinusLogProbMetric: 49.3960

Epoch 386: val_loss did not improve from 45.26012
196/196 - 34s - loss: 46.1273 - MinusLogProbMetric: 46.1273 - val_loss: 49.3960 - val_MinusLogProbMetric: 49.3960 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 387/1000
2023-10-27 20:58:49.064 
Epoch 387/1000 
	 loss: 45.6779, MinusLogProbMetric: 45.6779, val_loss: 47.7682, val_MinusLogProbMetric: 47.7682

Epoch 387: val_loss did not improve from 45.26012
196/196 - 35s - loss: 45.6779 - MinusLogProbMetric: 45.6779 - val_loss: 47.7682 - val_MinusLogProbMetric: 47.7682 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 388/1000
2023-10-27 20:59:23.007 
Epoch 388/1000 
	 loss: 46.4253, MinusLogProbMetric: 46.4253, val_loss: 45.7214, val_MinusLogProbMetric: 45.7214

Epoch 388: val_loss did not improve from 45.26012
196/196 - 34s - loss: 46.4253 - MinusLogProbMetric: 46.4253 - val_loss: 45.7214 - val_MinusLogProbMetric: 45.7214 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 389/1000
2023-10-27 20:59:57.011 
Epoch 389/1000 
	 loss: 45.7699, MinusLogProbMetric: 45.7699, val_loss: 46.2373, val_MinusLogProbMetric: 46.2373

Epoch 389: val_loss did not improve from 45.26012
196/196 - 34s - loss: 45.7699 - MinusLogProbMetric: 45.7699 - val_loss: 46.2373 - val_MinusLogProbMetric: 46.2373 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 390/1000
2023-10-27 21:00:31.299 
Epoch 390/1000 
	 loss: 46.8004, MinusLogProbMetric: 46.8004, val_loss: 46.4219, val_MinusLogProbMetric: 46.4219

Epoch 390: val_loss did not improve from 45.26012
196/196 - 34s - loss: 46.8004 - MinusLogProbMetric: 46.8004 - val_loss: 46.4219 - val_MinusLogProbMetric: 46.4219 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 391/1000
2023-10-27 21:01:06.578 
Epoch 391/1000 
	 loss: 45.3996, MinusLogProbMetric: 45.3996, val_loss: 45.6868, val_MinusLogProbMetric: 45.6868

Epoch 391: val_loss did not improve from 45.26012
196/196 - 35s - loss: 45.3996 - MinusLogProbMetric: 45.3996 - val_loss: 45.6868 - val_MinusLogProbMetric: 45.6868 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 392/1000
2023-10-27 21:01:41.391 
Epoch 392/1000 
	 loss: 46.1786, MinusLogProbMetric: 46.1786, val_loss: 49.2296, val_MinusLogProbMetric: 49.2296

Epoch 392: val_loss did not improve from 45.26012
196/196 - 35s - loss: 46.1786 - MinusLogProbMetric: 46.1786 - val_loss: 49.2296 - val_MinusLogProbMetric: 49.2296 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 393/1000
2023-10-27 21:02:15.826 
Epoch 393/1000 
	 loss: 45.6776, MinusLogProbMetric: 45.6776, val_loss: 45.8589, val_MinusLogProbMetric: 45.8589

Epoch 393: val_loss did not improve from 45.26012
196/196 - 34s - loss: 45.6776 - MinusLogProbMetric: 45.6776 - val_loss: 45.8589 - val_MinusLogProbMetric: 45.8589 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 394/1000
2023-10-27 21:02:50.580 
Epoch 394/1000 
	 loss: 45.5333, MinusLogProbMetric: 45.5333, val_loss: 45.8496, val_MinusLogProbMetric: 45.8496

Epoch 394: val_loss did not improve from 45.26012
196/196 - 35s - loss: 45.5333 - MinusLogProbMetric: 45.5333 - val_loss: 45.8496 - val_MinusLogProbMetric: 45.8496 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 395/1000
2023-10-27 21:03:25.284 
Epoch 395/1000 
	 loss: 46.5039, MinusLogProbMetric: 46.5039, val_loss: 45.9066, val_MinusLogProbMetric: 45.9066

Epoch 395: val_loss did not improve from 45.26012
196/196 - 35s - loss: 46.5039 - MinusLogProbMetric: 46.5039 - val_loss: 45.9066 - val_MinusLogProbMetric: 45.9066 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 396/1000
2023-10-27 21:03:59.800 
Epoch 396/1000 
	 loss: 45.5719, MinusLogProbMetric: 45.5719, val_loss: 47.9738, val_MinusLogProbMetric: 47.9738

Epoch 396: val_loss did not improve from 45.26012
196/196 - 35s - loss: 45.5719 - MinusLogProbMetric: 45.5719 - val_loss: 47.9738 - val_MinusLogProbMetric: 47.9738 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 397/1000
2023-10-27 21:04:34.130 
Epoch 397/1000 
	 loss: 45.9072, MinusLogProbMetric: 45.9072, val_loss: 46.4497, val_MinusLogProbMetric: 46.4497

Epoch 397: val_loss did not improve from 45.26012
196/196 - 34s - loss: 45.9072 - MinusLogProbMetric: 45.9072 - val_loss: 46.4497 - val_MinusLogProbMetric: 46.4497 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 398/1000
2023-10-27 21:05:08.155 
Epoch 398/1000 
	 loss: 45.4333, MinusLogProbMetric: 45.4333, val_loss: 47.0139, val_MinusLogProbMetric: 47.0139

Epoch 398: val_loss did not improve from 45.26012
196/196 - 34s - loss: 45.4333 - MinusLogProbMetric: 45.4333 - val_loss: 47.0139 - val_MinusLogProbMetric: 47.0139 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 399/1000
2023-10-27 21:05:41.904 
Epoch 399/1000 
	 loss: 45.4025, MinusLogProbMetric: 45.4025, val_loss: 46.5550, val_MinusLogProbMetric: 46.5550

Epoch 399: val_loss did not improve from 45.26012
196/196 - 34s - loss: 45.4025 - MinusLogProbMetric: 45.4025 - val_loss: 46.5550 - val_MinusLogProbMetric: 46.5550 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 400/1000
2023-10-27 21:06:15.721 
Epoch 400/1000 
	 loss: 45.5011, MinusLogProbMetric: 45.5011, val_loss: 48.2706, val_MinusLogProbMetric: 48.2706

Epoch 400: val_loss did not improve from 45.26012
196/196 - 34s - loss: 45.5011 - MinusLogProbMetric: 45.5011 - val_loss: 48.2706 - val_MinusLogProbMetric: 48.2706 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 401/1000
2023-10-27 21:06:50.384 
Epoch 401/1000 
	 loss: 45.6527, MinusLogProbMetric: 45.6527, val_loss: 47.8356, val_MinusLogProbMetric: 47.8356

Epoch 401: val_loss did not improve from 45.26012
196/196 - 35s - loss: 45.6527 - MinusLogProbMetric: 45.6527 - val_loss: 47.8356 - val_MinusLogProbMetric: 47.8356 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 402/1000
2023-10-27 21:07:24.523 
Epoch 402/1000 
	 loss: 45.7461, MinusLogProbMetric: 45.7461, val_loss: 45.7591, val_MinusLogProbMetric: 45.7591

Epoch 402: val_loss did not improve from 45.26012
196/196 - 34s - loss: 45.7461 - MinusLogProbMetric: 45.7461 - val_loss: 45.7591 - val_MinusLogProbMetric: 45.7591 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 403/1000
2023-10-27 21:07:59.002 
Epoch 403/1000 
	 loss: 47.3893, MinusLogProbMetric: 47.3893, val_loss: 47.2412, val_MinusLogProbMetric: 47.2412

Epoch 403: val_loss did not improve from 45.26012
196/196 - 34s - loss: 47.3893 - MinusLogProbMetric: 47.3893 - val_loss: 47.2412 - val_MinusLogProbMetric: 47.2412 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 404/1000
2023-10-27 21:08:33.855 
Epoch 404/1000 
	 loss: 45.6243, MinusLogProbMetric: 45.6243, val_loss: 46.1826, val_MinusLogProbMetric: 46.1826

Epoch 404: val_loss did not improve from 45.26012
196/196 - 35s - loss: 45.6243 - MinusLogProbMetric: 45.6243 - val_loss: 46.1826 - val_MinusLogProbMetric: 46.1826 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 405/1000
2023-10-27 21:09:08.391 
Epoch 405/1000 
	 loss: 45.2073, MinusLogProbMetric: 45.2073, val_loss: 45.8125, val_MinusLogProbMetric: 45.8125

Epoch 405: val_loss did not improve from 45.26012
196/196 - 35s - loss: 45.2073 - MinusLogProbMetric: 45.2073 - val_loss: 45.8125 - val_MinusLogProbMetric: 45.8125 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 406/1000
2023-10-27 21:09:43.857 
Epoch 406/1000 
	 loss: 45.4193, MinusLogProbMetric: 45.4193, val_loss: 45.0612, val_MinusLogProbMetric: 45.0612

Epoch 406: val_loss improved from 45.26012 to 45.06118, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 36s - loss: 45.4193 - MinusLogProbMetric: 45.4193 - val_loss: 45.0612 - val_MinusLogProbMetric: 45.0612 - lr: 3.3333e-04 - 36s/epoch - 183ms/step
Epoch 407/1000
2023-10-27 21:10:19.373 
Epoch 407/1000 
	 loss: 45.9164, MinusLogProbMetric: 45.9164, val_loss: 46.5091, val_MinusLogProbMetric: 46.5091

Epoch 407: val_loss did not improve from 45.06118
196/196 - 35s - loss: 45.9164 - MinusLogProbMetric: 45.9164 - val_loss: 46.5091 - val_MinusLogProbMetric: 46.5091 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 408/1000
2023-10-27 21:10:53.862 
Epoch 408/1000 
	 loss: 45.5793, MinusLogProbMetric: 45.5793, val_loss: 46.6812, val_MinusLogProbMetric: 46.6812

Epoch 408: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.5793 - MinusLogProbMetric: 45.5793 - val_loss: 46.6812 - val_MinusLogProbMetric: 46.6812 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 409/1000
2023-10-27 21:11:29.214 
Epoch 409/1000 
	 loss: 45.4119, MinusLogProbMetric: 45.4119, val_loss: 46.9633, val_MinusLogProbMetric: 46.9633

Epoch 409: val_loss did not improve from 45.06118
196/196 - 35s - loss: 45.4119 - MinusLogProbMetric: 45.4119 - val_loss: 46.9633 - val_MinusLogProbMetric: 46.9633 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 410/1000
2023-10-27 21:12:03.969 
Epoch 410/1000 
	 loss: 45.3430, MinusLogProbMetric: 45.3430, val_loss: 45.8092, val_MinusLogProbMetric: 45.8092

Epoch 410: val_loss did not improve from 45.06118
196/196 - 35s - loss: 45.3430 - MinusLogProbMetric: 45.3430 - val_loss: 45.8092 - val_MinusLogProbMetric: 45.8092 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 411/1000
2023-10-27 21:12:38.332 
Epoch 411/1000 
	 loss: 46.2366, MinusLogProbMetric: 46.2366, val_loss: 46.3733, val_MinusLogProbMetric: 46.3733

Epoch 411: val_loss did not improve from 45.06118
196/196 - 34s - loss: 46.2366 - MinusLogProbMetric: 46.2366 - val_loss: 46.3733 - val_MinusLogProbMetric: 46.3733 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 412/1000
2023-10-27 21:13:12.064 
Epoch 412/1000 
	 loss: 45.3268, MinusLogProbMetric: 45.3268, val_loss: 45.4698, val_MinusLogProbMetric: 45.4698

Epoch 412: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.3268 - MinusLogProbMetric: 45.3268 - val_loss: 45.4698 - val_MinusLogProbMetric: 45.4698 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 413/1000
2023-10-27 21:13:46.485 
Epoch 413/1000 
	 loss: 45.3751, MinusLogProbMetric: 45.3751, val_loss: 47.9465, val_MinusLogProbMetric: 47.9465

Epoch 413: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.3751 - MinusLogProbMetric: 45.3751 - val_loss: 47.9465 - val_MinusLogProbMetric: 47.9465 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 414/1000
2023-10-27 21:14:20.297 
Epoch 414/1000 
	 loss: 47.0792, MinusLogProbMetric: 47.0792, val_loss: 46.6935, val_MinusLogProbMetric: 46.6935

Epoch 414: val_loss did not improve from 45.06118
196/196 - 34s - loss: 47.0792 - MinusLogProbMetric: 47.0792 - val_loss: 46.6935 - val_MinusLogProbMetric: 46.6935 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 415/1000
2023-10-27 21:14:54.703 
Epoch 415/1000 
	 loss: 45.9117, MinusLogProbMetric: 45.9117, val_loss: 46.0966, val_MinusLogProbMetric: 46.0966

Epoch 415: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.9117 - MinusLogProbMetric: 45.9117 - val_loss: 46.0966 - val_MinusLogProbMetric: 46.0966 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 416/1000
2023-10-27 21:15:28.834 
Epoch 416/1000 
	 loss: 45.1489, MinusLogProbMetric: 45.1489, val_loss: 47.0364, val_MinusLogProbMetric: 47.0364

Epoch 416: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.1489 - MinusLogProbMetric: 45.1489 - val_loss: 47.0364 - val_MinusLogProbMetric: 47.0364 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 417/1000
2023-10-27 21:16:02.760 
Epoch 417/1000 
	 loss: 45.1178, MinusLogProbMetric: 45.1178, val_loss: 45.1789, val_MinusLogProbMetric: 45.1789

Epoch 417: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.1178 - MinusLogProbMetric: 45.1178 - val_loss: 45.1789 - val_MinusLogProbMetric: 45.1789 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 418/1000
2023-10-27 21:16:37.308 
Epoch 418/1000 
	 loss: 46.1462, MinusLogProbMetric: 46.1462, val_loss: 52.8864, val_MinusLogProbMetric: 52.8864

Epoch 418: val_loss did not improve from 45.06118
196/196 - 35s - loss: 46.1462 - MinusLogProbMetric: 46.1462 - val_loss: 52.8864 - val_MinusLogProbMetric: 52.8864 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 419/1000
2023-10-27 21:17:11.325 
Epoch 419/1000 
	 loss: 47.9721, MinusLogProbMetric: 47.9721, val_loss: 47.4725, val_MinusLogProbMetric: 47.4725

Epoch 419: val_loss did not improve from 45.06118
196/196 - 34s - loss: 47.9721 - MinusLogProbMetric: 47.9721 - val_loss: 47.4725 - val_MinusLogProbMetric: 47.4725 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 420/1000
2023-10-27 21:17:45.461 
Epoch 420/1000 
	 loss: 45.0489, MinusLogProbMetric: 45.0489, val_loss: 47.0077, val_MinusLogProbMetric: 47.0077

Epoch 420: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.0489 - MinusLogProbMetric: 45.0489 - val_loss: 47.0077 - val_MinusLogProbMetric: 47.0077 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 421/1000
2023-10-27 21:18:19.704 
Epoch 421/1000 
	 loss: 45.8153, MinusLogProbMetric: 45.8153, val_loss: 46.2786, val_MinusLogProbMetric: 46.2786

Epoch 421: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.8153 - MinusLogProbMetric: 45.8153 - val_loss: 46.2786 - val_MinusLogProbMetric: 46.2786 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 422/1000
2023-10-27 21:18:53.765 
Epoch 422/1000 
	 loss: 45.6774, MinusLogProbMetric: 45.6774, val_loss: 45.6352, val_MinusLogProbMetric: 45.6352

Epoch 422: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.6774 - MinusLogProbMetric: 45.6774 - val_loss: 45.6352 - val_MinusLogProbMetric: 45.6352 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 423/1000
2023-10-27 21:19:27.809 
Epoch 423/1000 
	 loss: 45.4592, MinusLogProbMetric: 45.4592, val_loss: 45.8924, val_MinusLogProbMetric: 45.8924

Epoch 423: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.4592 - MinusLogProbMetric: 45.4592 - val_loss: 45.8924 - val_MinusLogProbMetric: 45.8924 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 424/1000
2023-10-27 21:20:02.925 
Epoch 424/1000 
	 loss: 45.5636, MinusLogProbMetric: 45.5636, val_loss: 46.4893, val_MinusLogProbMetric: 46.4893

Epoch 424: val_loss did not improve from 45.06118
196/196 - 35s - loss: 45.5636 - MinusLogProbMetric: 45.5636 - val_loss: 46.4893 - val_MinusLogProbMetric: 46.4893 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 425/1000
2023-10-27 21:20:38.176 
Epoch 425/1000 
	 loss: 45.6626, MinusLogProbMetric: 45.6626, val_loss: 45.2001, val_MinusLogProbMetric: 45.2001

Epoch 425: val_loss did not improve from 45.06118
196/196 - 35s - loss: 45.6626 - MinusLogProbMetric: 45.6626 - val_loss: 45.2001 - val_MinusLogProbMetric: 45.2001 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 426/1000
2023-10-27 21:21:13.124 
Epoch 426/1000 
	 loss: 45.0565, MinusLogProbMetric: 45.0565, val_loss: 45.9953, val_MinusLogProbMetric: 45.9953

Epoch 426: val_loss did not improve from 45.06118
196/196 - 35s - loss: 45.0565 - MinusLogProbMetric: 45.0565 - val_loss: 45.9953 - val_MinusLogProbMetric: 45.9953 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 427/1000
2023-10-27 21:21:47.912 
Epoch 427/1000 
	 loss: 45.0847, MinusLogProbMetric: 45.0847, val_loss: 46.2038, val_MinusLogProbMetric: 46.2038

Epoch 427: val_loss did not improve from 45.06118
196/196 - 35s - loss: 45.0847 - MinusLogProbMetric: 45.0847 - val_loss: 46.2038 - val_MinusLogProbMetric: 46.2038 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 428/1000
2023-10-27 21:22:22.144 
Epoch 428/1000 
	 loss: 46.0293, MinusLogProbMetric: 46.0293, val_loss: 46.1153, val_MinusLogProbMetric: 46.1153

Epoch 428: val_loss did not improve from 45.06118
196/196 - 34s - loss: 46.0293 - MinusLogProbMetric: 46.0293 - val_loss: 46.1153 - val_MinusLogProbMetric: 46.1153 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 429/1000
2023-10-27 21:22:56.302 
Epoch 429/1000 
	 loss: 45.0969, MinusLogProbMetric: 45.0969, val_loss: 45.5571, val_MinusLogProbMetric: 45.5571

Epoch 429: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.0969 - MinusLogProbMetric: 45.0969 - val_loss: 45.5571 - val_MinusLogProbMetric: 45.5571 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 430/1000
2023-10-27 21:23:30.410 
Epoch 430/1000 
	 loss: 45.7239, MinusLogProbMetric: 45.7239, val_loss: 46.1529, val_MinusLogProbMetric: 46.1529

Epoch 430: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.7239 - MinusLogProbMetric: 45.7239 - val_loss: 46.1529 - val_MinusLogProbMetric: 46.1529 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 431/1000
2023-10-27 21:24:04.935 
Epoch 431/1000 
	 loss: 45.5492, MinusLogProbMetric: 45.5492, val_loss: 45.6759, val_MinusLogProbMetric: 45.6759

Epoch 431: val_loss did not improve from 45.06118
196/196 - 35s - loss: 45.5492 - MinusLogProbMetric: 45.5492 - val_loss: 45.6759 - val_MinusLogProbMetric: 45.6759 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 432/1000
2023-10-27 21:24:38.987 
Epoch 432/1000 
	 loss: 45.4615, MinusLogProbMetric: 45.4615, val_loss: 45.4559, val_MinusLogProbMetric: 45.4559

Epoch 432: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.4615 - MinusLogProbMetric: 45.4615 - val_loss: 45.4559 - val_MinusLogProbMetric: 45.4559 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 433/1000
2023-10-27 21:25:13.310 
Epoch 433/1000 
	 loss: 45.0605, MinusLogProbMetric: 45.0605, val_loss: 45.4520, val_MinusLogProbMetric: 45.4520

Epoch 433: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.0605 - MinusLogProbMetric: 45.0605 - val_loss: 45.4520 - val_MinusLogProbMetric: 45.4520 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 434/1000
2023-10-27 21:25:47.297 
Epoch 434/1000 
	 loss: 46.2898, MinusLogProbMetric: 46.2898, val_loss: 45.3950, val_MinusLogProbMetric: 45.3950

Epoch 434: val_loss did not improve from 45.06118
196/196 - 34s - loss: 46.2898 - MinusLogProbMetric: 46.2898 - val_loss: 45.3950 - val_MinusLogProbMetric: 45.3950 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 435/1000
2023-10-27 21:26:21.589 
Epoch 435/1000 
	 loss: 45.6473, MinusLogProbMetric: 45.6473, val_loss: 46.5971, val_MinusLogProbMetric: 46.5971

Epoch 435: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.6473 - MinusLogProbMetric: 45.6473 - val_loss: 46.5971 - val_MinusLogProbMetric: 46.5971 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 436/1000
2023-10-27 21:26:56.205 
Epoch 436/1000 
	 loss: 44.9396, MinusLogProbMetric: 44.9396, val_loss: 46.4937, val_MinusLogProbMetric: 46.4937

Epoch 436: val_loss did not improve from 45.06118
196/196 - 35s - loss: 44.9396 - MinusLogProbMetric: 44.9396 - val_loss: 46.4937 - val_MinusLogProbMetric: 46.4937 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 437/1000
2023-10-27 21:27:31.508 
Epoch 437/1000 
	 loss: 45.4048, MinusLogProbMetric: 45.4048, val_loss: 46.7038, val_MinusLogProbMetric: 46.7038

Epoch 437: val_loss did not improve from 45.06118
196/196 - 35s - loss: 45.4048 - MinusLogProbMetric: 45.4048 - val_loss: 46.7038 - val_MinusLogProbMetric: 46.7038 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 438/1000
2023-10-27 21:28:06.938 
Epoch 438/1000 
	 loss: 46.9159, MinusLogProbMetric: 46.9159, val_loss: 45.6086, val_MinusLogProbMetric: 45.6086

Epoch 438: val_loss did not improve from 45.06118
196/196 - 35s - loss: 46.9159 - MinusLogProbMetric: 46.9159 - val_loss: 45.6086 - val_MinusLogProbMetric: 45.6086 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 439/1000
2023-10-27 21:28:41.365 
Epoch 439/1000 
	 loss: 45.4037, MinusLogProbMetric: 45.4037, val_loss: 49.0097, val_MinusLogProbMetric: 49.0097

Epoch 439: val_loss did not improve from 45.06118
196/196 - 34s - loss: 45.4037 - MinusLogProbMetric: 45.4037 - val_loss: 49.0097 - val_MinusLogProbMetric: 49.0097 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 440/1000
2023-10-27 21:29:15.453 
Epoch 440/1000 
	 loss: 45.6573, MinusLogProbMetric: 45.6573, val_loss: 45.0068, val_MinusLogProbMetric: 45.0068

Epoch 440: val_loss improved from 45.06118 to 45.00685, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 45.6573 - MinusLogProbMetric: 45.6573 - val_loss: 45.0068 - val_MinusLogProbMetric: 45.0068 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 441/1000
2023-10-27 21:29:49.932 
Epoch 441/1000 
	 loss: 45.8713, MinusLogProbMetric: 45.8713, val_loss: 46.4312, val_MinusLogProbMetric: 46.4312

Epoch 441: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.8713 - MinusLogProbMetric: 45.8713 - val_loss: 46.4312 - val_MinusLogProbMetric: 46.4312 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 442/1000
2023-10-27 21:30:24.594 
Epoch 442/1000 
	 loss: 45.3722, MinusLogProbMetric: 45.3722, val_loss: 48.0149, val_MinusLogProbMetric: 48.0149

Epoch 442: val_loss did not improve from 45.00685
196/196 - 35s - loss: 45.3722 - MinusLogProbMetric: 45.3722 - val_loss: 48.0149 - val_MinusLogProbMetric: 48.0149 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 443/1000
2023-10-27 21:30:58.780 
Epoch 443/1000 
	 loss: 45.3159, MinusLogProbMetric: 45.3159, val_loss: 45.9879, val_MinusLogProbMetric: 45.9879

Epoch 443: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.3159 - MinusLogProbMetric: 45.3159 - val_loss: 45.9879 - val_MinusLogProbMetric: 45.9879 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 444/1000
2023-10-27 21:31:32.860 
Epoch 444/1000 
	 loss: 45.1134, MinusLogProbMetric: 45.1134, val_loss: 45.1592, val_MinusLogProbMetric: 45.1592

Epoch 444: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.1134 - MinusLogProbMetric: 45.1134 - val_loss: 45.1592 - val_MinusLogProbMetric: 45.1592 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 445/1000
2023-10-27 21:32:07.114 
Epoch 445/1000 
	 loss: 45.0744, MinusLogProbMetric: 45.0744, val_loss: 45.0886, val_MinusLogProbMetric: 45.0886

Epoch 445: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.0744 - MinusLogProbMetric: 45.0744 - val_loss: 45.0886 - val_MinusLogProbMetric: 45.0886 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 446/1000
2023-10-27 21:32:41.109 
Epoch 446/1000 
	 loss: 45.5537, MinusLogProbMetric: 45.5537, val_loss: 45.4958, val_MinusLogProbMetric: 45.4958

Epoch 446: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.5537 - MinusLogProbMetric: 45.5537 - val_loss: 45.4958 - val_MinusLogProbMetric: 45.4958 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 447/1000
2023-10-27 21:33:15.170 
Epoch 447/1000 
	 loss: 45.1150, MinusLogProbMetric: 45.1150, val_loss: 45.8322, val_MinusLogProbMetric: 45.8322

Epoch 447: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.1150 - MinusLogProbMetric: 45.1150 - val_loss: 45.8322 - val_MinusLogProbMetric: 45.8322 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 448/1000
2023-10-27 21:33:49.102 
Epoch 448/1000 
	 loss: 45.6251, MinusLogProbMetric: 45.6251, val_loss: 46.6020, val_MinusLogProbMetric: 46.6020

Epoch 448: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.6251 - MinusLogProbMetric: 45.6251 - val_loss: 46.6020 - val_MinusLogProbMetric: 46.6020 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 449/1000
2023-10-27 21:34:23.064 
Epoch 449/1000 
	 loss: 45.3330, MinusLogProbMetric: 45.3330, val_loss: 45.6058, val_MinusLogProbMetric: 45.6058

Epoch 449: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.3330 - MinusLogProbMetric: 45.3330 - val_loss: 45.6058 - val_MinusLogProbMetric: 45.6058 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 450/1000
2023-10-27 21:34:57.231 
Epoch 450/1000 
	 loss: 44.7648, MinusLogProbMetric: 44.7648, val_loss: 45.6600, val_MinusLogProbMetric: 45.6600

Epoch 450: val_loss did not improve from 45.00685
196/196 - 34s - loss: 44.7648 - MinusLogProbMetric: 44.7648 - val_loss: 45.6600 - val_MinusLogProbMetric: 45.6600 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 451/1000
2023-10-27 21:35:31.723 
Epoch 451/1000 
	 loss: 45.2413, MinusLogProbMetric: 45.2413, val_loss: 46.3531, val_MinusLogProbMetric: 46.3531

Epoch 451: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.2413 - MinusLogProbMetric: 45.2413 - val_loss: 46.3531 - val_MinusLogProbMetric: 46.3531 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 452/1000
2023-10-27 21:36:06.236 
Epoch 452/1000 
	 loss: 45.0262, MinusLogProbMetric: 45.0262, val_loss: 46.6736, val_MinusLogProbMetric: 46.6736

Epoch 452: val_loss did not improve from 45.00685
196/196 - 35s - loss: 45.0262 - MinusLogProbMetric: 45.0262 - val_loss: 46.6736 - val_MinusLogProbMetric: 46.6736 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 453/1000
2023-10-27 21:36:40.940 
Epoch 453/1000 
	 loss: 45.6838, MinusLogProbMetric: 45.6838, val_loss: 46.4681, val_MinusLogProbMetric: 46.4681

Epoch 453: val_loss did not improve from 45.00685
196/196 - 35s - loss: 45.6838 - MinusLogProbMetric: 45.6838 - val_loss: 46.4681 - val_MinusLogProbMetric: 46.4681 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 454/1000
2023-10-27 21:37:14.946 
Epoch 454/1000 
	 loss: 45.2662, MinusLogProbMetric: 45.2662, val_loss: 45.7145, val_MinusLogProbMetric: 45.7145

Epoch 454: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.2662 - MinusLogProbMetric: 45.2662 - val_loss: 45.7145 - val_MinusLogProbMetric: 45.7145 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 455/1000
2023-10-27 21:37:48.977 
Epoch 455/1000 
	 loss: 45.8464, MinusLogProbMetric: 45.8464, val_loss: 47.8004, val_MinusLogProbMetric: 47.8004

Epoch 455: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.8464 - MinusLogProbMetric: 45.8464 - val_loss: 47.8004 - val_MinusLogProbMetric: 47.8004 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 456/1000
2023-10-27 21:38:23.646 
Epoch 456/1000 
	 loss: 45.8747, MinusLogProbMetric: 45.8747, val_loss: 45.5291, val_MinusLogProbMetric: 45.5291

Epoch 456: val_loss did not improve from 45.00685
196/196 - 35s - loss: 45.8747 - MinusLogProbMetric: 45.8747 - val_loss: 45.5291 - val_MinusLogProbMetric: 45.5291 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 457/1000
2023-10-27 21:38:57.785 
Epoch 457/1000 
	 loss: 44.9216, MinusLogProbMetric: 44.9216, val_loss: 47.0614, val_MinusLogProbMetric: 47.0614

Epoch 457: val_loss did not improve from 45.00685
196/196 - 34s - loss: 44.9216 - MinusLogProbMetric: 44.9216 - val_loss: 47.0614 - val_MinusLogProbMetric: 47.0614 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 458/1000
2023-10-27 21:39:32.324 
Epoch 458/1000 
	 loss: 45.4596, MinusLogProbMetric: 45.4596, val_loss: 45.0531, val_MinusLogProbMetric: 45.0531

Epoch 458: val_loss did not improve from 45.00685
196/196 - 35s - loss: 45.4596 - MinusLogProbMetric: 45.4596 - val_loss: 45.0531 - val_MinusLogProbMetric: 45.0531 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 459/1000
2023-10-27 21:40:06.365 
Epoch 459/1000 
	 loss: 45.8336, MinusLogProbMetric: 45.8336, val_loss: 45.9384, val_MinusLogProbMetric: 45.9384

Epoch 459: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.8336 - MinusLogProbMetric: 45.8336 - val_loss: 45.9384 - val_MinusLogProbMetric: 45.9384 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 460/1000
2023-10-27 21:40:40.840 
Epoch 460/1000 
	 loss: 45.6539, MinusLogProbMetric: 45.6539, val_loss: 46.1249, val_MinusLogProbMetric: 46.1249

Epoch 460: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.6539 - MinusLogProbMetric: 45.6539 - val_loss: 46.1249 - val_MinusLogProbMetric: 46.1249 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 461/1000
2023-10-27 21:41:14.914 
Epoch 461/1000 
	 loss: 45.4555, MinusLogProbMetric: 45.4555, val_loss: 45.6526, val_MinusLogProbMetric: 45.6526

Epoch 461: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.4555 - MinusLogProbMetric: 45.4555 - val_loss: 45.6526 - val_MinusLogProbMetric: 45.6526 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 462/1000
2023-10-27 21:41:49.464 
Epoch 462/1000 
	 loss: 45.2124, MinusLogProbMetric: 45.2124, val_loss: 46.2199, val_MinusLogProbMetric: 46.2199

Epoch 462: val_loss did not improve from 45.00685
196/196 - 35s - loss: 45.2124 - MinusLogProbMetric: 45.2124 - val_loss: 46.2199 - val_MinusLogProbMetric: 46.2199 - lr: 3.3333e-04 - 35s/epoch - 176ms/step
Epoch 463/1000
2023-10-27 21:42:23.945 
Epoch 463/1000 
	 loss: 45.3608, MinusLogProbMetric: 45.3608, val_loss: 45.3851, val_MinusLogProbMetric: 45.3851

Epoch 463: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.3608 - MinusLogProbMetric: 45.3608 - val_loss: 45.3851 - val_MinusLogProbMetric: 45.3851 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 464/1000
2023-10-27 21:42:58.925 
Epoch 464/1000 
	 loss: 45.0424, MinusLogProbMetric: 45.0424, val_loss: 45.7606, val_MinusLogProbMetric: 45.7606

Epoch 464: val_loss did not improve from 45.00685
196/196 - 35s - loss: 45.0424 - MinusLogProbMetric: 45.0424 - val_loss: 45.7606 - val_MinusLogProbMetric: 45.7606 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 465/1000
2023-10-27 21:43:33.393 
Epoch 465/1000 
	 loss: 45.9371, MinusLogProbMetric: 45.9371, val_loss: 45.9368, val_MinusLogProbMetric: 45.9368

Epoch 465: val_loss did not improve from 45.00685
196/196 - 34s - loss: 45.9371 - MinusLogProbMetric: 45.9371 - val_loss: 45.9368 - val_MinusLogProbMetric: 45.9368 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 466/1000
2023-10-27 21:44:07.691 
Epoch 466/1000 
	 loss: 49.8764, MinusLogProbMetric: 49.8764, val_loss: 49.6986, val_MinusLogProbMetric: 49.6986

Epoch 466: val_loss did not improve from 45.00685
196/196 - 34s - loss: 49.8764 - MinusLogProbMetric: 49.8764 - val_loss: 49.6986 - val_MinusLogProbMetric: 49.6986 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 467/1000
2023-10-27 21:44:42.666 
Epoch 467/1000 
	 loss: 46.4362, MinusLogProbMetric: 46.4362, val_loss: 45.7190, val_MinusLogProbMetric: 45.7190

Epoch 467: val_loss did not improve from 45.00685
196/196 - 35s - loss: 46.4362 - MinusLogProbMetric: 46.4362 - val_loss: 45.7190 - val_MinusLogProbMetric: 45.7190 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 468/1000
2023-10-27 21:45:16.848 
Epoch 468/1000 
	 loss: 44.9445, MinusLogProbMetric: 44.9445, val_loss: 45.3513, val_MinusLogProbMetric: 45.3513

Epoch 468: val_loss did not improve from 45.00685
196/196 - 34s - loss: 44.9445 - MinusLogProbMetric: 44.9445 - val_loss: 45.3513 - val_MinusLogProbMetric: 45.3513 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 469/1000
2023-10-27 21:45:46.353 
Epoch 469/1000 
	 loss: 45.1556, MinusLogProbMetric: 45.1556, val_loss: 46.8762, val_MinusLogProbMetric: 46.8762

Epoch 469: val_loss did not improve from 45.00685
196/196 - 30s - loss: 45.1556 - MinusLogProbMetric: 45.1556 - val_loss: 46.8762 - val_MinusLogProbMetric: 46.8762 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 470/1000
2023-10-27 21:46:15.590 
Epoch 470/1000 
	 loss: 45.0475, MinusLogProbMetric: 45.0475, val_loss: 45.5052, val_MinusLogProbMetric: 45.5052

Epoch 470: val_loss did not improve from 45.00685
196/196 - 29s - loss: 45.0475 - MinusLogProbMetric: 45.0475 - val_loss: 45.5052 - val_MinusLogProbMetric: 45.5052 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 471/1000
2023-10-27 21:46:50.335 
Epoch 471/1000 
	 loss: 45.1217, MinusLogProbMetric: 45.1217, val_loss: 45.6472, val_MinusLogProbMetric: 45.6472

Epoch 471: val_loss did not improve from 45.00685
196/196 - 35s - loss: 45.1217 - MinusLogProbMetric: 45.1217 - val_loss: 45.6472 - val_MinusLogProbMetric: 45.6472 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 472/1000
2023-10-27 21:47:19.388 
Epoch 472/1000 
	 loss: 44.8547, MinusLogProbMetric: 44.8547, val_loss: 46.6735, val_MinusLogProbMetric: 46.6735

Epoch 472: val_loss did not improve from 45.00685
196/196 - 29s - loss: 44.8547 - MinusLogProbMetric: 44.8547 - val_loss: 46.6735 - val_MinusLogProbMetric: 46.6735 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 473/1000
2023-10-27 21:47:52.225 
Epoch 473/1000 
	 loss: 44.8112, MinusLogProbMetric: 44.8112, val_loss: 47.9211, val_MinusLogProbMetric: 47.9211

Epoch 473: val_loss did not improve from 45.00685
196/196 - 33s - loss: 44.8112 - MinusLogProbMetric: 44.8112 - val_loss: 47.9211 - val_MinusLogProbMetric: 47.9211 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 474/1000
2023-10-27 21:48:24.482 
Epoch 474/1000 
	 loss: 44.8361, MinusLogProbMetric: 44.8361, val_loss: 45.8667, val_MinusLogProbMetric: 45.8667

Epoch 474: val_loss did not improve from 45.00685
196/196 - 32s - loss: 44.8361 - MinusLogProbMetric: 44.8361 - val_loss: 45.8667 - val_MinusLogProbMetric: 45.8667 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 475/1000
2023-10-27 21:48:54.650 
Epoch 475/1000 
	 loss: 45.0940, MinusLogProbMetric: 45.0940, val_loss: 46.2356, val_MinusLogProbMetric: 46.2356

Epoch 475: val_loss did not improve from 45.00685
196/196 - 30s - loss: 45.0940 - MinusLogProbMetric: 45.0940 - val_loss: 46.2356 - val_MinusLogProbMetric: 46.2356 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 476/1000
2023-10-27 21:49:29.183 
Epoch 476/1000 
	 loss: 45.4949, MinusLogProbMetric: 45.4949, val_loss: 44.8521, val_MinusLogProbMetric: 44.8521

Epoch 476: val_loss improved from 45.00685 to 44.85213, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 45.4949 - MinusLogProbMetric: 45.4949 - val_loss: 44.8521 - val_MinusLogProbMetric: 44.8521 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 477/1000
2023-10-27 21:50:00.244 
Epoch 477/1000 
	 loss: 44.9195, MinusLogProbMetric: 44.9195, val_loss: 46.7163, val_MinusLogProbMetric: 46.7163

Epoch 477: val_loss did not improve from 44.85213
196/196 - 31s - loss: 44.9195 - MinusLogProbMetric: 44.9195 - val_loss: 46.7163 - val_MinusLogProbMetric: 46.7163 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 478/1000
2023-10-27 21:50:31.852 
Epoch 478/1000 
	 loss: 45.2100, MinusLogProbMetric: 45.2100, val_loss: 50.8273, val_MinusLogProbMetric: 50.8273

Epoch 478: val_loss did not improve from 44.85213
196/196 - 32s - loss: 45.2100 - MinusLogProbMetric: 45.2100 - val_loss: 50.8273 - val_MinusLogProbMetric: 50.8273 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 479/1000
2023-10-27 21:51:06.312 
Epoch 479/1000 
	 loss: 45.3493, MinusLogProbMetric: 45.3493, val_loss: 44.5836, val_MinusLogProbMetric: 44.5836

Epoch 479: val_loss improved from 44.85213 to 44.58364, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 45.3493 - MinusLogProbMetric: 45.3493 - val_loss: 44.5836 - val_MinusLogProbMetric: 44.5836 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 480/1000
2023-10-27 21:51:35.907 
Epoch 480/1000 
	 loss: 45.2366, MinusLogProbMetric: 45.2366, val_loss: 46.5681, val_MinusLogProbMetric: 46.5681

Epoch 480: val_loss did not improve from 44.58364
196/196 - 29s - loss: 45.2366 - MinusLogProbMetric: 45.2366 - val_loss: 46.5681 - val_MinusLogProbMetric: 46.5681 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 481/1000
2023-10-27 21:52:07.828 
Epoch 481/1000 
	 loss: 45.1253, MinusLogProbMetric: 45.1253, val_loss: 49.2651, val_MinusLogProbMetric: 49.2651

Epoch 481: val_loss did not improve from 44.58364
196/196 - 32s - loss: 45.1253 - MinusLogProbMetric: 45.1253 - val_loss: 49.2651 - val_MinusLogProbMetric: 49.2651 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 482/1000
2023-10-27 21:52:38.131 
Epoch 482/1000 
	 loss: 45.4298, MinusLogProbMetric: 45.4298, val_loss: 46.6202, val_MinusLogProbMetric: 46.6202

Epoch 482: val_loss did not improve from 44.58364
196/196 - 30s - loss: 45.4298 - MinusLogProbMetric: 45.4298 - val_loss: 46.6202 - val_MinusLogProbMetric: 46.6202 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 483/1000
2023-10-27 21:53:07.964 
Epoch 483/1000 
	 loss: 44.5352, MinusLogProbMetric: 44.5352, val_loss: 46.2228, val_MinusLogProbMetric: 46.2228

Epoch 483: val_loss did not improve from 44.58364
196/196 - 30s - loss: 44.5352 - MinusLogProbMetric: 44.5352 - val_loss: 46.2228 - val_MinusLogProbMetric: 46.2228 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 484/1000
2023-10-27 21:53:42.718 
Epoch 484/1000 
	 loss: 45.5539, MinusLogProbMetric: 45.5539, val_loss: 44.9776, val_MinusLogProbMetric: 44.9776

Epoch 484: val_loss did not improve from 44.58364
196/196 - 35s - loss: 45.5539 - MinusLogProbMetric: 45.5539 - val_loss: 44.9776 - val_MinusLogProbMetric: 44.9776 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 485/1000
2023-10-27 21:54:11.801 
Epoch 485/1000 
	 loss: 44.9030, MinusLogProbMetric: 44.9030, val_loss: 45.4319, val_MinusLogProbMetric: 45.4319

Epoch 485: val_loss did not improve from 44.58364
196/196 - 29s - loss: 44.9030 - MinusLogProbMetric: 44.9030 - val_loss: 45.4319 - val_MinusLogProbMetric: 45.4319 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 486/1000
2023-10-27 21:54:43.128 
Epoch 486/1000 
	 loss: 45.7551, MinusLogProbMetric: 45.7551, val_loss: 45.9760, val_MinusLogProbMetric: 45.9760

Epoch 486: val_loss did not improve from 44.58364
196/196 - 31s - loss: 45.7551 - MinusLogProbMetric: 45.7551 - val_loss: 45.9760 - val_MinusLogProbMetric: 45.9760 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 487/1000
2023-10-27 21:55:15.632 
Epoch 487/1000 
	 loss: 45.5207, MinusLogProbMetric: 45.5207, val_loss: 45.6832, val_MinusLogProbMetric: 45.6832

Epoch 487: val_loss did not improve from 44.58364
196/196 - 33s - loss: 45.5207 - MinusLogProbMetric: 45.5207 - val_loss: 45.6832 - val_MinusLogProbMetric: 45.6832 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 488/1000
2023-10-27 21:55:45.597 
Epoch 488/1000 
	 loss: 44.6430, MinusLogProbMetric: 44.6430, val_loss: 44.6512, val_MinusLogProbMetric: 44.6512

Epoch 488: val_loss did not improve from 44.58364
196/196 - 30s - loss: 44.6430 - MinusLogProbMetric: 44.6430 - val_loss: 44.6512 - val_MinusLogProbMetric: 44.6512 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 489/1000
2023-10-27 21:56:19.454 
Epoch 489/1000 
	 loss: 45.5347, MinusLogProbMetric: 45.5347, val_loss: 46.1876, val_MinusLogProbMetric: 46.1876

Epoch 489: val_loss did not improve from 44.58364
196/196 - 34s - loss: 45.5347 - MinusLogProbMetric: 45.5347 - val_loss: 46.1876 - val_MinusLogProbMetric: 46.1876 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 490/1000
2023-10-27 21:56:49.200 
Epoch 490/1000 
	 loss: 45.7819, MinusLogProbMetric: 45.7819, val_loss: 45.8111, val_MinusLogProbMetric: 45.8111

Epoch 490: val_loss did not improve from 44.58364
196/196 - 30s - loss: 45.7819 - MinusLogProbMetric: 45.7819 - val_loss: 45.8111 - val_MinusLogProbMetric: 45.8111 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 491/1000
2023-10-27 21:57:19.237 
Epoch 491/1000 
	 loss: 44.7382, MinusLogProbMetric: 44.7382, val_loss: 45.1644, val_MinusLogProbMetric: 45.1644

Epoch 491: val_loss did not improve from 44.58364
196/196 - 30s - loss: 44.7382 - MinusLogProbMetric: 44.7382 - val_loss: 45.1644 - val_MinusLogProbMetric: 45.1644 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 492/1000
2023-10-27 21:57:50.231 
Epoch 492/1000 
	 loss: 45.5442, MinusLogProbMetric: 45.5442, val_loss: 45.6546, val_MinusLogProbMetric: 45.6546

Epoch 492: val_loss did not improve from 44.58364
196/196 - 31s - loss: 45.5442 - MinusLogProbMetric: 45.5442 - val_loss: 45.6546 - val_MinusLogProbMetric: 45.6546 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 493/1000
2023-10-27 21:58:19.538 
Epoch 493/1000 
	 loss: 45.9552, MinusLogProbMetric: 45.9552, val_loss: 44.7993, val_MinusLogProbMetric: 44.7993

Epoch 493: val_loss did not improve from 44.58364
196/196 - 29s - loss: 45.9552 - MinusLogProbMetric: 45.9552 - val_loss: 44.7993 - val_MinusLogProbMetric: 44.7993 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 494/1000
2023-10-27 21:58:50.920 
Epoch 494/1000 
	 loss: 44.9827, MinusLogProbMetric: 44.9827, val_loss: 45.9968, val_MinusLogProbMetric: 45.9968

Epoch 494: val_loss did not improve from 44.58364
196/196 - 31s - loss: 44.9827 - MinusLogProbMetric: 44.9827 - val_loss: 45.9968 - val_MinusLogProbMetric: 45.9968 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 495/1000
2023-10-27 21:59:21.912 
Epoch 495/1000 
	 loss: 44.8759, MinusLogProbMetric: 44.8759, val_loss: 45.1782, val_MinusLogProbMetric: 45.1782

Epoch 495: val_loss did not improve from 44.58364
196/196 - 31s - loss: 44.8759 - MinusLogProbMetric: 44.8759 - val_loss: 45.1782 - val_MinusLogProbMetric: 45.1782 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 496/1000
2023-10-27 21:59:51.068 
Epoch 496/1000 
	 loss: 45.4859, MinusLogProbMetric: 45.4859, val_loss: 46.8598, val_MinusLogProbMetric: 46.8598

Epoch 496: val_loss did not improve from 44.58364
196/196 - 29s - loss: 45.4859 - MinusLogProbMetric: 45.4859 - val_loss: 46.8598 - val_MinusLogProbMetric: 46.8598 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 497/1000
2023-10-27 22:00:25.359 
Epoch 497/1000 
	 loss: 45.2502, MinusLogProbMetric: 45.2502, val_loss: 44.7707, val_MinusLogProbMetric: 44.7707

Epoch 497: val_loss did not improve from 44.58364
196/196 - 34s - loss: 45.2502 - MinusLogProbMetric: 45.2502 - val_loss: 44.7707 - val_MinusLogProbMetric: 44.7707 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 498/1000
2023-10-27 22:00:56.180 
Epoch 498/1000 
	 loss: 44.7891, MinusLogProbMetric: 44.7891, val_loss: 45.2157, val_MinusLogProbMetric: 45.2157

Epoch 498: val_loss did not improve from 44.58364
196/196 - 31s - loss: 44.7891 - MinusLogProbMetric: 44.7891 - val_loss: 45.2157 - val_MinusLogProbMetric: 45.2157 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 499/1000
2023-10-27 22:01:27.000 
Epoch 499/1000 
	 loss: 45.3482, MinusLogProbMetric: 45.3482, val_loss: 52.1040, val_MinusLogProbMetric: 52.1040

Epoch 499: val_loss did not improve from 44.58364
196/196 - 31s - loss: 45.3482 - MinusLogProbMetric: 45.3482 - val_loss: 52.1040 - val_MinusLogProbMetric: 52.1040 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 500/1000
2023-10-27 22:01:58.291 
Epoch 500/1000 
	 loss: 44.9389, MinusLogProbMetric: 44.9389, val_loss: 46.1609, val_MinusLogProbMetric: 46.1609

Epoch 500: val_loss did not improve from 44.58364
196/196 - 31s - loss: 44.9389 - MinusLogProbMetric: 44.9389 - val_loss: 46.1609 - val_MinusLogProbMetric: 46.1609 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 501/1000
2023-10-27 22:02:28.022 
Epoch 501/1000 
	 loss: 47.9465, MinusLogProbMetric: 47.9465, val_loss: 44.9133, val_MinusLogProbMetric: 44.9133

Epoch 501: val_loss did not improve from 44.58364
196/196 - 30s - loss: 47.9465 - MinusLogProbMetric: 47.9465 - val_loss: 44.9133 - val_MinusLogProbMetric: 44.9133 - lr: 3.3333e-04 - 30s/epoch - 152ms/step
Epoch 502/1000
2023-10-27 22:03:02.085 
Epoch 502/1000 
	 loss: 44.7714, MinusLogProbMetric: 44.7714, val_loss: 45.3300, val_MinusLogProbMetric: 45.3300

Epoch 502: val_loss did not improve from 44.58364
196/196 - 34s - loss: 44.7714 - MinusLogProbMetric: 44.7714 - val_loss: 45.3300 - val_MinusLogProbMetric: 45.3300 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 503/1000
2023-10-27 22:03:33.440 
Epoch 503/1000 
	 loss: 44.8585, MinusLogProbMetric: 44.8585, val_loss: 45.0702, val_MinusLogProbMetric: 45.0702

Epoch 503: val_loss did not improve from 44.58364
196/196 - 31s - loss: 44.8585 - MinusLogProbMetric: 44.8585 - val_loss: 45.0702 - val_MinusLogProbMetric: 45.0702 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 504/1000
2023-10-27 22:04:02.627 
Epoch 504/1000 
	 loss: 44.7317, MinusLogProbMetric: 44.7317, val_loss: 47.7612, val_MinusLogProbMetric: 47.7612

Epoch 504: val_loss did not improve from 44.58364
196/196 - 29s - loss: 44.7317 - MinusLogProbMetric: 44.7317 - val_loss: 47.7612 - val_MinusLogProbMetric: 47.7612 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 505/1000
2023-10-27 22:04:36.803 
Epoch 505/1000 
	 loss: 45.0296, MinusLogProbMetric: 45.0296, val_loss: 44.7942, val_MinusLogProbMetric: 44.7942

Epoch 505: val_loss did not improve from 44.58364
196/196 - 34s - loss: 45.0296 - MinusLogProbMetric: 45.0296 - val_loss: 44.7942 - val_MinusLogProbMetric: 44.7942 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 506/1000
2023-10-27 22:05:11.491 
Epoch 506/1000 
	 loss: 44.6710, MinusLogProbMetric: 44.6710, val_loss: 46.0630, val_MinusLogProbMetric: 46.0630

Epoch 506: val_loss did not improve from 44.58364
196/196 - 35s - loss: 44.6710 - MinusLogProbMetric: 44.6710 - val_loss: 46.0630 - val_MinusLogProbMetric: 46.0630 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 507/1000
2023-10-27 22:05:46.355 
Epoch 507/1000 
	 loss: 45.4635, MinusLogProbMetric: 45.4635, val_loss: 45.0031, val_MinusLogProbMetric: 45.0031

Epoch 507: val_loss did not improve from 44.58364
196/196 - 35s - loss: 45.4635 - MinusLogProbMetric: 45.4635 - val_loss: 45.0031 - val_MinusLogProbMetric: 45.0031 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 508/1000
2023-10-27 22:06:21.169 
Epoch 508/1000 
	 loss: 44.8455, MinusLogProbMetric: 44.8455, val_loss: 45.0018, val_MinusLogProbMetric: 45.0018

Epoch 508: val_loss did not improve from 44.58364
196/196 - 35s - loss: 44.8455 - MinusLogProbMetric: 44.8455 - val_loss: 45.0018 - val_MinusLogProbMetric: 45.0018 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 509/1000
2023-10-27 22:06:54.802 
Epoch 509/1000 
	 loss: 44.9497, MinusLogProbMetric: 44.9497, val_loss: 46.5503, val_MinusLogProbMetric: 46.5503

Epoch 509: val_loss did not improve from 44.58364
196/196 - 34s - loss: 44.9497 - MinusLogProbMetric: 44.9497 - val_loss: 46.5503 - val_MinusLogProbMetric: 46.5503 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 510/1000
2023-10-27 22:07:29.080 
Epoch 510/1000 
	 loss: 44.6842, MinusLogProbMetric: 44.6842, val_loss: 46.0093, val_MinusLogProbMetric: 46.0093

Epoch 510: val_loss did not improve from 44.58364
196/196 - 34s - loss: 44.6842 - MinusLogProbMetric: 44.6842 - val_loss: 46.0093 - val_MinusLogProbMetric: 46.0093 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 511/1000
2023-10-27 22:08:03.145 
Epoch 511/1000 
	 loss: 45.2533, MinusLogProbMetric: 45.2533, val_loss: 45.3251, val_MinusLogProbMetric: 45.3251

Epoch 511: val_loss did not improve from 44.58364
196/196 - 34s - loss: 45.2533 - MinusLogProbMetric: 45.2533 - val_loss: 45.3251 - val_MinusLogProbMetric: 45.3251 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 512/1000
2023-10-27 22:08:37.621 
Epoch 512/1000 
	 loss: 44.5355, MinusLogProbMetric: 44.5355, val_loss: 44.8149, val_MinusLogProbMetric: 44.8149

Epoch 512: val_loss did not improve from 44.58364
196/196 - 34s - loss: 44.5355 - MinusLogProbMetric: 44.5355 - val_loss: 44.8149 - val_MinusLogProbMetric: 44.8149 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 513/1000
2023-10-27 22:09:12.397 
Epoch 513/1000 
	 loss: 44.6795, MinusLogProbMetric: 44.6795, val_loss: 45.2607, val_MinusLogProbMetric: 45.2607

Epoch 513: val_loss did not improve from 44.58364
196/196 - 35s - loss: 44.6795 - MinusLogProbMetric: 44.6795 - val_loss: 45.2607 - val_MinusLogProbMetric: 45.2607 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 514/1000
2023-10-27 22:09:46.223 
Epoch 514/1000 
	 loss: 45.5943, MinusLogProbMetric: 45.5943, val_loss: 48.0188, val_MinusLogProbMetric: 48.0188

Epoch 514: val_loss did not improve from 44.58364
196/196 - 34s - loss: 45.5943 - MinusLogProbMetric: 45.5943 - val_loss: 48.0188 - val_MinusLogProbMetric: 48.0188 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 515/1000
2023-10-27 22:10:20.246 
Epoch 515/1000 
	 loss: 45.2035, MinusLogProbMetric: 45.2035, val_loss: 47.3166, val_MinusLogProbMetric: 47.3166

Epoch 515: val_loss did not improve from 44.58364
196/196 - 34s - loss: 45.2035 - MinusLogProbMetric: 45.2035 - val_loss: 47.3166 - val_MinusLogProbMetric: 47.3166 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 516/1000
2023-10-27 22:10:54.437 
Epoch 516/1000 
	 loss: 44.8913, MinusLogProbMetric: 44.8913, val_loss: 46.4420, val_MinusLogProbMetric: 46.4420

Epoch 516: val_loss did not improve from 44.58364
196/196 - 34s - loss: 44.8913 - MinusLogProbMetric: 44.8913 - val_loss: 46.4420 - val_MinusLogProbMetric: 46.4420 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 517/1000
2023-10-27 22:11:28.231 
Epoch 517/1000 
	 loss: 45.2943, MinusLogProbMetric: 45.2943, val_loss: 46.1221, val_MinusLogProbMetric: 46.1221

Epoch 517: val_loss did not improve from 44.58364
196/196 - 34s - loss: 45.2943 - MinusLogProbMetric: 45.2943 - val_loss: 46.1221 - val_MinusLogProbMetric: 46.1221 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 518/1000
2023-10-27 22:12:02.729 
Epoch 518/1000 
	 loss: 44.5599, MinusLogProbMetric: 44.5599, val_loss: 45.1764, val_MinusLogProbMetric: 45.1764

Epoch 518: val_loss did not improve from 44.58364
196/196 - 34s - loss: 44.5599 - MinusLogProbMetric: 44.5599 - val_loss: 45.1764 - val_MinusLogProbMetric: 45.1764 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 519/1000
2023-10-27 22:12:37.538 
Epoch 519/1000 
	 loss: 44.9783, MinusLogProbMetric: 44.9783, val_loss: 45.3810, val_MinusLogProbMetric: 45.3810

Epoch 519: val_loss did not improve from 44.58364
196/196 - 35s - loss: 44.9783 - MinusLogProbMetric: 44.9783 - val_loss: 45.3810 - val_MinusLogProbMetric: 45.3810 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 520/1000
2023-10-27 22:13:12.019 
Epoch 520/1000 
	 loss: 44.5952, MinusLogProbMetric: 44.5952, val_loss: 45.2182, val_MinusLogProbMetric: 45.2182

Epoch 520: val_loss did not improve from 44.58364
196/196 - 34s - loss: 44.5952 - MinusLogProbMetric: 44.5952 - val_loss: 45.2182 - val_MinusLogProbMetric: 45.2182 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 521/1000
2023-10-27 22:13:46.777 
Epoch 521/1000 
	 loss: 45.8543, MinusLogProbMetric: 45.8543, val_loss: 51.5886, val_MinusLogProbMetric: 51.5886

Epoch 521: val_loss did not improve from 44.58364
196/196 - 35s - loss: 45.8543 - MinusLogProbMetric: 45.8543 - val_loss: 51.5886 - val_MinusLogProbMetric: 51.5886 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 522/1000
2023-10-27 22:14:21.017 
Epoch 522/1000 
	 loss: 46.3395, MinusLogProbMetric: 46.3395, val_loss: 51.6216, val_MinusLogProbMetric: 51.6216

Epoch 522: val_loss did not improve from 44.58364
196/196 - 34s - loss: 46.3395 - MinusLogProbMetric: 46.3395 - val_loss: 51.6216 - val_MinusLogProbMetric: 51.6216 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 523/1000
2023-10-27 22:14:55.087 
Epoch 523/1000 
	 loss: 45.5401, MinusLogProbMetric: 45.5401, val_loss: 48.2395, val_MinusLogProbMetric: 48.2395

Epoch 523: val_loss did not improve from 44.58364
196/196 - 34s - loss: 45.5401 - MinusLogProbMetric: 45.5401 - val_loss: 48.2395 - val_MinusLogProbMetric: 48.2395 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 524/1000
2023-10-27 22:15:29.979 
Epoch 524/1000 
	 loss: 44.5667, MinusLogProbMetric: 44.5667, val_loss: 44.4487, val_MinusLogProbMetric: 44.4487

Epoch 524: val_loss improved from 44.58364 to 44.44865, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 44.5667 - MinusLogProbMetric: 44.5667 - val_loss: 44.4487 - val_MinusLogProbMetric: 44.4487 - lr: 3.3333e-04 - 35s/epoch - 181ms/step
Epoch 525/1000
2023-10-27 22:16:04.988 
Epoch 525/1000 
	 loss: 44.7972, MinusLogProbMetric: 44.7972, val_loss: 45.6674, val_MinusLogProbMetric: 45.6674

Epoch 525: val_loss did not improve from 44.44865
196/196 - 34s - loss: 44.7972 - MinusLogProbMetric: 44.7972 - val_loss: 45.6674 - val_MinusLogProbMetric: 45.6674 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 526/1000
2023-10-27 22:16:39.620 
Epoch 526/1000 
	 loss: 44.5085, MinusLogProbMetric: 44.5085, val_loss: 46.4539, val_MinusLogProbMetric: 46.4539

Epoch 526: val_loss did not improve from 44.44865
196/196 - 35s - loss: 44.5085 - MinusLogProbMetric: 44.5085 - val_loss: 46.4539 - val_MinusLogProbMetric: 46.4539 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 527/1000
2023-10-27 22:17:13.726 
Epoch 527/1000 
	 loss: 44.6179, MinusLogProbMetric: 44.6179, val_loss: 45.5131, val_MinusLogProbMetric: 45.5131

Epoch 527: val_loss did not improve from 44.44865
196/196 - 34s - loss: 44.6179 - MinusLogProbMetric: 44.6179 - val_loss: 45.5131 - val_MinusLogProbMetric: 45.5131 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 528/1000
2023-10-27 22:17:47.788 
Epoch 528/1000 
	 loss: 45.5986, MinusLogProbMetric: 45.5986, val_loss: 45.9397, val_MinusLogProbMetric: 45.9397

Epoch 528: val_loss did not improve from 44.44865
196/196 - 34s - loss: 45.5986 - MinusLogProbMetric: 45.5986 - val_loss: 45.9397 - val_MinusLogProbMetric: 45.9397 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 529/1000
2023-10-27 22:18:21.827 
Epoch 529/1000 
	 loss: 44.7038, MinusLogProbMetric: 44.7038, val_loss: 46.2237, val_MinusLogProbMetric: 46.2237

Epoch 529: val_loss did not improve from 44.44865
196/196 - 34s - loss: 44.7038 - MinusLogProbMetric: 44.7038 - val_loss: 46.2237 - val_MinusLogProbMetric: 46.2237 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 530/1000
2023-10-27 22:18:56.638 
Epoch 530/1000 
	 loss: 44.6679, MinusLogProbMetric: 44.6679, val_loss: 45.8327, val_MinusLogProbMetric: 45.8327

Epoch 530: val_loss did not improve from 44.44865
196/196 - 35s - loss: 44.6679 - MinusLogProbMetric: 44.6679 - val_loss: 45.8327 - val_MinusLogProbMetric: 45.8327 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 531/1000
2023-10-27 22:19:32.372 
Epoch 531/1000 
	 loss: 44.6914, MinusLogProbMetric: 44.6914, val_loss: 45.6289, val_MinusLogProbMetric: 45.6289

Epoch 531: val_loss did not improve from 44.44865
196/196 - 36s - loss: 44.6914 - MinusLogProbMetric: 44.6914 - val_loss: 45.6289 - val_MinusLogProbMetric: 45.6289 - lr: 3.3333e-04 - 36s/epoch - 182ms/step
Epoch 532/1000
2023-10-27 22:20:07.103 
Epoch 532/1000 
	 loss: 44.7177, MinusLogProbMetric: 44.7177, val_loss: 45.2975, val_MinusLogProbMetric: 45.2975

Epoch 532: val_loss did not improve from 44.44865
196/196 - 35s - loss: 44.7177 - MinusLogProbMetric: 44.7177 - val_loss: 45.2975 - val_MinusLogProbMetric: 45.2975 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 533/1000
2023-10-27 22:20:41.439 
Epoch 533/1000 
	 loss: 45.3311, MinusLogProbMetric: 45.3311, val_loss: 49.4631, val_MinusLogProbMetric: 49.4631

Epoch 533: val_loss did not improve from 44.44865
196/196 - 34s - loss: 45.3311 - MinusLogProbMetric: 45.3311 - val_loss: 49.4631 - val_MinusLogProbMetric: 49.4631 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 534/1000
2023-10-27 22:21:15.232 
Epoch 534/1000 
	 loss: 44.3864, MinusLogProbMetric: 44.3864, val_loss: 45.3411, val_MinusLogProbMetric: 45.3411

Epoch 534: val_loss did not improve from 44.44865
196/196 - 34s - loss: 44.3864 - MinusLogProbMetric: 44.3864 - val_loss: 45.3411 - val_MinusLogProbMetric: 45.3411 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 535/1000
2023-10-27 22:21:49.830 
Epoch 535/1000 
	 loss: 45.5268, MinusLogProbMetric: 45.5268, val_loss: 44.7970, val_MinusLogProbMetric: 44.7970

Epoch 535: val_loss did not improve from 44.44865
196/196 - 35s - loss: 45.5268 - MinusLogProbMetric: 45.5268 - val_loss: 44.7970 - val_MinusLogProbMetric: 44.7970 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 536/1000
2023-10-27 22:22:23.821 
Epoch 536/1000 
	 loss: 45.3026, MinusLogProbMetric: 45.3026, val_loss: 45.6406, val_MinusLogProbMetric: 45.6406

Epoch 536: val_loss did not improve from 44.44865
196/196 - 34s - loss: 45.3026 - MinusLogProbMetric: 45.3026 - val_loss: 45.6406 - val_MinusLogProbMetric: 45.6406 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 537/1000
2023-10-27 22:22:57.936 
Epoch 537/1000 
	 loss: 44.6799, MinusLogProbMetric: 44.6799, val_loss: 46.0469, val_MinusLogProbMetric: 46.0469

Epoch 537: val_loss did not improve from 44.44865
196/196 - 34s - loss: 44.6799 - MinusLogProbMetric: 44.6799 - val_loss: 46.0469 - val_MinusLogProbMetric: 46.0469 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 538/1000
2023-10-27 22:23:31.614 
Epoch 538/1000 
	 loss: 44.6459, MinusLogProbMetric: 44.6459, val_loss: 44.7263, val_MinusLogProbMetric: 44.7263

Epoch 538: val_loss did not improve from 44.44865
196/196 - 34s - loss: 44.6459 - MinusLogProbMetric: 44.6459 - val_loss: 44.7263 - val_MinusLogProbMetric: 44.7263 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 539/1000
2023-10-27 22:24:05.596 
Epoch 539/1000 
	 loss: 44.8619, MinusLogProbMetric: 44.8619, val_loss: 46.2684, val_MinusLogProbMetric: 46.2684

Epoch 539: val_loss did not improve from 44.44865
196/196 - 34s - loss: 44.8619 - MinusLogProbMetric: 44.8619 - val_loss: 46.2684 - val_MinusLogProbMetric: 46.2684 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 540/1000
2023-10-27 22:24:39.637 
Epoch 540/1000 
	 loss: 45.3357, MinusLogProbMetric: 45.3357, val_loss: 45.2692, val_MinusLogProbMetric: 45.2692

Epoch 540: val_loss did not improve from 44.44865
196/196 - 34s - loss: 45.3357 - MinusLogProbMetric: 45.3357 - val_loss: 45.2692 - val_MinusLogProbMetric: 45.2692 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 541/1000
2023-10-27 22:25:14.093 
Epoch 541/1000 
	 loss: 44.5872, MinusLogProbMetric: 44.5872, val_loss: 45.6170, val_MinusLogProbMetric: 45.6170

Epoch 541: val_loss did not improve from 44.44865
196/196 - 34s - loss: 44.5872 - MinusLogProbMetric: 44.5872 - val_loss: 45.6170 - val_MinusLogProbMetric: 45.6170 - lr: 3.3333e-04 - 34s/epoch - 176ms/step
Epoch 542/1000
2023-10-27 22:25:48.199 
Epoch 542/1000 
	 loss: 44.8046, MinusLogProbMetric: 44.8046, val_loss: 46.5438, val_MinusLogProbMetric: 46.5438

Epoch 542: val_loss did not improve from 44.44865
196/196 - 34s - loss: 44.8046 - MinusLogProbMetric: 44.8046 - val_loss: 46.5438 - val_MinusLogProbMetric: 46.5438 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 543/1000
2023-10-27 22:26:23.155 
Epoch 543/1000 
	 loss: 44.9943, MinusLogProbMetric: 44.9943, val_loss: 47.4071, val_MinusLogProbMetric: 47.4071

Epoch 543: val_loss did not improve from 44.44865
196/196 - 35s - loss: 44.9943 - MinusLogProbMetric: 44.9943 - val_loss: 47.4071 - val_MinusLogProbMetric: 47.4071 - lr: 3.3333e-04 - 35s/epoch - 178ms/step
Epoch 544/1000
2023-10-27 22:26:57.397 
Epoch 544/1000 
	 loss: 44.8905, MinusLogProbMetric: 44.8905, val_loss: 44.7854, val_MinusLogProbMetric: 44.7854

Epoch 544: val_loss did not improve from 44.44865
196/196 - 34s - loss: 44.8905 - MinusLogProbMetric: 44.8905 - val_loss: 44.7854 - val_MinusLogProbMetric: 44.7854 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 545/1000
2023-10-27 22:27:31.581 
Epoch 545/1000 
	 loss: 44.2913, MinusLogProbMetric: 44.2913, val_loss: 45.2044, val_MinusLogProbMetric: 45.2044

Epoch 545: val_loss did not improve from 44.44865
196/196 - 34s - loss: 44.2913 - MinusLogProbMetric: 44.2913 - val_loss: 45.2044 - val_MinusLogProbMetric: 45.2044 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 546/1000
2023-10-27 22:28:06.694 
Epoch 546/1000 
	 loss: 45.5876, MinusLogProbMetric: 45.5876, val_loss: 48.1188, val_MinusLogProbMetric: 48.1188

Epoch 546: val_loss did not improve from 44.44865
196/196 - 35s - loss: 45.5876 - MinusLogProbMetric: 45.5876 - val_loss: 48.1188 - val_MinusLogProbMetric: 48.1188 - lr: 3.3333e-04 - 35s/epoch - 179ms/step
Epoch 547/1000
2023-10-27 22:28:41.465 
Epoch 547/1000 
	 loss: 44.8299, MinusLogProbMetric: 44.8299, val_loss: 45.4780, val_MinusLogProbMetric: 45.4780

Epoch 547: val_loss did not improve from 44.44865
196/196 - 35s - loss: 44.8299 - MinusLogProbMetric: 44.8299 - val_loss: 45.4780 - val_MinusLogProbMetric: 45.4780 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 548/1000
2023-10-27 22:29:15.503 
Epoch 548/1000 
	 loss: 44.7909, MinusLogProbMetric: 44.7909, val_loss: 45.7973, val_MinusLogProbMetric: 45.7973

Epoch 548: val_loss did not improve from 44.44865
196/196 - 34s - loss: 44.7909 - MinusLogProbMetric: 44.7909 - val_loss: 45.7973 - val_MinusLogProbMetric: 45.7973 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 549/1000
2023-10-27 22:29:42.136 
Epoch 549/1000 
	 loss: 44.2421, MinusLogProbMetric: 44.2421, val_loss: 45.2920, val_MinusLogProbMetric: 45.2920

Epoch 549: val_loss did not improve from 44.44865
196/196 - 27s - loss: 44.2421 - MinusLogProbMetric: 44.2421 - val_loss: 45.2920 - val_MinusLogProbMetric: 45.2920 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 550/1000
2023-10-27 22:30:08.712 
Epoch 550/1000 
	 loss: 44.7075, MinusLogProbMetric: 44.7075, val_loss: 47.0218, val_MinusLogProbMetric: 47.0218

Epoch 550: val_loss did not improve from 44.44865
196/196 - 27s - loss: 44.7075 - MinusLogProbMetric: 44.7075 - val_loss: 47.0218 - val_MinusLogProbMetric: 47.0218 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 551/1000
2023-10-27 22:30:35.006 
Epoch 551/1000 
	 loss: 45.1578, MinusLogProbMetric: 45.1578, val_loss: 45.9206, val_MinusLogProbMetric: 45.9206

Epoch 551: val_loss did not improve from 44.44865
196/196 - 26s - loss: 45.1578 - MinusLogProbMetric: 45.1578 - val_loss: 45.9206 - val_MinusLogProbMetric: 45.9206 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 552/1000
2023-10-27 22:31:02.600 
Epoch 552/1000 
	 loss: 44.1447, MinusLogProbMetric: 44.1447, val_loss: 44.5548, val_MinusLogProbMetric: 44.5548

Epoch 552: val_loss did not improve from 44.44865
196/196 - 28s - loss: 44.1447 - MinusLogProbMetric: 44.1447 - val_loss: 44.5548 - val_MinusLogProbMetric: 44.5548 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 553/1000
2023-10-27 22:31:31.022 
Epoch 553/1000 
	 loss: 45.3650, MinusLogProbMetric: 45.3650, val_loss: 45.9366, val_MinusLogProbMetric: 45.9366

Epoch 553: val_loss did not improve from 44.44865
196/196 - 28s - loss: 45.3650 - MinusLogProbMetric: 45.3650 - val_loss: 45.9366 - val_MinusLogProbMetric: 45.9366 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 554/1000
2023-10-27 22:31:58.533 
Epoch 554/1000 
	 loss: 44.6436, MinusLogProbMetric: 44.6436, val_loss: 45.8876, val_MinusLogProbMetric: 45.8876

Epoch 554: val_loss did not improve from 44.44865
196/196 - 28s - loss: 44.6436 - MinusLogProbMetric: 44.6436 - val_loss: 45.8876 - val_MinusLogProbMetric: 45.8876 - lr: 3.3333e-04 - 28s/epoch - 140ms/step
Epoch 555/1000
2023-10-27 22:32:24.764 
Epoch 555/1000 
	 loss: 44.4832, MinusLogProbMetric: 44.4832, val_loss: 46.1698, val_MinusLogProbMetric: 46.1698

Epoch 555: val_loss did not improve from 44.44865
196/196 - 26s - loss: 44.4832 - MinusLogProbMetric: 44.4832 - val_loss: 46.1698 - val_MinusLogProbMetric: 46.1698 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 556/1000
2023-10-27 22:32:51.519 
Epoch 556/1000 
	 loss: 44.2653, MinusLogProbMetric: 44.2653, val_loss: 45.3347, val_MinusLogProbMetric: 45.3347

Epoch 556: val_loss did not improve from 44.44865
196/196 - 27s - loss: 44.2653 - MinusLogProbMetric: 44.2653 - val_loss: 45.3347 - val_MinusLogProbMetric: 45.3347 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 557/1000
2023-10-27 22:33:18.264 
Epoch 557/1000 
	 loss: 45.0952, MinusLogProbMetric: 45.0952, val_loss: 45.0017, val_MinusLogProbMetric: 45.0017

Epoch 557: val_loss did not improve from 44.44865
196/196 - 27s - loss: 45.0952 - MinusLogProbMetric: 45.0952 - val_loss: 45.0017 - val_MinusLogProbMetric: 45.0017 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 558/1000
2023-10-27 22:33:46.349 
Epoch 558/1000 
	 loss: 44.9990, MinusLogProbMetric: 44.9990, val_loss: 46.1090, val_MinusLogProbMetric: 46.1090

Epoch 558: val_loss did not improve from 44.44865
196/196 - 28s - loss: 44.9990 - MinusLogProbMetric: 44.9990 - val_loss: 46.1090 - val_MinusLogProbMetric: 46.1090 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 559/1000
2023-10-27 22:34:15.599 
Epoch 559/1000 
	 loss: 44.2786, MinusLogProbMetric: 44.2786, val_loss: 44.7440, val_MinusLogProbMetric: 44.7440

Epoch 559: val_loss did not improve from 44.44865
196/196 - 29s - loss: 44.2786 - MinusLogProbMetric: 44.2786 - val_loss: 44.7440 - val_MinusLogProbMetric: 44.7440 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 560/1000
2023-10-27 22:34:42.148 
Epoch 560/1000 
	 loss: 44.8902, MinusLogProbMetric: 44.8902, val_loss: 46.1512, val_MinusLogProbMetric: 46.1512

Epoch 560: val_loss did not improve from 44.44865
196/196 - 27s - loss: 44.8902 - MinusLogProbMetric: 44.8902 - val_loss: 46.1512 - val_MinusLogProbMetric: 46.1512 - lr: 3.3333e-04 - 27s/epoch - 135ms/step
Epoch 561/1000
2023-10-27 22:35:08.342 
Epoch 561/1000 
	 loss: 44.9191, MinusLogProbMetric: 44.9191, val_loss: 45.3998, val_MinusLogProbMetric: 45.3998

Epoch 561: val_loss did not improve from 44.44865
196/196 - 26s - loss: 44.9191 - MinusLogProbMetric: 44.9191 - val_loss: 45.3998 - val_MinusLogProbMetric: 45.3998 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 562/1000
2023-10-27 22:35:35.916 
Epoch 562/1000 
	 loss: 45.6113, MinusLogProbMetric: 45.6113, val_loss: 44.5047, val_MinusLogProbMetric: 44.5047

Epoch 562: val_loss did not improve from 44.44865
196/196 - 28s - loss: 45.6113 - MinusLogProbMetric: 45.6113 - val_loss: 44.5047 - val_MinusLogProbMetric: 44.5047 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 563/1000
2023-10-27 22:36:03.075 
Epoch 563/1000 
	 loss: 44.7517, MinusLogProbMetric: 44.7517, val_loss: 47.0002, val_MinusLogProbMetric: 47.0002

Epoch 563: val_loss did not improve from 44.44865
196/196 - 27s - loss: 44.7517 - MinusLogProbMetric: 44.7517 - val_loss: 47.0002 - val_MinusLogProbMetric: 47.0002 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 564/1000
2023-10-27 22:36:33.012 
Epoch 564/1000 
	 loss: 45.0356, MinusLogProbMetric: 45.0356, val_loss: 47.0187, val_MinusLogProbMetric: 47.0187

Epoch 564: val_loss did not improve from 44.44865
196/196 - 30s - loss: 45.0356 - MinusLogProbMetric: 45.0356 - val_loss: 47.0187 - val_MinusLogProbMetric: 47.0187 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 565/1000
2023-10-27 22:36:59.884 
Epoch 565/1000 
	 loss: 44.4824, MinusLogProbMetric: 44.4824, val_loss: 46.3419, val_MinusLogProbMetric: 46.3419

Epoch 565: val_loss did not improve from 44.44865
196/196 - 27s - loss: 44.4824 - MinusLogProbMetric: 44.4824 - val_loss: 46.3419 - val_MinusLogProbMetric: 46.3419 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 566/1000
2023-10-27 22:37:25.987 
Epoch 566/1000 
	 loss: 44.8740, MinusLogProbMetric: 44.8740, val_loss: 47.0484, val_MinusLogProbMetric: 47.0484

Epoch 566: val_loss did not improve from 44.44865
196/196 - 26s - loss: 44.8740 - MinusLogProbMetric: 44.8740 - val_loss: 47.0484 - val_MinusLogProbMetric: 47.0484 - lr: 3.3333e-04 - 26s/epoch - 133ms/step
Epoch 567/1000
2023-10-27 22:37:52.153 
Epoch 567/1000 
	 loss: 44.9252, MinusLogProbMetric: 44.9252, val_loss: 45.7336, val_MinusLogProbMetric: 45.7336

Epoch 567: val_loss did not improve from 44.44865
196/196 - 26s - loss: 44.9252 - MinusLogProbMetric: 44.9252 - val_loss: 45.7336 - val_MinusLogProbMetric: 45.7336 - lr: 3.3333e-04 - 26s/epoch - 133ms/step
Epoch 568/1000
2023-10-27 22:38:19.586 
Epoch 568/1000 
	 loss: 45.0311, MinusLogProbMetric: 45.0311, val_loss: 44.8116, val_MinusLogProbMetric: 44.8116

Epoch 568: val_loss did not improve from 44.44865
196/196 - 27s - loss: 45.0311 - MinusLogProbMetric: 45.0311 - val_loss: 44.8116 - val_MinusLogProbMetric: 44.8116 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 569/1000
2023-10-27 22:38:47.995 
Epoch 569/1000 
	 loss: 44.7569, MinusLogProbMetric: 44.7569, val_loss: 45.6806, val_MinusLogProbMetric: 45.6806

Epoch 569: val_loss did not improve from 44.44865
196/196 - 28s - loss: 44.7569 - MinusLogProbMetric: 44.7569 - val_loss: 45.6806 - val_MinusLogProbMetric: 45.6806 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 570/1000
2023-10-27 22:39:14.730 
Epoch 570/1000 
	 loss: 44.3688, MinusLogProbMetric: 44.3688, val_loss: 45.8892, val_MinusLogProbMetric: 45.8892

Epoch 570: val_loss did not improve from 44.44865
196/196 - 27s - loss: 44.3688 - MinusLogProbMetric: 44.3688 - val_loss: 45.8892 - val_MinusLogProbMetric: 45.8892 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 571/1000
2023-10-27 22:39:40.995 
Epoch 571/1000 
	 loss: 44.7328, MinusLogProbMetric: 44.7328, val_loss: 44.4799, val_MinusLogProbMetric: 44.4799

Epoch 571: val_loss did not improve from 44.44865
196/196 - 26s - loss: 44.7328 - MinusLogProbMetric: 44.7328 - val_loss: 44.4799 - val_MinusLogProbMetric: 44.4799 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 572/1000
2023-10-27 22:40:07.779 
Epoch 572/1000 
	 loss: 44.4855, MinusLogProbMetric: 44.4855, val_loss: 44.9722, val_MinusLogProbMetric: 44.9722

Epoch 572: val_loss did not improve from 44.44865
196/196 - 27s - loss: 44.4855 - MinusLogProbMetric: 44.4855 - val_loss: 44.9722 - val_MinusLogProbMetric: 44.9722 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 573/1000
2023-10-27 22:40:34.627 
Epoch 573/1000 
	 loss: 45.5399, MinusLogProbMetric: 45.5399, val_loss: 44.3111, val_MinusLogProbMetric: 44.3111

Epoch 573: val_loss improved from 44.44865 to 44.31107, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 27s - loss: 45.5399 - MinusLogProbMetric: 45.5399 - val_loss: 44.3111 - val_MinusLogProbMetric: 44.3111 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 574/1000
2023-10-27 22:41:01.572 
Epoch 574/1000 
	 loss: 44.1624, MinusLogProbMetric: 44.1624, val_loss: 44.9398, val_MinusLogProbMetric: 44.9398

Epoch 574: val_loss did not improve from 44.31107
196/196 - 27s - loss: 44.1624 - MinusLogProbMetric: 44.1624 - val_loss: 44.9398 - val_MinusLogProbMetric: 44.9398 - lr: 3.3333e-04 - 27s/epoch - 135ms/step
Epoch 575/1000
2023-10-27 22:41:28.070 
Epoch 575/1000 
	 loss: 44.5234, MinusLogProbMetric: 44.5234, val_loss: 46.8409, val_MinusLogProbMetric: 46.8409

Epoch 575: val_loss did not improve from 44.31107
196/196 - 26s - loss: 44.5234 - MinusLogProbMetric: 44.5234 - val_loss: 46.8409 - val_MinusLogProbMetric: 46.8409 - lr: 3.3333e-04 - 26s/epoch - 135ms/step
Epoch 576/1000
2023-10-27 22:41:54.657 
Epoch 576/1000 
	 loss: 44.2649, MinusLogProbMetric: 44.2649, val_loss: 44.5567, val_MinusLogProbMetric: 44.5567

Epoch 576: val_loss did not improve from 44.31107
196/196 - 27s - loss: 44.2649 - MinusLogProbMetric: 44.2649 - val_loss: 44.5567 - val_MinusLogProbMetric: 44.5567 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 577/1000
2023-10-27 22:42:21.579 
Epoch 577/1000 
	 loss: 44.9609, MinusLogProbMetric: 44.9609, val_loss: 46.5347, val_MinusLogProbMetric: 46.5347

Epoch 577: val_loss did not improve from 44.31107
196/196 - 27s - loss: 44.9609 - MinusLogProbMetric: 44.9609 - val_loss: 46.5347 - val_MinusLogProbMetric: 46.5347 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 578/1000
2023-10-27 22:42:49.244 
Epoch 578/1000 
	 loss: 44.3016, MinusLogProbMetric: 44.3016, val_loss: 44.8175, val_MinusLogProbMetric: 44.8175

Epoch 578: val_loss did not improve from 44.31107
196/196 - 28s - loss: 44.3016 - MinusLogProbMetric: 44.3016 - val_loss: 44.8175 - val_MinusLogProbMetric: 44.8175 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 579/1000
2023-10-27 22:43:17.134 
Epoch 579/1000 
	 loss: 44.2493, MinusLogProbMetric: 44.2493, val_loss: 44.2962, val_MinusLogProbMetric: 44.2962

Epoch 579: val_loss improved from 44.31107 to 44.29625, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 28s - loss: 44.2493 - MinusLogProbMetric: 44.2493 - val_loss: 44.2962 - val_MinusLogProbMetric: 44.2962 - lr: 3.3333e-04 - 28s/epoch - 144ms/step
Epoch 580/1000
2023-10-27 22:43:43.866 
Epoch 580/1000 
	 loss: 44.7212, MinusLogProbMetric: 44.7212, val_loss: 47.4902, val_MinusLogProbMetric: 47.4902

Epoch 580: val_loss did not improve from 44.29625
196/196 - 26s - loss: 44.7212 - MinusLogProbMetric: 44.7212 - val_loss: 47.4902 - val_MinusLogProbMetric: 47.4902 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 581/1000
2023-10-27 22:44:10.437 
Epoch 581/1000 
	 loss: 44.6892, MinusLogProbMetric: 44.6892, val_loss: 46.5922, val_MinusLogProbMetric: 46.5922

Epoch 581: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.6892 - MinusLogProbMetric: 44.6892 - val_loss: 46.5922 - val_MinusLogProbMetric: 46.5922 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 582/1000
2023-10-27 22:44:38.016 
Epoch 582/1000 
	 loss: 46.4796, MinusLogProbMetric: 46.4796, val_loss: 45.2145, val_MinusLogProbMetric: 45.2145

Epoch 582: val_loss did not improve from 44.29625
196/196 - 28s - loss: 46.4796 - MinusLogProbMetric: 46.4796 - val_loss: 45.2145 - val_MinusLogProbMetric: 45.2145 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 583/1000
2023-10-27 22:45:04.767 
Epoch 583/1000 
	 loss: 44.3190, MinusLogProbMetric: 44.3190, val_loss: 46.2057, val_MinusLogProbMetric: 46.2057

Epoch 583: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.3190 - MinusLogProbMetric: 44.3190 - val_loss: 46.2057 - val_MinusLogProbMetric: 46.2057 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 584/1000
2023-10-27 22:45:34.382 
Epoch 584/1000 
	 loss: 45.0365, MinusLogProbMetric: 45.0365, val_loss: 44.9362, val_MinusLogProbMetric: 44.9362

Epoch 584: val_loss did not improve from 44.29625
196/196 - 30s - loss: 45.0365 - MinusLogProbMetric: 45.0365 - val_loss: 44.9362 - val_MinusLogProbMetric: 44.9362 - lr: 3.3333e-04 - 30s/epoch - 151ms/step
Epoch 585/1000
2023-10-27 22:46:01.509 
Epoch 585/1000 
	 loss: 44.2395, MinusLogProbMetric: 44.2395, val_loss: 45.3410, val_MinusLogProbMetric: 45.3410

Epoch 585: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.2395 - MinusLogProbMetric: 44.2395 - val_loss: 45.3410 - val_MinusLogProbMetric: 45.3410 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 586/1000
2023-10-27 22:46:28.408 
Epoch 586/1000 
	 loss: 44.1577, MinusLogProbMetric: 44.1577, val_loss: 45.0799, val_MinusLogProbMetric: 45.0799

Epoch 586: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.1577 - MinusLogProbMetric: 44.1577 - val_loss: 45.0799 - val_MinusLogProbMetric: 45.0799 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 587/1000
2023-10-27 22:46:55.017 
Epoch 587/1000 
	 loss: 44.6430, MinusLogProbMetric: 44.6430, val_loss: 44.5587, val_MinusLogProbMetric: 44.5587

Epoch 587: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.6430 - MinusLogProbMetric: 44.6430 - val_loss: 44.5587 - val_MinusLogProbMetric: 44.5587 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 588/1000
2023-10-27 22:47:23.106 
Epoch 588/1000 
	 loss: 44.4199, MinusLogProbMetric: 44.4199, val_loss: 48.8574, val_MinusLogProbMetric: 48.8574

Epoch 588: val_loss did not improve from 44.29625
196/196 - 28s - loss: 44.4199 - MinusLogProbMetric: 44.4199 - val_loss: 48.8574 - val_MinusLogProbMetric: 48.8574 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 589/1000
2023-10-27 22:47:52.471 
Epoch 589/1000 
	 loss: 44.7451, MinusLogProbMetric: 44.7451, val_loss: 47.6081, val_MinusLogProbMetric: 47.6081

Epoch 589: val_loss did not improve from 44.29625
196/196 - 29s - loss: 44.7451 - MinusLogProbMetric: 44.7451 - val_loss: 47.6081 - val_MinusLogProbMetric: 47.6081 - lr: 3.3333e-04 - 29s/epoch - 150ms/step
Epoch 590/1000
2023-10-27 22:48:18.886 
Epoch 590/1000 
	 loss: 45.0333, MinusLogProbMetric: 45.0333, val_loss: 45.0211, val_MinusLogProbMetric: 45.0211

Epoch 590: val_loss did not improve from 44.29625
196/196 - 26s - loss: 45.0333 - MinusLogProbMetric: 45.0333 - val_loss: 45.0211 - val_MinusLogProbMetric: 45.0211 - lr: 3.3333e-04 - 26s/epoch - 135ms/step
Epoch 591/1000
2023-10-27 22:48:45.180 
Epoch 591/1000 
	 loss: 44.1305, MinusLogProbMetric: 44.1305, val_loss: 44.7488, val_MinusLogProbMetric: 44.7488

Epoch 591: val_loss did not improve from 44.29625
196/196 - 26s - loss: 44.1305 - MinusLogProbMetric: 44.1305 - val_loss: 44.7488 - val_MinusLogProbMetric: 44.7488 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 592/1000
2023-10-27 22:49:11.535 
Epoch 592/1000 
	 loss: 45.0305, MinusLogProbMetric: 45.0305, val_loss: 45.0228, val_MinusLogProbMetric: 45.0228

Epoch 592: val_loss did not improve from 44.29625
196/196 - 26s - loss: 45.0305 - MinusLogProbMetric: 45.0305 - val_loss: 45.0228 - val_MinusLogProbMetric: 45.0228 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 593/1000
2023-10-27 22:49:40.329 
Epoch 593/1000 
	 loss: 44.5741, MinusLogProbMetric: 44.5741, val_loss: 45.2937, val_MinusLogProbMetric: 45.2937

Epoch 593: val_loss did not improve from 44.29625
196/196 - 29s - loss: 44.5741 - MinusLogProbMetric: 44.5741 - val_loss: 45.2937 - val_MinusLogProbMetric: 45.2937 - lr: 3.3333e-04 - 29s/epoch - 147ms/step
Epoch 594/1000
2023-10-27 22:50:07.008 
Epoch 594/1000 
	 loss: 44.7379, MinusLogProbMetric: 44.7379, val_loss: 49.4548, val_MinusLogProbMetric: 49.4548

Epoch 594: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.7379 - MinusLogProbMetric: 44.7379 - val_loss: 49.4548 - val_MinusLogProbMetric: 49.4548 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 595/1000
2023-10-27 22:50:33.289 
Epoch 595/1000 
	 loss: 45.2984, MinusLogProbMetric: 45.2984, val_loss: 44.5214, val_MinusLogProbMetric: 44.5214

Epoch 595: val_loss did not improve from 44.29625
196/196 - 26s - loss: 45.2984 - MinusLogProbMetric: 45.2984 - val_loss: 44.5214 - val_MinusLogProbMetric: 44.5214 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 596/1000
2023-10-27 22:50:59.864 
Epoch 596/1000 
	 loss: 44.1554, MinusLogProbMetric: 44.1554, val_loss: 45.1305, val_MinusLogProbMetric: 45.1305

Epoch 596: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.1554 - MinusLogProbMetric: 44.1554 - val_loss: 45.1305 - val_MinusLogProbMetric: 45.1305 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 597/1000
2023-10-27 22:51:25.955 
Epoch 597/1000 
	 loss: 44.5300, MinusLogProbMetric: 44.5300, val_loss: 45.1813, val_MinusLogProbMetric: 45.1813

Epoch 597: val_loss did not improve from 44.29625
196/196 - 26s - loss: 44.5300 - MinusLogProbMetric: 44.5300 - val_loss: 45.1813 - val_MinusLogProbMetric: 45.1813 - lr: 3.3333e-04 - 26s/epoch - 133ms/step
Epoch 598/1000
2023-10-27 22:51:52.960 
Epoch 598/1000 
	 loss: 44.4598, MinusLogProbMetric: 44.4598, val_loss: 44.8950, val_MinusLogProbMetric: 44.8950

Epoch 598: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.4598 - MinusLogProbMetric: 44.4598 - val_loss: 44.8950 - val_MinusLogProbMetric: 44.8950 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 599/1000
2023-10-27 22:52:23.150 
Epoch 599/1000 
	 loss: 45.1531, MinusLogProbMetric: 45.1531, val_loss: 46.5524, val_MinusLogProbMetric: 46.5524

Epoch 599: val_loss did not improve from 44.29625
196/196 - 30s - loss: 45.1531 - MinusLogProbMetric: 45.1531 - val_loss: 46.5524 - val_MinusLogProbMetric: 46.5524 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 600/1000
2023-10-27 22:52:50.102 
Epoch 600/1000 
	 loss: 44.5056, MinusLogProbMetric: 44.5056, val_loss: 44.5509, val_MinusLogProbMetric: 44.5509

Epoch 600: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.5056 - MinusLogProbMetric: 44.5056 - val_loss: 44.5509 - val_MinusLogProbMetric: 44.5509 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 601/1000
2023-10-27 22:53:16.373 
Epoch 601/1000 
	 loss: 44.4279, MinusLogProbMetric: 44.4279, val_loss: 50.5949, val_MinusLogProbMetric: 50.5949

Epoch 601: val_loss did not improve from 44.29625
196/196 - 26s - loss: 44.4279 - MinusLogProbMetric: 44.4279 - val_loss: 50.5949 - val_MinusLogProbMetric: 50.5949 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 602/1000
2023-10-27 22:53:42.656 
Epoch 602/1000 
	 loss: 44.9505, MinusLogProbMetric: 44.9505, val_loss: 45.1670, val_MinusLogProbMetric: 45.1670

Epoch 602: val_loss did not improve from 44.29625
196/196 - 26s - loss: 44.9505 - MinusLogProbMetric: 44.9505 - val_loss: 45.1670 - val_MinusLogProbMetric: 45.1670 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 603/1000
2023-10-27 22:54:10.150 
Epoch 603/1000 
	 loss: 44.4702, MinusLogProbMetric: 44.4702, val_loss: 44.4470, val_MinusLogProbMetric: 44.4470

Epoch 603: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.4702 - MinusLogProbMetric: 44.4702 - val_loss: 44.4470 - val_MinusLogProbMetric: 44.4470 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 604/1000
2023-10-27 22:54:38.633 
Epoch 604/1000 
	 loss: 44.1699, MinusLogProbMetric: 44.1699, val_loss: 46.3197, val_MinusLogProbMetric: 46.3197

Epoch 604: val_loss did not improve from 44.29625
196/196 - 28s - loss: 44.1699 - MinusLogProbMetric: 44.1699 - val_loss: 46.3197 - val_MinusLogProbMetric: 46.3197 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 605/1000
2023-10-27 22:55:06.273 
Epoch 605/1000 
	 loss: 45.1834, MinusLogProbMetric: 45.1834, val_loss: 44.3695, val_MinusLogProbMetric: 44.3695

Epoch 605: val_loss did not improve from 44.29625
196/196 - 28s - loss: 45.1834 - MinusLogProbMetric: 45.1834 - val_loss: 44.3695 - val_MinusLogProbMetric: 44.3695 - lr: 3.3333e-04 - 28s/epoch - 141ms/step
Epoch 606/1000
2023-10-27 22:55:32.457 
Epoch 606/1000 
	 loss: 44.1621, MinusLogProbMetric: 44.1621, val_loss: 45.7995, val_MinusLogProbMetric: 45.7995

Epoch 606: val_loss did not improve from 44.29625
196/196 - 26s - loss: 44.1621 - MinusLogProbMetric: 44.1621 - val_loss: 45.7995 - val_MinusLogProbMetric: 45.7995 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 607/1000
2023-10-27 22:55:59.369 
Epoch 607/1000 
	 loss: 44.5119, MinusLogProbMetric: 44.5119, val_loss: 45.5653, val_MinusLogProbMetric: 45.5653

Epoch 607: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.5119 - MinusLogProbMetric: 44.5119 - val_loss: 45.5653 - val_MinusLogProbMetric: 45.5653 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 608/1000
2023-10-27 22:56:31.151 
Epoch 608/1000 
	 loss: 44.9736, MinusLogProbMetric: 44.9736, val_loss: 45.2657, val_MinusLogProbMetric: 45.2657

Epoch 608: val_loss did not improve from 44.29625
196/196 - 32s - loss: 44.9736 - MinusLogProbMetric: 44.9736 - val_loss: 45.2657 - val_MinusLogProbMetric: 45.2657 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 609/1000
2023-10-27 22:57:01.476 
Epoch 609/1000 
	 loss: 45.1568, MinusLogProbMetric: 45.1568, val_loss: 45.2166, val_MinusLogProbMetric: 45.2166

Epoch 609: val_loss did not improve from 44.29625
196/196 - 30s - loss: 45.1568 - MinusLogProbMetric: 45.1568 - val_loss: 45.2166 - val_MinusLogProbMetric: 45.2166 - lr: 3.3333e-04 - 30s/epoch - 155ms/step
Epoch 610/1000
2023-10-27 22:57:27.716 
Epoch 610/1000 
	 loss: 45.1091, MinusLogProbMetric: 45.1091, val_loss: 44.4750, val_MinusLogProbMetric: 44.4750

Epoch 610: val_loss did not improve from 44.29625
196/196 - 26s - loss: 45.1091 - MinusLogProbMetric: 45.1091 - val_loss: 44.4750 - val_MinusLogProbMetric: 44.4750 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 611/1000
2023-10-27 22:57:54.102 
Epoch 611/1000 
	 loss: 44.5759, MinusLogProbMetric: 44.5759, val_loss: 45.9092, val_MinusLogProbMetric: 45.9092

Epoch 611: val_loss did not improve from 44.29625
196/196 - 26s - loss: 44.5759 - MinusLogProbMetric: 44.5759 - val_loss: 45.9092 - val_MinusLogProbMetric: 45.9092 - lr: 3.3333e-04 - 26s/epoch - 135ms/step
Epoch 612/1000
2023-10-27 22:58:21.009 
Epoch 612/1000 
	 loss: 45.8524, MinusLogProbMetric: 45.8524, val_loss: 47.4320, val_MinusLogProbMetric: 47.4320

Epoch 612: val_loss did not improve from 44.29625
196/196 - 27s - loss: 45.8524 - MinusLogProbMetric: 45.8524 - val_loss: 47.4320 - val_MinusLogProbMetric: 47.4320 - lr: 3.3333e-04 - 27s/epoch - 137ms/step
Epoch 613/1000
2023-10-27 22:58:50.144 
Epoch 613/1000 
	 loss: 44.8445, MinusLogProbMetric: 44.8445, val_loss: 44.5125, val_MinusLogProbMetric: 44.5125

Epoch 613: val_loss did not improve from 44.29625
196/196 - 29s - loss: 44.8445 - MinusLogProbMetric: 44.8445 - val_loss: 44.5125 - val_MinusLogProbMetric: 44.5125 - lr: 3.3333e-04 - 29s/epoch - 149ms/step
Epoch 614/1000
2023-10-27 22:59:18.174 
Epoch 614/1000 
	 loss: 44.2799, MinusLogProbMetric: 44.2799, val_loss: 45.5228, val_MinusLogProbMetric: 45.5228

Epoch 614: val_loss did not improve from 44.29625
196/196 - 28s - loss: 44.2799 - MinusLogProbMetric: 44.2799 - val_loss: 45.5228 - val_MinusLogProbMetric: 45.5228 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 615/1000
2023-10-27 22:59:44.446 
Epoch 615/1000 
	 loss: 44.6014, MinusLogProbMetric: 44.6014, val_loss: 50.1317, val_MinusLogProbMetric: 50.1317

Epoch 615: val_loss did not improve from 44.29625
196/196 - 26s - loss: 44.6014 - MinusLogProbMetric: 44.6014 - val_loss: 50.1317 - val_MinusLogProbMetric: 50.1317 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 616/1000
2023-10-27 23:00:10.545 
Epoch 616/1000 
	 loss: 44.8959, MinusLogProbMetric: 44.8959, val_loss: 44.6534, val_MinusLogProbMetric: 44.6534

Epoch 616: val_loss did not improve from 44.29625
196/196 - 26s - loss: 44.8959 - MinusLogProbMetric: 44.8959 - val_loss: 44.6534 - val_MinusLogProbMetric: 44.6534 - lr: 3.3333e-04 - 26s/epoch - 133ms/step
Epoch 617/1000
2023-10-27 23:00:37.806 
Epoch 617/1000 
	 loss: 44.5056, MinusLogProbMetric: 44.5056, val_loss: 46.6467, val_MinusLogProbMetric: 46.6467

Epoch 617: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.5056 - MinusLogProbMetric: 44.5056 - val_loss: 46.6467 - val_MinusLogProbMetric: 46.6467 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 618/1000
2023-10-27 23:01:04.831 
Epoch 618/1000 
	 loss: 44.3732, MinusLogProbMetric: 44.3732, val_loss: 44.8167, val_MinusLogProbMetric: 44.8167

Epoch 618: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.3732 - MinusLogProbMetric: 44.3732 - val_loss: 44.8167 - val_MinusLogProbMetric: 44.8167 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 619/1000
2023-10-27 23:01:34.783 
Epoch 619/1000 
	 loss: 44.3934, MinusLogProbMetric: 44.3934, val_loss: 45.2899, val_MinusLogProbMetric: 45.2899

Epoch 619: val_loss did not improve from 44.29625
196/196 - 30s - loss: 44.3934 - MinusLogProbMetric: 44.3934 - val_loss: 45.2899 - val_MinusLogProbMetric: 45.2899 - lr: 3.3333e-04 - 30s/epoch - 153ms/step
Epoch 620/1000
2023-10-27 23:02:01.363 
Epoch 620/1000 
	 loss: 44.3317, MinusLogProbMetric: 44.3317, val_loss: 46.0108, val_MinusLogProbMetric: 46.0108

Epoch 620: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.3317 - MinusLogProbMetric: 44.3317 - val_loss: 46.0108 - val_MinusLogProbMetric: 46.0108 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 621/1000
2023-10-27 23:02:28.026 
Epoch 621/1000 
	 loss: 44.5786, MinusLogProbMetric: 44.5786, val_loss: 70.8152, val_MinusLogProbMetric: 70.8152

Epoch 621: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.5786 - MinusLogProbMetric: 44.5786 - val_loss: 70.8152 - val_MinusLogProbMetric: 70.8152 - lr: 3.3333e-04 - 27s/epoch - 136ms/step
Epoch 622/1000
2023-10-27 23:02:55.461 
Epoch 622/1000 
	 loss: 47.5866, MinusLogProbMetric: 47.5866, val_loss: 46.8589, val_MinusLogProbMetric: 46.8589

Epoch 622: val_loss did not improve from 44.29625
196/196 - 27s - loss: 47.5866 - MinusLogProbMetric: 47.5866 - val_loss: 46.8589 - val_MinusLogProbMetric: 46.8589 - lr: 3.3333e-04 - 27s/epoch - 140ms/step
Epoch 623/1000
2023-10-27 23:03:24.407 
Epoch 623/1000 
	 loss: 44.1289, MinusLogProbMetric: 44.1289, val_loss: 45.6704, val_MinusLogProbMetric: 45.6704

Epoch 623: val_loss did not improve from 44.29625
196/196 - 29s - loss: 44.1289 - MinusLogProbMetric: 44.1289 - val_loss: 45.6704 - val_MinusLogProbMetric: 45.6704 - lr: 3.3333e-04 - 29s/epoch - 148ms/step
Epoch 624/1000
2023-10-27 23:03:54.548 
Epoch 624/1000 
	 loss: 44.7239, MinusLogProbMetric: 44.7239, val_loss: 45.4934, val_MinusLogProbMetric: 45.4934

Epoch 624: val_loss did not improve from 44.29625
196/196 - 30s - loss: 44.7239 - MinusLogProbMetric: 44.7239 - val_loss: 45.4934 - val_MinusLogProbMetric: 45.4934 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 625/1000
2023-10-27 23:04:21.568 
Epoch 625/1000 
	 loss: 44.1876, MinusLogProbMetric: 44.1876, val_loss: 44.4967, val_MinusLogProbMetric: 44.4967

Epoch 625: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.1876 - MinusLogProbMetric: 44.1876 - val_loss: 44.4967 - val_MinusLogProbMetric: 44.4967 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 626/1000
2023-10-27 23:04:47.743 
Epoch 626/1000 
	 loss: 45.9724, MinusLogProbMetric: 45.9724, val_loss: 45.1032, val_MinusLogProbMetric: 45.1032

Epoch 626: val_loss did not improve from 44.29625
196/196 - 26s - loss: 45.9724 - MinusLogProbMetric: 45.9724 - val_loss: 45.1032 - val_MinusLogProbMetric: 45.1032 - lr: 3.3333e-04 - 26s/epoch - 134ms/step
Epoch 627/1000
2023-10-27 23:05:14.906 
Epoch 627/1000 
	 loss: 44.2191, MinusLogProbMetric: 44.2191, val_loss: 44.8133, val_MinusLogProbMetric: 44.8133

Epoch 627: val_loss did not improve from 44.29625
196/196 - 27s - loss: 44.2191 - MinusLogProbMetric: 44.2191 - val_loss: 44.8133 - val_MinusLogProbMetric: 44.8133 - lr: 3.3333e-04 - 27s/epoch - 139ms/step
Epoch 628/1000
2023-10-27 23:05:46.623 
Epoch 628/1000 
	 loss: 44.8835, MinusLogProbMetric: 44.8835, val_loss: 45.2264, val_MinusLogProbMetric: 45.2264

Epoch 628: val_loss did not improve from 44.29625
196/196 - 32s - loss: 44.8835 - MinusLogProbMetric: 44.8835 - val_loss: 45.2264 - val_MinusLogProbMetric: 45.2264 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 629/1000
2023-10-27 23:06:14.597 
Epoch 629/1000 
	 loss: 44.4995, MinusLogProbMetric: 44.4995, val_loss: 46.2878, val_MinusLogProbMetric: 46.2878

Epoch 629: val_loss did not improve from 44.29625
196/196 - 28s - loss: 44.4995 - MinusLogProbMetric: 44.4995 - val_loss: 46.2878 - val_MinusLogProbMetric: 46.2878 - lr: 3.3333e-04 - 28s/epoch - 143ms/step
Epoch 630/1000
2023-10-27 23:06:40.464 
Epoch 630/1000 
	 loss: 42.8611, MinusLogProbMetric: 42.8611, val_loss: 43.5869, val_MinusLogProbMetric: 43.5869

Epoch 630: val_loss improved from 44.29625 to 43.58693, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 26s - loss: 42.8611 - MinusLogProbMetric: 42.8611 - val_loss: 43.5869 - val_MinusLogProbMetric: 43.5869 - lr: 1.6667e-04 - 26s/epoch - 134ms/step
Epoch 631/1000
2023-10-27 23:07:07.318 
Epoch 631/1000 
	 loss: 42.6979, MinusLogProbMetric: 42.6979, val_loss: 43.9616, val_MinusLogProbMetric: 43.9616

Epoch 631: val_loss did not improve from 43.58693
196/196 - 26s - loss: 42.6979 - MinusLogProbMetric: 42.6979 - val_loss: 43.9616 - val_MinusLogProbMetric: 43.9616 - lr: 1.6667e-04 - 26s/epoch - 135ms/step
Epoch 632/1000
2023-10-27 23:07:33.874 
Epoch 632/1000 
	 loss: 42.8169, MinusLogProbMetric: 42.8169, val_loss: 44.5738, val_MinusLogProbMetric: 44.5738

Epoch 632: val_loss did not improve from 43.58693
196/196 - 27s - loss: 42.8169 - MinusLogProbMetric: 42.8169 - val_loss: 44.5738 - val_MinusLogProbMetric: 44.5738 - lr: 1.6667e-04 - 27s/epoch - 135ms/step
Epoch 633/1000
2023-10-27 23:08:01.899 
Epoch 633/1000 
	 loss: 42.8169, MinusLogProbMetric: 42.8169, val_loss: 46.2882, val_MinusLogProbMetric: 46.2882

Epoch 633: val_loss did not improve from 43.58693
196/196 - 28s - loss: 42.8169 - MinusLogProbMetric: 42.8169 - val_loss: 46.2882 - val_MinusLogProbMetric: 46.2882 - lr: 1.6667e-04 - 28s/epoch - 143ms/step
Epoch 634/1000
2023-10-27 23:08:30.259 
Epoch 634/1000 
	 loss: 43.5668, MinusLogProbMetric: 43.5668, val_loss: 44.2745, val_MinusLogProbMetric: 44.2745

Epoch 634: val_loss did not improve from 43.58693
196/196 - 28s - loss: 43.5668 - MinusLogProbMetric: 43.5668 - val_loss: 44.2745 - val_MinusLogProbMetric: 44.2745 - lr: 1.6667e-04 - 28s/epoch - 145ms/step
Epoch 635/1000
2023-10-27 23:08:56.480 
Epoch 635/1000 
	 loss: 42.8830, MinusLogProbMetric: 42.8830, val_loss: 43.9732, val_MinusLogProbMetric: 43.9732

Epoch 635: val_loss did not improve from 43.58693
196/196 - 26s - loss: 42.8830 - MinusLogProbMetric: 42.8830 - val_loss: 43.9732 - val_MinusLogProbMetric: 43.9732 - lr: 1.6667e-04 - 26s/epoch - 134ms/step
Epoch 636/1000
2023-10-27 23:09:23.471 
Epoch 636/1000 
	 loss: 42.8468, MinusLogProbMetric: 42.8468, val_loss: 44.6090, val_MinusLogProbMetric: 44.6090

Epoch 636: val_loss did not improve from 43.58693
196/196 - 27s - loss: 42.8468 - MinusLogProbMetric: 42.8468 - val_loss: 44.6090 - val_MinusLogProbMetric: 44.6090 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 637/1000
2023-10-27 23:09:52.144 
Epoch 637/1000 
	 loss: 42.8486, MinusLogProbMetric: 42.8486, val_loss: 50.4682, val_MinusLogProbMetric: 50.4682

Epoch 637: val_loss did not improve from 43.58693
196/196 - 29s - loss: 42.8486 - MinusLogProbMetric: 42.8486 - val_loss: 50.4682 - val_MinusLogProbMetric: 50.4682 - lr: 1.6667e-04 - 29s/epoch - 146ms/step
Epoch 638/1000
2023-10-27 23:10:19.594 
Epoch 638/1000 
	 loss: 43.0113, MinusLogProbMetric: 43.0113, val_loss: 43.8734, val_MinusLogProbMetric: 43.8734

Epoch 638: val_loss did not improve from 43.58693
196/196 - 27s - loss: 43.0113 - MinusLogProbMetric: 43.0113 - val_loss: 43.8734 - val_MinusLogProbMetric: 43.8734 - lr: 1.6667e-04 - 27s/epoch - 140ms/step
Epoch 639/1000
2023-10-27 23:10:49.424 
Epoch 639/1000 
	 loss: 42.8816, MinusLogProbMetric: 42.8816, val_loss: 44.0621, val_MinusLogProbMetric: 44.0621

Epoch 639: val_loss did not improve from 43.58693
196/196 - 30s - loss: 42.8816 - MinusLogProbMetric: 42.8816 - val_loss: 44.0621 - val_MinusLogProbMetric: 44.0621 - lr: 1.6667e-04 - 30s/epoch - 152ms/step
Epoch 640/1000
2023-10-27 23:11:16.286 
Epoch 640/1000 
	 loss: 42.7506, MinusLogProbMetric: 42.7506, val_loss: 43.6521, val_MinusLogProbMetric: 43.6521

Epoch 640: val_loss did not improve from 43.58693
196/196 - 27s - loss: 42.7506 - MinusLogProbMetric: 42.7506 - val_loss: 43.6521 - val_MinusLogProbMetric: 43.6521 - lr: 1.6667e-04 - 27s/epoch - 137ms/step
Epoch 641/1000
2023-10-27 23:11:42.926 
Epoch 641/1000 
	 loss: 42.8572, MinusLogProbMetric: 42.8572, val_loss: 45.0892, val_MinusLogProbMetric: 45.0892

Epoch 641: val_loss did not improve from 43.58693
196/196 - 27s - loss: 42.8572 - MinusLogProbMetric: 42.8572 - val_loss: 45.0892 - val_MinusLogProbMetric: 45.0892 - lr: 1.6667e-04 - 27s/epoch - 136ms/step
Epoch 642/1000
2023-10-27 23:12:10.010 
Epoch 642/1000 
	 loss: 42.8278, MinusLogProbMetric: 42.8278, val_loss: 43.5999, val_MinusLogProbMetric: 43.5999

Epoch 642: val_loss did not improve from 43.58693
196/196 - 27s - loss: 42.8278 - MinusLogProbMetric: 42.8278 - val_loss: 43.5999 - val_MinusLogProbMetric: 43.5999 - lr: 1.6667e-04 - 27s/epoch - 138ms/step
Epoch 643/1000
2023-10-27 23:12:41.691 
Epoch 643/1000 
	 loss: 42.7713, MinusLogProbMetric: 42.7713, val_loss: 43.4970, val_MinusLogProbMetric: 43.4970

Epoch 643: val_loss improved from 43.58693 to 43.49704, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 32s - loss: 42.7713 - MinusLogProbMetric: 42.7713 - val_loss: 43.4970 - val_MinusLogProbMetric: 43.4970 - lr: 1.6667e-04 - 32s/epoch - 164ms/step
Epoch 644/1000
2023-10-27 23:13:14.603 
Epoch 644/1000 
	 loss: 42.8258, MinusLogProbMetric: 42.8258, val_loss: 43.6032, val_MinusLogProbMetric: 43.6032

Epoch 644: val_loss did not improve from 43.49704
196/196 - 32s - loss: 42.8258 - MinusLogProbMetric: 42.8258 - val_loss: 43.6032 - val_MinusLogProbMetric: 43.6032 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 645/1000
2023-10-27 23:13:46.046 
Epoch 645/1000 
	 loss: 42.6437, MinusLogProbMetric: 42.6437, val_loss: 43.9152, val_MinusLogProbMetric: 43.9152

Epoch 645: val_loss did not improve from 43.49704
196/196 - 31s - loss: 42.6437 - MinusLogProbMetric: 42.6437 - val_loss: 43.9152 - val_MinusLogProbMetric: 43.9152 - lr: 1.6667e-04 - 31s/epoch - 160ms/step
Epoch 646/1000
2023-10-27 23:14:18.734 
Epoch 646/1000 
	 loss: 42.7567, MinusLogProbMetric: 42.7567, val_loss: 44.2322, val_MinusLogProbMetric: 44.2322

Epoch 646: val_loss did not improve from 43.49704
196/196 - 33s - loss: 42.7567 - MinusLogProbMetric: 42.7567 - val_loss: 44.2322 - val_MinusLogProbMetric: 44.2322 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 647/1000
2023-10-27 23:14:51.325 
Epoch 647/1000 
	 loss: 42.7294, MinusLogProbMetric: 42.7294, val_loss: 43.9171, val_MinusLogProbMetric: 43.9171

Epoch 647: val_loss did not improve from 43.49704
196/196 - 33s - loss: 42.7294 - MinusLogProbMetric: 42.7294 - val_loss: 43.9171 - val_MinusLogProbMetric: 43.9171 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 648/1000
2023-10-27 23:15:23.311 
Epoch 648/1000 
	 loss: 42.7484, MinusLogProbMetric: 42.7484, val_loss: 43.5751, val_MinusLogProbMetric: 43.5751

Epoch 648: val_loss did not improve from 43.49704
196/196 - 32s - loss: 42.7484 - MinusLogProbMetric: 42.7484 - val_loss: 43.5751 - val_MinusLogProbMetric: 43.5751 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 649/1000
2023-10-27 23:15:56.872 
Epoch 649/1000 
	 loss: 43.0190, MinusLogProbMetric: 43.0190, val_loss: 43.5900, val_MinusLogProbMetric: 43.5900

Epoch 649: val_loss did not improve from 43.49704
196/196 - 34s - loss: 43.0190 - MinusLogProbMetric: 43.0190 - val_loss: 43.5900 - val_MinusLogProbMetric: 43.5900 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 650/1000
2023-10-27 23:16:30.457 
Epoch 650/1000 
	 loss: 42.8280, MinusLogProbMetric: 42.8280, val_loss: 43.5889, val_MinusLogProbMetric: 43.5889

Epoch 650: val_loss did not improve from 43.49704
196/196 - 34s - loss: 42.8280 - MinusLogProbMetric: 42.8280 - val_loss: 43.5889 - val_MinusLogProbMetric: 43.5889 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 651/1000
2023-10-27 23:17:04.155 
Epoch 651/1000 
	 loss: 42.6460, MinusLogProbMetric: 42.6460, val_loss: 43.6359, val_MinusLogProbMetric: 43.6359

Epoch 651: val_loss did not improve from 43.49704
196/196 - 34s - loss: 42.6460 - MinusLogProbMetric: 42.6460 - val_loss: 43.6359 - val_MinusLogProbMetric: 43.6359 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 652/1000
2023-10-27 23:17:37.623 
Epoch 652/1000 
	 loss: 42.6625, MinusLogProbMetric: 42.6625, val_loss: 44.3935, val_MinusLogProbMetric: 44.3935

Epoch 652: val_loss did not improve from 43.49704
196/196 - 33s - loss: 42.6625 - MinusLogProbMetric: 42.6625 - val_loss: 44.3935 - val_MinusLogProbMetric: 44.3935 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 653/1000
2023-10-27 23:18:10.533 
Epoch 653/1000 
	 loss: 42.9743, MinusLogProbMetric: 42.9743, val_loss: 43.3417, val_MinusLogProbMetric: 43.3417

Epoch 653: val_loss improved from 43.49704 to 43.34170, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 33s - loss: 42.9743 - MinusLogProbMetric: 42.9743 - val_loss: 43.3417 - val_MinusLogProbMetric: 43.3417 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 654/1000
2023-10-27 23:18:43.739 
Epoch 654/1000 
	 loss: 42.6463, MinusLogProbMetric: 42.6463, val_loss: 43.7063, val_MinusLogProbMetric: 43.7063

Epoch 654: val_loss did not improve from 43.34170
196/196 - 33s - loss: 42.6463 - MinusLogProbMetric: 42.6463 - val_loss: 43.7063 - val_MinusLogProbMetric: 43.7063 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 655/1000
2023-10-27 23:19:15.696 
Epoch 655/1000 
	 loss: 43.2400, MinusLogProbMetric: 43.2400, val_loss: 43.7243, val_MinusLogProbMetric: 43.7243

Epoch 655: val_loss did not improve from 43.34170
196/196 - 32s - loss: 43.2400 - MinusLogProbMetric: 43.2400 - val_loss: 43.7243 - val_MinusLogProbMetric: 43.7243 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 656/1000
2023-10-27 23:19:46.938 
Epoch 656/1000 
	 loss: 42.7864, MinusLogProbMetric: 42.7864, val_loss: 43.3504, val_MinusLogProbMetric: 43.3504

Epoch 656: val_loss did not improve from 43.34170
196/196 - 31s - loss: 42.7864 - MinusLogProbMetric: 42.7864 - val_loss: 43.3504 - val_MinusLogProbMetric: 43.3504 - lr: 1.6667e-04 - 31s/epoch - 159ms/step
Epoch 657/1000
2023-10-27 23:20:18.943 
Epoch 657/1000 
	 loss: 42.8652, MinusLogProbMetric: 42.8652, val_loss: 43.4002, val_MinusLogProbMetric: 43.4002

Epoch 657: val_loss did not improve from 43.34170
196/196 - 32s - loss: 42.8652 - MinusLogProbMetric: 42.8652 - val_loss: 43.4002 - val_MinusLogProbMetric: 43.4002 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 658/1000
2023-10-27 23:20:50.824 
Epoch 658/1000 
	 loss: 42.9913, MinusLogProbMetric: 42.9913, val_loss: 43.5122, val_MinusLogProbMetric: 43.5122

Epoch 658: val_loss did not improve from 43.34170
196/196 - 32s - loss: 42.9913 - MinusLogProbMetric: 42.9913 - val_loss: 43.5122 - val_MinusLogProbMetric: 43.5122 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 659/1000
2023-10-27 23:21:24.538 
Epoch 659/1000 
	 loss: 42.9114, MinusLogProbMetric: 42.9114, val_loss: 43.9237, val_MinusLogProbMetric: 43.9237

Epoch 659: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.9114 - MinusLogProbMetric: 42.9114 - val_loss: 43.9237 - val_MinusLogProbMetric: 43.9237 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 660/1000
2023-10-27 23:21:57.753 
Epoch 660/1000 
	 loss: 42.6863, MinusLogProbMetric: 42.6863, val_loss: 43.8657, val_MinusLogProbMetric: 43.8657

Epoch 660: val_loss did not improve from 43.34170
196/196 - 33s - loss: 42.6863 - MinusLogProbMetric: 42.6863 - val_loss: 43.8657 - val_MinusLogProbMetric: 43.8657 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 661/1000
2023-10-27 23:22:30.955 
Epoch 661/1000 
	 loss: 42.9616, MinusLogProbMetric: 42.9616, val_loss: 43.8697, val_MinusLogProbMetric: 43.8697

Epoch 661: val_loss did not improve from 43.34170
196/196 - 33s - loss: 42.9616 - MinusLogProbMetric: 42.9616 - val_loss: 43.8697 - val_MinusLogProbMetric: 43.8697 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 662/1000
2023-10-27 23:23:03.974 
Epoch 662/1000 
	 loss: 42.5877, MinusLogProbMetric: 42.5877, val_loss: 43.4863, val_MinusLogProbMetric: 43.4863

Epoch 662: val_loss did not improve from 43.34170
196/196 - 33s - loss: 42.5877 - MinusLogProbMetric: 42.5877 - val_loss: 43.4863 - val_MinusLogProbMetric: 43.4863 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 663/1000
2023-10-27 23:23:36.990 
Epoch 663/1000 
	 loss: 42.9979, MinusLogProbMetric: 42.9979, val_loss: 43.6850, val_MinusLogProbMetric: 43.6850

Epoch 663: val_loss did not improve from 43.34170
196/196 - 33s - loss: 42.9979 - MinusLogProbMetric: 42.9979 - val_loss: 43.6850 - val_MinusLogProbMetric: 43.6850 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 664/1000
2023-10-27 23:24:10.750 
Epoch 664/1000 
	 loss: 43.0079, MinusLogProbMetric: 43.0079, val_loss: 45.8270, val_MinusLogProbMetric: 45.8270

Epoch 664: val_loss did not improve from 43.34170
196/196 - 34s - loss: 43.0079 - MinusLogProbMetric: 43.0079 - val_loss: 45.8270 - val_MinusLogProbMetric: 45.8270 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 665/1000
2023-10-27 23:24:44.000 
Epoch 665/1000 
	 loss: 42.7191, MinusLogProbMetric: 42.7191, val_loss: 43.3487, val_MinusLogProbMetric: 43.3487

Epoch 665: val_loss did not improve from 43.34170
196/196 - 33s - loss: 42.7191 - MinusLogProbMetric: 42.7191 - val_loss: 43.3487 - val_MinusLogProbMetric: 43.3487 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 666/1000
2023-10-27 23:25:15.860 
Epoch 666/1000 
	 loss: 42.6537, MinusLogProbMetric: 42.6537, val_loss: 45.1420, val_MinusLogProbMetric: 45.1420

Epoch 666: val_loss did not improve from 43.34170
196/196 - 32s - loss: 42.6537 - MinusLogProbMetric: 42.6537 - val_loss: 45.1420 - val_MinusLogProbMetric: 45.1420 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 667/1000
2023-10-27 23:25:48.854 
Epoch 667/1000 
	 loss: 43.1275, MinusLogProbMetric: 43.1275, val_loss: 43.4222, val_MinusLogProbMetric: 43.4222

Epoch 667: val_loss did not improve from 43.34170
196/196 - 33s - loss: 43.1275 - MinusLogProbMetric: 43.1275 - val_loss: 43.4222 - val_MinusLogProbMetric: 43.4222 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 668/1000
2023-10-27 23:26:22.337 
Epoch 668/1000 
	 loss: 42.6431, MinusLogProbMetric: 42.6431, val_loss: 43.5952, val_MinusLogProbMetric: 43.5952

Epoch 668: val_loss did not improve from 43.34170
196/196 - 33s - loss: 42.6431 - MinusLogProbMetric: 42.6431 - val_loss: 43.5952 - val_MinusLogProbMetric: 43.5952 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 669/1000
2023-10-27 23:26:56.343 
Epoch 669/1000 
	 loss: 43.1633, MinusLogProbMetric: 43.1633, val_loss: 43.9707, val_MinusLogProbMetric: 43.9707

Epoch 669: val_loss did not improve from 43.34170
196/196 - 34s - loss: 43.1633 - MinusLogProbMetric: 43.1633 - val_loss: 43.9707 - val_MinusLogProbMetric: 43.9707 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 670/1000
2023-10-27 23:27:29.499 
Epoch 670/1000 
	 loss: 42.6863, MinusLogProbMetric: 42.6863, val_loss: 43.7943, val_MinusLogProbMetric: 43.7943

Epoch 670: val_loss did not improve from 43.34170
196/196 - 33s - loss: 42.6863 - MinusLogProbMetric: 42.6863 - val_loss: 43.7943 - val_MinusLogProbMetric: 43.7943 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 671/1000
2023-10-27 23:28:02.034 
Epoch 671/1000 
	 loss: 43.3090, MinusLogProbMetric: 43.3090, val_loss: 43.7652, val_MinusLogProbMetric: 43.7652

Epoch 671: val_loss did not improve from 43.34170
196/196 - 33s - loss: 43.3090 - MinusLogProbMetric: 43.3090 - val_loss: 43.7652 - val_MinusLogProbMetric: 43.7652 - lr: 1.6667e-04 - 33s/epoch - 166ms/step
Epoch 672/1000
2023-10-27 23:28:34.000 
Epoch 672/1000 
	 loss: 42.6956, MinusLogProbMetric: 42.6956, val_loss: 43.8382, val_MinusLogProbMetric: 43.8382

Epoch 672: val_loss did not improve from 43.34170
196/196 - 32s - loss: 42.6956 - MinusLogProbMetric: 42.6956 - val_loss: 43.8382 - val_MinusLogProbMetric: 43.8382 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 673/1000
2023-10-27 23:29:05.916 
Epoch 673/1000 
	 loss: 42.7577, MinusLogProbMetric: 42.7577, val_loss: 43.7365, val_MinusLogProbMetric: 43.7365

Epoch 673: val_loss did not improve from 43.34170
196/196 - 32s - loss: 42.7577 - MinusLogProbMetric: 42.7577 - val_loss: 43.7365 - val_MinusLogProbMetric: 43.7365 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 674/1000
2023-10-27 23:29:39.546 
Epoch 674/1000 
	 loss: 42.9449, MinusLogProbMetric: 42.9449, val_loss: 43.4773, val_MinusLogProbMetric: 43.4773

Epoch 674: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.9449 - MinusLogProbMetric: 42.9449 - val_loss: 43.4773 - val_MinusLogProbMetric: 43.4773 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 675/1000
2023-10-27 23:30:12.465 
Epoch 675/1000 
	 loss: 43.0394, MinusLogProbMetric: 43.0394, val_loss: 43.5004, val_MinusLogProbMetric: 43.5004

Epoch 675: val_loss did not improve from 43.34170
196/196 - 33s - loss: 43.0394 - MinusLogProbMetric: 43.0394 - val_loss: 43.5004 - val_MinusLogProbMetric: 43.5004 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 676/1000
2023-10-27 23:30:44.796 
Epoch 676/1000 
	 loss: 42.5681, MinusLogProbMetric: 42.5681, val_loss: 44.1279, val_MinusLogProbMetric: 44.1279

Epoch 676: val_loss did not improve from 43.34170
196/196 - 32s - loss: 42.5681 - MinusLogProbMetric: 42.5681 - val_loss: 44.1279 - val_MinusLogProbMetric: 44.1279 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 677/1000
2023-10-27 23:31:16.417 
Epoch 677/1000 
	 loss: 42.6597, MinusLogProbMetric: 42.6597, val_loss: 43.7907, val_MinusLogProbMetric: 43.7907

Epoch 677: val_loss did not improve from 43.34170
196/196 - 32s - loss: 42.6597 - MinusLogProbMetric: 42.6597 - val_loss: 43.7907 - val_MinusLogProbMetric: 43.7907 - lr: 1.6667e-04 - 32s/epoch - 161ms/step
Epoch 678/1000
2023-10-27 23:31:49.876 
Epoch 678/1000 
	 loss: 42.7987, MinusLogProbMetric: 42.7987, val_loss: 43.5218, val_MinusLogProbMetric: 43.5218

Epoch 678: val_loss did not improve from 43.34170
196/196 - 33s - loss: 42.7987 - MinusLogProbMetric: 42.7987 - val_loss: 43.5218 - val_MinusLogProbMetric: 43.5218 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 679/1000
2023-10-27 23:32:23.718 
Epoch 679/1000 
	 loss: 43.2376, MinusLogProbMetric: 43.2376, val_loss: 44.2057, val_MinusLogProbMetric: 44.2057

Epoch 679: val_loss did not improve from 43.34170
196/196 - 34s - loss: 43.2376 - MinusLogProbMetric: 43.2376 - val_loss: 44.2057 - val_MinusLogProbMetric: 44.2057 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 680/1000
2023-10-27 23:32:57.702 
Epoch 680/1000 
	 loss: 42.7106, MinusLogProbMetric: 42.7106, val_loss: 43.7788, val_MinusLogProbMetric: 43.7788

Epoch 680: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.7106 - MinusLogProbMetric: 42.7106 - val_loss: 43.7788 - val_MinusLogProbMetric: 43.7788 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 681/1000
2023-10-27 23:33:31.839 
Epoch 681/1000 
	 loss: 42.5968, MinusLogProbMetric: 42.5968, val_loss: 43.4412, val_MinusLogProbMetric: 43.4412

Epoch 681: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.5968 - MinusLogProbMetric: 42.5968 - val_loss: 43.4412 - val_MinusLogProbMetric: 43.4412 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 682/1000
2023-10-27 23:34:06.023 
Epoch 682/1000 
	 loss: 43.3908, MinusLogProbMetric: 43.3908, val_loss: 43.9105, val_MinusLogProbMetric: 43.9105

Epoch 682: val_loss did not improve from 43.34170
196/196 - 34s - loss: 43.3908 - MinusLogProbMetric: 43.3908 - val_loss: 43.9105 - val_MinusLogProbMetric: 43.9105 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 683/1000
2023-10-27 23:34:39.812 
Epoch 683/1000 
	 loss: 42.6229, MinusLogProbMetric: 42.6229, val_loss: 43.8576, val_MinusLogProbMetric: 43.8576

Epoch 683: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.6229 - MinusLogProbMetric: 42.6229 - val_loss: 43.8576 - val_MinusLogProbMetric: 43.8576 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 684/1000
2023-10-27 23:35:13.434 
Epoch 684/1000 
	 loss: 42.7723, MinusLogProbMetric: 42.7723, val_loss: 44.0055, val_MinusLogProbMetric: 44.0055

Epoch 684: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.7723 - MinusLogProbMetric: 42.7723 - val_loss: 44.0055 - val_MinusLogProbMetric: 44.0055 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 685/1000
2023-10-27 23:35:47.535 
Epoch 685/1000 
	 loss: 43.5856, MinusLogProbMetric: 43.5856, val_loss: 44.1431, val_MinusLogProbMetric: 44.1431

Epoch 685: val_loss did not improve from 43.34170
196/196 - 34s - loss: 43.5856 - MinusLogProbMetric: 43.5856 - val_loss: 44.1431 - val_MinusLogProbMetric: 44.1431 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 686/1000
2023-10-27 23:36:21.447 
Epoch 686/1000 
	 loss: 42.8452, MinusLogProbMetric: 42.8452, val_loss: 43.9431, val_MinusLogProbMetric: 43.9431

Epoch 686: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.8452 - MinusLogProbMetric: 42.8452 - val_loss: 43.9431 - val_MinusLogProbMetric: 43.9431 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 687/1000
2023-10-27 23:36:55.334 
Epoch 687/1000 
	 loss: 42.6297, MinusLogProbMetric: 42.6297, val_loss: 43.7447, val_MinusLogProbMetric: 43.7447

Epoch 687: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.6297 - MinusLogProbMetric: 42.6297 - val_loss: 43.7447 - val_MinusLogProbMetric: 43.7447 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 688/1000
2023-10-27 23:37:28.513 
Epoch 688/1000 
	 loss: 42.9663, MinusLogProbMetric: 42.9663, val_loss: 43.7707, val_MinusLogProbMetric: 43.7707

Epoch 688: val_loss did not improve from 43.34170
196/196 - 33s - loss: 42.9663 - MinusLogProbMetric: 42.9663 - val_loss: 43.7707 - val_MinusLogProbMetric: 43.7707 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 689/1000
2023-10-27 23:38:02.306 
Epoch 689/1000 
	 loss: 42.9042, MinusLogProbMetric: 42.9042, val_loss: 44.4803, val_MinusLogProbMetric: 44.4803

Epoch 689: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.9042 - MinusLogProbMetric: 42.9042 - val_loss: 44.4803 - val_MinusLogProbMetric: 44.4803 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 690/1000
2023-10-27 23:38:35.903 
Epoch 690/1000 
	 loss: 42.6046, MinusLogProbMetric: 42.6046, val_loss: 43.4603, val_MinusLogProbMetric: 43.4603

Epoch 690: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.6046 - MinusLogProbMetric: 42.6046 - val_loss: 43.4603 - val_MinusLogProbMetric: 43.4603 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 691/1000
2023-10-27 23:39:09.711 
Epoch 691/1000 
	 loss: 42.7371, MinusLogProbMetric: 42.7371, val_loss: 44.2911, val_MinusLogProbMetric: 44.2911

Epoch 691: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.7371 - MinusLogProbMetric: 42.7371 - val_loss: 44.2911 - val_MinusLogProbMetric: 44.2911 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 692/1000
2023-10-27 23:39:43.256 
Epoch 692/1000 
	 loss: 42.8581, MinusLogProbMetric: 42.8581, val_loss: 43.5009, val_MinusLogProbMetric: 43.5009

Epoch 692: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.8581 - MinusLogProbMetric: 42.8581 - val_loss: 43.5009 - val_MinusLogProbMetric: 43.5009 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 693/1000
2023-10-27 23:40:17.015 
Epoch 693/1000 
	 loss: 42.7810, MinusLogProbMetric: 42.7810, val_loss: 44.2837, val_MinusLogProbMetric: 44.2837

Epoch 693: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.7810 - MinusLogProbMetric: 42.7810 - val_loss: 44.2837 - val_MinusLogProbMetric: 44.2837 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 694/1000
2023-10-27 23:40:50.706 
Epoch 694/1000 
	 loss: 42.6083, MinusLogProbMetric: 42.6083, val_loss: 44.1952, val_MinusLogProbMetric: 44.1952

Epoch 694: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.6083 - MinusLogProbMetric: 42.6083 - val_loss: 44.1952 - val_MinusLogProbMetric: 44.1952 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 695/1000
2023-10-27 23:41:21.103 
Epoch 695/1000 
	 loss: 42.7607, MinusLogProbMetric: 42.7607, val_loss: 44.0495, val_MinusLogProbMetric: 44.0495

Epoch 695: val_loss did not improve from 43.34170
196/196 - 30s - loss: 42.7607 - MinusLogProbMetric: 42.7607 - val_loss: 44.0495 - val_MinusLogProbMetric: 44.0495 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 696/1000
2023-10-27 23:41:50.190 
Epoch 696/1000 
	 loss: 42.8204, MinusLogProbMetric: 42.8204, val_loss: 44.4427, val_MinusLogProbMetric: 44.4427

Epoch 696: val_loss did not improve from 43.34170
196/196 - 29s - loss: 42.8204 - MinusLogProbMetric: 42.8204 - val_loss: 44.4427 - val_MinusLogProbMetric: 44.4427 - lr: 1.6667e-04 - 29s/epoch - 148ms/step
Epoch 697/1000
2023-10-27 23:42:22.964 
Epoch 697/1000 
	 loss: 42.7172, MinusLogProbMetric: 42.7172, val_loss: 43.3526, val_MinusLogProbMetric: 43.3526

Epoch 697: val_loss did not improve from 43.34170
196/196 - 33s - loss: 42.7172 - MinusLogProbMetric: 42.7172 - val_loss: 43.3526 - val_MinusLogProbMetric: 43.3526 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 698/1000
2023-10-27 23:42:56.835 
Epoch 698/1000 
	 loss: 42.8986, MinusLogProbMetric: 42.8986, val_loss: 44.1351, val_MinusLogProbMetric: 44.1351

Epoch 698: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.8986 - MinusLogProbMetric: 42.8986 - val_loss: 44.1351 - val_MinusLogProbMetric: 44.1351 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 699/1000
2023-10-27 23:43:30.598 
Epoch 699/1000 
	 loss: 43.3380, MinusLogProbMetric: 43.3380, val_loss: 44.2516, val_MinusLogProbMetric: 44.2516

Epoch 699: val_loss did not improve from 43.34170
196/196 - 34s - loss: 43.3380 - MinusLogProbMetric: 43.3380 - val_loss: 44.2516 - val_MinusLogProbMetric: 44.2516 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 700/1000
2023-10-27 23:44:04.389 
Epoch 700/1000 
	 loss: 42.8046, MinusLogProbMetric: 42.8046, val_loss: 43.5338, val_MinusLogProbMetric: 43.5338

Epoch 700: val_loss did not improve from 43.34170
196/196 - 34s - loss: 42.8046 - MinusLogProbMetric: 42.8046 - val_loss: 43.5338 - val_MinusLogProbMetric: 43.5338 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 701/1000
2023-10-27 23:44:34.772 
Epoch 701/1000 
	 loss: 42.7060, MinusLogProbMetric: 42.7060, val_loss: 46.8187, val_MinusLogProbMetric: 46.8187

Epoch 701: val_loss did not improve from 43.34170
196/196 - 30s - loss: 42.7060 - MinusLogProbMetric: 42.7060 - val_loss: 46.8187 - val_MinusLogProbMetric: 46.8187 - lr: 1.6667e-04 - 30s/epoch - 155ms/step
Epoch 702/1000
2023-10-27 23:45:02.975 
Epoch 702/1000 
	 loss: 42.9407, MinusLogProbMetric: 42.9407, val_loss: 44.1571, val_MinusLogProbMetric: 44.1571

Epoch 702: val_loss did not improve from 43.34170
196/196 - 28s - loss: 42.9407 - MinusLogProbMetric: 42.9407 - val_loss: 44.1571 - val_MinusLogProbMetric: 44.1571 - lr: 1.6667e-04 - 28s/epoch - 144ms/step
Epoch 703/1000
2023-10-27 23:45:33.154 
Epoch 703/1000 
	 loss: 42.7846, MinusLogProbMetric: 42.7846, val_loss: 43.4295, val_MinusLogProbMetric: 43.4295

Epoch 703: val_loss did not improve from 43.34170
196/196 - 30s - loss: 42.7846 - MinusLogProbMetric: 42.7846 - val_loss: 43.4295 - val_MinusLogProbMetric: 43.4295 - lr: 1.6667e-04 - 30s/epoch - 154ms/step
Epoch 704/1000
2023-10-27 23:46:07.069 
Epoch 704/1000 
	 loss: 41.9546, MinusLogProbMetric: 41.9546, val_loss: 43.0748, val_MinusLogProbMetric: 43.0748

Epoch 704: val_loss improved from 43.34170 to 43.07482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 41.9546 - MinusLogProbMetric: 41.9546 - val_loss: 43.0748 - val_MinusLogProbMetric: 43.0748 - lr: 8.3333e-05 - 34s/epoch - 175ms/step
Epoch 705/1000
2023-10-27 23:46:40.329 
Epoch 705/1000 
	 loss: 41.8710, MinusLogProbMetric: 41.8710, val_loss: 43.1078, val_MinusLogProbMetric: 43.1078

Epoch 705: val_loss did not improve from 43.07482
196/196 - 33s - loss: 41.8710 - MinusLogProbMetric: 41.8710 - val_loss: 43.1078 - val_MinusLogProbMetric: 43.1078 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 706/1000
2023-10-27 23:47:13.853 
Epoch 706/1000 
	 loss: 42.0466, MinusLogProbMetric: 42.0466, val_loss: 43.2522, val_MinusLogProbMetric: 43.2522

Epoch 706: val_loss did not improve from 43.07482
196/196 - 34s - loss: 42.0466 - MinusLogProbMetric: 42.0466 - val_loss: 43.2522 - val_MinusLogProbMetric: 43.2522 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 707/1000
2023-10-27 23:47:47.688 
Epoch 707/1000 
	 loss: 41.9110, MinusLogProbMetric: 41.9110, val_loss: 43.5374, val_MinusLogProbMetric: 43.5374

Epoch 707: val_loss did not improve from 43.07482
196/196 - 34s - loss: 41.9110 - MinusLogProbMetric: 41.9110 - val_loss: 43.5374 - val_MinusLogProbMetric: 43.5374 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 708/1000
2023-10-27 23:48:20.651 
Epoch 708/1000 
	 loss: 41.9957, MinusLogProbMetric: 41.9957, val_loss: 43.6043, val_MinusLogProbMetric: 43.6043

Epoch 708: val_loss did not improve from 43.07482
196/196 - 33s - loss: 41.9957 - MinusLogProbMetric: 41.9957 - val_loss: 43.6043 - val_MinusLogProbMetric: 43.6043 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 709/1000
2023-10-27 23:48:54.144 
Epoch 709/1000 
	 loss: 42.0020, MinusLogProbMetric: 42.0020, val_loss: 43.0007, val_MinusLogProbMetric: 43.0007

Epoch 709: val_loss improved from 43.07482 to 43.00069, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 42.0020 - MinusLogProbMetric: 42.0020 - val_loss: 43.0007 - val_MinusLogProbMetric: 43.0007 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 710/1000
2023-10-27 23:49:28.515 
Epoch 710/1000 
	 loss: 41.8720, MinusLogProbMetric: 41.8720, val_loss: 43.0990, val_MinusLogProbMetric: 43.0990

Epoch 710: val_loss did not improve from 43.00069
196/196 - 34s - loss: 41.8720 - MinusLogProbMetric: 41.8720 - val_loss: 43.0990 - val_MinusLogProbMetric: 43.0990 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 711/1000
2023-10-27 23:50:02.161 
Epoch 711/1000 
	 loss: 41.9227, MinusLogProbMetric: 41.9227, val_loss: 42.9775, val_MinusLogProbMetric: 42.9775

Epoch 711: val_loss improved from 43.00069 to 42.97754, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 41.9227 - MinusLogProbMetric: 41.9227 - val_loss: 42.9775 - val_MinusLogProbMetric: 42.9775 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 712/1000
2023-10-27 23:50:35.959 
Epoch 712/1000 
	 loss: 41.9643, MinusLogProbMetric: 41.9643, val_loss: 43.1449, val_MinusLogProbMetric: 43.1449

Epoch 712: val_loss did not improve from 42.97754
196/196 - 33s - loss: 41.9643 - MinusLogProbMetric: 41.9643 - val_loss: 43.1449 - val_MinusLogProbMetric: 43.1449 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 713/1000
2023-10-27 23:51:09.742 
Epoch 713/1000 
	 loss: 42.0557, MinusLogProbMetric: 42.0557, val_loss: 43.1657, val_MinusLogProbMetric: 43.1657

Epoch 713: val_loss did not improve from 42.97754
196/196 - 34s - loss: 42.0557 - MinusLogProbMetric: 42.0557 - val_loss: 43.1657 - val_MinusLogProbMetric: 43.1657 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 714/1000
2023-10-27 23:51:43.036 
Epoch 714/1000 
	 loss: 41.8498, MinusLogProbMetric: 41.8498, val_loss: 42.9495, val_MinusLogProbMetric: 42.9495

Epoch 714: val_loss improved from 42.97754 to 42.94955, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 41.8498 - MinusLogProbMetric: 41.8498 - val_loss: 42.9495 - val_MinusLogProbMetric: 42.9495 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 715/1000
2023-10-27 23:52:16.994 
Epoch 715/1000 
	 loss: 41.9199, MinusLogProbMetric: 41.9199, val_loss: 43.4930, val_MinusLogProbMetric: 43.4930

Epoch 715: val_loss did not improve from 42.94955
196/196 - 33s - loss: 41.9199 - MinusLogProbMetric: 41.9199 - val_loss: 43.4930 - val_MinusLogProbMetric: 43.4930 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 716/1000
2023-10-27 23:52:50.777 
Epoch 716/1000 
	 loss: 41.9547, MinusLogProbMetric: 41.9547, val_loss: 42.9843, val_MinusLogProbMetric: 42.9843

Epoch 716: val_loss did not improve from 42.94955
196/196 - 34s - loss: 41.9547 - MinusLogProbMetric: 41.9547 - val_loss: 42.9843 - val_MinusLogProbMetric: 42.9843 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 717/1000
2023-10-27 23:53:24.804 
Epoch 717/1000 
	 loss: 42.1674, MinusLogProbMetric: 42.1674, val_loss: 43.2087, val_MinusLogProbMetric: 43.2087

Epoch 717: val_loss did not improve from 42.94955
196/196 - 34s - loss: 42.1674 - MinusLogProbMetric: 42.1674 - val_loss: 43.2087 - val_MinusLogProbMetric: 43.2087 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 718/1000
2023-10-27 23:53:58.973 
Epoch 718/1000 
	 loss: 41.8789, MinusLogProbMetric: 41.8789, val_loss: 43.0496, val_MinusLogProbMetric: 43.0496

Epoch 718: val_loss did not improve from 42.94955
196/196 - 34s - loss: 41.8789 - MinusLogProbMetric: 41.8789 - val_loss: 43.0496 - val_MinusLogProbMetric: 43.0496 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 719/1000
2023-10-27 23:54:32.817 
Epoch 719/1000 
	 loss: 41.9290, MinusLogProbMetric: 41.9290, val_loss: 43.2810, val_MinusLogProbMetric: 43.2810

Epoch 719: val_loss did not improve from 42.94955
196/196 - 34s - loss: 41.9290 - MinusLogProbMetric: 41.9290 - val_loss: 43.2810 - val_MinusLogProbMetric: 43.2810 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 720/1000
2023-10-27 23:55:06.685 
Epoch 720/1000 
	 loss: 41.9355, MinusLogProbMetric: 41.9355, val_loss: 43.0099, val_MinusLogProbMetric: 43.0099

Epoch 720: val_loss did not improve from 42.94955
196/196 - 34s - loss: 41.9355 - MinusLogProbMetric: 41.9355 - val_loss: 43.0099 - val_MinusLogProbMetric: 43.0099 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 721/1000
2023-10-27 23:55:40.377 
Epoch 721/1000 
	 loss: 42.5677, MinusLogProbMetric: 42.5677, val_loss: 43.1101, val_MinusLogProbMetric: 43.1101

Epoch 721: val_loss did not improve from 42.94955
196/196 - 34s - loss: 42.5677 - MinusLogProbMetric: 42.5677 - val_loss: 43.1101 - val_MinusLogProbMetric: 43.1101 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 722/1000
2023-10-27 23:56:14.532 
Epoch 722/1000 
	 loss: 41.9582, MinusLogProbMetric: 41.9582, val_loss: 43.0317, val_MinusLogProbMetric: 43.0317

Epoch 722: val_loss did not improve from 42.94955
196/196 - 34s - loss: 41.9582 - MinusLogProbMetric: 41.9582 - val_loss: 43.0317 - val_MinusLogProbMetric: 43.0317 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 723/1000
2023-10-27 23:56:47.921 
Epoch 723/1000 
	 loss: 41.9507, MinusLogProbMetric: 41.9507, val_loss: 43.3157, val_MinusLogProbMetric: 43.3157

Epoch 723: val_loss did not improve from 42.94955
196/196 - 33s - loss: 41.9507 - MinusLogProbMetric: 41.9507 - val_loss: 43.3157 - val_MinusLogProbMetric: 43.3157 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 724/1000
2023-10-27 23:57:21.721 
Epoch 724/1000 
	 loss: 41.9811, MinusLogProbMetric: 41.9811, val_loss: 43.5255, val_MinusLogProbMetric: 43.5255

Epoch 724: val_loss did not improve from 42.94955
196/196 - 34s - loss: 41.9811 - MinusLogProbMetric: 41.9811 - val_loss: 43.5255 - val_MinusLogProbMetric: 43.5255 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 725/1000
2023-10-27 23:57:54.809 
Epoch 725/1000 
	 loss: 41.9807, MinusLogProbMetric: 41.9807, val_loss: 43.4660, val_MinusLogProbMetric: 43.4660

Epoch 725: val_loss did not improve from 42.94955
196/196 - 33s - loss: 41.9807 - MinusLogProbMetric: 41.9807 - val_loss: 43.4660 - val_MinusLogProbMetric: 43.4660 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 726/1000
2023-10-27 23:58:28.673 
Epoch 726/1000 
	 loss: 41.9811, MinusLogProbMetric: 41.9811, val_loss: 43.1330, val_MinusLogProbMetric: 43.1330

Epoch 726: val_loss did not improve from 42.94955
196/196 - 34s - loss: 41.9811 - MinusLogProbMetric: 41.9811 - val_loss: 43.1330 - val_MinusLogProbMetric: 43.1330 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 727/1000
2023-10-27 23:59:02.675 
Epoch 727/1000 
	 loss: 41.8401, MinusLogProbMetric: 41.8401, val_loss: 43.4195, val_MinusLogProbMetric: 43.4195

Epoch 727: val_loss did not improve from 42.94955
196/196 - 34s - loss: 41.8401 - MinusLogProbMetric: 41.8401 - val_loss: 43.4195 - val_MinusLogProbMetric: 43.4195 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 728/1000
2023-10-27 23:59:36.656 
Epoch 728/1000 
	 loss: 41.9181, MinusLogProbMetric: 41.9181, val_loss: 42.9812, val_MinusLogProbMetric: 42.9812

Epoch 728: val_loss did not improve from 42.94955
196/196 - 34s - loss: 41.9181 - MinusLogProbMetric: 41.9181 - val_loss: 42.9812 - val_MinusLogProbMetric: 42.9812 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 729/1000
2023-10-28 00:00:09.992 
Epoch 729/1000 
	 loss: 42.0005, MinusLogProbMetric: 42.0005, val_loss: 43.0086, val_MinusLogProbMetric: 43.0086

Epoch 729: val_loss did not improve from 42.94955
196/196 - 33s - loss: 42.0005 - MinusLogProbMetric: 42.0005 - val_loss: 43.0086 - val_MinusLogProbMetric: 43.0086 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 730/1000
2023-10-28 00:00:43.840 
Epoch 730/1000 
	 loss: 41.9605, MinusLogProbMetric: 41.9605, val_loss: 42.9593, val_MinusLogProbMetric: 42.9593

Epoch 730: val_loss did not improve from 42.94955
196/196 - 34s - loss: 41.9605 - MinusLogProbMetric: 41.9605 - val_loss: 42.9593 - val_MinusLogProbMetric: 42.9593 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 731/1000
2023-10-28 00:01:17.785 
Epoch 731/1000 
	 loss: 41.9175, MinusLogProbMetric: 41.9175, val_loss: 42.8821, val_MinusLogProbMetric: 42.8821

Epoch 731: val_loss improved from 42.94955 to 42.88206, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 35s - loss: 41.9175 - MinusLogProbMetric: 41.9175 - val_loss: 42.8821 - val_MinusLogProbMetric: 42.8821 - lr: 8.3333e-05 - 35s/epoch - 176ms/step
Epoch 732/1000
2023-10-28 00:01:52.228 
Epoch 732/1000 
	 loss: 42.0982, MinusLogProbMetric: 42.0982, val_loss: 43.0136, val_MinusLogProbMetric: 43.0136

Epoch 732: val_loss did not improve from 42.88206
196/196 - 34s - loss: 42.0982 - MinusLogProbMetric: 42.0982 - val_loss: 43.0136 - val_MinusLogProbMetric: 43.0136 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 733/1000
2023-10-28 00:02:25.839 
Epoch 733/1000 
	 loss: 41.9164, MinusLogProbMetric: 41.9164, val_loss: 44.3489, val_MinusLogProbMetric: 44.3489

Epoch 733: val_loss did not improve from 42.88206
196/196 - 34s - loss: 41.9164 - MinusLogProbMetric: 41.9164 - val_loss: 44.3489 - val_MinusLogProbMetric: 44.3489 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 734/1000
2023-10-28 00:02:59.815 
Epoch 734/1000 
	 loss: 41.9577, MinusLogProbMetric: 41.9577, val_loss: 42.9554, val_MinusLogProbMetric: 42.9554

Epoch 734: val_loss did not improve from 42.88206
196/196 - 34s - loss: 41.9577 - MinusLogProbMetric: 41.9577 - val_loss: 42.9554 - val_MinusLogProbMetric: 42.9554 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 735/1000
2023-10-28 00:03:33.389 
Epoch 735/1000 
	 loss: 41.9136, MinusLogProbMetric: 41.9136, val_loss: 43.3637, val_MinusLogProbMetric: 43.3637

Epoch 735: val_loss did not improve from 42.88206
196/196 - 34s - loss: 41.9136 - MinusLogProbMetric: 41.9136 - val_loss: 43.3637 - val_MinusLogProbMetric: 43.3637 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 736/1000
2023-10-28 00:04:07.234 
Epoch 736/1000 
	 loss: 42.2345, MinusLogProbMetric: 42.2345, val_loss: 43.1555, val_MinusLogProbMetric: 43.1555

Epoch 736: val_loss did not improve from 42.88206
196/196 - 34s - loss: 42.2345 - MinusLogProbMetric: 42.2345 - val_loss: 43.1555 - val_MinusLogProbMetric: 43.1555 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 737/1000
2023-10-28 00:04:41.198 
Epoch 737/1000 
	 loss: 41.8259, MinusLogProbMetric: 41.8259, val_loss: 43.2271, val_MinusLogProbMetric: 43.2271

Epoch 737: val_loss did not improve from 42.88206
196/196 - 34s - loss: 41.8259 - MinusLogProbMetric: 41.8259 - val_loss: 43.2271 - val_MinusLogProbMetric: 43.2271 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 738/1000
2023-10-28 00:05:14.843 
Epoch 738/1000 
	 loss: 42.1002, MinusLogProbMetric: 42.1002, val_loss: 43.4374, val_MinusLogProbMetric: 43.4374

Epoch 738: val_loss did not improve from 42.88206
196/196 - 34s - loss: 42.1002 - MinusLogProbMetric: 42.1002 - val_loss: 43.4374 - val_MinusLogProbMetric: 43.4374 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 739/1000
2023-10-28 00:05:48.421 
Epoch 739/1000 
	 loss: 41.8806, MinusLogProbMetric: 41.8806, val_loss: 43.0391, val_MinusLogProbMetric: 43.0391

Epoch 739: val_loss did not improve from 42.88206
196/196 - 34s - loss: 41.8806 - MinusLogProbMetric: 41.8806 - val_loss: 43.0391 - val_MinusLogProbMetric: 43.0391 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 740/1000
2023-10-28 00:06:22.375 
Epoch 740/1000 
	 loss: 42.0227, MinusLogProbMetric: 42.0227, val_loss: 43.2203, val_MinusLogProbMetric: 43.2203

Epoch 740: val_loss did not improve from 42.88206
196/196 - 34s - loss: 42.0227 - MinusLogProbMetric: 42.0227 - val_loss: 43.2203 - val_MinusLogProbMetric: 43.2203 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 741/1000
2023-10-28 00:06:56.539 
Epoch 741/1000 
	 loss: 41.9357, MinusLogProbMetric: 41.9357, val_loss: 43.2476, val_MinusLogProbMetric: 43.2476

Epoch 741: val_loss did not improve from 42.88206
196/196 - 34s - loss: 41.9357 - MinusLogProbMetric: 41.9357 - val_loss: 43.2476 - val_MinusLogProbMetric: 43.2476 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 742/1000
2023-10-28 00:07:30.499 
Epoch 742/1000 
	 loss: 41.8864, MinusLogProbMetric: 41.8864, val_loss: 43.2454, val_MinusLogProbMetric: 43.2454

Epoch 742: val_loss did not improve from 42.88206
196/196 - 34s - loss: 41.8864 - MinusLogProbMetric: 41.8864 - val_loss: 43.2454 - val_MinusLogProbMetric: 43.2454 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 743/1000
2023-10-28 00:08:03.584 
Epoch 743/1000 
	 loss: 41.9081, MinusLogProbMetric: 41.9081, val_loss: 42.7774, val_MinusLogProbMetric: 42.7774

Epoch 743: val_loss improved from 42.88206 to 42.77738, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 41.9081 - MinusLogProbMetric: 41.9081 - val_loss: 42.7774 - val_MinusLogProbMetric: 42.7774 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 744/1000
2023-10-28 00:08:37.501 
Epoch 744/1000 
	 loss: 41.9176, MinusLogProbMetric: 41.9176, val_loss: 43.5101, val_MinusLogProbMetric: 43.5101

Epoch 744: val_loss did not improve from 42.77738
196/196 - 33s - loss: 41.9176 - MinusLogProbMetric: 41.9176 - val_loss: 43.5101 - val_MinusLogProbMetric: 43.5101 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 745/1000
2023-10-28 00:09:11.033 
Epoch 745/1000 
	 loss: 41.9178, MinusLogProbMetric: 41.9178, val_loss: 43.5581, val_MinusLogProbMetric: 43.5581

Epoch 745: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9178 - MinusLogProbMetric: 41.9178 - val_loss: 43.5581 - val_MinusLogProbMetric: 43.5581 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 746/1000
2023-10-28 00:09:44.590 
Epoch 746/1000 
	 loss: 42.0854, MinusLogProbMetric: 42.0854, val_loss: 43.1224, val_MinusLogProbMetric: 43.1224

Epoch 746: val_loss did not improve from 42.77738
196/196 - 34s - loss: 42.0854 - MinusLogProbMetric: 42.0854 - val_loss: 43.1224 - val_MinusLogProbMetric: 43.1224 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 747/1000
2023-10-28 00:10:17.622 
Epoch 747/1000 
	 loss: 41.8280, MinusLogProbMetric: 41.8280, val_loss: 43.0718, val_MinusLogProbMetric: 43.0718

Epoch 747: val_loss did not improve from 42.77738
196/196 - 33s - loss: 41.8280 - MinusLogProbMetric: 41.8280 - val_loss: 43.0718 - val_MinusLogProbMetric: 43.0718 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 748/1000
2023-10-28 00:10:51.381 
Epoch 748/1000 
	 loss: 41.9029, MinusLogProbMetric: 41.9029, val_loss: 42.8959, val_MinusLogProbMetric: 42.8959

Epoch 748: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9029 - MinusLogProbMetric: 41.9029 - val_loss: 42.8959 - val_MinusLogProbMetric: 42.8959 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 749/1000
2023-10-28 00:11:24.947 
Epoch 749/1000 
	 loss: 41.8886, MinusLogProbMetric: 41.8886, val_loss: 43.1899, val_MinusLogProbMetric: 43.1899

Epoch 749: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8886 - MinusLogProbMetric: 41.8886 - val_loss: 43.1899 - val_MinusLogProbMetric: 43.1899 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 750/1000
2023-10-28 00:11:58.292 
Epoch 750/1000 
	 loss: 42.1200, MinusLogProbMetric: 42.1200, val_loss: 43.1810, val_MinusLogProbMetric: 43.1810

Epoch 750: val_loss did not improve from 42.77738
196/196 - 33s - loss: 42.1200 - MinusLogProbMetric: 42.1200 - val_loss: 43.1810 - val_MinusLogProbMetric: 43.1810 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 751/1000
2023-10-28 00:12:31.616 
Epoch 751/1000 
	 loss: 41.8757, MinusLogProbMetric: 41.8757, val_loss: 43.0948, val_MinusLogProbMetric: 43.0948

Epoch 751: val_loss did not improve from 42.77738
196/196 - 33s - loss: 41.8757 - MinusLogProbMetric: 41.8757 - val_loss: 43.0948 - val_MinusLogProbMetric: 43.0948 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 752/1000
2023-10-28 00:13:05.375 
Epoch 752/1000 
	 loss: 41.8711, MinusLogProbMetric: 41.8711, val_loss: 44.0405, val_MinusLogProbMetric: 44.0405

Epoch 752: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8711 - MinusLogProbMetric: 41.8711 - val_loss: 44.0405 - val_MinusLogProbMetric: 44.0405 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 753/1000
2023-10-28 00:13:38.818 
Epoch 753/1000 
	 loss: 42.0675, MinusLogProbMetric: 42.0675, val_loss: 43.1367, val_MinusLogProbMetric: 43.1367

Epoch 753: val_loss did not improve from 42.77738
196/196 - 33s - loss: 42.0675 - MinusLogProbMetric: 42.0675 - val_loss: 43.1367 - val_MinusLogProbMetric: 43.1367 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 754/1000
2023-10-28 00:14:12.196 
Epoch 754/1000 
	 loss: 41.8568, MinusLogProbMetric: 41.8568, val_loss: 42.9989, val_MinusLogProbMetric: 42.9989

Epoch 754: val_loss did not improve from 42.77738
196/196 - 33s - loss: 41.8568 - MinusLogProbMetric: 41.8568 - val_loss: 42.9989 - val_MinusLogProbMetric: 42.9989 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 755/1000
2023-10-28 00:14:45.999 
Epoch 755/1000 
	 loss: 41.9113, MinusLogProbMetric: 41.9113, val_loss: 43.1554, val_MinusLogProbMetric: 43.1554

Epoch 755: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9113 - MinusLogProbMetric: 41.9113 - val_loss: 43.1554 - val_MinusLogProbMetric: 43.1554 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 756/1000
2023-10-28 00:15:19.972 
Epoch 756/1000 
	 loss: 42.0291, MinusLogProbMetric: 42.0291, val_loss: 42.9810, val_MinusLogProbMetric: 42.9810

Epoch 756: val_loss did not improve from 42.77738
196/196 - 34s - loss: 42.0291 - MinusLogProbMetric: 42.0291 - val_loss: 42.9810 - val_MinusLogProbMetric: 42.9810 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 757/1000
2023-10-28 00:15:53.668 
Epoch 757/1000 
	 loss: 41.8825, MinusLogProbMetric: 41.8825, val_loss: 43.2545, val_MinusLogProbMetric: 43.2545

Epoch 757: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8825 - MinusLogProbMetric: 41.8825 - val_loss: 43.2545 - val_MinusLogProbMetric: 43.2545 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 758/1000
2023-10-28 00:16:27.817 
Epoch 758/1000 
	 loss: 42.0358, MinusLogProbMetric: 42.0358, val_loss: 43.0144, val_MinusLogProbMetric: 43.0144

Epoch 758: val_loss did not improve from 42.77738
196/196 - 34s - loss: 42.0358 - MinusLogProbMetric: 42.0358 - val_loss: 43.0144 - val_MinusLogProbMetric: 43.0144 - lr: 8.3333e-05 - 34s/epoch - 174ms/step
Epoch 759/1000
2023-10-28 00:17:01.483 
Epoch 759/1000 
	 loss: 41.9671, MinusLogProbMetric: 41.9671, val_loss: 43.1004, val_MinusLogProbMetric: 43.1004

Epoch 759: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9671 - MinusLogProbMetric: 41.9671 - val_loss: 43.1004 - val_MinusLogProbMetric: 43.1004 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 760/1000
2023-10-28 00:17:35.052 
Epoch 760/1000 
	 loss: 41.8385, MinusLogProbMetric: 41.8385, val_loss: 43.0743, val_MinusLogProbMetric: 43.0743

Epoch 760: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8385 - MinusLogProbMetric: 41.8385 - val_loss: 43.0743 - val_MinusLogProbMetric: 43.0743 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 761/1000
2023-10-28 00:18:08.627 
Epoch 761/1000 
	 loss: 41.8569, MinusLogProbMetric: 41.8569, val_loss: 43.1866, val_MinusLogProbMetric: 43.1866

Epoch 761: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8569 - MinusLogProbMetric: 41.8569 - val_loss: 43.1866 - val_MinusLogProbMetric: 43.1866 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 762/1000
2023-10-28 00:18:42.253 
Epoch 762/1000 
	 loss: 41.9496, MinusLogProbMetric: 41.9496, val_loss: 43.1122, val_MinusLogProbMetric: 43.1122

Epoch 762: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9496 - MinusLogProbMetric: 41.9496 - val_loss: 43.1122 - val_MinusLogProbMetric: 43.1122 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 763/1000
2023-10-28 00:19:16.026 
Epoch 763/1000 
	 loss: 41.9488, MinusLogProbMetric: 41.9488, val_loss: 43.1060, val_MinusLogProbMetric: 43.1060

Epoch 763: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9488 - MinusLogProbMetric: 41.9488 - val_loss: 43.1060 - val_MinusLogProbMetric: 43.1060 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 764/1000
2023-10-28 00:19:49.100 
Epoch 764/1000 
	 loss: 41.9648, MinusLogProbMetric: 41.9648, val_loss: 43.4922, val_MinusLogProbMetric: 43.4922

Epoch 764: val_loss did not improve from 42.77738
196/196 - 33s - loss: 41.9648 - MinusLogProbMetric: 41.9648 - val_loss: 43.4922 - val_MinusLogProbMetric: 43.4922 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 765/1000
2023-10-28 00:20:21.385 
Epoch 765/1000 
	 loss: 42.0086, MinusLogProbMetric: 42.0086, val_loss: 43.3849, val_MinusLogProbMetric: 43.3849

Epoch 765: val_loss did not improve from 42.77738
196/196 - 32s - loss: 42.0086 - MinusLogProbMetric: 42.0086 - val_loss: 43.3849 - val_MinusLogProbMetric: 43.3849 - lr: 8.3333e-05 - 32s/epoch - 165ms/step
Epoch 766/1000
2023-10-28 00:20:55.067 
Epoch 766/1000 
	 loss: 41.8065, MinusLogProbMetric: 41.8065, val_loss: 42.9371, val_MinusLogProbMetric: 42.9371

Epoch 766: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8065 - MinusLogProbMetric: 41.8065 - val_loss: 42.9371 - val_MinusLogProbMetric: 42.9371 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 767/1000
2023-10-28 00:21:28.518 
Epoch 767/1000 
	 loss: 42.1849, MinusLogProbMetric: 42.1849, val_loss: 43.3172, val_MinusLogProbMetric: 43.3172

Epoch 767: val_loss did not improve from 42.77738
196/196 - 33s - loss: 42.1849 - MinusLogProbMetric: 42.1849 - val_loss: 43.3172 - val_MinusLogProbMetric: 43.3172 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 768/1000
2023-10-28 00:22:01.748 
Epoch 768/1000 
	 loss: 41.9054, MinusLogProbMetric: 41.9054, val_loss: 43.0512, val_MinusLogProbMetric: 43.0512

Epoch 768: val_loss did not improve from 42.77738
196/196 - 33s - loss: 41.9054 - MinusLogProbMetric: 41.9054 - val_loss: 43.0512 - val_MinusLogProbMetric: 43.0512 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 769/1000
2023-10-28 00:22:35.699 
Epoch 769/1000 
	 loss: 41.8657, MinusLogProbMetric: 41.8657, val_loss: 43.4878, val_MinusLogProbMetric: 43.4878

Epoch 769: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8657 - MinusLogProbMetric: 41.8657 - val_loss: 43.4878 - val_MinusLogProbMetric: 43.4878 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 770/1000
2023-10-28 00:23:09.586 
Epoch 770/1000 
	 loss: 41.8798, MinusLogProbMetric: 41.8798, val_loss: 43.4148, val_MinusLogProbMetric: 43.4148

Epoch 770: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8798 - MinusLogProbMetric: 41.8798 - val_loss: 43.4148 - val_MinusLogProbMetric: 43.4148 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 771/1000
2023-10-28 00:23:43.381 
Epoch 771/1000 
	 loss: 41.9698, MinusLogProbMetric: 41.9698, val_loss: 43.3202, val_MinusLogProbMetric: 43.3202

Epoch 771: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9698 - MinusLogProbMetric: 41.9698 - val_loss: 43.3202 - val_MinusLogProbMetric: 43.3202 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 772/1000
2023-10-28 00:24:17.348 
Epoch 772/1000 
	 loss: 42.0703, MinusLogProbMetric: 42.0703, val_loss: 42.9964, val_MinusLogProbMetric: 42.9964

Epoch 772: val_loss did not improve from 42.77738
196/196 - 34s - loss: 42.0703 - MinusLogProbMetric: 42.0703 - val_loss: 42.9964 - val_MinusLogProbMetric: 42.9964 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 773/1000
2023-10-28 00:24:51.197 
Epoch 773/1000 
	 loss: 41.9324, MinusLogProbMetric: 41.9324, val_loss: 43.5912, val_MinusLogProbMetric: 43.5912

Epoch 773: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9324 - MinusLogProbMetric: 41.9324 - val_loss: 43.5912 - val_MinusLogProbMetric: 43.5912 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 774/1000
2023-10-28 00:25:25.034 
Epoch 774/1000 
	 loss: 41.8964, MinusLogProbMetric: 41.8964, val_loss: 43.0595, val_MinusLogProbMetric: 43.0595

Epoch 774: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8964 - MinusLogProbMetric: 41.8964 - val_loss: 43.0595 - val_MinusLogProbMetric: 43.0595 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 775/1000
2023-10-28 00:25:58.887 
Epoch 775/1000 
	 loss: 42.0194, MinusLogProbMetric: 42.0194, val_loss: 43.1172, val_MinusLogProbMetric: 43.1172

Epoch 775: val_loss did not improve from 42.77738
196/196 - 34s - loss: 42.0194 - MinusLogProbMetric: 42.0194 - val_loss: 43.1172 - val_MinusLogProbMetric: 43.1172 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 776/1000
2023-10-28 00:26:32.428 
Epoch 776/1000 
	 loss: 41.9292, MinusLogProbMetric: 41.9292, val_loss: 43.3577, val_MinusLogProbMetric: 43.3577

Epoch 776: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9292 - MinusLogProbMetric: 41.9292 - val_loss: 43.3577 - val_MinusLogProbMetric: 43.3577 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 777/1000
2023-10-28 00:27:06.071 
Epoch 777/1000 
	 loss: 41.8448, MinusLogProbMetric: 41.8448, val_loss: 43.3038, val_MinusLogProbMetric: 43.3038

Epoch 777: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8448 - MinusLogProbMetric: 41.8448 - val_loss: 43.3038 - val_MinusLogProbMetric: 43.3038 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 778/1000
2023-10-28 00:27:39.826 
Epoch 778/1000 
	 loss: 41.9598, MinusLogProbMetric: 41.9598, val_loss: 42.9411, val_MinusLogProbMetric: 42.9411

Epoch 778: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9598 - MinusLogProbMetric: 41.9598 - val_loss: 42.9411 - val_MinusLogProbMetric: 42.9411 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 779/1000
2023-10-28 00:28:13.558 
Epoch 779/1000 
	 loss: 41.9588, MinusLogProbMetric: 41.9588, val_loss: 42.8716, val_MinusLogProbMetric: 42.8716

Epoch 779: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9588 - MinusLogProbMetric: 41.9588 - val_loss: 42.8716 - val_MinusLogProbMetric: 42.8716 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 780/1000
2023-10-28 00:28:47.010 
Epoch 780/1000 
	 loss: 41.9096, MinusLogProbMetric: 41.9096, val_loss: 43.1512, val_MinusLogProbMetric: 43.1512

Epoch 780: val_loss did not improve from 42.77738
196/196 - 33s - loss: 41.9096 - MinusLogProbMetric: 41.9096 - val_loss: 43.1512 - val_MinusLogProbMetric: 43.1512 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 781/1000
2023-10-28 00:29:20.583 
Epoch 781/1000 
	 loss: 41.8258, MinusLogProbMetric: 41.8258, val_loss: 43.2169, val_MinusLogProbMetric: 43.2169

Epoch 781: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8258 - MinusLogProbMetric: 41.8258 - val_loss: 43.2169 - val_MinusLogProbMetric: 43.2169 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 782/1000
2023-10-28 00:29:53.885 
Epoch 782/1000 
	 loss: 41.9358, MinusLogProbMetric: 41.9358, val_loss: 43.6830, val_MinusLogProbMetric: 43.6830

Epoch 782: val_loss did not improve from 42.77738
196/196 - 33s - loss: 41.9358 - MinusLogProbMetric: 41.9358 - val_loss: 43.6830 - val_MinusLogProbMetric: 43.6830 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 783/1000
2023-10-28 00:30:27.442 
Epoch 783/1000 
	 loss: 41.9133, MinusLogProbMetric: 41.9133, val_loss: 43.1552, val_MinusLogProbMetric: 43.1552

Epoch 783: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9133 - MinusLogProbMetric: 41.9133 - val_loss: 43.1552 - val_MinusLogProbMetric: 43.1552 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 784/1000
2023-10-28 00:31:01.232 
Epoch 784/1000 
	 loss: 42.0367, MinusLogProbMetric: 42.0367, val_loss: 43.0443, val_MinusLogProbMetric: 43.0443

Epoch 784: val_loss did not improve from 42.77738
196/196 - 34s - loss: 42.0367 - MinusLogProbMetric: 42.0367 - val_loss: 43.0443 - val_MinusLogProbMetric: 43.0443 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 785/1000
2023-10-28 00:31:35.069 
Epoch 785/1000 
	 loss: 41.9823, MinusLogProbMetric: 41.9823, val_loss: 43.2750, val_MinusLogProbMetric: 43.2750

Epoch 785: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9823 - MinusLogProbMetric: 41.9823 - val_loss: 43.2750 - val_MinusLogProbMetric: 43.2750 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 786/1000
2023-10-28 00:32:08.600 
Epoch 786/1000 
	 loss: 41.8104, MinusLogProbMetric: 41.8104, val_loss: 43.0670, val_MinusLogProbMetric: 43.0670

Epoch 786: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8104 - MinusLogProbMetric: 41.8104 - val_loss: 43.0670 - val_MinusLogProbMetric: 43.0670 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 787/1000
2023-10-28 00:32:42.413 
Epoch 787/1000 
	 loss: 41.9568, MinusLogProbMetric: 41.9568, val_loss: 43.1718, val_MinusLogProbMetric: 43.1718

Epoch 787: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9568 - MinusLogProbMetric: 41.9568 - val_loss: 43.1718 - val_MinusLogProbMetric: 43.1718 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 788/1000
2023-10-28 00:33:15.942 
Epoch 788/1000 
	 loss: 41.8582, MinusLogProbMetric: 41.8582, val_loss: 42.8688, val_MinusLogProbMetric: 42.8688

Epoch 788: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8582 - MinusLogProbMetric: 41.8582 - val_loss: 42.8688 - val_MinusLogProbMetric: 42.8688 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 789/1000
2023-10-28 00:33:49.397 
Epoch 789/1000 
	 loss: 41.8907, MinusLogProbMetric: 41.8907, val_loss: 43.0984, val_MinusLogProbMetric: 43.0984

Epoch 789: val_loss did not improve from 42.77738
196/196 - 33s - loss: 41.8907 - MinusLogProbMetric: 41.8907 - val_loss: 43.0984 - val_MinusLogProbMetric: 43.0984 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 790/1000
2023-10-28 00:34:23.099 
Epoch 790/1000 
	 loss: 41.8891, MinusLogProbMetric: 41.8891, val_loss: 43.0644, val_MinusLogProbMetric: 43.0644

Epoch 790: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8891 - MinusLogProbMetric: 41.8891 - val_loss: 43.0644 - val_MinusLogProbMetric: 43.0644 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 791/1000
2023-10-28 00:34:56.675 
Epoch 791/1000 
	 loss: 41.9017, MinusLogProbMetric: 41.9017, val_loss: 43.0055, val_MinusLogProbMetric: 43.0055

Epoch 791: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.9017 - MinusLogProbMetric: 41.9017 - val_loss: 43.0055 - val_MinusLogProbMetric: 43.0055 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 792/1000
2023-10-28 00:35:29.869 
Epoch 792/1000 
	 loss: 41.8901, MinusLogProbMetric: 41.8901, val_loss: 43.1915, val_MinusLogProbMetric: 43.1915

Epoch 792: val_loss did not improve from 42.77738
196/196 - 33s - loss: 41.8901 - MinusLogProbMetric: 41.8901 - val_loss: 43.1915 - val_MinusLogProbMetric: 43.1915 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 793/1000
2023-10-28 00:36:03.592 
Epoch 793/1000 
	 loss: 41.8581, MinusLogProbMetric: 41.8581, val_loss: 43.2890, val_MinusLogProbMetric: 43.2890

Epoch 793: val_loss did not improve from 42.77738
196/196 - 34s - loss: 41.8581 - MinusLogProbMetric: 41.8581 - val_loss: 43.2890 - val_MinusLogProbMetric: 43.2890 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 794/1000
2023-10-28 00:36:36.980 
Epoch 794/1000 
	 loss: 41.5516, MinusLogProbMetric: 41.5516, val_loss: 42.9602, val_MinusLogProbMetric: 42.9602

Epoch 794: val_loss did not improve from 42.77738
196/196 - 33s - loss: 41.5516 - MinusLogProbMetric: 41.5516 - val_loss: 42.9602 - val_MinusLogProbMetric: 42.9602 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 795/1000
2023-10-28 00:37:09.377 
Epoch 795/1000 
	 loss: 41.5203, MinusLogProbMetric: 41.5203, val_loss: 42.9689, val_MinusLogProbMetric: 42.9689

Epoch 795: val_loss did not improve from 42.77738
196/196 - 32s - loss: 41.5203 - MinusLogProbMetric: 41.5203 - val_loss: 42.9689 - val_MinusLogProbMetric: 42.9689 - lr: 4.1667e-05 - 32s/epoch - 165ms/step
Epoch 796/1000
2023-10-28 00:37:43.004 
Epoch 796/1000 
	 loss: 41.5179, MinusLogProbMetric: 41.5179, val_loss: 42.7643, val_MinusLogProbMetric: 42.7643

Epoch 796: val_loss improved from 42.77738 to 42.76430, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 41.5179 - MinusLogProbMetric: 41.5179 - val_loss: 42.7643 - val_MinusLogProbMetric: 42.7643 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 797/1000
2023-10-28 00:38:17.153 
Epoch 797/1000 
	 loss: 41.5268, MinusLogProbMetric: 41.5268, val_loss: 42.8450, val_MinusLogProbMetric: 42.8450

Epoch 797: val_loss did not improve from 42.76430
196/196 - 34s - loss: 41.5268 - MinusLogProbMetric: 41.5268 - val_loss: 42.8450 - val_MinusLogProbMetric: 42.8450 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 798/1000
2023-10-28 00:38:50.661 
Epoch 798/1000 
	 loss: 41.5461, MinusLogProbMetric: 41.5461, val_loss: 42.9651, val_MinusLogProbMetric: 42.9651

Epoch 798: val_loss did not improve from 42.76430
196/196 - 34s - loss: 41.5461 - MinusLogProbMetric: 41.5461 - val_loss: 42.9651 - val_MinusLogProbMetric: 42.9651 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 799/1000
2023-10-28 00:39:24.568 
Epoch 799/1000 
	 loss: 41.5294, MinusLogProbMetric: 41.5294, val_loss: 42.9273, val_MinusLogProbMetric: 42.9273

Epoch 799: val_loss did not improve from 42.76430
196/196 - 34s - loss: 41.5294 - MinusLogProbMetric: 41.5294 - val_loss: 42.9273 - val_MinusLogProbMetric: 42.9273 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 800/1000
2023-10-28 00:39:56.373 
Epoch 800/1000 
	 loss: 41.5244, MinusLogProbMetric: 41.5244, val_loss: 42.9144, val_MinusLogProbMetric: 42.9144

Epoch 800: val_loss did not improve from 42.76430
196/196 - 32s - loss: 41.5244 - MinusLogProbMetric: 41.5244 - val_loss: 42.9144 - val_MinusLogProbMetric: 42.9144 - lr: 4.1667e-05 - 32s/epoch - 162ms/step
Epoch 801/1000
2023-10-28 00:40:30.231 
Epoch 801/1000 
	 loss: 41.5071, MinusLogProbMetric: 41.5071, val_loss: 42.7564, val_MinusLogProbMetric: 42.7564

Epoch 801: val_loss improved from 42.76430 to 42.75637, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 41.5071 - MinusLogProbMetric: 41.5071 - val_loss: 42.7564 - val_MinusLogProbMetric: 42.7564 - lr: 4.1667e-05 - 34s/epoch - 175ms/step
Epoch 802/1000
2023-10-28 00:41:04.053 
Epoch 802/1000 
	 loss: 41.5086, MinusLogProbMetric: 41.5086, val_loss: 42.9321, val_MinusLogProbMetric: 42.9321

Epoch 802: val_loss did not improve from 42.75637
196/196 - 33s - loss: 41.5086 - MinusLogProbMetric: 41.5086 - val_loss: 42.9321 - val_MinusLogProbMetric: 42.9321 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 803/1000
2023-10-28 00:41:37.718 
Epoch 803/1000 
	 loss: 41.5531, MinusLogProbMetric: 41.5531, val_loss: 42.8167, val_MinusLogProbMetric: 42.8167

Epoch 803: val_loss did not improve from 42.75637
196/196 - 34s - loss: 41.5531 - MinusLogProbMetric: 41.5531 - val_loss: 42.8167 - val_MinusLogProbMetric: 42.8167 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 804/1000
2023-10-28 00:42:11.205 
Epoch 804/1000 
	 loss: 41.5216, MinusLogProbMetric: 41.5216, val_loss: 42.7468, val_MinusLogProbMetric: 42.7468

Epoch 804: val_loss improved from 42.75637 to 42.74678, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 41.5216 - MinusLogProbMetric: 41.5216 - val_loss: 42.7468 - val_MinusLogProbMetric: 42.7468 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 805/1000
2023-10-28 00:42:45.159 
Epoch 805/1000 
	 loss: 41.5282, MinusLogProbMetric: 41.5282, val_loss: 42.7922, val_MinusLogProbMetric: 42.7922

Epoch 805: val_loss did not improve from 42.74678
196/196 - 33s - loss: 41.5282 - MinusLogProbMetric: 41.5282 - val_loss: 42.7922 - val_MinusLogProbMetric: 42.7922 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 806/1000
2023-10-28 00:43:18.753 
Epoch 806/1000 
	 loss: 41.5344, MinusLogProbMetric: 41.5344, val_loss: 42.8491, val_MinusLogProbMetric: 42.8491

Epoch 806: val_loss did not improve from 42.74678
196/196 - 34s - loss: 41.5344 - MinusLogProbMetric: 41.5344 - val_loss: 42.8491 - val_MinusLogProbMetric: 42.8491 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 807/1000
2023-10-28 00:43:52.066 
Epoch 807/1000 
	 loss: 41.5344, MinusLogProbMetric: 41.5344, val_loss: 42.8519, val_MinusLogProbMetric: 42.8519

Epoch 807: val_loss did not improve from 42.74678
196/196 - 33s - loss: 41.5344 - MinusLogProbMetric: 41.5344 - val_loss: 42.8519 - val_MinusLogProbMetric: 42.8519 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 808/1000
2023-10-28 00:44:25.608 
Epoch 808/1000 
	 loss: 41.5017, MinusLogProbMetric: 41.5017, val_loss: 42.9622, val_MinusLogProbMetric: 42.9622

Epoch 808: val_loss did not improve from 42.74678
196/196 - 34s - loss: 41.5017 - MinusLogProbMetric: 41.5017 - val_loss: 42.9622 - val_MinusLogProbMetric: 42.9622 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 809/1000
2023-10-28 00:44:59.347 
Epoch 809/1000 
	 loss: 41.5151, MinusLogProbMetric: 41.5151, val_loss: 42.8965, val_MinusLogProbMetric: 42.8965

Epoch 809: val_loss did not improve from 42.74678
196/196 - 34s - loss: 41.5151 - MinusLogProbMetric: 41.5151 - val_loss: 42.8965 - val_MinusLogProbMetric: 42.8965 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 810/1000
2023-10-28 00:45:33.171 
Epoch 810/1000 
	 loss: 41.5470, MinusLogProbMetric: 41.5470, val_loss: 43.0332, val_MinusLogProbMetric: 43.0332

Epoch 810: val_loss did not improve from 42.74678
196/196 - 34s - loss: 41.5470 - MinusLogProbMetric: 41.5470 - val_loss: 43.0332 - val_MinusLogProbMetric: 43.0332 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 811/1000
2023-10-28 00:46:06.846 
Epoch 811/1000 
	 loss: 41.4995, MinusLogProbMetric: 41.4995, val_loss: 42.8565, val_MinusLogProbMetric: 42.8565

Epoch 811: val_loss did not improve from 42.74678
196/196 - 34s - loss: 41.4995 - MinusLogProbMetric: 41.4995 - val_loss: 42.8565 - val_MinusLogProbMetric: 42.8565 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 812/1000
2023-10-28 00:46:40.470 
Epoch 812/1000 
	 loss: 41.5242, MinusLogProbMetric: 41.5242, val_loss: 42.9481, val_MinusLogProbMetric: 42.9481

Epoch 812: val_loss did not improve from 42.74678
196/196 - 34s - loss: 41.5242 - MinusLogProbMetric: 41.5242 - val_loss: 42.9481 - val_MinusLogProbMetric: 42.9481 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 813/1000
2023-10-28 00:47:13.956 
Epoch 813/1000 
	 loss: 41.5203, MinusLogProbMetric: 41.5203, val_loss: 42.7518, val_MinusLogProbMetric: 42.7518

Epoch 813: val_loss did not improve from 42.74678
196/196 - 33s - loss: 41.5203 - MinusLogProbMetric: 41.5203 - val_loss: 42.7518 - val_MinusLogProbMetric: 42.7518 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 814/1000
2023-10-28 00:47:47.641 
Epoch 814/1000 
	 loss: 41.5244, MinusLogProbMetric: 41.5244, val_loss: 42.8590, val_MinusLogProbMetric: 42.8590

Epoch 814: val_loss did not improve from 42.74678
196/196 - 34s - loss: 41.5244 - MinusLogProbMetric: 41.5244 - val_loss: 42.8590 - val_MinusLogProbMetric: 42.8590 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 815/1000
2023-10-28 00:48:21.231 
Epoch 815/1000 
	 loss: 41.5188, MinusLogProbMetric: 41.5188, val_loss: 43.2466, val_MinusLogProbMetric: 43.2466

Epoch 815: val_loss did not improve from 42.74678
196/196 - 34s - loss: 41.5188 - MinusLogProbMetric: 41.5188 - val_loss: 43.2466 - val_MinusLogProbMetric: 43.2466 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 816/1000
2023-10-28 00:48:54.920 
Epoch 816/1000 
	 loss: 41.5302, MinusLogProbMetric: 41.5302, val_loss: 42.9451, val_MinusLogProbMetric: 42.9451

Epoch 816: val_loss did not improve from 42.74678
196/196 - 34s - loss: 41.5302 - MinusLogProbMetric: 41.5302 - val_loss: 42.9451 - val_MinusLogProbMetric: 42.9451 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 817/1000
2023-10-28 00:49:28.551 
Epoch 817/1000 
	 loss: 41.4968, MinusLogProbMetric: 41.4968, val_loss: 42.6995, val_MinusLogProbMetric: 42.6995

Epoch 817: val_loss improved from 42.74678 to 42.69952, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 34s - loss: 41.4968 - MinusLogProbMetric: 41.4968 - val_loss: 42.6995 - val_MinusLogProbMetric: 42.6995 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 818/1000
2023-10-28 00:50:01.636 
Epoch 818/1000 
	 loss: 41.5488, MinusLogProbMetric: 41.5488, val_loss: 42.9964, val_MinusLogProbMetric: 42.9964

Epoch 818: val_loss did not improve from 42.69952
196/196 - 33s - loss: 41.5488 - MinusLogProbMetric: 41.5488 - val_loss: 42.9964 - val_MinusLogProbMetric: 42.9964 - lr: 4.1667e-05 - 33s/epoch - 166ms/step
Epoch 819/1000
2023-10-28 00:50:35.190 
Epoch 819/1000 
	 loss: 41.5333, MinusLogProbMetric: 41.5333, val_loss: 42.8039, val_MinusLogProbMetric: 42.8039

Epoch 819: val_loss did not improve from 42.69952
196/196 - 34s - loss: 41.5333 - MinusLogProbMetric: 41.5333 - val_loss: 42.8039 - val_MinusLogProbMetric: 42.8039 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 820/1000
2023-10-28 00:51:08.871 
Epoch 820/1000 
	 loss: 41.5128, MinusLogProbMetric: 41.5128, val_loss: 42.7307, val_MinusLogProbMetric: 42.7307

Epoch 820: val_loss did not improve from 42.69952
196/196 - 34s - loss: 41.5128 - MinusLogProbMetric: 41.5128 - val_loss: 42.7307 - val_MinusLogProbMetric: 42.7307 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 821/1000
2023-10-28 00:51:40.826 
Epoch 821/1000 
	 loss: 41.5128, MinusLogProbMetric: 41.5128, val_loss: 42.8883, val_MinusLogProbMetric: 42.8883

Epoch 821: val_loss did not improve from 42.69952
196/196 - 32s - loss: 41.5128 - MinusLogProbMetric: 41.5128 - val_loss: 42.8883 - val_MinusLogProbMetric: 42.8883 - lr: 4.1667e-05 - 32s/epoch - 163ms/step
Epoch 822/1000
2023-10-28 00:52:14.492 
Epoch 822/1000 
	 loss: 41.5270, MinusLogProbMetric: 41.5270, val_loss: 42.8256, val_MinusLogProbMetric: 42.8256

Epoch 822: val_loss did not improve from 42.69952
196/196 - 34s - loss: 41.5270 - MinusLogProbMetric: 41.5270 - val_loss: 42.8256 - val_MinusLogProbMetric: 42.8256 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 823/1000
2023-10-28 00:52:47.690 
Epoch 823/1000 
	 loss: 41.5119, MinusLogProbMetric: 41.5119, val_loss: 42.8037, val_MinusLogProbMetric: 42.8037

Epoch 823: val_loss did not improve from 42.69952
196/196 - 33s - loss: 41.5119 - MinusLogProbMetric: 41.5119 - val_loss: 42.8037 - val_MinusLogProbMetric: 42.8037 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 824/1000
2023-10-28 00:53:21.325 
Epoch 824/1000 
	 loss: 41.5162, MinusLogProbMetric: 41.5162, val_loss: 42.7437, val_MinusLogProbMetric: 42.7437

Epoch 824: val_loss did not improve from 42.69952
196/196 - 34s - loss: 41.5162 - MinusLogProbMetric: 41.5162 - val_loss: 42.7437 - val_MinusLogProbMetric: 42.7437 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 825/1000
2023-10-28 00:53:54.915 
Epoch 825/1000 
	 loss: 41.5352, MinusLogProbMetric: 41.5352, val_loss: 42.8280, val_MinusLogProbMetric: 42.8280

Epoch 825: val_loss did not improve from 42.69952
196/196 - 34s - loss: 41.5352 - MinusLogProbMetric: 41.5352 - val_loss: 42.8280 - val_MinusLogProbMetric: 42.8280 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 826/1000
2023-10-28 00:54:27.216 
Epoch 826/1000 
	 loss: 41.5197, MinusLogProbMetric: 41.5197, val_loss: 42.8019, val_MinusLogProbMetric: 42.8019

Epoch 826: val_loss did not improve from 42.69952
196/196 - 32s - loss: 41.5197 - MinusLogProbMetric: 41.5197 - val_loss: 42.8019 - val_MinusLogProbMetric: 42.8019 - lr: 4.1667e-05 - 32s/epoch - 165ms/step
Epoch 827/1000
2023-10-28 00:55:01.050 
Epoch 827/1000 
	 loss: 41.4876, MinusLogProbMetric: 41.4876, val_loss: 42.9364, val_MinusLogProbMetric: 42.9364

Epoch 827: val_loss did not improve from 42.69952
196/196 - 34s - loss: 41.4876 - MinusLogProbMetric: 41.4876 - val_loss: 42.9364 - val_MinusLogProbMetric: 42.9364 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 828/1000
2023-10-28 00:55:34.722 
Epoch 828/1000 
	 loss: 41.4934, MinusLogProbMetric: 41.4934, val_loss: 42.7158, val_MinusLogProbMetric: 42.7158

Epoch 828: val_loss did not improve from 42.69952
196/196 - 34s - loss: 41.4934 - MinusLogProbMetric: 41.4934 - val_loss: 42.7158 - val_MinusLogProbMetric: 42.7158 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 829/1000
2023-10-28 00:56:08.530 
Epoch 829/1000 
	 loss: 41.5059, MinusLogProbMetric: 41.5059, val_loss: 43.2422, val_MinusLogProbMetric: 43.2422

Epoch 829: val_loss did not improve from 42.69952
196/196 - 34s - loss: 41.5059 - MinusLogProbMetric: 41.5059 - val_loss: 43.2422 - val_MinusLogProbMetric: 43.2422 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 830/1000
2023-10-28 00:56:42.316 
Epoch 830/1000 
	 loss: 41.5047, MinusLogProbMetric: 41.5047, val_loss: 42.8215, val_MinusLogProbMetric: 42.8215

Epoch 830: val_loss did not improve from 42.69952
196/196 - 34s - loss: 41.5047 - MinusLogProbMetric: 41.5047 - val_loss: 42.8215 - val_MinusLogProbMetric: 42.8215 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 831/1000
2023-10-28 00:57:15.547 
Epoch 831/1000 
	 loss: 41.5160, MinusLogProbMetric: 41.5160, val_loss: 42.8308, val_MinusLogProbMetric: 42.8308

Epoch 831: val_loss did not improve from 42.69952
196/196 - 33s - loss: 41.5160 - MinusLogProbMetric: 41.5160 - val_loss: 42.8308 - val_MinusLogProbMetric: 42.8308 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 832/1000
2023-10-28 00:57:49.223 
Epoch 832/1000 
	 loss: 41.4933, MinusLogProbMetric: 41.4933, val_loss: 42.8871, val_MinusLogProbMetric: 42.8871

Epoch 832: val_loss did not improve from 42.69952
196/196 - 34s - loss: 41.4933 - MinusLogProbMetric: 41.4933 - val_loss: 42.8871 - val_MinusLogProbMetric: 42.8871 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 833/1000
2023-10-28 00:58:20.829 
Epoch 833/1000 
	 loss: 41.5335, MinusLogProbMetric: 41.5335, val_loss: 42.8912, val_MinusLogProbMetric: 42.8912

Epoch 833: val_loss did not improve from 42.69952
196/196 - 32s - loss: 41.5335 - MinusLogProbMetric: 41.5335 - val_loss: 42.8912 - val_MinusLogProbMetric: 42.8912 - lr: 4.1667e-05 - 32s/epoch - 161ms/step
Epoch 834/1000
2023-10-28 00:58:49.341 
Epoch 834/1000 
	 loss: 41.5152, MinusLogProbMetric: 41.5152, val_loss: 42.7956, val_MinusLogProbMetric: 42.7956

Epoch 834: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.5152 - MinusLogProbMetric: 41.5152 - val_loss: 42.7956 - val_MinusLogProbMetric: 42.7956 - lr: 4.1667e-05 - 29s/epoch - 145ms/step
Epoch 835/1000
2023-10-28 00:59:18.147 
Epoch 835/1000 
	 loss: 41.5122, MinusLogProbMetric: 41.5122, val_loss: 42.7897, val_MinusLogProbMetric: 42.7897

Epoch 835: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.5122 - MinusLogProbMetric: 41.5122 - val_loss: 42.7897 - val_MinusLogProbMetric: 42.7897 - lr: 4.1667e-05 - 29s/epoch - 147ms/step
Epoch 836/1000
2023-10-28 00:59:46.815 
Epoch 836/1000 
	 loss: 41.5027, MinusLogProbMetric: 41.5027, val_loss: 42.8902, val_MinusLogProbMetric: 42.8902

Epoch 836: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.5027 - MinusLogProbMetric: 41.5027 - val_loss: 42.8902 - val_MinusLogProbMetric: 42.8902 - lr: 4.1667e-05 - 29s/epoch - 146ms/step
Epoch 837/1000
2023-10-28 01:00:16.574 
Epoch 837/1000 
	 loss: 41.5029, MinusLogProbMetric: 41.5029, val_loss: 42.9076, val_MinusLogProbMetric: 42.9076

Epoch 837: val_loss did not improve from 42.69952
196/196 - 30s - loss: 41.5029 - MinusLogProbMetric: 41.5029 - val_loss: 42.9076 - val_MinusLogProbMetric: 42.9076 - lr: 4.1667e-05 - 30s/epoch - 152ms/step
Epoch 838/1000
2023-10-28 01:00:47.008 
Epoch 838/1000 
	 loss: 41.5616, MinusLogProbMetric: 41.5616, val_loss: 42.8827, val_MinusLogProbMetric: 42.8827

Epoch 838: val_loss did not improve from 42.69952
196/196 - 30s - loss: 41.5616 - MinusLogProbMetric: 41.5616 - val_loss: 42.8827 - val_MinusLogProbMetric: 42.8827 - lr: 4.1667e-05 - 30s/epoch - 155ms/step
Epoch 839/1000
2023-10-28 01:01:15.702 
Epoch 839/1000 
	 loss: 41.5574, MinusLogProbMetric: 41.5574, val_loss: 42.7902, val_MinusLogProbMetric: 42.7902

Epoch 839: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.5574 - MinusLogProbMetric: 41.5574 - val_loss: 42.7902 - val_MinusLogProbMetric: 42.7902 - lr: 4.1667e-05 - 29s/epoch - 146ms/step
Epoch 840/1000
2023-10-28 01:01:44.023 
Epoch 840/1000 
	 loss: 41.4841, MinusLogProbMetric: 41.4841, val_loss: 42.7615, val_MinusLogProbMetric: 42.7615

Epoch 840: val_loss did not improve from 42.69952
196/196 - 28s - loss: 41.4841 - MinusLogProbMetric: 41.4841 - val_loss: 42.7615 - val_MinusLogProbMetric: 42.7615 - lr: 4.1667e-05 - 28s/epoch - 144ms/step
Epoch 841/1000
2023-10-28 01:02:12.631 
Epoch 841/1000 
	 loss: 41.5186, MinusLogProbMetric: 41.5186, val_loss: 42.8244, val_MinusLogProbMetric: 42.8244

Epoch 841: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.5186 - MinusLogProbMetric: 41.5186 - val_loss: 42.8244 - val_MinusLogProbMetric: 42.8244 - lr: 4.1667e-05 - 29s/epoch - 146ms/step
Epoch 842/1000
2023-10-28 01:02:42.148 
Epoch 842/1000 
	 loss: 41.5052, MinusLogProbMetric: 41.5052, val_loss: 42.8930, val_MinusLogProbMetric: 42.8930

Epoch 842: val_loss did not improve from 42.69952
196/196 - 30s - loss: 41.5052 - MinusLogProbMetric: 41.5052 - val_loss: 42.8930 - val_MinusLogProbMetric: 42.8930 - lr: 4.1667e-05 - 30s/epoch - 151ms/step
Epoch 843/1000
2023-10-28 01:03:13.655 
Epoch 843/1000 
	 loss: 41.5107, MinusLogProbMetric: 41.5107, val_loss: 42.9733, val_MinusLogProbMetric: 42.9733

Epoch 843: val_loss did not improve from 42.69952
196/196 - 32s - loss: 41.5107 - MinusLogProbMetric: 41.5107 - val_loss: 42.9733 - val_MinusLogProbMetric: 42.9733 - lr: 4.1667e-05 - 32s/epoch - 161ms/step
Epoch 844/1000
2023-10-28 01:03:42.157 
Epoch 844/1000 
	 loss: 41.4913, MinusLogProbMetric: 41.4913, val_loss: 42.9025, val_MinusLogProbMetric: 42.9025

Epoch 844: val_loss did not improve from 42.69952
196/196 - 28s - loss: 41.4913 - MinusLogProbMetric: 41.4913 - val_loss: 42.9025 - val_MinusLogProbMetric: 42.9025 - lr: 4.1667e-05 - 28s/epoch - 145ms/step
Epoch 845/1000
2023-10-28 01:04:10.945 
Epoch 845/1000 
	 loss: 41.5126, MinusLogProbMetric: 41.5126, val_loss: 42.7683, val_MinusLogProbMetric: 42.7683

Epoch 845: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.5126 - MinusLogProbMetric: 41.5126 - val_loss: 42.7683 - val_MinusLogProbMetric: 42.7683 - lr: 4.1667e-05 - 29s/epoch - 147ms/step
Epoch 846/1000
2023-10-28 01:04:39.550 
Epoch 846/1000 
	 loss: 41.4821, MinusLogProbMetric: 41.4821, val_loss: 42.7753, val_MinusLogProbMetric: 42.7753

Epoch 846: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.4821 - MinusLogProbMetric: 41.4821 - val_loss: 42.7753 - val_MinusLogProbMetric: 42.7753 - lr: 4.1667e-05 - 29s/epoch - 146ms/step
Epoch 847/1000
2023-10-28 01:05:10.016 
Epoch 847/1000 
	 loss: 41.5131, MinusLogProbMetric: 41.5131, val_loss: 43.0878, val_MinusLogProbMetric: 43.0878

Epoch 847: val_loss did not improve from 42.69952
196/196 - 30s - loss: 41.5131 - MinusLogProbMetric: 41.5131 - val_loss: 43.0878 - val_MinusLogProbMetric: 43.0878 - lr: 4.1667e-05 - 30s/epoch - 155ms/step
Epoch 848/1000
2023-10-28 01:05:40.847 
Epoch 848/1000 
	 loss: 41.4901, MinusLogProbMetric: 41.4901, val_loss: 43.0559, val_MinusLogProbMetric: 43.0559

Epoch 848: val_loss did not improve from 42.69952
196/196 - 31s - loss: 41.4901 - MinusLogProbMetric: 41.4901 - val_loss: 43.0559 - val_MinusLogProbMetric: 43.0559 - lr: 4.1667e-05 - 31s/epoch - 157ms/step
Epoch 849/1000
2023-10-28 01:06:11.096 
Epoch 849/1000 
	 loss: 41.5178, MinusLogProbMetric: 41.5178, val_loss: 42.8708, val_MinusLogProbMetric: 42.8708

Epoch 849: val_loss did not improve from 42.69952
196/196 - 30s - loss: 41.5178 - MinusLogProbMetric: 41.5178 - val_loss: 42.8708 - val_MinusLogProbMetric: 42.8708 - lr: 4.1667e-05 - 30s/epoch - 154ms/step
Epoch 850/1000
2023-10-28 01:06:39.607 
Epoch 850/1000 
	 loss: 41.5185, MinusLogProbMetric: 41.5185, val_loss: 42.7502, val_MinusLogProbMetric: 42.7502

Epoch 850: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.5185 - MinusLogProbMetric: 41.5185 - val_loss: 42.7502 - val_MinusLogProbMetric: 42.7502 - lr: 4.1667e-05 - 29s/epoch - 145ms/step
Epoch 851/1000
2023-10-28 01:07:08.446 
Epoch 851/1000 
	 loss: 41.5001, MinusLogProbMetric: 41.5001, val_loss: 42.8243, val_MinusLogProbMetric: 42.8243

Epoch 851: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.5001 - MinusLogProbMetric: 41.5001 - val_loss: 42.8243 - val_MinusLogProbMetric: 42.8243 - lr: 4.1667e-05 - 29s/epoch - 147ms/step
Epoch 852/1000
2023-10-28 01:07:39.062 
Epoch 852/1000 
	 loss: 41.5343, MinusLogProbMetric: 41.5343, val_loss: 42.8246, val_MinusLogProbMetric: 42.8246

Epoch 852: val_loss did not improve from 42.69952
196/196 - 31s - loss: 41.5343 - MinusLogProbMetric: 41.5343 - val_loss: 42.8246 - val_MinusLogProbMetric: 42.8246 - lr: 4.1667e-05 - 31s/epoch - 156ms/step
Epoch 853/1000
2023-10-28 01:08:09.354 
Epoch 853/1000 
	 loss: 41.4978, MinusLogProbMetric: 41.4978, val_loss: 42.8856, val_MinusLogProbMetric: 42.8856

Epoch 853: val_loss did not improve from 42.69952
196/196 - 30s - loss: 41.4978 - MinusLogProbMetric: 41.4978 - val_loss: 42.8856 - val_MinusLogProbMetric: 42.8856 - lr: 4.1667e-05 - 30s/epoch - 155ms/step
Epoch 854/1000
2023-10-28 01:08:39.454 
Epoch 854/1000 
	 loss: 41.5050, MinusLogProbMetric: 41.5050, val_loss: 42.8287, val_MinusLogProbMetric: 42.8287

Epoch 854: val_loss did not improve from 42.69952
196/196 - 30s - loss: 41.5050 - MinusLogProbMetric: 41.5050 - val_loss: 42.8287 - val_MinusLogProbMetric: 42.8287 - lr: 4.1667e-05 - 30s/epoch - 154ms/step
Epoch 855/1000
2023-10-28 01:09:08.241 
Epoch 855/1000 
	 loss: 41.4779, MinusLogProbMetric: 41.4779, val_loss: 42.7083, val_MinusLogProbMetric: 42.7083

Epoch 855: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.4779 - MinusLogProbMetric: 41.4779 - val_loss: 42.7083 - val_MinusLogProbMetric: 42.7083 - lr: 4.1667e-05 - 29s/epoch - 147ms/step
Epoch 856/1000
2023-10-28 01:09:37.111 
Epoch 856/1000 
	 loss: 41.5142, MinusLogProbMetric: 41.5142, val_loss: 42.7494, val_MinusLogProbMetric: 42.7494

Epoch 856: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.5142 - MinusLogProbMetric: 41.5142 - val_loss: 42.7494 - val_MinusLogProbMetric: 42.7494 - lr: 4.1667e-05 - 29s/epoch - 147ms/step
Epoch 857/1000
2023-10-28 01:10:06.032 
Epoch 857/1000 
	 loss: 41.5177, MinusLogProbMetric: 41.5177, val_loss: 43.0837, val_MinusLogProbMetric: 43.0837

Epoch 857: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.5177 - MinusLogProbMetric: 41.5177 - val_loss: 43.0837 - val_MinusLogProbMetric: 43.0837 - lr: 4.1667e-05 - 29s/epoch - 148ms/step
Epoch 858/1000
2023-10-28 01:10:36.338 
Epoch 858/1000 
	 loss: 41.4985, MinusLogProbMetric: 41.4985, val_loss: 42.7563, val_MinusLogProbMetric: 42.7563

Epoch 858: val_loss did not improve from 42.69952
196/196 - 30s - loss: 41.4985 - MinusLogProbMetric: 41.4985 - val_loss: 42.7563 - val_MinusLogProbMetric: 42.7563 - lr: 4.1667e-05 - 30s/epoch - 155ms/step
Epoch 859/1000
2023-10-28 01:11:06.851 
Epoch 859/1000 
	 loss: 41.5096, MinusLogProbMetric: 41.5096, val_loss: 42.9127, val_MinusLogProbMetric: 42.9127

Epoch 859: val_loss did not improve from 42.69952
196/196 - 31s - loss: 41.5096 - MinusLogProbMetric: 41.5096 - val_loss: 42.9127 - val_MinusLogProbMetric: 42.9127 - lr: 4.1667e-05 - 31s/epoch - 156ms/step
Epoch 860/1000
2023-10-28 01:11:35.443 
Epoch 860/1000 
	 loss: 41.4959, MinusLogProbMetric: 41.4959, val_loss: 42.9414, val_MinusLogProbMetric: 42.9414

Epoch 860: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.4959 - MinusLogProbMetric: 41.4959 - val_loss: 42.9414 - val_MinusLogProbMetric: 42.9414 - lr: 4.1667e-05 - 29s/epoch - 146ms/step
Epoch 861/1000
2023-10-28 01:12:04.442 
Epoch 861/1000 
	 loss: 41.5034, MinusLogProbMetric: 41.5034, val_loss: 43.0242, val_MinusLogProbMetric: 43.0242

Epoch 861: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.5034 - MinusLogProbMetric: 41.5034 - val_loss: 43.0242 - val_MinusLogProbMetric: 43.0242 - lr: 4.1667e-05 - 29s/epoch - 148ms/step
Epoch 862/1000
2023-10-28 01:12:33.514 
Epoch 862/1000 
	 loss: 41.4752, MinusLogProbMetric: 41.4752, val_loss: 42.9534, val_MinusLogProbMetric: 42.9534

Epoch 862: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.4752 - MinusLogProbMetric: 41.4752 - val_loss: 42.9534 - val_MinusLogProbMetric: 42.9534 - lr: 4.1667e-05 - 29s/epoch - 148ms/step
Epoch 863/1000
2023-10-28 01:13:03.858 
Epoch 863/1000 
	 loss: 41.4809, MinusLogProbMetric: 41.4809, val_loss: 42.9079, val_MinusLogProbMetric: 42.9079

Epoch 863: val_loss did not improve from 42.69952
196/196 - 30s - loss: 41.4809 - MinusLogProbMetric: 41.4809 - val_loss: 42.9079 - val_MinusLogProbMetric: 42.9079 - lr: 4.1667e-05 - 30s/epoch - 155ms/step
Epoch 864/1000
2023-10-28 01:13:35.425 
Epoch 864/1000 
	 loss: 41.5202, MinusLogProbMetric: 41.5202, val_loss: 42.9093, val_MinusLogProbMetric: 42.9093

Epoch 864: val_loss did not improve from 42.69952
196/196 - 32s - loss: 41.5202 - MinusLogProbMetric: 41.5202 - val_loss: 42.9093 - val_MinusLogProbMetric: 42.9093 - lr: 4.1667e-05 - 32s/epoch - 161ms/step
Epoch 865/1000
2023-10-28 01:14:03.941 
Epoch 865/1000 
	 loss: 41.4888, MinusLogProbMetric: 41.4888, val_loss: 42.8654, val_MinusLogProbMetric: 42.8654

Epoch 865: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.4888 - MinusLogProbMetric: 41.4888 - val_loss: 42.8654 - val_MinusLogProbMetric: 42.8654 - lr: 4.1667e-05 - 29s/epoch - 145ms/step
Epoch 866/1000
2023-10-28 01:14:32.808 
Epoch 866/1000 
	 loss: 41.5245, MinusLogProbMetric: 41.5245, val_loss: 42.8171, val_MinusLogProbMetric: 42.8171

Epoch 866: val_loss did not improve from 42.69952
196/196 - 29s - loss: 41.5245 - MinusLogProbMetric: 41.5245 - val_loss: 42.8171 - val_MinusLogProbMetric: 42.8171 - lr: 4.1667e-05 - 29s/epoch - 147ms/step
Epoch 867/1000
2023-10-28 01:15:00.136 
Epoch 867/1000 
	 loss: 41.4734, MinusLogProbMetric: 41.4734, val_loss: 42.8428, val_MinusLogProbMetric: 42.8428

Epoch 867: val_loss did not improve from 42.69952
196/196 - 27s - loss: 41.4734 - MinusLogProbMetric: 41.4734 - val_loss: 42.8428 - val_MinusLogProbMetric: 42.8428 - lr: 4.1667e-05 - 27s/epoch - 139ms/step
Epoch 868/1000
2023-10-28 01:15:28.362 
Epoch 868/1000 
	 loss: 41.3918, MinusLogProbMetric: 41.3918, val_loss: 42.6773, val_MinusLogProbMetric: 42.6773

Epoch 868: val_loss improved from 42.69952 to 42.67731, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 29s - loss: 41.3918 - MinusLogProbMetric: 41.3918 - val_loss: 42.6773 - val_MinusLogProbMetric: 42.6773 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 869/1000
2023-10-28 01:15:59.597 
Epoch 869/1000 
	 loss: 41.3837, MinusLogProbMetric: 41.3837, val_loss: 42.7958, val_MinusLogProbMetric: 42.7958

Epoch 869: val_loss did not improve from 42.67731
196/196 - 31s - loss: 41.3837 - MinusLogProbMetric: 41.3837 - val_loss: 42.7958 - val_MinusLogProbMetric: 42.7958 - lr: 2.0833e-05 - 31s/epoch - 156ms/step
Epoch 870/1000
2023-10-28 01:16:29.353 
Epoch 870/1000 
	 loss: 41.3863, MinusLogProbMetric: 41.3863, val_loss: 42.8064, val_MinusLogProbMetric: 42.8064

Epoch 870: val_loss did not improve from 42.67731
196/196 - 30s - loss: 41.3863 - MinusLogProbMetric: 41.3863 - val_loss: 42.8064 - val_MinusLogProbMetric: 42.8064 - lr: 2.0833e-05 - 30s/epoch - 152ms/step
Epoch 871/1000
2023-10-28 01:16:57.864 
Epoch 871/1000 
	 loss: 41.3811, MinusLogProbMetric: 41.3811, val_loss: 42.7358, val_MinusLogProbMetric: 42.7358

Epoch 871: val_loss did not improve from 42.67731
196/196 - 29s - loss: 41.3811 - MinusLogProbMetric: 41.3811 - val_loss: 42.7358 - val_MinusLogProbMetric: 42.7358 - lr: 2.0833e-05 - 29s/epoch - 145ms/step
Epoch 872/1000
2023-10-28 01:17:26.482 
Epoch 872/1000 
	 loss: 41.3829, MinusLogProbMetric: 41.3829, val_loss: 42.7168, val_MinusLogProbMetric: 42.7168

Epoch 872: val_loss did not improve from 42.67731
196/196 - 29s - loss: 41.3829 - MinusLogProbMetric: 41.3829 - val_loss: 42.7168 - val_MinusLogProbMetric: 42.7168 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 873/1000
2023-10-28 01:17:54.847 
Epoch 873/1000 
	 loss: 41.3759, MinusLogProbMetric: 41.3759, val_loss: 42.8382, val_MinusLogProbMetric: 42.8382

Epoch 873: val_loss did not improve from 42.67731
196/196 - 28s - loss: 41.3759 - MinusLogProbMetric: 41.3759 - val_loss: 42.8382 - val_MinusLogProbMetric: 42.8382 - lr: 2.0833e-05 - 28s/epoch - 145ms/step
Epoch 874/1000
2023-10-28 01:18:24.060 
Epoch 874/1000 
	 loss: 41.3725, MinusLogProbMetric: 41.3725, val_loss: 42.9665, val_MinusLogProbMetric: 42.9665

Epoch 874: val_loss did not improve from 42.67731
196/196 - 29s - loss: 41.3725 - MinusLogProbMetric: 41.3725 - val_loss: 42.9665 - val_MinusLogProbMetric: 42.9665 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 875/1000
2023-10-28 01:18:54.872 
Epoch 875/1000 
	 loss: 41.3831, MinusLogProbMetric: 41.3831, val_loss: 42.8344, val_MinusLogProbMetric: 42.8344

Epoch 875: val_loss did not improve from 42.67731
196/196 - 31s - loss: 41.3831 - MinusLogProbMetric: 41.3831 - val_loss: 42.8344 - val_MinusLogProbMetric: 42.8344 - lr: 2.0833e-05 - 31s/epoch - 157ms/step
Epoch 876/1000
2023-10-28 01:19:24.409 
Epoch 876/1000 
	 loss: 41.3782, MinusLogProbMetric: 41.3782, val_loss: 42.7929, val_MinusLogProbMetric: 42.7929

Epoch 876: val_loss did not improve from 42.67731
196/196 - 30s - loss: 41.3782 - MinusLogProbMetric: 41.3782 - val_loss: 42.7929 - val_MinusLogProbMetric: 42.7929 - lr: 2.0833e-05 - 30s/epoch - 151ms/step
Epoch 877/1000
2023-10-28 01:19:52.893 
Epoch 877/1000 
	 loss: 41.3762, MinusLogProbMetric: 41.3762, val_loss: 42.7380, val_MinusLogProbMetric: 42.7380

Epoch 877: val_loss did not improve from 42.67731
196/196 - 28s - loss: 41.3762 - MinusLogProbMetric: 41.3762 - val_loss: 42.7380 - val_MinusLogProbMetric: 42.7380 - lr: 2.0833e-05 - 28s/epoch - 145ms/step
Epoch 878/1000
2023-10-28 01:20:21.732 
Epoch 878/1000 
	 loss: 41.3765, MinusLogProbMetric: 41.3765, val_loss: 42.7790, val_MinusLogProbMetric: 42.7790

Epoch 878: val_loss did not improve from 42.67731
196/196 - 29s - loss: 41.3765 - MinusLogProbMetric: 41.3765 - val_loss: 42.7790 - val_MinusLogProbMetric: 42.7790 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 879/1000
2023-10-28 01:20:52.114 
Epoch 879/1000 
	 loss: 41.3745, MinusLogProbMetric: 41.3745, val_loss: 42.6749, val_MinusLogProbMetric: 42.6749

Epoch 879: val_loss improved from 42.67731 to 42.67490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 31s - loss: 41.3745 - MinusLogProbMetric: 41.3745 - val_loss: 42.6749 - val_MinusLogProbMetric: 42.6749 - lr: 2.0833e-05 - 31s/epoch - 158ms/step
Epoch 880/1000
2023-10-28 01:21:22.442 
Epoch 880/1000 
	 loss: 41.3826, MinusLogProbMetric: 41.3826, val_loss: 42.7564, val_MinusLogProbMetric: 42.7564

Epoch 880: val_loss did not improve from 42.67490
196/196 - 30s - loss: 41.3826 - MinusLogProbMetric: 41.3826 - val_loss: 42.7564 - val_MinusLogProbMetric: 42.7564 - lr: 2.0833e-05 - 30s/epoch - 152ms/step
Epoch 881/1000
2023-10-28 01:21:52.497 
Epoch 881/1000 
	 loss: 41.3809, MinusLogProbMetric: 41.3809, val_loss: 42.7863, val_MinusLogProbMetric: 42.7863

Epoch 881: val_loss did not improve from 42.67490
196/196 - 30s - loss: 41.3809 - MinusLogProbMetric: 41.3809 - val_loss: 42.7863 - val_MinusLogProbMetric: 42.7863 - lr: 2.0833e-05 - 30s/epoch - 153ms/step
Epoch 882/1000
2023-10-28 01:22:21.269 
Epoch 882/1000 
	 loss: 41.3676, MinusLogProbMetric: 41.3676, val_loss: 42.7104, val_MinusLogProbMetric: 42.7104

Epoch 882: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3676 - MinusLogProbMetric: 41.3676 - val_loss: 42.7104 - val_MinusLogProbMetric: 42.7104 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 883/1000
2023-10-28 01:22:50.127 
Epoch 883/1000 
	 loss: 41.3727, MinusLogProbMetric: 41.3727, val_loss: 42.7580, val_MinusLogProbMetric: 42.7580

Epoch 883: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3727 - MinusLogProbMetric: 41.3727 - val_loss: 42.7580 - val_MinusLogProbMetric: 42.7580 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 884/1000
2023-10-28 01:23:18.789 
Epoch 884/1000 
	 loss: 41.3832, MinusLogProbMetric: 41.3832, val_loss: 42.7190, val_MinusLogProbMetric: 42.7190

Epoch 884: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3832 - MinusLogProbMetric: 41.3832 - val_loss: 42.7190 - val_MinusLogProbMetric: 42.7190 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 885/1000
2023-10-28 01:23:49.324 
Epoch 885/1000 
	 loss: 41.3681, MinusLogProbMetric: 41.3681, val_loss: 42.8045, val_MinusLogProbMetric: 42.8045

Epoch 885: val_loss did not improve from 42.67490
196/196 - 31s - loss: 41.3681 - MinusLogProbMetric: 41.3681 - val_loss: 42.8045 - val_MinusLogProbMetric: 42.8045 - lr: 2.0833e-05 - 31s/epoch - 156ms/step
Epoch 886/1000
2023-10-28 01:24:20.631 
Epoch 886/1000 
	 loss: 41.3735, MinusLogProbMetric: 41.3735, val_loss: 42.7578, val_MinusLogProbMetric: 42.7578

Epoch 886: val_loss did not improve from 42.67490
196/196 - 31s - loss: 41.3735 - MinusLogProbMetric: 41.3735 - val_loss: 42.7578 - val_MinusLogProbMetric: 42.7578 - lr: 2.0833e-05 - 31s/epoch - 160ms/step
Epoch 887/1000
2023-10-28 01:24:49.921 
Epoch 887/1000 
	 loss: 41.3763, MinusLogProbMetric: 41.3763, val_loss: 42.7892, val_MinusLogProbMetric: 42.7892

Epoch 887: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3763 - MinusLogProbMetric: 41.3763 - val_loss: 42.7892 - val_MinusLogProbMetric: 42.7892 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 888/1000
2023-10-28 01:25:19.035 
Epoch 888/1000 
	 loss: 41.3702, MinusLogProbMetric: 41.3702, val_loss: 42.9123, val_MinusLogProbMetric: 42.9123

Epoch 888: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3702 - MinusLogProbMetric: 41.3702 - val_loss: 42.9123 - val_MinusLogProbMetric: 42.9123 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 889/1000
2023-10-28 01:25:48.004 
Epoch 889/1000 
	 loss: 41.3744, MinusLogProbMetric: 41.3744, val_loss: 42.6751, val_MinusLogProbMetric: 42.6751

Epoch 889: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3744 - MinusLogProbMetric: 41.3744 - val_loss: 42.6751 - val_MinusLogProbMetric: 42.6751 - lr: 2.0833e-05 - 29s/epoch - 148ms/step
Epoch 890/1000
2023-10-28 01:26:16.866 
Epoch 890/1000 
	 loss: 41.3739, MinusLogProbMetric: 41.3739, val_loss: 42.7703, val_MinusLogProbMetric: 42.7703

Epoch 890: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3739 - MinusLogProbMetric: 41.3739 - val_loss: 42.7703 - val_MinusLogProbMetric: 42.7703 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 891/1000
2023-10-28 01:26:48.102 
Epoch 891/1000 
	 loss: 41.3687, MinusLogProbMetric: 41.3687, val_loss: 42.8636, val_MinusLogProbMetric: 42.8636

Epoch 891: val_loss did not improve from 42.67490
196/196 - 31s - loss: 41.3687 - MinusLogProbMetric: 41.3687 - val_loss: 42.8636 - val_MinusLogProbMetric: 42.8636 - lr: 2.0833e-05 - 31s/epoch - 159ms/step
Epoch 892/1000
2023-10-28 01:27:19.624 
Epoch 892/1000 
	 loss: 41.3710, MinusLogProbMetric: 41.3710, val_loss: 42.7257, val_MinusLogProbMetric: 42.7257

Epoch 892: val_loss did not improve from 42.67490
196/196 - 32s - loss: 41.3710 - MinusLogProbMetric: 41.3710 - val_loss: 42.7257 - val_MinusLogProbMetric: 42.7257 - lr: 2.0833e-05 - 32s/epoch - 161ms/step
Epoch 893/1000
2023-10-28 01:27:48.419 
Epoch 893/1000 
	 loss: 41.3772, MinusLogProbMetric: 41.3772, val_loss: 42.7395, val_MinusLogProbMetric: 42.7395

Epoch 893: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3772 - MinusLogProbMetric: 41.3772 - val_loss: 42.7395 - val_MinusLogProbMetric: 42.7395 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 894/1000
2023-10-28 01:28:17.230 
Epoch 894/1000 
	 loss: 41.3751, MinusLogProbMetric: 41.3751, val_loss: 42.9940, val_MinusLogProbMetric: 42.9940

Epoch 894: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3751 - MinusLogProbMetric: 41.3751 - val_loss: 42.9940 - val_MinusLogProbMetric: 42.9940 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 895/1000
2023-10-28 01:28:46.088 
Epoch 895/1000 
	 loss: 41.3893, MinusLogProbMetric: 41.3893, val_loss: 42.7450, val_MinusLogProbMetric: 42.7450

Epoch 895: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3893 - MinusLogProbMetric: 41.3893 - val_loss: 42.7450 - val_MinusLogProbMetric: 42.7450 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 896/1000
2023-10-28 01:29:17.345 
Epoch 896/1000 
	 loss: 41.3831, MinusLogProbMetric: 41.3831, val_loss: 42.8352, val_MinusLogProbMetric: 42.8352

Epoch 896: val_loss did not improve from 42.67490
196/196 - 31s - loss: 41.3831 - MinusLogProbMetric: 41.3831 - val_loss: 42.8352 - val_MinusLogProbMetric: 42.8352 - lr: 2.0833e-05 - 31s/epoch - 159ms/step
Epoch 897/1000
2023-10-28 01:29:47.096 
Epoch 897/1000 
	 loss: 41.3670, MinusLogProbMetric: 41.3670, val_loss: 42.8240, val_MinusLogProbMetric: 42.8240

Epoch 897: val_loss did not improve from 42.67490
196/196 - 30s - loss: 41.3670 - MinusLogProbMetric: 41.3670 - val_loss: 42.8240 - val_MinusLogProbMetric: 42.8240 - lr: 2.0833e-05 - 30s/epoch - 152ms/step
Epoch 898/1000
2023-10-28 01:30:16.601 
Epoch 898/1000 
	 loss: 41.3766, MinusLogProbMetric: 41.3766, val_loss: 42.7746, val_MinusLogProbMetric: 42.7746

Epoch 898: val_loss did not improve from 42.67490
196/196 - 30s - loss: 41.3766 - MinusLogProbMetric: 41.3766 - val_loss: 42.7746 - val_MinusLogProbMetric: 42.7746 - lr: 2.0833e-05 - 30s/epoch - 151ms/step
Epoch 899/1000
2023-10-28 01:30:45.159 
Epoch 899/1000 
	 loss: 41.3788, MinusLogProbMetric: 41.3788, val_loss: 42.7138, val_MinusLogProbMetric: 42.7138

Epoch 899: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3788 - MinusLogProbMetric: 41.3788 - val_loss: 42.7138 - val_MinusLogProbMetric: 42.7138 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 900/1000
2023-10-28 01:31:13.815 
Epoch 900/1000 
	 loss: 41.3706, MinusLogProbMetric: 41.3706, val_loss: 42.8353, val_MinusLogProbMetric: 42.8353

Epoch 900: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3706 - MinusLogProbMetric: 41.3706 - val_loss: 42.8353 - val_MinusLogProbMetric: 42.8353 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 901/1000
2023-10-28 01:31:43.042 
Epoch 901/1000 
	 loss: 41.3692, MinusLogProbMetric: 41.3692, val_loss: 42.8291, val_MinusLogProbMetric: 42.8291

Epoch 901: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3692 - MinusLogProbMetric: 41.3692 - val_loss: 42.8291 - val_MinusLogProbMetric: 42.8291 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 902/1000
2023-10-28 01:32:12.919 
Epoch 902/1000 
	 loss: 41.3670, MinusLogProbMetric: 41.3670, val_loss: 42.8255, val_MinusLogProbMetric: 42.8255

Epoch 902: val_loss did not improve from 42.67490
196/196 - 30s - loss: 41.3670 - MinusLogProbMetric: 41.3670 - val_loss: 42.8255 - val_MinusLogProbMetric: 42.8255 - lr: 2.0833e-05 - 30s/epoch - 152ms/step
Epoch 903/1000
2023-10-28 01:32:43.381 
Epoch 903/1000 
	 loss: 41.3715, MinusLogProbMetric: 41.3715, val_loss: 42.6922, val_MinusLogProbMetric: 42.6922

Epoch 903: val_loss did not improve from 42.67490
196/196 - 30s - loss: 41.3715 - MinusLogProbMetric: 41.3715 - val_loss: 42.6922 - val_MinusLogProbMetric: 42.6922 - lr: 2.0833e-05 - 30s/epoch - 155ms/step
Epoch 904/1000
2023-10-28 01:33:12.183 
Epoch 904/1000 
	 loss: 41.3792, MinusLogProbMetric: 41.3792, val_loss: 42.8475, val_MinusLogProbMetric: 42.8475

Epoch 904: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3792 - MinusLogProbMetric: 41.3792 - val_loss: 42.8475 - val_MinusLogProbMetric: 42.8475 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 905/1000
2023-10-28 01:33:40.735 
Epoch 905/1000 
	 loss: 41.3718, MinusLogProbMetric: 41.3718, val_loss: 42.7035, val_MinusLogProbMetric: 42.7035

Epoch 905: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3718 - MinusLogProbMetric: 41.3718 - val_loss: 42.7035 - val_MinusLogProbMetric: 42.7035 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 906/1000
2023-10-28 01:34:09.393 
Epoch 906/1000 
	 loss: 41.3697, MinusLogProbMetric: 41.3697, val_loss: 42.7544, val_MinusLogProbMetric: 42.7544

Epoch 906: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3697 - MinusLogProbMetric: 41.3697 - val_loss: 42.7544 - val_MinusLogProbMetric: 42.7544 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 907/1000
2023-10-28 01:34:39.614 
Epoch 907/1000 
	 loss: 41.3758, MinusLogProbMetric: 41.3758, val_loss: 42.8542, val_MinusLogProbMetric: 42.8542

Epoch 907: val_loss did not improve from 42.67490
196/196 - 30s - loss: 41.3758 - MinusLogProbMetric: 41.3758 - val_loss: 42.8542 - val_MinusLogProbMetric: 42.8542 - lr: 2.0833e-05 - 30s/epoch - 154ms/step
Epoch 908/1000
2023-10-28 01:35:08.775 
Epoch 908/1000 
	 loss: 41.3717, MinusLogProbMetric: 41.3717, val_loss: 42.7076, val_MinusLogProbMetric: 42.7076

Epoch 908: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3717 - MinusLogProbMetric: 41.3717 - val_loss: 42.7076 - val_MinusLogProbMetric: 42.7076 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 909/1000
2023-10-28 01:35:38.468 
Epoch 909/1000 
	 loss: 41.3622, MinusLogProbMetric: 41.3622, val_loss: 42.8696, val_MinusLogProbMetric: 42.8696

Epoch 909: val_loss did not improve from 42.67490
196/196 - 30s - loss: 41.3622 - MinusLogProbMetric: 41.3622 - val_loss: 42.8696 - val_MinusLogProbMetric: 42.8696 - lr: 2.0833e-05 - 30s/epoch - 151ms/step
Epoch 910/1000
2023-10-28 01:36:07.268 
Epoch 910/1000 
	 loss: 41.3659, MinusLogProbMetric: 41.3659, val_loss: 42.7104, val_MinusLogProbMetric: 42.7104

Epoch 910: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3659 - MinusLogProbMetric: 41.3659 - val_loss: 42.7104 - val_MinusLogProbMetric: 42.7104 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 911/1000
2023-10-28 01:36:36.176 
Epoch 911/1000 
	 loss: 41.3736, MinusLogProbMetric: 41.3736, val_loss: 42.7788, val_MinusLogProbMetric: 42.7788

Epoch 911: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3736 - MinusLogProbMetric: 41.3736 - val_loss: 42.7788 - val_MinusLogProbMetric: 42.7788 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 912/1000
2023-10-28 01:37:05.113 
Epoch 912/1000 
	 loss: 41.3687, MinusLogProbMetric: 41.3687, val_loss: 42.7037, val_MinusLogProbMetric: 42.7037

Epoch 912: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3687 - MinusLogProbMetric: 41.3687 - val_loss: 42.7037 - val_MinusLogProbMetric: 42.7037 - lr: 2.0833e-05 - 29s/epoch - 148ms/step
Epoch 913/1000
2023-10-28 01:37:34.317 
Epoch 913/1000 
	 loss: 41.3840, MinusLogProbMetric: 41.3840, val_loss: 42.8179, val_MinusLogProbMetric: 42.8179

Epoch 913: val_loss did not improve from 42.67490
196/196 - 29s - loss: 41.3840 - MinusLogProbMetric: 41.3840 - val_loss: 42.8179 - val_MinusLogProbMetric: 42.8179 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 914/1000
2023-10-28 01:38:03.810 
Epoch 914/1000 
	 loss: 41.3737, MinusLogProbMetric: 41.3737, val_loss: 42.6729, val_MinusLogProbMetric: 42.6729

Epoch 914: val_loss improved from 42.67490 to 42.67290, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 30s - loss: 41.3737 - MinusLogProbMetric: 41.3737 - val_loss: 42.6729 - val_MinusLogProbMetric: 42.6729 - lr: 2.0833e-05 - 30s/epoch - 153ms/step
Epoch 915/1000
2023-10-28 01:38:32.719 
Epoch 915/1000 
	 loss: 41.3659, MinusLogProbMetric: 41.3659, val_loss: 42.7178, val_MinusLogProbMetric: 42.7178

Epoch 915: val_loss did not improve from 42.67290
196/196 - 28s - loss: 41.3659 - MinusLogProbMetric: 41.3659 - val_loss: 42.7178 - val_MinusLogProbMetric: 42.7178 - lr: 2.0833e-05 - 28s/epoch - 145ms/step
Epoch 916/1000
2023-10-28 01:39:01.412 
Epoch 916/1000 
	 loss: 41.3632, MinusLogProbMetric: 41.3632, val_loss: 42.7866, val_MinusLogProbMetric: 42.7866

Epoch 916: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3632 - MinusLogProbMetric: 41.3632 - val_loss: 42.7866 - val_MinusLogProbMetric: 42.7866 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 917/1000
2023-10-28 01:39:30.007 
Epoch 917/1000 
	 loss: 41.3660, MinusLogProbMetric: 41.3660, val_loss: 42.8495, val_MinusLogProbMetric: 42.8495

Epoch 917: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3660 - MinusLogProbMetric: 41.3660 - val_loss: 42.8495 - val_MinusLogProbMetric: 42.8495 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 918/1000
2023-10-28 01:40:01.288 
Epoch 918/1000 
	 loss: 41.3623, MinusLogProbMetric: 41.3623, val_loss: 42.8650, val_MinusLogProbMetric: 42.8650

Epoch 918: val_loss did not improve from 42.67290
196/196 - 31s - loss: 41.3623 - MinusLogProbMetric: 41.3623 - val_loss: 42.8650 - val_MinusLogProbMetric: 42.8650 - lr: 2.0833e-05 - 31s/epoch - 160ms/step
Epoch 919/1000
2023-10-28 01:40:32.209 
Epoch 919/1000 
	 loss: 41.3712, MinusLogProbMetric: 41.3712, val_loss: 42.6736, val_MinusLogProbMetric: 42.6736

Epoch 919: val_loss did not improve from 42.67290
196/196 - 31s - loss: 41.3712 - MinusLogProbMetric: 41.3712 - val_loss: 42.6736 - val_MinusLogProbMetric: 42.6736 - lr: 2.0833e-05 - 31s/epoch - 158ms/step
Epoch 920/1000
2023-10-28 01:41:02.003 
Epoch 920/1000 
	 loss: 41.3615, MinusLogProbMetric: 41.3615, val_loss: 42.7593, val_MinusLogProbMetric: 42.7593

Epoch 920: val_loss did not improve from 42.67290
196/196 - 30s - loss: 41.3615 - MinusLogProbMetric: 41.3615 - val_loss: 42.7593 - val_MinusLogProbMetric: 42.7593 - lr: 2.0833e-05 - 30s/epoch - 152ms/step
Epoch 921/1000
2023-10-28 01:41:30.679 
Epoch 921/1000 
	 loss: 41.3623, MinusLogProbMetric: 41.3623, val_loss: 42.7976, val_MinusLogProbMetric: 42.7976

Epoch 921: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3623 - MinusLogProbMetric: 41.3623 - val_loss: 42.7976 - val_MinusLogProbMetric: 42.7976 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 922/1000
2023-10-28 01:41:59.677 
Epoch 922/1000 
	 loss: 41.3668, MinusLogProbMetric: 41.3668, val_loss: 42.7449, val_MinusLogProbMetric: 42.7449

Epoch 922: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3668 - MinusLogProbMetric: 41.3668 - val_loss: 42.7449 - val_MinusLogProbMetric: 42.7449 - lr: 2.0833e-05 - 29s/epoch - 148ms/step
Epoch 923/1000
2023-10-28 01:42:28.219 
Epoch 923/1000 
	 loss: 41.3573, MinusLogProbMetric: 41.3573, val_loss: 42.7461, val_MinusLogProbMetric: 42.7461

Epoch 923: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3573 - MinusLogProbMetric: 41.3573 - val_loss: 42.7461 - val_MinusLogProbMetric: 42.7461 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 924/1000
2023-10-28 01:43:00.473 
Epoch 924/1000 
	 loss: 41.3584, MinusLogProbMetric: 41.3584, val_loss: 42.7811, val_MinusLogProbMetric: 42.7811

Epoch 924: val_loss did not improve from 42.67290
196/196 - 32s - loss: 41.3584 - MinusLogProbMetric: 41.3584 - val_loss: 42.7811 - val_MinusLogProbMetric: 42.7811 - lr: 2.0833e-05 - 32s/epoch - 165ms/step
Epoch 925/1000
2023-10-28 01:43:31.417 
Epoch 925/1000 
	 loss: 41.3620, MinusLogProbMetric: 41.3620, val_loss: 42.7610, val_MinusLogProbMetric: 42.7610

Epoch 925: val_loss did not improve from 42.67290
196/196 - 31s - loss: 41.3620 - MinusLogProbMetric: 41.3620 - val_loss: 42.7610 - val_MinusLogProbMetric: 42.7610 - lr: 2.0833e-05 - 31s/epoch - 158ms/step
Epoch 926/1000
2023-10-28 01:44:01.011 
Epoch 926/1000 
	 loss: 41.3603, MinusLogProbMetric: 41.3603, val_loss: 42.7549, val_MinusLogProbMetric: 42.7549

Epoch 926: val_loss did not improve from 42.67290
196/196 - 30s - loss: 41.3603 - MinusLogProbMetric: 41.3603 - val_loss: 42.7549 - val_MinusLogProbMetric: 42.7549 - lr: 2.0833e-05 - 30s/epoch - 151ms/step
Epoch 927/1000
2023-10-28 01:44:29.683 
Epoch 927/1000 
	 loss: 41.3567, MinusLogProbMetric: 41.3567, val_loss: 42.7395, val_MinusLogProbMetric: 42.7395

Epoch 927: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3567 - MinusLogProbMetric: 41.3567 - val_loss: 42.7395 - val_MinusLogProbMetric: 42.7395 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 928/1000
2023-10-28 01:44:58.314 
Epoch 928/1000 
	 loss: 41.3516, MinusLogProbMetric: 41.3516, val_loss: 42.8371, val_MinusLogProbMetric: 42.8371

Epoch 928: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3516 - MinusLogProbMetric: 41.3516 - val_loss: 42.8371 - val_MinusLogProbMetric: 42.8371 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 929/1000
2023-10-28 01:45:28.527 
Epoch 929/1000 
	 loss: 41.3676, MinusLogProbMetric: 41.3676, val_loss: 42.8405, val_MinusLogProbMetric: 42.8405

Epoch 929: val_loss did not improve from 42.67290
196/196 - 30s - loss: 41.3676 - MinusLogProbMetric: 41.3676 - val_loss: 42.8405 - val_MinusLogProbMetric: 42.8405 - lr: 2.0833e-05 - 30s/epoch - 154ms/step
Epoch 930/1000
2023-10-28 01:45:58.688 
Epoch 930/1000 
	 loss: 41.3579, MinusLogProbMetric: 41.3579, val_loss: 42.8288, val_MinusLogProbMetric: 42.8288

Epoch 930: val_loss did not improve from 42.67290
196/196 - 30s - loss: 41.3579 - MinusLogProbMetric: 41.3579 - val_loss: 42.8288 - val_MinusLogProbMetric: 42.8288 - lr: 2.0833e-05 - 30s/epoch - 154ms/step
Epoch 931/1000
2023-10-28 01:46:28.194 
Epoch 931/1000 
	 loss: 41.3625, MinusLogProbMetric: 41.3625, val_loss: 42.9017, val_MinusLogProbMetric: 42.9017

Epoch 931: val_loss did not improve from 42.67290
196/196 - 30s - loss: 41.3625 - MinusLogProbMetric: 41.3625 - val_loss: 42.9017 - val_MinusLogProbMetric: 42.9017 - lr: 2.0833e-05 - 30s/epoch - 151ms/step
Epoch 932/1000
2023-10-28 01:46:57.229 
Epoch 932/1000 
	 loss: 41.3604, MinusLogProbMetric: 41.3604, val_loss: 42.8177, val_MinusLogProbMetric: 42.8177

Epoch 932: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3604 - MinusLogProbMetric: 41.3604 - val_loss: 42.8177 - val_MinusLogProbMetric: 42.8177 - lr: 2.0833e-05 - 29s/epoch - 148ms/step
Epoch 933/1000
2023-10-28 01:47:26.080 
Epoch 933/1000 
	 loss: 41.3599, MinusLogProbMetric: 41.3599, val_loss: 42.8251, val_MinusLogProbMetric: 42.8251

Epoch 933: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3599 - MinusLogProbMetric: 41.3599 - val_loss: 42.8251 - val_MinusLogProbMetric: 42.8251 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 934/1000
2023-10-28 01:47:54.787 
Epoch 934/1000 
	 loss: 41.3626, MinusLogProbMetric: 41.3626, val_loss: 42.7106, val_MinusLogProbMetric: 42.7106

Epoch 934: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3626 - MinusLogProbMetric: 41.3626 - val_loss: 42.7106 - val_MinusLogProbMetric: 42.7106 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 935/1000
2023-10-28 01:48:25.162 
Epoch 935/1000 
	 loss: 41.3591, MinusLogProbMetric: 41.3591, val_loss: 42.8072, val_MinusLogProbMetric: 42.8072

Epoch 935: val_loss did not improve from 42.67290
196/196 - 30s - loss: 41.3591 - MinusLogProbMetric: 41.3591 - val_loss: 42.8072 - val_MinusLogProbMetric: 42.8072 - lr: 2.0833e-05 - 30s/epoch - 155ms/step
Epoch 936/1000
2023-10-28 01:48:56.253 
Epoch 936/1000 
	 loss: 41.3625, MinusLogProbMetric: 41.3625, val_loss: 42.7236, val_MinusLogProbMetric: 42.7236

Epoch 936: val_loss did not improve from 42.67290
196/196 - 31s - loss: 41.3625 - MinusLogProbMetric: 41.3625 - val_loss: 42.7236 - val_MinusLogProbMetric: 42.7236 - lr: 2.0833e-05 - 31s/epoch - 159ms/step
Epoch 937/1000
2023-10-28 01:49:25.461 
Epoch 937/1000 
	 loss: 41.3464, MinusLogProbMetric: 41.3464, val_loss: 42.8652, val_MinusLogProbMetric: 42.8652

Epoch 937: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3464 - MinusLogProbMetric: 41.3464 - val_loss: 42.8652 - val_MinusLogProbMetric: 42.8652 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 938/1000
2023-10-28 01:49:54.197 
Epoch 938/1000 
	 loss: 41.3583, MinusLogProbMetric: 41.3583, val_loss: 42.8810, val_MinusLogProbMetric: 42.8810

Epoch 938: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3583 - MinusLogProbMetric: 41.3583 - val_loss: 42.8810 - val_MinusLogProbMetric: 42.8810 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 939/1000
2023-10-28 01:50:22.741 
Epoch 939/1000 
	 loss: 41.3572, MinusLogProbMetric: 41.3572, val_loss: 42.9000, val_MinusLogProbMetric: 42.9000

Epoch 939: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3572 - MinusLogProbMetric: 41.3572 - val_loss: 42.9000 - val_MinusLogProbMetric: 42.9000 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 940/1000
2023-10-28 01:50:51.766 
Epoch 940/1000 
	 loss: 41.3540, MinusLogProbMetric: 41.3540, val_loss: 42.8438, val_MinusLogProbMetric: 42.8438

Epoch 940: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3540 - MinusLogProbMetric: 41.3540 - val_loss: 42.8438 - val_MinusLogProbMetric: 42.8438 - lr: 2.0833e-05 - 29s/epoch - 148ms/step
Epoch 941/1000
2023-10-28 01:51:22.270 
Epoch 941/1000 
	 loss: 41.3611, MinusLogProbMetric: 41.3611, val_loss: 42.7955, val_MinusLogProbMetric: 42.7955

Epoch 941: val_loss did not improve from 42.67290
196/196 - 31s - loss: 41.3611 - MinusLogProbMetric: 41.3611 - val_loss: 42.7955 - val_MinusLogProbMetric: 42.7955 - lr: 2.0833e-05 - 31s/epoch - 156ms/step
Epoch 942/1000
2023-10-28 01:51:53.051 
Epoch 942/1000 
	 loss: 41.3618, MinusLogProbMetric: 41.3618, val_loss: 42.8723, val_MinusLogProbMetric: 42.8723

Epoch 942: val_loss did not improve from 42.67290
196/196 - 31s - loss: 41.3618 - MinusLogProbMetric: 41.3618 - val_loss: 42.8723 - val_MinusLogProbMetric: 42.8723 - lr: 2.0833e-05 - 31s/epoch - 157ms/step
Epoch 943/1000
2023-10-28 01:52:21.655 
Epoch 943/1000 
	 loss: 41.3639, MinusLogProbMetric: 41.3639, val_loss: 42.9450, val_MinusLogProbMetric: 42.9450

Epoch 943: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3639 - MinusLogProbMetric: 41.3639 - val_loss: 42.9450 - val_MinusLogProbMetric: 42.9450 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 944/1000
2023-10-28 01:52:50.341 
Epoch 944/1000 
	 loss: 41.3504, MinusLogProbMetric: 41.3504, val_loss: 42.8505, val_MinusLogProbMetric: 42.8505

Epoch 944: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3504 - MinusLogProbMetric: 41.3504 - val_loss: 42.8505 - val_MinusLogProbMetric: 42.8505 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 945/1000
2023-10-28 01:53:19.039 
Epoch 945/1000 
	 loss: 41.3582, MinusLogProbMetric: 41.3582, val_loss: 42.7055, val_MinusLogProbMetric: 42.7055

Epoch 945: val_loss did not improve from 42.67290
196/196 - 29s - loss: 41.3582 - MinusLogProbMetric: 41.3582 - val_loss: 42.7055 - val_MinusLogProbMetric: 42.7055 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 946/1000
2023-10-28 01:53:48.922 
Epoch 946/1000 
	 loss: 41.3526, MinusLogProbMetric: 41.3526, val_loss: 42.8073, val_MinusLogProbMetric: 42.8073

Epoch 946: val_loss did not improve from 42.67290
196/196 - 30s - loss: 41.3526 - MinusLogProbMetric: 41.3526 - val_loss: 42.8073 - val_MinusLogProbMetric: 42.8073 - lr: 2.0833e-05 - 30s/epoch - 152ms/step
Epoch 947/1000
2023-10-28 01:54:19.007 
Epoch 947/1000 
	 loss: 41.3641, MinusLogProbMetric: 41.3641, val_loss: 42.8834, val_MinusLogProbMetric: 42.8834

Epoch 947: val_loss did not improve from 42.67290
196/196 - 30s - loss: 41.3641 - MinusLogProbMetric: 41.3641 - val_loss: 42.8834 - val_MinusLogProbMetric: 42.8834 - lr: 2.0833e-05 - 30s/epoch - 153ms/step
Epoch 948/1000
2023-10-28 01:54:49.528 
Epoch 948/1000 
	 loss: 41.3534, MinusLogProbMetric: 41.3534, val_loss: 42.7718, val_MinusLogProbMetric: 42.7718

Epoch 948: val_loss did not improve from 42.67290
196/196 - 31s - loss: 41.3534 - MinusLogProbMetric: 41.3534 - val_loss: 42.7718 - val_MinusLogProbMetric: 42.7718 - lr: 2.0833e-05 - 31s/epoch - 156ms/step
Epoch 949/1000
2023-10-28 01:55:18.026 
Epoch 949/1000 
	 loss: 41.3572, MinusLogProbMetric: 41.3572, val_loss: 42.7406, val_MinusLogProbMetric: 42.7406

Epoch 949: val_loss did not improve from 42.67290
196/196 - 28s - loss: 41.3572 - MinusLogProbMetric: 41.3572 - val_loss: 42.7406 - val_MinusLogProbMetric: 42.7406 - lr: 2.0833e-05 - 28s/epoch - 145ms/step
Epoch 950/1000
2023-10-28 01:55:46.781 
Epoch 950/1000 
	 loss: 41.3559, MinusLogProbMetric: 41.3559, val_loss: 42.6433, val_MinusLogProbMetric: 42.6433

Epoch 950: val_loss improved from 42.67290 to 42.64330, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 29s - loss: 41.3559 - MinusLogProbMetric: 41.3559 - val_loss: 42.6433 - val_MinusLogProbMetric: 42.6433 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 951/1000
2023-10-28 01:56:15.774 
Epoch 951/1000 
	 loss: 41.3484, MinusLogProbMetric: 41.3484, val_loss: 42.7709, val_MinusLogProbMetric: 42.7709

Epoch 951: val_loss did not improve from 42.64330
196/196 - 29s - loss: 41.3484 - MinusLogProbMetric: 41.3484 - val_loss: 42.7709 - val_MinusLogProbMetric: 42.7709 - lr: 2.0833e-05 - 29s/epoch - 146ms/step
Epoch 952/1000
2023-10-28 01:56:45.959 
Epoch 952/1000 
	 loss: 41.3611, MinusLogProbMetric: 41.3611, val_loss: 42.7473, val_MinusLogProbMetric: 42.7473

Epoch 952: val_loss did not improve from 42.64330
196/196 - 30s - loss: 41.3611 - MinusLogProbMetric: 41.3611 - val_loss: 42.7473 - val_MinusLogProbMetric: 42.7473 - lr: 2.0833e-05 - 30s/epoch - 154ms/step
Epoch 953/1000
2023-10-28 01:57:16.376 
Epoch 953/1000 
	 loss: 41.3466, MinusLogProbMetric: 41.3466, val_loss: 42.7635, val_MinusLogProbMetric: 42.7635

Epoch 953: val_loss did not improve from 42.64330
196/196 - 30s - loss: 41.3466 - MinusLogProbMetric: 41.3466 - val_loss: 42.7635 - val_MinusLogProbMetric: 42.7635 - lr: 2.0833e-05 - 30s/epoch - 155ms/step
Epoch 954/1000
2023-10-28 01:57:45.606 
Epoch 954/1000 
	 loss: 41.3528, MinusLogProbMetric: 41.3528, val_loss: 42.6903, val_MinusLogProbMetric: 42.6903

Epoch 954: val_loss did not improve from 42.64330
196/196 - 29s - loss: 41.3528 - MinusLogProbMetric: 41.3528 - val_loss: 42.6903 - val_MinusLogProbMetric: 42.6903 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 955/1000
2023-10-28 01:58:15.912 
Epoch 955/1000 
	 loss: 41.3503, MinusLogProbMetric: 41.3503, val_loss: 42.7668, val_MinusLogProbMetric: 42.7668

Epoch 955: val_loss did not improve from 42.64330
196/196 - 30s - loss: 41.3503 - MinusLogProbMetric: 41.3503 - val_loss: 42.7668 - val_MinusLogProbMetric: 42.7668 - lr: 2.0833e-05 - 30s/epoch - 155ms/step
Epoch 956/1000
2023-10-28 01:58:45.045 
Epoch 956/1000 
	 loss: 41.3410, MinusLogProbMetric: 41.3410, val_loss: 42.7920, val_MinusLogProbMetric: 42.7920

Epoch 956: val_loss did not improve from 42.64330
196/196 - 29s - loss: 41.3410 - MinusLogProbMetric: 41.3410 - val_loss: 42.7920 - val_MinusLogProbMetric: 42.7920 - lr: 2.0833e-05 - 29s/epoch - 149ms/step
Epoch 957/1000
2023-10-28 01:59:15.818 
Epoch 957/1000 
	 loss: 41.3557, MinusLogProbMetric: 41.3557, val_loss: 42.8804, val_MinusLogProbMetric: 42.8804

Epoch 957: val_loss did not improve from 42.64330
196/196 - 31s - loss: 41.3557 - MinusLogProbMetric: 41.3557 - val_loss: 42.8804 - val_MinusLogProbMetric: 42.8804 - lr: 2.0833e-05 - 31s/epoch - 157ms/step
Epoch 958/1000
2023-10-28 01:59:44.670 
Epoch 958/1000 
	 loss: 41.3489, MinusLogProbMetric: 41.3489, val_loss: 42.6895, val_MinusLogProbMetric: 42.6895

Epoch 958: val_loss did not improve from 42.64330
196/196 - 29s - loss: 41.3489 - MinusLogProbMetric: 41.3489 - val_loss: 42.6895 - val_MinusLogProbMetric: 42.6895 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 959/1000
2023-10-28 02:00:15.393 
Epoch 959/1000 
	 loss: 41.3559, MinusLogProbMetric: 41.3559, val_loss: 42.7346, val_MinusLogProbMetric: 42.7346

Epoch 959: val_loss did not improve from 42.64330
196/196 - 31s - loss: 41.3559 - MinusLogProbMetric: 41.3559 - val_loss: 42.7346 - val_MinusLogProbMetric: 42.7346 - lr: 2.0833e-05 - 31s/epoch - 157ms/step
Epoch 960/1000
2023-10-28 02:00:45.118 
Epoch 960/1000 
	 loss: 41.3491, MinusLogProbMetric: 41.3491, val_loss: 42.7554, val_MinusLogProbMetric: 42.7554

Epoch 960: val_loss did not improve from 42.64330
196/196 - 30s - loss: 41.3491 - MinusLogProbMetric: 41.3491 - val_loss: 42.7554 - val_MinusLogProbMetric: 42.7554 - lr: 2.0833e-05 - 30s/epoch - 152ms/step
Epoch 961/1000
2023-10-28 02:01:14.073 
Epoch 961/1000 
	 loss: 41.3664, MinusLogProbMetric: 41.3664, val_loss: 42.7600, val_MinusLogProbMetric: 42.7600

Epoch 961: val_loss did not improve from 42.64330
196/196 - 29s - loss: 41.3664 - MinusLogProbMetric: 41.3664 - val_loss: 42.7600 - val_MinusLogProbMetric: 42.7600 - lr: 2.0833e-05 - 29s/epoch - 148ms/step
Epoch 962/1000
2023-10-28 02:01:45.912 
Epoch 962/1000 
	 loss: 41.3382, MinusLogProbMetric: 41.3382, val_loss: 42.7133, val_MinusLogProbMetric: 42.7133

Epoch 962: val_loss did not improve from 42.64330
196/196 - 32s - loss: 41.3382 - MinusLogProbMetric: 41.3382 - val_loss: 42.7133 - val_MinusLogProbMetric: 42.7133 - lr: 2.0833e-05 - 32s/epoch - 162ms/step
Epoch 963/1000
2023-10-28 02:02:14.789 
Epoch 963/1000 
	 loss: 41.3591, MinusLogProbMetric: 41.3591, val_loss: 42.7931, val_MinusLogProbMetric: 42.7931

Epoch 963: val_loss did not improve from 42.64330
196/196 - 29s - loss: 41.3591 - MinusLogProbMetric: 41.3591 - val_loss: 42.7931 - val_MinusLogProbMetric: 42.7931 - lr: 2.0833e-05 - 29s/epoch - 147ms/step
Epoch 964/1000
2023-10-28 02:02:46.804 
Epoch 964/1000 
	 loss: 41.3568, MinusLogProbMetric: 41.3568, val_loss: 42.6404, val_MinusLogProbMetric: 42.6404

Epoch 964: val_loss improved from 42.64330 to 42.64041, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_426/weights/best_weights.h5
196/196 - 33s - loss: 41.3568 - MinusLogProbMetric: 41.3568 - val_loss: 42.6404 - val_MinusLogProbMetric: 42.6404 - lr: 2.0833e-05 - 33s/epoch - 166ms/step
Epoch 965/1000
2023-10-28 02:03:20.875 
Epoch 965/1000 
	 loss: 41.3678, MinusLogProbMetric: 41.3678, val_loss: 42.7147, val_MinusLogProbMetric: 42.7147

Epoch 965: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3678 - MinusLogProbMetric: 41.3678 - val_loss: 42.7147 - val_MinusLogProbMetric: 42.7147 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 966/1000
2023-10-28 02:03:54.344 
Epoch 966/1000 
	 loss: 41.3456, MinusLogProbMetric: 41.3456, val_loss: 42.8430, val_MinusLogProbMetric: 42.8430

Epoch 966: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3456 - MinusLogProbMetric: 41.3456 - val_loss: 42.8430 - val_MinusLogProbMetric: 42.8430 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 967/1000
2023-10-28 02:04:27.874 
Epoch 967/1000 
	 loss: 41.3534, MinusLogProbMetric: 41.3534, val_loss: 42.7339, val_MinusLogProbMetric: 42.7339

Epoch 967: val_loss did not improve from 42.64041
196/196 - 34s - loss: 41.3534 - MinusLogProbMetric: 41.3534 - val_loss: 42.7339 - val_MinusLogProbMetric: 42.7339 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 968/1000
2023-10-28 02:05:01.340 
Epoch 968/1000 
	 loss: 41.3472, MinusLogProbMetric: 41.3472, val_loss: 42.9401, val_MinusLogProbMetric: 42.9401

Epoch 968: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3472 - MinusLogProbMetric: 41.3472 - val_loss: 42.9401 - val_MinusLogProbMetric: 42.9401 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 969/1000
2023-10-28 02:05:34.792 
Epoch 969/1000 
	 loss: 41.3547, MinusLogProbMetric: 41.3547, val_loss: 42.7591, val_MinusLogProbMetric: 42.7591

Epoch 969: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3547 - MinusLogProbMetric: 41.3547 - val_loss: 42.7591 - val_MinusLogProbMetric: 42.7591 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 970/1000
2023-10-28 02:06:08.367 
Epoch 970/1000 
	 loss: 41.3337, MinusLogProbMetric: 41.3337, val_loss: 42.8046, val_MinusLogProbMetric: 42.8046

Epoch 970: val_loss did not improve from 42.64041
196/196 - 34s - loss: 41.3337 - MinusLogProbMetric: 41.3337 - val_loss: 42.8046 - val_MinusLogProbMetric: 42.8046 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 971/1000
2023-10-28 02:06:41.532 
Epoch 971/1000 
	 loss: 41.3603, MinusLogProbMetric: 41.3603, val_loss: 42.7175, val_MinusLogProbMetric: 42.7175

Epoch 971: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3603 - MinusLogProbMetric: 41.3603 - val_loss: 42.7175 - val_MinusLogProbMetric: 42.7175 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 972/1000
2023-10-28 02:07:14.811 
Epoch 972/1000 
	 loss: 41.3453, MinusLogProbMetric: 41.3453, val_loss: 42.8445, val_MinusLogProbMetric: 42.8445

Epoch 972: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3453 - MinusLogProbMetric: 41.3453 - val_loss: 42.8445 - val_MinusLogProbMetric: 42.8445 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 973/1000
2023-10-28 02:07:48.571 
Epoch 973/1000 
	 loss: 41.3444, MinusLogProbMetric: 41.3444, val_loss: 42.6937, val_MinusLogProbMetric: 42.6937

Epoch 973: val_loss did not improve from 42.64041
196/196 - 34s - loss: 41.3444 - MinusLogProbMetric: 41.3444 - val_loss: 42.6937 - val_MinusLogProbMetric: 42.6937 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 974/1000
2023-10-28 02:08:22.736 
Epoch 974/1000 
	 loss: 41.3505, MinusLogProbMetric: 41.3505, val_loss: 42.8956, val_MinusLogProbMetric: 42.8956

Epoch 974: val_loss did not improve from 42.64041
196/196 - 34s - loss: 41.3505 - MinusLogProbMetric: 41.3505 - val_loss: 42.8956 - val_MinusLogProbMetric: 42.8956 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 975/1000
2023-10-28 02:08:55.979 
Epoch 975/1000 
	 loss: 41.3587, MinusLogProbMetric: 41.3587, val_loss: 42.7792, val_MinusLogProbMetric: 42.7792

Epoch 975: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3587 - MinusLogProbMetric: 41.3587 - val_loss: 42.7792 - val_MinusLogProbMetric: 42.7792 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 976/1000
2023-10-28 02:09:29.749 
Epoch 976/1000 
	 loss: 41.3453, MinusLogProbMetric: 41.3453, val_loss: 42.8770, val_MinusLogProbMetric: 42.8770

Epoch 976: val_loss did not improve from 42.64041
196/196 - 34s - loss: 41.3453 - MinusLogProbMetric: 41.3453 - val_loss: 42.8770 - val_MinusLogProbMetric: 42.8770 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 977/1000
2023-10-28 02:10:02.481 
Epoch 977/1000 
	 loss: 41.3436, MinusLogProbMetric: 41.3436, val_loss: 42.7869, val_MinusLogProbMetric: 42.7869

Epoch 977: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3436 - MinusLogProbMetric: 41.3436 - val_loss: 42.7869 - val_MinusLogProbMetric: 42.7869 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 978/1000
2023-10-28 02:10:35.918 
Epoch 978/1000 
	 loss: 41.3483, MinusLogProbMetric: 41.3483, val_loss: 42.7739, val_MinusLogProbMetric: 42.7739

Epoch 978: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3483 - MinusLogProbMetric: 41.3483 - val_loss: 42.7739 - val_MinusLogProbMetric: 42.7739 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 979/1000
2023-10-28 02:11:08.918 
Epoch 979/1000 
	 loss: 41.3627, MinusLogProbMetric: 41.3627, val_loss: 42.8965, val_MinusLogProbMetric: 42.8965

Epoch 979: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3627 - MinusLogProbMetric: 41.3627 - val_loss: 42.8965 - val_MinusLogProbMetric: 42.8965 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 980/1000
2023-10-28 02:11:40.199 
Epoch 980/1000 
	 loss: 41.3435, MinusLogProbMetric: 41.3435, val_loss: 42.8224, val_MinusLogProbMetric: 42.8224

Epoch 980: val_loss did not improve from 42.64041
196/196 - 31s - loss: 41.3435 - MinusLogProbMetric: 41.3435 - val_loss: 42.8224 - val_MinusLogProbMetric: 42.8224 - lr: 2.0833e-05 - 31s/epoch - 160ms/step
Epoch 981/1000
2023-10-28 02:12:13.620 
Epoch 981/1000 
	 loss: 41.3494, MinusLogProbMetric: 41.3494, val_loss: 42.7964, val_MinusLogProbMetric: 42.7964

Epoch 981: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3494 - MinusLogProbMetric: 41.3494 - val_loss: 42.7964 - val_MinusLogProbMetric: 42.7964 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 982/1000
2023-10-28 02:12:47.387 
Epoch 982/1000 
	 loss: 41.3475, MinusLogProbMetric: 41.3475, val_loss: 42.8885, val_MinusLogProbMetric: 42.8885

Epoch 982: val_loss did not improve from 42.64041
196/196 - 34s - loss: 41.3475 - MinusLogProbMetric: 41.3475 - val_loss: 42.8885 - val_MinusLogProbMetric: 42.8885 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 983/1000
2023-10-28 02:13:20.699 
Epoch 983/1000 
	 loss: 41.3356, MinusLogProbMetric: 41.3356, val_loss: 42.8878, val_MinusLogProbMetric: 42.8878

Epoch 983: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3356 - MinusLogProbMetric: 41.3356 - val_loss: 42.8878 - val_MinusLogProbMetric: 42.8878 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 984/1000
2023-10-28 02:13:54.557 
Epoch 984/1000 
	 loss: 41.3422, MinusLogProbMetric: 41.3422, val_loss: 42.8887, val_MinusLogProbMetric: 42.8887

Epoch 984: val_loss did not improve from 42.64041
196/196 - 34s - loss: 41.3422 - MinusLogProbMetric: 41.3422 - val_loss: 42.8887 - val_MinusLogProbMetric: 42.8887 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 985/1000
2023-10-28 02:14:28.032 
Epoch 985/1000 
	 loss: 41.3444, MinusLogProbMetric: 41.3444, val_loss: 42.6955, val_MinusLogProbMetric: 42.6955

Epoch 985: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3444 - MinusLogProbMetric: 41.3444 - val_loss: 42.6955 - val_MinusLogProbMetric: 42.6955 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 986/1000
2023-10-28 02:15:01.716 
Epoch 986/1000 
	 loss: 41.3358, MinusLogProbMetric: 41.3358, val_loss: 42.8520, val_MinusLogProbMetric: 42.8520

Epoch 986: val_loss did not improve from 42.64041
196/196 - 34s - loss: 41.3358 - MinusLogProbMetric: 41.3358 - val_loss: 42.8520 - val_MinusLogProbMetric: 42.8520 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 987/1000
2023-10-28 02:15:35.236 
Epoch 987/1000 
	 loss: 41.3542, MinusLogProbMetric: 41.3542, val_loss: 42.7961, val_MinusLogProbMetric: 42.7961

Epoch 987: val_loss did not improve from 42.64041
196/196 - 34s - loss: 41.3542 - MinusLogProbMetric: 41.3542 - val_loss: 42.7961 - val_MinusLogProbMetric: 42.7961 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 988/1000
2023-10-28 02:16:08.508 
Epoch 988/1000 
	 loss: 41.3405, MinusLogProbMetric: 41.3405, val_loss: 42.7230, val_MinusLogProbMetric: 42.7230

Epoch 988: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3405 - MinusLogProbMetric: 41.3405 - val_loss: 42.7230 - val_MinusLogProbMetric: 42.7230 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 989/1000
2023-10-28 02:16:41.718 
Epoch 989/1000 
	 loss: 41.3378, MinusLogProbMetric: 41.3378, val_loss: 42.8962, val_MinusLogProbMetric: 42.8962

Epoch 989: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3378 - MinusLogProbMetric: 41.3378 - val_loss: 42.8962 - val_MinusLogProbMetric: 42.8962 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 990/1000
2023-10-28 02:17:14.685 
Epoch 990/1000 
	 loss: 41.3409, MinusLogProbMetric: 41.3409, val_loss: 42.8645, val_MinusLogProbMetric: 42.8645

Epoch 990: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3409 - MinusLogProbMetric: 41.3409 - val_loss: 42.8645 - val_MinusLogProbMetric: 42.8645 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 991/1000
2023-10-28 02:17:46.674 
Epoch 991/1000 
	 loss: 41.3413, MinusLogProbMetric: 41.3413, val_loss: 42.8009, val_MinusLogProbMetric: 42.8009

Epoch 991: val_loss did not improve from 42.64041
196/196 - 32s - loss: 41.3413 - MinusLogProbMetric: 41.3413 - val_loss: 42.8009 - val_MinusLogProbMetric: 42.8009 - lr: 2.0833e-05 - 32s/epoch - 163ms/step
Epoch 992/1000
2023-10-28 02:18:19.625 
Epoch 992/1000 
	 loss: 41.3432, MinusLogProbMetric: 41.3432, val_loss: 42.8069, val_MinusLogProbMetric: 42.8069

Epoch 992: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3432 - MinusLogProbMetric: 41.3432 - val_loss: 42.8069 - val_MinusLogProbMetric: 42.8069 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 993/1000
2023-10-28 02:18:50.616 
Epoch 993/1000 
	 loss: 41.3321, MinusLogProbMetric: 41.3321, val_loss: 42.7067, val_MinusLogProbMetric: 42.7067

Epoch 993: val_loss did not improve from 42.64041
196/196 - 31s - loss: 41.3321 - MinusLogProbMetric: 41.3321 - val_loss: 42.7067 - val_MinusLogProbMetric: 42.7067 - lr: 2.0833e-05 - 31s/epoch - 158ms/step
Epoch 994/1000
2023-10-28 02:19:21.795 
Epoch 994/1000 
	 loss: 41.3470, MinusLogProbMetric: 41.3470, val_loss: 42.7231, val_MinusLogProbMetric: 42.7231

Epoch 994: val_loss did not improve from 42.64041
196/196 - 31s - loss: 41.3470 - MinusLogProbMetric: 41.3470 - val_loss: 42.7231 - val_MinusLogProbMetric: 42.7231 - lr: 2.0833e-05 - 31s/epoch - 159ms/step
Epoch 995/1000
2023-10-28 02:19:55.276 
Epoch 995/1000 
	 loss: 41.3515, MinusLogProbMetric: 41.3515, val_loss: 42.8308, val_MinusLogProbMetric: 42.8308

Epoch 995: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3515 - MinusLogProbMetric: 41.3515 - val_loss: 42.8308 - val_MinusLogProbMetric: 42.8308 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 996/1000
2023-10-28 02:20:28.451 
Epoch 996/1000 
	 loss: 41.3431, MinusLogProbMetric: 41.3431, val_loss: 42.7660, val_MinusLogProbMetric: 42.7660

Epoch 996: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3431 - MinusLogProbMetric: 41.3431 - val_loss: 42.7660 - val_MinusLogProbMetric: 42.7660 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 997/1000
2023-10-28 02:21:02.137 
Epoch 997/1000 
	 loss: 41.3334, MinusLogProbMetric: 41.3334, val_loss: 42.6987, val_MinusLogProbMetric: 42.6987

Epoch 997: val_loss did not improve from 42.64041
196/196 - 34s - loss: 41.3334 - MinusLogProbMetric: 41.3334 - val_loss: 42.6987 - val_MinusLogProbMetric: 42.6987 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 998/1000
2023-10-28 02:21:35.910 
Epoch 998/1000 
	 loss: 41.3372, MinusLogProbMetric: 41.3372, val_loss: 42.7937, val_MinusLogProbMetric: 42.7937

Epoch 998: val_loss did not improve from 42.64041
196/196 - 34s - loss: 41.3372 - MinusLogProbMetric: 41.3372 - val_loss: 42.7937 - val_MinusLogProbMetric: 42.7937 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 999/1000
2023-10-28 02:22:09.425 
Epoch 999/1000 
	 loss: 41.3443, MinusLogProbMetric: 41.3443, val_loss: 42.8220, val_MinusLogProbMetric: 42.8220

Epoch 999: val_loss did not improve from 42.64041
196/196 - 34s - loss: 41.3443 - MinusLogProbMetric: 41.3443 - val_loss: 42.8220 - val_MinusLogProbMetric: 42.8220 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 1000/1000
2023-10-28 02:22:42.913 
Epoch 1000/1000 
	 loss: 41.3331, MinusLogProbMetric: 41.3331, val_loss: 42.6680, val_MinusLogProbMetric: 42.6680

Epoch 1000: val_loss did not improve from 42.64041
196/196 - 33s - loss: 41.3331 - MinusLogProbMetric: 41.3331 - val_loss: 42.6680 - val_MinusLogProbMetric: 42.6680 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 440.
Model trained in 32680.44 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.63 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 0.92 s.
===========
Run 426/720 done in 32751.02 s.
===========

Directory ../../results/CsplineN_new/run_427/ already exists.
Skipping it.
===========
Run 427/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_428/ already exists.
Skipping it.
===========
Run 428/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_429/ already exists.
Skipping it.
===========
Run 429/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_430/ already exists.
Skipping it.
===========
Run 430/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_431/ already exists.
Skipping it.
===========
Run 431/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_432/ already exists.
Skipping it.
===========
Run 432/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_433/ already exists.
Skipping it.
===========
Run 433/720 already exists. Skipping it.
===========

===========
Generating train data for run 434.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_434/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_434/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_434/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_434
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_95"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_96 (InputLayer)       [(None, 100)]             0         
                                                                 
 log_prob_layer_15 (LogProbL  (None,)                  2200950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,200,950
Trainable params: 2,200,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_15/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_15'")
self.model: <keras.engine.functional.Functional object at 0x7f7ac8532380>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f740123d3c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f740123d3c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73b1375a80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73b092b100>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73b092b670>, <keras.callbacks.ModelCheckpoint object at 0x7f73b092b730>, <keras.callbacks.EarlyStopping object at 0x7f73b092b9a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73b092b9d0>, <keras.callbacks.TerminateOnNaN object at 0x7f73b092b610>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_434/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 434/720 with hyperparameters:
timestamp = 2023-10-28 02:22:48.110269
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2200950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 24: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 02:23:44.080 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 6054.5894, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 56s - loss: nan - MinusLogProbMetric: 6054.5894 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 56s/epoch - 285ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 434.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_434/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_434/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_434/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_434
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_101"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_102 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_16 (LogProbL  (None,)                  2200950   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,200,950
Trainable params: 2,200,950
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_16/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_16'")
self.model: <keras.engine.functional.Functional object at 0x7f755c5e7ee0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7658295420>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7658295420>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f75a43f1150>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f75f8323880>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f75f8323df0>, <keras.callbacks.ModelCheckpoint object at 0x7f75f8323eb0>, <keras.callbacks.EarlyStopping object at 0x7f75f8323f70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f75f8323f40>, <keras.callbacks.TerminateOnNaN object at 0x7f75f8323dc0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_434/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 434/720 with hyperparameters:
timestamp = 2023-10-28 02:23:46.731157
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 2200950
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
2023-10-28 02:25:05.755 
Epoch 1/1000 
	 loss: 1462.2646, MinusLogProbMetric: 1462.2646, val_loss: 403.9733, val_MinusLogProbMetric: 403.9733

Epoch 1: val_loss improved from inf to 403.97327, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 79s - loss: 1462.2646 - MinusLogProbMetric: 1462.2646 - val_loss: 403.9733 - val_MinusLogProbMetric: 403.9733 - lr: 3.3333e-04 - 79s/epoch - 405ms/step
Epoch 2/1000
2023-10-28 02:25:38.949 
Epoch 2/1000 
	 loss: 358.2235, MinusLogProbMetric: 358.2235, val_loss: 322.2538, val_MinusLogProbMetric: 322.2538

Epoch 2: val_loss improved from 403.97327 to 322.25381, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 358.2235 - MinusLogProbMetric: 358.2235 - val_loss: 322.2538 - val_MinusLogProbMetric: 322.2538 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 3/1000
2023-10-28 02:26:12.985 
Epoch 3/1000 
	 loss: 224.6069, MinusLogProbMetric: 224.6069, val_loss: 237.3062, val_MinusLogProbMetric: 237.3062

Epoch 3: val_loss improved from 322.25381 to 237.30623, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 224.6069 - MinusLogProbMetric: 224.6069 - val_loss: 237.3062 - val_MinusLogProbMetric: 237.3062 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 4/1000
2023-10-28 02:26:46.920 
Epoch 4/1000 
	 loss: 232.6890, MinusLogProbMetric: 232.6890, val_loss: 182.4191, val_MinusLogProbMetric: 182.4191

Epoch 4: val_loss improved from 237.30623 to 182.41907, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 232.6890 - MinusLogProbMetric: 232.6890 - val_loss: 182.4191 - val_MinusLogProbMetric: 182.4191 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 5/1000
2023-10-28 02:27:20.763 
Epoch 5/1000 
	 loss: 166.7152, MinusLogProbMetric: 166.7152, val_loss: 154.1019, val_MinusLogProbMetric: 154.1019

Epoch 5: val_loss improved from 182.41907 to 154.10191, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 166.7152 - MinusLogProbMetric: 166.7152 - val_loss: 154.1019 - val_MinusLogProbMetric: 154.1019 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 6/1000
2023-10-28 02:27:54.478 
Epoch 6/1000 
	 loss: 163.4652, MinusLogProbMetric: 163.4652, val_loss: 148.1896, val_MinusLogProbMetric: 148.1896

Epoch 6: val_loss improved from 154.10191 to 148.18964, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 163.4652 - MinusLogProbMetric: 163.4652 - val_loss: 148.1896 - val_MinusLogProbMetric: 148.1896 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 7/1000
2023-10-28 02:28:28.424 
Epoch 7/1000 
	 loss: 136.0470, MinusLogProbMetric: 136.0470, val_loss: 130.0911, val_MinusLogProbMetric: 130.0911

Epoch 7: val_loss improved from 148.18964 to 130.09108, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 136.0470 - MinusLogProbMetric: 136.0470 - val_loss: 130.0911 - val_MinusLogProbMetric: 130.0911 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 8/1000
2023-10-28 02:29:02.101 
Epoch 8/1000 
	 loss: 123.7972, MinusLogProbMetric: 123.7972, val_loss: 120.1354, val_MinusLogProbMetric: 120.1354

Epoch 8: val_loss improved from 130.09108 to 120.13535, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 123.7972 - MinusLogProbMetric: 123.7972 - val_loss: 120.1354 - val_MinusLogProbMetric: 120.1354 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 9/1000
2023-10-28 02:29:35.571 
Epoch 9/1000 
	 loss: 116.4009, MinusLogProbMetric: 116.4009, val_loss: 112.5174, val_MinusLogProbMetric: 112.5174

Epoch 9: val_loss improved from 120.13535 to 112.51745, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 116.4009 - MinusLogProbMetric: 116.4009 - val_loss: 112.5174 - val_MinusLogProbMetric: 112.5174 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 10/1000
2023-10-28 02:30:09.348 
Epoch 10/1000 
	 loss: 108.6288, MinusLogProbMetric: 108.6288, val_loss: 108.8315, val_MinusLogProbMetric: 108.8315

Epoch 10: val_loss improved from 112.51745 to 108.83148, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 108.6288 - MinusLogProbMetric: 108.6288 - val_loss: 108.8315 - val_MinusLogProbMetric: 108.8315 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 11/1000
2023-10-28 02:30:42.786 
Epoch 11/1000 
	 loss: 102.9422, MinusLogProbMetric: 102.9422, val_loss: 102.3870, val_MinusLogProbMetric: 102.3870

Epoch 11: val_loss improved from 108.83148 to 102.38700, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 102.9422 - MinusLogProbMetric: 102.9422 - val_loss: 102.3870 - val_MinusLogProbMetric: 102.3870 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 12/1000
2023-10-28 02:31:16.451 
Epoch 12/1000 
	 loss: 103.5356, MinusLogProbMetric: 103.5356, val_loss: 133.0016, val_MinusLogProbMetric: 133.0016

Epoch 12: val_loss did not improve from 102.38700
196/196 - 33s - loss: 103.5356 - MinusLogProbMetric: 103.5356 - val_loss: 133.0016 - val_MinusLogProbMetric: 133.0016 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 13/1000
2023-10-28 02:31:48.168 
Epoch 13/1000 
	 loss: 98.6200, MinusLogProbMetric: 98.6200, val_loss: 93.7251, val_MinusLogProbMetric: 93.7251

Epoch 13: val_loss improved from 102.38700 to 93.72509, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 32s - loss: 98.6200 - MinusLogProbMetric: 98.6200 - val_loss: 93.7251 - val_MinusLogProbMetric: 93.7251 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 14/1000
2023-10-28 02:32:19.602 
Epoch 14/1000 
	 loss: 91.2144, MinusLogProbMetric: 91.2144, val_loss: 90.4242, val_MinusLogProbMetric: 90.4242

Epoch 14: val_loss improved from 93.72509 to 90.42420, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 32s - loss: 91.2144 - MinusLogProbMetric: 91.2144 - val_loss: 90.4242 - val_MinusLogProbMetric: 90.4242 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 15/1000
2023-10-28 02:32:53.313 
Epoch 15/1000 
	 loss: 86.4037, MinusLogProbMetric: 86.4037, val_loss: 84.3714, val_MinusLogProbMetric: 84.3714

Epoch 15: val_loss improved from 90.42420 to 84.37137, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 86.4037 - MinusLogProbMetric: 86.4037 - val_loss: 84.3714 - val_MinusLogProbMetric: 84.3714 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 16/1000
2023-10-28 02:33:26.710 
Epoch 16/1000 
	 loss: 84.8709, MinusLogProbMetric: 84.8709, val_loss: 84.1855, val_MinusLogProbMetric: 84.1855

Epoch 16: val_loss improved from 84.37137 to 84.18549, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 84.8709 - MinusLogProbMetric: 84.8709 - val_loss: 84.1855 - val_MinusLogProbMetric: 84.1855 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 17/1000
2023-10-28 02:33:59.786 
Epoch 17/1000 
	 loss: 81.3818, MinusLogProbMetric: 81.3818, val_loss: 80.7026, val_MinusLogProbMetric: 80.7026

Epoch 17: val_loss improved from 84.18549 to 80.70255, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 81.3818 - MinusLogProbMetric: 81.3818 - val_loss: 80.7026 - val_MinusLogProbMetric: 80.7026 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 18/1000
2023-10-28 02:34:33.296 
Epoch 18/1000 
	 loss: 78.6791, MinusLogProbMetric: 78.6791, val_loss: 78.1452, val_MinusLogProbMetric: 78.1452

Epoch 18: val_loss improved from 80.70255 to 78.14523, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 78.6791 - MinusLogProbMetric: 78.6791 - val_loss: 78.1452 - val_MinusLogProbMetric: 78.1452 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 19/1000
2023-10-28 02:35:07.306 
Epoch 19/1000 
	 loss: 76.8365, MinusLogProbMetric: 76.8365, val_loss: 82.7173, val_MinusLogProbMetric: 82.7173

Epoch 19: val_loss did not improve from 78.14523
196/196 - 33s - loss: 76.8365 - MinusLogProbMetric: 76.8365 - val_loss: 82.7173 - val_MinusLogProbMetric: 82.7173 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 20/1000
2023-10-28 02:35:40.930 
Epoch 20/1000 
	 loss: 75.0995, MinusLogProbMetric: 75.0995, val_loss: 73.7353, val_MinusLogProbMetric: 73.7353

Epoch 20: val_loss improved from 78.14523 to 73.73525, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 75.0995 - MinusLogProbMetric: 75.0995 - val_loss: 73.7353 - val_MinusLogProbMetric: 73.7353 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 21/1000
2023-10-28 02:36:14.738 
Epoch 21/1000 
	 loss: 73.3269, MinusLogProbMetric: 73.3269, val_loss: 73.0213, val_MinusLogProbMetric: 73.0213

Epoch 21: val_loss improved from 73.73525 to 73.02132, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 73.3269 - MinusLogProbMetric: 73.3269 - val_loss: 73.0213 - val_MinusLogProbMetric: 73.0213 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 22/1000
2023-10-28 02:36:48.913 
Epoch 22/1000 
	 loss: 71.7182, MinusLogProbMetric: 71.7182, val_loss: 70.8385, val_MinusLogProbMetric: 70.8385

Epoch 22: val_loss improved from 73.02132 to 70.83854, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 71.7182 - MinusLogProbMetric: 71.7182 - val_loss: 70.8385 - val_MinusLogProbMetric: 70.8385 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 23/1000
2023-10-28 02:37:22.597 
Epoch 23/1000 
	 loss: 70.2252, MinusLogProbMetric: 70.2252, val_loss: 73.8308, val_MinusLogProbMetric: 73.8308

Epoch 23: val_loss did not improve from 70.83854
196/196 - 33s - loss: 70.2252 - MinusLogProbMetric: 70.2252 - val_loss: 73.8308 - val_MinusLogProbMetric: 73.8308 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 24/1000
2023-10-28 02:37:56.326 
Epoch 24/1000 
	 loss: 69.1579, MinusLogProbMetric: 69.1579, val_loss: 71.5409, val_MinusLogProbMetric: 71.5409

Epoch 24: val_loss did not improve from 70.83854
196/196 - 34s - loss: 69.1579 - MinusLogProbMetric: 69.1579 - val_loss: 71.5409 - val_MinusLogProbMetric: 71.5409 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 25/1000
2023-10-28 02:38:30.174 
Epoch 25/1000 
	 loss: 68.0333, MinusLogProbMetric: 68.0333, val_loss: 68.6552, val_MinusLogProbMetric: 68.6552

Epoch 25: val_loss improved from 70.83854 to 68.65520, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 68.0333 - MinusLogProbMetric: 68.0333 - val_loss: 68.6552 - val_MinusLogProbMetric: 68.6552 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 26/1000
2023-10-28 02:39:03.943 
Epoch 26/1000 
	 loss: 67.0937, MinusLogProbMetric: 67.0937, val_loss: 66.6314, val_MinusLogProbMetric: 66.6314

Epoch 26: val_loss improved from 68.65520 to 66.63144, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 67.0937 - MinusLogProbMetric: 67.0937 - val_loss: 66.6314 - val_MinusLogProbMetric: 66.6314 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 27/1000
2023-10-28 02:39:37.418 
Epoch 27/1000 
	 loss: 65.9080, MinusLogProbMetric: 65.9080, val_loss: 65.4744, val_MinusLogProbMetric: 65.4744

Epoch 27: val_loss improved from 66.63144 to 65.47440, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 65.9080 - MinusLogProbMetric: 65.9080 - val_loss: 65.4744 - val_MinusLogProbMetric: 65.4744 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 28/1000
2023-10-28 02:40:10.754 
Epoch 28/1000 
	 loss: 64.8793, MinusLogProbMetric: 64.8793, val_loss: 64.9791, val_MinusLogProbMetric: 64.9791

Epoch 28: val_loss improved from 65.47440 to 64.97906, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 64.8793 - MinusLogProbMetric: 64.8793 - val_loss: 64.9791 - val_MinusLogProbMetric: 64.9791 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 29/1000
2023-10-28 02:40:44.453 
Epoch 29/1000 
	 loss: 64.3133, MinusLogProbMetric: 64.3133, val_loss: 64.5828, val_MinusLogProbMetric: 64.5828

Epoch 29: val_loss improved from 64.97906 to 64.58281, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 64.3133 - MinusLogProbMetric: 64.3133 - val_loss: 64.5828 - val_MinusLogProbMetric: 64.5828 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 30/1000
2023-10-28 02:41:17.990 
Epoch 30/1000 
	 loss: 63.5325, MinusLogProbMetric: 63.5325, val_loss: 63.8485, val_MinusLogProbMetric: 63.8485

Epoch 30: val_loss improved from 64.58281 to 63.84846, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 63.5325 - MinusLogProbMetric: 63.5325 - val_loss: 63.8485 - val_MinusLogProbMetric: 63.8485 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 31/1000
2023-10-28 02:41:51.655 
Epoch 31/1000 
	 loss: 62.8123, MinusLogProbMetric: 62.8123, val_loss: 63.2524, val_MinusLogProbMetric: 63.2524

Epoch 31: val_loss improved from 63.84846 to 63.25236, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 62.8123 - MinusLogProbMetric: 62.8123 - val_loss: 63.2524 - val_MinusLogProbMetric: 63.2524 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 32/1000
2023-10-28 02:42:25.918 
Epoch 32/1000 
	 loss: 62.0789, MinusLogProbMetric: 62.0789, val_loss: 62.6963, val_MinusLogProbMetric: 62.6963

Epoch 32: val_loss improved from 63.25236 to 62.69627, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 62.0789 - MinusLogProbMetric: 62.0789 - val_loss: 62.6963 - val_MinusLogProbMetric: 62.6963 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 33/1000
2023-10-28 02:42:59.552 
Epoch 33/1000 
	 loss: 61.4004, MinusLogProbMetric: 61.4004, val_loss: 61.8263, val_MinusLogProbMetric: 61.8263

Epoch 33: val_loss improved from 62.69627 to 61.82629, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 61.4004 - MinusLogProbMetric: 61.4004 - val_loss: 61.8263 - val_MinusLogProbMetric: 61.8263 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 34/1000
2023-10-28 02:43:33.281 
Epoch 34/1000 
	 loss: 60.9974, MinusLogProbMetric: 60.9974, val_loss: 61.0319, val_MinusLogProbMetric: 61.0319

Epoch 34: val_loss improved from 61.82629 to 61.03194, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 60.9974 - MinusLogProbMetric: 60.9974 - val_loss: 61.0319 - val_MinusLogProbMetric: 61.0319 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 35/1000
2023-10-28 02:44:06.951 
Epoch 35/1000 
	 loss: 60.5337, MinusLogProbMetric: 60.5337, val_loss: 62.5048, val_MinusLogProbMetric: 62.5048

Epoch 35: val_loss did not improve from 61.03194
196/196 - 33s - loss: 60.5337 - MinusLogProbMetric: 60.5337 - val_loss: 62.5048 - val_MinusLogProbMetric: 62.5048 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 36/1000
2023-10-28 02:44:40.013 
Epoch 36/1000 
	 loss: 60.2078, MinusLogProbMetric: 60.2078, val_loss: 60.8298, val_MinusLogProbMetric: 60.8298

Epoch 36: val_loss improved from 61.03194 to 60.82980, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 60.2078 - MinusLogProbMetric: 60.2078 - val_loss: 60.8298 - val_MinusLogProbMetric: 60.8298 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 37/1000
2023-10-28 02:45:14.002 
Epoch 37/1000 
	 loss: 59.6834, MinusLogProbMetric: 59.6834, val_loss: 59.3698, val_MinusLogProbMetric: 59.3698

Epoch 37: val_loss improved from 60.82980 to 59.36977, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 59.6834 - MinusLogProbMetric: 59.6834 - val_loss: 59.3698 - val_MinusLogProbMetric: 59.3698 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 38/1000
2023-10-28 02:45:48.008 
Epoch 38/1000 
	 loss: 59.0277, MinusLogProbMetric: 59.0277, val_loss: 59.6707, val_MinusLogProbMetric: 59.6707

Epoch 38: val_loss did not improve from 59.36977
196/196 - 33s - loss: 59.0277 - MinusLogProbMetric: 59.0277 - val_loss: 59.6707 - val_MinusLogProbMetric: 59.6707 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 39/1000
2023-10-28 02:46:21.243 
Epoch 39/1000 
	 loss: 58.7330, MinusLogProbMetric: 58.7330, val_loss: 59.3610, val_MinusLogProbMetric: 59.3610

Epoch 39: val_loss improved from 59.36977 to 59.36099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 58.7330 - MinusLogProbMetric: 58.7330 - val_loss: 59.3610 - val_MinusLogProbMetric: 59.3610 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 40/1000
2023-10-28 02:46:55.099 
Epoch 40/1000 
	 loss: 58.4242, MinusLogProbMetric: 58.4242, val_loss: 58.5275, val_MinusLogProbMetric: 58.5275

Epoch 40: val_loss improved from 59.36099 to 58.52750, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 58.4242 - MinusLogProbMetric: 58.4242 - val_loss: 58.5275 - val_MinusLogProbMetric: 58.5275 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 41/1000
2023-10-28 02:47:29.114 
Epoch 41/1000 
	 loss: 57.8969, MinusLogProbMetric: 57.8969, val_loss: 57.3600, val_MinusLogProbMetric: 57.3600

Epoch 41: val_loss improved from 58.52750 to 57.36003, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 57.8969 - MinusLogProbMetric: 57.8969 - val_loss: 57.3600 - val_MinusLogProbMetric: 57.3600 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 42/1000
2023-10-28 02:48:03.247 
Epoch 42/1000 
	 loss: 57.5556, MinusLogProbMetric: 57.5556, val_loss: 57.3213, val_MinusLogProbMetric: 57.3213

Epoch 42: val_loss improved from 57.36003 to 57.32128, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 57.5556 - MinusLogProbMetric: 57.5556 - val_loss: 57.3213 - val_MinusLogProbMetric: 57.3213 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 43/1000
2023-10-28 02:48:37.101 
Epoch 43/1000 
	 loss: 57.6420, MinusLogProbMetric: 57.6420, val_loss: 57.4490, val_MinusLogProbMetric: 57.4490

Epoch 43: val_loss did not improve from 57.32128
196/196 - 33s - loss: 57.6420 - MinusLogProbMetric: 57.6420 - val_loss: 57.4490 - val_MinusLogProbMetric: 57.4490 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 44/1000
2023-10-28 02:49:09.656 
Epoch 44/1000 
	 loss: 56.9291, MinusLogProbMetric: 56.9291, val_loss: 56.5645, val_MinusLogProbMetric: 56.5645

Epoch 44: val_loss improved from 57.32128 to 56.56453, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 56.9291 - MinusLogProbMetric: 56.9291 - val_loss: 56.5645 - val_MinusLogProbMetric: 56.5645 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 45/1000
2023-10-28 02:49:43.355 
Epoch 45/1000 
	 loss: 56.6999, MinusLogProbMetric: 56.6999, val_loss: 57.6243, val_MinusLogProbMetric: 57.6243

Epoch 45: val_loss did not improve from 56.56453
196/196 - 33s - loss: 56.6999 - MinusLogProbMetric: 56.6999 - val_loss: 57.6243 - val_MinusLogProbMetric: 57.6243 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 46/1000
2023-10-28 02:50:16.744 
Epoch 46/1000 
	 loss: 56.6428, MinusLogProbMetric: 56.6428, val_loss: 58.2119, val_MinusLogProbMetric: 58.2119

Epoch 46: val_loss did not improve from 56.56453
196/196 - 33s - loss: 56.6428 - MinusLogProbMetric: 56.6428 - val_loss: 58.2119 - val_MinusLogProbMetric: 58.2119 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 47/1000
2023-10-28 02:50:49.716 
Epoch 47/1000 
	 loss: 56.0122, MinusLogProbMetric: 56.0122, val_loss: 57.9450, val_MinusLogProbMetric: 57.9450

Epoch 47: val_loss did not improve from 56.56453
196/196 - 33s - loss: 56.0122 - MinusLogProbMetric: 56.0122 - val_loss: 57.9450 - val_MinusLogProbMetric: 57.9450 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 48/1000
2023-10-28 02:51:22.892 
Epoch 48/1000 
	 loss: 55.8420, MinusLogProbMetric: 55.8420, val_loss: 56.1865, val_MinusLogProbMetric: 56.1865

Epoch 48: val_loss improved from 56.56453 to 56.18648, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 55.8420 - MinusLogProbMetric: 55.8420 - val_loss: 56.1865 - val_MinusLogProbMetric: 56.1865 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 49/1000
2023-10-28 02:51:56.621 
Epoch 49/1000 
	 loss: 55.6747, MinusLogProbMetric: 55.6747, val_loss: 56.0224, val_MinusLogProbMetric: 56.0224

Epoch 49: val_loss improved from 56.18648 to 56.02240, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 55.6747 - MinusLogProbMetric: 55.6747 - val_loss: 56.0224 - val_MinusLogProbMetric: 56.0224 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 50/1000
2023-10-28 02:52:30.359 
Epoch 50/1000 
	 loss: 55.9673, MinusLogProbMetric: 55.9673, val_loss: 57.5380, val_MinusLogProbMetric: 57.5380

Epoch 50: val_loss did not improve from 56.02240
196/196 - 33s - loss: 55.9673 - MinusLogProbMetric: 55.9673 - val_loss: 57.5380 - val_MinusLogProbMetric: 57.5380 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 51/1000
2023-10-28 02:53:03.734 
Epoch 51/1000 
	 loss: 54.9709, MinusLogProbMetric: 54.9709, val_loss: 56.8372, val_MinusLogProbMetric: 56.8372

Epoch 51: val_loss did not improve from 56.02240
196/196 - 33s - loss: 54.9709 - MinusLogProbMetric: 54.9709 - val_loss: 56.8372 - val_MinusLogProbMetric: 56.8372 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 52/1000
2023-10-28 02:53:37.197 
Epoch 52/1000 
	 loss: 55.8108, MinusLogProbMetric: 55.8108, val_loss: 55.7409, val_MinusLogProbMetric: 55.7409

Epoch 52: val_loss improved from 56.02240 to 55.74090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 55.8108 - MinusLogProbMetric: 55.8108 - val_loss: 55.7409 - val_MinusLogProbMetric: 55.7409 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 53/1000
2023-10-28 02:54:10.642 
Epoch 53/1000 
	 loss: 54.5738, MinusLogProbMetric: 54.5738, val_loss: 55.6588, val_MinusLogProbMetric: 55.6588

Epoch 53: val_loss improved from 55.74090 to 55.65882, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 54.5738 - MinusLogProbMetric: 54.5738 - val_loss: 55.6588 - val_MinusLogProbMetric: 55.6588 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 54/1000
2023-10-28 02:54:44.280 
Epoch 54/1000 
	 loss: 54.5082, MinusLogProbMetric: 54.5082, val_loss: 54.2416, val_MinusLogProbMetric: 54.2416

Epoch 54: val_loss improved from 55.65882 to 54.24156, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 54.5082 - MinusLogProbMetric: 54.5082 - val_loss: 54.2416 - val_MinusLogProbMetric: 54.2416 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 55/1000
2023-10-28 02:55:18.303 
Epoch 55/1000 
	 loss: 54.4077, MinusLogProbMetric: 54.4077, val_loss: 56.2299, val_MinusLogProbMetric: 56.2299

Epoch 55: val_loss did not improve from 54.24156
196/196 - 33s - loss: 54.4077 - MinusLogProbMetric: 54.4077 - val_loss: 56.2299 - val_MinusLogProbMetric: 56.2299 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 56/1000
2023-10-28 02:55:51.499 
Epoch 56/1000 
	 loss: 54.4310, MinusLogProbMetric: 54.4310, val_loss: 54.1412, val_MinusLogProbMetric: 54.1412

Epoch 56: val_loss improved from 54.24156 to 54.14125, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 54.4310 - MinusLogProbMetric: 54.4310 - val_loss: 54.1412 - val_MinusLogProbMetric: 54.1412 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 57/1000
2023-10-28 02:56:25.621 
Epoch 57/1000 
	 loss: 54.1393, MinusLogProbMetric: 54.1393, val_loss: 53.4714, val_MinusLogProbMetric: 53.4714

Epoch 57: val_loss improved from 54.14125 to 53.47141, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 54.1393 - MinusLogProbMetric: 54.1393 - val_loss: 53.4714 - val_MinusLogProbMetric: 53.4714 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 58/1000
2023-10-28 02:56:59.683 
Epoch 58/1000 
	 loss: 53.6653, MinusLogProbMetric: 53.6653, val_loss: 54.0029, val_MinusLogProbMetric: 54.0029

Epoch 58: val_loss did not improve from 53.47141
196/196 - 34s - loss: 53.6653 - MinusLogProbMetric: 53.6653 - val_loss: 54.0029 - val_MinusLogProbMetric: 54.0029 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 59/1000
2023-10-28 02:57:32.615 
Epoch 59/1000 
	 loss: 53.8803, MinusLogProbMetric: 53.8803, val_loss: 53.6095, val_MinusLogProbMetric: 53.6095

Epoch 59: val_loss did not improve from 53.47141
196/196 - 33s - loss: 53.8803 - MinusLogProbMetric: 53.8803 - val_loss: 53.6095 - val_MinusLogProbMetric: 53.6095 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 60/1000
2023-10-28 02:58:05.019 
Epoch 60/1000 
	 loss: 53.0814, MinusLogProbMetric: 53.0814, val_loss: 54.8467, val_MinusLogProbMetric: 54.8467

Epoch 60: val_loss did not improve from 53.47141
196/196 - 32s - loss: 53.0814 - MinusLogProbMetric: 53.0814 - val_loss: 54.8467 - val_MinusLogProbMetric: 54.8467 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 61/1000
2023-10-28 02:58:38.213 
Epoch 61/1000 
	 loss: 53.2784, MinusLogProbMetric: 53.2784, val_loss: 54.4309, val_MinusLogProbMetric: 54.4309

Epoch 61: val_loss did not improve from 53.47141
196/196 - 33s - loss: 53.2784 - MinusLogProbMetric: 53.2784 - val_loss: 54.4309 - val_MinusLogProbMetric: 54.4309 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 62/1000
2023-10-28 02:59:11.857 
Epoch 62/1000 
	 loss: 54.0329, MinusLogProbMetric: 54.0329, val_loss: 53.6379, val_MinusLogProbMetric: 53.6379

Epoch 62: val_loss did not improve from 53.47141
196/196 - 34s - loss: 54.0329 - MinusLogProbMetric: 54.0329 - val_loss: 53.6379 - val_MinusLogProbMetric: 53.6379 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 63/1000
2023-10-28 02:59:45.678 
Epoch 63/1000 
	 loss: 53.0400, MinusLogProbMetric: 53.0400, val_loss: 54.2739, val_MinusLogProbMetric: 54.2739

Epoch 63: val_loss did not improve from 53.47141
196/196 - 34s - loss: 53.0400 - MinusLogProbMetric: 53.0400 - val_loss: 54.2739 - val_MinusLogProbMetric: 54.2739 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 64/1000
2023-10-28 03:00:18.809 
Epoch 64/1000 
	 loss: 52.8721, MinusLogProbMetric: 52.8721, val_loss: 52.8621, val_MinusLogProbMetric: 52.8621

Epoch 64: val_loss improved from 53.47141 to 52.86206, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 52.8721 - MinusLogProbMetric: 52.8721 - val_loss: 52.8621 - val_MinusLogProbMetric: 52.8621 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 65/1000
2023-10-28 03:00:52.083 
Epoch 65/1000 
	 loss: 52.5723, MinusLogProbMetric: 52.5723, val_loss: 53.2192, val_MinusLogProbMetric: 53.2192

Epoch 65: val_loss did not improve from 52.86206
196/196 - 33s - loss: 52.5723 - MinusLogProbMetric: 52.5723 - val_loss: 53.2192 - val_MinusLogProbMetric: 53.2192 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 66/1000
2023-10-28 03:01:25.102 
Epoch 66/1000 
	 loss: 52.9006, MinusLogProbMetric: 52.9006, val_loss: 53.7817, val_MinusLogProbMetric: 53.7817

Epoch 66: val_loss did not improve from 52.86206
196/196 - 33s - loss: 52.9006 - MinusLogProbMetric: 52.9006 - val_loss: 53.7817 - val_MinusLogProbMetric: 53.7817 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 67/1000
2023-10-28 03:01:58.314 
Epoch 67/1000 
	 loss: 52.6035, MinusLogProbMetric: 52.6035, val_loss: 52.2973, val_MinusLogProbMetric: 52.2973

Epoch 67: val_loss improved from 52.86206 to 52.29733, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 52.6035 - MinusLogProbMetric: 52.6035 - val_loss: 52.2973 - val_MinusLogProbMetric: 52.2973 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 68/1000
2023-10-28 03:02:31.790 
Epoch 68/1000 
	 loss: 52.3706, MinusLogProbMetric: 52.3706, val_loss: 56.6872, val_MinusLogProbMetric: 56.6872

Epoch 68: val_loss did not improve from 52.29733
196/196 - 33s - loss: 52.3706 - MinusLogProbMetric: 52.3706 - val_loss: 56.6872 - val_MinusLogProbMetric: 56.6872 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 69/1000
2023-10-28 03:03:05.341 
Epoch 69/1000 
	 loss: 52.6156, MinusLogProbMetric: 52.6156, val_loss: 53.9635, val_MinusLogProbMetric: 53.9635

Epoch 69: val_loss did not improve from 52.29733
196/196 - 34s - loss: 52.6156 - MinusLogProbMetric: 52.6156 - val_loss: 53.9635 - val_MinusLogProbMetric: 53.9635 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 70/1000
2023-10-28 03:03:38.506 
Epoch 70/1000 
	 loss: 52.3866, MinusLogProbMetric: 52.3866, val_loss: 52.3767, val_MinusLogProbMetric: 52.3767

Epoch 70: val_loss did not improve from 52.29733
196/196 - 33s - loss: 52.3866 - MinusLogProbMetric: 52.3866 - val_loss: 52.3767 - val_MinusLogProbMetric: 52.3767 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 71/1000
2023-10-28 03:04:11.786 
Epoch 71/1000 
	 loss: 52.2634, MinusLogProbMetric: 52.2634, val_loss: 56.3991, val_MinusLogProbMetric: 56.3991

Epoch 71: val_loss did not improve from 52.29733
196/196 - 33s - loss: 52.2634 - MinusLogProbMetric: 52.2634 - val_loss: 56.3991 - val_MinusLogProbMetric: 56.3991 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 72/1000
2023-10-28 03:04:45.042 
Epoch 72/1000 
	 loss: 53.2440, MinusLogProbMetric: 53.2440, val_loss: 53.4481, val_MinusLogProbMetric: 53.4481

Epoch 72: val_loss did not improve from 52.29733
196/196 - 33s - loss: 53.2440 - MinusLogProbMetric: 53.2440 - val_loss: 53.4481 - val_MinusLogProbMetric: 53.4481 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 73/1000
2023-10-28 03:05:17.916 
Epoch 73/1000 
	 loss: 51.8501, MinusLogProbMetric: 51.8501, val_loss: 52.8750, val_MinusLogProbMetric: 52.8750

Epoch 73: val_loss did not improve from 52.29733
196/196 - 33s - loss: 51.8501 - MinusLogProbMetric: 51.8501 - val_loss: 52.8750 - val_MinusLogProbMetric: 52.8750 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 74/1000
2023-10-28 03:05:51.277 
Epoch 74/1000 
	 loss: 51.7321, MinusLogProbMetric: 51.7321, val_loss: 52.1548, val_MinusLogProbMetric: 52.1548

Epoch 74: val_loss improved from 52.29733 to 52.15482, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 51.7321 - MinusLogProbMetric: 51.7321 - val_loss: 52.1548 - val_MinusLogProbMetric: 52.1548 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 75/1000
2023-10-28 03:06:24.949 
Epoch 75/1000 
	 loss: 51.7879, MinusLogProbMetric: 51.7879, val_loss: 51.4555, val_MinusLogProbMetric: 51.4555

Epoch 75: val_loss improved from 52.15482 to 51.45552, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 51.7879 - MinusLogProbMetric: 51.7879 - val_loss: 51.4555 - val_MinusLogProbMetric: 51.4555 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 76/1000
2023-10-28 03:06:58.647 
Epoch 76/1000 
	 loss: 51.6123, MinusLogProbMetric: 51.6123, val_loss: 54.1073, val_MinusLogProbMetric: 54.1073

Epoch 76: val_loss did not improve from 51.45552
196/196 - 33s - loss: 51.6123 - MinusLogProbMetric: 51.6123 - val_loss: 54.1073 - val_MinusLogProbMetric: 54.1073 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 77/1000
2023-10-28 03:07:31.754 
Epoch 77/1000 
	 loss: 51.8700, MinusLogProbMetric: 51.8700, val_loss: 52.1403, val_MinusLogProbMetric: 52.1403

Epoch 77: val_loss did not improve from 51.45552
196/196 - 33s - loss: 51.8700 - MinusLogProbMetric: 51.8700 - val_loss: 52.1403 - val_MinusLogProbMetric: 52.1403 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 78/1000
2023-10-28 03:08:05.275 
Epoch 78/1000 
	 loss: 51.3399, MinusLogProbMetric: 51.3399, val_loss: 52.4754, val_MinusLogProbMetric: 52.4754

Epoch 78: val_loss did not improve from 51.45552
196/196 - 34s - loss: 51.3399 - MinusLogProbMetric: 51.3399 - val_loss: 52.4754 - val_MinusLogProbMetric: 52.4754 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 79/1000
2023-10-28 03:08:38.792 
Epoch 79/1000 
	 loss: 51.1204, MinusLogProbMetric: 51.1204, val_loss: 51.2363, val_MinusLogProbMetric: 51.2363

Epoch 79: val_loss improved from 51.45552 to 51.23633, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 51.1204 - MinusLogProbMetric: 51.1204 - val_loss: 51.2363 - val_MinusLogProbMetric: 51.2363 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 80/1000
2023-10-28 03:09:12.346 
Epoch 80/1000 
	 loss: 51.6114, MinusLogProbMetric: 51.6114, val_loss: 51.9851, val_MinusLogProbMetric: 51.9851

Epoch 80: val_loss did not improve from 51.23633
196/196 - 33s - loss: 51.6114 - MinusLogProbMetric: 51.6114 - val_loss: 51.9851 - val_MinusLogProbMetric: 51.9851 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 81/1000
2023-10-28 03:09:45.583 
Epoch 81/1000 
	 loss: 50.9574, MinusLogProbMetric: 50.9574, val_loss: 51.1483, val_MinusLogProbMetric: 51.1483

Epoch 81: val_loss improved from 51.23633 to 51.14827, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 50.9574 - MinusLogProbMetric: 50.9574 - val_loss: 51.1483 - val_MinusLogProbMetric: 51.1483 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 82/1000
2023-10-28 03:10:19.613 
Epoch 82/1000 
	 loss: 51.4449, MinusLogProbMetric: 51.4449, val_loss: 64.7430, val_MinusLogProbMetric: 64.7430

Epoch 82: val_loss did not improve from 51.14827
196/196 - 33s - loss: 51.4449 - MinusLogProbMetric: 51.4449 - val_loss: 64.7430 - val_MinusLogProbMetric: 64.7430 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 83/1000
2023-10-28 03:10:53.033 
Epoch 83/1000 
	 loss: 53.4897, MinusLogProbMetric: 53.4897, val_loss: 52.1805, val_MinusLogProbMetric: 52.1805

Epoch 83: val_loss did not improve from 51.14827
196/196 - 33s - loss: 53.4897 - MinusLogProbMetric: 53.4897 - val_loss: 52.1805 - val_MinusLogProbMetric: 52.1805 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 84/1000
2023-10-28 03:11:26.054 
Epoch 84/1000 
	 loss: 50.8899, MinusLogProbMetric: 50.8899, val_loss: 51.1485, val_MinusLogProbMetric: 51.1485

Epoch 84: val_loss did not improve from 51.14827
196/196 - 33s - loss: 50.8899 - MinusLogProbMetric: 50.8899 - val_loss: 51.1485 - val_MinusLogProbMetric: 51.1485 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 85/1000
2023-10-28 03:11:59.179 
Epoch 85/1000 
	 loss: 51.1506, MinusLogProbMetric: 51.1506, val_loss: 51.4753, val_MinusLogProbMetric: 51.4753

Epoch 85: val_loss did not improve from 51.14827
196/196 - 33s - loss: 51.1506 - MinusLogProbMetric: 51.1506 - val_loss: 51.4753 - val_MinusLogProbMetric: 51.4753 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 86/1000
2023-10-28 03:12:32.223 
Epoch 86/1000 
	 loss: 51.0604, MinusLogProbMetric: 51.0604, val_loss: 50.7506, val_MinusLogProbMetric: 50.7506

Epoch 86: val_loss improved from 51.14827 to 50.75064, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 51.0604 - MinusLogProbMetric: 51.0604 - val_loss: 50.7506 - val_MinusLogProbMetric: 50.7506 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 87/1000
2023-10-28 03:13:06.156 
Epoch 87/1000 
	 loss: 50.4282, MinusLogProbMetric: 50.4282, val_loss: 50.7632, val_MinusLogProbMetric: 50.7632

Epoch 87: val_loss did not improve from 50.75064
196/196 - 33s - loss: 50.4282 - MinusLogProbMetric: 50.4282 - val_loss: 50.7632 - val_MinusLogProbMetric: 50.7632 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 88/1000
2023-10-28 03:13:39.471 
Epoch 88/1000 
	 loss: 50.5791, MinusLogProbMetric: 50.5791, val_loss: 50.6268, val_MinusLogProbMetric: 50.6268

Epoch 88: val_loss improved from 50.75064 to 50.62684, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 50.5791 - MinusLogProbMetric: 50.5791 - val_loss: 50.6268 - val_MinusLogProbMetric: 50.6268 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 89/1000
2023-10-28 03:14:13.609 
Epoch 89/1000 
	 loss: 50.4787, MinusLogProbMetric: 50.4787, val_loss: 51.7365, val_MinusLogProbMetric: 51.7365

Epoch 89: val_loss did not improve from 50.62684
196/196 - 34s - loss: 50.4787 - MinusLogProbMetric: 50.4787 - val_loss: 51.7365 - val_MinusLogProbMetric: 51.7365 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 90/1000
2023-10-28 03:14:47.128 
Epoch 90/1000 
	 loss: 50.5739, MinusLogProbMetric: 50.5739, val_loss: 51.1741, val_MinusLogProbMetric: 51.1741

Epoch 90: val_loss did not improve from 50.62684
196/196 - 34s - loss: 50.5739 - MinusLogProbMetric: 50.5739 - val_loss: 51.1741 - val_MinusLogProbMetric: 51.1741 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 91/1000
2023-10-28 03:15:20.293 
Epoch 91/1000 
	 loss: 50.4637, MinusLogProbMetric: 50.4637, val_loss: 50.8089, val_MinusLogProbMetric: 50.8089

Epoch 91: val_loss did not improve from 50.62684
196/196 - 33s - loss: 50.4637 - MinusLogProbMetric: 50.4637 - val_loss: 50.8089 - val_MinusLogProbMetric: 50.8089 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 92/1000
2023-10-28 03:15:53.777 
Epoch 92/1000 
	 loss: 50.3767, MinusLogProbMetric: 50.3767, val_loss: 49.7049, val_MinusLogProbMetric: 49.7049

Epoch 92: val_loss improved from 50.62684 to 49.70490, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 50.3767 - MinusLogProbMetric: 50.3767 - val_loss: 49.7049 - val_MinusLogProbMetric: 49.7049 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 93/1000
2023-10-28 03:16:27.918 
Epoch 93/1000 
	 loss: 50.0823, MinusLogProbMetric: 50.0823, val_loss: 50.1134, val_MinusLogProbMetric: 50.1134

Epoch 93: val_loss did not improve from 49.70490
196/196 - 34s - loss: 50.0823 - MinusLogProbMetric: 50.0823 - val_loss: 50.1134 - val_MinusLogProbMetric: 50.1134 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 94/1000
2023-10-28 03:17:01.128 
Epoch 94/1000 
	 loss: 50.0349, MinusLogProbMetric: 50.0349, val_loss: 50.6779, val_MinusLogProbMetric: 50.6779

Epoch 94: val_loss did not improve from 49.70490
196/196 - 33s - loss: 50.0349 - MinusLogProbMetric: 50.0349 - val_loss: 50.6779 - val_MinusLogProbMetric: 50.6779 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 95/1000
2023-10-28 03:17:34.250 
Epoch 95/1000 
	 loss: 50.2469, MinusLogProbMetric: 50.2469, val_loss: 49.9383, val_MinusLogProbMetric: 49.9383

Epoch 95: val_loss did not improve from 49.70490
196/196 - 33s - loss: 50.2469 - MinusLogProbMetric: 50.2469 - val_loss: 49.9383 - val_MinusLogProbMetric: 49.9383 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 96/1000
2023-10-28 03:18:07.643 
Epoch 96/1000 
	 loss: 49.9128, MinusLogProbMetric: 49.9128, val_loss: 50.6492, val_MinusLogProbMetric: 50.6492

Epoch 96: val_loss did not improve from 49.70490
196/196 - 33s - loss: 49.9128 - MinusLogProbMetric: 49.9128 - val_loss: 50.6492 - val_MinusLogProbMetric: 50.6492 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 97/1000
2023-10-28 03:18:41.145 
Epoch 97/1000 
	 loss: 49.8834, MinusLogProbMetric: 49.8834, val_loss: 50.3927, val_MinusLogProbMetric: 50.3927

Epoch 97: val_loss did not improve from 49.70490
196/196 - 33s - loss: 49.8834 - MinusLogProbMetric: 49.8834 - val_loss: 50.3927 - val_MinusLogProbMetric: 50.3927 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 98/1000
2023-10-28 03:19:14.718 
Epoch 98/1000 
	 loss: 50.0817, MinusLogProbMetric: 50.0817, val_loss: 50.2919, val_MinusLogProbMetric: 50.2919

Epoch 98: val_loss did not improve from 49.70490
196/196 - 34s - loss: 50.0817 - MinusLogProbMetric: 50.0817 - val_loss: 50.2919 - val_MinusLogProbMetric: 50.2919 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 99/1000
2023-10-28 03:19:48.170 
Epoch 99/1000 
	 loss: 49.6573, MinusLogProbMetric: 49.6573, val_loss: 51.0725, val_MinusLogProbMetric: 51.0725

Epoch 99: val_loss did not improve from 49.70490
196/196 - 33s - loss: 49.6573 - MinusLogProbMetric: 49.6573 - val_loss: 51.0725 - val_MinusLogProbMetric: 51.0725 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 100/1000
2023-10-28 03:20:21.571 
Epoch 100/1000 
	 loss: 49.4803, MinusLogProbMetric: 49.4803, val_loss: 49.6510, val_MinusLogProbMetric: 49.6510

Epoch 100: val_loss improved from 49.70490 to 49.65102, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 49.4803 - MinusLogProbMetric: 49.4803 - val_loss: 49.6510 - val_MinusLogProbMetric: 49.6510 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 101/1000
2023-10-28 03:20:55.389 
Epoch 101/1000 
	 loss: 50.0264, MinusLogProbMetric: 50.0264, val_loss: 51.7400, val_MinusLogProbMetric: 51.7400

Epoch 101: val_loss did not improve from 49.65102
196/196 - 33s - loss: 50.0264 - MinusLogProbMetric: 50.0264 - val_loss: 51.7400 - val_MinusLogProbMetric: 51.7400 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 102/1000
2023-10-28 03:21:28.541 
Epoch 102/1000 
	 loss: 49.5357, MinusLogProbMetric: 49.5357, val_loss: 49.9157, val_MinusLogProbMetric: 49.9157

Epoch 102: val_loss did not improve from 49.65102
196/196 - 33s - loss: 49.5357 - MinusLogProbMetric: 49.5357 - val_loss: 49.9157 - val_MinusLogProbMetric: 49.9157 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 103/1000
2023-10-28 03:22:01.541 
Epoch 103/1000 
	 loss: 49.7222, MinusLogProbMetric: 49.7222, val_loss: 50.9878, val_MinusLogProbMetric: 50.9878

Epoch 103: val_loss did not improve from 49.65102
196/196 - 33s - loss: 49.7222 - MinusLogProbMetric: 49.7222 - val_loss: 50.9878 - val_MinusLogProbMetric: 50.9878 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 104/1000
2023-10-28 03:22:34.557 
Epoch 104/1000 
	 loss: 49.2542, MinusLogProbMetric: 49.2542, val_loss: 51.4270, val_MinusLogProbMetric: 51.4270

Epoch 104: val_loss did not improve from 49.65102
196/196 - 33s - loss: 49.2542 - MinusLogProbMetric: 49.2542 - val_loss: 51.4270 - val_MinusLogProbMetric: 51.4270 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 105/1000
2023-10-28 03:23:07.773 
Epoch 105/1000 
	 loss: 49.5328, MinusLogProbMetric: 49.5328, val_loss: 48.7742, val_MinusLogProbMetric: 48.7742

Epoch 105: val_loss improved from 49.65102 to 48.77417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 49.5328 - MinusLogProbMetric: 49.5328 - val_loss: 48.7742 - val_MinusLogProbMetric: 48.7742 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 106/1000
2023-10-28 03:23:41.501 
Epoch 106/1000 
	 loss: 49.3442, MinusLogProbMetric: 49.3442, val_loss: 48.9121, val_MinusLogProbMetric: 48.9121

Epoch 106: val_loss did not improve from 48.77417
196/196 - 33s - loss: 49.3442 - MinusLogProbMetric: 49.3442 - val_loss: 48.9121 - val_MinusLogProbMetric: 48.9121 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 107/1000
2023-10-28 03:24:14.568 
Epoch 107/1000 
	 loss: 49.3619, MinusLogProbMetric: 49.3619, val_loss: 49.2071, val_MinusLogProbMetric: 49.2071

Epoch 107: val_loss did not improve from 48.77417
196/196 - 33s - loss: 49.3619 - MinusLogProbMetric: 49.3619 - val_loss: 49.2071 - val_MinusLogProbMetric: 49.2071 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 108/1000
2023-10-28 03:24:47.477 
Epoch 108/1000 
	 loss: 49.6906, MinusLogProbMetric: 49.6906, val_loss: 50.3722, val_MinusLogProbMetric: 50.3722

Epoch 108: val_loss did not improve from 48.77417
196/196 - 33s - loss: 49.6906 - MinusLogProbMetric: 49.6906 - val_loss: 50.3722 - val_MinusLogProbMetric: 50.3722 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 109/1000
2023-10-28 03:25:20.919 
Epoch 109/1000 
	 loss: 49.1022, MinusLogProbMetric: 49.1022, val_loss: 49.4448, val_MinusLogProbMetric: 49.4448

Epoch 109: val_loss did not improve from 48.77417
196/196 - 33s - loss: 49.1022 - MinusLogProbMetric: 49.1022 - val_loss: 49.4448 - val_MinusLogProbMetric: 49.4448 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 110/1000
2023-10-28 03:25:54.438 
Epoch 110/1000 
	 loss: 49.0625, MinusLogProbMetric: 49.0625, val_loss: 49.8828, val_MinusLogProbMetric: 49.8828

Epoch 110: val_loss did not improve from 48.77417
196/196 - 34s - loss: 49.0625 - MinusLogProbMetric: 49.0625 - val_loss: 49.8828 - val_MinusLogProbMetric: 49.8828 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 111/1000
2023-10-28 03:26:27.663 
Epoch 111/1000 
	 loss: 49.5599, MinusLogProbMetric: 49.5599, val_loss: 48.5802, val_MinusLogProbMetric: 48.5802

Epoch 111: val_loss improved from 48.77417 to 48.58015, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 49.5599 - MinusLogProbMetric: 49.5599 - val_loss: 48.5802 - val_MinusLogProbMetric: 48.5802 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 112/1000
2023-10-28 03:27:01.651 
Epoch 112/1000 
	 loss: 48.8815, MinusLogProbMetric: 48.8815, val_loss: 48.9357, val_MinusLogProbMetric: 48.9357

Epoch 112: val_loss did not improve from 48.58015
196/196 - 33s - loss: 48.8815 - MinusLogProbMetric: 48.8815 - val_loss: 48.9357 - val_MinusLogProbMetric: 48.9357 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 113/1000
2023-10-28 03:27:35.030 
Epoch 113/1000 
	 loss: 48.9386, MinusLogProbMetric: 48.9386, val_loss: 48.8206, val_MinusLogProbMetric: 48.8206

Epoch 113: val_loss did not improve from 48.58015
196/196 - 33s - loss: 48.9386 - MinusLogProbMetric: 48.9386 - val_loss: 48.8206 - val_MinusLogProbMetric: 48.8206 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 114/1000
2023-10-28 03:28:08.301 
Epoch 114/1000 
	 loss: 49.3432, MinusLogProbMetric: 49.3432, val_loss: 50.9063, val_MinusLogProbMetric: 50.9063

Epoch 114: val_loss did not improve from 48.58015
196/196 - 33s - loss: 49.3432 - MinusLogProbMetric: 49.3432 - val_loss: 50.9063 - val_MinusLogProbMetric: 50.9063 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 115/1000
2023-10-28 03:28:42.168 
Epoch 115/1000 
	 loss: 49.1587, MinusLogProbMetric: 49.1587, val_loss: 50.4227, val_MinusLogProbMetric: 50.4227

Epoch 115: val_loss did not improve from 48.58015
196/196 - 34s - loss: 49.1587 - MinusLogProbMetric: 49.1587 - val_loss: 50.4227 - val_MinusLogProbMetric: 50.4227 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 116/1000
2023-10-28 03:29:15.531 
Epoch 116/1000 
	 loss: 48.7585, MinusLogProbMetric: 48.7585, val_loss: 48.8981, val_MinusLogProbMetric: 48.8981

Epoch 116: val_loss did not improve from 48.58015
196/196 - 33s - loss: 48.7585 - MinusLogProbMetric: 48.7585 - val_loss: 48.8981 - val_MinusLogProbMetric: 48.8981 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 117/1000
2023-10-28 03:29:49.006 
Epoch 117/1000 
	 loss: 49.0004, MinusLogProbMetric: 49.0004, val_loss: 49.8589, val_MinusLogProbMetric: 49.8589

Epoch 117: val_loss did not improve from 48.58015
196/196 - 33s - loss: 49.0004 - MinusLogProbMetric: 49.0004 - val_loss: 49.8589 - val_MinusLogProbMetric: 49.8589 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 118/1000
2023-10-28 03:30:22.286 
Epoch 118/1000 
	 loss: 48.9148, MinusLogProbMetric: 48.9148, val_loss: 50.0382, val_MinusLogProbMetric: 50.0382

Epoch 118: val_loss did not improve from 48.58015
196/196 - 33s - loss: 48.9148 - MinusLogProbMetric: 48.9148 - val_loss: 50.0382 - val_MinusLogProbMetric: 50.0382 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 119/1000
2023-10-28 03:30:55.704 
Epoch 119/1000 
	 loss: 48.6827, MinusLogProbMetric: 48.6827, val_loss: 49.9653, val_MinusLogProbMetric: 49.9653

Epoch 119: val_loss did not improve from 48.58015
196/196 - 33s - loss: 48.6827 - MinusLogProbMetric: 48.6827 - val_loss: 49.9653 - val_MinusLogProbMetric: 49.9653 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 120/1000
2023-10-28 03:31:28.766 
Epoch 120/1000 
	 loss: 48.7793, MinusLogProbMetric: 48.7793, val_loss: 50.2026, val_MinusLogProbMetric: 50.2026

Epoch 120: val_loss did not improve from 48.58015
196/196 - 33s - loss: 48.7793 - MinusLogProbMetric: 48.7793 - val_loss: 50.2026 - val_MinusLogProbMetric: 50.2026 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 121/1000
2023-10-28 03:32:01.848 
Epoch 121/1000 
	 loss: 49.0717, MinusLogProbMetric: 49.0717, val_loss: 49.3729, val_MinusLogProbMetric: 49.3729

Epoch 121: val_loss did not improve from 48.58015
196/196 - 33s - loss: 49.0717 - MinusLogProbMetric: 49.0717 - val_loss: 49.3729 - val_MinusLogProbMetric: 49.3729 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 122/1000
2023-10-28 03:32:35.260 
Epoch 122/1000 
	 loss: 48.4196, MinusLogProbMetric: 48.4196, val_loss: 48.1692, val_MinusLogProbMetric: 48.1692

Epoch 122: val_loss improved from 48.58015 to 48.16924, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 48.4196 - MinusLogProbMetric: 48.4196 - val_loss: 48.1692 - val_MinusLogProbMetric: 48.1692 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 123/1000
2023-10-28 03:33:08.420 
Epoch 123/1000 
	 loss: 48.5234, MinusLogProbMetric: 48.5234, val_loss: 49.5116, val_MinusLogProbMetric: 49.5116

Epoch 123: val_loss did not improve from 48.16924
196/196 - 33s - loss: 48.5234 - MinusLogProbMetric: 48.5234 - val_loss: 49.5116 - val_MinusLogProbMetric: 49.5116 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 124/1000
2023-10-28 03:33:40.486 
Epoch 124/1000 
	 loss: 48.1627, MinusLogProbMetric: 48.1627, val_loss: 49.7498, val_MinusLogProbMetric: 49.7498

Epoch 124: val_loss did not improve from 48.16924
196/196 - 32s - loss: 48.1627 - MinusLogProbMetric: 48.1627 - val_loss: 49.7498 - val_MinusLogProbMetric: 49.7498 - lr: 3.3333e-04 - 32s/epoch - 164ms/step
Epoch 125/1000
2023-10-28 03:34:14.038 
Epoch 125/1000 
	 loss: 48.6251, MinusLogProbMetric: 48.6251, val_loss: 50.7354, val_MinusLogProbMetric: 50.7354

Epoch 125: val_loss did not improve from 48.16924
196/196 - 34s - loss: 48.6251 - MinusLogProbMetric: 48.6251 - val_loss: 50.7354 - val_MinusLogProbMetric: 50.7354 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 126/1000
2023-10-28 03:34:47.730 
Epoch 126/1000 
	 loss: 48.6548, MinusLogProbMetric: 48.6548, val_loss: 50.3140, val_MinusLogProbMetric: 50.3140

Epoch 126: val_loss did not improve from 48.16924
196/196 - 34s - loss: 48.6548 - MinusLogProbMetric: 48.6548 - val_loss: 50.3140 - val_MinusLogProbMetric: 50.3140 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 127/1000
2023-10-28 03:35:21.499 
Epoch 127/1000 
	 loss: 48.1678, MinusLogProbMetric: 48.1678, val_loss: 49.4287, val_MinusLogProbMetric: 49.4287

Epoch 127: val_loss did not improve from 48.16924
196/196 - 34s - loss: 48.1678 - MinusLogProbMetric: 48.1678 - val_loss: 49.4287 - val_MinusLogProbMetric: 49.4287 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 128/1000
2023-10-28 03:35:54.459 
Epoch 128/1000 
	 loss: 48.1693, MinusLogProbMetric: 48.1693, val_loss: 49.2161, val_MinusLogProbMetric: 49.2161

Epoch 128: val_loss did not improve from 48.16924
196/196 - 33s - loss: 48.1693 - MinusLogProbMetric: 48.1693 - val_loss: 49.2161 - val_MinusLogProbMetric: 49.2161 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 129/1000
2023-10-28 03:36:27.663 
Epoch 129/1000 
	 loss: 48.4260, MinusLogProbMetric: 48.4260, val_loss: 49.5139, val_MinusLogProbMetric: 49.5139

Epoch 129: val_loss did not improve from 48.16924
196/196 - 33s - loss: 48.4260 - MinusLogProbMetric: 48.4260 - val_loss: 49.5139 - val_MinusLogProbMetric: 49.5139 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 130/1000
2023-10-28 03:37:00.778 
Epoch 130/1000 
	 loss: 48.1516, MinusLogProbMetric: 48.1516, val_loss: 49.1367, val_MinusLogProbMetric: 49.1367

Epoch 130: val_loss did not improve from 48.16924
196/196 - 33s - loss: 48.1516 - MinusLogProbMetric: 48.1516 - val_loss: 49.1367 - val_MinusLogProbMetric: 49.1367 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 131/1000
2023-10-28 03:37:33.890 
Epoch 131/1000 
	 loss: 49.5103, MinusLogProbMetric: 49.5103, val_loss: 48.1103, val_MinusLogProbMetric: 48.1103

Epoch 131: val_loss improved from 48.16924 to 48.11034, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 49.5103 - MinusLogProbMetric: 49.5103 - val_loss: 48.1103 - val_MinusLogProbMetric: 48.1103 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 132/1000
2023-10-28 03:38:06.983 
Epoch 132/1000 
	 loss: 47.9336, MinusLogProbMetric: 47.9336, val_loss: 48.9382, val_MinusLogProbMetric: 48.9382

Epoch 132: val_loss did not improve from 48.11034
196/196 - 33s - loss: 47.9336 - MinusLogProbMetric: 47.9336 - val_loss: 48.9382 - val_MinusLogProbMetric: 48.9382 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 133/1000
2023-10-28 03:38:38.166 
Epoch 133/1000 
	 loss: 48.1898, MinusLogProbMetric: 48.1898, val_loss: 47.5842, val_MinusLogProbMetric: 47.5842

Epoch 133: val_loss improved from 48.11034 to 47.58419, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 32s - loss: 48.1898 - MinusLogProbMetric: 48.1898 - val_loss: 47.5842 - val_MinusLogProbMetric: 47.5842 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 134/1000
2023-10-28 03:39:09.651 
Epoch 134/1000 
	 loss: 47.9757, MinusLogProbMetric: 47.9757, val_loss: 48.0661, val_MinusLogProbMetric: 48.0661

Epoch 134: val_loss did not improve from 47.58419
196/196 - 31s - loss: 47.9757 - MinusLogProbMetric: 47.9757 - val_loss: 48.0661 - val_MinusLogProbMetric: 48.0661 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 135/1000
2023-10-28 03:39:38.024 
Epoch 135/1000 
	 loss: 48.4129, MinusLogProbMetric: 48.4129, val_loss: 49.0346, val_MinusLogProbMetric: 49.0346

Epoch 135: val_loss did not improve from 47.58419
196/196 - 28s - loss: 48.4129 - MinusLogProbMetric: 48.4129 - val_loss: 49.0346 - val_MinusLogProbMetric: 49.0346 - lr: 3.3333e-04 - 28s/epoch - 145ms/step
Epoch 136/1000
2023-10-28 03:40:05.117 
Epoch 136/1000 
	 loss: 48.2200, MinusLogProbMetric: 48.2200, val_loss: 47.7813, val_MinusLogProbMetric: 47.7813

Epoch 136: val_loss did not improve from 47.58419
196/196 - 27s - loss: 48.2200 - MinusLogProbMetric: 48.2200 - val_loss: 47.7813 - val_MinusLogProbMetric: 47.7813 - lr: 3.3333e-04 - 27s/epoch - 138ms/step
Epoch 137/1000
2023-10-28 03:40:35.320 
Epoch 137/1000 
	 loss: 47.8777, MinusLogProbMetric: 47.8777, val_loss: 48.8921, val_MinusLogProbMetric: 48.8921

Epoch 137: val_loss did not improve from 47.58419
196/196 - 30s - loss: 47.8777 - MinusLogProbMetric: 47.8777 - val_loss: 48.8921 - val_MinusLogProbMetric: 48.8921 - lr: 3.3333e-04 - 30s/epoch - 154ms/step
Epoch 138/1000
2023-10-28 03:41:06.390 
Epoch 138/1000 
	 loss: 48.1442, MinusLogProbMetric: 48.1442, val_loss: 48.8613, val_MinusLogProbMetric: 48.8613

Epoch 138: val_loss did not improve from 47.58419
196/196 - 31s - loss: 48.1442 - MinusLogProbMetric: 48.1442 - val_loss: 48.8613 - val_MinusLogProbMetric: 48.8613 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 139/1000
2023-10-28 03:41:39.112 
Epoch 139/1000 
	 loss: 48.1595, MinusLogProbMetric: 48.1595, val_loss: 48.5296, val_MinusLogProbMetric: 48.5296

Epoch 139: val_loss did not improve from 47.58419
196/196 - 33s - loss: 48.1595 - MinusLogProbMetric: 48.1595 - val_loss: 48.5296 - val_MinusLogProbMetric: 48.5296 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 140/1000
2023-10-28 03:42:10.547 
Epoch 140/1000 
	 loss: 47.9135, MinusLogProbMetric: 47.9135, val_loss: 48.6722, val_MinusLogProbMetric: 48.6722

Epoch 140: val_loss did not improve from 47.58419
196/196 - 31s - loss: 47.9135 - MinusLogProbMetric: 47.9135 - val_loss: 48.6722 - val_MinusLogProbMetric: 48.6722 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 141/1000
2023-10-28 03:42:43.713 
Epoch 141/1000 
	 loss: 47.8052, MinusLogProbMetric: 47.8052, val_loss: 48.4290, val_MinusLogProbMetric: 48.4290

Epoch 141: val_loss did not improve from 47.58419
196/196 - 33s - loss: 47.8052 - MinusLogProbMetric: 47.8052 - val_loss: 48.4290 - val_MinusLogProbMetric: 48.4290 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 142/1000
2023-10-28 03:43:17.400 
Epoch 142/1000 
	 loss: 51.3259, MinusLogProbMetric: 51.3259, val_loss: 48.6930, val_MinusLogProbMetric: 48.6930

Epoch 142: val_loss did not improve from 47.58419
196/196 - 34s - loss: 51.3259 - MinusLogProbMetric: 51.3259 - val_loss: 48.6930 - val_MinusLogProbMetric: 48.6930 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 143/1000
2023-10-28 03:43:51.024 
Epoch 143/1000 
	 loss: 47.5059, MinusLogProbMetric: 47.5059, val_loss: 48.9621, val_MinusLogProbMetric: 48.9621

Epoch 143: val_loss did not improve from 47.58419
196/196 - 34s - loss: 47.5059 - MinusLogProbMetric: 47.5059 - val_loss: 48.9621 - val_MinusLogProbMetric: 48.9621 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 144/1000
2023-10-28 03:44:25.323 
Epoch 144/1000 
	 loss: 47.6167, MinusLogProbMetric: 47.6167, val_loss: 49.1523, val_MinusLogProbMetric: 49.1523

Epoch 144: val_loss did not improve from 47.58419
196/196 - 34s - loss: 47.6167 - MinusLogProbMetric: 47.6167 - val_loss: 49.1523 - val_MinusLogProbMetric: 49.1523 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 145/1000
2023-10-28 03:44:58.592 
Epoch 145/1000 
	 loss: 47.9407, MinusLogProbMetric: 47.9407, val_loss: 47.9844, val_MinusLogProbMetric: 47.9844

Epoch 145: val_loss did not improve from 47.58419
196/196 - 33s - loss: 47.9407 - MinusLogProbMetric: 47.9407 - val_loss: 47.9844 - val_MinusLogProbMetric: 47.9844 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 146/1000
2023-10-28 03:45:32.254 
Epoch 146/1000 
	 loss: 47.5418, MinusLogProbMetric: 47.5418, val_loss: 47.4764, val_MinusLogProbMetric: 47.4764

Epoch 146: val_loss improved from 47.58419 to 47.47638, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 47.5418 - MinusLogProbMetric: 47.5418 - val_loss: 47.4764 - val_MinusLogProbMetric: 47.4764 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 147/1000
2023-10-28 03:46:05.895 
Epoch 147/1000 
	 loss: 47.6720, MinusLogProbMetric: 47.6720, val_loss: 48.1274, val_MinusLogProbMetric: 48.1274

Epoch 147: val_loss did not improve from 47.47638
196/196 - 33s - loss: 47.6720 - MinusLogProbMetric: 47.6720 - val_loss: 48.1274 - val_MinusLogProbMetric: 48.1274 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 148/1000
2023-10-28 03:46:39.046 
Epoch 148/1000 
	 loss: 47.7450, MinusLogProbMetric: 47.7450, val_loss: 48.8047, val_MinusLogProbMetric: 48.8047

Epoch 148: val_loss did not improve from 47.47638
196/196 - 33s - loss: 47.7450 - MinusLogProbMetric: 47.7450 - val_loss: 48.8047 - val_MinusLogProbMetric: 48.8047 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 149/1000
2023-10-28 03:47:12.174 
Epoch 149/1000 
	 loss: 47.5392, MinusLogProbMetric: 47.5392, val_loss: 47.5647, val_MinusLogProbMetric: 47.5647

Epoch 149: val_loss did not improve from 47.47638
196/196 - 33s - loss: 47.5392 - MinusLogProbMetric: 47.5392 - val_loss: 47.5647 - val_MinusLogProbMetric: 47.5647 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 150/1000
2023-10-28 03:47:45.460 
Epoch 150/1000 
	 loss: 47.4228, MinusLogProbMetric: 47.4228, val_loss: 47.1724, val_MinusLogProbMetric: 47.1724

Epoch 150: val_loss improved from 47.47638 to 47.17245, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 47.4228 - MinusLogProbMetric: 47.4228 - val_loss: 47.1724 - val_MinusLogProbMetric: 47.1724 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 151/1000
2023-10-28 03:48:19.547 
Epoch 151/1000 
	 loss: 47.6502, MinusLogProbMetric: 47.6502, val_loss: 56.9449, val_MinusLogProbMetric: 56.9449

Epoch 151: val_loss did not improve from 47.17245
196/196 - 34s - loss: 47.6502 - MinusLogProbMetric: 47.6502 - val_loss: 56.9449 - val_MinusLogProbMetric: 56.9449 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 152/1000
2023-10-28 03:48:52.850 
Epoch 152/1000 
	 loss: 48.7105, MinusLogProbMetric: 48.7105, val_loss: 47.5804, val_MinusLogProbMetric: 47.5804

Epoch 152: val_loss did not improve from 47.17245
196/196 - 33s - loss: 48.7105 - MinusLogProbMetric: 48.7105 - val_loss: 47.5804 - val_MinusLogProbMetric: 47.5804 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 153/1000
2023-10-28 03:49:26.141 
Epoch 153/1000 
	 loss: 47.9581, MinusLogProbMetric: 47.9581, val_loss: 47.2411, val_MinusLogProbMetric: 47.2411

Epoch 153: val_loss did not improve from 47.17245
196/196 - 33s - loss: 47.9581 - MinusLogProbMetric: 47.9581 - val_loss: 47.2411 - val_MinusLogProbMetric: 47.2411 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 154/1000
2023-10-28 03:49:59.370 
Epoch 154/1000 
	 loss: 47.2230, MinusLogProbMetric: 47.2230, val_loss: 48.3603, val_MinusLogProbMetric: 48.3603

Epoch 154: val_loss did not improve from 47.17245
196/196 - 33s - loss: 47.2230 - MinusLogProbMetric: 47.2230 - val_loss: 48.3603 - val_MinusLogProbMetric: 48.3603 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 155/1000
2023-10-28 03:50:32.822 
Epoch 155/1000 
	 loss: 47.3224, MinusLogProbMetric: 47.3224, val_loss: 47.3341, val_MinusLogProbMetric: 47.3341

Epoch 155: val_loss did not improve from 47.17245
196/196 - 33s - loss: 47.3224 - MinusLogProbMetric: 47.3224 - val_loss: 47.3341 - val_MinusLogProbMetric: 47.3341 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 156/1000
2023-10-28 03:51:06.226 
Epoch 156/1000 
	 loss: 47.9021, MinusLogProbMetric: 47.9021, val_loss: 47.1309, val_MinusLogProbMetric: 47.1309

Epoch 156: val_loss improved from 47.17245 to 47.13090, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 47.9021 - MinusLogProbMetric: 47.9021 - val_loss: 47.1309 - val_MinusLogProbMetric: 47.1309 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 157/1000
2023-10-28 03:51:40.234 
Epoch 157/1000 
	 loss: 47.7182, MinusLogProbMetric: 47.7182, val_loss: 48.6804, val_MinusLogProbMetric: 48.6804

Epoch 157: val_loss did not improve from 47.13090
196/196 - 33s - loss: 47.7182 - MinusLogProbMetric: 47.7182 - val_loss: 48.6804 - val_MinusLogProbMetric: 48.6804 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 158/1000
2023-10-28 03:52:13.550 
Epoch 158/1000 
	 loss: 47.5831, MinusLogProbMetric: 47.5831, val_loss: 47.4403, val_MinusLogProbMetric: 47.4403

Epoch 158: val_loss did not improve from 47.13090
196/196 - 33s - loss: 47.5831 - MinusLogProbMetric: 47.5831 - val_loss: 47.4403 - val_MinusLogProbMetric: 47.4403 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 159/1000
2023-10-28 03:52:46.994 
Epoch 159/1000 
	 loss: 47.5109, MinusLogProbMetric: 47.5109, val_loss: 46.8210, val_MinusLogProbMetric: 46.8210

Epoch 159: val_loss improved from 47.13090 to 46.82099, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 47.5109 - MinusLogProbMetric: 47.5109 - val_loss: 46.8210 - val_MinusLogProbMetric: 46.8210 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 160/1000
2023-10-28 03:53:20.809 
Epoch 160/1000 
	 loss: 47.2831, MinusLogProbMetric: 47.2831, val_loss: 47.3619, val_MinusLogProbMetric: 47.3619

Epoch 160: val_loss did not improve from 46.82099
196/196 - 33s - loss: 47.2831 - MinusLogProbMetric: 47.2831 - val_loss: 47.3619 - val_MinusLogProbMetric: 47.3619 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 161/1000
2023-10-28 03:53:54.473 
Epoch 161/1000 
	 loss: 56.0552, MinusLogProbMetric: 56.0552, val_loss: 49.8471, val_MinusLogProbMetric: 49.8471

Epoch 161: val_loss did not improve from 46.82099
196/196 - 34s - loss: 56.0552 - MinusLogProbMetric: 56.0552 - val_loss: 49.8471 - val_MinusLogProbMetric: 49.8471 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 162/1000
2023-10-28 03:54:27.601 
Epoch 162/1000 
	 loss: 48.1186, MinusLogProbMetric: 48.1186, val_loss: 48.1715, val_MinusLogProbMetric: 48.1715

Epoch 162: val_loss did not improve from 46.82099
196/196 - 33s - loss: 48.1186 - MinusLogProbMetric: 48.1186 - val_loss: 48.1715 - val_MinusLogProbMetric: 48.1715 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 163/1000
2023-10-28 03:55:00.730 
Epoch 163/1000 
	 loss: 47.5885, MinusLogProbMetric: 47.5885, val_loss: 47.8541, val_MinusLogProbMetric: 47.8541

Epoch 163: val_loss did not improve from 46.82099
196/196 - 33s - loss: 47.5885 - MinusLogProbMetric: 47.5885 - val_loss: 47.8541 - val_MinusLogProbMetric: 47.8541 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 164/1000
2023-10-28 03:55:34.185 
Epoch 164/1000 
	 loss: 47.3005, MinusLogProbMetric: 47.3005, val_loss: 47.8401, val_MinusLogProbMetric: 47.8401

Epoch 164: val_loss did not improve from 46.82099
196/196 - 33s - loss: 47.3005 - MinusLogProbMetric: 47.3005 - val_loss: 47.8401 - val_MinusLogProbMetric: 47.8401 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 165/1000
2023-10-28 03:56:06.482 
Epoch 165/1000 
	 loss: 47.1859, MinusLogProbMetric: 47.1859, val_loss: 47.8371, val_MinusLogProbMetric: 47.8371

Epoch 165: val_loss did not improve from 46.82099
196/196 - 32s - loss: 47.1859 - MinusLogProbMetric: 47.1859 - val_loss: 47.8371 - val_MinusLogProbMetric: 47.8371 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 166/1000
2023-10-28 03:56:39.590 
Epoch 166/1000 
	 loss: 47.0958, MinusLogProbMetric: 47.0958, val_loss: 47.3679, val_MinusLogProbMetric: 47.3679

Epoch 166: val_loss did not improve from 46.82099
196/196 - 33s - loss: 47.0958 - MinusLogProbMetric: 47.0958 - val_loss: 47.3679 - val_MinusLogProbMetric: 47.3679 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 167/1000
2023-10-28 03:57:12.737 
Epoch 167/1000 
	 loss: 47.2926, MinusLogProbMetric: 47.2926, val_loss: 48.9623, val_MinusLogProbMetric: 48.9623

Epoch 167: val_loss did not improve from 46.82099
196/196 - 33s - loss: 47.2926 - MinusLogProbMetric: 47.2926 - val_loss: 48.9623 - val_MinusLogProbMetric: 48.9623 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 168/1000
2023-10-28 03:57:45.819 
Epoch 168/1000 
	 loss: 47.5547, MinusLogProbMetric: 47.5547, val_loss: 49.8788, val_MinusLogProbMetric: 49.8788

Epoch 168: val_loss did not improve from 46.82099
196/196 - 33s - loss: 47.5547 - MinusLogProbMetric: 47.5547 - val_loss: 49.8788 - val_MinusLogProbMetric: 49.8788 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 169/1000
2023-10-28 03:58:19.532 
Epoch 169/1000 
	 loss: 47.1872, MinusLogProbMetric: 47.1872, val_loss: 47.0676, val_MinusLogProbMetric: 47.0676

Epoch 169: val_loss did not improve from 46.82099
196/196 - 34s - loss: 47.1872 - MinusLogProbMetric: 47.1872 - val_loss: 47.0676 - val_MinusLogProbMetric: 47.0676 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 170/1000
2023-10-28 03:58:53.338 
Epoch 170/1000 
	 loss: 46.9925, MinusLogProbMetric: 46.9925, val_loss: 48.9162, val_MinusLogProbMetric: 48.9162

Epoch 170: val_loss did not improve from 46.82099
196/196 - 34s - loss: 46.9925 - MinusLogProbMetric: 46.9925 - val_loss: 48.9162 - val_MinusLogProbMetric: 48.9162 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 171/1000
2023-10-28 03:59:27.039 
Epoch 171/1000 
	 loss: 47.1629, MinusLogProbMetric: 47.1629, val_loss: 47.3192, val_MinusLogProbMetric: 47.3192

Epoch 171: val_loss did not improve from 46.82099
196/196 - 34s - loss: 47.1629 - MinusLogProbMetric: 47.1629 - val_loss: 47.3192 - val_MinusLogProbMetric: 47.3192 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 172/1000
2023-10-28 04:00:00.578 
Epoch 172/1000 
	 loss: 47.6679, MinusLogProbMetric: 47.6679, val_loss: 48.9181, val_MinusLogProbMetric: 48.9181

Epoch 172: val_loss did not improve from 46.82099
196/196 - 34s - loss: 47.6679 - MinusLogProbMetric: 47.6679 - val_loss: 48.9181 - val_MinusLogProbMetric: 48.9181 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 173/1000
2023-10-28 04:00:34.064 
Epoch 173/1000 
	 loss: 47.8082, MinusLogProbMetric: 47.8082, val_loss: 47.7066, val_MinusLogProbMetric: 47.7066

Epoch 173: val_loss did not improve from 46.82099
196/196 - 33s - loss: 47.8082 - MinusLogProbMetric: 47.8082 - val_loss: 47.7066 - val_MinusLogProbMetric: 47.7066 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 174/1000
2023-10-28 04:01:07.702 
Epoch 174/1000 
	 loss: 46.7910, MinusLogProbMetric: 46.7910, val_loss: 48.4300, val_MinusLogProbMetric: 48.4300

Epoch 174: val_loss did not improve from 46.82099
196/196 - 34s - loss: 46.7910 - MinusLogProbMetric: 46.7910 - val_loss: 48.4300 - val_MinusLogProbMetric: 48.4300 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 175/1000
2023-10-28 04:01:41.482 
Epoch 175/1000 
	 loss: 47.7158, MinusLogProbMetric: 47.7158, val_loss: 47.5227, val_MinusLogProbMetric: 47.5227

Epoch 175: val_loss did not improve from 46.82099
196/196 - 34s - loss: 47.7158 - MinusLogProbMetric: 47.7158 - val_loss: 47.5227 - val_MinusLogProbMetric: 47.5227 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 176/1000
2023-10-28 04:02:15.689 
Epoch 176/1000 
	 loss: 47.0405, MinusLogProbMetric: 47.0405, val_loss: 47.0872, val_MinusLogProbMetric: 47.0872

Epoch 176: val_loss did not improve from 46.82099
196/196 - 34s - loss: 47.0405 - MinusLogProbMetric: 47.0405 - val_loss: 47.0872 - val_MinusLogProbMetric: 47.0872 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 177/1000
2023-10-28 04:02:49.988 
Epoch 177/1000 
	 loss: 47.0096, MinusLogProbMetric: 47.0096, val_loss: 47.9092, val_MinusLogProbMetric: 47.9092

Epoch 177: val_loss did not improve from 46.82099
196/196 - 34s - loss: 47.0096 - MinusLogProbMetric: 47.0096 - val_loss: 47.9092 - val_MinusLogProbMetric: 47.9092 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 178/1000
2023-10-28 04:03:24.196 
Epoch 178/1000 
	 loss: 46.8577, MinusLogProbMetric: 46.8577, val_loss: 48.3416, val_MinusLogProbMetric: 48.3416

Epoch 178: val_loss did not improve from 46.82099
196/196 - 34s - loss: 46.8577 - MinusLogProbMetric: 46.8577 - val_loss: 48.3416 - val_MinusLogProbMetric: 48.3416 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 179/1000
2023-10-28 04:03:58.171 
Epoch 179/1000 
	 loss: 46.8050, MinusLogProbMetric: 46.8050, val_loss: 46.9984, val_MinusLogProbMetric: 46.9984

Epoch 179: val_loss did not improve from 46.82099
196/196 - 34s - loss: 46.8050 - MinusLogProbMetric: 46.8050 - val_loss: 46.9984 - val_MinusLogProbMetric: 46.9984 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 180/1000
2023-10-28 04:04:31.465 
Epoch 180/1000 
	 loss: 46.9158, MinusLogProbMetric: 46.9158, val_loss: 46.3905, val_MinusLogProbMetric: 46.3905

Epoch 180: val_loss improved from 46.82099 to 46.39052, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 46.9158 - MinusLogProbMetric: 46.9158 - val_loss: 46.3905 - val_MinusLogProbMetric: 46.3905 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 181/1000
2023-10-28 04:05:05.238 
Epoch 181/1000 
	 loss: 46.6167, MinusLogProbMetric: 46.6167, val_loss: 46.5784, val_MinusLogProbMetric: 46.5784

Epoch 181: val_loss did not improve from 46.39052
196/196 - 33s - loss: 46.6167 - MinusLogProbMetric: 46.6167 - val_loss: 46.5784 - val_MinusLogProbMetric: 46.5784 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 182/1000
2023-10-28 04:05:38.328 
Epoch 182/1000 
	 loss: 47.5212, MinusLogProbMetric: 47.5212, val_loss: 48.0260, val_MinusLogProbMetric: 48.0260

Epoch 182: val_loss did not improve from 46.39052
196/196 - 33s - loss: 47.5212 - MinusLogProbMetric: 47.5212 - val_loss: 48.0260 - val_MinusLogProbMetric: 48.0260 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 183/1000
2023-10-28 04:06:09.936 
Epoch 183/1000 
	 loss: 46.9827, MinusLogProbMetric: 46.9827, val_loss: 48.4324, val_MinusLogProbMetric: 48.4324

Epoch 183: val_loss did not improve from 46.39052
196/196 - 32s - loss: 46.9827 - MinusLogProbMetric: 46.9827 - val_loss: 48.4324 - val_MinusLogProbMetric: 48.4324 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 184/1000
2023-10-28 04:06:40.611 
Epoch 184/1000 
	 loss: 46.8142, MinusLogProbMetric: 46.8142, val_loss: 47.0211, val_MinusLogProbMetric: 47.0211

Epoch 184: val_loss did not improve from 46.39052
196/196 - 31s - loss: 46.8142 - MinusLogProbMetric: 46.8142 - val_loss: 47.0211 - val_MinusLogProbMetric: 47.0211 - lr: 3.3333e-04 - 31s/epoch - 156ms/step
Epoch 185/1000
2023-10-28 04:07:13.438 
Epoch 185/1000 
	 loss: 46.8609, MinusLogProbMetric: 46.8609, val_loss: 48.2166, val_MinusLogProbMetric: 48.2166

Epoch 185: val_loss did not improve from 46.39052
196/196 - 33s - loss: 46.8609 - MinusLogProbMetric: 46.8609 - val_loss: 48.2166 - val_MinusLogProbMetric: 48.2166 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 186/1000
2023-10-28 04:07:47.007 
Epoch 186/1000 
	 loss: 46.7797, MinusLogProbMetric: 46.7797, val_loss: 48.7364, val_MinusLogProbMetric: 48.7364

Epoch 186: val_loss did not improve from 46.39052
196/196 - 34s - loss: 46.7797 - MinusLogProbMetric: 46.7797 - val_loss: 48.7364 - val_MinusLogProbMetric: 48.7364 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 187/1000
2023-10-28 04:08:20.429 
Epoch 187/1000 
	 loss: 47.2196, MinusLogProbMetric: 47.2196, val_loss: 49.0343, val_MinusLogProbMetric: 49.0343

Epoch 187: val_loss did not improve from 46.39052
196/196 - 33s - loss: 47.2196 - MinusLogProbMetric: 47.2196 - val_loss: 49.0343 - val_MinusLogProbMetric: 49.0343 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 188/1000
2023-10-28 04:08:53.455 
Epoch 188/1000 
	 loss: 46.6856, MinusLogProbMetric: 46.6856, val_loss: 47.5627, val_MinusLogProbMetric: 47.5627

Epoch 188: val_loss did not improve from 46.39052
196/196 - 33s - loss: 46.6856 - MinusLogProbMetric: 46.6856 - val_loss: 47.5627 - val_MinusLogProbMetric: 47.5627 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 189/1000
2023-10-28 04:09:27.096 
Epoch 189/1000 
	 loss: 46.8859, MinusLogProbMetric: 46.8859, val_loss: 46.8154, val_MinusLogProbMetric: 46.8154

Epoch 189: val_loss did not improve from 46.39052
196/196 - 34s - loss: 46.8859 - MinusLogProbMetric: 46.8859 - val_loss: 46.8154 - val_MinusLogProbMetric: 46.8154 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 190/1000
2023-10-28 04:10:00.258 
Epoch 190/1000 
	 loss: 46.5845, MinusLogProbMetric: 46.5845, val_loss: 48.8895, val_MinusLogProbMetric: 48.8895

Epoch 190: val_loss did not improve from 46.39052
196/196 - 33s - loss: 46.5845 - MinusLogProbMetric: 46.5845 - val_loss: 48.8895 - val_MinusLogProbMetric: 48.8895 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 191/1000
2023-10-28 04:10:33.274 
Epoch 191/1000 
	 loss: 46.9507, MinusLogProbMetric: 46.9507, val_loss: 46.9567, val_MinusLogProbMetric: 46.9567

Epoch 191: val_loss did not improve from 46.39052
196/196 - 33s - loss: 46.9507 - MinusLogProbMetric: 46.9507 - val_loss: 46.9567 - val_MinusLogProbMetric: 46.9567 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 192/1000
2023-10-28 04:11:06.423 
Epoch 192/1000 
	 loss: 46.8847, MinusLogProbMetric: 46.8847, val_loss: 50.3120, val_MinusLogProbMetric: 50.3120

Epoch 192: val_loss did not improve from 46.39052
196/196 - 33s - loss: 46.8847 - MinusLogProbMetric: 46.8847 - val_loss: 50.3120 - val_MinusLogProbMetric: 50.3120 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 193/1000
2023-10-28 04:11:39.527 
Epoch 193/1000 
	 loss: 46.7101, MinusLogProbMetric: 46.7101, val_loss: 46.7781, val_MinusLogProbMetric: 46.7781

Epoch 193: val_loss did not improve from 46.39052
196/196 - 33s - loss: 46.7101 - MinusLogProbMetric: 46.7101 - val_loss: 46.7781 - val_MinusLogProbMetric: 46.7781 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 194/1000
2023-10-28 04:12:12.950 
Epoch 194/1000 
	 loss: 46.7329, MinusLogProbMetric: 46.7329, val_loss: 47.1794, val_MinusLogProbMetric: 47.1794

Epoch 194: val_loss did not improve from 46.39052
196/196 - 33s - loss: 46.7329 - MinusLogProbMetric: 46.7329 - val_loss: 47.1794 - val_MinusLogProbMetric: 47.1794 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 195/1000
2023-10-28 04:12:46.162 
Epoch 195/1000 
	 loss: 46.7025, MinusLogProbMetric: 46.7025, val_loss: 49.9560, val_MinusLogProbMetric: 49.9560

Epoch 195: val_loss did not improve from 46.39052
196/196 - 33s - loss: 46.7025 - MinusLogProbMetric: 46.7025 - val_loss: 49.9560 - val_MinusLogProbMetric: 49.9560 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 196/1000
2023-10-28 04:13:19.186 
Epoch 196/1000 
	 loss: 47.2609, MinusLogProbMetric: 47.2609, val_loss: 46.1089, val_MinusLogProbMetric: 46.1089

Epoch 196: val_loss improved from 46.39052 to 46.10890, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 47.2609 - MinusLogProbMetric: 47.2609 - val_loss: 46.1089 - val_MinusLogProbMetric: 46.1089 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 197/1000
2023-10-28 04:13:52.760 
Epoch 197/1000 
	 loss: 46.5116, MinusLogProbMetric: 46.5116, val_loss: 48.2384, val_MinusLogProbMetric: 48.2384

Epoch 197: val_loss did not improve from 46.10890
196/196 - 33s - loss: 46.5116 - MinusLogProbMetric: 46.5116 - val_loss: 48.2384 - val_MinusLogProbMetric: 48.2384 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 198/1000
2023-10-28 04:14:26.134 
Epoch 198/1000 
	 loss: 47.3480, MinusLogProbMetric: 47.3480, val_loss: 47.5831, val_MinusLogProbMetric: 47.5831

Epoch 198: val_loss did not improve from 46.10890
196/196 - 33s - loss: 47.3480 - MinusLogProbMetric: 47.3480 - val_loss: 47.5831 - val_MinusLogProbMetric: 47.5831 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 199/1000
2023-10-28 04:14:59.643 
Epoch 199/1000 
	 loss: 46.7275, MinusLogProbMetric: 46.7275, val_loss: 47.7194, val_MinusLogProbMetric: 47.7194

Epoch 199: val_loss did not improve from 46.10890
196/196 - 34s - loss: 46.7275 - MinusLogProbMetric: 46.7275 - val_loss: 47.7194 - val_MinusLogProbMetric: 47.7194 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 200/1000
2023-10-28 04:15:32.936 
Epoch 200/1000 
	 loss: 46.5845, MinusLogProbMetric: 46.5845, val_loss: 46.8179, val_MinusLogProbMetric: 46.8179

Epoch 200: val_loss did not improve from 46.10890
196/196 - 33s - loss: 46.5845 - MinusLogProbMetric: 46.5845 - val_loss: 46.8179 - val_MinusLogProbMetric: 46.8179 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 201/1000
2023-10-28 04:16:05.888 
Epoch 201/1000 
	 loss: 46.8984, MinusLogProbMetric: 46.8984, val_loss: 46.2006, val_MinusLogProbMetric: 46.2006

Epoch 201: val_loss did not improve from 46.10890
196/196 - 33s - loss: 46.8984 - MinusLogProbMetric: 46.8984 - val_loss: 46.2006 - val_MinusLogProbMetric: 46.2006 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 202/1000
2023-10-28 04:16:39.042 
Epoch 202/1000 
	 loss: 46.4464, MinusLogProbMetric: 46.4464, val_loss: 46.5537, val_MinusLogProbMetric: 46.5537

Epoch 202: val_loss did not improve from 46.10890
196/196 - 33s - loss: 46.4464 - MinusLogProbMetric: 46.4464 - val_loss: 46.5537 - val_MinusLogProbMetric: 46.5537 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 203/1000
2023-10-28 04:17:12.885 
Epoch 203/1000 
	 loss: 46.9920, MinusLogProbMetric: 46.9920, val_loss: 47.3261, val_MinusLogProbMetric: 47.3261

Epoch 203: val_loss did not improve from 46.10890
196/196 - 34s - loss: 46.9920 - MinusLogProbMetric: 46.9920 - val_loss: 47.3261 - val_MinusLogProbMetric: 47.3261 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 204/1000
2023-10-28 04:17:46.200 
Epoch 204/1000 
	 loss: 46.7660, MinusLogProbMetric: 46.7660, val_loss: 48.1138, val_MinusLogProbMetric: 48.1138

Epoch 204: val_loss did not improve from 46.10890
196/196 - 33s - loss: 46.7660 - MinusLogProbMetric: 46.7660 - val_loss: 48.1138 - val_MinusLogProbMetric: 48.1138 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 205/1000
2023-10-28 04:18:21.392 
Epoch 205/1000 
	 loss: 46.3314, MinusLogProbMetric: 46.3314, val_loss: 46.4605, val_MinusLogProbMetric: 46.4605

Epoch 205: val_loss did not improve from 46.10890
196/196 - 35s - loss: 46.3314 - MinusLogProbMetric: 46.3314 - val_loss: 46.4605 - val_MinusLogProbMetric: 46.4605 - lr: 3.3333e-04 - 35s/epoch - 180ms/step
Epoch 206/1000
2023-10-28 04:18:55.995 
Epoch 206/1000 
	 loss: 46.6019, MinusLogProbMetric: 46.6019, val_loss: 47.6813, val_MinusLogProbMetric: 47.6813

Epoch 206: val_loss did not improve from 46.10890
196/196 - 35s - loss: 46.6019 - MinusLogProbMetric: 46.6019 - val_loss: 47.6813 - val_MinusLogProbMetric: 47.6813 - lr: 3.3333e-04 - 35s/epoch - 177ms/step
Epoch 207/1000
2023-10-28 04:19:29.425 
Epoch 207/1000 
	 loss: 46.6805, MinusLogProbMetric: 46.6805, val_loss: 45.9092, val_MinusLogProbMetric: 45.9092

Epoch 207: val_loss improved from 46.10890 to 45.90919, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 46.6805 - MinusLogProbMetric: 46.6805 - val_loss: 45.9092 - val_MinusLogProbMetric: 45.9092 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 208/1000
2023-10-28 04:20:03.462 
Epoch 208/1000 
	 loss: 46.9159, MinusLogProbMetric: 46.9159, val_loss: 46.6038, val_MinusLogProbMetric: 46.6038

Epoch 208: val_loss did not improve from 45.90919
196/196 - 33s - loss: 46.9159 - MinusLogProbMetric: 46.9159 - val_loss: 46.6038 - val_MinusLogProbMetric: 46.6038 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 209/1000
2023-10-28 04:20:36.955 
Epoch 209/1000 
	 loss: 46.4377, MinusLogProbMetric: 46.4377, val_loss: 46.7622, val_MinusLogProbMetric: 46.7622

Epoch 209: val_loss did not improve from 45.90919
196/196 - 33s - loss: 46.4377 - MinusLogProbMetric: 46.4377 - val_loss: 46.7622 - val_MinusLogProbMetric: 46.7622 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 210/1000
2023-10-28 04:21:10.318 
Epoch 210/1000 
	 loss: 46.6451, MinusLogProbMetric: 46.6451, val_loss: 47.7592, val_MinusLogProbMetric: 47.7592

Epoch 210: val_loss did not improve from 45.90919
196/196 - 33s - loss: 46.6451 - MinusLogProbMetric: 46.6451 - val_loss: 47.7592 - val_MinusLogProbMetric: 47.7592 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 211/1000
2023-10-28 04:21:43.694 
Epoch 211/1000 
	 loss: 46.5371, MinusLogProbMetric: 46.5371, val_loss: 46.9759, val_MinusLogProbMetric: 46.9759

Epoch 211: val_loss did not improve from 45.90919
196/196 - 33s - loss: 46.5371 - MinusLogProbMetric: 46.5371 - val_loss: 46.9759 - val_MinusLogProbMetric: 46.9759 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 212/1000
2023-10-28 04:22:17.007 
Epoch 212/1000 
	 loss: 46.4844, MinusLogProbMetric: 46.4844, val_loss: 45.7780, val_MinusLogProbMetric: 45.7780

Epoch 212: val_loss improved from 45.90919 to 45.77804, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 46.4844 - MinusLogProbMetric: 46.4844 - val_loss: 45.7780 - val_MinusLogProbMetric: 45.7780 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 213/1000
2023-10-28 04:22:50.573 
Epoch 213/1000 
	 loss: 46.3430, MinusLogProbMetric: 46.3430, val_loss: 49.1804, val_MinusLogProbMetric: 49.1804

Epoch 213: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.3430 - MinusLogProbMetric: 46.3430 - val_loss: 49.1804 - val_MinusLogProbMetric: 49.1804 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 214/1000
2023-10-28 04:23:23.946 
Epoch 214/1000 
	 loss: 46.3418, MinusLogProbMetric: 46.3418, val_loss: 49.2514, val_MinusLogProbMetric: 49.2514

Epoch 214: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.3418 - MinusLogProbMetric: 46.3418 - val_loss: 49.2514 - val_MinusLogProbMetric: 49.2514 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 215/1000
2023-10-28 04:23:57.209 
Epoch 215/1000 
	 loss: 46.7136, MinusLogProbMetric: 46.7136, val_loss: 46.1558, val_MinusLogProbMetric: 46.1558

Epoch 215: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.7136 - MinusLogProbMetric: 46.7136 - val_loss: 46.1558 - val_MinusLogProbMetric: 46.1558 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 216/1000
2023-10-28 04:24:30.127 
Epoch 216/1000 
	 loss: 46.6631, MinusLogProbMetric: 46.6631, val_loss: 47.1072, val_MinusLogProbMetric: 47.1072

Epoch 216: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.6631 - MinusLogProbMetric: 46.6631 - val_loss: 47.1072 - val_MinusLogProbMetric: 47.1072 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 217/1000
2023-10-28 04:25:03.170 
Epoch 217/1000 
	 loss: 46.2556, MinusLogProbMetric: 46.2556, val_loss: 46.2801, val_MinusLogProbMetric: 46.2801

Epoch 217: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.2556 - MinusLogProbMetric: 46.2556 - val_loss: 46.2801 - val_MinusLogProbMetric: 46.2801 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 218/1000
2023-10-28 04:25:36.369 
Epoch 218/1000 
	 loss: 46.5049, MinusLogProbMetric: 46.5049, val_loss: 47.3941, val_MinusLogProbMetric: 47.3941

Epoch 218: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.5049 - MinusLogProbMetric: 46.5049 - val_loss: 47.3941 - val_MinusLogProbMetric: 47.3941 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 219/1000
2023-10-28 04:26:09.669 
Epoch 219/1000 
	 loss: 46.4531, MinusLogProbMetric: 46.4531, val_loss: 47.0866, val_MinusLogProbMetric: 47.0866

Epoch 219: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.4531 - MinusLogProbMetric: 46.4531 - val_loss: 47.0866 - val_MinusLogProbMetric: 47.0866 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 220/1000
2023-10-28 04:26:42.725 
Epoch 220/1000 
	 loss: 47.0513, MinusLogProbMetric: 47.0513, val_loss: 46.3250, val_MinusLogProbMetric: 46.3250

Epoch 220: val_loss did not improve from 45.77804
196/196 - 33s - loss: 47.0513 - MinusLogProbMetric: 47.0513 - val_loss: 46.3250 - val_MinusLogProbMetric: 46.3250 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 221/1000
2023-10-28 04:27:15.585 
Epoch 221/1000 
	 loss: 47.2669, MinusLogProbMetric: 47.2669, val_loss: 46.7753, val_MinusLogProbMetric: 46.7753

Epoch 221: val_loss did not improve from 45.77804
196/196 - 33s - loss: 47.2669 - MinusLogProbMetric: 47.2669 - val_loss: 46.7753 - val_MinusLogProbMetric: 46.7753 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 222/1000
2023-10-28 04:27:48.747 
Epoch 222/1000 
	 loss: 46.4528, MinusLogProbMetric: 46.4528, val_loss: 47.6006, val_MinusLogProbMetric: 47.6006

Epoch 222: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.4528 - MinusLogProbMetric: 46.4528 - val_loss: 47.6006 - val_MinusLogProbMetric: 47.6006 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 223/1000
2023-10-28 04:28:22.018 
Epoch 223/1000 
	 loss: 46.2130, MinusLogProbMetric: 46.2130, val_loss: 45.9992, val_MinusLogProbMetric: 45.9992

Epoch 223: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.2130 - MinusLogProbMetric: 46.2130 - val_loss: 45.9992 - val_MinusLogProbMetric: 45.9992 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 224/1000
2023-10-28 04:28:55.466 
Epoch 224/1000 
	 loss: 46.2434, MinusLogProbMetric: 46.2434, val_loss: 48.0992, val_MinusLogProbMetric: 48.0992

Epoch 224: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.2434 - MinusLogProbMetric: 46.2434 - val_loss: 48.0992 - val_MinusLogProbMetric: 48.0992 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 225/1000
2023-10-28 04:29:28.730 
Epoch 225/1000 
	 loss: 46.5589, MinusLogProbMetric: 46.5589, val_loss: 53.7539, val_MinusLogProbMetric: 53.7539

Epoch 225: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.5589 - MinusLogProbMetric: 46.5589 - val_loss: 53.7539 - val_MinusLogProbMetric: 53.7539 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 226/1000
2023-10-28 04:30:01.074 
Epoch 226/1000 
	 loss: 46.6709, MinusLogProbMetric: 46.6709, val_loss: 49.4388, val_MinusLogProbMetric: 49.4388

Epoch 226: val_loss did not improve from 45.77804
196/196 - 32s - loss: 46.6709 - MinusLogProbMetric: 46.6709 - val_loss: 49.4388 - val_MinusLogProbMetric: 49.4388 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 227/1000
2023-10-28 04:30:34.210 
Epoch 227/1000 
	 loss: 46.3903, MinusLogProbMetric: 46.3903, val_loss: 47.5370, val_MinusLogProbMetric: 47.5370

Epoch 227: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.3903 - MinusLogProbMetric: 46.3903 - val_loss: 47.5370 - val_MinusLogProbMetric: 47.5370 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 228/1000
2023-10-28 04:31:07.558 
Epoch 228/1000 
	 loss: 45.9772, MinusLogProbMetric: 45.9772, val_loss: 46.9036, val_MinusLogProbMetric: 46.9036

Epoch 228: val_loss did not improve from 45.77804
196/196 - 33s - loss: 45.9772 - MinusLogProbMetric: 45.9772 - val_loss: 46.9036 - val_MinusLogProbMetric: 46.9036 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 229/1000
2023-10-28 04:31:39.849 
Epoch 229/1000 
	 loss: 46.1746, MinusLogProbMetric: 46.1746, val_loss: 48.0244, val_MinusLogProbMetric: 48.0244

Epoch 229: val_loss did not improve from 45.77804
196/196 - 32s - loss: 46.1746 - MinusLogProbMetric: 46.1746 - val_loss: 48.0244 - val_MinusLogProbMetric: 48.0244 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 230/1000
2023-10-28 04:32:13.176 
Epoch 230/1000 
	 loss: 46.2226, MinusLogProbMetric: 46.2226, val_loss: 46.5333, val_MinusLogProbMetric: 46.5333

Epoch 230: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.2226 - MinusLogProbMetric: 46.2226 - val_loss: 46.5333 - val_MinusLogProbMetric: 46.5333 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 231/1000
2023-10-28 04:32:47.054 
Epoch 231/1000 
	 loss: 45.9581, MinusLogProbMetric: 45.9581, val_loss: 46.8095, val_MinusLogProbMetric: 46.8095

Epoch 231: val_loss did not improve from 45.77804
196/196 - 34s - loss: 45.9581 - MinusLogProbMetric: 45.9581 - val_loss: 46.8095 - val_MinusLogProbMetric: 46.8095 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 232/1000
2023-10-28 04:33:20.593 
Epoch 232/1000 
	 loss: 46.5404, MinusLogProbMetric: 46.5404, val_loss: 46.9806, val_MinusLogProbMetric: 46.9806

Epoch 232: val_loss did not improve from 45.77804
196/196 - 34s - loss: 46.5404 - MinusLogProbMetric: 46.5404 - val_loss: 46.9806 - val_MinusLogProbMetric: 46.9806 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 233/1000
2023-10-28 04:33:53.941 
Epoch 233/1000 
	 loss: 45.7323, MinusLogProbMetric: 45.7323, val_loss: 47.4904, val_MinusLogProbMetric: 47.4904

Epoch 233: val_loss did not improve from 45.77804
196/196 - 33s - loss: 45.7323 - MinusLogProbMetric: 45.7323 - val_loss: 47.4904 - val_MinusLogProbMetric: 47.4904 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 234/1000
2023-10-28 04:34:26.954 
Epoch 234/1000 
	 loss: 46.1168, MinusLogProbMetric: 46.1168, val_loss: 47.1249, val_MinusLogProbMetric: 47.1249

Epoch 234: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.1168 - MinusLogProbMetric: 46.1168 - val_loss: 47.1249 - val_MinusLogProbMetric: 47.1249 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 235/1000
2023-10-28 04:34:59.819 
Epoch 235/1000 
	 loss: 46.1359, MinusLogProbMetric: 46.1359, val_loss: 46.3005, val_MinusLogProbMetric: 46.3005

Epoch 235: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.1359 - MinusLogProbMetric: 46.1359 - val_loss: 46.3005 - val_MinusLogProbMetric: 46.3005 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 236/1000
2023-10-28 04:35:32.736 
Epoch 236/1000 
	 loss: 46.1622, MinusLogProbMetric: 46.1622, val_loss: 46.6737, val_MinusLogProbMetric: 46.6737

Epoch 236: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.1622 - MinusLogProbMetric: 46.1622 - val_loss: 46.6737 - val_MinusLogProbMetric: 46.6737 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 237/1000
2023-10-28 04:36:05.788 
Epoch 237/1000 
	 loss: 46.4631, MinusLogProbMetric: 46.4631, val_loss: 48.0121, val_MinusLogProbMetric: 48.0121

Epoch 237: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.4631 - MinusLogProbMetric: 46.4631 - val_loss: 48.0121 - val_MinusLogProbMetric: 48.0121 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 238/1000
2023-10-28 04:36:38.833 
Epoch 238/1000 
	 loss: 46.2118, MinusLogProbMetric: 46.2118, val_loss: 47.1878, val_MinusLogProbMetric: 47.1878

Epoch 238: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.2118 - MinusLogProbMetric: 46.2118 - val_loss: 47.1878 - val_MinusLogProbMetric: 47.1878 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 239/1000
2023-10-28 04:37:12.076 
Epoch 239/1000 
	 loss: 45.9063, MinusLogProbMetric: 45.9063, val_loss: 45.8834, val_MinusLogProbMetric: 45.8834

Epoch 239: val_loss did not improve from 45.77804
196/196 - 33s - loss: 45.9063 - MinusLogProbMetric: 45.9063 - val_loss: 45.8834 - val_MinusLogProbMetric: 45.8834 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 240/1000
2023-10-28 04:37:45.297 
Epoch 240/1000 
	 loss: 46.1690, MinusLogProbMetric: 46.1690, val_loss: 47.9876, val_MinusLogProbMetric: 47.9876

Epoch 240: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.1690 - MinusLogProbMetric: 46.1690 - val_loss: 47.9876 - val_MinusLogProbMetric: 47.9876 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 241/1000
2023-10-28 04:38:18.652 
Epoch 241/1000 
	 loss: 45.8072, MinusLogProbMetric: 45.8072, val_loss: 46.7504, val_MinusLogProbMetric: 46.7504

Epoch 241: val_loss did not improve from 45.77804
196/196 - 33s - loss: 45.8072 - MinusLogProbMetric: 45.8072 - val_loss: 46.7504 - val_MinusLogProbMetric: 46.7504 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 242/1000
2023-10-28 04:38:52.138 
Epoch 242/1000 
	 loss: 46.0806, MinusLogProbMetric: 46.0806, val_loss: 46.6164, val_MinusLogProbMetric: 46.6164

Epoch 242: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.0806 - MinusLogProbMetric: 46.0806 - val_loss: 46.6164 - val_MinusLogProbMetric: 46.6164 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 243/1000
2023-10-28 04:39:25.453 
Epoch 243/1000 
	 loss: 46.3274, MinusLogProbMetric: 46.3274, val_loss: 45.9525, val_MinusLogProbMetric: 45.9525

Epoch 243: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.3274 - MinusLogProbMetric: 46.3274 - val_loss: 45.9525 - val_MinusLogProbMetric: 45.9525 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 244/1000
2023-10-28 04:39:58.786 
Epoch 244/1000 
	 loss: 46.0628, MinusLogProbMetric: 46.0628, val_loss: 46.1230, val_MinusLogProbMetric: 46.1230

Epoch 244: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.0628 - MinusLogProbMetric: 46.0628 - val_loss: 46.1230 - val_MinusLogProbMetric: 46.1230 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 245/1000
2023-10-28 04:40:32.122 
Epoch 245/1000 
	 loss: 46.1390, MinusLogProbMetric: 46.1390, val_loss: 47.2495, val_MinusLogProbMetric: 47.2495

Epoch 245: val_loss did not improve from 45.77804
196/196 - 33s - loss: 46.1390 - MinusLogProbMetric: 46.1390 - val_loss: 47.2495 - val_MinusLogProbMetric: 47.2495 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 246/1000
2023-10-28 04:41:05.642 
Epoch 246/1000 
	 loss: 45.9052, MinusLogProbMetric: 45.9052, val_loss: 45.9597, val_MinusLogProbMetric: 45.9597

Epoch 246: val_loss did not improve from 45.77804
196/196 - 34s - loss: 45.9052 - MinusLogProbMetric: 45.9052 - val_loss: 45.9597 - val_MinusLogProbMetric: 45.9597 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 247/1000
2023-10-28 04:41:38.932 
Epoch 247/1000 
	 loss: 45.9686, MinusLogProbMetric: 45.9686, val_loss: 47.8001, val_MinusLogProbMetric: 47.8001

Epoch 247: val_loss did not improve from 45.77804
196/196 - 33s - loss: 45.9686 - MinusLogProbMetric: 45.9686 - val_loss: 47.8001 - val_MinusLogProbMetric: 47.8001 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 248/1000
2023-10-28 04:42:11.995 
Epoch 248/1000 
	 loss: 45.7721, MinusLogProbMetric: 45.7721, val_loss: 47.2837, val_MinusLogProbMetric: 47.2837

Epoch 248: val_loss did not improve from 45.77804
196/196 - 33s - loss: 45.7721 - MinusLogProbMetric: 45.7721 - val_loss: 47.2837 - val_MinusLogProbMetric: 47.2837 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 249/1000
2023-10-28 04:42:45.161 
Epoch 249/1000 
	 loss: 45.9338, MinusLogProbMetric: 45.9338, val_loss: 45.7755, val_MinusLogProbMetric: 45.7755

Epoch 249: val_loss improved from 45.77804 to 45.77548, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 45.9338 - MinusLogProbMetric: 45.9338 - val_loss: 45.7755 - val_MinusLogProbMetric: 45.7755 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 250/1000
2023-10-28 04:43:18.859 
Epoch 250/1000 
	 loss: 45.7774, MinusLogProbMetric: 45.7774, val_loss: 45.7044, val_MinusLogProbMetric: 45.7044

Epoch 250: val_loss improved from 45.77548 to 45.70440, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 45.7774 - MinusLogProbMetric: 45.7774 - val_loss: 45.7044 - val_MinusLogProbMetric: 45.7044 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 251/1000
2023-10-28 04:43:52.415 
Epoch 251/1000 
	 loss: 45.7396, MinusLogProbMetric: 45.7396, val_loss: 46.4450, val_MinusLogProbMetric: 46.4450

Epoch 251: val_loss did not improve from 45.70440
196/196 - 33s - loss: 45.7396 - MinusLogProbMetric: 45.7396 - val_loss: 46.4450 - val_MinusLogProbMetric: 46.4450 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 252/1000
2023-10-28 04:44:25.733 
Epoch 252/1000 
	 loss: 46.1869, MinusLogProbMetric: 46.1869, val_loss: 46.6580, val_MinusLogProbMetric: 46.6580

Epoch 252: val_loss did not improve from 45.70440
196/196 - 33s - loss: 46.1869 - MinusLogProbMetric: 46.1869 - val_loss: 46.6580 - val_MinusLogProbMetric: 46.6580 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 253/1000
2023-10-28 04:44:58.945 
Epoch 253/1000 
	 loss: 45.7821, MinusLogProbMetric: 45.7821, val_loss: 45.6670, val_MinusLogProbMetric: 45.6670

Epoch 253: val_loss improved from 45.70440 to 45.66703, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 45.7821 - MinusLogProbMetric: 45.7821 - val_loss: 45.6670 - val_MinusLogProbMetric: 45.6670 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 254/1000
2023-10-28 04:45:32.720 
Epoch 254/1000 
	 loss: 46.0602, MinusLogProbMetric: 46.0602, val_loss: 46.4298, val_MinusLogProbMetric: 46.4298

Epoch 254: val_loss did not improve from 45.66703
196/196 - 33s - loss: 46.0602 - MinusLogProbMetric: 46.0602 - val_loss: 46.4298 - val_MinusLogProbMetric: 46.4298 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 255/1000
2023-10-28 04:46:05.994 
Epoch 255/1000 
	 loss: 45.8001, MinusLogProbMetric: 45.8001, val_loss: 48.1419, val_MinusLogProbMetric: 48.1419

Epoch 255: val_loss did not improve from 45.66703
196/196 - 33s - loss: 45.8001 - MinusLogProbMetric: 45.8001 - val_loss: 48.1419 - val_MinusLogProbMetric: 48.1419 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 256/1000
2023-10-28 04:46:39.245 
Epoch 256/1000 
	 loss: 45.8837, MinusLogProbMetric: 45.8837, val_loss: 46.8069, val_MinusLogProbMetric: 46.8069

Epoch 256: val_loss did not improve from 45.66703
196/196 - 33s - loss: 45.8837 - MinusLogProbMetric: 45.8837 - val_loss: 46.8069 - val_MinusLogProbMetric: 46.8069 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 257/1000
2023-10-28 04:47:12.588 
Epoch 257/1000 
	 loss: 46.0881, MinusLogProbMetric: 46.0881, val_loss: 47.3155, val_MinusLogProbMetric: 47.3155

Epoch 257: val_loss did not improve from 45.66703
196/196 - 33s - loss: 46.0881 - MinusLogProbMetric: 46.0881 - val_loss: 47.3155 - val_MinusLogProbMetric: 47.3155 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 258/1000
2023-10-28 04:47:45.501 
Epoch 258/1000 
	 loss: 46.5860, MinusLogProbMetric: 46.5860, val_loss: 45.5942, val_MinusLogProbMetric: 45.5942

Epoch 258: val_loss improved from 45.66703 to 45.59417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 46.5860 - MinusLogProbMetric: 46.5860 - val_loss: 45.5942 - val_MinusLogProbMetric: 45.5942 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 259/1000
2023-10-28 04:48:19.045 
Epoch 259/1000 
	 loss: 46.0243, MinusLogProbMetric: 46.0243, val_loss: 45.7106, val_MinusLogProbMetric: 45.7106

Epoch 259: val_loss did not improve from 45.59417
196/196 - 33s - loss: 46.0243 - MinusLogProbMetric: 46.0243 - val_loss: 45.7106 - val_MinusLogProbMetric: 45.7106 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 260/1000
2023-10-28 04:48:51.923 
Epoch 260/1000 
	 loss: 45.8294, MinusLogProbMetric: 45.8294, val_loss: 46.3125, val_MinusLogProbMetric: 46.3125

Epoch 260: val_loss did not improve from 45.59417
196/196 - 33s - loss: 45.8294 - MinusLogProbMetric: 45.8294 - val_loss: 46.3125 - val_MinusLogProbMetric: 46.3125 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 261/1000
2023-10-28 04:49:24.937 
Epoch 261/1000 
	 loss: 45.6608, MinusLogProbMetric: 45.6608, val_loss: 46.2927, val_MinusLogProbMetric: 46.2927

Epoch 261: val_loss did not improve from 45.59417
196/196 - 33s - loss: 45.6608 - MinusLogProbMetric: 45.6608 - val_loss: 46.2927 - val_MinusLogProbMetric: 46.2927 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 262/1000
2023-10-28 04:49:58.185 
Epoch 262/1000 
	 loss: 46.1218, MinusLogProbMetric: 46.1218, val_loss: 46.2810, val_MinusLogProbMetric: 46.2810

Epoch 262: val_loss did not improve from 45.59417
196/196 - 33s - loss: 46.1218 - MinusLogProbMetric: 46.1218 - val_loss: 46.2810 - val_MinusLogProbMetric: 46.2810 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 263/1000
2023-10-28 04:50:31.546 
Epoch 263/1000 
	 loss: 45.6922, MinusLogProbMetric: 45.6922, val_loss: 46.0591, val_MinusLogProbMetric: 46.0591

Epoch 263: val_loss did not improve from 45.59417
196/196 - 33s - loss: 45.6922 - MinusLogProbMetric: 45.6922 - val_loss: 46.0591 - val_MinusLogProbMetric: 46.0591 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 264/1000
2023-10-28 04:51:05.056 
Epoch 264/1000 
	 loss: 46.3365, MinusLogProbMetric: 46.3365, val_loss: 46.2271, val_MinusLogProbMetric: 46.2271

Epoch 264: val_loss did not improve from 45.59417
196/196 - 34s - loss: 46.3365 - MinusLogProbMetric: 46.3365 - val_loss: 46.2271 - val_MinusLogProbMetric: 46.2271 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 265/1000
2023-10-28 04:51:38.402 
Epoch 265/1000 
	 loss: 45.7271, MinusLogProbMetric: 45.7271, val_loss: 47.7663, val_MinusLogProbMetric: 47.7663

Epoch 265: val_loss did not improve from 45.59417
196/196 - 33s - loss: 45.7271 - MinusLogProbMetric: 45.7271 - val_loss: 47.7663 - val_MinusLogProbMetric: 47.7663 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 266/1000
2023-10-28 04:52:11.469 
Epoch 266/1000 
	 loss: 45.6759, MinusLogProbMetric: 45.6759, val_loss: 45.6002, val_MinusLogProbMetric: 45.6002

Epoch 266: val_loss did not improve from 45.59417
196/196 - 33s - loss: 45.6759 - MinusLogProbMetric: 45.6759 - val_loss: 45.6002 - val_MinusLogProbMetric: 45.6002 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 267/1000
2023-10-28 04:52:42.770 
Epoch 267/1000 
	 loss: 45.9132, MinusLogProbMetric: 45.9132, val_loss: 46.1020, val_MinusLogProbMetric: 46.1020

Epoch 267: val_loss did not improve from 45.59417
196/196 - 31s - loss: 45.9132 - MinusLogProbMetric: 45.9132 - val_loss: 46.1020 - val_MinusLogProbMetric: 46.1020 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 268/1000
2023-10-28 04:53:15.669 
Epoch 268/1000 
	 loss: 45.8954, MinusLogProbMetric: 45.8954, val_loss: 45.3421, val_MinusLogProbMetric: 45.3421

Epoch 268: val_loss improved from 45.59417 to 45.34208, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 45.8954 - MinusLogProbMetric: 45.8954 - val_loss: 45.3421 - val_MinusLogProbMetric: 45.3421 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 269/1000
2023-10-28 04:53:49.089 
Epoch 269/1000 
	 loss: 45.7645, MinusLogProbMetric: 45.7645, val_loss: 45.6174, val_MinusLogProbMetric: 45.6174

Epoch 269: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.7645 - MinusLogProbMetric: 45.7645 - val_loss: 45.6174 - val_MinusLogProbMetric: 45.6174 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 270/1000
2023-10-28 04:54:22.360 
Epoch 270/1000 
	 loss: 46.1433, MinusLogProbMetric: 46.1433, val_loss: 49.2879, val_MinusLogProbMetric: 49.2879

Epoch 270: val_loss did not improve from 45.34208
196/196 - 33s - loss: 46.1433 - MinusLogProbMetric: 46.1433 - val_loss: 49.2879 - val_MinusLogProbMetric: 49.2879 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 271/1000
2023-10-28 04:54:55.216 
Epoch 271/1000 
	 loss: 46.1516, MinusLogProbMetric: 46.1516, val_loss: 45.4723, val_MinusLogProbMetric: 45.4723

Epoch 271: val_loss did not improve from 45.34208
196/196 - 33s - loss: 46.1516 - MinusLogProbMetric: 46.1516 - val_loss: 45.4723 - val_MinusLogProbMetric: 45.4723 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 272/1000
2023-10-28 04:55:28.478 
Epoch 272/1000 
	 loss: 45.6167, MinusLogProbMetric: 45.6167, val_loss: 45.7833, val_MinusLogProbMetric: 45.7833

Epoch 272: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.6167 - MinusLogProbMetric: 45.6167 - val_loss: 45.7833 - val_MinusLogProbMetric: 45.7833 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 273/1000
2023-10-28 04:56:01.203 
Epoch 273/1000 
	 loss: 45.6486, MinusLogProbMetric: 45.6486, val_loss: 46.2386, val_MinusLogProbMetric: 46.2386

Epoch 273: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.6486 - MinusLogProbMetric: 45.6486 - val_loss: 46.2386 - val_MinusLogProbMetric: 46.2386 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 274/1000
2023-10-28 04:56:32.025 
Epoch 274/1000 
	 loss: 45.5924, MinusLogProbMetric: 45.5924, val_loss: 46.7888, val_MinusLogProbMetric: 46.7888

Epoch 274: val_loss did not improve from 45.34208
196/196 - 31s - loss: 45.5924 - MinusLogProbMetric: 45.5924 - val_loss: 46.7888 - val_MinusLogProbMetric: 46.7888 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 275/1000
2023-10-28 04:57:02.740 
Epoch 275/1000 
	 loss: 45.7882, MinusLogProbMetric: 45.7882, val_loss: 46.5389, val_MinusLogProbMetric: 46.5389

Epoch 275: val_loss did not improve from 45.34208
196/196 - 31s - loss: 45.7882 - MinusLogProbMetric: 45.7882 - val_loss: 46.5389 - val_MinusLogProbMetric: 46.5389 - lr: 3.3333e-04 - 31s/epoch - 157ms/step
Epoch 276/1000
2023-10-28 04:57:35.831 
Epoch 276/1000 
	 loss: 45.4256, MinusLogProbMetric: 45.4256, val_loss: 46.9145, val_MinusLogProbMetric: 46.9145

Epoch 276: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.4256 - MinusLogProbMetric: 45.4256 - val_loss: 46.9145 - val_MinusLogProbMetric: 46.9145 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 277/1000
2023-10-28 04:58:08.917 
Epoch 277/1000 
	 loss: 45.5540, MinusLogProbMetric: 45.5540, val_loss: 46.5090, val_MinusLogProbMetric: 46.5090

Epoch 277: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.5540 - MinusLogProbMetric: 45.5540 - val_loss: 46.5090 - val_MinusLogProbMetric: 46.5090 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 278/1000
2023-10-28 04:58:41.993 
Epoch 278/1000 
	 loss: 45.6598, MinusLogProbMetric: 45.6598, val_loss: 45.7132, val_MinusLogProbMetric: 45.7132

Epoch 278: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.6598 - MinusLogProbMetric: 45.6598 - val_loss: 45.7132 - val_MinusLogProbMetric: 45.7132 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 279/1000
2023-10-28 04:59:15.341 
Epoch 279/1000 
	 loss: 45.6729, MinusLogProbMetric: 45.6729, val_loss: 47.9180, val_MinusLogProbMetric: 47.9180

Epoch 279: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.6729 - MinusLogProbMetric: 45.6729 - val_loss: 47.9180 - val_MinusLogProbMetric: 47.9180 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 280/1000
2023-10-28 04:59:48.559 
Epoch 280/1000 
	 loss: 45.4259, MinusLogProbMetric: 45.4259, val_loss: 45.4277, val_MinusLogProbMetric: 45.4277

Epoch 280: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.4259 - MinusLogProbMetric: 45.4259 - val_loss: 45.4277 - val_MinusLogProbMetric: 45.4277 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 281/1000
2023-10-28 05:00:21.691 
Epoch 281/1000 
	 loss: 45.8401, MinusLogProbMetric: 45.8401, val_loss: 45.4498, val_MinusLogProbMetric: 45.4498

Epoch 281: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.8401 - MinusLogProbMetric: 45.8401 - val_loss: 45.4498 - val_MinusLogProbMetric: 45.4498 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 282/1000
2023-10-28 05:00:54.890 
Epoch 282/1000 
	 loss: 45.6023, MinusLogProbMetric: 45.6023, val_loss: 47.4672, val_MinusLogProbMetric: 47.4672

Epoch 282: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.6023 - MinusLogProbMetric: 45.6023 - val_loss: 47.4672 - val_MinusLogProbMetric: 47.4672 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 283/1000
2023-10-28 05:01:28.382 
Epoch 283/1000 
	 loss: 45.8420, MinusLogProbMetric: 45.8420, val_loss: 48.3072, val_MinusLogProbMetric: 48.3072

Epoch 283: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.8420 - MinusLogProbMetric: 45.8420 - val_loss: 48.3072 - val_MinusLogProbMetric: 48.3072 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 284/1000
2023-10-28 05:02:01.745 
Epoch 284/1000 
	 loss: 45.6522, MinusLogProbMetric: 45.6522, val_loss: 46.0429, val_MinusLogProbMetric: 46.0429

Epoch 284: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.6522 - MinusLogProbMetric: 45.6522 - val_loss: 46.0429 - val_MinusLogProbMetric: 46.0429 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 285/1000
2023-10-28 05:02:34.932 
Epoch 285/1000 
	 loss: 45.7576, MinusLogProbMetric: 45.7576, val_loss: 49.3742, val_MinusLogProbMetric: 49.3742

Epoch 285: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.7576 - MinusLogProbMetric: 45.7576 - val_loss: 49.3742 - val_MinusLogProbMetric: 49.3742 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 286/1000
2023-10-28 05:03:07.972 
Epoch 286/1000 
	 loss: 46.5107, MinusLogProbMetric: 46.5107, val_loss: 46.4033, val_MinusLogProbMetric: 46.4033

Epoch 286: val_loss did not improve from 45.34208
196/196 - 33s - loss: 46.5107 - MinusLogProbMetric: 46.5107 - val_loss: 46.4033 - val_MinusLogProbMetric: 46.4033 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 287/1000
2023-10-28 05:03:41.334 
Epoch 287/1000 
	 loss: 45.3198, MinusLogProbMetric: 45.3198, val_loss: 45.7126, val_MinusLogProbMetric: 45.7126

Epoch 287: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.3198 - MinusLogProbMetric: 45.3198 - val_loss: 45.7126 - val_MinusLogProbMetric: 45.7126 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 288/1000
2023-10-28 05:04:14.749 
Epoch 288/1000 
	 loss: 45.5053, MinusLogProbMetric: 45.5053, val_loss: 46.9664, val_MinusLogProbMetric: 46.9664

Epoch 288: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.5053 - MinusLogProbMetric: 45.5053 - val_loss: 46.9664 - val_MinusLogProbMetric: 46.9664 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 289/1000
2023-10-28 05:04:47.935 
Epoch 289/1000 
	 loss: 46.1327, MinusLogProbMetric: 46.1327, val_loss: 46.1000, val_MinusLogProbMetric: 46.1000

Epoch 289: val_loss did not improve from 45.34208
196/196 - 33s - loss: 46.1327 - MinusLogProbMetric: 46.1327 - val_loss: 46.1000 - val_MinusLogProbMetric: 46.1000 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 290/1000
2023-10-28 05:05:21.179 
Epoch 290/1000 
	 loss: 45.6560, MinusLogProbMetric: 45.6560, val_loss: 46.7247, val_MinusLogProbMetric: 46.7247

Epoch 290: val_loss did not improve from 45.34208
196/196 - 33s - loss: 45.6560 - MinusLogProbMetric: 45.6560 - val_loss: 46.7247 - val_MinusLogProbMetric: 46.7247 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 291/1000
2023-10-28 05:05:54.377 
Epoch 291/1000 
	 loss: 45.2402, MinusLogProbMetric: 45.2402, val_loss: 44.9808, val_MinusLogProbMetric: 44.9808

Epoch 291: val_loss improved from 45.34208 to 44.98083, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 45.2402 - MinusLogProbMetric: 45.2402 - val_loss: 44.9808 - val_MinusLogProbMetric: 44.9808 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 292/1000
2023-10-28 05:06:27.878 
Epoch 292/1000 
	 loss: 45.6811, MinusLogProbMetric: 45.6811, val_loss: 46.4566, val_MinusLogProbMetric: 46.4566

Epoch 292: val_loss did not improve from 44.98083
196/196 - 33s - loss: 45.6811 - MinusLogProbMetric: 45.6811 - val_loss: 46.4566 - val_MinusLogProbMetric: 46.4566 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 293/1000
2023-10-28 05:07:00.933 
Epoch 293/1000 
	 loss: 45.8282, MinusLogProbMetric: 45.8282, val_loss: 45.1615, val_MinusLogProbMetric: 45.1615

Epoch 293: val_loss did not improve from 44.98083
196/196 - 33s - loss: 45.8282 - MinusLogProbMetric: 45.8282 - val_loss: 45.1615 - val_MinusLogProbMetric: 45.1615 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 294/1000
2023-10-28 05:07:34.267 
Epoch 294/1000 
	 loss: 45.8864, MinusLogProbMetric: 45.8864, val_loss: 46.6161, val_MinusLogProbMetric: 46.6161

Epoch 294: val_loss did not improve from 44.98083
196/196 - 33s - loss: 45.8864 - MinusLogProbMetric: 45.8864 - val_loss: 46.6161 - val_MinusLogProbMetric: 46.6161 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 295/1000
2023-10-28 05:08:07.481 
Epoch 295/1000 
	 loss: 45.3448, MinusLogProbMetric: 45.3448, val_loss: 46.8432, val_MinusLogProbMetric: 46.8432

Epoch 295: val_loss did not improve from 44.98083
196/196 - 33s - loss: 45.3448 - MinusLogProbMetric: 45.3448 - val_loss: 46.8432 - val_MinusLogProbMetric: 46.8432 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 296/1000
2023-10-28 05:08:41.043 
Epoch 296/1000 
	 loss: 45.7821, MinusLogProbMetric: 45.7821, val_loss: 47.3750, val_MinusLogProbMetric: 47.3750

Epoch 296: val_loss did not improve from 44.98083
196/196 - 34s - loss: 45.7821 - MinusLogProbMetric: 45.7821 - val_loss: 47.3750 - val_MinusLogProbMetric: 47.3750 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 297/1000
2023-10-28 05:09:14.514 
Epoch 297/1000 
	 loss: 45.5982, MinusLogProbMetric: 45.5982, val_loss: 47.2238, val_MinusLogProbMetric: 47.2238

Epoch 297: val_loss did not improve from 44.98083
196/196 - 33s - loss: 45.5982 - MinusLogProbMetric: 45.5982 - val_loss: 47.2238 - val_MinusLogProbMetric: 47.2238 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 298/1000
2023-10-28 05:09:47.565 
Epoch 298/1000 
	 loss: 45.3549, MinusLogProbMetric: 45.3549, val_loss: 47.4063, val_MinusLogProbMetric: 47.4063

Epoch 298: val_loss did not improve from 44.98083
196/196 - 33s - loss: 45.3549 - MinusLogProbMetric: 45.3549 - val_loss: 47.4063 - val_MinusLogProbMetric: 47.4063 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 299/1000
2023-10-28 05:10:20.891 
Epoch 299/1000 
	 loss: 45.4563, MinusLogProbMetric: 45.4563, val_loss: 45.7609, val_MinusLogProbMetric: 45.7609

Epoch 299: val_loss did not improve from 44.98083
196/196 - 33s - loss: 45.4563 - MinusLogProbMetric: 45.4563 - val_loss: 45.7609 - val_MinusLogProbMetric: 45.7609 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 300/1000
2023-10-28 05:10:54.066 
Epoch 300/1000 
	 loss: 45.3869, MinusLogProbMetric: 45.3869, val_loss: 45.5946, val_MinusLogProbMetric: 45.5946

Epoch 300: val_loss did not improve from 44.98083
196/196 - 33s - loss: 45.3869 - MinusLogProbMetric: 45.3869 - val_loss: 45.5946 - val_MinusLogProbMetric: 45.5946 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 301/1000
2023-10-28 05:11:26.888 
Epoch 301/1000 
	 loss: 45.6600, MinusLogProbMetric: 45.6600, val_loss: 46.5054, val_MinusLogProbMetric: 46.5054

Epoch 301: val_loss did not improve from 44.98083
196/196 - 33s - loss: 45.6600 - MinusLogProbMetric: 45.6600 - val_loss: 46.5054 - val_MinusLogProbMetric: 46.5054 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 302/1000
2023-10-28 05:11:59.826 
Epoch 302/1000 
	 loss: 45.4779, MinusLogProbMetric: 45.4779, val_loss: 45.6493, val_MinusLogProbMetric: 45.6493

Epoch 302: val_loss did not improve from 44.98083
196/196 - 33s - loss: 45.4779 - MinusLogProbMetric: 45.4779 - val_loss: 45.6493 - val_MinusLogProbMetric: 45.6493 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 303/1000
2023-10-28 05:12:33.055 
Epoch 303/1000 
	 loss: 45.1800, MinusLogProbMetric: 45.1800, val_loss: 45.9378, val_MinusLogProbMetric: 45.9378

Epoch 303: val_loss did not improve from 44.98083
196/196 - 33s - loss: 45.1800 - MinusLogProbMetric: 45.1800 - val_loss: 45.9378 - val_MinusLogProbMetric: 45.9378 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 304/1000
2023-10-28 05:13:06.296 
Epoch 304/1000 
	 loss: 45.4150, MinusLogProbMetric: 45.4150, val_loss: 45.4301, val_MinusLogProbMetric: 45.4301

Epoch 304: val_loss did not improve from 44.98083
196/196 - 33s - loss: 45.4150 - MinusLogProbMetric: 45.4150 - val_loss: 45.4301 - val_MinusLogProbMetric: 45.4301 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 305/1000
2023-10-28 05:13:39.525 
Epoch 305/1000 
	 loss: 46.0542, MinusLogProbMetric: 46.0542, val_loss: 45.0823, val_MinusLogProbMetric: 45.0823

Epoch 305: val_loss did not improve from 44.98083
196/196 - 33s - loss: 46.0542 - MinusLogProbMetric: 46.0542 - val_loss: 45.0823 - val_MinusLogProbMetric: 45.0823 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 306/1000
2023-10-28 05:14:12.278 
Epoch 306/1000 
	 loss: 45.3987, MinusLogProbMetric: 45.3987, val_loss: 47.9687, val_MinusLogProbMetric: 47.9687

Epoch 306: val_loss did not improve from 44.98083
196/196 - 33s - loss: 45.3987 - MinusLogProbMetric: 45.3987 - val_loss: 47.9687 - val_MinusLogProbMetric: 47.9687 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 307/1000
2023-10-28 05:14:45.814 
Epoch 307/1000 
	 loss: 45.4385, MinusLogProbMetric: 45.4385, val_loss: 44.9708, val_MinusLogProbMetric: 44.9708

Epoch 307: val_loss improved from 44.98083 to 44.97080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 45.4385 - MinusLogProbMetric: 45.4385 - val_loss: 44.9708 - val_MinusLogProbMetric: 44.9708 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 308/1000
2023-10-28 05:15:19.535 
Epoch 308/1000 
	 loss: 45.4516, MinusLogProbMetric: 45.4516, val_loss: 46.7637, val_MinusLogProbMetric: 46.7637

Epoch 308: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.4516 - MinusLogProbMetric: 45.4516 - val_loss: 46.7637 - val_MinusLogProbMetric: 46.7637 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 309/1000
2023-10-28 05:15:52.732 
Epoch 309/1000 
	 loss: 45.3811, MinusLogProbMetric: 45.3811, val_loss: 45.9084, val_MinusLogProbMetric: 45.9084

Epoch 309: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.3811 - MinusLogProbMetric: 45.3811 - val_loss: 45.9084 - val_MinusLogProbMetric: 45.9084 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 310/1000
2023-10-28 05:16:25.948 
Epoch 310/1000 
	 loss: 45.6816, MinusLogProbMetric: 45.6816, val_loss: 45.3373, val_MinusLogProbMetric: 45.3373

Epoch 310: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.6816 - MinusLogProbMetric: 45.6816 - val_loss: 45.3373 - val_MinusLogProbMetric: 45.3373 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 311/1000
2023-10-28 05:16:59.004 
Epoch 311/1000 
	 loss: 45.2179, MinusLogProbMetric: 45.2179, val_loss: 45.2816, val_MinusLogProbMetric: 45.2816

Epoch 311: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.2179 - MinusLogProbMetric: 45.2179 - val_loss: 45.2816 - val_MinusLogProbMetric: 45.2816 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 312/1000
2023-10-28 05:17:31.978 
Epoch 312/1000 
	 loss: 45.1773, MinusLogProbMetric: 45.1773, val_loss: 46.1939, val_MinusLogProbMetric: 46.1939

Epoch 312: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.1773 - MinusLogProbMetric: 45.1773 - val_loss: 46.1939 - val_MinusLogProbMetric: 46.1939 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 313/1000
2023-10-28 05:18:04.904 
Epoch 313/1000 
	 loss: 45.2121, MinusLogProbMetric: 45.2121, val_loss: 45.6053, val_MinusLogProbMetric: 45.6053

Epoch 313: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.2121 - MinusLogProbMetric: 45.2121 - val_loss: 45.6053 - val_MinusLogProbMetric: 45.6053 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 314/1000
2023-10-28 05:18:38.051 
Epoch 314/1000 
	 loss: 45.2778, MinusLogProbMetric: 45.2778, val_loss: 47.1824, val_MinusLogProbMetric: 47.1824

Epoch 314: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.2778 - MinusLogProbMetric: 45.2778 - val_loss: 47.1824 - val_MinusLogProbMetric: 47.1824 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 315/1000
2023-10-28 05:19:11.256 
Epoch 315/1000 
	 loss: 45.8201, MinusLogProbMetric: 45.8201, val_loss: 46.2814, val_MinusLogProbMetric: 46.2814

Epoch 315: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.8201 - MinusLogProbMetric: 45.8201 - val_loss: 46.2814 - val_MinusLogProbMetric: 46.2814 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 316/1000
2023-10-28 05:19:44.604 
Epoch 316/1000 
	 loss: 45.2484, MinusLogProbMetric: 45.2484, val_loss: 45.3443, val_MinusLogProbMetric: 45.3443

Epoch 316: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.2484 - MinusLogProbMetric: 45.2484 - val_loss: 45.3443 - val_MinusLogProbMetric: 45.3443 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 317/1000
2023-10-28 05:20:17.781 
Epoch 317/1000 
	 loss: 45.7909, MinusLogProbMetric: 45.7909, val_loss: 46.0548, val_MinusLogProbMetric: 46.0548

Epoch 317: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.7909 - MinusLogProbMetric: 45.7909 - val_loss: 46.0548 - val_MinusLogProbMetric: 46.0548 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 318/1000
2023-10-28 05:20:50.347 
Epoch 318/1000 
	 loss: 45.1998, MinusLogProbMetric: 45.1998, val_loss: 45.7054, val_MinusLogProbMetric: 45.7054

Epoch 318: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.1998 - MinusLogProbMetric: 45.1998 - val_loss: 45.7054 - val_MinusLogProbMetric: 45.7054 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 319/1000
2023-10-28 05:21:23.712 
Epoch 319/1000 
	 loss: 45.2989, MinusLogProbMetric: 45.2989, val_loss: 46.2295, val_MinusLogProbMetric: 46.2295

Epoch 319: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.2989 - MinusLogProbMetric: 45.2989 - val_loss: 46.2295 - val_MinusLogProbMetric: 46.2295 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 320/1000
2023-10-28 05:21:57.208 
Epoch 320/1000 
	 loss: 45.7332, MinusLogProbMetric: 45.7332, val_loss: 47.2147, val_MinusLogProbMetric: 47.2147

Epoch 320: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.7332 - MinusLogProbMetric: 45.7332 - val_loss: 47.2147 - val_MinusLogProbMetric: 47.2147 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 321/1000
2023-10-28 05:22:30.649 
Epoch 321/1000 
	 loss: 45.2455, MinusLogProbMetric: 45.2455, val_loss: 45.7949, val_MinusLogProbMetric: 45.7949

Epoch 321: val_loss did not improve from 44.97080
196/196 - 33s - loss: 45.2455 - MinusLogProbMetric: 45.2455 - val_loss: 45.7949 - val_MinusLogProbMetric: 45.7949 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 322/1000
2023-10-28 05:23:04.261 
Epoch 322/1000 
	 loss: 45.6732, MinusLogProbMetric: 45.6732, val_loss: 45.5917, val_MinusLogProbMetric: 45.5917

Epoch 322: val_loss did not improve from 44.97080
196/196 - 34s - loss: 45.6732 - MinusLogProbMetric: 45.6732 - val_loss: 45.5917 - val_MinusLogProbMetric: 45.5917 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 323/1000
2023-10-28 05:23:37.798 
Epoch 323/1000 
	 loss: 44.9696, MinusLogProbMetric: 44.9696, val_loss: 44.9259, val_MinusLogProbMetric: 44.9259

Epoch 323: val_loss improved from 44.97080 to 44.92590, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 44.9696 - MinusLogProbMetric: 44.9696 - val_loss: 44.9259 - val_MinusLogProbMetric: 44.9259 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 324/1000
2023-10-28 05:24:11.609 
Epoch 324/1000 
	 loss: 45.6793, MinusLogProbMetric: 45.6793, val_loss: 45.1815, val_MinusLogProbMetric: 45.1815

Epoch 324: val_loss did not improve from 44.92590
196/196 - 33s - loss: 45.6793 - MinusLogProbMetric: 45.6793 - val_loss: 45.1815 - val_MinusLogProbMetric: 45.1815 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 325/1000
2023-10-28 05:24:45.285 
Epoch 325/1000 
	 loss: 45.2181, MinusLogProbMetric: 45.2181, val_loss: 44.7329, val_MinusLogProbMetric: 44.7329

Epoch 325: val_loss improved from 44.92590 to 44.73289, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 45.2181 - MinusLogProbMetric: 45.2181 - val_loss: 44.7329 - val_MinusLogProbMetric: 44.7329 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 326/1000
2023-10-28 05:25:19.533 
Epoch 326/1000 
	 loss: 45.1309, MinusLogProbMetric: 45.1309, val_loss: 46.1443, val_MinusLogProbMetric: 46.1443

Epoch 326: val_loss did not improve from 44.73289
196/196 - 34s - loss: 45.1309 - MinusLogProbMetric: 45.1309 - val_loss: 46.1443 - val_MinusLogProbMetric: 46.1443 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 327/1000
2023-10-28 05:25:52.526 
Epoch 327/1000 
	 loss: 44.9665, MinusLogProbMetric: 44.9665, val_loss: 46.9836, val_MinusLogProbMetric: 46.9836

Epoch 327: val_loss did not improve from 44.73289
196/196 - 33s - loss: 44.9665 - MinusLogProbMetric: 44.9665 - val_loss: 46.9836 - val_MinusLogProbMetric: 46.9836 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 328/1000
2023-10-28 05:26:25.834 
Epoch 328/1000 
	 loss: 45.3172, MinusLogProbMetric: 45.3172, val_loss: 55.2341, val_MinusLogProbMetric: 55.2341

Epoch 328: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.3172 - MinusLogProbMetric: 45.3172 - val_loss: 55.2341 - val_MinusLogProbMetric: 55.2341 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 329/1000
2023-10-28 05:26:59.350 
Epoch 329/1000 
	 loss: 46.3330, MinusLogProbMetric: 46.3330, val_loss: 45.9690, val_MinusLogProbMetric: 45.9690

Epoch 329: val_loss did not improve from 44.73289
196/196 - 34s - loss: 46.3330 - MinusLogProbMetric: 46.3330 - val_loss: 45.9690 - val_MinusLogProbMetric: 45.9690 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 330/1000
2023-10-28 05:27:32.539 
Epoch 330/1000 
	 loss: 45.0448, MinusLogProbMetric: 45.0448, val_loss: 44.7695, val_MinusLogProbMetric: 44.7695

Epoch 330: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.0448 - MinusLogProbMetric: 45.0448 - val_loss: 44.7695 - val_MinusLogProbMetric: 44.7695 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 331/1000
2023-10-28 05:28:05.545 
Epoch 331/1000 
	 loss: 45.0904, MinusLogProbMetric: 45.0904, val_loss: 44.8661, val_MinusLogProbMetric: 44.8661

Epoch 331: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.0904 - MinusLogProbMetric: 45.0904 - val_loss: 44.8661 - val_MinusLogProbMetric: 44.8661 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 332/1000
2023-10-28 05:28:38.688 
Epoch 332/1000 
	 loss: 45.3087, MinusLogProbMetric: 45.3087, val_loss: 47.2418, val_MinusLogProbMetric: 47.2418

Epoch 332: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.3087 - MinusLogProbMetric: 45.3087 - val_loss: 47.2418 - val_MinusLogProbMetric: 47.2418 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 333/1000
2023-10-28 05:29:11.847 
Epoch 333/1000 
	 loss: 45.1065, MinusLogProbMetric: 45.1065, val_loss: 45.6465, val_MinusLogProbMetric: 45.6465

Epoch 333: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.1065 - MinusLogProbMetric: 45.1065 - val_loss: 45.6465 - val_MinusLogProbMetric: 45.6465 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 334/1000
2023-10-28 05:29:44.923 
Epoch 334/1000 
	 loss: 45.1008, MinusLogProbMetric: 45.1008, val_loss: 44.9189, val_MinusLogProbMetric: 44.9189

Epoch 334: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.1008 - MinusLogProbMetric: 45.1008 - val_loss: 44.9189 - val_MinusLogProbMetric: 44.9189 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 335/1000
2023-10-28 05:30:18.022 
Epoch 335/1000 
	 loss: 45.2099, MinusLogProbMetric: 45.2099, val_loss: 45.9949, val_MinusLogProbMetric: 45.9949

Epoch 335: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.2099 - MinusLogProbMetric: 45.2099 - val_loss: 45.9949 - val_MinusLogProbMetric: 45.9949 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 336/1000
2023-10-28 05:30:51.088 
Epoch 336/1000 
	 loss: 45.0180, MinusLogProbMetric: 45.0180, val_loss: 45.3169, val_MinusLogProbMetric: 45.3169

Epoch 336: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.0180 - MinusLogProbMetric: 45.0180 - val_loss: 45.3169 - val_MinusLogProbMetric: 45.3169 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 337/1000
2023-10-28 05:31:24.348 
Epoch 337/1000 
	 loss: 45.3501, MinusLogProbMetric: 45.3501, val_loss: 45.2241, val_MinusLogProbMetric: 45.2241

Epoch 337: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.3501 - MinusLogProbMetric: 45.3501 - val_loss: 45.2241 - val_MinusLogProbMetric: 45.2241 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 338/1000
2023-10-28 05:31:57.491 
Epoch 338/1000 
	 loss: 45.1252, MinusLogProbMetric: 45.1252, val_loss: 46.3623, val_MinusLogProbMetric: 46.3623

Epoch 338: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.1252 - MinusLogProbMetric: 45.1252 - val_loss: 46.3623 - val_MinusLogProbMetric: 46.3623 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 339/1000
2023-10-28 05:32:30.661 
Epoch 339/1000 
	 loss: 45.0047, MinusLogProbMetric: 45.0047, val_loss: 45.6676, val_MinusLogProbMetric: 45.6676

Epoch 339: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.0047 - MinusLogProbMetric: 45.0047 - val_loss: 45.6676 - val_MinusLogProbMetric: 45.6676 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 340/1000
2023-10-28 05:33:04.082 
Epoch 340/1000 
	 loss: 45.4342, MinusLogProbMetric: 45.4342, val_loss: 46.1618, val_MinusLogProbMetric: 46.1618

Epoch 340: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.4342 - MinusLogProbMetric: 45.4342 - val_loss: 46.1618 - val_MinusLogProbMetric: 46.1618 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 341/1000
2023-10-28 05:33:37.147 
Epoch 341/1000 
	 loss: 45.1717, MinusLogProbMetric: 45.1717, val_loss: 45.5096, val_MinusLogProbMetric: 45.5096

Epoch 341: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.1717 - MinusLogProbMetric: 45.1717 - val_loss: 45.5096 - val_MinusLogProbMetric: 45.5096 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 342/1000
2023-10-28 05:34:10.822 
Epoch 342/1000 
	 loss: 45.1585, MinusLogProbMetric: 45.1585, val_loss: 46.5565, val_MinusLogProbMetric: 46.5565

Epoch 342: val_loss did not improve from 44.73289
196/196 - 34s - loss: 45.1585 - MinusLogProbMetric: 45.1585 - val_loss: 46.5565 - val_MinusLogProbMetric: 46.5565 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 343/1000
2023-10-28 05:34:42.679 
Epoch 343/1000 
	 loss: 45.0797, MinusLogProbMetric: 45.0797, val_loss: 45.1435, val_MinusLogProbMetric: 45.1435

Epoch 343: val_loss did not improve from 44.73289
196/196 - 32s - loss: 45.0797 - MinusLogProbMetric: 45.0797 - val_loss: 45.1435 - val_MinusLogProbMetric: 45.1435 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 344/1000
2023-10-28 05:35:13.964 
Epoch 344/1000 
	 loss: 45.2070, MinusLogProbMetric: 45.2070, val_loss: 45.9748, val_MinusLogProbMetric: 45.9748

Epoch 344: val_loss did not improve from 44.73289
196/196 - 31s - loss: 45.2070 - MinusLogProbMetric: 45.2070 - val_loss: 45.9748 - val_MinusLogProbMetric: 45.9748 - lr: 3.3333e-04 - 31s/epoch - 160ms/step
Epoch 345/1000
2023-10-28 05:35:47.195 
Epoch 345/1000 
	 loss: 45.0009, MinusLogProbMetric: 45.0009, val_loss: 48.5114, val_MinusLogProbMetric: 48.5114

Epoch 345: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.0009 - MinusLogProbMetric: 45.0009 - val_loss: 48.5114 - val_MinusLogProbMetric: 48.5114 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 346/1000
2023-10-28 05:36:21.022 
Epoch 346/1000 
	 loss: 45.0594, MinusLogProbMetric: 45.0594, val_loss: 50.6006, val_MinusLogProbMetric: 50.6006

Epoch 346: val_loss did not improve from 44.73289
196/196 - 34s - loss: 45.0594 - MinusLogProbMetric: 45.0594 - val_loss: 50.6006 - val_MinusLogProbMetric: 50.6006 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 347/1000
2023-10-28 05:36:54.417 
Epoch 347/1000 
	 loss: 45.4666, MinusLogProbMetric: 45.4666, val_loss: 46.3842, val_MinusLogProbMetric: 46.3842

Epoch 347: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.4666 - MinusLogProbMetric: 45.4666 - val_loss: 46.3842 - val_MinusLogProbMetric: 46.3842 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 348/1000
2023-10-28 05:37:27.864 
Epoch 348/1000 
	 loss: 45.2431, MinusLogProbMetric: 45.2431, val_loss: 45.0834, val_MinusLogProbMetric: 45.0834

Epoch 348: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.2431 - MinusLogProbMetric: 45.2431 - val_loss: 45.0834 - val_MinusLogProbMetric: 45.0834 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 349/1000
2023-10-28 05:37:59.758 
Epoch 349/1000 
	 loss: 45.0818, MinusLogProbMetric: 45.0818, val_loss: 45.2622, val_MinusLogProbMetric: 45.2622

Epoch 349: val_loss did not improve from 44.73289
196/196 - 32s - loss: 45.0818 - MinusLogProbMetric: 45.0818 - val_loss: 45.2622 - val_MinusLogProbMetric: 45.2622 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 350/1000
2023-10-28 05:38:33.360 
Epoch 350/1000 
	 loss: 44.8619, MinusLogProbMetric: 44.8619, val_loss: 45.4381, val_MinusLogProbMetric: 45.4381

Epoch 350: val_loss did not improve from 44.73289
196/196 - 34s - loss: 44.8619 - MinusLogProbMetric: 44.8619 - val_loss: 45.4381 - val_MinusLogProbMetric: 45.4381 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 351/1000
2023-10-28 05:39:06.865 
Epoch 351/1000 
	 loss: 44.9004, MinusLogProbMetric: 44.9004, val_loss: 45.4809, val_MinusLogProbMetric: 45.4809

Epoch 351: val_loss did not improve from 44.73289
196/196 - 34s - loss: 44.9004 - MinusLogProbMetric: 44.9004 - val_loss: 45.4809 - val_MinusLogProbMetric: 45.4809 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 352/1000
2023-10-28 05:39:39.960 
Epoch 352/1000 
	 loss: 45.2511, MinusLogProbMetric: 45.2511, val_loss: 45.9351, val_MinusLogProbMetric: 45.9351

Epoch 352: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.2511 - MinusLogProbMetric: 45.2511 - val_loss: 45.9351 - val_MinusLogProbMetric: 45.9351 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 353/1000
2023-10-28 05:40:13.250 
Epoch 353/1000 
	 loss: 44.7784, MinusLogProbMetric: 44.7784, val_loss: 45.3044, val_MinusLogProbMetric: 45.3044

Epoch 353: val_loss did not improve from 44.73289
196/196 - 33s - loss: 44.7784 - MinusLogProbMetric: 44.7784 - val_loss: 45.3044 - val_MinusLogProbMetric: 45.3044 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 354/1000
2023-10-28 05:40:46.498 
Epoch 354/1000 
	 loss: 44.9797, MinusLogProbMetric: 44.9797, val_loss: 45.0204, val_MinusLogProbMetric: 45.0204

Epoch 354: val_loss did not improve from 44.73289
196/196 - 33s - loss: 44.9797 - MinusLogProbMetric: 44.9797 - val_loss: 45.0204 - val_MinusLogProbMetric: 45.0204 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 355/1000
2023-10-28 05:41:19.963 
Epoch 355/1000 
	 loss: 45.2489, MinusLogProbMetric: 45.2489, val_loss: 45.6256, val_MinusLogProbMetric: 45.6256

Epoch 355: val_loss did not improve from 44.73289
196/196 - 33s - loss: 45.2489 - MinusLogProbMetric: 45.2489 - val_loss: 45.6256 - val_MinusLogProbMetric: 45.6256 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 356/1000
2023-10-28 05:41:53.000 
Epoch 356/1000 
	 loss: 44.9881, MinusLogProbMetric: 44.9881, val_loss: 44.4920, val_MinusLogProbMetric: 44.4920

Epoch 356: val_loss improved from 44.73289 to 44.49201, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 44.9881 - MinusLogProbMetric: 44.9881 - val_loss: 44.4920 - val_MinusLogProbMetric: 44.4920 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 357/1000
2023-10-28 05:42:27.046 
Epoch 357/1000 
	 loss: 44.8473, MinusLogProbMetric: 44.8473, val_loss: 46.2309, val_MinusLogProbMetric: 46.2309

Epoch 357: val_loss did not improve from 44.49201
196/196 - 33s - loss: 44.8473 - MinusLogProbMetric: 44.8473 - val_loss: 46.2309 - val_MinusLogProbMetric: 46.2309 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 358/1000
2023-10-28 05:43:00.461 
Epoch 358/1000 
	 loss: 45.4604, MinusLogProbMetric: 45.4604, val_loss: 45.1254, val_MinusLogProbMetric: 45.1254

Epoch 358: val_loss did not improve from 44.49201
196/196 - 33s - loss: 45.4604 - MinusLogProbMetric: 45.4604 - val_loss: 45.1254 - val_MinusLogProbMetric: 45.1254 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 359/1000
2023-10-28 05:43:33.194 
Epoch 359/1000 
	 loss: 44.6627, MinusLogProbMetric: 44.6627, val_loss: 44.5564, val_MinusLogProbMetric: 44.5564

Epoch 359: val_loss did not improve from 44.49201
196/196 - 33s - loss: 44.6627 - MinusLogProbMetric: 44.6627 - val_loss: 44.5564 - val_MinusLogProbMetric: 44.5564 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 360/1000
2023-10-28 05:44:06.478 
Epoch 360/1000 
	 loss: 45.0269, MinusLogProbMetric: 45.0269, val_loss: 44.3722, val_MinusLogProbMetric: 44.3722

Epoch 360: val_loss improved from 44.49201 to 44.37217, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 45.0269 - MinusLogProbMetric: 45.0269 - val_loss: 44.3722 - val_MinusLogProbMetric: 44.3722 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 361/1000
2023-10-28 05:44:40.036 
Epoch 361/1000 
	 loss: 44.8975, MinusLogProbMetric: 44.8975, val_loss: 47.1329, val_MinusLogProbMetric: 47.1329

Epoch 361: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.8975 - MinusLogProbMetric: 44.8975 - val_loss: 47.1329 - val_MinusLogProbMetric: 47.1329 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 362/1000
2023-10-28 05:45:13.369 
Epoch 362/1000 
	 loss: 45.7826, MinusLogProbMetric: 45.7826, val_loss: 49.9069, val_MinusLogProbMetric: 49.9069

Epoch 362: val_loss did not improve from 44.37217
196/196 - 33s - loss: 45.7826 - MinusLogProbMetric: 45.7826 - val_loss: 49.9069 - val_MinusLogProbMetric: 49.9069 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 363/1000
2023-10-28 05:45:46.815 
Epoch 363/1000 
	 loss: 45.2227, MinusLogProbMetric: 45.2227, val_loss: 44.9886, val_MinusLogProbMetric: 44.9886

Epoch 363: val_loss did not improve from 44.37217
196/196 - 33s - loss: 45.2227 - MinusLogProbMetric: 45.2227 - val_loss: 44.9886 - val_MinusLogProbMetric: 44.9886 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 364/1000
2023-10-28 05:46:20.554 
Epoch 364/1000 
	 loss: 44.6880, MinusLogProbMetric: 44.6880, val_loss: 44.8144, val_MinusLogProbMetric: 44.8144

Epoch 364: val_loss did not improve from 44.37217
196/196 - 34s - loss: 44.6880 - MinusLogProbMetric: 44.6880 - val_loss: 44.8144 - val_MinusLogProbMetric: 44.8144 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 365/1000
2023-10-28 05:46:54.329 
Epoch 365/1000 
	 loss: 45.4779, MinusLogProbMetric: 45.4779, val_loss: 49.6700, val_MinusLogProbMetric: 49.6700

Epoch 365: val_loss did not improve from 44.37217
196/196 - 34s - loss: 45.4779 - MinusLogProbMetric: 45.4779 - val_loss: 49.6700 - val_MinusLogProbMetric: 49.6700 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 366/1000
2023-10-28 05:47:28.026 
Epoch 366/1000 
	 loss: 45.0146, MinusLogProbMetric: 45.0146, val_loss: 45.4820, val_MinusLogProbMetric: 45.4820

Epoch 366: val_loss did not improve from 44.37217
196/196 - 34s - loss: 45.0146 - MinusLogProbMetric: 45.0146 - val_loss: 45.4820 - val_MinusLogProbMetric: 45.4820 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 367/1000
2023-10-28 05:48:01.378 
Epoch 367/1000 
	 loss: 45.1530, MinusLogProbMetric: 45.1530, val_loss: 45.4741, val_MinusLogProbMetric: 45.4741

Epoch 367: val_loss did not improve from 44.37217
196/196 - 33s - loss: 45.1530 - MinusLogProbMetric: 45.1530 - val_loss: 45.4741 - val_MinusLogProbMetric: 45.4741 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 368/1000
2023-10-28 05:48:34.968 
Epoch 368/1000 
	 loss: 44.6323, MinusLogProbMetric: 44.6323, val_loss: 45.0234, val_MinusLogProbMetric: 45.0234

Epoch 368: val_loss did not improve from 44.37217
196/196 - 34s - loss: 44.6323 - MinusLogProbMetric: 44.6323 - val_loss: 45.0234 - val_MinusLogProbMetric: 45.0234 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 369/1000
2023-10-28 05:49:08.504 
Epoch 369/1000 
	 loss: 45.1358, MinusLogProbMetric: 45.1358, val_loss: 44.9266, val_MinusLogProbMetric: 44.9266

Epoch 369: val_loss did not improve from 44.37217
196/196 - 34s - loss: 45.1358 - MinusLogProbMetric: 45.1358 - val_loss: 44.9266 - val_MinusLogProbMetric: 44.9266 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 370/1000
2023-10-28 05:49:41.782 
Epoch 370/1000 
	 loss: 44.8029, MinusLogProbMetric: 44.8029, val_loss: 45.6193, val_MinusLogProbMetric: 45.6193

Epoch 370: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.8029 - MinusLogProbMetric: 44.8029 - val_loss: 45.6193 - val_MinusLogProbMetric: 45.6193 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 371/1000
2023-10-28 05:50:15.342 
Epoch 371/1000 
	 loss: 44.7168, MinusLogProbMetric: 44.7168, val_loss: 46.2918, val_MinusLogProbMetric: 46.2918

Epoch 371: val_loss did not improve from 44.37217
196/196 - 34s - loss: 44.7168 - MinusLogProbMetric: 44.7168 - val_loss: 46.2918 - val_MinusLogProbMetric: 46.2918 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 372/1000
2023-10-28 05:50:48.349 
Epoch 372/1000 
	 loss: 44.8079, MinusLogProbMetric: 44.8079, val_loss: 45.8461, val_MinusLogProbMetric: 45.8461

Epoch 372: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.8079 - MinusLogProbMetric: 44.8079 - val_loss: 45.8461 - val_MinusLogProbMetric: 45.8461 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 373/1000
2023-10-28 05:51:21.704 
Epoch 373/1000 
	 loss: 45.1415, MinusLogProbMetric: 45.1415, val_loss: 45.0751, val_MinusLogProbMetric: 45.0751

Epoch 373: val_loss did not improve from 44.37217
196/196 - 33s - loss: 45.1415 - MinusLogProbMetric: 45.1415 - val_loss: 45.0751 - val_MinusLogProbMetric: 45.0751 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 374/1000
2023-10-28 05:51:55.017 
Epoch 374/1000 
	 loss: 45.3664, MinusLogProbMetric: 45.3664, val_loss: 45.1144, val_MinusLogProbMetric: 45.1144

Epoch 374: val_loss did not improve from 44.37217
196/196 - 33s - loss: 45.3664 - MinusLogProbMetric: 45.3664 - val_loss: 45.1144 - val_MinusLogProbMetric: 45.1144 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 375/1000
2023-10-28 05:52:28.521 
Epoch 375/1000 
	 loss: 45.1174, MinusLogProbMetric: 45.1174, val_loss: 45.2660, val_MinusLogProbMetric: 45.2660

Epoch 375: val_loss did not improve from 44.37217
196/196 - 33s - loss: 45.1174 - MinusLogProbMetric: 45.1174 - val_loss: 45.2660 - val_MinusLogProbMetric: 45.2660 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 376/1000
2023-10-28 05:53:01.864 
Epoch 376/1000 
	 loss: 44.6944, MinusLogProbMetric: 44.6944, val_loss: 44.9358, val_MinusLogProbMetric: 44.9358

Epoch 376: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.6944 - MinusLogProbMetric: 44.6944 - val_loss: 44.9358 - val_MinusLogProbMetric: 44.9358 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 377/1000
2023-10-28 05:53:35.020 
Epoch 377/1000 
	 loss: 44.8596, MinusLogProbMetric: 44.8596, val_loss: 45.7058, val_MinusLogProbMetric: 45.7058

Epoch 377: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.8596 - MinusLogProbMetric: 44.8596 - val_loss: 45.7058 - val_MinusLogProbMetric: 45.7058 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 378/1000
2023-10-28 05:54:08.126 
Epoch 378/1000 
	 loss: 44.9614, MinusLogProbMetric: 44.9614, val_loss: 46.8224, val_MinusLogProbMetric: 46.8224

Epoch 378: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.9614 - MinusLogProbMetric: 44.9614 - val_loss: 46.8224 - val_MinusLogProbMetric: 46.8224 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 379/1000
2023-10-28 05:54:41.631 
Epoch 379/1000 
	 loss: 44.7345, MinusLogProbMetric: 44.7345, val_loss: 45.9539, val_MinusLogProbMetric: 45.9539

Epoch 379: val_loss did not improve from 44.37217
196/196 - 34s - loss: 44.7345 - MinusLogProbMetric: 44.7345 - val_loss: 45.9539 - val_MinusLogProbMetric: 45.9539 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 380/1000
2023-10-28 05:55:14.753 
Epoch 380/1000 
	 loss: 45.1395, MinusLogProbMetric: 45.1395, val_loss: 45.4438, val_MinusLogProbMetric: 45.4438

Epoch 380: val_loss did not improve from 44.37217
196/196 - 33s - loss: 45.1395 - MinusLogProbMetric: 45.1395 - val_loss: 45.4438 - val_MinusLogProbMetric: 45.4438 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 381/1000
2023-10-28 05:55:48.048 
Epoch 381/1000 
	 loss: 44.7392, MinusLogProbMetric: 44.7392, val_loss: 50.2641, val_MinusLogProbMetric: 50.2641

Epoch 381: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.7392 - MinusLogProbMetric: 44.7392 - val_loss: 50.2641 - val_MinusLogProbMetric: 50.2641 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 382/1000
2023-10-28 05:56:21.604 
Epoch 382/1000 
	 loss: 45.0861, MinusLogProbMetric: 45.0861, val_loss: 45.3633, val_MinusLogProbMetric: 45.3633

Epoch 382: val_loss did not improve from 44.37217
196/196 - 34s - loss: 45.0861 - MinusLogProbMetric: 45.0861 - val_loss: 45.3633 - val_MinusLogProbMetric: 45.3633 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 383/1000
2023-10-28 05:56:54.912 
Epoch 383/1000 
	 loss: 45.1448, MinusLogProbMetric: 45.1448, val_loss: 45.9128, val_MinusLogProbMetric: 45.9128

Epoch 383: val_loss did not improve from 44.37217
196/196 - 33s - loss: 45.1448 - MinusLogProbMetric: 45.1448 - val_loss: 45.9128 - val_MinusLogProbMetric: 45.9128 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 384/1000
2023-10-28 05:57:28.696 
Epoch 384/1000 
	 loss: 44.6627, MinusLogProbMetric: 44.6627, val_loss: 45.6957, val_MinusLogProbMetric: 45.6957

Epoch 384: val_loss did not improve from 44.37217
196/196 - 34s - loss: 44.6627 - MinusLogProbMetric: 44.6627 - val_loss: 45.6957 - val_MinusLogProbMetric: 45.6957 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 385/1000
2023-10-28 05:58:02.014 
Epoch 385/1000 
	 loss: 44.7812, MinusLogProbMetric: 44.7812, val_loss: 45.6456, val_MinusLogProbMetric: 45.6456

Epoch 385: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.7812 - MinusLogProbMetric: 44.7812 - val_loss: 45.6456 - val_MinusLogProbMetric: 45.6456 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 386/1000
2023-10-28 05:58:35.324 
Epoch 386/1000 
	 loss: 45.0114, MinusLogProbMetric: 45.0114, val_loss: 46.6736, val_MinusLogProbMetric: 46.6736

Epoch 386: val_loss did not improve from 44.37217
196/196 - 33s - loss: 45.0114 - MinusLogProbMetric: 45.0114 - val_loss: 46.6736 - val_MinusLogProbMetric: 46.6736 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 387/1000
2023-10-28 05:59:08.586 
Epoch 387/1000 
	 loss: 44.6502, MinusLogProbMetric: 44.6502, val_loss: 47.1396, val_MinusLogProbMetric: 47.1396

Epoch 387: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.6502 - MinusLogProbMetric: 44.6502 - val_loss: 47.1396 - val_MinusLogProbMetric: 47.1396 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 388/1000
2023-10-28 05:59:42.392 
Epoch 388/1000 
	 loss: 45.2849, MinusLogProbMetric: 45.2849, val_loss: 45.2048, val_MinusLogProbMetric: 45.2048

Epoch 388: val_loss did not improve from 44.37217
196/196 - 34s - loss: 45.2849 - MinusLogProbMetric: 45.2849 - val_loss: 45.2048 - val_MinusLogProbMetric: 45.2048 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 389/1000
2023-10-28 06:00:16.568 
Epoch 389/1000 
	 loss: 44.7004, MinusLogProbMetric: 44.7004, val_loss: 46.0780, val_MinusLogProbMetric: 46.0780

Epoch 389: val_loss did not improve from 44.37217
196/196 - 34s - loss: 44.7004 - MinusLogProbMetric: 44.7004 - val_loss: 46.0780 - val_MinusLogProbMetric: 46.0780 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 390/1000
2023-10-28 06:00:49.900 
Epoch 390/1000 
	 loss: 44.8797, MinusLogProbMetric: 44.8797, val_loss: 47.4606, val_MinusLogProbMetric: 47.4606

Epoch 390: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.8797 - MinusLogProbMetric: 44.8797 - val_loss: 47.4606 - val_MinusLogProbMetric: 47.4606 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 391/1000
2023-10-28 06:01:23.238 
Epoch 391/1000 
	 loss: 44.9133, MinusLogProbMetric: 44.9133, val_loss: 45.5304, val_MinusLogProbMetric: 45.5304

Epoch 391: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.9133 - MinusLogProbMetric: 44.9133 - val_loss: 45.5304 - val_MinusLogProbMetric: 45.5304 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 392/1000
2023-10-28 06:01:56.691 
Epoch 392/1000 
	 loss: 44.8604, MinusLogProbMetric: 44.8604, val_loss: 47.0873, val_MinusLogProbMetric: 47.0873

Epoch 392: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.8604 - MinusLogProbMetric: 44.8604 - val_loss: 47.0873 - val_MinusLogProbMetric: 47.0873 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 393/1000
2023-10-28 06:02:30.140 
Epoch 393/1000 
	 loss: 44.4469, MinusLogProbMetric: 44.4469, val_loss: 45.0291, val_MinusLogProbMetric: 45.0291

Epoch 393: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.4469 - MinusLogProbMetric: 44.4469 - val_loss: 45.0291 - val_MinusLogProbMetric: 45.0291 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 394/1000
2023-10-28 06:03:03.616 
Epoch 394/1000 
	 loss: 44.9705, MinusLogProbMetric: 44.9705, val_loss: 45.1480, val_MinusLogProbMetric: 45.1480

Epoch 394: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.9705 - MinusLogProbMetric: 44.9705 - val_loss: 45.1480 - val_MinusLogProbMetric: 45.1480 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 395/1000
2023-10-28 06:03:37.205 
Epoch 395/1000 
	 loss: 44.6774, MinusLogProbMetric: 44.6774, val_loss: 46.6230, val_MinusLogProbMetric: 46.6230

Epoch 395: val_loss did not improve from 44.37217
196/196 - 34s - loss: 44.6774 - MinusLogProbMetric: 44.6774 - val_loss: 46.6230 - val_MinusLogProbMetric: 46.6230 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 396/1000
2023-10-28 06:04:10.344 
Epoch 396/1000 
	 loss: 44.7451, MinusLogProbMetric: 44.7451, val_loss: 45.4476, val_MinusLogProbMetric: 45.4476

Epoch 396: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.7451 - MinusLogProbMetric: 44.7451 - val_loss: 45.4476 - val_MinusLogProbMetric: 45.4476 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 397/1000
2023-10-28 06:04:43.517 
Epoch 397/1000 
	 loss: 44.7086, MinusLogProbMetric: 44.7086, val_loss: 44.9552, val_MinusLogProbMetric: 44.9552

Epoch 397: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.7086 - MinusLogProbMetric: 44.7086 - val_loss: 44.9552 - val_MinusLogProbMetric: 44.9552 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 398/1000
2023-10-28 06:05:16.933 
Epoch 398/1000 
	 loss: 44.4760, MinusLogProbMetric: 44.4760, val_loss: 45.9584, val_MinusLogProbMetric: 45.9584

Epoch 398: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.4760 - MinusLogProbMetric: 44.4760 - val_loss: 45.9584 - val_MinusLogProbMetric: 45.9584 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 399/1000
2023-10-28 06:05:50.333 
Epoch 399/1000 
	 loss: 44.7149, MinusLogProbMetric: 44.7149, val_loss: 49.6282, val_MinusLogProbMetric: 49.6282

Epoch 399: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.7149 - MinusLogProbMetric: 44.7149 - val_loss: 49.6282 - val_MinusLogProbMetric: 49.6282 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 400/1000
2023-10-28 06:06:23.588 
Epoch 400/1000 
	 loss: 44.9615, MinusLogProbMetric: 44.9615, val_loss: 45.2939, val_MinusLogProbMetric: 45.2939

Epoch 400: val_loss did not improve from 44.37217
196/196 - 33s - loss: 44.9615 - MinusLogProbMetric: 44.9615 - val_loss: 45.2939 - val_MinusLogProbMetric: 45.2939 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 401/1000
2023-10-28 06:06:56.760 
Epoch 401/1000 
	 loss: 44.4829, MinusLogProbMetric: 44.4829, val_loss: 44.3176, val_MinusLogProbMetric: 44.3176

Epoch 401: val_loss improved from 44.37217 to 44.31761, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 44.4829 - MinusLogProbMetric: 44.4829 - val_loss: 44.3176 - val_MinusLogProbMetric: 44.3176 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 402/1000
2023-10-28 06:07:30.505 
Epoch 402/1000 
	 loss: 44.5755, MinusLogProbMetric: 44.5755, val_loss: 44.5685, val_MinusLogProbMetric: 44.5685

Epoch 402: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.5755 - MinusLogProbMetric: 44.5755 - val_loss: 44.5685 - val_MinusLogProbMetric: 44.5685 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 403/1000
2023-10-28 06:08:04.200 
Epoch 403/1000 
	 loss: 44.7969, MinusLogProbMetric: 44.7969, val_loss: 46.0699, val_MinusLogProbMetric: 46.0699

Epoch 403: val_loss did not improve from 44.31761
196/196 - 34s - loss: 44.7969 - MinusLogProbMetric: 44.7969 - val_loss: 46.0699 - val_MinusLogProbMetric: 46.0699 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 404/1000
2023-10-28 06:08:37.489 
Epoch 404/1000 
	 loss: 44.7545, MinusLogProbMetric: 44.7545, val_loss: 45.0599, val_MinusLogProbMetric: 45.0599

Epoch 404: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.7545 - MinusLogProbMetric: 44.7545 - val_loss: 45.0599 - val_MinusLogProbMetric: 45.0599 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 405/1000
2023-10-28 06:09:10.783 
Epoch 405/1000 
	 loss: 44.4425, MinusLogProbMetric: 44.4425, val_loss: 44.6127, val_MinusLogProbMetric: 44.6127

Epoch 405: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.4425 - MinusLogProbMetric: 44.4425 - val_loss: 44.6127 - val_MinusLogProbMetric: 44.6127 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 406/1000
2023-10-28 06:09:44.267 
Epoch 406/1000 
	 loss: 44.4528, MinusLogProbMetric: 44.4528, val_loss: 45.7581, val_MinusLogProbMetric: 45.7581

Epoch 406: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.4528 - MinusLogProbMetric: 44.4528 - val_loss: 45.7581 - val_MinusLogProbMetric: 45.7581 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 407/1000
2023-10-28 06:10:17.804 
Epoch 407/1000 
	 loss: 44.7037, MinusLogProbMetric: 44.7037, val_loss: 45.9694, val_MinusLogProbMetric: 45.9694

Epoch 407: val_loss did not improve from 44.31761
196/196 - 34s - loss: 44.7037 - MinusLogProbMetric: 44.7037 - val_loss: 45.9694 - val_MinusLogProbMetric: 45.9694 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 408/1000
2023-10-28 06:10:50.876 
Epoch 408/1000 
	 loss: 44.6658, MinusLogProbMetric: 44.6658, val_loss: 45.8533, val_MinusLogProbMetric: 45.8533

Epoch 408: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.6658 - MinusLogProbMetric: 44.6658 - val_loss: 45.8533 - val_MinusLogProbMetric: 45.8533 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 409/1000
2023-10-28 06:11:23.930 
Epoch 409/1000 
	 loss: 44.6035, MinusLogProbMetric: 44.6035, val_loss: 48.4872, val_MinusLogProbMetric: 48.4872

Epoch 409: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.6035 - MinusLogProbMetric: 44.6035 - val_loss: 48.4872 - val_MinusLogProbMetric: 48.4872 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 410/1000
2023-10-28 06:11:57.050 
Epoch 410/1000 
	 loss: 44.7140, MinusLogProbMetric: 44.7140, val_loss: 46.0650, val_MinusLogProbMetric: 46.0650

Epoch 410: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.7140 - MinusLogProbMetric: 44.7140 - val_loss: 46.0650 - val_MinusLogProbMetric: 46.0650 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 411/1000
2023-10-28 06:12:30.227 
Epoch 411/1000 
	 loss: 44.6211, MinusLogProbMetric: 44.6211, val_loss: 45.7386, val_MinusLogProbMetric: 45.7386

Epoch 411: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.6211 - MinusLogProbMetric: 44.6211 - val_loss: 45.7386 - val_MinusLogProbMetric: 45.7386 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 412/1000
2023-10-28 06:13:03.534 
Epoch 412/1000 
	 loss: 44.9101, MinusLogProbMetric: 44.9101, val_loss: 45.6100, val_MinusLogProbMetric: 45.6100

Epoch 412: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.9101 - MinusLogProbMetric: 44.9101 - val_loss: 45.6100 - val_MinusLogProbMetric: 45.6100 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 413/1000
2023-10-28 06:13:36.091 
Epoch 413/1000 
	 loss: 44.7272, MinusLogProbMetric: 44.7272, val_loss: 44.7621, val_MinusLogProbMetric: 44.7621

Epoch 413: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.7272 - MinusLogProbMetric: 44.7272 - val_loss: 44.7621 - val_MinusLogProbMetric: 44.7621 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 414/1000
2023-10-28 06:14:07.311 
Epoch 414/1000 
	 loss: 44.9047, MinusLogProbMetric: 44.9047, val_loss: 45.9038, val_MinusLogProbMetric: 45.9038

Epoch 414: val_loss did not improve from 44.31761
196/196 - 31s - loss: 44.9047 - MinusLogProbMetric: 44.9047 - val_loss: 45.9038 - val_MinusLogProbMetric: 45.9038 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 415/1000
2023-10-28 06:14:40.378 
Epoch 415/1000 
	 loss: 44.7127, MinusLogProbMetric: 44.7127, val_loss: 45.2104, val_MinusLogProbMetric: 45.2104

Epoch 415: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.7127 - MinusLogProbMetric: 44.7127 - val_loss: 45.2104 - val_MinusLogProbMetric: 45.2104 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 416/1000
2023-10-28 06:15:13.971 
Epoch 416/1000 
	 loss: 44.5324, MinusLogProbMetric: 44.5324, val_loss: 45.2348, val_MinusLogProbMetric: 45.2348

Epoch 416: val_loss did not improve from 44.31761
196/196 - 34s - loss: 44.5324 - MinusLogProbMetric: 44.5324 - val_loss: 45.2348 - val_MinusLogProbMetric: 45.2348 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 417/1000
2023-10-28 06:15:47.391 
Epoch 417/1000 
	 loss: 44.7637, MinusLogProbMetric: 44.7637, val_loss: 44.7184, val_MinusLogProbMetric: 44.7184

Epoch 417: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.7637 - MinusLogProbMetric: 44.7637 - val_loss: 44.7184 - val_MinusLogProbMetric: 44.7184 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 418/1000
2023-10-28 06:16:20.513 
Epoch 418/1000 
	 loss: 44.6360, MinusLogProbMetric: 44.6360, val_loss: 48.0943, val_MinusLogProbMetric: 48.0943

Epoch 418: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.6360 - MinusLogProbMetric: 44.6360 - val_loss: 48.0943 - val_MinusLogProbMetric: 48.0943 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 419/1000
2023-10-28 06:16:53.668 
Epoch 419/1000 
	 loss: 44.7176, MinusLogProbMetric: 44.7176, val_loss: 44.5528, val_MinusLogProbMetric: 44.5528

Epoch 419: val_loss did not improve from 44.31761
196/196 - 33s - loss: 44.7176 - MinusLogProbMetric: 44.7176 - val_loss: 44.5528 - val_MinusLogProbMetric: 44.5528 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 420/1000
2023-10-28 06:17:27.331 
Epoch 420/1000 
	 loss: 44.4831, MinusLogProbMetric: 44.4831, val_loss: 45.3615, val_MinusLogProbMetric: 45.3615

Epoch 420: val_loss did not improve from 44.31761
196/196 - 34s - loss: 44.4831 - MinusLogProbMetric: 44.4831 - val_loss: 45.3615 - val_MinusLogProbMetric: 45.3615 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 421/1000
2023-10-28 06:18:00.645 
Epoch 421/1000 
	 loss: 44.7381, MinusLogProbMetric: 44.7381, val_loss: 44.2308, val_MinusLogProbMetric: 44.2308

Epoch 421: val_loss improved from 44.31761 to 44.23080, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 44.7381 - MinusLogProbMetric: 44.7381 - val_loss: 44.2308 - val_MinusLogProbMetric: 44.2308 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 422/1000
2023-10-28 06:18:34.490 
Epoch 422/1000 
	 loss: 44.2833, MinusLogProbMetric: 44.2833, val_loss: 44.1356, val_MinusLogProbMetric: 44.1356

Epoch 422: val_loss improved from 44.23080 to 44.13556, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 44.2833 - MinusLogProbMetric: 44.2833 - val_loss: 44.1356 - val_MinusLogProbMetric: 44.1356 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 423/1000
2023-10-28 06:19:08.737 
Epoch 423/1000 
	 loss: 44.5647, MinusLogProbMetric: 44.5647, val_loss: 47.1740, val_MinusLogProbMetric: 47.1740

Epoch 423: val_loss did not improve from 44.13556
196/196 - 34s - loss: 44.5647 - MinusLogProbMetric: 44.5647 - val_loss: 47.1740 - val_MinusLogProbMetric: 47.1740 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 424/1000
2023-10-28 06:19:41.270 
Epoch 424/1000 
	 loss: 44.5055, MinusLogProbMetric: 44.5055, val_loss: 45.1581, val_MinusLogProbMetric: 45.1581

Epoch 424: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.5055 - MinusLogProbMetric: 44.5055 - val_loss: 45.1581 - val_MinusLogProbMetric: 45.1581 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 425/1000
2023-10-28 06:20:13.220 
Epoch 425/1000 
	 loss: 44.6676, MinusLogProbMetric: 44.6676, val_loss: 45.3776, val_MinusLogProbMetric: 45.3776

Epoch 425: val_loss did not improve from 44.13556
196/196 - 32s - loss: 44.6676 - MinusLogProbMetric: 44.6676 - val_loss: 45.3776 - val_MinusLogProbMetric: 45.3776 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 426/1000
2023-10-28 06:20:46.800 
Epoch 426/1000 
	 loss: 44.7011, MinusLogProbMetric: 44.7011, val_loss: 44.7355, val_MinusLogProbMetric: 44.7355

Epoch 426: val_loss did not improve from 44.13556
196/196 - 34s - loss: 44.7011 - MinusLogProbMetric: 44.7011 - val_loss: 44.7355 - val_MinusLogProbMetric: 44.7355 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 427/1000
2023-10-28 06:21:20.120 
Epoch 427/1000 
	 loss: 44.4450, MinusLogProbMetric: 44.4450, val_loss: 45.7018, val_MinusLogProbMetric: 45.7018

Epoch 427: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.4450 - MinusLogProbMetric: 44.4450 - val_loss: 45.7018 - val_MinusLogProbMetric: 45.7018 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 428/1000
2023-10-28 06:21:53.371 
Epoch 428/1000 
	 loss: 44.5658, MinusLogProbMetric: 44.5658, val_loss: 45.0647, val_MinusLogProbMetric: 45.0647

Epoch 428: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.5658 - MinusLogProbMetric: 44.5658 - val_loss: 45.0647 - val_MinusLogProbMetric: 45.0647 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 429/1000
2023-10-28 06:22:26.950 
Epoch 429/1000 
	 loss: 44.7718, MinusLogProbMetric: 44.7718, val_loss: 46.8404, val_MinusLogProbMetric: 46.8404

Epoch 429: val_loss did not improve from 44.13556
196/196 - 34s - loss: 44.7718 - MinusLogProbMetric: 44.7718 - val_loss: 46.8404 - val_MinusLogProbMetric: 46.8404 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 430/1000
2023-10-28 06:23:00.211 
Epoch 430/1000 
	 loss: 44.6745, MinusLogProbMetric: 44.6745, val_loss: 44.9160, val_MinusLogProbMetric: 44.9160

Epoch 430: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.6745 - MinusLogProbMetric: 44.6745 - val_loss: 44.9160 - val_MinusLogProbMetric: 44.9160 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 431/1000
2023-10-28 06:23:33.360 
Epoch 431/1000 
	 loss: 44.5465, MinusLogProbMetric: 44.5465, val_loss: 45.7109, val_MinusLogProbMetric: 45.7109

Epoch 431: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.5465 - MinusLogProbMetric: 44.5465 - val_loss: 45.7109 - val_MinusLogProbMetric: 45.7109 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 432/1000
2023-10-28 06:24:06.794 
Epoch 432/1000 
	 loss: 44.8395, MinusLogProbMetric: 44.8395, val_loss: 45.9806, val_MinusLogProbMetric: 45.9806

Epoch 432: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.8395 - MinusLogProbMetric: 44.8395 - val_loss: 45.9806 - val_MinusLogProbMetric: 45.9806 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 433/1000
2023-10-28 06:24:39.822 
Epoch 433/1000 
	 loss: 45.4912, MinusLogProbMetric: 45.4912, val_loss: 44.4643, val_MinusLogProbMetric: 44.4643

Epoch 433: val_loss did not improve from 44.13556
196/196 - 33s - loss: 45.4912 - MinusLogProbMetric: 45.4912 - val_loss: 44.4643 - val_MinusLogProbMetric: 44.4643 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 434/1000
2023-10-28 06:25:12.972 
Epoch 434/1000 
	 loss: 44.5806, MinusLogProbMetric: 44.5806, val_loss: 44.7089, val_MinusLogProbMetric: 44.7089

Epoch 434: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.5806 - MinusLogProbMetric: 44.5806 - val_loss: 44.7089 - val_MinusLogProbMetric: 44.7089 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 435/1000
2023-10-28 06:25:46.088 
Epoch 435/1000 
	 loss: 44.2187, MinusLogProbMetric: 44.2187, val_loss: 44.2774, val_MinusLogProbMetric: 44.2774

Epoch 435: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.2187 - MinusLogProbMetric: 44.2187 - val_loss: 44.2774 - val_MinusLogProbMetric: 44.2774 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 436/1000
2023-10-28 06:26:19.241 
Epoch 436/1000 
	 loss: 44.3005, MinusLogProbMetric: 44.3005, val_loss: 45.7926, val_MinusLogProbMetric: 45.7926

Epoch 436: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.3005 - MinusLogProbMetric: 44.3005 - val_loss: 45.7926 - val_MinusLogProbMetric: 45.7926 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 437/1000
2023-10-28 06:26:52.504 
Epoch 437/1000 
	 loss: 44.4033, MinusLogProbMetric: 44.4033, val_loss: 45.0787, val_MinusLogProbMetric: 45.0787

Epoch 437: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.4033 - MinusLogProbMetric: 44.4033 - val_loss: 45.0787 - val_MinusLogProbMetric: 45.0787 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 438/1000
2023-10-28 06:27:25.924 
Epoch 438/1000 
	 loss: 44.4008, MinusLogProbMetric: 44.4008, val_loss: 45.5956, val_MinusLogProbMetric: 45.5956

Epoch 438: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.4008 - MinusLogProbMetric: 44.4008 - val_loss: 45.5956 - val_MinusLogProbMetric: 45.5956 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 439/1000
2023-10-28 06:27:59.028 
Epoch 439/1000 
	 loss: 44.4551, MinusLogProbMetric: 44.4551, val_loss: 45.1157, val_MinusLogProbMetric: 45.1157

Epoch 439: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.4551 - MinusLogProbMetric: 44.4551 - val_loss: 45.1157 - val_MinusLogProbMetric: 45.1157 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 440/1000
2023-10-28 06:28:32.245 
Epoch 440/1000 
	 loss: 44.7986, MinusLogProbMetric: 44.7986, val_loss: 45.5701, val_MinusLogProbMetric: 45.5701

Epoch 440: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.7986 - MinusLogProbMetric: 44.7986 - val_loss: 45.5701 - val_MinusLogProbMetric: 45.5701 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 441/1000
2023-10-28 06:29:05.555 
Epoch 441/1000 
	 loss: 44.6234, MinusLogProbMetric: 44.6234, val_loss: 53.4973, val_MinusLogProbMetric: 53.4973

Epoch 441: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.6234 - MinusLogProbMetric: 44.6234 - val_loss: 53.4973 - val_MinusLogProbMetric: 53.4973 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 442/1000
2023-10-28 06:29:38.961 
Epoch 442/1000 
	 loss: 45.1597, MinusLogProbMetric: 45.1597, val_loss: 47.3885, val_MinusLogProbMetric: 47.3885

Epoch 442: val_loss did not improve from 44.13556
196/196 - 33s - loss: 45.1597 - MinusLogProbMetric: 45.1597 - val_loss: 47.3885 - val_MinusLogProbMetric: 47.3885 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 443/1000
2023-10-28 06:30:11.769 
Epoch 443/1000 
	 loss: 44.7614, MinusLogProbMetric: 44.7614, val_loss: 45.0925, val_MinusLogProbMetric: 45.0925

Epoch 443: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.7614 - MinusLogProbMetric: 44.7614 - val_loss: 45.0925 - val_MinusLogProbMetric: 45.0925 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 444/1000
2023-10-28 06:30:45.138 
Epoch 444/1000 
	 loss: 44.5504, MinusLogProbMetric: 44.5504, val_loss: 44.6641, val_MinusLogProbMetric: 44.6641

Epoch 444: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.5504 - MinusLogProbMetric: 44.5504 - val_loss: 44.6641 - val_MinusLogProbMetric: 44.6641 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 445/1000
2023-10-28 06:31:18.410 
Epoch 445/1000 
	 loss: 44.2731, MinusLogProbMetric: 44.2731, val_loss: 47.0661, val_MinusLogProbMetric: 47.0661

Epoch 445: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.2731 - MinusLogProbMetric: 44.2731 - val_loss: 47.0661 - val_MinusLogProbMetric: 47.0661 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 446/1000
2023-10-28 06:31:51.586 
Epoch 446/1000 
	 loss: 44.6532, MinusLogProbMetric: 44.6532, val_loss: 45.9796, val_MinusLogProbMetric: 45.9796

Epoch 446: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.6532 - MinusLogProbMetric: 44.6532 - val_loss: 45.9796 - val_MinusLogProbMetric: 45.9796 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 447/1000
2023-10-28 06:32:25.136 
Epoch 447/1000 
	 loss: 44.8405, MinusLogProbMetric: 44.8405, val_loss: 44.4621, val_MinusLogProbMetric: 44.4621

Epoch 447: val_loss did not improve from 44.13556
196/196 - 34s - loss: 44.8405 - MinusLogProbMetric: 44.8405 - val_loss: 44.4621 - val_MinusLogProbMetric: 44.4621 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 448/1000
2023-10-28 06:32:56.848 
Epoch 448/1000 
	 loss: 44.1282, MinusLogProbMetric: 44.1282, val_loss: 47.1678, val_MinusLogProbMetric: 47.1678

Epoch 448: val_loss did not improve from 44.13556
196/196 - 32s - loss: 44.1282 - MinusLogProbMetric: 44.1282 - val_loss: 47.1678 - val_MinusLogProbMetric: 47.1678 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 449/1000
2023-10-28 06:33:29.606 
Epoch 449/1000 
	 loss: 44.5188, MinusLogProbMetric: 44.5188, val_loss: 44.8191, val_MinusLogProbMetric: 44.8191

Epoch 449: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.5188 - MinusLogProbMetric: 44.5188 - val_loss: 44.8191 - val_MinusLogProbMetric: 44.8191 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 450/1000
2023-10-28 06:34:02.702 
Epoch 450/1000 
	 loss: 44.1088, MinusLogProbMetric: 44.1088, val_loss: 44.6273, val_MinusLogProbMetric: 44.6273

Epoch 450: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.1088 - MinusLogProbMetric: 44.1088 - val_loss: 44.6273 - val_MinusLogProbMetric: 44.6273 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 451/1000
2023-10-28 06:34:35.935 
Epoch 451/1000 
	 loss: 44.6032, MinusLogProbMetric: 44.6032, val_loss: 46.8652, val_MinusLogProbMetric: 46.8652

Epoch 451: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.6032 - MinusLogProbMetric: 44.6032 - val_loss: 46.8652 - val_MinusLogProbMetric: 46.8652 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 452/1000
2023-10-28 06:35:09.459 
Epoch 452/1000 
	 loss: 44.6711, MinusLogProbMetric: 44.6711, val_loss: 44.9846, val_MinusLogProbMetric: 44.9846

Epoch 452: val_loss did not improve from 44.13556
196/196 - 34s - loss: 44.6711 - MinusLogProbMetric: 44.6711 - val_loss: 44.9846 - val_MinusLogProbMetric: 44.9846 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 453/1000
2023-10-28 06:35:42.806 
Epoch 453/1000 
	 loss: 44.5433, MinusLogProbMetric: 44.5433, val_loss: 46.1729, val_MinusLogProbMetric: 46.1729

Epoch 453: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.5433 - MinusLogProbMetric: 44.5433 - val_loss: 46.1729 - val_MinusLogProbMetric: 46.1729 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 454/1000
2023-10-28 06:36:16.620 
Epoch 454/1000 
	 loss: 45.0343, MinusLogProbMetric: 45.0343, val_loss: 45.2002, val_MinusLogProbMetric: 45.2002

Epoch 454: val_loss did not improve from 44.13556
196/196 - 34s - loss: 45.0343 - MinusLogProbMetric: 45.0343 - val_loss: 45.2002 - val_MinusLogProbMetric: 45.2002 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 455/1000
2023-10-28 06:36:49.970 
Epoch 455/1000 
	 loss: 44.8098, MinusLogProbMetric: 44.8098, val_loss: 44.5265, val_MinusLogProbMetric: 44.5265

Epoch 455: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.8098 - MinusLogProbMetric: 44.8098 - val_loss: 44.5265 - val_MinusLogProbMetric: 44.5265 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 456/1000
2023-10-28 06:37:23.325 
Epoch 456/1000 
	 loss: 44.3991, MinusLogProbMetric: 44.3991, val_loss: 44.9432, val_MinusLogProbMetric: 44.9432

Epoch 456: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.3991 - MinusLogProbMetric: 44.3991 - val_loss: 44.9432 - val_MinusLogProbMetric: 44.9432 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 457/1000
2023-10-28 06:37:56.752 
Epoch 457/1000 
	 loss: 44.4392, MinusLogProbMetric: 44.4392, val_loss: 45.9226, val_MinusLogProbMetric: 45.9226

Epoch 457: val_loss did not improve from 44.13556
196/196 - 33s - loss: 44.4392 - MinusLogProbMetric: 44.4392 - val_loss: 45.9226 - val_MinusLogProbMetric: 45.9226 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 458/1000
2023-10-28 06:38:30.298 
Epoch 458/1000 
	 loss: 44.2113, MinusLogProbMetric: 44.2113, val_loss: 44.0642, val_MinusLogProbMetric: 44.0642

Epoch 458: val_loss improved from 44.13556 to 44.06417, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 44.2113 - MinusLogProbMetric: 44.2113 - val_loss: 44.0642 - val_MinusLogProbMetric: 44.0642 - lr: 3.3333e-04 - 34s/epoch - 174ms/step
Epoch 459/1000
2023-10-28 06:39:03.883 
Epoch 459/1000 
	 loss: 44.9177, MinusLogProbMetric: 44.9177, val_loss: 46.1454, val_MinusLogProbMetric: 46.1454

Epoch 459: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.9177 - MinusLogProbMetric: 44.9177 - val_loss: 46.1454 - val_MinusLogProbMetric: 46.1454 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 460/1000
2023-10-28 06:39:37.491 
Epoch 460/1000 
	 loss: 44.3823, MinusLogProbMetric: 44.3823, val_loss: 44.5382, val_MinusLogProbMetric: 44.5382

Epoch 460: val_loss did not improve from 44.06417
196/196 - 34s - loss: 44.3823 - MinusLogProbMetric: 44.3823 - val_loss: 44.5382 - val_MinusLogProbMetric: 44.5382 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 461/1000
2023-10-28 06:40:10.799 
Epoch 461/1000 
	 loss: 44.1695, MinusLogProbMetric: 44.1695, val_loss: 47.1464, val_MinusLogProbMetric: 47.1464

Epoch 461: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.1695 - MinusLogProbMetric: 44.1695 - val_loss: 47.1464 - val_MinusLogProbMetric: 47.1464 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 462/1000
2023-10-28 06:40:43.966 
Epoch 462/1000 
	 loss: 44.3996, MinusLogProbMetric: 44.3996, val_loss: 45.3671, val_MinusLogProbMetric: 45.3671

Epoch 462: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.3996 - MinusLogProbMetric: 44.3996 - val_loss: 45.3671 - val_MinusLogProbMetric: 45.3671 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 463/1000
2023-10-28 06:41:17.406 
Epoch 463/1000 
	 loss: 44.3599, MinusLogProbMetric: 44.3599, val_loss: 46.9255, val_MinusLogProbMetric: 46.9255

Epoch 463: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.3599 - MinusLogProbMetric: 44.3599 - val_loss: 46.9255 - val_MinusLogProbMetric: 46.9255 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 464/1000
2023-10-28 06:41:50.537 
Epoch 464/1000 
	 loss: 44.5137, MinusLogProbMetric: 44.5137, val_loss: 45.5105, val_MinusLogProbMetric: 45.5105

Epoch 464: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.5137 - MinusLogProbMetric: 44.5137 - val_loss: 45.5105 - val_MinusLogProbMetric: 45.5105 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 465/1000
2023-10-28 06:42:23.672 
Epoch 465/1000 
	 loss: 44.1727, MinusLogProbMetric: 44.1727, val_loss: 44.6288, val_MinusLogProbMetric: 44.6288

Epoch 465: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.1727 - MinusLogProbMetric: 44.1727 - val_loss: 44.6288 - val_MinusLogProbMetric: 44.6288 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 466/1000
2023-10-28 06:42:56.632 
Epoch 466/1000 
	 loss: 44.4111, MinusLogProbMetric: 44.4111, val_loss: 47.0149, val_MinusLogProbMetric: 47.0149

Epoch 466: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.4111 - MinusLogProbMetric: 44.4111 - val_loss: 47.0149 - val_MinusLogProbMetric: 47.0149 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 467/1000
2023-10-28 06:43:30.001 
Epoch 467/1000 
	 loss: 44.4785, MinusLogProbMetric: 44.4785, val_loss: 46.5416, val_MinusLogProbMetric: 46.5416

Epoch 467: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.4785 - MinusLogProbMetric: 44.4785 - val_loss: 46.5416 - val_MinusLogProbMetric: 46.5416 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 468/1000
2023-10-28 06:44:03.037 
Epoch 468/1000 
	 loss: 44.4259, MinusLogProbMetric: 44.4259, val_loss: 44.6788, val_MinusLogProbMetric: 44.6788

Epoch 468: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.4259 - MinusLogProbMetric: 44.4259 - val_loss: 44.6788 - val_MinusLogProbMetric: 44.6788 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 469/1000
2023-10-28 06:44:36.303 
Epoch 469/1000 
	 loss: 44.5323, MinusLogProbMetric: 44.5323, val_loss: 45.3719, val_MinusLogProbMetric: 45.3719

Epoch 469: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.5323 - MinusLogProbMetric: 44.5323 - val_loss: 45.3719 - val_MinusLogProbMetric: 45.3719 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 470/1000
2023-10-28 06:45:09.600 
Epoch 470/1000 
	 loss: 44.2655, MinusLogProbMetric: 44.2655, val_loss: 44.6451, val_MinusLogProbMetric: 44.6451

Epoch 470: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.2655 - MinusLogProbMetric: 44.2655 - val_loss: 44.6451 - val_MinusLogProbMetric: 44.6451 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 471/1000
2023-10-28 06:45:42.852 
Epoch 471/1000 
	 loss: 44.2579, MinusLogProbMetric: 44.2579, val_loss: 45.7049, val_MinusLogProbMetric: 45.7049

Epoch 471: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.2579 - MinusLogProbMetric: 44.2579 - val_loss: 45.7049 - val_MinusLogProbMetric: 45.7049 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 472/1000
2023-10-28 06:46:16.440 
Epoch 472/1000 
	 loss: 44.5702, MinusLogProbMetric: 44.5702, val_loss: 45.6917, val_MinusLogProbMetric: 45.6917

Epoch 472: val_loss did not improve from 44.06417
196/196 - 34s - loss: 44.5702 - MinusLogProbMetric: 44.5702 - val_loss: 45.6917 - val_MinusLogProbMetric: 45.6917 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 473/1000
2023-10-28 06:46:50.040 
Epoch 473/1000 
	 loss: 44.3759, MinusLogProbMetric: 44.3759, val_loss: 47.1402, val_MinusLogProbMetric: 47.1402

Epoch 473: val_loss did not improve from 44.06417
196/196 - 34s - loss: 44.3759 - MinusLogProbMetric: 44.3759 - val_loss: 47.1402 - val_MinusLogProbMetric: 47.1402 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 474/1000
2023-10-28 06:47:23.264 
Epoch 474/1000 
	 loss: 44.4818, MinusLogProbMetric: 44.4818, val_loss: 45.6216, val_MinusLogProbMetric: 45.6216

Epoch 474: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.4818 - MinusLogProbMetric: 44.4818 - val_loss: 45.6216 - val_MinusLogProbMetric: 45.6216 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 475/1000
2023-10-28 06:47:56.713 
Epoch 475/1000 
	 loss: 44.5566, MinusLogProbMetric: 44.5566, val_loss: 47.0137, val_MinusLogProbMetric: 47.0137

Epoch 475: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.5566 - MinusLogProbMetric: 44.5566 - val_loss: 47.0137 - val_MinusLogProbMetric: 47.0137 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 476/1000
2023-10-28 06:48:30.526 
Epoch 476/1000 
	 loss: 44.3866, MinusLogProbMetric: 44.3866, val_loss: 44.9634, val_MinusLogProbMetric: 44.9634

Epoch 476: val_loss did not improve from 44.06417
196/196 - 34s - loss: 44.3866 - MinusLogProbMetric: 44.3866 - val_loss: 44.9634 - val_MinusLogProbMetric: 44.9634 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 477/1000
2023-10-28 06:49:04.775 
Epoch 477/1000 
	 loss: 44.3567, MinusLogProbMetric: 44.3567, val_loss: 44.9034, val_MinusLogProbMetric: 44.9034

Epoch 477: val_loss did not improve from 44.06417
196/196 - 34s - loss: 44.3567 - MinusLogProbMetric: 44.3567 - val_loss: 44.9034 - val_MinusLogProbMetric: 44.9034 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 478/1000
2023-10-28 06:49:38.409 
Epoch 478/1000 
	 loss: 44.3291, MinusLogProbMetric: 44.3291, val_loss: 45.2374, val_MinusLogProbMetric: 45.2374

Epoch 478: val_loss did not improve from 44.06417
196/196 - 34s - loss: 44.3291 - MinusLogProbMetric: 44.3291 - val_loss: 45.2374 - val_MinusLogProbMetric: 45.2374 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 479/1000
2023-10-28 06:50:11.913 
Epoch 479/1000 
	 loss: 44.1166, MinusLogProbMetric: 44.1166, val_loss: 45.0345, val_MinusLogProbMetric: 45.0345

Epoch 479: val_loss did not improve from 44.06417
196/196 - 34s - loss: 44.1166 - MinusLogProbMetric: 44.1166 - val_loss: 45.0345 - val_MinusLogProbMetric: 45.0345 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 480/1000
2023-10-28 06:50:45.530 
Epoch 480/1000 
	 loss: 44.0753, MinusLogProbMetric: 44.0753, val_loss: 45.1066, val_MinusLogProbMetric: 45.1066

Epoch 480: val_loss did not improve from 44.06417
196/196 - 34s - loss: 44.0753 - MinusLogProbMetric: 44.0753 - val_loss: 45.1066 - val_MinusLogProbMetric: 45.1066 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 481/1000
2023-10-28 06:51:18.791 
Epoch 481/1000 
	 loss: 44.5377, MinusLogProbMetric: 44.5377, val_loss: 44.8303, val_MinusLogProbMetric: 44.8303

Epoch 481: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.5377 - MinusLogProbMetric: 44.5377 - val_loss: 44.8303 - val_MinusLogProbMetric: 44.8303 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 482/1000
2023-10-28 06:51:51.823 
Epoch 482/1000 
	 loss: 44.7526, MinusLogProbMetric: 44.7526, val_loss: 45.6817, val_MinusLogProbMetric: 45.6817

Epoch 482: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.7526 - MinusLogProbMetric: 44.7526 - val_loss: 45.6817 - val_MinusLogProbMetric: 45.6817 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 483/1000
2023-10-28 06:52:25.225 
Epoch 483/1000 
	 loss: 44.1122, MinusLogProbMetric: 44.1122, val_loss: 44.7130, val_MinusLogProbMetric: 44.7130

Epoch 483: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.1122 - MinusLogProbMetric: 44.1122 - val_loss: 44.7130 - val_MinusLogProbMetric: 44.7130 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 484/1000
2023-10-28 06:52:58.713 
Epoch 484/1000 
	 loss: 44.4798, MinusLogProbMetric: 44.4798, val_loss: 44.6864, val_MinusLogProbMetric: 44.6864

Epoch 484: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.4798 - MinusLogProbMetric: 44.4798 - val_loss: 44.6864 - val_MinusLogProbMetric: 44.6864 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 485/1000
2023-10-28 06:53:31.954 
Epoch 485/1000 
	 loss: 44.3740, MinusLogProbMetric: 44.3740, val_loss: 45.4255, val_MinusLogProbMetric: 45.4255

Epoch 485: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.3740 - MinusLogProbMetric: 44.3740 - val_loss: 45.4255 - val_MinusLogProbMetric: 45.4255 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 486/1000
2023-10-28 06:54:05.019 
Epoch 486/1000 
	 loss: 44.2705, MinusLogProbMetric: 44.2705, val_loss: 45.6246, val_MinusLogProbMetric: 45.6246

Epoch 486: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.2705 - MinusLogProbMetric: 44.2705 - val_loss: 45.6246 - val_MinusLogProbMetric: 45.6246 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 487/1000
2023-10-28 06:54:38.426 
Epoch 487/1000 
	 loss: 44.2514, MinusLogProbMetric: 44.2514, val_loss: 46.5781, val_MinusLogProbMetric: 46.5781

Epoch 487: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.2514 - MinusLogProbMetric: 44.2514 - val_loss: 46.5781 - val_MinusLogProbMetric: 46.5781 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 488/1000
2023-10-28 06:55:11.419 
Epoch 488/1000 
	 loss: 44.1959, MinusLogProbMetric: 44.1959, val_loss: 44.6190, val_MinusLogProbMetric: 44.6190

Epoch 488: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.1959 - MinusLogProbMetric: 44.1959 - val_loss: 44.6190 - val_MinusLogProbMetric: 44.6190 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 489/1000
2023-10-28 06:55:44.697 
Epoch 489/1000 
	 loss: 44.2279, MinusLogProbMetric: 44.2279, val_loss: 45.5206, val_MinusLogProbMetric: 45.5206

Epoch 489: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.2279 - MinusLogProbMetric: 44.2279 - val_loss: 45.5206 - val_MinusLogProbMetric: 45.5206 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 490/1000
2023-10-28 06:56:18.125 
Epoch 490/1000 
	 loss: 44.3225, MinusLogProbMetric: 44.3225, val_loss: 46.1564, val_MinusLogProbMetric: 46.1564

Epoch 490: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.3225 - MinusLogProbMetric: 44.3225 - val_loss: 46.1564 - val_MinusLogProbMetric: 46.1564 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 491/1000
2023-10-28 06:56:51.633 
Epoch 491/1000 
	 loss: 44.4507, MinusLogProbMetric: 44.4507, val_loss: 44.4609, val_MinusLogProbMetric: 44.4609

Epoch 491: val_loss did not improve from 44.06417
196/196 - 34s - loss: 44.4507 - MinusLogProbMetric: 44.4507 - val_loss: 44.4609 - val_MinusLogProbMetric: 44.4609 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 492/1000
2023-10-28 06:57:24.653 
Epoch 492/1000 
	 loss: 44.4357, MinusLogProbMetric: 44.4357, val_loss: 44.8577, val_MinusLogProbMetric: 44.8578

Epoch 492: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.4357 - MinusLogProbMetric: 44.4357 - val_loss: 44.8577 - val_MinusLogProbMetric: 44.8578 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 493/1000
2023-10-28 06:57:57.897 
Epoch 493/1000 
	 loss: 43.9389, MinusLogProbMetric: 43.9389, val_loss: 44.2811, val_MinusLogProbMetric: 44.2811

Epoch 493: val_loss did not improve from 44.06417
196/196 - 33s - loss: 43.9389 - MinusLogProbMetric: 43.9389 - val_loss: 44.2811 - val_MinusLogProbMetric: 44.2811 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 494/1000
2023-10-28 06:58:31.007 
Epoch 494/1000 
	 loss: 44.1012, MinusLogProbMetric: 44.1012, val_loss: 44.5195, val_MinusLogProbMetric: 44.5195

Epoch 494: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.1012 - MinusLogProbMetric: 44.1012 - val_loss: 44.5195 - val_MinusLogProbMetric: 44.5195 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 495/1000
2023-10-28 06:59:04.237 
Epoch 495/1000 
	 loss: 44.0594, MinusLogProbMetric: 44.0594, val_loss: 44.9668, val_MinusLogProbMetric: 44.9668

Epoch 495: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.0594 - MinusLogProbMetric: 44.0594 - val_loss: 44.9668 - val_MinusLogProbMetric: 44.9668 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 496/1000
2023-10-28 06:59:37.373 
Epoch 496/1000 
	 loss: 44.4261, MinusLogProbMetric: 44.4261, val_loss: 44.9419, val_MinusLogProbMetric: 44.9419

Epoch 496: val_loss did not improve from 44.06417
196/196 - 33s - loss: 44.4261 - MinusLogProbMetric: 44.4261 - val_loss: 44.9419 - val_MinusLogProbMetric: 44.9419 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 497/1000
2023-10-28 07:00:10.989 
Epoch 497/1000 
	 loss: 44.2532, MinusLogProbMetric: 44.2532, val_loss: 44.8138, val_MinusLogProbMetric: 44.8138

Epoch 497: val_loss did not improve from 44.06417
196/196 - 34s - loss: 44.2532 - MinusLogProbMetric: 44.2532 - val_loss: 44.8138 - val_MinusLogProbMetric: 44.8138 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 498/1000
2023-10-28 07:00:44.294 
Epoch 498/1000 
	 loss: 44.1669, MinusLogProbMetric: 44.1669, val_loss: 43.7861, val_MinusLogProbMetric: 43.7861

Epoch 498: val_loss improved from 44.06417 to 43.78611, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 44.1669 - MinusLogProbMetric: 44.1669 - val_loss: 43.7861 - val_MinusLogProbMetric: 43.7861 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 499/1000
2023-10-28 07:01:18.373 
Epoch 499/1000 
	 loss: 44.2269, MinusLogProbMetric: 44.2269, val_loss: 46.3961, val_MinusLogProbMetric: 46.3961

Epoch 499: val_loss did not improve from 43.78611
196/196 - 34s - loss: 44.2269 - MinusLogProbMetric: 44.2269 - val_loss: 46.3961 - val_MinusLogProbMetric: 46.3961 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 500/1000
2023-10-28 07:01:51.498 
Epoch 500/1000 
	 loss: 44.0292, MinusLogProbMetric: 44.0292, val_loss: 47.5417, val_MinusLogProbMetric: 47.5417

Epoch 500: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.0292 - MinusLogProbMetric: 44.0292 - val_loss: 47.5417 - val_MinusLogProbMetric: 47.5417 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 501/1000
2023-10-28 07:02:24.800 
Epoch 501/1000 
	 loss: 44.5693, MinusLogProbMetric: 44.5693, val_loss: 44.2863, val_MinusLogProbMetric: 44.2863

Epoch 501: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.5693 - MinusLogProbMetric: 44.5693 - val_loss: 44.2863 - val_MinusLogProbMetric: 44.2863 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 502/1000
2023-10-28 07:02:58.112 
Epoch 502/1000 
	 loss: 43.9209, MinusLogProbMetric: 43.9209, val_loss: 44.8888, val_MinusLogProbMetric: 44.8888

Epoch 502: val_loss did not improve from 43.78611
196/196 - 33s - loss: 43.9209 - MinusLogProbMetric: 43.9209 - val_loss: 44.8888 - val_MinusLogProbMetric: 44.8888 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 503/1000
2023-10-28 07:03:30.939 
Epoch 503/1000 
	 loss: 44.3611, MinusLogProbMetric: 44.3611, val_loss: 44.8182, val_MinusLogProbMetric: 44.8182

Epoch 503: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.3611 - MinusLogProbMetric: 44.3611 - val_loss: 44.8182 - val_MinusLogProbMetric: 44.8182 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 504/1000
2023-10-28 07:04:04.238 
Epoch 504/1000 
	 loss: 43.9752, MinusLogProbMetric: 43.9752, val_loss: 44.4134, val_MinusLogProbMetric: 44.4134

Epoch 504: val_loss did not improve from 43.78611
196/196 - 33s - loss: 43.9752 - MinusLogProbMetric: 43.9752 - val_loss: 44.4134 - val_MinusLogProbMetric: 44.4134 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 505/1000
2023-10-28 07:04:37.594 
Epoch 505/1000 
	 loss: 44.0545, MinusLogProbMetric: 44.0545, val_loss: 44.7849, val_MinusLogProbMetric: 44.7849

Epoch 505: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.0545 - MinusLogProbMetric: 44.0545 - val_loss: 44.7849 - val_MinusLogProbMetric: 44.7849 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 506/1000
2023-10-28 07:05:10.631 
Epoch 506/1000 
	 loss: 44.2330, MinusLogProbMetric: 44.2330, val_loss: 43.9996, val_MinusLogProbMetric: 43.9996

Epoch 506: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.2330 - MinusLogProbMetric: 44.2330 - val_loss: 43.9996 - val_MinusLogProbMetric: 43.9996 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 507/1000
2023-10-28 07:05:44.013 
Epoch 507/1000 
	 loss: 44.2490, MinusLogProbMetric: 44.2490, val_loss: 44.3340, val_MinusLogProbMetric: 44.3340

Epoch 507: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.2490 - MinusLogProbMetric: 44.2490 - val_loss: 44.3340 - val_MinusLogProbMetric: 44.3340 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 508/1000
2023-10-28 07:06:17.590 
Epoch 508/1000 
	 loss: 44.2712, MinusLogProbMetric: 44.2712, val_loss: 45.1221, val_MinusLogProbMetric: 45.1221

Epoch 508: val_loss did not improve from 43.78611
196/196 - 34s - loss: 44.2712 - MinusLogProbMetric: 44.2712 - val_loss: 45.1221 - val_MinusLogProbMetric: 45.1221 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 509/1000
2023-10-28 07:06:50.811 
Epoch 509/1000 
	 loss: 44.0996, MinusLogProbMetric: 44.0996, val_loss: 45.0126, val_MinusLogProbMetric: 45.0126

Epoch 509: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.0996 - MinusLogProbMetric: 44.0996 - val_loss: 45.0126 - val_MinusLogProbMetric: 45.0126 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 510/1000
2023-10-28 07:07:24.199 
Epoch 510/1000 
	 loss: 44.1506, MinusLogProbMetric: 44.1506, val_loss: 45.3567, val_MinusLogProbMetric: 45.3567

Epoch 510: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.1506 - MinusLogProbMetric: 44.1506 - val_loss: 45.3567 - val_MinusLogProbMetric: 45.3567 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 511/1000
2023-10-28 07:07:57.835 
Epoch 511/1000 
	 loss: 44.2802, MinusLogProbMetric: 44.2802, val_loss: 45.6828, val_MinusLogProbMetric: 45.6828

Epoch 511: val_loss did not improve from 43.78611
196/196 - 34s - loss: 44.2802 - MinusLogProbMetric: 44.2802 - val_loss: 45.6828 - val_MinusLogProbMetric: 45.6828 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 512/1000
2023-10-28 07:08:31.088 
Epoch 512/1000 
	 loss: 44.0998, MinusLogProbMetric: 44.0998, val_loss: 44.3485, val_MinusLogProbMetric: 44.3485

Epoch 512: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.0998 - MinusLogProbMetric: 44.0998 - val_loss: 44.3485 - val_MinusLogProbMetric: 44.3485 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 513/1000
2023-10-28 07:09:04.382 
Epoch 513/1000 
	 loss: 45.2650, MinusLogProbMetric: 45.2650, val_loss: 45.0693, val_MinusLogProbMetric: 45.0693

Epoch 513: val_loss did not improve from 43.78611
196/196 - 33s - loss: 45.2650 - MinusLogProbMetric: 45.2650 - val_loss: 45.0693 - val_MinusLogProbMetric: 45.0693 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 514/1000
2023-10-28 07:09:37.803 
Epoch 514/1000 
	 loss: 44.1901, MinusLogProbMetric: 44.1901, val_loss: 47.1793, val_MinusLogProbMetric: 47.1793

Epoch 514: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.1901 - MinusLogProbMetric: 44.1901 - val_loss: 47.1793 - val_MinusLogProbMetric: 47.1793 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 515/1000
2023-10-28 07:10:11.082 
Epoch 515/1000 
	 loss: 44.2725, MinusLogProbMetric: 44.2725, val_loss: 44.0144, val_MinusLogProbMetric: 44.0144

Epoch 515: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.2725 - MinusLogProbMetric: 44.2725 - val_loss: 44.0144 - val_MinusLogProbMetric: 44.0144 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 516/1000
2023-10-28 07:10:44.019 
Epoch 516/1000 
	 loss: 44.1202, MinusLogProbMetric: 44.1202, val_loss: 44.3093, val_MinusLogProbMetric: 44.3093

Epoch 516: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.1202 - MinusLogProbMetric: 44.1202 - val_loss: 44.3093 - val_MinusLogProbMetric: 44.3093 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 517/1000
2023-10-28 07:11:17.595 
Epoch 517/1000 
	 loss: 44.1001, MinusLogProbMetric: 44.1001, val_loss: 44.9334, val_MinusLogProbMetric: 44.9334

Epoch 517: val_loss did not improve from 43.78611
196/196 - 34s - loss: 44.1001 - MinusLogProbMetric: 44.1001 - val_loss: 44.9334 - val_MinusLogProbMetric: 44.9334 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 518/1000
2023-10-28 07:11:51.144 
Epoch 518/1000 
	 loss: 43.8342, MinusLogProbMetric: 43.8342, val_loss: 44.5869, val_MinusLogProbMetric: 44.5869

Epoch 518: val_loss did not improve from 43.78611
196/196 - 34s - loss: 43.8342 - MinusLogProbMetric: 43.8342 - val_loss: 44.5869 - val_MinusLogProbMetric: 44.5869 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 519/1000
2023-10-28 07:12:24.509 
Epoch 519/1000 
	 loss: 43.9531, MinusLogProbMetric: 43.9531, val_loss: 45.0296, val_MinusLogProbMetric: 45.0296

Epoch 519: val_loss did not improve from 43.78611
196/196 - 33s - loss: 43.9531 - MinusLogProbMetric: 43.9531 - val_loss: 45.0296 - val_MinusLogProbMetric: 45.0296 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 520/1000
2023-10-28 07:12:57.677 
Epoch 520/1000 
	 loss: 44.0673, MinusLogProbMetric: 44.0673, val_loss: 44.2729, val_MinusLogProbMetric: 44.2729

Epoch 520: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.0673 - MinusLogProbMetric: 44.0673 - val_loss: 44.2729 - val_MinusLogProbMetric: 44.2729 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 521/1000
2023-10-28 07:13:30.828 
Epoch 521/1000 
	 loss: 43.9350, MinusLogProbMetric: 43.9350, val_loss: 46.3494, val_MinusLogProbMetric: 46.3494

Epoch 521: val_loss did not improve from 43.78611
196/196 - 33s - loss: 43.9350 - MinusLogProbMetric: 43.9350 - val_loss: 46.3494 - val_MinusLogProbMetric: 46.3494 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 522/1000
2023-10-28 07:14:04.101 
Epoch 522/1000 
	 loss: 44.0380, MinusLogProbMetric: 44.0380, val_loss: 44.5118, val_MinusLogProbMetric: 44.5118

Epoch 522: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.0380 - MinusLogProbMetric: 44.0380 - val_loss: 44.5118 - val_MinusLogProbMetric: 44.5118 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 523/1000
2023-10-28 07:14:35.686 
Epoch 523/1000 
	 loss: 44.4432, MinusLogProbMetric: 44.4432, val_loss: 44.9639, val_MinusLogProbMetric: 44.9639

Epoch 523: val_loss did not improve from 43.78611
196/196 - 32s - loss: 44.4432 - MinusLogProbMetric: 44.4432 - val_loss: 44.9639 - val_MinusLogProbMetric: 44.9639 - lr: 3.3333e-04 - 32s/epoch - 161ms/step
Epoch 524/1000
2023-10-28 07:15:07.947 
Epoch 524/1000 
	 loss: 44.2050, MinusLogProbMetric: 44.2050, val_loss: 46.1891, val_MinusLogProbMetric: 46.1891

Epoch 524: val_loss did not improve from 43.78611
196/196 - 32s - loss: 44.2050 - MinusLogProbMetric: 44.2050 - val_loss: 46.1891 - val_MinusLogProbMetric: 46.1891 - lr: 3.3333e-04 - 32s/epoch - 165ms/step
Epoch 525/1000
2023-10-28 07:15:41.272 
Epoch 525/1000 
	 loss: 44.4609, MinusLogProbMetric: 44.4609, val_loss: 44.2563, val_MinusLogProbMetric: 44.2563

Epoch 525: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.4609 - MinusLogProbMetric: 44.4609 - val_loss: 44.2563 - val_MinusLogProbMetric: 44.2563 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 526/1000
2023-10-28 07:16:14.485 
Epoch 526/1000 
	 loss: 44.3853, MinusLogProbMetric: 44.3853, val_loss: 45.7271, val_MinusLogProbMetric: 45.7271

Epoch 526: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.3853 - MinusLogProbMetric: 44.3853 - val_loss: 45.7271 - val_MinusLogProbMetric: 45.7271 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 527/1000
2023-10-28 07:16:47.934 
Epoch 527/1000 
	 loss: 44.4301, MinusLogProbMetric: 44.4301, val_loss: 44.8769, val_MinusLogProbMetric: 44.8769

Epoch 527: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.4301 - MinusLogProbMetric: 44.4301 - val_loss: 44.8769 - val_MinusLogProbMetric: 44.8769 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 528/1000
2023-10-28 07:17:21.166 
Epoch 528/1000 
	 loss: 44.0713, MinusLogProbMetric: 44.0713, val_loss: 44.2363, val_MinusLogProbMetric: 44.2363

Epoch 528: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.0713 - MinusLogProbMetric: 44.0713 - val_loss: 44.2363 - val_MinusLogProbMetric: 44.2363 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 529/1000
2023-10-28 07:17:54.719 
Epoch 529/1000 
	 loss: 43.8746, MinusLogProbMetric: 43.8746, val_loss: 44.1472, val_MinusLogProbMetric: 44.1472

Epoch 529: val_loss did not improve from 43.78611
196/196 - 34s - loss: 43.8746 - MinusLogProbMetric: 43.8746 - val_loss: 44.1472 - val_MinusLogProbMetric: 44.1472 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 530/1000
2023-10-28 07:18:28.242 
Epoch 530/1000 
	 loss: 43.8952, MinusLogProbMetric: 43.8952, val_loss: 44.7703, val_MinusLogProbMetric: 44.7703

Epoch 530: val_loss did not improve from 43.78611
196/196 - 34s - loss: 43.8952 - MinusLogProbMetric: 43.8952 - val_loss: 44.7703 - val_MinusLogProbMetric: 44.7703 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 531/1000
2023-10-28 07:19:01.443 
Epoch 531/1000 
	 loss: 44.4434, MinusLogProbMetric: 44.4434, val_loss: 44.7502, val_MinusLogProbMetric: 44.7502

Epoch 531: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.4434 - MinusLogProbMetric: 44.4434 - val_loss: 44.7502 - val_MinusLogProbMetric: 44.7502 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 532/1000
2023-10-28 07:19:34.482 
Epoch 532/1000 
	 loss: 43.9814, MinusLogProbMetric: 43.9814, val_loss: 44.8829, val_MinusLogProbMetric: 44.8829

Epoch 532: val_loss did not improve from 43.78611
196/196 - 33s - loss: 43.9814 - MinusLogProbMetric: 43.9814 - val_loss: 44.8829 - val_MinusLogProbMetric: 44.8829 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 533/1000
2023-10-28 07:20:07.710 
Epoch 533/1000 
	 loss: 44.2793, MinusLogProbMetric: 44.2793, val_loss: 44.4385, val_MinusLogProbMetric: 44.4385

Epoch 533: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.2793 - MinusLogProbMetric: 44.2793 - val_loss: 44.4385 - val_MinusLogProbMetric: 44.4385 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 534/1000
2023-10-28 07:20:40.864 
Epoch 534/1000 
	 loss: 44.0329, MinusLogProbMetric: 44.0329, val_loss: 47.2482, val_MinusLogProbMetric: 47.2482

Epoch 534: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.0329 - MinusLogProbMetric: 44.0329 - val_loss: 47.2482 - val_MinusLogProbMetric: 47.2482 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 535/1000
2023-10-28 07:21:14.053 
Epoch 535/1000 
	 loss: 44.1553, MinusLogProbMetric: 44.1553, val_loss: 44.5771, val_MinusLogProbMetric: 44.5771

Epoch 535: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.1553 - MinusLogProbMetric: 44.1553 - val_loss: 44.5771 - val_MinusLogProbMetric: 44.5771 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 536/1000
2023-10-28 07:21:47.098 
Epoch 536/1000 
	 loss: 44.0329, MinusLogProbMetric: 44.0329, val_loss: 44.0058, val_MinusLogProbMetric: 44.0058

Epoch 536: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.0329 - MinusLogProbMetric: 44.0329 - val_loss: 44.0058 - val_MinusLogProbMetric: 44.0058 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 537/1000
2023-10-28 07:22:20.192 
Epoch 537/1000 
	 loss: 43.7571, MinusLogProbMetric: 43.7571, val_loss: 44.0671, val_MinusLogProbMetric: 44.0671

Epoch 537: val_loss did not improve from 43.78611
196/196 - 33s - loss: 43.7571 - MinusLogProbMetric: 43.7571 - val_loss: 44.0671 - val_MinusLogProbMetric: 44.0671 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 538/1000
2023-10-28 07:22:53.272 
Epoch 538/1000 
	 loss: 43.8775, MinusLogProbMetric: 43.8775, val_loss: 47.1151, val_MinusLogProbMetric: 47.1151

Epoch 538: val_loss did not improve from 43.78611
196/196 - 33s - loss: 43.8775 - MinusLogProbMetric: 43.8775 - val_loss: 47.1151 - val_MinusLogProbMetric: 47.1151 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 539/1000
2023-10-28 07:23:26.464 
Epoch 539/1000 
	 loss: 44.1715, MinusLogProbMetric: 44.1715, val_loss: 46.1784, val_MinusLogProbMetric: 46.1784

Epoch 539: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.1715 - MinusLogProbMetric: 44.1715 - val_loss: 46.1784 - val_MinusLogProbMetric: 46.1784 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 540/1000
2023-10-28 07:23:59.737 
Epoch 540/1000 
	 loss: 44.1138, MinusLogProbMetric: 44.1138, val_loss: 45.2311, val_MinusLogProbMetric: 45.2311

Epoch 540: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.1138 - MinusLogProbMetric: 44.1138 - val_loss: 45.2311 - val_MinusLogProbMetric: 45.2311 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 541/1000
2023-10-28 07:24:33.443 
Epoch 541/1000 
	 loss: 43.9486, MinusLogProbMetric: 43.9486, val_loss: 45.6559, val_MinusLogProbMetric: 45.6559

Epoch 541: val_loss did not improve from 43.78611
196/196 - 34s - loss: 43.9486 - MinusLogProbMetric: 43.9486 - val_loss: 45.6559 - val_MinusLogProbMetric: 45.6559 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 542/1000
2023-10-28 07:25:06.697 
Epoch 542/1000 
	 loss: 44.1332, MinusLogProbMetric: 44.1332, val_loss: 45.2121, val_MinusLogProbMetric: 45.2121

Epoch 542: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.1332 - MinusLogProbMetric: 44.1332 - val_loss: 45.2121 - val_MinusLogProbMetric: 45.2121 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 543/1000
2023-10-28 07:25:40.211 
Epoch 543/1000 
	 loss: 43.9450, MinusLogProbMetric: 43.9450, val_loss: 45.2881, val_MinusLogProbMetric: 45.2881

Epoch 543: val_loss did not improve from 43.78611
196/196 - 34s - loss: 43.9450 - MinusLogProbMetric: 43.9450 - val_loss: 45.2881 - val_MinusLogProbMetric: 45.2881 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 544/1000
2023-10-28 07:26:13.081 
Epoch 544/1000 
	 loss: 44.3657, MinusLogProbMetric: 44.3657, val_loss: 44.9428, val_MinusLogProbMetric: 44.9428

Epoch 544: val_loss did not improve from 43.78611
196/196 - 33s - loss: 44.3657 - MinusLogProbMetric: 44.3657 - val_loss: 44.9428 - val_MinusLogProbMetric: 44.9428 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 545/1000
2023-10-28 07:26:46.989 
Epoch 545/1000 
	 loss: 44.7623, MinusLogProbMetric: 44.7623, val_loss: 46.0567, val_MinusLogProbMetric: 46.0567

Epoch 545: val_loss did not improve from 43.78611
196/196 - 34s - loss: 44.7623 - MinusLogProbMetric: 44.7623 - val_loss: 46.0567 - val_MinusLogProbMetric: 46.0567 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 546/1000
2023-10-28 07:27:20.725 
Epoch 546/1000 
	 loss: 44.3390, MinusLogProbMetric: 44.3390, val_loss: 44.4678, val_MinusLogProbMetric: 44.4678

Epoch 546: val_loss did not improve from 43.78611
196/196 - 34s - loss: 44.3390 - MinusLogProbMetric: 44.3390 - val_loss: 44.4678 - val_MinusLogProbMetric: 44.4678 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 547/1000
2023-10-28 07:27:54.145 
Epoch 547/1000 
	 loss: 43.9858, MinusLogProbMetric: 43.9858, val_loss: 44.5301, val_MinusLogProbMetric: 44.5301

Epoch 547: val_loss did not improve from 43.78611
196/196 - 33s - loss: 43.9858 - MinusLogProbMetric: 43.9858 - val_loss: 44.5301 - val_MinusLogProbMetric: 44.5301 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 548/1000
2023-10-28 07:28:27.929 
Epoch 548/1000 
	 loss: 44.1980, MinusLogProbMetric: 44.1980, val_loss: 43.7744, val_MinusLogProbMetric: 43.7744

Epoch 548: val_loss improved from 43.78611 to 43.77443, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 44.1980 - MinusLogProbMetric: 44.1980 - val_loss: 43.7744 - val_MinusLogProbMetric: 43.7744 - lr: 3.3333e-04 - 34s/epoch - 175ms/step
Epoch 549/1000
2023-10-28 07:29:02.076 
Epoch 549/1000 
	 loss: 44.3300, MinusLogProbMetric: 44.3300, val_loss: 44.5483, val_MinusLogProbMetric: 44.5483

Epoch 549: val_loss did not improve from 43.77443
196/196 - 34s - loss: 44.3300 - MinusLogProbMetric: 44.3300 - val_loss: 44.5483 - val_MinusLogProbMetric: 44.5483 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 550/1000
2023-10-28 07:29:35.349 
Epoch 550/1000 
	 loss: 44.0900, MinusLogProbMetric: 44.0900, val_loss: 44.6286, val_MinusLogProbMetric: 44.6286

Epoch 550: val_loss did not improve from 43.77443
196/196 - 33s - loss: 44.0900 - MinusLogProbMetric: 44.0900 - val_loss: 44.6286 - val_MinusLogProbMetric: 44.6286 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 551/1000
2023-10-28 07:30:08.519 
Epoch 551/1000 
	 loss: 43.8534, MinusLogProbMetric: 43.8534, val_loss: 45.2581, val_MinusLogProbMetric: 45.2581

Epoch 551: val_loss did not improve from 43.77443
196/196 - 33s - loss: 43.8534 - MinusLogProbMetric: 43.8534 - val_loss: 45.2581 - val_MinusLogProbMetric: 45.2581 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 552/1000
2023-10-28 07:30:42.011 
Epoch 552/1000 
	 loss: 44.4118, MinusLogProbMetric: 44.4118, val_loss: 44.3857, val_MinusLogProbMetric: 44.3857

Epoch 552: val_loss did not improve from 43.77443
196/196 - 33s - loss: 44.4118 - MinusLogProbMetric: 44.4118 - val_loss: 44.3857 - val_MinusLogProbMetric: 44.3857 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 553/1000
2023-10-28 07:31:15.398 
Epoch 553/1000 
	 loss: 43.8963, MinusLogProbMetric: 43.8963, val_loss: 45.5893, val_MinusLogProbMetric: 45.5893

Epoch 553: val_loss did not improve from 43.77443
196/196 - 33s - loss: 43.8963 - MinusLogProbMetric: 43.8963 - val_loss: 45.5893 - val_MinusLogProbMetric: 45.5893 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 554/1000
2023-10-28 07:31:48.028 
Epoch 554/1000 
	 loss: 43.8778, MinusLogProbMetric: 43.8778, val_loss: 44.2497, val_MinusLogProbMetric: 44.2497

Epoch 554: val_loss did not improve from 43.77443
196/196 - 33s - loss: 43.8778 - MinusLogProbMetric: 43.8778 - val_loss: 44.2497 - val_MinusLogProbMetric: 44.2497 - lr: 3.3333e-04 - 33s/epoch - 166ms/step
Epoch 555/1000
2023-10-28 07:32:21.353 
Epoch 555/1000 
	 loss: 44.0301, MinusLogProbMetric: 44.0301, val_loss: 45.0187, val_MinusLogProbMetric: 45.0187

Epoch 555: val_loss did not improve from 43.77443
196/196 - 33s - loss: 44.0301 - MinusLogProbMetric: 44.0301 - val_loss: 45.0187 - val_MinusLogProbMetric: 45.0187 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 556/1000
2023-10-28 07:32:54.576 
Epoch 556/1000 
	 loss: 43.8793, MinusLogProbMetric: 43.8793, val_loss: 45.1340, val_MinusLogProbMetric: 45.1340

Epoch 556: val_loss did not improve from 43.77443
196/196 - 33s - loss: 43.8793 - MinusLogProbMetric: 43.8793 - val_loss: 45.1340 - val_MinusLogProbMetric: 45.1340 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 557/1000
2023-10-28 07:33:28.047 
Epoch 557/1000 
	 loss: 44.0847, MinusLogProbMetric: 44.0847, val_loss: 44.7753, val_MinusLogProbMetric: 44.7753

Epoch 557: val_loss did not improve from 43.77443
196/196 - 33s - loss: 44.0847 - MinusLogProbMetric: 44.0847 - val_loss: 44.7753 - val_MinusLogProbMetric: 44.7753 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 558/1000
2023-10-28 07:34:01.523 
Epoch 558/1000 
	 loss: 43.9935, MinusLogProbMetric: 43.9935, val_loss: 49.3130, val_MinusLogProbMetric: 49.3130

Epoch 558: val_loss did not improve from 43.77443
196/196 - 33s - loss: 43.9935 - MinusLogProbMetric: 43.9935 - val_loss: 49.3130 - val_MinusLogProbMetric: 49.3130 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 559/1000
2023-10-28 07:34:34.860 
Epoch 559/1000 
	 loss: 43.9871, MinusLogProbMetric: 43.9871, val_loss: 43.7067, val_MinusLogProbMetric: 43.7067

Epoch 559: val_loss improved from 43.77443 to 43.70669, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 43.9871 - MinusLogProbMetric: 43.9871 - val_loss: 43.7067 - val_MinusLogProbMetric: 43.7067 - lr: 3.3333e-04 - 34s/epoch - 173ms/step
Epoch 560/1000
2023-10-28 07:35:08.974 
Epoch 560/1000 
	 loss: 44.1992, MinusLogProbMetric: 44.1992, val_loss: 44.2038, val_MinusLogProbMetric: 44.2038

Epoch 560: val_loss did not improve from 43.70669
196/196 - 34s - loss: 44.1992 - MinusLogProbMetric: 44.1992 - val_loss: 44.2038 - val_MinusLogProbMetric: 44.2038 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 561/1000
2023-10-28 07:35:42.062 
Epoch 561/1000 
	 loss: 44.2545, MinusLogProbMetric: 44.2545, val_loss: 44.6549, val_MinusLogProbMetric: 44.6549

Epoch 561: val_loss did not improve from 43.70669
196/196 - 33s - loss: 44.2545 - MinusLogProbMetric: 44.2545 - val_loss: 44.6549 - val_MinusLogProbMetric: 44.6549 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 562/1000
2023-10-28 07:36:13.178 
Epoch 562/1000 
	 loss: 44.0323, MinusLogProbMetric: 44.0323, val_loss: 45.5843, val_MinusLogProbMetric: 45.5843

Epoch 562: val_loss did not improve from 43.70669
196/196 - 31s - loss: 44.0323 - MinusLogProbMetric: 44.0323 - val_loss: 45.5843 - val_MinusLogProbMetric: 45.5843 - lr: 3.3333e-04 - 31s/epoch - 159ms/step
Epoch 563/1000
2023-10-28 07:36:44.181 
Epoch 563/1000 
	 loss: 44.2528, MinusLogProbMetric: 44.2528, val_loss: 44.5091, val_MinusLogProbMetric: 44.5091

Epoch 563: val_loss did not improve from 43.70669
196/196 - 31s - loss: 44.2528 - MinusLogProbMetric: 44.2528 - val_loss: 44.5091 - val_MinusLogProbMetric: 44.5091 - lr: 3.3333e-04 - 31s/epoch - 158ms/step
Epoch 564/1000
2023-10-28 07:37:16.205 
Epoch 564/1000 
	 loss: 44.5053, MinusLogProbMetric: 44.5053, val_loss: 45.0549, val_MinusLogProbMetric: 45.0549

Epoch 564: val_loss did not improve from 43.70669
196/196 - 32s - loss: 44.5053 - MinusLogProbMetric: 44.5053 - val_loss: 45.0549 - val_MinusLogProbMetric: 45.0549 - lr: 3.3333e-04 - 32s/epoch - 163ms/step
Epoch 565/1000
2023-10-28 07:37:49.423 
Epoch 565/1000 
	 loss: 43.6985, MinusLogProbMetric: 43.6985, val_loss: 45.5205, val_MinusLogProbMetric: 45.5205

Epoch 565: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.6985 - MinusLogProbMetric: 43.6985 - val_loss: 45.5205 - val_MinusLogProbMetric: 45.5205 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 566/1000
2023-10-28 07:38:22.808 
Epoch 566/1000 
	 loss: 44.1443, MinusLogProbMetric: 44.1443, val_loss: 44.6986, val_MinusLogProbMetric: 44.6986

Epoch 566: val_loss did not improve from 43.70669
196/196 - 33s - loss: 44.1443 - MinusLogProbMetric: 44.1443 - val_loss: 44.6986 - val_MinusLogProbMetric: 44.6986 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 567/1000
2023-10-28 07:38:56.461 
Epoch 567/1000 
	 loss: 43.7572, MinusLogProbMetric: 43.7572, val_loss: 44.5986, val_MinusLogProbMetric: 44.5986

Epoch 567: val_loss did not improve from 43.70669
196/196 - 34s - loss: 43.7572 - MinusLogProbMetric: 43.7572 - val_loss: 44.5986 - val_MinusLogProbMetric: 44.5986 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 568/1000
2023-10-28 07:39:29.748 
Epoch 568/1000 
	 loss: 43.9514, MinusLogProbMetric: 43.9514, val_loss: 44.9379, val_MinusLogProbMetric: 44.9379

Epoch 568: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.9514 - MinusLogProbMetric: 43.9514 - val_loss: 44.9379 - val_MinusLogProbMetric: 44.9379 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 569/1000
2023-10-28 07:40:03.271 
Epoch 569/1000 
	 loss: 43.8824, MinusLogProbMetric: 43.8824, val_loss: 44.8482, val_MinusLogProbMetric: 44.8482

Epoch 569: val_loss did not improve from 43.70669
196/196 - 34s - loss: 43.8824 - MinusLogProbMetric: 43.8824 - val_loss: 44.8482 - val_MinusLogProbMetric: 44.8482 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 570/1000
2023-10-28 07:40:36.722 
Epoch 570/1000 
	 loss: 43.7575, MinusLogProbMetric: 43.7575, val_loss: 44.9339, val_MinusLogProbMetric: 44.9339

Epoch 570: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.7575 - MinusLogProbMetric: 43.7575 - val_loss: 44.9339 - val_MinusLogProbMetric: 44.9339 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 571/1000
2023-10-28 07:41:10.083 
Epoch 571/1000 
	 loss: 43.9582, MinusLogProbMetric: 43.9582, val_loss: 44.2538, val_MinusLogProbMetric: 44.2538

Epoch 571: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.9582 - MinusLogProbMetric: 43.9582 - val_loss: 44.2538 - val_MinusLogProbMetric: 44.2538 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 572/1000
2023-10-28 07:41:43.204 
Epoch 572/1000 
	 loss: 44.0048, MinusLogProbMetric: 44.0048, val_loss: 45.3611, val_MinusLogProbMetric: 45.3611

Epoch 572: val_loss did not improve from 43.70669
196/196 - 33s - loss: 44.0048 - MinusLogProbMetric: 44.0048 - val_loss: 45.3611 - val_MinusLogProbMetric: 45.3611 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 573/1000
2023-10-28 07:42:16.688 
Epoch 573/1000 
	 loss: 43.6807, MinusLogProbMetric: 43.6807, val_loss: 44.2590, val_MinusLogProbMetric: 44.2590

Epoch 573: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.6807 - MinusLogProbMetric: 43.6807 - val_loss: 44.2590 - val_MinusLogProbMetric: 44.2590 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 574/1000
2023-10-28 07:42:50.087 
Epoch 574/1000 
	 loss: 44.0419, MinusLogProbMetric: 44.0419, val_loss: 44.8875, val_MinusLogProbMetric: 44.8875

Epoch 574: val_loss did not improve from 43.70669
196/196 - 33s - loss: 44.0419 - MinusLogProbMetric: 44.0419 - val_loss: 44.8875 - val_MinusLogProbMetric: 44.8875 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 575/1000
2023-10-28 07:43:23.259 
Epoch 575/1000 
	 loss: 43.5750, MinusLogProbMetric: 43.5750, val_loss: 45.2790, val_MinusLogProbMetric: 45.2790

Epoch 575: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.5750 - MinusLogProbMetric: 43.5750 - val_loss: 45.2790 - val_MinusLogProbMetric: 45.2790 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 576/1000
2023-10-28 07:43:56.903 
Epoch 576/1000 
	 loss: 43.8585, MinusLogProbMetric: 43.8585, val_loss: 46.2428, val_MinusLogProbMetric: 46.2428

Epoch 576: val_loss did not improve from 43.70669
196/196 - 34s - loss: 43.8585 - MinusLogProbMetric: 43.8585 - val_loss: 46.2428 - val_MinusLogProbMetric: 46.2428 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 577/1000
2023-10-28 07:44:30.283 
Epoch 577/1000 
	 loss: 43.9386, MinusLogProbMetric: 43.9386, val_loss: 44.5611, val_MinusLogProbMetric: 44.5611

Epoch 577: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.9386 - MinusLogProbMetric: 43.9386 - val_loss: 44.5611 - val_MinusLogProbMetric: 44.5611 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 578/1000
2023-10-28 07:45:02.116 
Epoch 578/1000 
	 loss: 43.8907, MinusLogProbMetric: 43.8907, val_loss: 44.3614, val_MinusLogProbMetric: 44.3614

Epoch 578: val_loss did not improve from 43.70669
196/196 - 32s - loss: 43.8907 - MinusLogProbMetric: 43.8907 - val_loss: 44.3614 - val_MinusLogProbMetric: 44.3614 - lr: 3.3333e-04 - 32s/epoch - 162ms/step
Epoch 579/1000
2023-10-28 07:45:35.356 
Epoch 579/1000 
	 loss: 43.9225, MinusLogProbMetric: 43.9225, val_loss: 44.4007, val_MinusLogProbMetric: 44.4007

Epoch 579: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.9225 - MinusLogProbMetric: 43.9225 - val_loss: 44.4007 - val_MinusLogProbMetric: 44.4007 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 580/1000
2023-10-28 07:46:08.453 
Epoch 580/1000 
	 loss: 43.9420, MinusLogProbMetric: 43.9420, val_loss: 45.0997, val_MinusLogProbMetric: 45.0997

Epoch 580: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.9420 - MinusLogProbMetric: 43.9420 - val_loss: 45.0997 - val_MinusLogProbMetric: 45.0997 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 581/1000
2023-10-28 07:46:41.963 
Epoch 581/1000 
	 loss: 43.8238, MinusLogProbMetric: 43.8238, val_loss: 45.3893, val_MinusLogProbMetric: 45.3893

Epoch 581: val_loss did not improve from 43.70669
196/196 - 34s - loss: 43.8238 - MinusLogProbMetric: 43.8238 - val_loss: 45.3893 - val_MinusLogProbMetric: 45.3893 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 582/1000
2023-10-28 07:47:15.495 
Epoch 582/1000 
	 loss: 43.6045, MinusLogProbMetric: 43.6045, val_loss: 46.0469, val_MinusLogProbMetric: 46.0469

Epoch 582: val_loss did not improve from 43.70669
196/196 - 34s - loss: 43.6045 - MinusLogProbMetric: 43.6045 - val_loss: 46.0469 - val_MinusLogProbMetric: 46.0469 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 583/1000
2023-10-28 07:47:49.062 
Epoch 583/1000 
	 loss: 44.0175, MinusLogProbMetric: 44.0175, val_loss: 44.7657, val_MinusLogProbMetric: 44.7657

Epoch 583: val_loss did not improve from 43.70669
196/196 - 34s - loss: 44.0175 - MinusLogProbMetric: 44.0175 - val_loss: 44.7657 - val_MinusLogProbMetric: 44.7657 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 584/1000
2023-10-28 07:48:22.715 
Epoch 584/1000 
	 loss: 43.8080, MinusLogProbMetric: 43.8080, val_loss: 43.7471, val_MinusLogProbMetric: 43.7471

Epoch 584: val_loss did not improve from 43.70669
196/196 - 34s - loss: 43.8080 - MinusLogProbMetric: 43.8080 - val_loss: 43.7471 - val_MinusLogProbMetric: 43.7471 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 585/1000
2023-10-28 07:48:55.937 
Epoch 585/1000 
	 loss: 44.2054, MinusLogProbMetric: 44.2054, val_loss: 44.3669, val_MinusLogProbMetric: 44.3669

Epoch 585: val_loss did not improve from 43.70669
196/196 - 33s - loss: 44.2054 - MinusLogProbMetric: 44.2054 - val_loss: 44.3669 - val_MinusLogProbMetric: 44.3669 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 586/1000
2023-10-28 07:49:29.036 
Epoch 586/1000 
	 loss: 44.6861, MinusLogProbMetric: 44.6861, val_loss: 44.1094, val_MinusLogProbMetric: 44.1094

Epoch 586: val_loss did not improve from 43.70669
196/196 - 33s - loss: 44.6861 - MinusLogProbMetric: 44.6861 - val_loss: 44.1094 - val_MinusLogProbMetric: 44.1094 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 587/1000
2023-10-28 07:50:02.386 
Epoch 587/1000 
	 loss: 44.1056, MinusLogProbMetric: 44.1056, val_loss: 44.3352, val_MinusLogProbMetric: 44.3352

Epoch 587: val_loss did not improve from 43.70669
196/196 - 33s - loss: 44.1056 - MinusLogProbMetric: 44.1056 - val_loss: 44.3352 - val_MinusLogProbMetric: 44.3352 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 588/1000
2023-10-28 07:50:35.892 
Epoch 588/1000 
	 loss: 43.7545, MinusLogProbMetric: 43.7545, val_loss: 44.3157, val_MinusLogProbMetric: 44.3157

Epoch 588: val_loss did not improve from 43.70669
196/196 - 34s - loss: 43.7545 - MinusLogProbMetric: 43.7545 - val_loss: 44.3157 - val_MinusLogProbMetric: 44.3157 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 589/1000
2023-10-28 07:51:08.968 
Epoch 589/1000 
	 loss: 43.9825, MinusLogProbMetric: 43.9825, val_loss: 43.9267, val_MinusLogProbMetric: 43.9267

Epoch 589: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.9825 - MinusLogProbMetric: 43.9825 - val_loss: 43.9267 - val_MinusLogProbMetric: 43.9267 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 590/1000
2023-10-28 07:51:42.336 
Epoch 590/1000 
	 loss: 43.5076, MinusLogProbMetric: 43.5076, val_loss: 44.3206, val_MinusLogProbMetric: 44.3206

Epoch 590: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.5076 - MinusLogProbMetric: 43.5076 - val_loss: 44.3206 - val_MinusLogProbMetric: 44.3206 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 591/1000
2023-10-28 07:52:15.649 
Epoch 591/1000 
	 loss: 43.7477, MinusLogProbMetric: 43.7477, val_loss: 44.1141, val_MinusLogProbMetric: 44.1141

Epoch 591: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.7477 - MinusLogProbMetric: 43.7477 - val_loss: 44.1141 - val_MinusLogProbMetric: 44.1141 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 592/1000
2023-10-28 07:52:48.838 
Epoch 592/1000 
	 loss: 43.4018, MinusLogProbMetric: 43.4018, val_loss: 43.8628, val_MinusLogProbMetric: 43.8628

Epoch 592: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.4018 - MinusLogProbMetric: 43.4018 - val_loss: 43.8628 - val_MinusLogProbMetric: 43.8628 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 593/1000
2023-10-28 07:53:21.938 
Epoch 593/1000 
	 loss: 43.9659, MinusLogProbMetric: 43.9659, val_loss: 47.3961, val_MinusLogProbMetric: 47.3961

Epoch 593: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.9659 - MinusLogProbMetric: 43.9659 - val_loss: 47.3961 - val_MinusLogProbMetric: 47.3961 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 594/1000
2023-10-28 07:53:55.110 
Epoch 594/1000 
	 loss: 44.3426, MinusLogProbMetric: 44.3426, val_loss: 44.9670, val_MinusLogProbMetric: 44.9670

Epoch 594: val_loss did not improve from 43.70669
196/196 - 33s - loss: 44.3426 - MinusLogProbMetric: 44.3426 - val_loss: 44.9670 - val_MinusLogProbMetric: 44.9670 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 595/1000
2023-10-28 07:54:28.281 
Epoch 595/1000 
	 loss: 43.6890, MinusLogProbMetric: 43.6890, val_loss: 44.1677, val_MinusLogProbMetric: 44.1677

Epoch 595: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.6890 - MinusLogProbMetric: 43.6890 - val_loss: 44.1677 - val_MinusLogProbMetric: 44.1677 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 596/1000
2023-10-28 07:55:01.516 
Epoch 596/1000 
	 loss: 43.6470, MinusLogProbMetric: 43.6470, val_loss: 44.8362, val_MinusLogProbMetric: 44.8362

Epoch 596: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.6470 - MinusLogProbMetric: 43.6470 - val_loss: 44.8362 - val_MinusLogProbMetric: 44.8362 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 597/1000
2023-10-28 07:55:34.286 
Epoch 597/1000 
	 loss: 43.9132, MinusLogProbMetric: 43.9132, val_loss: 44.6953, val_MinusLogProbMetric: 44.6953

Epoch 597: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.9132 - MinusLogProbMetric: 43.9132 - val_loss: 44.6953 - val_MinusLogProbMetric: 44.6953 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 598/1000
2023-10-28 07:56:07.947 
Epoch 598/1000 
	 loss: 43.6617, MinusLogProbMetric: 43.6617, val_loss: 44.1347, val_MinusLogProbMetric: 44.1347

Epoch 598: val_loss did not improve from 43.70669
196/196 - 34s - loss: 43.6617 - MinusLogProbMetric: 43.6617 - val_loss: 44.1347 - val_MinusLogProbMetric: 44.1347 - lr: 3.3333e-04 - 34s/epoch - 172ms/step
Epoch 599/1000
2023-10-28 07:56:40.683 
Epoch 599/1000 
	 loss: 43.6712, MinusLogProbMetric: 43.6712, val_loss: 44.2078, val_MinusLogProbMetric: 44.2078

Epoch 599: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.6712 - MinusLogProbMetric: 43.6712 - val_loss: 44.2078 - val_MinusLogProbMetric: 44.2078 - lr: 3.3333e-04 - 33s/epoch - 167ms/step
Epoch 600/1000
2023-10-28 07:57:14.294 
Epoch 600/1000 
	 loss: 44.2289, MinusLogProbMetric: 44.2289, val_loss: 44.6853, val_MinusLogProbMetric: 44.6853

Epoch 600: val_loss did not improve from 43.70669
196/196 - 34s - loss: 44.2289 - MinusLogProbMetric: 44.2289 - val_loss: 44.6853 - val_MinusLogProbMetric: 44.6853 - lr: 3.3333e-04 - 34s/epoch - 171ms/step
Epoch 601/1000
2023-10-28 07:57:47.741 
Epoch 601/1000 
	 loss: 43.4921, MinusLogProbMetric: 43.4921, val_loss: 44.0116, val_MinusLogProbMetric: 44.0116

Epoch 601: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.4921 - MinusLogProbMetric: 43.4921 - val_loss: 44.0116 - val_MinusLogProbMetric: 44.0116 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 602/1000
2023-10-28 07:58:21.146 
Epoch 602/1000 
	 loss: 43.5688, MinusLogProbMetric: 43.5688, val_loss: 43.7573, val_MinusLogProbMetric: 43.7573

Epoch 602: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.5688 - MinusLogProbMetric: 43.5688 - val_loss: 43.7573 - val_MinusLogProbMetric: 43.7573 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 603/1000
2023-10-28 07:58:54.495 
Epoch 603/1000 
	 loss: 43.4865, MinusLogProbMetric: 43.4865, val_loss: 44.5380, val_MinusLogProbMetric: 44.5380

Epoch 603: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.4865 - MinusLogProbMetric: 43.4865 - val_loss: 44.5380 - val_MinusLogProbMetric: 44.5380 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 604/1000
2023-10-28 07:59:27.955 
Epoch 604/1000 
	 loss: 44.0208, MinusLogProbMetric: 44.0208, val_loss: 44.6252, val_MinusLogProbMetric: 44.6252

Epoch 604: val_loss did not improve from 43.70669
196/196 - 33s - loss: 44.0208 - MinusLogProbMetric: 44.0208 - val_loss: 44.6252 - val_MinusLogProbMetric: 44.6252 - lr: 3.3333e-04 - 33s/epoch - 171ms/step
Epoch 605/1000
2023-10-28 08:00:01.299 
Epoch 605/1000 
	 loss: 43.5609, MinusLogProbMetric: 43.5609, val_loss: 44.1885, val_MinusLogProbMetric: 44.1885

Epoch 605: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.5609 - MinusLogProbMetric: 43.5609 - val_loss: 44.1885 - val_MinusLogProbMetric: 44.1885 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 606/1000
2023-10-28 08:00:34.713 
Epoch 606/1000 
	 loss: 43.6368, MinusLogProbMetric: 43.6368, val_loss: 44.1683, val_MinusLogProbMetric: 44.1683

Epoch 606: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.6368 - MinusLogProbMetric: 43.6368 - val_loss: 44.1683 - val_MinusLogProbMetric: 44.1683 - lr: 3.3333e-04 - 33s/epoch - 170ms/step
Epoch 607/1000
2023-10-28 08:01:07.859 
Epoch 607/1000 
	 loss: 43.7311, MinusLogProbMetric: 43.7311, val_loss: 44.3671, val_MinusLogProbMetric: 44.3671

Epoch 607: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.7311 - MinusLogProbMetric: 43.7311 - val_loss: 44.3671 - val_MinusLogProbMetric: 44.3671 - lr: 3.3333e-04 - 33s/epoch - 169ms/step
Epoch 608/1000
2023-10-28 08:01:40.888 
Epoch 608/1000 
	 loss: 43.7909, MinusLogProbMetric: 43.7909, val_loss: 44.8720, val_MinusLogProbMetric: 44.8720

Epoch 608: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.7909 - MinusLogProbMetric: 43.7909 - val_loss: 44.8720 - val_MinusLogProbMetric: 44.8720 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 609/1000
2023-10-28 08:02:13.888 
Epoch 609/1000 
	 loss: 43.5956, MinusLogProbMetric: 43.5956, val_loss: 44.5343, val_MinusLogProbMetric: 44.5343

Epoch 609: val_loss did not improve from 43.70669
196/196 - 33s - loss: 43.5956 - MinusLogProbMetric: 43.5956 - val_loss: 44.5343 - val_MinusLogProbMetric: 44.5343 - lr: 3.3333e-04 - 33s/epoch - 168ms/step
Epoch 610/1000
2023-10-28 08:02:46.793 
Epoch 610/1000 
	 loss: 42.2008, MinusLogProbMetric: 42.2008, val_loss: 42.9975, val_MinusLogProbMetric: 42.9975

Epoch 610: val_loss improved from 43.70669 to 42.99751, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 42.2008 - MinusLogProbMetric: 42.2008 - val_loss: 42.9975 - val_MinusLogProbMetric: 42.9975 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 611/1000
2023-10-28 08:03:20.208 
Epoch 611/1000 
	 loss: 42.1571, MinusLogProbMetric: 42.1571, val_loss: 43.5129, val_MinusLogProbMetric: 43.5129

Epoch 611: val_loss did not improve from 42.99751
196/196 - 33s - loss: 42.1571 - MinusLogProbMetric: 42.1571 - val_loss: 43.5129 - val_MinusLogProbMetric: 43.5129 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 612/1000
2023-10-28 08:03:53.435 
Epoch 612/1000 
	 loss: 42.1849, MinusLogProbMetric: 42.1849, val_loss: 43.3515, val_MinusLogProbMetric: 43.3515

Epoch 612: val_loss did not improve from 42.99751
196/196 - 33s - loss: 42.1849 - MinusLogProbMetric: 42.1849 - val_loss: 43.3515 - val_MinusLogProbMetric: 43.3515 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 613/1000
2023-10-28 08:04:26.780 
Epoch 613/1000 
	 loss: 42.5099, MinusLogProbMetric: 42.5099, val_loss: 43.1627, val_MinusLogProbMetric: 43.1627

Epoch 613: val_loss did not improve from 42.99751
196/196 - 33s - loss: 42.5099 - MinusLogProbMetric: 42.5099 - val_loss: 43.1627 - val_MinusLogProbMetric: 43.1627 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 614/1000
2023-10-28 08:05:00.153 
Epoch 614/1000 
	 loss: 42.3110, MinusLogProbMetric: 42.3110, val_loss: 43.5337, val_MinusLogProbMetric: 43.5337

Epoch 614: val_loss did not improve from 42.99751
196/196 - 33s - loss: 42.3110 - MinusLogProbMetric: 42.3110 - val_loss: 43.5337 - val_MinusLogProbMetric: 43.5337 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 615/1000
2023-10-28 08:05:33.458 
Epoch 615/1000 
	 loss: 42.3134, MinusLogProbMetric: 42.3134, val_loss: 43.9595, val_MinusLogProbMetric: 43.9595

Epoch 615: val_loss did not improve from 42.99751
196/196 - 33s - loss: 42.3134 - MinusLogProbMetric: 42.3134 - val_loss: 43.9595 - val_MinusLogProbMetric: 43.9595 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 616/1000
2023-10-28 08:06:06.518 
Epoch 616/1000 
	 loss: 42.3129, MinusLogProbMetric: 42.3129, val_loss: 43.1250, val_MinusLogProbMetric: 43.1250

Epoch 616: val_loss did not improve from 42.99751
196/196 - 33s - loss: 42.3129 - MinusLogProbMetric: 42.3129 - val_loss: 43.1250 - val_MinusLogProbMetric: 43.1250 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 617/1000
2023-10-28 08:06:39.770 
Epoch 617/1000 
	 loss: 42.3970, MinusLogProbMetric: 42.3970, val_loss: 43.0592, val_MinusLogProbMetric: 43.0592

Epoch 617: val_loss did not improve from 42.99751
196/196 - 33s - loss: 42.3970 - MinusLogProbMetric: 42.3970 - val_loss: 43.0592 - val_MinusLogProbMetric: 43.0592 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 618/1000
2023-10-28 08:07:12.924 
Epoch 618/1000 
	 loss: 42.2444, MinusLogProbMetric: 42.2444, val_loss: 43.6393, val_MinusLogProbMetric: 43.6393

Epoch 618: val_loss did not improve from 42.99751
196/196 - 33s - loss: 42.2444 - MinusLogProbMetric: 42.2444 - val_loss: 43.6393 - val_MinusLogProbMetric: 43.6393 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 619/1000
2023-10-28 08:07:46.189 
Epoch 619/1000 
	 loss: 42.3786, MinusLogProbMetric: 42.3786, val_loss: 44.1388, val_MinusLogProbMetric: 44.1388

Epoch 619: val_loss did not improve from 42.99751
196/196 - 33s - loss: 42.3786 - MinusLogProbMetric: 42.3786 - val_loss: 44.1388 - val_MinusLogProbMetric: 44.1388 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 620/1000
2023-10-28 08:08:19.501 
Epoch 620/1000 
	 loss: 42.3509, MinusLogProbMetric: 42.3509, val_loss: 44.7886, val_MinusLogProbMetric: 44.7886

Epoch 620: val_loss did not improve from 42.99751
196/196 - 33s - loss: 42.3509 - MinusLogProbMetric: 42.3509 - val_loss: 44.7886 - val_MinusLogProbMetric: 44.7886 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 621/1000
2023-10-28 08:08:53.358 
Epoch 621/1000 
	 loss: 42.2616, MinusLogProbMetric: 42.2616, val_loss: 43.0408, val_MinusLogProbMetric: 43.0408

Epoch 621: val_loss did not improve from 42.99751
196/196 - 34s - loss: 42.2616 - MinusLogProbMetric: 42.2616 - val_loss: 43.0408 - val_MinusLogProbMetric: 43.0408 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 622/1000
2023-10-28 08:09:27.092 
Epoch 622/1000 
	 loss: 42.1546, MinusLogProbMetric: 42.1546, val_loss: 43.1072, val_MinusLogProbMetric: 43.1072

Epoch 622: val_loss did not improve from 42.99751
196/196 - 34s - loss: 42.1546 - MinusLogProbMetric: 42.1546 - val_loss: 43.1072 - val_MinusLogProbMetric: 43.1072 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 623/1000
2023-10-28 08:10:00.190 
Epoch 623/1000 
	 loss: 42.1553, MinusLogProbMetric: 42.1553, val_loss: 43.9043, val_MinusLogProbMetric: 43.9043

Epoch 623: val_loss did not improve from 42.99751
196/196 - 33s - loss: 42.1553 - MinusLogProbMetric: 42.1553 - val_loss: 43.9043 - val_MinusLogProbMetric: 43.9043 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 624/1000
2023-10-28 08:10:33.254 
Epoch 624/1000 
	 loss: 42.1553, MinusLogProbMetric: 42.1553, val_loss: 43.1617, val_MinusLogProbMetric: 43.1617

Epoch 624: val_loss did not improve from 42.99751
196/196 - 33s - loss: 42.1553 - MinusLogProbMetric: 42.1553 - val_loss: 43.1617 - val_MinusLogProbMetric: 43.1617 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 625/1000
2023-10-28 08:11:06.527 
Epoch 625/1000 
	 loss: 42.1484, MinusLogProbMetric: 42.1484, val_loss: 43.1271, val_MinusLogProbMetric: 43.1271

Epoch 625: val_loss did not improve from 42.99751
196/196 - 33s - loss: 42.1484 - MinusLogProbMetric: 42.1484 - val_loss: 43.1271 - val_MinusLogProbMetric: 43.1271 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 626/1000
2023-10-28 08:11:39.762 
Epoch 626/1000 
	 loss: 42.2117, MinusLogProbMetric: 42.2117, val_loss: 42.9958, val_MinusLogProbMetric: 42.9958

Epoch 626: val_loss improved from 42.99751 to 42.99581, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 42.2117 - MinusLogProbMetric: 42.2117 - val_loss: 42.9958 - val_MinusLogProbMetric: 42.9958 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 627/1000
2023-10-28 08:12:13.703 
Epoch 627/1000 
	 loss: 42.2260, MinusLogProbMetric: 42.2260, val_loss: 43.0012, val_MinusLogProbMetric: 43.0012

Epoch 627: val_loss did not improve from 42.99581
196/196 - 33s - loss: 42.2260 - MinusLogProbMetric: 42.2260 - val_loss: 43.0012 - val_MinusLogProbMetric: 43.0012 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 628/1000
2023-10-28 08:12:47.138 
Epoch 628/1000 
	 loss: 42.1804, MinusLogProbMetric: 42.1804, val_loss: 43.0096, val_MinusLogProbMetric: 43.0096

Epoch 628: val_loss did not improve from 42.99581
196/196 - 33s - loss: 42.1804 - MinusLogProbMetric: 42.1804 - val_loss: 43.0096 - val_MinusLogProbMetric: 43.0096 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 629/1000
2023-10-28 08:13:20.025 
Epoch 629/1000 
	 loss: 42.1146, MinusLogProbMetric: 42.1146, val_loss: 43.2526, val_MinusLogProbMetric: 43.2526

Epoch 629: val_loss did not improve from 42.99581
196/196 - 33s - loss: 42.1146 - MinusLogProbMetric: 42.1146 - val_loss: 43.2526 - val_MinusLogProbMetric: 43.2526 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 630/1000
2023-10-28 08:13:53.561 
Epoch 630/1000 
	 loss: 42.1014, MinusLogProbMetric: 42.1014, val_loss: 43.1943, val_MinusLogProbMetric: 43.1943

Epoch 630: val_loss did not improve from 42.99581
196/196 - 34s - loss: 42.1014 - MinusLogProbMetric: 42.1014 - val_loss: 43.1943 - val_MinusLogProbMetric: 43.1943 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 631/1000
2023-10-28 08:14:27.009 
Epoch 631/1000 
	 loss: 42.3692, MinusLogProbMetric: 42.3692, val_loss: 43.0960, val_MinusLogProbMetric: 43.0960

Epoch 631: val_loss did not improve from 42.99581
196/196 - 33s - loss: 42.3692 - MinusLogProbMetric: 42.3692 - val_loss: 43.0960 - val_MinusLogProbMetric: 43.0960 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 632/1000
2023-10-28 08:15:00.244 
Epoch 632/1000 
	 loss: 42.4307, MinusLogProbMetric: 42.4307, val_loss: 43.4851, val_MinusLogProbMetric: 43.4851

Epoch 632: val_loss did not improve from 42.99581
196/196 - 33s - loss: 42.4307 - MinusLogProbMetric: 42.4307 - val_loss: 43.4851 - val_MinusLogProbMetric: 43.4851 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 633/1000
2023-10-28 08:15:33.707 
Epoch 633/1000 
	 loss: 42.1953, MinusLogProbMetric: 42.1953, val_loss: 42.8629, val_MinusLogProbMetric: 42.8629

Epoch 633: val_loss improved from 42.99581 to 42.86288, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 42.1953 - MinusLogProbMetric: 42.1953 - val_loss: 42.8629 - val_MinusLogProbMetric: 42.8629 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 634/1000
2023-10-28 08:16:07.723 
Epoch 634/1000 
	 loss: 42.1925, MinusLogProbMetric: 42.1925, val_loss: 43.6682, val_MinusLogProbMetric: 43.6682

Epoch 634: val_loss did not improve from 42.86288
196/196 - 33s - loss: 42.1925 - MinusLogProbMetric: 42.1925 - val_loss: 43.6682 - val_MinusLogProbMetric: 43.6682 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 635/1000
2023-10-28 08:16:41.095 
Epoch 635/1000 
	 loss: 42.2351, MinusLogProbMetric: 42.2351, val_loss: 43.2319, val_MinusLogProbMetric: 43.2319

Epoch 635: val_loss did not improve from 42.86288
196/196 - 33s - loss: 42.2351 - MinusLogProbMetric: 42.2351 - val_loss: 43.2319 - val_MinusLogProbMetric: 43.2319 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 636/1000
2023-10-28 08:17:14.760 
Epoch 636/1000 
	 loss: 42.2261, MinusLogProbMetric: 42.2261, val_loss: 42.9955, val_MinusLogProbMetric: 42.9955

Epoch 636: val_loss did not improve from 42.86288
196/196 - 34s - loss: 42.2261 - MinusLogProbMetric: 42.2261 - val_loss: 42.9955 - val_MinusLogProbMetric: 42.9955 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 637/1000
2023-10-28 08:17:48.137 
Epoch 637/1000 
	 loss: 42.2287, MinusLogProbMetric: 42.2287, val_loss: 43.0123, val_MinusLogProbMetric: 43.0123

Epoch 637: val_loss did not improve from 42.86288
196/196 - 33s - loss: 42.2287 - MinusLogProbMetric: 42.2287 - val_loss: 43.0123 - val_MinusLogProbMetric: 43.0123 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 638/1000
2023-10-28 08:18:21.630 
Epoch 638/1000 
	 loss: 42.1798, MinusLogProbMetric: 42.1798, val_loss: 43.1957, val_MinusLogProbMetric: 43.1957

Epoch 638: val_loss did not improve from 42.86288
196/196 - 33s - loss: 42.1798 - MinusLogProbMetric: 42.1798 - val_loss: 43.1957 - val_MinusLogProbMetric: 43.1957 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 639/1000
2023-10-28 08:18:55.443 
Epoch 639/1000 
	 loss: 42.2402, MinusLogProbMetric: 42.2402, val_loss: 43.3384, val_MinusLogProbMetric: 43.3384

Epoch 639: val_loss did not improve from 42.86288
196/196 - 34s - loss: 42.2402 - MinusLogProbMetric: 42.2402 - val_loss: 43.3384 - val_MinusLogProbMetric: 43.3384 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 640/1000
2023-10-28 08:19:28.787 
Epoch 640/1000 
	 loss: 42.2249, MinusLogProbMetric: 42.2249, val_loss: 42.9343, val_MinusLogProbMetric: 42.9343

Epoch 640: val_loss did not improve from 42.86288
196/196 - 33s - loss: 42.2249 - MinusLogProbMetric: 42.2249 - val_loss: 42.9343 - val_MinusLogProbMetric: 42.9343 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 641/1000
2023-10-28 08:20:01.917 
Epoch 641/1000 
	 loss: 42.1883, MinusLogProbMetric: 42.1883, val_loss: 43.0745, val_MinusLogProbMetric: 43.0745

Epoch 641: val_loss did not improve from 42.86288
196/196 - 33s - loss: 42.1883 - MinusLogProbMetric: 42.1883 - val_loss: 43.0745 - val_MinusLogProbMetric: 43.0745 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 642/1000
2023-10-28 08:20:34.966 
Epoch 642/1000 
	 loss: 42.1343, MinusLogProbMetric: 42.1343, val_loss: 43.2039, val_MinusLogProbMetric: 43.2039

Epoch 642: val_loss did not improve from 42.86288
196/196 - 33s - loss: 42.1343 - MinusLogProbMetric: 42.1343 - val_loss: 43.2039 - val_MinusLogProbMetric: 43.2039 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 643/1000
2023-10-28 08:21:07.919 
Epoch 643/1000 
	 loss: 42.2875, MinusLogProbMetric: 42.2875, val_loss: 43.4706, val_MinusLogProbMetric: 43.4706

Epoch 643: val_loss did not improve from 42.86288
196/196 - 33s - loss: 42.2875 - MinusLogProbMetric: 42.2875 - val_loss: 43.4706 - val_MinusLogProbMetric: 43.4706 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 644/1000
2023-10-28 08:21:40.871 
Epoch 644/1000 
	 loss: 42.2348, MinusLogProbMetric: 42.2348, val_loss: 42.9083, val_MinusLogProbMetric: 42.9083

Epoch 644: val_loss did not improve from 42.86288
196/196 - 33s - loss: 42.2348 - MinusLogProbMetric: 42.2348 - val_loss: 42.9083 - val_MinusLogProbMetric: 42.9083 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 645/1000
2023-10-28 08:22:14.301 
Epoch 645/1000 
	 loss: 42.5395, MinusLogProbMetric: 42.5395, val_loss: 43.0657, val_MinusLogProbMetric: 43.0657

Epoch 645: val_loss did not improve from 42.86288
196/196 - 33s - loss: 42.5395 - MinusLogProbMetric: 42.5395 - val_loss: 43.0657 - val_MinusLogProbMetric: 43.0657 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 646/1000
2023-10-28 08:22:47.561 
Epoch 646/1000 
	 loss: 42.0830, MinusLogProbMetric: 42.0830, val_loss: 44.2289, val_MinusLogProbMetric: 44.2289

Epoch 646: val_loss did not improve from 42.86288
196/196 - 33s - loss: 42.0830 - MinusLogProbMetric: 42.0830 - val_loss: 44.2289 - val_MinusLogProbMetric: 44.2289 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 647/1000
2023-10-28 08:23:20.938 
Epoch 647/1000 
	 loss: 42.2058, MinusLogProbMetric: 42.2058, val_loss: 43.6853, val_MinusLogProbMetric: 43.6853

Epoch 647: val_loss did not improve from 42.86288
196/196 - 33s - loss: 42.2058 - MinusLogProbMetric: 42.2058 - val_loss: 43.6853 - val_MinusLogProbMetric: 43.6853 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 648/1000
2023-10-28 08:23:54.362 
Epoch 648/1000 
	 loss: 42.2099, MinusLogProbMetric: 42.2099, val_loss: 42.8192, val_MinusLogProbMetric: 42.8192

Epoch 648: val_loss improved from 42.86288 to 42.81922, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 42.2099 - MinusLogProbMetric: 42.2099 - val_loss: 42.8192 - val_MinusLogProbMetric: 42.8192 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 649/1000
2023-10-28 08:24:28.487 
Epoch 649/1000 
	 loss: 42.0974, MinusLogProbMetric: 42.0974, val_loss: 43.2121, val_MinusLogProbMetric: 43.2121

Epoch 649: val_loss did not improve from 42.81922
196/196 - 34s - loss: 42.0974 - MinusLogProbMetric: 42.0974 - val_loss: 43.2121 - val_MinusLogProbMetric: 43.2121 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 650/1000
2023-10-28 08:25:01.910 
Epoch 650/1000 
	 loss: 42.1307, MinusLogProbMetric: 42.1307, val_loss: 43.1811, val_MinusLogProbMetric: 43.1811

Epoch 650: val_loss did not improve from 42.81922
196/196 - 33s - loss: 42.1307 - MinusLogProbMetric: 42.1307 - val_loss: 43.1811 - val_MinusLogProbMetric: 43.1811 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 651/1000
2023-10-28 08:25:35.165 
Epoch 651/1000 
	 loss: 42.1440, MinusLogProbMetric: 42.1440, val_loss: 43.5156, val_MinusLogProbMetric: 43.5156

Epoch 651: val_loss did not improve from 42.81922
196/196 - 33s - loss: 42.1440 - MinusLogProbMetric: 42.1440 - val_loss: 43.5156 - val_MinusLogProbMetric: 43.5156 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 652/1000
2023-10-28 08:26:08.288 
Epoch 652/1000 
	 loss: 42.2830, MinusLogProbMetric: 42.2830, val_loss: 43.0189, val_MinusLogProbMetric: 43.0189

Epoch 652: val_loss did not improve from 42.81922
196/196 - 33s - loss: 42.2830 - MinusLogProbMetric: 42.2830 - val_loss: 43.0189 - val_MinusLogProbMetric: 43.0189 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 653/1000
2023-10-28 08:26:41.657 
Epoch 653/1000 
	 loss: 42.0651, MinusLogProbMetric: 42.0651, val_loss: 43.0612, val_MinusLogProbMetric: 43.0612

Epoch 653: val_loss did not improve from 42.81922
196/196 - 33s - loss: 42.0651 - MinusLogProbMetric: 42.0651 - val_loss: 43.0612 - val_MinusLogProbMetric: 43.0612 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 654/1000
2023-10-28 08:27:14.745 
Epoch 654/1000 
	 loss: 42.2291, MinusLogProbMetric: 42.2291, val_loss: 42.8465, val_MinusLogProbMetric: 42.8465

Epoch 654: val_loss did not improve from 42.81922
196/196 - 33s - loss: 42.2291 - MinusLogProbMetric: 42.2291 - val_loss: 42.8465 - val_MinusLogProbMetric: 42.8465 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 655/1000
2023-10-28 08:27:48.057 
Epoch 655/1000 
	 loss: 42.5667, MinusLogProbMetric: 42.5667, val_loss: 42.7544, val_MinusLogProbMetric: 42.7544

Epoch 655: val_loss improved from 42.81922 to 42.75441, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 42.5667 - MinusLogProbMetric: 42.5667 - val_loss: 42.7544 - val_MinusLogProbMetric: 42.7544 - lr: 1.6667e-04 - 34s/epoch - 173ms/step
Epoch 656/1000
2023-10-28 08:28:21.645 
Epoch 656/1000 
	 loss: 42.2754, MinusLogProbMetric: 42.2754, val_loss: 43.3643, val_MinusLogProbMetric: 43.3643

Epoch 656: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.2754 - MinusLogProbMetric: 42.2754 - val_loss: 43.3643 - val_MinusLogProbMetric: 43.3643 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 657/1000
2023-10-28 08:28:54.903 
Epoch 657/1000 
	 loss: 42.3280, MinusLogProbMetric: 42.3280, val_loss: 42.9399, val_MinusLogProbMetric: 42.9399

Epoch 657: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.3280 - MinusLogProbMetric: 42.3280 - val_loss: 42.9399 - val_MinusLogProbMetric: 42.9399 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 658/1000
2023-10-28 08:29:28.072 
Epoch 658/1000 
	 loss: 42.1111, MinusLogProbMetric: 42.1111, val_loss: 43.1837, val_MinusLogProbMetric: 43.1837

Epoch 658: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.1111 - MinusLogProbMetric: 42.1111 - val_loss: 43.1837 - val_MinusLogProbMetric: 43.1837 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 659/1000
2023-10-28 08:30:01.339 
Epoch 659/1000 
	 loss: 42.1995, MinusLogProbMetric: 42.1995, val_loss: 43.2676, val_MinusLogProbMetric: 43.2676

Epoch 659: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.1995 - MinusLogProbMetric: 42.1995 - val_loss: 43.2676 - val_MinusLogProbMetric: 43.2676 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 660/1000
2023-10-28 08:30:34.336 
Epoch 660/1000 
	 loss: 42.3210, MinusLogProbMetric: 42.3210, val_loss: 43.0287, val_MinusLogProbMetric: 43.0287

Epoch 660: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.3210 - MinusLogProbMetric: 42.3210 - val_loss: 43.0287 - val_MinusLogProbMetric: 43.0287 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 661/1000
2023-10-28 08:31:07.355 
Epoch 661/1000 
	 loss: 42.1983, MinusLogProbMetric: 42.1983, val_loss: 43.3365, val_MinusLogProbMetric: 43.3365

Epoch 661: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.1983 - MinusLogProbMetric: 42.1983 - val_loss: 43.3365 - val_MinusLogProbMetric: 43.3365 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 662/1000
2023-10-28 08:31:40.704 
Epoch 662/1000 
	 loss: 42.2602, MinusLogProbMetric: 42.2602, val_loss: 43.0383, val_MinusLogProbMetric: 43.0383

Epoch 662: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.2602 - MinusLogProbMetric: 42.2602 - val_loss: 43.0383 - val_MinusLogProbMetric: 43.0383 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 663/1000
2023-10-28 08:32:13.873 
Epoch 663/1000 
	 loss: 42.1809, MinusLogProbMetric: 42.1809, val_loss: 42.9822, val_MinusLogProbMetric: 42.9822

Epoch 663: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.1809 - MinusLogProbMetric: 42.1809 - val_loss: 42.9822 - val_MinusLogProbMetric: 42.9822 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 664/1000
2023-10-28 08:32:45.762 
Epoch 664/1000 
	 loss: 42.1623, MinusLogProbMetric: 42.1623, val_loss: 44.0874, val_MinusLogProbMetric: 44.0874

Epoch 664: val_loss did not improve from 42.75441
196/196 - 32s - loss: 42.1623 - MinusLogProbMetric: 42.1623 - val_loss: 44.0874 - val_MinusLogProbMetric: 44.0874 - lr: 1.6667e-04 - 32s/epoch - 163ms/step
Epoch 665/1000
2023-10-28 08:33:18.191 
Epoch 665/1000 
	 loss: 42.2978, MinusLogProbMetric: 42.2978, val_loss: 43.0438, val_MinusLogProbMetric: 43.0438

Epoch 665: val_loss did not improve from 42.75441
196/196 - 32s - loss: 42.2978 - MinusLogProbMetric: 42.2978 - val_loss: 43.0438 - val_MinusLogProbMetric: 43.0438 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 666/1000
2023-10-28 08:33:51.387 
Epoch 666/1000 
	 loss: 42.1847, MinusLogProbMetric: 42.1847, val_loss: 43.0708, val_MinusLogProbMetric: 43.0708

Epoch 666: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.1847 - MinusLogProbMetric: 42.1847 - val_loss: 43.0708 - val_MinusLogProbMetric: 43.0708 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 667/1000
2023-10-28 08:34:24.776 
Epoch 667/1000 
	 loss: 42.1920, MinusLogProbMetric: 42.1920, val_loss: 43.1436, val_MinusLogProbMetric: 43.1436

Epoch 667: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.1920 - MinusLogProbMetric: 42.1920 - val_loss: 43.1436 - val_MinusLogProbMetric: 43.1436 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 668/1000
2023-10-28 08:34:57.923 
Epoch 668/1000 
	 loss: 42.2645, MinusLogProbMetric: 42.2645, val_loss: 44.0572, val_MinusLogProbMetric: 44.0572

Epoch 668: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.2645 - MinusLogProbMetric: 42.2645 - val_loss: 44.0572 - val_MinusLogProbMetric: 44.0572 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 669/1000
2023-10-28 08:35:30.704 
Epoch 669/1000 
	 loss: 42.2313, MinusLogProbMetric: 42.2313, val_loss: 43.2288, val_MinusLogProbMetric: 43.2288

Epoch 669: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.2313 - MinusLogProbMetric: 42.2313 - val_loss: 43.2288 - val_MinusLogProbMetric: 43.2288 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 670/1000
2023-10-28 08:36:03.833 
Epoch 670/1000 
	 loss: 42.4453, MinusLogProbMetric: 42.4453, val_loss: 42.9712, val_MinusLogProbMetric: 42.9712

Epoch 670: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.4453 - MinusLogProbMetric: 42.4453 - val_loss: 42.9712 - val_MinusLogProbMetric: 42.9712 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 671/1000
2023-10-28 08:36:37.182 
Epoch 671/1000 
	 loss: 42.4611, MinusLogProbMetric: 42.4611, val_loss: 43.2178, val_MinusLogProbMetric: 43.2178

Epoch 671: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.4611 - MinusLogProbMetric: 42.4611 - val_loss: 43.2178 - val_MinusLogProbMetric: 43.2178 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 672/1000
2023-10-28 08:37:10.504 
Epoch 672/1000 
	 loss: 42.1357, MinusLogProbMetric: 42.1357, val_loss: 42.8186, val_MinusLogProbMetric: 42.8186

Epoch 672: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.1357 - MinusLogProbMetric: 42.1357 - val_loss: 42.8186 - val_MinusLogProbMetric: 42.8186 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 673/1000
2023-10-28 08:37:43.671 
Epoch 673/1000 
	 loss: 42.2228, MinusLogProbMetric: 42.2228, val_loss: 43.1017, val_MinusLogProbMetric: 43.1017

Epoch 673: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.2228 - MinusLogProbMetric: 42.2228 - val_loss: 43.1017 - val_MinusLogProbMetric: 43.1017 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 674/1000
2023-10-28 08:38:17.222 
Epoch 674/1000 
	 loss: 42.0980, MinusLogProbMetric: 42.0980, val_loss: 44.5984, val_MinusLogProbMetric: 44.5984

Epoch 674: val_loss did not improve from 42.75441
196/196 - 34s - loss: 42.0980 - MinusLogProbMetric: 42.0980 - val_loss: 44.5984 - val_MinusLogProbMetric: 44.5984 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 675/1000
2023-10-28 08:38:50.888 
Epoch 675/1000 
	 loss: 42.1708, MinusLogProbMetric: 42.1708, val_loss: 43.4500, val_MinusLogProbMetric: 43.4500

Epoch 675: val_loss did not improve from 42.75441
196/196 - 34s - loss: 42.1708 - MinusLogProbMetric: 42.1708 - val_loss: 43.4500 - val_MinusLogProbMetric: 43.4500 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 676/1000
2023-10-28 08:39:24.224 
Epoch 676/1000 
	 loss: 42.1342, MinusLogProbMetric: 42.1342, val_loss: 43.5678, val_MinusLogProbMetric: 43.5678

Epoch 676: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.1342 - MinusLogProbMetric: 42.1342 - val_loss: 43.5678 - val_MinusLogProbMetric: 43.5678 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 677/1000
2023-10-28 08:39:57.024 
Epoch 677/1000 
	 loss: 42.0911, MinusLogProbMetric: 42.0911, val_loss: 42.9208, val_MinusLogProbMetric: 42.9208

Epoch 677: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.0911 - MinusLogProbMetric: 42.0911 - val_loss: 42.9208 - val_MinusLogProbMetric: 42.9208 - lr: 1.6667e-04 - 33s/epoch - 167ms/step
Epoch 678/1000
2023-10-28 08:40:30.522 
Epoch 678/1000 
	 loss: 42.1727, MinusLogProbMetric: 42.1727, val_loss: 43.3237, val_MinusLogProbMetric: 43.3237

Epoch 678: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.1727 - MinusLogProbMetric: 42.1727 - val_loss: 43.3237 - val_MinusLogProbMetric: 43.3237 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 679/1000
2023-10-28 08:41:03.978 
Epoch 679/1000 
	 loss: 42.1960, MinusLogProbMetric: 42.1960, val_loss: 43.8845, val_MinusLogProbMetric: 43.8845

Epoch 679: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.1960 - MinusLogProbMetric: 42.1960 - val_loss: 43.8845 - val_MinusLogProbMetric: 43.8845 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 680/1000
2023-10-28 08:41:37.264 
Epoch 680/1000 
	 loss: 42.1849, MinusLogProbMetric: 42.1849, val_loss: 43.4026, val_MinusLogProbMetric: 43.4026

Epoch 680: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.1849 - MinusLogProbMetric: 42.1849 - val_loss: 43.4026 - val_MinusLogProbMetric: 43.4026 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 681/1000
2023-10-28 08:42:10.422 
Epoch 681/1000 
	 loss: 42.0622, MinusLogProbMetric: 42.0622, val_loss: 42.9069, val_MinusLogProbMetric: 42.9069

Epoch 681: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.0622 - MinusLogProbMetric: 42.0622 - val_loss: 42.9069 - val_MinusLogProbMetric: 42.9069 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 682/1000
2023-10-28 08:42:43.649 
Epoch 682/1000 
	 loss: 42.3478, MinusLogProbMetric: 42.3478, val_loss: 43.0105, val_MinusLogProbMetric: 43.0105

Epoch 682: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.3478 - MinusLogProbMetric: 42.3478 - val_loss: 43.0105 - val_MinusLogProbMetric: 43.0105 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 683/1000
2023-10-28 08:43:16.699 
Epoch 683/1000 
	 loss: 42.1646, MinusLogProbMetric: 42.1646, val_loss: 43.1868, val_MinusLogProbMetric: 43.1868

Epoch 683: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.1646 - MinusLogProbMetric: 42.1646 - val_loss: 43.1868 - val_MinusLogProbMetric: 43.1868 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 684/1000
2023-10-28 08:43:50.061 
Epoch 684/1000 
	 loss: 42.2309, MinusLogProbMetric: 42.2309, val_loss: 43.6896, val_MinusLogProbMetric: 43.6896

Epoch 684: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.2309 - MinusLogProbMetric: 42.2309 - val_loss: 43.6896 - val_MinusLogProbMetric: 43.6896 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 685/1000
2023-10-28 08:44:23.504 
Epoch 685/1000 
	 loss: 42.3088, MinusLogProbMetric: 42.3088, val_loss: 43.5675, val_MinusLogProbMetric: 43.5675

Epoch 685: val_loss did not improve from 42.75441
196/196 - 33s - loss: 42.3088 - MinusLogProbMetric: 42.3088 - val_loss: 43.5675 - val_MinusLogProbMetric: 43.5675 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 686/1000
2023-10-28 08:44:57.256 
Epoch 686/1000 
	 loss: 41.9710, MinusLogProbMetric: 41.9710, val_loss: 42.9222, val_MinusLogProbMetric: 42.9222

Epoch 686: val_loss did not improve from 42.75441
196/196 - 34s - loss: 41.9710 - MinusLogProbMetric: 41.9710 - val_loss: 42.9222 - val_MinusLogProbMetric: 42.9222 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 687/1000
2023-10-28 08:45:31.244 
Epoch 687/1000 
	 loss: 42.3244, MinusLogProbMetric: 42.3244, val_loss: 42.6522, val_MinusLogProbMetric: 42.6522

Epoch 687: val_loss improved from 42.75441 to 42.65215, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 35s - loss: 42.3244 - MinusLogProbMetric: 42.3244 - val_loss: 42.6522 - val_MinusLogProbMetric: 42.6522 - lr: 1.6667e-04 - 35s/epoch - 176ms/step
Epoch 688/1000
2023-10-28 08:46:05.619 
Epoch 688/1000 
	 loss: 42.0985, MinusLogProbMetric: 42.0985, val_loss: 42.8354, val_MinusLogProbMetric: 42.8354

Epoch 688: val_loss did not improve from 42.65215
196/196 - 34s - loss: 42.0985 - MinusLogProbMetric: 42.0985 - val_loss: 42.8354 - val_MinusLogProbMetric: 42.8354 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 689/1000
2023-10-28 08:46:39.131 
Epoch 689/1000 
	 loss: 42.4164, MinusLogProbMetric: 42.4164, val_loss: 43.2200, val_MinusLogProbMetric: 43.2200

Epoch 689: val_loss did not improve from 42.65215
196/196 - 34s - loss: 42.4164 - MinusLogProbMetric: 42.4164 - val_loss: 43.2200 - val_MinusLogProbMetric: 43.2200 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 690/1000
2023-10-28 08:47:12.619 
Epoch 690/1000 
	 loss: 42.3642, MinusLogProbMetric: 42.3642, val_loss: 43.2406, val_MinusLogProbMetric: 43.2406

Epoch 690: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.3642 - MinusLogProbMetric: 42.3642 - val_loss: 43.2406 - val_MinusLogProbMetric: 43.2406 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 691/1000
2023-10-28 08:47:45.683 
Epoch 691/1000 
	 loss: 42.0684, MinusLogProbMetric: 42.0684, val_loss: 42.7094, val_MinusLogProbMetric: 42.7094

Epoch 691: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.0684 - MinusLogProbMetric: 42.0684 - val_loss: 42.7094 - val_MinusLogProbMetric: 42.7094 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 692/1000
2023-10-28 08:48:17.977 
Epoch 692/1000 
	 loss: 42.4043, MinusLogProbMetric: 42.4043, val_loss: 42.8982, val_MinusLogProbMetric: 42.8982

Epoch 692: val_loss did not improve from 42.65215
196/196 - 32s - loss: 42.4043 - MinusLogProbMetric: 42.4043 - val_loss: 42.8982 - val_MinusLogProbMetric: 42.8982 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 693/1000
2023-10-28 08:48:51.416 
Epoch 693/1000 
	 loss: 42.0456, MinusLogProbMetric: 42.0456, val_loss: 43.7842, val_MinusLogProbMetric: 43.7842

Epoch 693: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.0456 - MinusLogProbMetric: 42.0456 - val_loss: 43.7842 - val_MinusLogProbMetric: 43.7842 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 694/1000
2023-10-28 08:49:25.068 
Epoch 694/1000 
	 loss: 42.3129, MinusLogProbMetric: 42.3129, val_loss: 43.6919, val_MinusLogProbMetric: 43.6919

Epoch 694: val_loss did not improve from 42.65215
196/196 - 34s - loss: 42.3129 - MinusLogProbMetric: 42.3129 - val_loss: 43.6919 - val_MinusLogProbMetric: 43.6919 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 695/1000
2023-10-28 08:49:58.521 
Epoch 695/1000 
	 loss: 42.0611, MinusLogProbMetric: 42.0611, val_loss: 44.1221, val_MinusLogProbMetric: 44.1221

Epoch 695: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.0611 - MinusLogProbMetric: 42.0611 - val_loss: 44.1221 - val_MinusLogProbMetric: 44.1221 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 696/1000
2023-10-28 08:50:31.633 
Epoch 696/1000 
	 loss: 42.3158, MinusLogProbMetric: 42.3158, val_loss: 43.0428, val_MinusLogProbMetric: 43.0428

Epoch 696: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.3158 - MinusLogProbMetric: 42.3158 - val_loss: 43.0428 - val_MinusLogProbMetric: 43.0428 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 697/1000
2023-10-28 08:51:05.149 
Epoch 697/1000 
	 loss: 42.1569, MinusLogProbMetric: 42.1569, val_loss: 43.3884, val_MinusLogProbMetric: 43.3884

Epoch 697: val_loss did not improve from 42.65215
196/196 - 34s - loss: 42.1569 - MinusLogProbMetric: 42.1569 - val_loss: 43.3884 - val_MinusLogProbMetric: 43.3884 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 698/1000
2023-10-28 08:51:38.477 
Epoch 698/1000 
	 loss: 42.1830, MinusLogProbMetric: 42.1830, val_loss: 43.8523, val_MinusLogProbMetric: 43.8523

Epoch 698: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.1830 - MinusLogProbMetric: 42.1830 - val_loss: 43.8523 - val_MinusLogProbMetric: 43.8523 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 699/1000
2023-10-28 08:52:11.993 
Epoch 699/1000 
	 loss: 42.3246, MinusLogProbMetric: 42.3246, val_loss: 42.8136, val_MinusLogProbMetric: 42.8136

Epoch 699: val_loss did not improve from 42.65215
196/196 - 34s - loss: 42.3246 - MinusLogProbMetric: 42.3246 - val_loss: 42.8136 - val_MinusLogProbMetric: 42.8136 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 700/1000
2023-10-28 08:52:45.576 
Epoch 700/1000 
	 loss: 42.1207, MinusLogProbMetric: 42.1207, val_loss: 42.8958, val_MinusLogProbMetric: 42.8958

Epoch 700: val_loss did not improve from 42.65215
196/196 - 34s - loss: 42.1207 - MinusLogProbMetric: 42.1207 - val_loss: 42.8958 - val_MinusLogProbMetric: 42.8958 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 701/1000
2023-10-28 08:53:19.022 
Epoch 701/1000 
	 loss: 42.2931, MinusLogProbMetric: 42.2931, val_loss: 42.9683, val_MinusLogProbMetric: 42.9683

Epoch 701: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.2931 - MinusLogProbMetric: 42.2931 - val_loss: 42.9683 - val_MinusLogProbMetric: 42.9683 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 702/1000
2023-10-28 08:53:53.197 
Epoch 702/1000 
	 loss: 42.3894, MinusLogProbMetric: 42.3894, val_loss: 43.2338, val_MinusLogProbMetric: 43.2338

Epoch 702: val_loss did not improve from 42.65215
196/196 - 34s - loss: 42.3894 - MinusLogProbMetric: 42.3894 - val_loss: 43.2338 - val_MinusLogProbMetric: 43.2338 - lr: 1.6667e-04 - 34s/epoch - 174ms/step
Epoch 703/1000
2023-10-28 08:54:26.595 
Epoch 703/1000 
	 loss: 42.1071, MinusLogProbMetric: 42.1071, val_loss: 43.0801, val_MinusLogProbMetric: 43.0801

Epoch 703: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.1071 - MinusLogProbMetric: 42.1071 - val_loss: 43.0801 - val_MinusLogProbMetric: 43.0801 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 704/1000
2023-10-28 08:54:59.822 
Epoch 704/1000 
	 loss: 42.1460, MinusLogProbMetric: 42.1460, val_loss: 43.1027, val_MinusLogProbMetric: 43.1027

Epoch 704: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.1460 - MinusLogProbMetric: 42.1460 - val_loss: 43.1027 - val_MinusLogProbMetric: 43.1027 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 705/1000
2023-10-28 08:55:33.145 
Epoch 705/1000 
	 loss: 42.0229, MinusLogProbMetric: 42.0229, val_loss: 43.1402, val_MinusLogProbMetric: 43.1402

Epoch 705: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.0229 - MinusLogProbMetric: 42.0229 - val_loss: 43.1402 - val_MinusLogProbMetric: 43.1402 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 706/1000
2023-10-28 08:56:06.092 
Epoch 706/1000 
	 loss: 42.2638, MinusLogProbMetric: 42.2638, val_loss: 44.0104, val_MinusLogProbMetric: 44.0104

Epoch 706: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.2638 - MinusLogProbMetric: 42.2638 - val_loss: 44.0104 - val_MinusLogProbMetric: 44.0104 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 707/1000
2023-10-28 08:56:39.296 
Epoch 707/1000 
	 loss: 42.2778, MinusLogProbMetric: 42.2778, val_loss: 43.0982, val_MinusLogProbMetric: 43.0982

Epoch 707: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.2778 - MinusLogProbMetric: 42.2778 - val_loss: 43.0982 - val_MinusLogProbMetric: 43.0982 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 708/1000
2023-10-28 08:57:12.362 
Epoch 708/1000 
	 loss: 42.2194, MinusLogProbMetric: 42.2194, val_loss: 43.2266, val_MinusLogProbMetric: 43.2266

Epoch 708: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.2194 - MinusLogProbMetric: 42.2194 - val_loss: 43.2266 - val_MinusLogProbMetric: 43.2266 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 709/1000
2023-10-28 08:57:45.716 
Epoch 709/1000 
	 loss: 42.2380, MinusLogProbMetric: 42.2380, val_loss: 43.3232, val_MinusLogProbMetric: 43.3232

Epoch 709: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.2380 - MinusLogProbMetric: 42.2380 - val_loss: 43.3232 - val_MinusLogProbMetric: 43.3232 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 710/1000
2023-10-28 08:58:18.583 
Epoch 710/1000 
	 loss: 42.0032, MinusLogProbMetric: 42.0032, val_loss: 43.4742, val_MinusLogProbMetric: 43.4742

Epoch 710: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.0032 - MinusLogProbMetric: 42.0032 - val_loss: 43.4742 - val_MinusLogProbMetric: 43.4742 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 711/1000
2023-10-28 08:58:52.101 
Epoch 711/1000 
	 loss: 42.3228, MinusLogProbMetric: 42.3228, val_loss: 42.7664, val_MinusLogProbMetric: 42.7664

Epoch 711: val_loss did not improve from 42.65215
196/196 - 34s - loss: 42.3228 - MinusLogProbMetric: 42.3228 - val_loss: 42.7664 - val_MinusLogProbMetric: 42.7664 - lr: 1.6667e-04 - 34s/epoch - 171ms/step
Epoch 712/1000
2023-10-28 08:59:25.421 
Epoch 712/1000 
	 loss: 42.0613, MinusLogProbMetric: 42.0613, val_loss: 42.8070, val_MinusLogProbMetric: 42.8070

Epoch 712: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.0613 - MinusLogProbMetric: 42.0613 - val_loss: 42.8070 - val_MinusLogProbMetric: 42.8070 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 713/1000
2023-10-28 08:59:59.095 
Epoch 713/1000 
	 loss: 42.2550, MinusLogProbMetric: 42.2550, val_loss: 43.0752, val_MinusLogProbMetric: 43.0752

Epoch 713: val_loss did not improve from 42.65215
196/196 - 34s - loss: 42.2550 - MinusLogProbMetric: 42.2550 - val_loss: 43.0752 - val_MinusLogProbMetric: 43.0752 - lr: 1.6667e-04 - 34s/epoch - 172ms/step
Epoch 714/1000
2023-10-28 09:00:32.273 
Epoch 714/1000 
	 loss: 42.1830, MinusLogProbMetric: 42.1830, val_loss: 42.6768, val_MinusLogProbMetric: 42.6768

Epoch 714: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.1830 - MinusLogProbMetric: 42.1830 - val_loss: 42.6768 - val_MinusLogProbMetric: 42.6768 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 715/1000
2023-10-28 09:01:05.636 
Epoch 715/1000 
	 loss: 42.2393, MinusLogProbMetric: 42.2393, val_loss: 43.1052, val_MinusLogProbMetric: 43.1052

Epoch 715: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.2393 - MinusLogProbMetric: 42.2393 - val_loss: 43.1052 - val_MinusLogProbMetric: 43.1052 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 716/1000
2023-10-28 09:01:38.662 
Epoch 716/1000 
	 loss: 42.2582, MinusLogProbMetric: 42.2582, val_loss: 42.9607, val_MinusLogProbMetric: 42.9607

Epoch 716: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.2582 - MinusLogProbMetric: 42.2582 - val_loss: 42.9607 - val_MinusLogProbMetric: 42.9607 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 717/1000
2023-10-28 09:02:11.815 
Epoch 717/1000 
	 loss: 42.1045, MinusLogProbMetric: 42.1045, val_loss: 42.9935, val_MinusLogProbMetric: 42.9935

Epoch 717: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.1045 - MinusLogProbMetric: 42.1045 - val_loss: 42.9935 - val_MinusLogProbMetric: 42.9935 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 718/1000
2023-10-28 09:02:44.987 
Epoch 718/1000 
	 loss: 42.1597, MinusLogProbMetric: 42.1597, val_loss: 43.9920, val_MinusLogProbMetric: 43.9920

Epoch 718: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.1597 - MinusLogProbMetric: 42.1597 - val_loss: 43.9920 - val_MinusLogProbMetric: 43.9920 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 719/1000
2023-10-28 09:03:17.985 
Epoch 719/1000 
	 loss: 42.2581, MinusLogProbMetric: 42.2581, val_loss: 43.0865, val_MinusLogProbMetric: 43.0865

Epoch 719: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.2581 - MinusLogProbMetric: 42.2581 - val_loss: 43.0865 - val_MinusLogProbMetric: 43.0865 - lr: 1.6667e-04 - 33s/epoch - 168ms/step
Epoch 720/1000
2023-10-28 09:03:51.184 
Epoch 720/1000 
	 loss: 41.9701, MinusLogProbMetric: 41.9701, val_loss: 42.7960, val_MinusLogProbMetric: 42.7960

Epoch 720: val_loss did not improve from 42.65215
196/196 - 33s - loss: 41.9701 - MinusLogProbMetric: 41.9701 - val_loss: 42.7960 - val_MinusLogProbMetric: 42.7960 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 721/1000
2023-10-28 09:04:24.424 
Epoch 721/1000 
	 loss: 42.3543, MinusLogProbMetric: 42.3543, val_loss: 43.2469, val_MinusLogProbMetric: 43.2469

Epoch 721: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.3543 - MinusLogProbMetric: 42.3543 - val_loss: 43.2469 - val_MinusLogProbMetric: 43.2469 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 722/1000
2023-10-28 09:04:57.670 
Epoch 722/1000 
	 loss: 42.0958, MinusLogProbMetric: 42.0958, val_loss: 43.3803, val_MinusLogProbMetric: 43.3803

Epoch 722: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.0958 - MinusLogProbMetric: 42.0958 - val_loss: 43.3803 - val_MinusLogProbMetric: 43.3803 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 723/1000
2023-10-28 09:05:31.160 
Epoch 723/1000 
	 loss: 42.1897, MinusLogProbMetric: 42.1897, val_loss: 42.9947, val_MinusLogProbMetric: 42.9947

Epoch 723: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.1897 - MinusLogProbMetric: 42.1897 - val_loss: 42.9947 - val_MinusLogProbMetric: 42.9947 - lr: 1.6667e-04 - 33s/epoch - 171ms/step
Epoch 724/1000
2023-10-28 09:06:04.486 
Epoch 724/1000 
	 loss: 42.1264, MinusLogProbMetric: 42.1264, val_loss: 43.2848, val_MinusLogProbMetric: 43.2848

Epoch 724: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.1264 - MinusLogProbMetric: 42.1264 - val_loss: 43.2848 - val_MinusLogProbMetric: 43.2848 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 725/1000
2023-10-28 09:06:36.840 
Epoch 725/1000 
	 loss: 42.1931, MinusLogProbMetric: 42.1931, val_loss: 43.2428, val_MinusLogProbMetric: 43.2428

Epoch 725: val_loss did not improve from 42.65215
196/196 - 32s - loss: 42.1931 - MinusLogProbMetric: 42.1931 - val_loss: 43.2428 - val_MinusLogProbMetric: 43.2428 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 726/1000
2023-10-28 09:07:10.084 
Epoch 726/1000 
	 loss: 42.1205, MinusLogProbMetric: 42.1205, val_loss: 42.7614, val_MinusLogProbMetric: 42.7614

Epoch 726: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.1205 - MinusLogProbMetric: 42.1205 - val_loss: 42.7614 - val_MinusLogProbMetric: 42.7614 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 727/1000
2023-10-28 09:07:43.343 
Epoch 727/1000 
	 loss: 42.2384, MinusLogProbMetric: 42.2384, val_loss: 43.2078, val_MinusLogProbMetric: 43.2078

Epoch 727: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.2384 - MinusLogProbMetric: 42.2384 - val_loss: 43.2078 - val_MinusLogProbMetric: 43.2078 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 728/1000
2023-10-28 09:08:16.618 
Epoch 728/1000 
	 loss: 42.0309, MinusLogProbMetric: 42.0309, val_loss: 43.5655, val_MinusLogProbMetric: 43.5655

Epoch 728: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.0309 - MinusLogProbMetric: 42.0309 - val_loss: 43.5655 - val_MinusLogProbMetric: 43.5655 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 729/1000
2023-10-28 09:08:49.902 
Epoch 729/1000 
	 loss: 42.1118, MinusLogProbMetric: 42.1118, val_loss: 43.4952, val_MinusLogProbMetric: 43.4952

Epoch 729: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.1118 - MinusLogProbMetric: 42.1118 - val_loss: 43.4952 - val_MinusLogProbMetric: 43.4952 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 730/1000
2023-10-28 09:09:23.126 
Epoch 730/1000 
	 loss: 42.4181, MinusLogProbMetric: 42.4181, val_loss: 43.0956, val_MinusLogProbMetric: 43.0956

Epoch 730: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.4181 - MinusLogProbMetric: 42.4181 - val_loss: 43.0956 - val_MinusLogProbMetric: 43.0956 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 731/1000
2023-10-28 09:09:56.450 
Epoch 731/1000 
	 loss: 42.0627, MinusLogProbMetric: 42.0627, val_loss: 43.3168, val_MinusLogProbMetric: 43.3168

Epoch 731: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.0627 - MinusLogProbMetric: 42.0627 - val_loss: 43.3168 - val_MinusLogProbMetric: 43.3168 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 732/1000
2023-10-28 09:10:29.714 
Epoch 732/1000 
	 loss: 42.2497, MinusLogProbMetric: 42.2497, val_loss: 42.8877, val_MinusLogProbMetric: 42.8877

Epoch 732: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.2497 - MinusLogProbMetric: 42.2497 - val_loss: 42.8877 - val_MinusLogProbMetric: 42.8877 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 733/1000
2023-10-28 09:11:03.098 
Epoch 733/1000 
	 loss: 42.1458, MinusLogProbMetric: 42.1458, val_loss: 43.3032, val_MinusLogProbMetric: 43.3032

Epoch 733: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.1458 - MinusLogProbMetric: 42.1458 - val_loss: 43.3032 - val_MinusLogProbMetric: 43.3032 - lr: 1.6667e-04 - 33s/epoch - 170ms/step
Epoch 734/1000
2023-10-28 09:11:36.215 
Epoch 734/1000 
	 loss: 42.1076, MinusLogProbMetric: 42.1076, val_loss: 43.0132, val_MinusLogProbMetric: 43.0132

Epoch 734: val_loss did not improve from 42.65215
196/196 - 33s - loss: 42.1076 - MinusLogProbMetric: 42.1076 - val_loss: 43.0132 - val_MinusLogProbMetric: 43.0132 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 735/1000
2023-10-28 09:12:07.056 
Epoch 735/1000 
	 loss: 42.0597, MinusLogProbMetric: 42.0597, val_loss: 43.2484, val_MinusLogProbMetric: 43.2484

Epoch 735: val_loss did not improve from 42.65215
196/196 - 31s - loss: 42.0597 - MinusLogProbMetric: 42.0597 - val_loss: 43.2484 - val_MinusLogProbMetric: 43.2484 - lr: 1.6667e-04 - 31s/epoch - 157ms/step
Epoch 736/1000
2023-10-28 09:12:39.393 
Epoch 736/1000 
	 loss: 42.5355, MinusLogProbMetric: 42.5355, val_loss: 43.1762, val_MinusLogProbMetric: 43.1762

Epoch 736: val_loss did not improve from 42.65215
196/196 - 32s - loss: 42.5355 - MinusLogProbMetric: 42.5355 - val_loss: 43.1762 - val_MinusLogProbMetric: 43.1762 - lr: 1.6667e-04 - 32s/epoch - 165ms/step
Epoch 737/1000
2023-10-28 09:13:12.498 
Epoch 737/1000 
	 loss: 41.9877, MinusLogProbMetric: 41.9877, val_loss: 42.9440, val_MinusLogProbMetric: 42.9440

Epoch 737: val_loss did not improve from 42.65215
196/196 - 33s - loss: 41.9877 - MinusLogProbMetric: 41.9877 - val_loss: 42.9440 - val_MinusLogProbMetric: 42.9440 - lr: 1.6667e-04 - 33s/epoch - 169ms/step
Epoch 738/1000
2023-10-28 09:13:45.192 
Epoch 738/1000 
	 loss: 41.4791, MinusLogProbMetric: 41.4791, val_loss: 42.4284, val_MinusLogProbMetric: 42.4284

Epoch 738: val_loss improved from 42.65215 to 42.42836, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 41.4791 - MinusLogProbMetric: 41.4791 - val_loss: 42.4284 - val_MinusLogProbMetric: 42.4284 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 739/1000
2023-10-28 09:14:18.398 
Epoch 739/1000 
	 loss: 41.4649, MinusLogProbMetric: 41.4649, val_loss: 43.2453, val_MinusLogProbMetric: 43.2453

Epoch 739: val_loss did not improve from 42.42836
196/196 - 33s - loss: 41.4649 - MinusLogProbMetric: 41.4649 - val_loss: 43.2453 - val_MinusLogProbMetric: 43.2453 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 740/1000
2023-10-28 09:14:50.506 
Epoch 740/1000 
	 loss: 41.5103, MinusLogProbMetric: 41.5103, val_loss: 42.5372, val_MinusLogProbMetric: 42.5372

Epoch 740: val_loss did not improve from 42.42836
196/196 - 32s - loss: 41.5103 - MinusLogProbMetric: 41.5103 - val_loss: 42.5372 - val_MinusLogProbMetric: 42.5372 - lr: 8.3333e-05 - 32s/epoch - 164ms/step
Epoch 741/1000
2023-10-28 09:15:21.698 
Epoch 741/1000 
	 loss: 41.5173, MinusLogProbMetric: 41.5173, val_loss: 42.3438, val_MinusLogProbMetric: 42.3438

Epoch 741: val_loss improved from 42.42836 to 42.34384, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 32s - loss: 41.5173 - MinusLogProbMetric: 41.5173 - val_loss: 42.3438 - val_MinusLogProbMetric: 42.3438 - lr: 8.3333e-05 - 32s/epoch - 162ms/step
Epoch 742/1000
2023-10-28 09:15:52.799 
Epoch 742/1000 
	 loss: 41.5274, MinusLogProbMetric: 41.5274, val_loss: 42.5843, val_MinusLogProbMetric: 42.5843

Epoch 742: val_loss did not improve from 42.34384
196/196 - 31s - loss: 41.5274 - MinusLogProbMetric: 41.5274 - val_loss: 42.5843 - val_MinusLogProbMetric: 42.5843 - lr: 8.3333e-05 - 31s/epoch - 156ms/step
Epoch 743/1000
2023-10-28 09:16:24.440 
Epoch 743/1000 
	 loss: 41.5083, MinusLogProbMetric: 41.5083, val_loss: 42.7950, val_MinusLogProbMetric: 42.7950

Epoch 743: val_loss did not improve from 42.34384
196/196 - 32s - loss: 41.5083 - MinusLogProbMetric: 41.5083 - val_loss: 42.7950 - val_MinusLogProbMetric: 42.7950 - lr: 8.3333e-05 - 32s/epoch - 161ms/step
Epoch 744/1000
2023-10-28 09:16:57.774 
Epoch 744/1000 
	 loss: 41.5403, MinusLogProbMetric: 41.5403, val_loss: 42.5152, val_MinusLogProbMetric: 42.5152

Epoch 744: val_loss did not improve from 42.34384
196/196 - 33s - loss: 41.5403 - MinusLogProbMetric: 41.5403 - val_loss: 42.5152 - val_MinusLogProbMetric: 42.5152 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 745/1000
2023-10-28 09:17:30.899 
Epoch 745/1000 
	 loss: 41.4495, MinusLogProbMetric: 41.4495, val_loss: 42.5521, val_MinusLogProbMetric: 42.5521

Epoch 745: val_loss did not improve from 42.34384
196/196 - 33s - loss: 41.4495 - MinusLogProbMetric: 41.4495 - val_loss: 42.5521 - val_MinusLogProbMetric: 42.5521 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 746/1000
2023-10-28 09:18:04.161 
Epoch 746/1000 
	 loss: 41.5929, MinusLogProbMetric: 41.5929, val_loss: 42.5137, val_MinusLogProbMetric: 42.5137

Epoch 746: val_loss did not improve from 42.34384
196/196 - 33s - loss: 41.5929 - MinusLogProbMetric: 41.5929 - val_loss: 42.5137 - val_MinusLogProbMetric: 42.5137 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 747/1000
2023-10-28 09:18:37.619 
Epoch 747/1000 
	 loss: 41.4929, MinusLogProbMetric: 41.4929, val_loss: 42.4172, val_MinusLogProbMetric: 42.4172

Epoch 747: val_loss did not improve from 42.34384
196/196 - 33s - loss: 41.4929 - MinusLogProbMetric: 41.4929 - val_loss: 42.4172 - val_MinusLogProbMetric: 42.4172 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 748/1000
2023-10-28 09:19:10.662 
Epoch 748/1000 
	 loss: 41.6197, MinusLogProbMetric: 41.6197, val_loss: 42.3818, val_MinusLogProbMetric: 42.3818

Epoch 748: val_loss did not improve from 42.34384
196/196 - 33s - loss: 41.6197 - MinusLogProbMetric: 41.6197 - val_loss: 42.3818 - val_MinusLogProbMetric: 42.3818 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 749/1000
2023-10-28 09:19:40.904 
Epoch 749/1000 
	 loss: 41.6071, MinusLogProbMetric: 41.6071, val_loss: 42.6467, val_MinusLogProbMetric: 42.6467

Epoch 749: val_loss did not improve from 42.34384
196/196 - 30s - loss: 41.6071 - MinusLogProbMetric: 41.6071 - val_loss: 42.6467 - val_MinusLogProbMetric: 42.6467 - lr: 8.3333e-05 - 30s/epoch - 154ms/step
Epoch 750/1000
2023-10-28 09:20:10.568 
Epoch 750/1000 
	 loss: 41.5115, MinusLogProbMetric: 41.5115, val_loss: 43.1109, val_MinusLogProbMetric: 43.1109

Epoch 750: val_loss did not improve from 42.34384
196/196 - 30s - loss: 41.5115 - MinusLogProbMetric: 41.5115 - val_loss: 43.1109 - val_MinusLogProbMetric: 43.1109 - lr: 8.3333e-05 - 30s/epoch - 151ms/step
Epoch 751/1000
2023-10-28 09:20:40.945 
Epoch 751/1000 
	 loss: 41.5589, MinusLogProbMetric: 41.5589, val_loss: 42.7294, val_MinusLogProbMetric: 42.7294

Epoch 751: val_loss did not improve from 42.34384
196/196 - 30s - loss: 41.5589 - MinusLogProbMetric: 41.5589 - val_loss: 42.7294 - val_MinusLogProbMetric: 42.7294 - lr: 8.3333e-05 - 30s/epoch - 155ms/step
Epoch 752/1000
2023-10-28 09:21:09.708 
Epoch 752/1000 
	 loss: 41.5631, MinusLogProbMetric: 41.5631, val_loss: 42.6286, val_MinusLogProbMetric: 42.6286

Epoch 752: val_loss did not improve from 42.34384
196/196 - 29s - loss: 41.5631 - MinusLogProbMetric: 41.5631 - val_loss: 42.6286 - val_MinusLogProbMetric: 42.6286 - lr: 8.3333e-05 - 29s/epoch - 147ms/step
Epoch 753/1000
2023-10-28 09:21:40.982 
Epoch 753/1000 
	 loss: 41.4906, MinusLogProbMetric: 41.4906, val_loss: 42.4724, val_MinusLogProbMetric: 42.4724

Epoch 753: val_loss did not improve from 42.34384
196/196 - 31s - loss: 41.4906 - MinusLogProbMetric: 41.4906 - val_loss: 42.4724 - val_MinusLogProbMetric: 42.4724 - lr: 8.3333e-05 - 31s/epoch - 160ms/step
Epoch 754/1000
2023-10-28 09:22:10.123 
Epoch 754/1000 
	 loss: 41.4621, MinusLogProbMetric: 41.4621, val_loss: 42.4827, val_MinusLogProbMetric: 42.4827

Epoch 754: val_loss did not improve from 42.34384
196/196 - 29s - loss: 41.4621 - MinusLogProbMetric: 41.4621 - val_loss: 42.4827 - val_MinusLogProbMetric: 42.4827 - lr: 8.3333e-05 - 29s/epoch - 149ms/step
Epoch 755/1000
2023-10-28 09:22:39.272 
Epoch 755/1000 
	 loss: 41.5711, MinusLogProbMetric: 41.5711, val_loss: 42.3732, val_MinusLogProbMetric: 42.3732

Epoch 755: val_loss did not improve from 42.34384
196/196 - 29s - loss: 41.5711 - MinusLogProbMetric: 41.5711 - val_loss: 42.3732 - val_MinusLogProbMetric: 42.3732 - lr: 8.3333e-05 - 29s/epoch - 149ms/step
Epoch 756/1000
2023-10-28 09:23:08.935 
Epoch 756/1000 
	 loss: 41.5556, MinusLogProbMetric: 41.5556, val_loss: 42.4802, val_MinusLogProbMetric: 42.4802

Epoch 756: val_loss did not improve from 42.34384
196/196 - 30s - loss: 41.5556 - MinusLogProbMetric: 41.5556 - val_loss: 42.4802 - val_MinusLogProbMetric: 42.4802 - lr: 8.3333e-05 - 30s/epoch - 151ms/step
Epoch 757/1000
2023-10-28 09:23:38.222 
Epoch 757/1000 
	 loss: 41.4868, MinusLogProbMetric: 41.4868, val_loss: 42.6062, val_MinusLogProbMetric: 42.6062

Epoch 757: val_loss did not improve from 42.34384
196/196 - 29s - loss: 41.4868 - MinusLogProbMetric: 41.4868 - val_loss: 42.6062 - val_MinusLogProbMetric: 42.6062 - lr: 8.3333e-05 - 29s/epoch - 149ms/step
Epoch 758/1000
2023-10-28 09:24:08.919 
Epoch 758/1000 
	 loss: 41.5751, MinusLogProbMetric: 41.5751, val_loss: 44.2330, val_MinusLogProbMetric: 44.2330

Epoch 758: val_loss did not improve from 42.34384
196/196 - 31s - loss: 41.5751 - MinusLogProbMetric: 41.5751 - val_loss: 44.2330 - val_MinusLogProbMetric: 44.2330 - lr: 8.3333e-05 - 31s/epoch - 157ms/step
Epoch 759/1000
2023-10-28 09:24:37.575 
Epoch 759/1000 
	 loss: 41.5490, MinusLogProbMetric: 41.5490, val_loss: 42.4015, val_MinusLogProbMetric: 42.4015

Epoch 759: val_loss did not improve from 42.34384
196/196 - 29s - loss: 41.5490 - MinusLogProbMetric: 41.5490 - val_loss: 42.4015 - val_MinusLogProbMetric: 42.4015 - lr: 8.3333e-05 - 29s/epoch - 146ms/step
Epoch 760/1000
2023-10-28 09:25:08.384 
Epoch 760/1000 
	 loss: 41.4453, MinusLogProbMetric: 41.4453, val_loss: 42.8360, val_MinusLogProbMetric: 42.8360

Epoch 760: val_loss did not improve from 42.34384
196/196 - 31s - loss: 41.4453 - MinusLogProbMetric: 41.4453 - val_loss: 42.8360 - val_MinusLogProbMetric: 42.8360 - lr: 8.3333e-05 - 31s/epoch - 157ms/step
Epoch 761/1000
2023-10-28 09:25:37.886 
Epoch 761/1000 
	 loss: 41.4782, MinusLogProbMetric: 41.4782, val_loss: 42.5249, val_MinusLogProbMetric: 42.5249

Epoch 761: val_loss did not improve from 42.34384
196/196 - 29s - loss: 41.4782 - MinusLogProbMetric: 41.4782 - val_loss: 42.5249 - val_MinusLogProbMetric: 42.5249 - lr: 8.3333e-05 - 29s/epoch - 151ms/step
Epoch 762/1000
2023-10-28 09:26:06.647 
Epoch 762/1000 
	 loss: 41.5837, MinusLogProbMetric: 41.5837, val_loss: 43.0184, val_MinusLogProbMetric: 43.0184

Epoch 762: val_loss did not improve from 42.34384
196/196 - 29s - loss: 41.5837 - MinusLogProbMetric: 41.5837 - val_loss: 43.0184 - val_MinusLogProbMetric: 43.0184 - lr: 8.3333e-05 - 29s/epoch - 147ms/step
Epoch 763/1000
2023-10-28 09:26:37.095 
Epoch 763/1000 
	 loss: 41.4763, MinusLogProbMetric: 41.4763, val_loss: 42.8532, val_MinusLogProbMetric: 42.8532

Epoch 763: val_loss did not improve from 42.34384
196/196 - 30s - loss: 41.4763 - MinusLogProbMetric: 41.4763 - val_loss: 42.8532 - val_MinusLogProbMetric: 42.8532 - lr: 8.3333e-05 - 30s/epoch - 155ms/step
Epoch 764/1000
2023-10-28 09:27:06.104 
Epoch 764/1000 
	 loss: 41.6485, MinusLogProbMetric: 41.6485, val_loss: 42.4864, val_MinusLogProbMetric: 42.4864

Epoch 764: val_loss did not improve from 42.34384
196/196 - 29s - loss: 41.6485 - MinusLogProbMetric: 41.6485 - val_loss: 42.4864 - val_MinusLogProbMetric: 42.4864 - lr: 8.3333e-05 - 29s/epoch - 148ms/step
Epoch 765/1000
2023-10-28 09:27:37.583 
Epoch 765/1000 
	 loss: 41.4272, MinusLogProbMetric: 41.4272, val_loss: 42.4510, val_MinusLogProbMetric: 42.4510

Epoch 765: val_loss did not improve from 42.34384
196/196 - 31s - loss: 41.4272 - MinusLogProbMetric: 41.4272 - val_loss: 42.4510 - val_MinusLogProbMetric: 42.4510 - lr: 8.3333e-05 - 31s/epoch - 161ms/step
Epoch 766/1000
2023-10-28 09:28:05.981 
Epoch 766/1000 
	 loss: 41.4775, MinusLogProbMetric: 41.4775, val_loss: 42.4446, val_MinusLogProbMetric: 42.4446

Epoch 766: val_loss did not improve from 42.34384
196/196 - 28s - loss: 41.4775 - MinusLogProbMetric: 41.4775 - val_loss: 42.4446 - val_MinusLogProbMetric: 42.4446 - lr: 8.3333e-05 - 28s/epoch - 145ms/step
Epoch 767/1000
2023-10-28 09:28:35.667 
Epoch 767/1000 
	 loss: 41.8205, MinusLogProbMetric: 41.8205, val_loss: 42.4677, val_MinusLogProbMetric: 42.4677

Epoch 767: val_loss did not improve from 42.34384
196/196 - 30s - loss: 41.8205 - MinusLogProbMetric: 41.8205 - val_loss: 42.4677 - val_MinusLogProbMetric: 42.4677 - lr: 8.3333e-05 - 30s/epoch - 151ms/step
Epoch 768/1000
2023-10-28 09:29:06.309 
Epoch 768/1000 
	 loss: 41.4900, MinusLogProbMetric: 41.4900, val_loss: 42.7758, val_MinusLogProbMetric: 42.7758

Epoch 768: val_loss did not improve from 42.34384
196/196 - 31s - loss: 41.4900 - MinusLogProbMetric: 41.4900 - val_loss: 42.7758 - val_MinusLogProbMetric: 42.7758 - lr: 8.3333e-05 - 31s/epoch - 156ms/step
Epoch 769/1000
2023-10-28 09:29:34.938 
Epoch 769/1000 
	 loss: 41.5429, MinusLogProbMetric: 41.5429, val_loss: 42.7686, val_MinusLogProbMetric: 42.7686

Epoch 769: val_loss did not improve from 42.34384
196/196 - 29s - loss: 41.5429 - MinusLogProbMetric: 41.5429 - val_loss: 42.7686 - val_MinusLogProbMetric: 42.7686 - lr: 8.3333e-05 - 29s/epoch - 146ms/step
Epoch 770/1000
2023-10-28 09:30:07.239 
Epoch 770/1000 
	 loss: 41.4568, MinusLogProbMetric: 41.4568, val_loss: 42.9107, val_MinusLogProbMetric: 42.9107

Epoch 770: val_loss did not improve from 42.34384
196/196 - 32s - loss: 41.4568 - MinusLogProbMetric: 41.4568 - val_loss: 42.9107 - val_MinusLogProbMetric: 42.9107 - lr: 8.3333e-05 - 32s/epoch - 165ms/step
Epoch 771/1000
2023-10-28 09:30:35.792 
Epoch 771/1000 
	 loss: 41.5917, MinusLogProbMetric: 41.5917, val_loss: 42.6466, val_MinusLogProbMetric: 42.6466

Epoch 771: val_loss did not improve from 42.34384
196/196 - 29s - loss: 41.5917 - MinusLogProbMetric: 41.5917 - val_loss: 42.6466 - val_MinusLogProbMetric: 42.6466 - lr: 8.3333e-05 - 29s/epoch - 146ms/step
Epoch 772/1000
2023-10-28 09:31:06.592 
Epoch 772/1000 
	 loss: 41.4405, MinusLogProbMetric: 41.4405, val_loss: 42.3204, val_MinusLogProbMetric: 42.3204

Epoch 772: val_loss improved from 42.34384 to 42.32038, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 31s - loss: 41.4405 - MinusLogProbMetric: 41.4405 - val_loss: 42.3204 - val_MinusLogProbMetric: 42.3204 - lr: 8.3333e-05 - 31s/epoch - 160ms/step
Epoch 773/1000
2023-10-28 09:31:35.287 
Epoch 773/1000 
	 loss: 41.5602, MinusLogProbMetric: 41.5602, val_loss: 42.7620, val_MinusLogProbMetric: 42.7620

Epoch 773: val_loss did not improve from 42.32038
196/196 - 28s - loss: 41.5602 - MinusLogProbMetric: 41.5602 - val_loss: 42.7620 - val_MinusLogProbMetric: 42.7620 - lr: 8.3333e-05 - 28s/epoch - 144ms/step
Epoch 774/1000
2023-10-28 09:32:04.424 
Epoch 774/1000 
	 loss: 41.4340, MinusLogProbMetric: 41.4340, val_loss: 43.0349, val_MinusLogProbMetric: 43.0349

Epoch 774: val_loss did not improve from 42.32038
196/196 - 29s - loss: 41.4340 - MinusLogProbMetric: 41.4340 - val_loss: 43.0349 - val_MinusLogProbMetric: 43.0349 - lr: 8.3333e-05 - 29s/epoch - 149ms/step
Epoch 775/1000
2023-10-28 09:32:35.432 
Epoch 775/1000 
	 loss: 41.5710, MinusLogProbMetric: 41.5710, val_loss: 42.5117, val_MinusLogProbMetric: 42.5117

Epoch 775: val_loss did not improve from 42.32038
196/196 - 31s - loss: 41.5710 - MinusLogProbMetric: 41.5710 - val_loss: 42.5117 - val_MinusLogProbMetric: 42.5117 - lr: 8.3333e-05 - 31s/epoch - 158ms/step
Epoch 776/1000
2023-10-28 09:33:04.275 
Epoch 776/1000 
	 loss: 41.6507, MinusLogProbMetric: 41.6507, val_loss: 42.5608, val_MinusLogProbMetric: 42.5608

Epoch 776: val_loss did not improve from 42.32038
196/196 - 29s - loss: 41.6507 - MinusLogProbMetric: 41.6507 - val_loss: 42.5608 - val_MinusLogProbMetric: 42.5608 - lr: 8.3333e-05 - 29s/epoch - 147ms/step
Epoch 777/1000
2023-10-28 09:33:34.724 
Epoch 777/1000 
	 loss: 41.5480, MinusLogProbMetric: 41.5480, val_loss: 42.5684, val_MinusLogProbMetric: 42.5684

Epoch 777: val_loss did not improve from 42.32038
196/196 - 30s - loss: 41.5480 - MinusLogProbMetric: 41.5480 - val_loss: 42.5684 - val_MinusLogProbMetric: 42.5684 - lr: 8.3333e-05 - 30s/epoch - 155ms/step
Epoch 778/1000
2023-10-28 09:34:04.201 
Epoch 778/1000 
	 loss: 41.4989, MinusLogProbMetric: 41.4989, val_loss: 42.6273, val_MinusLogProbMetric: 42.6273

Epoch 778: val_loss did not improve from 42.32038
196/196 - 29s - loss: 41.4989 - MinusLogProbMetric: 41.4989 - val_loss: 42.6273 - val_MinusLogProbMetric: 42.6273 - lr: 8.3333e-05 - 29s/epoch - 150ms/step
Epoch 779/1000
2023-10-28 09:34:34.095 
Epoch 779/1000 
	 loss: 41.4871, MinusLogProbMetric: 41.4871, val_loss: 42.8318, val_MinusLogProbMetric: 42.8318

Epoch 779: val_loss did not improve from 42.32038
196/196 - 30s - loss: 41.4871 - MinusLogProbMetric: 41.4871 - val_loss: 42.8318 - val_MinusLogProbMetric: 42.8318 - lr: 8.3333e-05 - 30s/epoch - 153ms/step
Epoch 780/1000
2023-10-28 09:35:07.023 
Epoch 780/1000 
	 loss: 41.5138, MinusLogProbMetric: 41.5138, val_loss: 42.4375, val_MinusLogProbMetric: 42.4375

Epoch 780: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.5138 - MinusLogProbMetric: 41.5138 - val_loss: 42.4375 - val_MinusLogProbMetric: 42.4375 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 781/1000
2023-10-28 09:35:40.036 
Epoch 781/1000 
	 loss: 41.6838, MinusLogProbMetric: 41.6838, val_loss: 42.7366, val_MinusLogProbMetric: 42.7366

Epoch 781: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.6838 - MinusLogProbMetric: 41.6838 - val_loss: 42.7366 - val_MinusLogProbMetric: 42.7366 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 782/1000
2023-10-28 09:36:13.193 
Epoch 782/1000 
	 loss: 41.4383, MinusLogProbMetric: 41.4383, val_loss: 42.7272, val_MinusLogProbMetric: 42.7272

Epoch 782: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.4383 - MinusLogProbMetric: 41.4383 - val_loss: 42.7272 - val_MinusLogProbMetric: 42.7272 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 783/1000
2023-10-28 09:36:46.461 
Epoch 783/1000 
	 loss: 41.4411, MinusLogProbMetric: 41.4411, val_loss: 42.6030, val_MinusLogProbMetric: 42.6030

Epoch 783: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.4411 - MinusLogProbMetric: 41.4411 - val_loss: 42.6030 - val_MinusLogProbMetric: 42.6030 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 784/1000
2023-10-28 09:37:19.638 
Epoch 784/1000 
	 loss: 41.4901, MinusLogProbMetric: 41.4901, val_loss: 42.5898, val_MinusLogProbMetric: 42.5898

Epoch 784: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.4901 - MinusLogProbMetric: 41.4901 - val_loss: 42.5898 - val_MinusLogProbMetric: 42.5898 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 785/1000
2023-10-28 09:37:52.927 
Epoch 785/1000 
	 loss: 41.5037, MinusLogProbMetric: 41.5037, val_loss: 42.6090, val_MinusLogProbMetric: 42.6090

Epoch 785: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.5037 - MinusLogProbMetric: 41.5037 - val_loss: 42.6090 - val_MinusLogProbMetric: 42.6090 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 786/1000
2023-10-28 09:38:25.969 
Epoch 786/1000 
	 loss: 41.4798, MinusLogProbMetric: 41.4798, val_loss: 43.0525, val_MinusLogProbMetric: 43.0525

Epoch 786: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.4798 - MinusLogProbMetric: 41.4798 - val_loss: 43.0525 - val_MinusLogProbMetric: 43.0525 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 787/1000
2023-10-28 09:38:59.057 
Epoch 787/1000 
	 loss: 41.4601, MinusLogProbMetric: 41.4601, val_loss: 43.7037, val_MinusLogProbMetric: 43.7037

Epoch 787: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.4601 - MinusLogProbMetric: 41.4601 - val_loss: 43.7037 - val_MinusLogProbMetric: 43.7037 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 788/1000
2023-10-28 09:39:32.262 
Epoch 788/1000 
	 loss: 41.4468, MinusLogProbMetric: 41.4468, val_loss: 42.8606, val_MinusLogProbMetric: 42.8606

Epoch 788: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.4468 - MinusLogProbMetric: 41.4468 - val_loss: 42.8606 - val_MinusLogProbMetric: 42.8606 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 789/1000
2023-10-28 09:40:05.390 
Epoch 789/1000 
	 loss: 41.4724, MinusLogProbMetric: 41.4724, val_loss: 42.7984, val_MinusLogProbMetric: 42.7984

Epoch 789: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.4724 - MinusLogProbMetric: 41.4724 - val_loss: 42.7984 - val_MinusLogProbMetric: 42.7984 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 790/1000
2023-10-28 09:40:38.647 
Epoch 790/1000 
	 loss: 41.4571, MinusLogProbMetric: 41.4571, val_loss: 43.3103, val_MinusLogProbMetric: 43.3103

Epoch 790: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.4571 - MinusLogProbMetric: 41.4571 - val_loss: 43.3103 - val_MinusLogProbMetric: 43.3103 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 791/1000
2023-10-28 09:41:11.683 
Epoch 791/1000 
	 loss: 41.6362, MinusLogProbMetric: 41.6362, val_loss: 43.2468, val_MinusLogProbMetric: 43.2468

Epoch 791: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.6362 - MinusLogProbMetric: 41.6362 - val_loss: 43.2468 - val_MinusLogProbMetric: 43.2468 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 792/1000
2023-10-28 09:41:44.928 
Epoch 792/1000 
	 loss: 41.4298, MinusLogProbMetric: 41.4298, val_loss: 42.6587, val_MinusLogProbMetric: 42.6587

Epoch 792: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.4298 - MinusLogProbMetric: 41.4298 - val_loss: 42.6587 - val_MinusLogProbMetric: 42.6587 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 793/1000
2023-10-28 09:42:18.112 
Epoch 793/1000 
	 loss: 41.4678, MinusLogProbMetric: 41.4678, val_loss: 43.0120, val_MinusLogProbMetric: 43.0120

Epoch 793: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.4678 - MinusLogProbMetric: 41.4678 - val_loss: 43.0120 - val_MinusLogProbMetric: 43.0120 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 794/1000
2023-10-28 09:42:51.633 
Epoch 794/1000 
	 loss: 41.4222, MinusLogProbMetric: 41.4222, val_loss: 42.4061, val_MinusLogProbMetric: 42.4061

Epoch 794: val_loss did not improve from 42.32038
196/196 - 34s - loss: 41.4222 - MinusLogProbMetric: 41.4222 - val_loss: 42.4061 - val_MinusLogProbMetric: 42.4061 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 795/1000
2023-10-28 09:43:25.047 
Epoch 795/1000 
	 loss: 41.4691, MinusLogProbMetric: 41.4691, val_loss: 42.9448, val_MinusLogProbMetric: 42.9448

Epoch 795: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.4691 - MinusLogProbMetric: 41.4691 - val_loss: 42.9448 - val_MinusLogProbMetric: 42.9448 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 796/1000
2023-10-28 09:43:58.351 
Epoch 796/1000 
	 loss: 41.4878, MinusLogProbMetric: 41.4878, val_loss: 44.1329, val_MinusLogProbMetric: 44.1329

Epoch 796: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.4878 - MinusLogProbMetric: 41.4878 - val_loss: 44.1329 - val_MinusLogProbMetric: 44.1329 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 797/1000
2023-10-28 09:44:31.926 
Epoch 797/1000 
	 loss: 41.4419, MinusLogProbMetric: 41.4419, val_loss: 42.4305, val_MinusLogProbMetric: 42.4305

Epoch 797: val_loss did not improve from 42.32038
196/196 - 34s - loss: 41.4419 - MinusLogProbMetric: 41.4419 - val_loss: 42.4305 - val_MinusLogProbMetric: 42.4305 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 798/1000
2023-10-28 09:45:05.245 
Epoch 798/1000 
	 loss: 41.4962, MinusLogProbMetric: 41.4962, val_loss: 42.6198, val_MinusLogProbMetric: 42.6198

Epoch 798: val_loss did not improve from 42.32038
196/196 - 33s - loss: 41.4962 - MinusLogProbMetric: 41.4962 - val_loss: 42.6198 - val_MinusLogProbMetric: 42.6198 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 799/1000
2023-10-28 09:45:38.707 
Epoch 799/1000 
	 loss: 41.5015, MinusLogProbMetric: 41.5015, val_loss: 42.2884, val_MinusLogProbMetric: 42.2884

Epoch 799: val_loss improved from 42.32038 to 42.28845, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 41.5015 - MinusLogProbMetric: 41.5015 - val_loss: 42.2884 - val_MinusLogProbMetric: 42.2884 - lr: 8.3333e-05 - 34s/epoch - 173ms/step
Epoch 800/1000
2023-10-28 09:46:12.648 
Epoch 800/1000 
	 loss: 41.4884, MinusLogProbMetric: 41.4884, val_loss: 42.6329, val_MinusLogProbMetric: 42.6329

Epoch 800: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4884 - MinusLogProbMetric: 41.4884 - val_loss: 42.6329 - val_MinusLogProbMetric: 42.6329 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 801/1000
2023-10-28 09:46:45.934 
Epoch 801/1000 
	 loss: 41.4420, MinusLogProbMetric: 41.4420, val_loss: 42.5081, val_MinusLogProbMetric: 42.5081

Epoch 801: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4420 - MinusLogProbMetric: 41.4420 - val_loss: 42.5081 - val_MinusLogProbMetric: 42.5081 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 802/1000
2023-10-28 09:47:19.068 
Epoch 802/1000 
	 loss: 41.4220, MinusLogProbMetric: 41.4220, val_loss: 42.4945, val_MinusLogProbMetric: 42.4945

Epoch 802: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4220 - MinusLogProbMetric: 41.4220 - val_loss: 42.4945 - val_MinusLogProbMetric: 42.4945 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 803/1000
2023-10-28 09:47:51.982 
Epoch 803/1000 
	 loss: 41.4484, MinusLogProbMetric: 41.4484, val_loss: 42.6723, val_MinusLogProbMetric: 42.6723

Epoch 803: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4484 - MinusLogProbMetric: 41.4484 - val_loss: 42.6723 - val_MinusLogProbMetric: 42.6723 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 804/1000
2023-10-28 09:48:25.306 
Epoch 804/1000 
	 loss: 41.4490, MinusLogProbMetric: 41.4490, val_loss: 42.6217, val_MinusLogProbMetric: 42.6217

Epoch 804: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4490 - MinusLogProbMetric: 41.4490 - val_loss: 42.6217 - val_MinusLogProbMetric: 42.6217 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 805/1000
2023-10-28 09:48:58.463 
Epoch 805/1000 
	 loss: 41.4312, MinusLogProbMetric: 41.4312, val_loss: 42.3103, val_MinusLogProbMetric: 42.3103

Epoch 805: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4312 - MinusLogProbMetric: 41.4312 - val_loss: 42.3103 - val_MinusLogProbMetric: 42.3103 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 806/1000
2023-10-28 09:49:31.634 
Epoch 806/1000 
	 loss: 41.4681, MinusLogProbMetric: 41.4681, val_loss: 42.5306, val_MinusLogProbMetric: 42.5306

Epoch 806: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4681 - MinusLogProbMetric: 41.4681 - val_loss: 42.5306 - val_MinusLogProbMetric: 42.5306 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 807/1000
2023-10-28 09:50:04.794 
Epoch 807/1000 
	 loss: 41.4139, MinusLogProbMetric: 41.4139, val_loss: 42.4193, val_MinusLogProbMetric: 42.4193

Epoch 807: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4139 - MinusLogProbMetric: 41.4139 - val_loss: 42.4193 - val_MinusLogProbMetric: 42.4193 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 808/1000
2023-10-28 09:50:38.005 
Epoch 808/1000 
	 loss: 41.4610, MinusLogProbMetric: 41.4610, val_loss: 42.7463, val_MinusLogProbMetric: 42.7463

Epoch 808: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4610 - MinusLogProbMetric: 41.4610 - val_loss: 42.7463 - val_MinusLogProbMetric: 42.7463 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 809/1000
2023-10-28 09:51:11.494 
Epoch 809/1000 
	 loss: 41.4944, MinusLogProbMetric: 41.4944, val_loss: 42.6811, val_MinusLogProbMetric: 42.6811

Epoch 809: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4944 - MinusLogProbMetric: 41.4944 - val_loss: 42.6811 - val_MinusLogProbMetric: 42.6811 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 810/1000
2023-10-28 09:51:44.904 
Epoch 810/1000 
	 loss: 41.4045, MinusLogProbMetric: 41.4045, val_loss: 42.3300, val_MinusLogProbMetric: 42.3300

Epoch 810: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4045 - MinusLogProbMetric: 41.4045 - val_loss: 42.3300 - val_MinusLogProbMetric: 42.3300 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 811/1000
2023-10-28 09:52:17.976 
Epoch 811/1000 
	 loss: 41.4476, MinusLogProbMetric: 41.4476, val_loss: 42.5926, val_MinusLogProbMetric: 42.5926

Epoch 811: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4476 - MinusLogProbMetric: 41.4476 - val_loss: 42.5926 - val_MinusLogProbMetric: 42.5926 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 812/1000
2023-10-28 09:52:51.409 
Epoch 812/1000 
	 loss: 41.4112, MinusLogProbMetric: 41.4112, val_loss: 42.5188, val_MinusLogProbMetric: 42.5188

Epoch 812: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4112 - MinusLogProbMetric: 41.4112 - val_loss: 42.5188 - val_MinusLogProbMetric: 42.5188 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 813/1000
2023-10-28 09:53:24.718 
Epoch 813/1000 
	 loss: 41.4518, MinusLogProbMetric: 41.4518, val_loss: 43.3682, val_MinusLogProbMetric: 43.3682

Epoch 813: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4518 - MinusLogProbMetric: 41.4518 - val_loss: 43.3682 - val_MinusLogProbMetric: 43.3682 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 814/1000
2023-10-28 09:53:57.817 
Epoch 814/1000 
	 loss: 41.4462, MinusLogProbMetric: 41.4462, val_loss: 42.4166, val_MinusLogProbMetric: 42.4166

Epoch 814: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4462 - MinusLogProbMetric: 41.4462 - val_loss: 42.4166 - val_MinusLogProbMetric: 42.4166 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 815/1000
2023-10-28 09:54:31.052 
Epoch 815/1000 
	 loss: 41.4460, MinusLogProbMetric: 41.4460, val_loss: 42.3812, val_MinusLogProbMetric: 42.3812

Epoch 815: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4460 - MinusLogProbMetric: 41.4460 - val_loss: 42.3812 - val_MinusLogProbMetric: 42.3812 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 816/1000
2023-10-28 09:55:04.276 
Epoch 816/1000 
	 loss: 41.4563, MinusLogProbMetric: 41.4563, val_loss: 42.4051, val_MinusLogProbMetric: 42.4051

Epoch 816: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4563 - MinusLogProbMetric: 41.4563 - val_loss: 42.4051 - val_MinusLogProbMetric: 42.4051 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 817/1000
2023-10-28 09:55:37.884 
Epoch 817/1000 
	 loss: 41.4361, MinusLogProbMetric: 41.4361, val_loss: 42.5046, val_MinusLogProbMetric: 42.5046

Epoch 817: val_loss did not improve from 42.28845
196/196 - 34s - loss: 41.4361 - MinusLogProbMetric: 41.4361 - val_loss: 42.5046 - val_MinusLogProbMetric: 42.5046 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 818/1000
2023-10-28 09:56:11.245 
Epoch 818/1000 
	 loss: 41.4673, MinusLogProbMetric: 41.4673, val_loss: 42.5207, val_MinusLogProbMetric: 42.5207

Epoch 818: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4673 - MinusLogProbMetric: 41.4673 - val_loss: 42.5207 - val_MinusLogProbMetric: 42.5207 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 819/1000
2023-10-28 09:56:44.804 
Epoch 819/1000 
	 loss: 41.4398, MinusLogProbMetric: 41.4398, val_loss: 42.4576, val_MinusLogProbMetric: 42.4576

Epoch 819: val_loss did not improve from 42.28845
196/196 - 34s - loss: 41.4398 - MinusLogProbMetric: 41.4398 - val_loss: 42.4576 - val_MinusLogProbMetric: 42.4576 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 820/1000
2023-10-28 09:57:18.182 
Epoch 820/1000 
	 loss: 41.4492, MinusLogProbMetric: 41.4492, val_loss: 42.5540, val_MinusLogProbMetric: 42.5540

Epoch 820: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4492 - MinusLogProbMetric: 41.4492 - val_loss: 42.5540 - val_MinusLogProbMetric: 42.5540 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 821/1000
2023-10-28 09:57:51.863 
Epoch 821/1000 
	 loss: 41.6439, MinusLogProbMetric: 41.6439, val_loss: 42.5227, val_MinusLogProbMetric: 42.5227

Epoch 821: val_loss did not improve from 42.28845
196/196 - 34s - loss: 41.6439 - MinusLogProbMetric: 41.6439 - val_loss: 42.5227 - val_MinusLogProbMetric: 42.5227 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 822/1000
2023-10-28 09:58:25.448 
Epoch 822/1000 
	 loss: 41.4967, MinusLogProbMetric: 41.4967, val_loss: 42.9879, val_MinusLogProbMetric: 42.9879

Epoch 822: val_loss did not improve from 42.28845
196/196 - 34s - loss: 41.4967 - MinusLogProbMetric: 41.4967 - val_loss: 42.9879 - val_MinusLogProbMetric: 42.9879 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 823/1000
2023-10-28 09:58:58.979 
Epoch 823/1000 
	 loss: 41.4511, MinusLogProbMetric: 41.4511, val_loss: 42.7870, val_MinusLogProbMetric: 42.7870

Epoch 823: val_loss did not improve from 42.28845
196/196 - 34s - loss: 41.4511 - MinusLogProbMetric: 41.4511 - val_loss: 42.7870 - val_MinusLogProbMetric: 42.7870 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 824/1000
2023-10-28 09:59:32.310 
Epoch 824/1000 
	 loss: 41.4197, MinusLogProbMetric: 41.4197, val_loss: 42.7126, val_MinusLogProbMetric: 42.7126

Epoch 824: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4197 - MinusLogProbMetric: 41.4197 - val_loss: 42.7126 - val_MinusLogProbMetric: 42.7126 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 825/1000
2023-10-28 10:00:05.802 
Epoch 825/1000 
	 loss: 41.4462, MinusLogProbMetric: 41.4462, val_loss: 42.5040, val_MinusLogProbMetric: 42.5040

Epoch 825: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4462 - MinusLogProbMetric: 41.4462 - val_loss: 42.5040 - val_MinusLogProbMetric: 42.5040 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 826/1000
2023-10-28 10:00:39.329 
Epoch 826/1000 
	 loss: 41.3902, MinusLogProbMetric: 41.3902, val_loss: 42.3859, val_MinusLogProbMetric: 42.3859

Epoch 826: val_loss did not improve from 42.28845
196/196 - 34s - loss: 41.3902 - MinusLogProbMetric: 41.3902 - val_loss: 42.3859 - val_MinusLogProbMetric: 42.3859 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 827/1000
2023-10-28 10:01:12.658 
Epoch 827/1000 
	 loss: 41.4403, MinusLogProbMetric: 41.4403, val_loss: 43.3795, val_MinusLogProbMetric: 43.3795

Epoch 827: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4403 - MinusLogProbMetric: 41.4403 - val_loss: 43.3795 - val_MinusLogProbMetric: 43.3795 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 828/1000
2023-10-28 10:01:45.870 
Epoch 828/1000 
	 loss: 41.5128, MinusLogProbMetric: 41.5128, val_loss: 43.7368, val_MinusLogProbMetric: 43.7368

Epoch 828: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.5128 - MinusLogProbMetric: 41.5128 - val_loss: 43.7368 - val_MinusLogProbMetric: 43.7368 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 829/1000
2023-10-28 10:02:19.123 
Epoch 829/1000 
	 loss: 41.5703, MinusLogProbMetric: 41.5703, val_loss: 42.5302, val_MinusLogProbMetric: 42.5302

Epoch 829: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.5703 - MinusLogProbMetric: 41.5703 - val_loss: 42.5302 - val_MinusLogProbMetric: 42.5302 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 830/1000
2023-10-28 10:02:51.933 
Epoch 830/1000 
	 loss: 41.4695, MinusLogProbMetric: 41.4695, val_loss: 42.5098, val_MinusLogProbMetric: 42.5098

Epoch 830: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4695 - MinusLogProbMetric: 41.4695 - val_loss: 42.5098 - val_MinusLogProbMetric: 42.5098 - lr: 8.3333e-05 - 33s/epoch - 167ms/step
Epoch 831/1000
2023-10-28 10:03:25.200 
Epoch 831/1000 
	 loss: 41.4754, MinusLogProbMetric: 41.4754, val_loss: 42.5788, val_MinusLogProbMetric: 42.5788

Epoch 831: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4754 - MinusLogProbMetric: 41.4754 - val_loss: 42.5788 - val_MinusLogProbMetric: 42.5788 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 832/1000
2023-10-28 10:03:58.765 
Epoch 832/1000 
	 loss: 41.3677, MinusLogProbMetric: 41.3677, val_loss: 42.7251, val_MinusLogProbMetric: 42.7251

Epoch 832: val_loss did not improve from 42.28845
196/196 - 34s - loss: 41.3677 - MinusLogProbMetric: 41.3677 - val_loss: 42.7251 - val_MinusLogProbMetric: 42.7251 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 833/1000
2023-10-28 10:04:31.822 
Epoch 833/1000 
	 loss: 41.4372, MinusLogProbMetric: 41.4372, val_loss: 42.3530, val_MinusLogProbMetric: 42.3530

Epoch 833: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4372 - MinusLogProbMetric: 41.4372 - val_loss: 42.3530 - val_MinusLogProbMetric: 42.3530 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 834/1000
2023-10-28 10:05:05.113 
Epoch 834/1000 
	 loss: 41.3994, MinusLogProbMetric: 41.3994, val_loss: 42.4734, val_MinusLogProbMetric: 42.4734

Epoch 834: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.3994 - MinusLogProbMetric: 41.3994 - val_loss: 42.4734 - val_MinusLogProbMetric: 42.4734 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 835/1000
2023-10-28 10:05:38.808 
Epoch 835/1000 
	 loss: 41.4450, MinusLogProbMetric: 41.4450, val_loss: 42.4849, val_MinusLogProbMetric: 42.4849

Epoch 835: val_loss did not improve from 42.28845
196/196 - 34s - loss: 41.4450 - MinusLogProbMetric: 41.4450 - val_loss: 42.4849 - val_MinusLogProbMetric: 42.4849 - lr: 8.3333e-05 - 34s/epoch - 172ms/step
Epoch 836/1000
2023-10-28 10:06:12.415 
Epoch 836/1000 
	 loss: 41.4612, MinusLogProbMetric: 41.4612, val_loss: 42.4655, val_MinusLogProbMetric: 42.4655

Epoch 836: val_loss did not improve from 42.28845
196/196 - 34s - loss: 41.4612 - MinusLogProbMetric: 41.4612 - val_loss: 42.4655 - val_MinusLogProbMetric: 42.4655 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 837/1000
2023-10-28 10:06:46.019 
Epoch 837/1000 
	 loss: 41.4428, MinusLogProbMetric: 41.4428, val_loss: 42.6704, val_MinusLogProbMetric: 42.6704

Epoch 837: val_loss did not improve from 42.28845
196/196 - 34s - loss: 41.4428 - MinusLogProbMetric: 41.4428 - val_loss: 42.6704 - val_MinusLogProbMetric: 42.6704 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 838/1000
2023-10-28 10:07:19.240 
Epoch 838/1000 
	 loss: 41.4232, MinusLogProbMetric: 41.4232, val_loss: 42.4930, val_MinusLogProbMetric: 42.4930

Epoch 838: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4232 - MinusLogProbMetric: 41.4232 - val_loss: 42.4930 - val_MinusLogProbMetric: 42.4930 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 839/1000
2023-10-28 10:07:52.362 
Epoch 839/1000 
	 loss: 41.4752, MinusLogProbMetric: 41.4752, val_loss: 42.4576, val_MinusLogProbMetric: 42.4576

Epoch 839: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4752 - MinusLogProbMetric: 41.4752 - val_loss: 42.4576 - val_MinusLogProbMetric: 42.4576 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 840/1000
2023-10-28 10:08:25.961 
Epoch 840/1000 
	 loss: 41.3721, MinusLogProbMetric: 41.3721, val_loss: 42.3450, val_MinusLogProbMetric: 42.3450

Epoch 840: val_loss did not improve from 42.28845
196/196 - 34s - loss: 41.3721 - MinusLogProbMetric: 41.3721 - val_loss: 42.3450 - val_MinusLogProbMetric: 42.3450 - lr: 8.3333e-05 - 34s/epoch - 171ms/step
Epoch 841/1000
2023-10-28 10:08:59.408 
Epoch 841/1000 
	 loss: 41.6319, MinusLogProbMetric: 41.6319, val_loss: 42.7808, val_MinusLogProbMetric: 42.7808

Epoch 841: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.6319 - MinusLogProbMetric: 41.6319 - val_loss: 42.7808 - val_MinusLogProbMetric: 42.7808 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 842/1000
2023-10-28 10:09:32.732 
Epoch 842/1000 
	 loss: 41.4154, MinusLogProbMetric: 41.4154, val_loss: 42.3347, val_MinusLogProbMetric: 42.3347

Epoch 842: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4154 - MinusLogProbMetric: 41.4154 - val_loss: 42.3347 - val_MinusLogProbMetric: 42.3347 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 843/1000
2023-10-28 10:10:04.075 
Epoch 843/1000 
	 loss: 41.4242, MinusLogProbMetric: 41.4242, val_loss: 43.3136, val_MinusLogProbMetric: 43.3136

Epoch 843: val_loss did not improve from 42.28845
196/196 - 31s - loss: 41.4242 - MinusLogProbMetric: 41.4242 - val_loss: 43.3136 - val_MinusLogProbMetric: 43.3136 - lr: 8.3333e-05 - 31s/epoch - 160ms/step
Epoch 844/1000
2023-10-28 10:10:32.789 
Epoch 844/1000 
	 loss: 41.4678, MinusLogProbMetric: 41.4678, val_loss: 42.7912, val_MinusLogProbMetric: 42.7912

Epoch 844: val_loss did not improve from 42.28845
196/196 - 29s - loss: 41.4678 - MinusLogProbMetric: 41.4678 - val_loss: 42.7912 - val_MinusLogProbMetric: 42.7912 - lr: 8.3333e-05 - 29s/epoch - 146ms/step
Epoch 845/1000
2023-10-28 10:11:03.653 
Epoch 845/1000 
	 loss: 41.4376, MinusLogProbMetric: 41.4376, val_loss: 42.5455, val_MinusLogProbMetric: 42.5455

Epoch 845: val_loss did not improve from 42.28845
196/196 - 31s - loss: 41.4376 - MinusLogProbMetric: 41.4376 - val_loss: 42.5455 - val_MinusLogProbMetric: 42.5455 - lr: 8.3333e-05 - 31s/epoch - 157ms/step
Epoch 846/1000
2023-10-28 10:11:37.053 
Epoch 846/1000 
	 loss: 41.4306, MinusLogProbMetric: 41.4306, val_loss: 42.5558, val_MinusLogProbMetric: 42.5558

Epoch 846: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4306 - MinusLogProbMetric: 41.4306 - val_loss: 42.5558 - val_MinusLogProbMetric: 42.5558 - lr: 8.3333e-05 - 33s/epoch - 170ms/step
Epoch 847/1000
2023-10-28 10:12:09.923 
Epoch 847/1000 
	 loss: 41.3904, MinusLogProbMetric: 41.3904, val_loss: 42.3997, val_MinusLogProbMetric: 42.3997

Epoch 847: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.3904 - MinusLogProbMetric: 41.3904 - val_loss: 42.3997 - val_MinusLogProbMetric: 42.3997 - lr: 8.3333e-05 - 33s/epoch - 168ms/step
Epoch 848/1000
2023-10-28 10:12:43.372 
Epoch 848/1000 
	 loss: 41.4596, MinusLogProbMetric: 41.4596, val_loss: 42.6069, val_MinusLogProbMetric: 42.6069

Epoch 848: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4596 - MinusLogProbMetric: 41.4596 - val_loss: 42.6069 - val_MinusLogProbMetric: 42.6069 - lr: 8.3333e-05 - 33s/epoch - 171ms/step
Epoch 849/1000
2023-10-28 10:13:16.564 
Epoch 849/1000 
	 loss: 41.4021, MinusLogProbMetric: 41.4021, val_loss: 43.1907, val_MinusLogProbMetric: 43.1907

Epoch 849: val_loss did not improve from 42.28845
196/196 - 33s - loss: 41.4021 - MinusLogProbMetric: 41.4021 - val_loss: 43.1907 - val_MinusLogProbMetric: 43.1907 - lr: 8.3333e-05 - 33s/epoch - 169ms/step
Epoch 850/1000
2023-10-28 10:13:49.874 
Epoch 850/1000 
	 loss: 41.1490, MinusLogProbMetric: 41.1490, val_loss: 42.2877, val_MinusLogProbMetric: 42.2877

Epoch 850: val_loss improved from 42.28845 to 42.28771, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 41.1490 - MinusLogProbMetric: 41.1490 - val_loss: 42.2877 - val_MinusLogProbMetric: 42.2877 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 851/1000
2023-10-28 10:14:23.609 
Epoch 851/1000 
	 loss: 41.1328, MinusLogProbMetric: 41.1328, val_loss: 42.2884, val_MinusLogProbMetric: 42.2884

Epoch 851: val_loss did not improve from 42.28771
196/196 - 33s - loss: 41.1328 - MinusLogProbMetric: 41.1328 - val_loss: 42.2884 - val_MinusLogProbMetric: 42.2884 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 852/1000
2023-10-28 10:14:56.937 
Epoch 852/1000 
	 loss: 41.1339, MinusLogProbMetric: 41.1339, val_loss: 42.2358, val_MinusLogProbMetric: 42.2358

Epoch 852: val_loss improved from 42.28771 to 42.23580, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 41.1339 - MinusLogProbMetric: 41.1339 - val_loss: 42.2358 - val_MinusLogProbMetric: 42.2358 - lr: 4.1667e-05 - 34s/epoch - 173ms/step
Epoch 853/1000
2023-10-28 10:15:30.536 
Epoch 853/1000 
	 loss: 41.1228, MinusLogProbMetric: 41.1228, val_loss: 42.3053, val_MinusLogProbMetric: 42.3053

Epoch 853: val_loss did not improve from 42.23580
196/196 - 33s - loss: 41.1228 - MinusLogProbMetric: 41.1228 - val_loss: 42.3053 - val_MinusLogProbMetric: 42.3053 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 854/1000
2023-10-28 10:16:03.999 
Epoch 854/1000 
	 loss: 41.1285, MinusLogProbMetric: 41.1285, val_loss: 42.2853, val_MinusLogProbMetric: 42.2853

Epoch 854: val_loss did not improve from 42.23580
196/196 - 33s - loss: 41.1285 - MinusLogProbMetric: 41.1285 - val_loss: 42.2853 - val_MinusLogProbMetric: 42.2853 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 855/1000
2023-10-28 10:16:37.148 
Epoch 855/1000 
	 loss: 41.1202, MinusLogProbMetric: 41.1202, val_loss: 42.4218, val_MinusLogProbMetric: 42.4218

Epoch 855: val_loss did not improve from 42.23580
196/196 - 33s - loss: 41.1202 - MinusLogProbMetric: 41.1202 - val_loss: 42.4218 - val_MinusLogProbMetric: 42.4218 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 856/1000
2023-10-28 10:17:10.400 
Epoch 856/1000 
	 loss: 41.1127, MinusLogProbMetric: 41.1127, val_loss: 42.3586, val_MinusLogProbMetric: 42.3586

Epoch 856: val_loss did not improve from 42.23580
196/196 - 33s - loss: 41.1127 - MinusLogProbMetric: 41.1127 - val_loss: 42.3586 - val_MinusLogProbMetric: 42.3586 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 857/1000
2023-10-28 10:17:41.473 
Epoch 857/1000 
	 loss: 41.1506, MinusLogProbMetric: 41.1506, val_loss: 42.3225, val_MinusLogProbMetric: 42.3225

Epoch 857: val_loss did not improve from 42.23580
196/196 - 31s - loss: 41.1506 - MinusLogProbMetric: 41.1506 - val_loss: 42.3225 - val_MinusLogProbMetric: 42.3225 - lr: 4.1667e-05 - 31s/epoch - 159ms/step
Epoch 858/1000
2023-10-28 10:18:13.971 
Epoch 858/1000 
	 loss: 41.1190, MinusLogProbMetric: 41.1190, val_loss: 42.3468, val_MinusLogProbMetric: 42.3468

Epoch 858: val_loss did not improve from 42.23580
196/196 - 32s - loss: 41.1190 - MinusLogProbMetric: 41.1190 - val_loss: 42.3468 - val_MinusLogProbMetric: 42.3468 - lr: 4.1667e-05 - 32s/epoch - 166ms/step
Epoch 859/1000
2023-10-28 10:18:45.103 
Epoch 859/1000 
	 loss: 41.1223, MinusLogProbMetric: 41.1223, val_loss: 42.2506, val_MinusLogProbMetric: 42.2506

Epoch 859: val_loss did not improve from 42.23580
196/196 - 31s - loss: 41.1223 - MinusLogProbMetric: 41.1223 - val_loss: 42.2506 - val_MinusLogProbMetric: 42.2506 - lr: 4.1667e-05 - 31s/epoch - 159ms/step
Epoch 860/1000
2023-10-28 10:19:18.356 
Epoch 860/1000 
	 loss: 41.0950, MinusLogProbMetric: 41.0950, val_loss: 42.2367, val_MinusLogProbMetric: 42.2367

Epoch 860: val_loss did not improve from 42.23580
196/196 - 33s - loss: 41.0950 - MinusLogProbMetric: 41.0950 - val_loss: 42.2367 - val_MinusLogProbMetric: 42.2367 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 861/1000
2023-10-28 10:19:52.126 
Epoch 861/1000 
	 loss: 41.1329, MinusLogProbMetric: 41.1329, val_loss: 42.3771, val_MinusLogProbMetric: 42.3771

Epoch 861: val_loss did not improve from 42.23580
196/196 - 34s - loss: 41.1329 - MinusLogProbMetric: 41.1329 - val_loss: 42.3771 - val_MinusLogProbMetric: 42.3771 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 862/1000
2023-10-28 10:20:26.166 
Epoch 862/1000 
	 loss: 41.1236, MinusLogProbMetric: 41.1236, val_loss: 42.3060, val_MinusLogProbMetric: 42.3060

Epoch 862: val_loss did not improve from 42.23580
196/196 - 34s - loss: 41.1236 - MinusLogProbMetric: 41.1236 - val_loss: 42.3060 - val_MinusLogProbMetric: 42.3060 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 863/1000
2023-10-28 10:20:59.059 
Epoch 863/1000 
	 loss: 41.1402, MinusLogProbMetric: 41.1402, val_loss: 42.3035, val_MinusLogProbMetric: 42.3035

Epoch 863: val_loss did not improve from 42.23580
196/196 - 33s - loss: 41.1402 - MinusLogProbMetric: 41.1402 - val_loss: 42.3035 - val_MinusLogProbMetric: 42.3035 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 864/1000
2023-10-28 10:21:31.381 
Epoch 864/1000 
	 loss: 41.1028, MinusLogProbMetric: 41.1028, val_loss: 42.2830, val_MinusLogProbMetric: 42.2830

Epoch 864: val_loss did not improve from 42.23580
196/196 - 32s - loss: 41.1028 - MinusLogProbMetric: 41.1028 - val_loss: 42.2830 - val_MinusLogProbMetric: 42.2830 - lr: 4.1667e-05 - 32s/epoch - 165ms/step
Epoch 865/1000
2023-10-28 10:22:04.928 
Epoch 865/1000 
	 loss: 41.1126, MinusLogProbMetric: 41.1126, val_loss: 42.3553, val_MinusLogProbMetric: 42.3553

Epoch 865: val_loss did not improve from 42.23580
196/196 - 34s - loss: 41.1126 - MinusLogProbMetric: 41.1126 - val_loss: 42.3553 - val_MinusLogProbMetric: 42.3553 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 866/1000
2023-10-28 10:22:38.454 
Epoch 866/1000 
	 loss: 41.1377, MinusLogProbMetric: 41.1377, val_loss: 42.2048, val_MinusLogProbMetric: 42.2048

Epoch 866: val_loss improved from 42.23580 to 42.20476, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 41.1377 - MinusLogProbMetric: 41.1377 - val_loss: 42.2048 - val_MinusLogProbMetric: 42.2048 - lr: 4.1667e-05 - 34s/epoch - 174ms/step
Epoch 867/1000
2023-10-28 10:23:12.358 
Epoch 867/1000 
	 loss: 41.1242, MinusLogProbMetric: 41.1242, val_loss: 42.7768, val_MinusLogProbMetric: 42.7768

Epoch 867: val_loss did not improve from 42.20476
196/196 - 33s - loss: 41.1242 - MinusLogProbMetric: 41.1242 - val_loss: 42.7768 - val_MinusLogProbMetric: 42.7768 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 868/1000
2023-10-28 10:23:45.633 
Epoch 868/1000 
	 loss: 41.1351, MinusLogProbMetric: 41.1351, val_loss: 42.4021, val_MinusLogProbMetric: 42.4021

Epoch 868: val_loss did not improve from 42.20476
196/196 - 33s - loss: 41.1351 - MinusLogProbMetric: 41.1351 - val_loss: 42.4021 - val_MinusLogProbMetric: 42.4021 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 869/1000
2023-10-28 10:24:18.934 
Epoch 869/1000 
	 loss: 41.1486, MinusLogProbMetric: 41.1486, val_loss: 42.2193, val_MinusLogProbMetric: 42.2193

Epoch 869: val_loss did not improve from 42.20476
196/196 - 33s - loss: 41.1486 - MinusLogProbMetric: 41.1486 - val_loss: 42.2193 - val_MinusLogProbMetric: 42.2193 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 870/1000
2023-10-28 10:24:50.349 
Epoch 870/1000 
	 loss: 41.1434, MinusLogProbMetric: 41.1434, val_loss: 42.4194, val_MinusLogProbMetric: 42.4194

Epoch 870: val_loss did not improve from 42.20476
196/196 - 31s - loss: 41.1434 - MinusLogProbMetric: 41.1434 - val_loss: 42.4194 - val_MinusLogProbMetric: 42.4194 - lr: 4.1667e-05 - 31s/epoch - 160ms/step
Epoch 871/1000
2023-10-28 10:25:21.542 
Epoch 871/1000 
	 loss: 41.2032, MinusLogProbMetric: 41.2032, val_loss: 42.3015, val_MinusLogProbMetric: 42.3015

Epoch 871: val_loss did not improve from 42.20476
196/196 - 31s - loss: 41.2032 - MinusLogProbMetric: 41.2032 - val_loss: 42.3015 - val_MinusLogProbMetric: 42.3015 - lr: 4.1667e-05 - 31s/epoch - 159ms/step
Epoch 872/1000
2023-10-28 10:25:52.900 
Epoch 872/1000 
	 loss: 41.1041, MinusLogProbMetric: 41.1041, val_loss: 42.2328, val_MinusLogProbMetric: 42.2328

Epoch 872: val_loss did not improve from 42.20476
196/196 - 31s - loss: 41.1041 - MinusLogProbMetric: 41.1041 - val_loss: 42.2328 - val_MinusLogProbMetric: 42.2328 - lr: 4.1667e-05 - 31s/epoch - 160ms/step
Epoch 873/1000
2023-10-28 10:26:25.740 
Epoch 873/1000 
	 loss: 41.1999, MinusLogProbMetric: 41.1999, val_loss: 42.2622, val_MinusLogProbMetric: 42.2622

Epoch 873: val_loss did not improve from 42.20476
196/196 - 33s - loss: 41.1999 - MinusLogProbMetric: 41.1999 - val_loss: 42.2622 - val_MinusLogProbMetric: 42.2622 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 874/1000
2023-10-28 10:26:58.970 
Epoch 874/1000 
	 loss: 41.1687, MinusLogProbMetric: 41.1687, val_loss: 42.3707, val_MinusLogProbMetric: 42.3707

Epoch 874: val_loss did not improve from 42.20476
196/196 - 33s - loss: 41.1687 - MinusLogProbMetric: 41.1687 - val_loss: 42.3707 - val_MinusLogProbMetric: 42.3707 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 875/1000
2023-10-28 10:27:32.239 
Epoch 875/1000 
	 loss: 41.0964, MinusLogProbMetric: 41.0964, val_loss: 42.7762, val_MinusLogProbMetric: 42.7762

Epoch 875: val_loss did not improve from 42.20476
196/196 - 33s - loss: 41.0964 - MinusLogProbMetric: 41.0964 - val_loss: 42.7762 - val_MinusLogProbMetric: 42.7762 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 876/1000
2023-10-28 10:28:05.512 
Epoch 876/1000 
	 loss: 41.1138, MinusLogProbMetric: 41.1138, val_loss: 42.5661, val_MinusLogProbMetric: 42.5661

Epoch 876: val_loss did not improve from 42.20476
196/196 - 33s - loss: 41.1138 - MinusLogProbMetric: 41.1138 - val_loss: 42.5661 - val_MinusLogProbMetric: 42.5661 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 877/1000
2023-10-28 10:28:36.599 
Epoch 877/1000 
	 loss: 41.0950, MinusLogProbMetric: 41.0950, val_loss: 42.2523, val_MinusLogProbMetric: 42.2523

Epoch 877: val_loss did not improve from 42.20476
196/196 - 31s - loss: 41.0950 - MinusLogProbMetric: 41.0950 - val_loss: 42.2523 - val_MinusLogProbMetric: 42.2523 - lr: 4.1667e-05 - 31s/epoch - 159ms/step
Epoch 878/1000
2023-10-28 10:29:06.727 
Epoch 878/1000 
	 loss: 41.1651, MinusLogProbMetric: 41.1651, val_loss: 42.2515, val_MinusLogProbMetric: 42.2515

Epoch 878: val_loss did not improve from 42.20476
196/196 - 30s - loss: 41.1651 - MinusLogProbMetric: 41.1651 - val_loss: 42.2515 - val_MinusLogProbMetric: 42.2515 - lr: 4.1667e-05 - 30s/epoch - 154ms/step
Epoch 879/1000
2023-10-28 10:29:37.765 
Epoch 879/1000 
	 loss: 41.1056, MinusLogProbMetric: 41.1056, val_loss: 42.1714, val_MinusLogProbMetric: 42.1714

Epoch 879: val_loss improved from 42.20476 to 42.17144, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 32s - loss: 41.1056 - MinusLogProbMetric: 41.1056 - val_loss: 42.1714 - val_MinusLogProbMetric: 42.1714 - lr: 4.1667e-05 - 32s/epoch - 161ms/step
Epoch 880/1000
2023-10-28 10:30:11.807 
Epoch 880/1000 
	 loss: 41.0925, MinusLogProbMetric: 41.0925, val_loss: 42.2423, val_MinusLogProbMetric: 42.2423

Epoch 880: val_loss did not improve from 42.17144
196/196 - 34s - loss: 41.0925 - MinusLogProbMetric: 41.0925 - val_loss: 42.2423 - val_MinusLogProbMetric: 42.2423 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 881/1000
2023-10-28 10:30:45.262 
Epoch 881/1000 
	 loss: 41.1045, MinusLogProbMetric: 41.1045, val_loss: 42.2084, val_MinusLogProbMetric: 42.2084

Epoch 881: val_loss did not improve from 42.17144
196/196 - 33s - loss: 41.1045 - MinusLogProbMetric: 41.1045 - val_loss: 42.2084 - val_MinusLogProbMetric: 42.2084 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 882/1000
2023-10-28 10:31:18.780 
Epoch 882/1000 
	 loss: 41.1271, MinusLogProbMetric: 41.1271, val_loss: 42.3240, val_MinusLogProbMetric: 42.3240

Epoch 882: val_loss did not improve from 42.17144
196/196 - 34s - loss: 41.1271 - MinusLogProbMetric: 41.1271 - val_loss: 42.3240 - val_MinusLogProbMetric: 42.3240 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 883/1000
2023-10-28 10:31:51.894 
Epoch 883/1000 
	 loss: 41.0925, MinusLogProbMetric: 41.0925, val_loss: 42.3133, val_MinusLogProbMetric: 42.3133

Epoch 883: val_loss did not improve from 42.17144
196/196 - 33s - loss: 41.0925 - MinusLogProbMetric: 41.0925 - val_loss: 42.3133 - val_MinusLogProbMetric: 42.3133 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 884/1000
2023-10-28 10:32:25.300 
Epoch 884/1000 
	 loss: 41.1114, MinusLogProbMetric: 41.1114, val_loss: 42.2879, val_MinusLogProbMetric: 42.2879

Epoch 884: val_loss did not improve from 42.17144
196/196 - 33s - loss: 41.1114 - MinusLogProbMetric: 41.1114 - val_loss: 42.2879 - val_MinusLogProbMetric: 42.2879 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 885/1000
2023-10-28 10:32:58.677 
Epoch 885/1000 
	 loss: 41.2462, MinusLogProbMetric: 41.2462, val_loss: 42.2592, val_MinusLogProbMetric: 42.2592

Epoch 885: val_loss did not improve from 42.17144
196/196 - 33s - loss: 41.2462 - MinusLogProbMetric: 41.2462 - val_loss: 42.2592 - val_MinusLogProbMetric: 42.2592 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 886/1000
2023-10-28 10:33:31.909 
Epoch 886/1000 
	 loss: 41.0869, MinusLogProbMetric: 41.0869, val_loss: 42.2023, val_MinusLogProbMetric: 42.2023

Epoch 886: val_loss did not improve from 42.17144
196/196 - 33s - loss: 41.0869 - MinusLogProbMetric: 41.0869 - val_loss: 42.2023 - val_MinusLogProbMetric: 42.2023 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 887/1000
2023-10-28 10:34:05.079 
Epoch 887/1000 
	 loss: 41.1066, MinusLogProbMetric: 41.1066, val_loss: 42.2087, val_MinusLogProbMetric: 42.2087

Epoch 887: val_loss did not improve from 42.17144
196/196 - 33s - loss: 41.1066 - MinusLogProbMetric: 41.1066 - val_loss: 42.2087 - val_MinusLogProbMetric: 42.2087 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 888/1000
2023-10-28 10:34:38.160 
Epoch 888/1000 
	 loss: 41.1025, MinusLogProbMetric: 41.1025, val_loss: 42.1616, val_MinusLogProbMetric: 42.1616

Epoch 888: val_loss improved from 42.17144 to 42.16159, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 41.1025 - MinusLogProbMetric: 41.1025 - val_loss: 42.1616 - val_MinusLogProbMetric: 42.1616 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 889/1000
2023-10-28 10:35:11.195 
Epoch 889/1000 
	 loss: 41.1077, MinusLogProbMetric: 41.1077, val_loss: 42.2788, val_MinusLogProbMetric: 42.2788

Epoch 889: val_loss did not improve from 42.16159
196/196 - 32s - loss: 41.1077 - MinusLogProbMetric: 41.1077 - val_loss: 42.2788 - val_MinusLogProbMetric: 42.2788 - lr: 4.1667e-05 - 32s/epoch - 165ms/step
Epoch 890/1000
2023-10-28 10:35:43.844 
Epoch 890/1000 
	 loss: 41.1080, MinusLogProbMetric: 41.1080, val_loss: 42.2139, val_MinusLogProbMetric: 42.2139

Epoch 890: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1080 - MinusLogProbMetric: 41.1080 - val_loss: 42.2139 - val_MinusLogProbMetric: 42.2139 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 891/1000
2023-10-28 10:36:17.284 
Epoch 891/1000 
	 loss: 41.1003, MinusLogProbMetric: 41.1003, val_loss: 42.2538, val_MinusLogProbMetric: 42.2538

Epoch 891: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1003 - MinusLogProbMetric: 41.1003 - val_loss: 42.2538 - val_MinusLogProbMetric: 42.2538 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 892/1000
2023-10-28 10:36:50.218 
Epoch 892/1000 
	 loss: 41.1029, MinusLogProbMetric: 41.1029, val_loss: 42.1904, val_MinusLogProbMetric: 42.1904

Epoch 892: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1029 - MinusLogProbMetric: 41.1029 - val_loss: 42.1904 - val_MinusLogProbMetric: 42.1904 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 893/1000
2023-10-28 10:37:22.859 
Epoch 893/1000 
	 loss: 41.1357, MinusLogProbMetric: 41.1357, val_loss: 42.2732, val_MinusLogProbMetric: 42.2732

Epoch 893: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1357 - MinusLogProbMetric: 41.1357 - val_loss: 42.2732 - val_MinusLogProbMetric: 42.2732 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 894/1000
2023-10-28 10:37:56.066 
Epoch 894/1000 
	 loss: 41.1070, MinusLogProbMetric: 41.1070, val_loss: 42.3465, val_MinusLogProbMetric: 42.3465

Epoch 894: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1070 - MinusLogProbMetric: 41.1070 - val_loss: 42.3465 - val_MinusLogProbMetric: 42.3465 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 895/1000
2023-10-28 10:38:29.372 
Epoch 895/1000 
	 loss: 41.1170, MinusLogProbMetric: 41.1170, val_loss: 42.4228, val_MinusLogProbMetric: 42.4228

Epoch 895: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1170 - MinusLogProbMetric: 41.1170 - val_loss: 42.4228 - val_MinusLogProbMetric: 42.4228 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 896/1000
2023-10-28 10:39:02.776 
Epoch 896/1000 
	 loss: 41.1137, MinusLogProbMetric: 41.1137, val_loss: 42.3782, val_MinusLogProbMetric: 42.3782

Epoch 896: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1137 - MinusLogProbMetric: 41.1137 - val_loss: 42.3782 - val_MinusLogProbMetric: 42.3782 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 897/1000
2023-10-28 10:39:36.357 
Epoch 897/1000 
	 loss: 41.1295, MinusLogProbMetric: 41.1295, val_loss: 42.2207, val_MinusLogProbMetric: 42.2207

Epoch 897: val_loss did not improve from 42.16159
196/196 - 34s - loss: 41.1295 - MinusLogProbMetric: 41.1295 - val_loss: 42.2207 - val_MinusLogProbMetric: 42.2207 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 898/1000
2023-10-28 10:40:09.996 
Epoch 898/1000 
	 loss: 41.1740, MinusLogProbMetric: 41.1740, val_loss: 42.2604, val_MinusLogProbMetric: 42.2604

Epoch 898: val_loss did not improve from 42.16159
196/196 - 34s - loss: 41.1740 - MinusLogProbMetric: 41.1740 - val_loss: 42.2604 - val_MinusLogProbMetric: 42.2604 - lr: 4.1667e-05 - 34s/epoch - 172ms/step
Epoch 899/1000
2023-10-28 10:40:43.363 
Epoch 899/1000 
	 loss: 41.1018, MinusLogProbMetric: 41.1018, val_loss: 42.4710, val_MinusLogProbMetric: 42.4710

Epoch 899: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1018 - MinusLogProbMetric: 41.1018 - val_loss: 42.4710 - val_MinusLogProbMetric: 42.4710 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 900/1000
2023-10-28 10:41:16.650 
Epoch 900/1000 
	 loss: 41.1021, MinusLogProbMetric: 41.1021, val_loss: 42.4059, val_MinusLogProbMetric: 42.4059

Epoch 900: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1021 - MinusLogProbMetric: 41.1021 - val_loss: 42.4059 - val_MinusLogProbMetric: 42.4059 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 901/1000
2023-10-28 10:41:50.162 
Epoch 901/1000 
	 loss: 41.0932, MinusLogProbMetric: 41.0932, val_loss: 42.4162, val_MinusLogProbMetric: 42.4162

Epoch 901: val_loss did not improve from 42.16159
196/196 - 34s - loss: 41.0932 - MinusLogProbMetric: 41.0932 - val_loss: 42.4162 - val_MinusLogProbMetric: 42.4162 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 902/1000
2023-10-28 10:42:23.408 
Epoch 902/1000 
	 loss: 41.1083, MinusLogProbMetric: 41.1083, val_loss: 42.2843, val_MinusLogProbMetric: 42.2843

Epoch 902: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1083 - MinusLogProbMetric: 41.1083 - val_loss: 42.2843 - val_MinusLogProbMetric: 42.2843 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 903/1000
2023-10-28 10:42:56.816 
Epoch 903/1000 
	 loss: 41.0865, MinusLogProbMetric: 41.0865, val_loss: 42.2893, val_MinusLogProbMetric: 42.2893

Epoch 903: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.0865 - MinusLogProbMetric: 41.0865 - val_loss: 42.2893 - val_MinusLogProbMetric: 42.2893 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 904/1000
2023-10-28 10:43:29.996 
Epoch 904/1000 
	 loss: 41.0867, MinusLogProbMetric: 41.0867, val_loss: 42.3312, val_MinusLogProbMetric: 42.3312

Epoch 904: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.0867 - MinusLogProbMetric: 41.0867 - val_loss: 42.3312 - val_MinusLogProbMetric: 42.3312 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 905/1000
2023-10-28 10:44:03.301 
Epoch 905/1000 
	 loss: 41.1060, MinusLogProbMetric: 41.1060, val_loss: 42.3217, val_MinusLogProbMetric: 42.3217

Epoch 905: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1060 - MinusLogProbMetric: 41.1060 - val_loss: 42.3217 - val_MinusLogProbMetric: 42.3217 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 906/1000
2023-10-28 10:44:36.701 
Epoch 906/1000 
	 loss: 41.0943, MinusLogProbMetric: 41.0943, val_loss: 42.3313, val_MinusLogProbMetric: 42.3313

Epoch 906: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.0943 - MinusLogProbMetric: 41.0943 - val_loss: 42.3313 - val_MinusLogProbMetric: 42.3313 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 907/1000
2023-10-28 10:45:10.260 
Epoch 907/1000 
	 loss: 41.1076, MinusLogProbMetric: 41.1076, val_loss: 42.1976, val_MinusLogProbMetric: 42.1976

Epoch 907: val_loss did not improve from 42.16159
196/196 - 34s - loss: 41.1076 - MinusLogProbMetric: 41.1076 - val_loss: 42.1976 - val_MinusLogProbMetric: 42.1976 - lr: 4.1667e-05 - 34s/epoch - 171ms/step
Epoch 908/1000
2023-10-28 10:45:43.702 
Epoch 908/1000 
	 loss: 41.1101, MinusLogProbMetric: 41.1101, val_loss: 42.2203, val_MinusLogProbMetric: 42.2203

Epoch 908: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1101 - MinusLogProbMetric: 41.1101 - val_loss: 42.2203 - val_MinusLogProbMetric: 42.2203 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 909/1000
2023-10-28 10:46:17.137 
Epoch 909/1000 
	 loss: 41.1062, MinusLogProbMetric: 41.1062, val_loss: 42.2963, val_MinusLogProbMetric: 42.2963

Epoch 909: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1062 - MinusLogProbMetric: 41.1062 - val_loss: 42.2963 - val_MinusLogProbMetric: 42.2963 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 910/1000
2023-10-28 10:46:50.216 
Epoch 910/1000 
	 loss: 41.1335, MinusLogProbMetric: 41.1335, val_loss: 42.3351, val_MinusLogProbMetric: 42.3351

Epoch 910: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1335 - MinusLogProbMetric: 41.1335 - val_loss: 42.3351 - val_MinusLogProbMetric: 42.3351 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 911/1000
2023-10-28 10:47:23.541 
Epoch 911/1000 
	 loss: 41.0855, MinusLogProbMetric: 41.0855, val_loss: 42.2126, val_MinusLogProbMetric: 42.2126

Epoch 911: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.0855 - MinusLogProbMetric: 41.0855 - val_loss: 42.2126 - val_MinusLogProbMetric: 42.2126 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 912/1000
2023-10-28 10:47:56.408 
Epoch 912/1000 
	 loss: 41.1211, MinusLogProbMetric: 41.1211, val_loss: 42.3955, val_MinusLogProbMetric: 42.3955

Epoch 912: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1211 - MinusLogProbMetric: 41.1211 - val_loss: 42.3955 - val_MinusLogProbMetric: 42.3955 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 913/1000
2023-10-28 10:48:29.474 
Epoch 913/1000 
	 loss: 41.1190, MinusLogProbMetric: 41.1190, val_loss: 42.2425, val_MinusLogProbMetric: 42.2425

Epoch 913: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1190 - MinusLogProbMetric: 41.1190 - val_loss: 42.2425 - val_MinusLogProbMetric: 42.2425 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 914/1000
2023-10-28 10:49:02.927 
Epoch 914/1000 
	 loss: 41.0916, MinusLogProbMetric: 41.0916, val_loss: 42.2539, val_MinusLogProbMetric: 42.2539

Epoch 914: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.0916 - MinusLogProbMetric: 41.0916 - val_loss: 42.2539 - val_MinusLogProbMetric: 42.2539 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 915/1000
2023-10-28 10:49:36.063 
Epoch 915/1000 
	 loss: 41.0969, MinusLogProbMetric: 41.0969, val_loss: 42.2143, val_MinusLogProbMetric: 42.2143

Epoch 915: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.0969 - MinusLogProbMetric: 41.0969 - val_loss: 42.2143 - val_MinusLogProbMetric: 42.2143 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 916/1000
2023-10-28 10:50:09.320 
Epoch 916/1000 
	 loss: 41.1001, MinusLogProbMetric: 41.1001, val_loss: 42.3862, val_MinusLogProbMetric: 42.3862

Epoch 916: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1001 - MinusLogProbMetric: 41.1001 - val_loss: 42.3862 - val_MinusLogProbMetric: 42.3862 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 917/1000
2023-10-28 10:50:42.550 
Epoch 917/1000 
	 loss: 41.1139, MinusLogProbMetric: 41.1139, val_loss: 42.5530, val_MinusLogProbMetric: 42.5530

Epoch 917: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1139 - MinusLogProbMetric: 41.1139 - val_loss: 42.5530 - val_MinusLogProbMetric: 42.5530 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 918/1000
2023-10-28 10:51:15.443 
Epoch 918/1000 
	 loss: 41.1422, MinusLogProbMetric: 41.1422, val_loss: 42.2794, val_MinusLogProbMetric: 42.2794

Epoch 918: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1422 - MinusLogProbMetric: 41.1422 - val_loss: 42.2794 - val_MinusLogProbMetric: 42.2794 - lr: 4.1667e-05 - 33s/epoch - 168ms/step
Epoch 919/1000
2023-10-28 10:51:48.918 
Epoch 919/1000 
	 loss: 41.1090, MinusLogProbMetric: 41.1090, val_loss: 42.2822, val_MinusLogProbMetric: 42.2822

Epoch 919: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1090 - MinusLogProbMetric: 41.1090 - val_loss: 42.2822 - val_MinusLogProbMetric: 42.2822 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 920/1000
2023-10-28 10:52:21.704 
Epoch 920/1000 
	 loss: 41.0821, MinusLogProbMetric: 41.0821, val_loss: 42.3952, val_MinusLogProbMetric: 42.3952

Epoch 920: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.0821 - MinusLogProbMetric: 41.0821 - val_loss: 42.3952 - val_MinusLogProbMetric: 42.3952 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 921/1000
2023-10-28 10:52:52.773 
Epoch 921/1000 
	 loss: 41.0916, MinusLogProbMetric: 41.0916, val_loss: 42.3297, val_MinusLogProbMetric: 42.3297

Epoch 921: val_loss did not improve from 42.16159
196/196 - 31s - loss: 41.0916 - MinusLogProbMetric: 41.0916 - val_loss: 42.3297 - val_MinusLogProbMetric: 42.3297 - lr: 4.1667e-05 - 31s/epoch - 159ms/step
Epoch 922/1000
2023-10-28 10:53:25.942 
Epoch 922/1000 
	 loss: 41.1634, MinusLogProbMetric: 41.1634, val_loss: 42.2335, val_MinusLogProbMetric: 42.2335

Epoch 922: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1634 - MinusLogProbMetric: 41.1634 - val_loss: 42.2335 - val_MinusLogProbMetric: 42.2335 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 923/1000
2023-10-28 10:53:59.195 
Epoch 923/1000 
	 loss: 41.0751, MinusLogProbMetric: 41.0751, val_loss: 42.3882, val_MinusLogProbMetric: 42.3882

Epoch 923: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.0751 - MinusLogProbMetric: 41.0751 - val_loss: 42.3882 - val_MinusLogProbMetric: 42.3882 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 924/1000
2023-10-28 10:54:32.590 
Epoch 924/1000 
	 loss: 41.0908, MinusLogProbMetric: 41.0908, val_loss: 42.4894, val_MinusLogProbMetric: 42.4894

Epoch 924: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.0908 - MinusLogProbMetric: 41.0908 - val_loss: 42.4894 - val_MinusLogProbMetric: 42.4894 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 925/1000
2023-10-28 10:55:05.778 
Epoch 925/1000 
	 loss: 41.1000, MinusLogProbMetric: 41.1000, val_loss: 42.2192, val_MinusLogProbMetric: 42.2192

Epoch 925: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1000 - MinusLogProbMetric: 41.1000 - val_loss: 42.2192 - val_MinusLogProbMetric: 42.2192 - lr: 4.1667e-05 - 33s/epoch - 169ms/step
Epoch 926/1000
2023-10-28 10:55:38.287 
Epoch 926/1000 
	 loss: 41.0944, MinusLogProbMetric: 41.0944, val_loss: 42.1715, val_MinusLogProbMetric: 42.1715

Epoch 926: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.0944 - MinusLogProbMetric: 41.0944 - val_loss: 42.1715 - val_MinusLogProbMetric: 42.1715 - lr: 4.1667e-05 - 33s/epoch - 166ms/step
Epoch 927/1000
2023-10-28 10:56:09.307 
Epoch 927/1000 
	 loss: 41.0960, MinusLogProbMetric: 41.0960, val_loss: 42.3228, val_MinusLogProbMetric: 42.3228

Epoch 927: val_loss did not improve from 42.16159
196/196 - 31s - loss: 41.0960 - MinusLogProbMetric: 41.0960 - val_loss: 42.3228 - val_MinusLogProbMetric: 42.3228 - lr: 4.1667e-05 - 31s/epoch - 158ms/step
Epoch 928/1000
2023-10-28 10:56:42.113 
Epoch 928/1000 
	 loss: 41.0717, MinusLogProbMetric: 41.0717, val_loss: 42.3241, val_MinusLogProbMetric: 42.3241

Epoch 928: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.0717 - MinusLogProbMetric: 41.0717 - val_loss: 42.3241 - val_MinusLogProbMetric: 42.3241 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 929/1000
2023-10-28 10:57:15.400 
Epoch 929/1000 
	 loss: 41.1065, MinusLogProbMetric: 41.1065, val_loss: 42.3661, val_MinusLogProbMetric: 42.3661

Epoch 929: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1065 - MinusLogProbMetric: 41.1065 - val_loss: 42.3661 - val_MinusLogProbMetric: 42.3661 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 930/1000
2023-10-28 10:57:48.676 
Epoch 930/1000 
	 loss: 41.1415, MinusLogProbMetric: 41.1415, val_loss: 42.3694, val_MinusLogProbMetric: 42.3694

Epoch 930: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1415 - MinusLogProbMetric: 41.1415 - val_loss: 42.3694 - val_MinusLogProbMetric: 42.3694 - lr: 4.1667e-05 - 33s/epoch - 170ms/step
Epoch 931/1000
2023-10-28 10:58:21.492 
Epoch 931/1000 
	 loss: 41.1185, MinusLogProbMetric: 41.1185, val_loss: 42.2239, val_MinusLogProbMetric: 42.2239

Epoch 931: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.1185 - MinusLogProbMetric: 41.1185 - val_loss: 42.2239 - val_MinusLogProbMetric: 42.2239 - lr: 4.1667e-05 - 33s/epoch - 167ms/step
Epoch 932/1000
2023-10-28 10:58:49.283 
Epoch 932/1000 
	 loss: 41.1221, MinusLogProbMetric: 41.1221, val_loss: 43.6114, val_MinusLogProbMetric: 43.6114

Epoch 932: val_loss did not improve from 42.16159
196/196 - 28s - loss: 41.1221 - MinusLogProbMetric: 41.1221 - val_loss: 43.6114 - val_MinusLogProbMetric: 43.6114 - lr: 4.1667e-05 - 28s/epoch - 142ms/step
Epoch 933/1000
2023-10-28 10:59:14.672 
Epoch 933/1000 
	 loss: 41.1331, MinusLogProbMetric: 41.1331, val_loss: 42.3936, val_MinusLogProbMetric: 42.3936

Epoch 933: val_loss did not improve from 42.16159
196/196 - 25s - loss: 41.1331 - MinusLogProbMetric: 41.1331 - val_loss: 42.3936 - val_MinusLogProbMetric: 42.3936 - lr: 4.1667e-05 - 25s/epoch - 130ms/step
Epoch 934/1000
2023-10-28 10:59:40.060 
Epoch 934/1000 
	 loss: 41.0789, MinusLogProbMetric: 41.0789, val_loss: 42.2685, val_MinusLogProbMetric: 42.2685

Epoch 934: val_loss did not improve from 42.16159
196/196 - 25s - loss: 41.0789 - MinusLogProbMetric: 41.0789 - val_loss: 42.2685 - val_MinusLogProbMetric: 42.2685 - lr: 4.1667e-05 - 25s/epoch - 130ms/step
Epoch 935/1000
2023-10-28 11:00:05.720 
Epoch 935/1000 
	 loss: 41.0710, MinusLogProbMetric: 41.0710, val_loss: 42.2340, val_MinusLogProbMetric: 42.2340

Epoch 935: val_loss did not improve from 42.16159
196/196 - 26s - loss: 41.0710 - MinusLogProbMetric: 41.0710 - val_loss: 42.2340 - val_MinusLogProbMetric: 42.2340 - lr: 4.1667e-05 - 26s/epoch - 131ms/step
Epoch 936/1000
2023-10-28 11:00:32.980 
Epoch 936/1000 
	 loss: 41.1196, MinusLogProbMetric: 41.1196, val_loss: 42.1869, val_MinusLogProbMetric: 42.1869

Epoch 936: val_loss did not improve from 42.16159
196/196 - 27s - loss: 41.1196 - MinusLogProbMetric: 41.1196 - val_loss: 42.1869 - val_MinusLogProbMetric: 42.1869 - lr: 4.1667e-05 - 27s/epoch - 139ms/step
Epoch 937/1000
2023-10-28 11:01:04.874 
Epoch 937/1000 
	 loss: 41.0520, MinusLogProbMetric: 41.0520, val_loss: 42.6550, val_MinusLogProbMetric: 42.6550

Epoch 937: val_loss did not improve from 42.16159
196/196 - 32s - loss: 41.0520 - MinusLogProbMetric: 41.0520 - val_loss: 42.6550 - val_MinusLogProbMetric: 42.6550 - lr: 4.1667e-05 - 32s/epoch - 163ms/step
Epoch 938/1000
2023-10-28 11:01:38.362 
Epoch 938/1000 
	 loss: 41.0943, MinusLogProbMetric: 41.0943, val_loss: 42.1969, val_MinusLogProbMetric: 42.1969

Epoch 938: val_loss did not improve from 42.16159
196/196 - 33s - loss: 41.0943 - MinusLogProbMetric: 41.0943 - val_loss: 42.1969 - val_MinusLogProbMetric: 42.1969 - lr: 4.1667e-05 - 33s/epoch - 171ms/step
Epoch 939/1000
2023-10-28 11:02:10.952 
Epoch 939/1000 
	 loss: 40.9677, MinusLogProbMetric: 40.9677, val_loss: 42.2142, val_MinusLogProbMetric: 42.2142

Epoch 939: val_loss did not improve from 42.16159
196/196 - 33s - loss: 40.9677 - MinusLogProbMetric: 40.9677 - val_loss: 42.2142 - val_MinusLogProbMetric: 42.2142 - lr: 2.0833e-05 - 33s/epoch - 166ms/step
Epoch 940/1000
2023-10-28 11:02:44.589 
Epoch 940/1000 
	 loss: 40.9714, MinusLogProbMetric: 40.9714, val_loss: 42.1741, val_MinusLogProbMetric: 42.1741

Epoch 940: val_loss did not improve from 42.16159
196/196 - 34s - loss: 40.9714 - MinusLogProbMetric: 40.9714 - val_loss: 42.1741 - val_MinusLogProbMetric: 42.1741 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 941/1000
2023-10-28 11:03:17.946 
Epoch 941/1000 
	 loss: 40.9690, MinusLogProbMetric: 40.9690, val_loss: 42.2734, val_MinusLogProbMetric: 42.2734

Epoch 941: val_loss did not improve from 42.16159
196/196 - 33s - loss: 40.9690 - MinusLogProbMetric: 40.9690 - val_loss: 42.2734 - val_MinusLogProbMetric: 42.2734 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 942/1000
2023-10-28 11:03:51.344 
Epoch 942/1000 
	 loss: 40.9787, MinusLogProbMetric: 40.9787, val_loss: 42.2019, val_MinusLogProbMetric: 42.2019

Epoch 942: val_loss did not improve from 42.16159
196/196 - 33s - loss: 40.9787 - MinusLogProbMetric: 40.9787 - val_loss: 42.2019 - val_MinusLogProbMetric: 42.2019 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 943/1000
2023-10-28 11:04:24.251 
Epoch 943/1000 
	 loss: 40.9859, MinusLogProbMetric: 40.9859, val_loss: 42.2065, val_MinusLogProbMetric: 42.2065

Epoch 943: val_loss did not improve from 42.16159
196/196 - 33s - loss: 40.9859 - MinusLogProbMetric: 40.9859 - val_loss: 42.2065 - val_MinusLogProbMetric: 42.2065 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 944/1000
2023-10-28 11:04:57.800 
Epoch 944/1000 
	 loss: 40.9697, MinusLogProbMetric: 40.9697, val_loss: 42.1653, val_MinusLogProbMetric: 42.1653

Epoch 944: val_loss did not improve from 42.16159
196/196 - 34s - loss: 40.9697 - MinusLogProbMetric: 40.9697 - val_loss: 42.1653 - val_MinusLogProbMetric: 42.1653 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 945/1000
2023-10-28 11:05:30.742 
Epoch 945/1000 
	 loss: 40.9720, MinusLogProbMetric: 40.9720, val_loss: 42.1455, val_MinusLogProbMetric: 42.1455

Epoch 945: val_loss improved from 42.16159 to 42.14553, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 33s - loss: 40.9720 - MinusLogProbMetric: 40.9720 - val_loss: 42.1455 - val_MinusLogProbMetric: 42.1455 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 946/1000
2023-10-28 11:06:04.539 
Epoch 946/1000 
	 loss: 40.9659, MinusLogProbMetric: 40.9659, val_loss: 42.2176, val_MinusLogProbMetric: 42.2176

Epoch 946: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9659 - MinusLogProbMetric: 40.9659 - val_loss: 42.2176 - val_MinusLogProbMetric: 42.2176 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 947/1000
2023-10-28 11:06:37.604 
Epoch 947/1000 
	 loss: 40.9871, MinusLogProbMetric: 40.9871, val_loss: 42.1884, val_MinusLogProbMetric: 42.1884

Epoch 947: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9871 - MinusLogProbMetric: 40.9871 - val_loss: 42.1884 - val_MinusLogProbMetric: 42.1884 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 948/1000
2023-10-28 11:07:11.133 
Epoch 948/1000 
	 loss: 40.9678, MinusLogProbMetric: 40.9678, val_loss: 42.4711, val_MinusLogProbMetric: 42.4711

Epoch 948: val_loss did not improve from 42.14553
196/196 - 34s - loss: 40.9678 - MinusLogProbMetric: 40.9678 - val_loss: 42.4711 - val_MinusLogProbMetric: 42.4711 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 949/1000
2023-10-28 11:07:44.730 
Epoch 949/1000 
	 loss: 40.9761, MinusLogProbMetric: 40.9761, val_loss: 42.3307, val_MinusLogProbMetric: 42.3307

Epoch 949: val_loss did not improve from 42.14553
196/196 - 34s - loss: 40.9761 - MinusLogProbMetric: 40.9761 - val_loss: 42.3307 - val_MinusLogProbMetric: 42.3307 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 950/1000
2023-10-28 11:08:18.078 
Epoch 950/1000 
	 loss: 40.9631, MinusLogProbMetric: 40.9631, val_loss: 42.2428, val_MinusLogProbMetric: 42.2428

Epoch 950: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9631 - MinusLogProbMetric: 40.9631 - val_loss: 42.2428 - val_MinusLogProbMetric: 42.2428 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 951/1000
2023-10-28 11:08:51.576 
Epoch 951/1000 
	 loss: 40.9692, MinusLogProbMetric: 40.9692, val_loss: 42.2145, val_MinusLogProbMetric: 42.2145

Epoch 951: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9692 - MinusLogProbMetric: 40.9692 - val_loss: 42.2145 - val_MinusLogProbMetric: 42.2145 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 952/1000
2023-10-28 11:09:24.900 
Epoch 952/1000 
	 loss: 40.9685, MinusLogProbMetric: 40.9685, val_loss: 42.1859, val_MinusLogProbMetric: 42.1859

Epoch 952: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9685 - MinusLogProbMetric: 40.9685 - val_loss: 42.1859 - val_MinusLogProbMetric: 42.1859 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 953/1000
2023-10-28 11:09:58.460 
Epoch 953/1000 
	 loss: 40.9618, MinusLogProbMetric: 40.9618, val_loss: 42.1828, val_MinusLogProbMetric: 42.1828

Epoch 953: val_loss did not improve from 42.14553
196/196 - 34s - loss: 40.9618 - MinusLogProbMetric: 40.9618 - val_loss: 42.1828 - val_MinusLogProbMetric: 42.1828 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 954/1000
2023-10-28 11:10:29.193 
Epoch 954/1000 
	 loss: 40.9705, MinusLogProbMetric: 40.9705, val_loss: 42.1476, val_MinusLogProbMetric: 42.1476

Epoch 954: val_loss did not improve from 42.14553
196/196 - 31s - loss: 40.9705 - MinusLogProbMetric: 40.9705 - val_loss: 42.1476 - val_MinusLogProbMetric: 42.1476 - lr: 2.0833e-05 - 31s/epoch - 157ms/step
Epoch 955/1000
2023-10-28 11:11:01.854 
Epoch 955/1000 
	 loss: 40.9703, MinusLogProbMetric: 40.9703, val_loss: 42.3094, val_MinusLogProbMetric: 42.3094

Epoch 955: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9703 - MinusLogProbMetric: 40.9703 - val_loss: 42.3094 - val_MinusLogProbMetric: 42.3094 - lr: 2.0833e-05 - 33s/epoch - 167ms/step
Epoch 956/1000
2023-10-28 11:11:35.257 
Epoch 956/1000 
	 loss: 40.9770, MinusLogProbMetric: 40.9770, val_loss: 42.2572, val_MinusLogProbMetric: 42.2572

Epoch 956: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9770 - MinusLogProbMetric: 40.9770 - val_loss: 42.2572 - val_MinusLogProbMetric: 42.2572 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 957/1000
2023-10-28 11:12:08.757 
Epoch 957/1000 
	 loss: 40.9678, MinusLogProbMetric: 40.9678, val_loss: 42.2249, val_MinusLogProbMetric: 42.2249

Epoch 957: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9678 - MinusLogProbMetric: 40.9678 - val_loss: 42.2249 - val_MinusLogProbMetric: 42.2249 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 958/1000
2023-10-28 11:12:42.010 
Epoch 958/1000 
	 loss: 40.9577, MinusLogProbMetric: 40.9577, val_loss: 42.2027, val_MinusLogProbMetric: 42.2027

Epoch 958: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9577 - MinusLogProbMetric: 40.9577 - val_loss: 42.2027 - val_MinusLogProbMetric: 42.2027 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 959/1000
2023-10-28 11:13:15.682 
Epoch 959/1000 
	 loss: 40.9639, MinusLogProbMetric: 40.9639, val_loss: 42.1598, val_MinusLogProbMetric: 42.1598

Epoch 959: val_loss did not improve from 42.14553
196/196 - 34s - loss: 40.9639 - MinusLogProbMetric: 40.9639 - val_loss: 42.1598 - val_MinusLogProbMetric: 42.1598 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 960/1000
2023-10-28 11:13:50.343 
Epoch 960/1000 
	 loss: 40.9650, MinusLogProbMetric: 40.9650, val_loss: 42.2079, val_MinusLogProbMetric: 42.2079

Epoch 960: val_loss did not improve from 42.14553
196/196 - 35s - loss: 40.9650 - MinusLogProbMetric: 40.9650 - val_loss: 42.2079 - val_MinusLogProbMetric: 42.2079 - lr: 2.0833e-05 - 35s/epoch - 177ms/step
Epoch 961/1000
2023-10-28 11:14:23.726 
Epoch 961/1000 
	 loss: 40.9662, MinusLogProbMetric: 40.9662, val_loss: 42.3560, val_MinusLogProbMetric: 42.3560

Epoch 961: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9662 - MinusLogProbMetric: 40.9662 - val_loss: 42.3560 - val_MinusLogProbMetric: 42.3560 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 962/1000
2023-10-28 11:14:56.977 
Epoch 962/1000 
	 loss: 40.9751, MinusLogProbMetric: 40.9751, val_loss: 42.1523, val_MinusLogProbMetric: 42.1523

Epoch 962: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9751 - MinusLogProbMetric: 40.9751 - val_loss: 42.1523 - val_MinusLogProbMetric: 42.1523 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 963/1000
2023-10-28 11:15:30.337 
Epoch 963/1000 
	 loss: 40.9533, MinusLogProbMetric: 40.9533, val_loss: 42.2434, val_MinusLogProbMetric: 42.2434

Epoch 963: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9533 - MinusLogProbMetric: 40.9533 - val_loss: 42.2434 - val_MinusLogProbMetric: 42.2434 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 964/1000
2023-10-28 11:16:03.831 
Epoch 964/1000 
	 loss: 40.9644, MinusLogProbMetric: 40.9644, val_loss: 42.2508, val_MinusLogProbMetric: 42.2508

Epoch 964: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9644 - MinusLogProbMetric: 40.9644 - val_loss: 42.2508 - val_MinusLogProbMetric: 42.2508 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 965/1000
2023-10-28 11:16:37.241 
Epoch 965/1000 
	 loss: 40.9510, MinusLogProbMetric: 40.9510, val_loss: 42.2126, val_MinusLogProbMetric: 42.2126

Epoch 965: val_loss did not improve from 42.14553
196/196 - 33s - loss: 40.9510 - MinusLogProbMetric: 40.9510 - val_loss: 42.2126 - val_MinusLogProbMetric: 42.2126 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 966/1000
2023-10-28 11:17:10.663 
Epoch 966/1000 
	 loss: 40.9665, MinusLogProbMetric: 40.9665, val_loss: 42.1415, val_MinusLogProbMetric: 42.1415

Epoch 966: val_loss improved from 42.14553 to 42.14149, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 40.9665 - MinusLogProbMetric: 40.9665 - val_loss: 42.1415 - val_MinusLogProbMetric: 42.1415 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 967/1000
2023-10-28 11:17:44.559 
Epoch 967/1000 
	 loss: 40.9556, MinusLogProbMetric: 40.9556, val_loss: 42.2310, val_MinusLogProbMetric: 42.2310

Epoch 967: val_loss did not improve from 42.14149
196/196 - 33s - loss: 40.9556 - MinusLogProbMetric: 40.9556 - val_loss: 42.2310 - val_MinusLogProbMetric: 42.2310 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 968/1000
2023-10-28 11:18:17.739 
Epoch 968/1000 
	 loss: 40.9659, MinusLogProbMetric: 40.9659, val_loss: 42.2379, val_MinusLogProbMetric: 42.2379

Epoch 968: val_loss did not improve from 42.14149
196/196 - 33s - loss: 40.9659 - MinusLogProbMetric: 40.9659 - val_loss: 42.2379 - val_MinusLogProbMetric: 42.2379 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 969/1000
2023-10-28 11:18:51.053 
Epoch 969/1000 
	 loss: 40.9624, MinusLogProbMetric: 40.9624, val_loss: 42.1818, val_MinusLogProbMetric: 42.1818

Epoch 969: val_loss did not improve from 42.14149
196/196 - 33s - loss: 40.9624 - MinusLogProbMetric: 40.9624 - val_loss: 42.1818 - val_MinusLogProbMetric: 42.1818 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 970/1000
2023-10-28 11:19:24.144 
Epoch 970/1000 
	 loss: 40.9576, MinusLogProbMetric: 40.9576, val_loss: 42.3485, val_MinusLogProbMetric: 42.3485

Epoch 970: val_loss did not improve from 42.14149
196/196 - 33s - loss: 40.9576 - MinusLogProbMetric: 40.9576 - val_loss: 42.3485 - val_MinusLogProbMetric: 42.3485 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 971/1000
2023-10-28 11:19:57.273 
Epoch 971/1000 
	 loss: 40.9556, MinusLogProbMetric: 40.9556, val_loss: 42.1573, val_MinusLogProbMetric: 42.1573

Epoch 971: val_loss did not improve from 42.14149
196/196 - 33s - loss: 40.9556 - MinusLogProbMetric: 40.9556 - val_loss: 42.1573 - val_MinusLogProbMetric: 42.1573 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 972/1000
2023-10-28 11:20:30.568 
Epoch 972/1000 
	 loss: 40.9485, MinusLogProbMetric: 40.9485, val_loss: 42.2625, val_MinusLogProbMetric: 42.2625

Epoch 972: val_loss did not improve from 42.14149
196/196 - 33s - loss: 40.9485 - MinusLogProbMetric: 40.9485 - val_loss: 42.2625 - val_MinusLogProbMetric: 42.2625 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 973/1000
2023-10-28 11:21:03.013 
Epoch 973/1000 
	 loss: 40.9954, MinusLogProbMetric: 40.9954, val_loss: 42.1884, val_MinusLogProbMetric: 42.1884

Epoch 973: val_loss did not improve from 42.14149
196/196 - 32s - loss: 40.9954 - MinusLogProbMetric: 40.9954 - val_loss: 42.1884 - val_MinusLogProbMetric: 42.1884 - lr: 2.0833e-05 - 32s/epoch - 166ms/step
Epoch 974/1000
2023-10-28 11:21:36.086 
Epoch 974/1000 
	 loss: 40.9577, MinusLogProbMetric: 40.9577, val_loss: 42.2475, val_MinusLogProbMetric: 42.2475

Epoch 974: val_loss did not improve from 42.14149
196/196 - 33s - loss: 40.9577 - MinusLogProbMetric: 40.9577 - val_loss: 42.2475 - val_MinusLogProbMetric: 42.2475 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 975/1000
2023-10-28 11:22:09.086 
Epoch 975/1000 
	 loss: 40.9473, MinusLogProbMetric: 40.9473, val_loss: 42.2422, val_MinusLogProbMetric: 42.2422

Epoch 975: val_loss did not improve from 42.14149
196/196 - 33s - loss: 40.9473 - MinusLogProbMetric: 40.9473 - val_loss: 42.2422 - val_MinusLogProbMetric: 42.2422 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 976/1000
2023-10-28 11:22:42.599 
Epoch 976/1000 
	 loss: 40.9708, MinusLogProbMetric: 40.9708, val_loss: 42.2908, val_MinusLogProbMetric: 42.2908

Epoch 976: val_loss did not improve from 42.14149
196/196 - 34s - loss: 40.9708 - MinusLogProbMetric: 40.9708 - val_loss: 42.2908 - val_MinusLogProbMetric: 42.2908 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 977/1000
2023-10-28 11:23:16.932 
Epoch 977/1000 
	 loss: 40.9662, MinusLogProbMetric: 40.9662, val_loss: 42.1943, val_MinusLogProbMetric: 42.1943

Epoch 977: val_loss did not improve from 42.14149
196/196 - 34s - loss: 40.9662 - MinusLogProbMetric: 40.9662 - val_loss: 42.1943 - val_MinusLogProbMetric: 42.1943 - lr: 2.0833e-05 - 34s/epoch - 175ms/step
Epoch 978/1000
2023-10-28 11:23:50.725 
Epoch 978/1000 
	 loss: 40.9418, MinusLogProbMetric: 40.9418, val_loss: 42.2253, val_MinusLogProbMetric: 42.2253

Epoch 978: val_loss did not improve from 42.14149
196/196 - 34s - loss: 40.9418 - MinusLogProbMetric: 40.9418 - val_loss: 42.2253 - val_MinusLogProbMetric: 42.2253 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 979/1000
2023-10-28 11:24:24.255 
Epoch 979/1000 
	 loss: 40.9572, MinusLogProbMetric: 40.9572, val_loss: 42.1525, val_MinusLogProbMetric: 42.1525

Epoch 979: val_loss did not improve from 42.14149
196/196 - 34s - loss: 40.9572 - MinusLogProbMetric: 40.9572 - val_loss: 42.1525 - val_MinusLogProbMetric: 42.1525 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 980/1000
2023-10-28 11:24:57.593 
Epoch 980/1000 
	 loss: 40.9599, MinusLogProbMetric: 40.9599, val_loss: 42.1516, val_MinusLogProbMetric: 42.1516

Epoch 980: val_loss did not improve from 42.14149
196/196 - 33s - loss: 40.9599 - MinusLogProbMetric: 40.9599 - val_loss: 42.1516 - val_MinusLogProbMetric: 42.1516 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 981/1000
2023-10-28 11:25:30.984 
Epoch 981/1000 
	 loss: 40.9662, MinusLogProbMetric: 40.9662, val_loss: 42.2320, val_MinusLogProbMetric: 42.2320

Epoch 981: val_loss did not improve from 42.14149
196/196 - 33s - loss: 40.9662 - MinusLogProbMetric: 40.9662 - val_loss: 42.2320 - val_MinusLogProbMetric: 42.2320 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 982/1000
2023-10-28 11:26:04.598 
Epoch 982/1000 
	 loss: 40.9514, MinusLogProbMetric: 40.9514, val_loss: 42.2438, val_MinusLogProbMetric: 42.2438

Epoch 982: val_loss did not improve from 42.14149
196/196 - 34s - loss: 40.9514 - MinusLogProbMetric: 40.9514 - val_loss: 42.2438 - val_MinusLogProbMetric: 42.2438 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 983/1000
2023-10-28 11:26:38.103 
Epoch 983/1000 
	 loss: 40.9576, MinusLogProbMetric: 40.9576, val_loss: 42.1370, val_MinusLogProbMetric: 42.1370

Epoch 983: val_loss improved from 42.14149 to 42.13696, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 40.9576 - MinusLogProbMetric: 40.9576 - val_loss: 42.1370 - val_MinusLogProbMetric: 42.1370 - lr: 2.0833e-05 - 34s/epoch - 174ms/step
Epoch 984/1000
2023-10-28 11:27:12.335 
Epoch 984/1000 
	 loss: 40.9522, MinusLogProbMetric: 40.9522, val_loss: 42.2441, val_MinusLogProbMetric: 42.2441

Epoch 984: val_loss did not improve from 42.13696
196/196 - 34s - loss: 40.9522 - MinusLogProbMetric: 40.9522 - val_loss: 42.2441 - val_MinusLogProbMetric: 42.2441 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Epoch 985/1000
2023-10-28 11:27:45.888 
Epoch 985/1000 
	 loss: 40.9676, MinusLogProbMetric: 40.9676, val_loss: 42.2263, val_MinusLogProbMetric: 42.2263

Epoch 985: val_loss did not improve from 42.13696
196/196 - 34s - loss: 40.9676 - MinusLogProbMetric: 40.9676 - val_loss: 42.2263 - val_MinusLogProbMetric: 42.2263 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 986/1000
2023-10-28 11:28:19.080 
Epoch 986/1000 
	 loss: 40.9544, MinusLogProbMetric: 40.9544, val_loss: 42.1531, val_MinusLogProbMetric: 42.1531

Epoch 986: val_loss did not improve from 42.13696
196/196 - 33s - loss: 40.9544 - MinusLogProbMetric: 40.9544 - val_loss: 42.1531 - val_MinusLogProbMetric: 42.1531 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 987/1000
2023-10-28 11:28:52.284 
Epoch 987/1000 
	 loss: 40.9430, MinusLogProbMetric: 40.9430, val_loss: 42.1833, val_MinusLogProbMetric: 42.1833

Epoch 987: val_loss did not improve from 42.13696
196/196 - 33s - loss: 40.9430 - MinusLogProbMetric: 40.9430 - val_loss: 42.1833 - val_MinusLogProbMetric: 42.1833 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 988/1000
2023-10-28 11:29:25.804 
Epoch 988/1000 
	 loss: 40.9369, MinusLogProbMetric: 40.9369, val_loss: 42.2139, val_MinusLogProbMetric: 42.2139

Epoch 988: val_loss did not improve from 42.13696
196/196 - 34s - loss: 40.9369 - MinusLogProbMetric: 40.9369 - val_loss: 42.2139 - val_MinusLogProbMetric: 42.2139 - lr: 2.0833e-05 - 34s/epoch - 171ms/step
Epoch 989/1000
2023-10-28 11:29:59.174 
Epoch 989/1000 
	 loss: 40.9612, MinusLogProbMetric: 40.9612, val_loss: 42.1247, val_MinusLogProbMetric: 42.1247

Epoch 989: val_loss improved from 42.13696 to 42.12475, saving model to /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_434/weights/best_weights.h5
196/196 - 34s - loss: 40.9612 - MinusLogProbMetric: 40.9612 - val_loss: 42.1247 - val_MinusLogProbMetric: 42.1247 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 990/1000
2023-10-28 11:30:32.869 
Epoch 990/1000 
	 loss: 40.9650, MinusLogProbMetric: 40.9650, val_loss: 42.4731, val_MinusLogProbMetric: 42.4731

Epoch 990: val_loss did not improve from 42.12475
196/196 - 33s - loss: 40.9650 - MinusLogProbMetric: 40.9650 - val_loss: 42.4731 - val_MinusLogProbMetric: 42.4731 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 991/1000
2023-10-28 11:31:05.912 
Epoch 991/1000 
	 loss: 40.9789, MinusLogProbMetric: 40.9789, val_loss: 42.2768, val_MinusLogProbMetric: 42.2768

Epoch 991: val_loss did not improve from 42.12475
196/196 - 33s - loss: 40.9789 - MinusLogProbMetric: 40.9789 - val_loss: 42.2768 - val_MinusLogProbMetric: 42.2768 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 992/1000
2023-10-28 11:31:39.753 
Epoch 992/1000 
	 loss: 40.9465, MinusLogProbMetric: 40.9465, val_loss: 42.2379, val_MinusLogProbMetric: 42.2379

Epoch 992: val_loss did not improve from 42.12475
196/196 - 34s - loss: 40.9465 - MinusLogProbMetric: 40.9465 - val_loss: 42.2379 - val_MinusLogProbMetric: 42.2379 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 993/1000
2023-10-28 11:32:13.165 
Epoch 993/1000 
	 loss: 40.9466, MinusLogProbMetric: 40.9466, val_loss: 42.2180, val_MinusLogProbMetric: 42.2180

Epoch 993: val_loss did not improve from 42.12475
196/196 - 33s - loss: 40.9466 - MinusLogProbMetric: 40.9466 - val_loss: 42.2180 - val_MinusLogProbMetric: 42.2180 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 994/1000
2023-10-28 11:32:46.062 
Epoch 994/1000 
	 loss: 40.9504, MinusLogProbMetric: 40.9504, val_loss: 42.3183, val_MinusLogProbMetric: 42.3183

Epoch 994: val_loss did not improve from 42.12475
196/196 - 33s - loss: 40.9504 - MinusLogProbMetric: 40.9504 - val_loss: 42.3183 - val_MinusLogProbMetric: 42.3183 - lr: 2.0833e-05 - 33s/epoch - 168ms/step
Epoch 995/1000
2023-10-28 11:33:19.915 
Epoch 995/1000 
	 loss: 40.9612, MinusLogProbMetric: 40.9612, val_loss: 42.2895, val_MinusLogProbMetric: 42.2895

Epoch 995: val_loss did not improve from 42.12475
196/196 - 34s - loss: 40.9612 - MinusLogProbMetric: 40.9612 - val_loss: 42.2895 - val_MinusLogProbMetric: 42.2895 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 996/1000
2023-10-28 11:33:53.824 
Epoch 996/1000 
	 loss: 40.9567, MinusLogProbMetric: 40.9567, val_loss: 42.1839, val_MinusLogProbMetric: 42.1839

Epoch 996: val_loss did not improve from 42.12475
196/196 - 34s - loss: 40.9567 - MinusLogProbMetric: 40.9567 - val_loss: 42.1839 - val_MinusLogProbMetric: 42.1839 - lr: 2.0833e-05 - 34s/epoch - 173ms/step
Epoch 997/1000
2023-10-28 11:34:27.280 
Epoch 997/1000 
	 loss: 40.9386, MinusLogProbMetric: 40.9386, val_loss: 42.1891, val_MinusLogProbMetric: 42.1891

Epoch 997: val_loss did not improve from 42.12475
196/196 - 33s - loss: 40.9386 - MinusLogProbMetric: 40.9386 - val_loss: 42.1891 - val_MinusLogProbMetric: 42.1891 - lr: 2.0833e-05 - 33s/epoch - 171ms/step
Epoch 998/1000
2023-10-28 11:35:00.406 
Epoch 998/1000 
	 loss: 40.9651, MinusLogProbMetric: 40.9651, val_loss: 42.2298, val_MinusLogProbMetric: 42.2298

Epoch 998: val_loss did not improve from 42.12475
196/196 - 33s - loss: 40.9651 - MinusLogProbMetric: 40.9651 - val_loss: 42.2298 - val_MinusLogProbMetric: 42.2298 - lr: 2.0833e-05 - 33s/epoch - 169ms/step
Epoch 999/1000
2023-10-28 11:35:33.792 
Epoch 999/1000 
	 loss: 40.9354, MinusLogProbMetric: 40.9354, val_loss: 42.1285, val_MinusLogProbMetric: 42.1285

Epoch 999: val_loss did not improve from 42.12475
196/196 - 33s - loss: 40.9354 - MinusLogProbMetric: 40.9354 - val_loss: 42.1285 - val_MinusLogProbMetric: 42.1285 - lr: 2.0833e-05 - 33s/epoch - 170ms/step
Epoch 1000/1000
2023-10-28 11:36:07.597 
Epoch 1000/1000 
	 loss: 40.9691, MinusLogProbMetric: 40.9691, val_loss: 42.1443, val_MinusLogProbMetric: 42.1443

Epoch 1000: val_loss did not improve from 42.12475
196/196 - 34s - loss: 40.9691 - MinusLogProbMetric: 40.9691 - val_loss: 42.1443 - val_MinusLogProbMetric: 42.1443 - lr: 2.0833e-05 - 34s/epoch - 172ms/step
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Parsing input distribution...
Input distribution is a tfp.distributions.Distribution object.
Training succeeded with seed 520.
Model trained in 33140.96 s.

===========
Computing predictions
===========

Computing metrics...
Checking and setting numerical distributions.
Resetting dist_num.
Resetting dist_num.
Metrics computed in 0.66 s.
                ===========
                print("===========
Failed to plot
===========
")
                Exception type: ValueError
                Exception message: Number of rows must be a positive integer, not 0
                Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 481, Func.Name : prediction_function, Message : Plotters.cornerplotter(X_data_test.numpy(),X_data_nf.numpy(),path_to_results,ndims,norm=True) # type: ignore', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/Plotters.py , Line : 156, Func.Name : cornerplotter, Message : figure=corner.corner(target_samples,color=\'red\',bins=n_bins,labels=[r"%s" % s for s in labels],normalize1d=True)', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/../../../code/corner.py , Line : 235, Func.Name : corner, Message : fig, axes = pl.subplots(K, K, figsize=(dim, dim))', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/pyplot.py , Line : 1502, Func.Name : subplots, Message : axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 905, Func.Name : subplots, Message : gs = self.add_gridspec(nrows, ncols, figure=self, **gridspec_kw)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/figure.py , Line : 1527, Func.Name : add_gridspec, Message : gs = GridSpec(nrows=nrows, ncols=ncols, figure=self, **kwargs)', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 379, Func.Name : __init__, Message : super().__init__(nrows, ncols,', 'File : /local_data/scratch/rtorre/anaconda3/envs/tf2_12/lib/python3.10/site-packages/matplotlib/gridspec.py , Line : 49, Func.Name : __init__, Message : raise ValueError(']
                ===========

results.txt saved
results.json saved
Results log saved
Model predictions computed in 0.89 s.
===========
Run 434/720 done in 33204.65 s.
===========

Directory ../../results/CsplineN_new/run_435/ already exists.
Skipping it.
===========
Run 435/720 already exists. Skipping it.
===========

Directory ../../results/CsplineN_new/run_436/ already exists.
Skipping it.
===========
Run 436/720 already exists. Skipping it.
===========

===========
Generating train data for run 437.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_437
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_112"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_113 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_17 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_17/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_17'")
self.model: <keras.engine.functional.Functional object at 0x7f76c06d11e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f750c21d840>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f750c21d840>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7520195bd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73f1c87250>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73f1c87cd0>, <keras.callbacks.ModelCheckpoint object at 0x7f73f1c87f10>, <keras.callbacks.EarlyStopping object at 0x7f73f1c85630>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73f1c84610>, <keras.callbacks.TerminateOnNaN object at 0x7f73f1c879d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_437/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 437/720 with hyperparameters:
timestamp = 2023-10-28 11:36:15.842429
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 11:38:03.750 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12321.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 108s - loss: nan - MinusLogProbMetric: 12321.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 108s/epoch - 551ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 437.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_437
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_123"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_124 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_18 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_18/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_18'")
self.model: <keras.engine.functional.Functional object at 0x7f73abeb3b80>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f73f0a57bb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f73f0a57bb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73a9676650>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73abecdea0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73abece410>, <keras.callbacks.ModelCheckpoint object at 0x7f73abece4d0>, <keras.callbacks.EarlyStopping object at 0x7f73abece740>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73abece770>, <keras.callbacks.TerminateOnNaN object at 0x7f73abece3b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_437/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 437/720 with hyperparameters:
timestamp = 2023-10-28 11:38:10.797361
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 11:39:59.743 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12321.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 109s - loss: nan - MinusLogProbMetric: 12321.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 109s/epoch - 556ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 437.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_437
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_134"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_135 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_19 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_19/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_19'")
self.model: <keras.engine.functional.Functional object at 0x7f72501d7a30>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f73f1f6feb0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f73f1f6feb0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73b12b5d50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f72dc217eb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7400f18460>, <keras.callbacks.ModelCheckpoint object at 0x7f7400f18520>, <keras.callbacks.EarlyStopping object at 0x7f7400f18790>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7400f187c0>, <keras.callbacks.TerminateOnNaN object at 0x7f7400f18400>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_437/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 437/720 with hyperparameters:
timestamp = 2023-10-28 11:40:06.458077
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 11:41:53.735 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12321.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 107s - loss: nan - MinusLogProbMetric: 12321.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 107s/epoch - 547ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 437.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_437
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_145"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_146 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_20 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_20/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_20'")
self.model: <keras.engine.functional.Functional object at 0x7f73b33d3d60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f73b338c910>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f73b338c910>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73aa9a5ba0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73aa9f8c40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73aa9f91b0>, <keras.callbacks.ModelCheckpoint object at 0x7f73aa9f9270>, <keras.callbacks.EarlyStopping object at 0x7f73aa9f94e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73aa9f9510>, <keras.callbacks.TerminateOnNaN object at 0x7f73aa9f9150>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_437/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 437/720 with hyperparameters:
timestamp = 2023-10-28 11:42:02.152169
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 11:43:52.218 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12321.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 110s - loss: nan - MinusLogProbMetric: 12321.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 110s/epoch - 561ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 437.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_437
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_156"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_157 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_21 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_21/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_21'")
self.model: <keras.engine.functional.Functional object at 0x7f73aafaf970>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f76c02578b0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f76c02578b0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73b0f676d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73a9e7e050>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73a9e7e5c0>, <keras.callbacks.ModelCheckpoint object at 0x7f73a9e7e680>, <keras.callbacks.EarlyStopping object at 0x7f73a9e7e8f0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73a9e7e920>, <keras.callbacks.TerminateOnNaN object at 0x7f73a9e7e560>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_437/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 437/720 with hyperparameters:
timestamp = 2023-10-28 11:43:59.158397
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 11:45:48.800 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12321.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 110s - loss: nan - MinusLogProbMetric: 12321.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 110s/epoch - 559ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 437.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_437
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_167"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_168 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_22 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_22/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_22'")
self.model: <keras.engine.functional.Functional object at 0x7f7145143160>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7144ed9450>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7144ed9450>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f76c023a560>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7145163190>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7145163700>, <keras.callbacks.ModelCheckpoint object at 0x7f71451637c0>, <keras.callbacks.EarlyStopping object at 0x7f7145163a30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7145163a60>, <keras.callbacks.TerminateOnNaN object at 0x7f71451636a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_437/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 437/720 with hyperparameters:
timestamp = 2023-10-28 11:45:55.423244
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 11:47:44.301 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12321.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 109s - loss: nan - MinusLogProbMetric: 12321.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 109s/epoch - 555ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 437.
===========
Train data generated in 0.27 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_437
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_178"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_179 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_23 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_23/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_23'")
self.model: <keras.engine.functional.Functional object at 0x7f73a9fcfa60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f73f1f57f40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f73f1f57f40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f75ded52650>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73ab6bb970>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73ab6b8b50>, <keras.callbacks.ModelCheckpoint object at 0x7f73ab6bbbb0>, <keras.callbacks.EarlyStopping object at 0x7f73ab6b9000>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73ab6bbf10>, <keras.callbacks.TerminateOnNaN object at 0x7f73ab6ba350>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_437/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 437/720 with hyperparameters:
timestamp = 2023-10-28 11:47:50.801671
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 11:49:41.584 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12321.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 111s - loss: nan - MinusLogProbMetric: 12321.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 111s/epoch - 565ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 437.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_437
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_189"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_190 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_24 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_24/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_24'")
self.model: <keras.engine.functional.Functional object at 0x7f71d01123e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f708743afe0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f708743afe0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7aaf80ac20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f71d0198880>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f71d019aaa0>, <keras.callbacks.ModelCheckpoint object at 0x7f71d019a860>, <keras.callbacks.EarlyStopping object at 0x7f71d019b5b0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f71d019b520>, <keras.callbacks.TerminateOnNaN object at 0x7f71d019b0d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_437/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 437/720 with hyperparameters:
timestamp = 2023-10-28 11:49:48.493428
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 11:51:38.603 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12321.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 110s - loss: nan - MinusLogProbMetric: 12321.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 110s/epoch - 562ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 437.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_437
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_200"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_201 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_25 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_25/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_25'")
self.model: <keras.engine.functional.Functional object at 0x7f72404791e0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7b04eaaa70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7b04eaaa70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7145772e30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f75da449300>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f75da449870>, <keras.callbacks.ModelCheckpoint object at 0x7f75da449930>, <keras.callbacks.EarlyStopping object at 0x7f75da449ba0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f75da449bd0>, <keras.callbacks.TerminateOnNaN object at 0x7f75da449810>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_437/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 437/720 with hyperparameters:
timestamp = 2023-10-28 11:51:45.802788
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 11:53:36.377 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12321.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 110s - loss: nan - MinusLogProbMetric: 12321.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 110s/epoch - 564ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 437.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_437
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_211"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_212 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_26 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_26/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_26'")
self.model: <keras.engine.functional.Functional object at 0x7f71b827faf0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f73a0b71420>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f73a0b71420>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73f0859870>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7145272290>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7145272800>, <keras.callbacks.ModelCheckpoint object at 0x7f71452728c0>, <keras.callbacks.EarlyStopping object at 0x7f7145272b30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7145272b60>, <keras.callbacks.TerminateOnNaN object at 0x7f71452727a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_437/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 437/720 with hyperparameters:
timestamp = 2023-10-28 11:53:43.820121
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 11:55:37.487 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12321.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 114s - loss: nan - MinusLogProbMetric: 12321.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 114s/epoch - 580ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 437.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_437/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_437
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_222"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_223 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_27 (LogProbL  (None,)                  1879020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 1,879,020
Trainable params: 1,879,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_27/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_27'")
self.model: <keras.engine.functional.Functional object at 0x7f77283d3d90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f72386fbe80>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f72386fbe80>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f77283e8ee0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f772836b580>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_437/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f772836baf0>, <keras.callbacks.ModelCheckpoint object at 0x7f772836bbb0>, <keras.callbacks.EarlyStopping object at 0x7f772836be20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f772836be50>, <keras.callbacks.TerminateOnNaN object at 0x7f772836ba90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_437/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 437/720 with hyperparameters:
timestamp = 2023-10-28 11:55:45.264618
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 1879020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 11:57:38.997 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12321.0664, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 114s - loss: nan - MinusLogProbMetric: 12321.0664 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 114s/epoch - 580ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 437/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 438.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_438
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_233"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_234 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_28 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_28/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_28'")
self.model: <keras.engine.functional.Functional object at 0x7f76187460b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f70864d9360>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f70864d9360>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f70852c2110>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f70852595d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7085259b40>, <keras.callbacks.ModelCheckpoint object at 0x7f7085259c00>, <keras.callbacks.EarlyStopping object at 0x7f7085259e70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7085259ea0>, <keras.callbacks.TerminateOnNaN object at 0x7f7085259ae0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_438/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 438/720 with hyperparameters:
timestamp = 2023-10-28 11:57:45.860105
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 11:59:36.273 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11904.4072, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 110s - loss: nan - MinusLogProbMetric: 11904.4072 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 110s/epoch - 563ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 438.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_438
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_244"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_245 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_29 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_29/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_29'")
self.model: <keras.engine.functional.Functional object at 0x7f76d037a530>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f70852350f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f70852350f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73a8457cd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f74050841c0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7405084730>, <keras.callbacks.ModelCheckpoint object at 0x7f74050847f0>, <keras.callbacks.EarlyStopping object at 0x7f7405084a60>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7405084a90>, <keras.callbacks.TerminateOnNaN object at 0x7f74050846d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_438/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 438/720 with hyperparameters:
timestamp = 2023-10-28 11:59:42.512650
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:01:33.336 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11904.4072, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 111s - loss: nan - MinusLogProbMetric: 11904.4072 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 111s/epoch - 565ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 438.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_438
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_255"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_256 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_30 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_30/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_30'")
self.model: <keras.engine.functional.Functional object at 0x7f7400868b50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7ad1834520>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7ad1834520>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f74020a21d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73f349cd30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73f349e6e0>, <keras.callbacks.ModelCheckpoint object at 0x7f73f349c130>, <keras.callbacks.EarlyStopping object at 0x7f73f349d810>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73f349f520>, <keras.callbacks.TerminateOnNaN object at 0x7f73f349e5f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_438/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 438/720 with hyperparameters:
timestamp = 2023-10-28 12:01:40.750208
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:03:27.585 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11904.4072, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 107s - loss: nan - MinusLogProbMetric: 11904.4072 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 107s/epoch - 545ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 438.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_438
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_266"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_267 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_31 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_31/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_31'")
self.model: <keras.engine.functional.Functional object at 0x7f73b3ce2ec0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f73b2f43f10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f73b2f43f10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73b39739a0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73b2a13bb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73b2a7c610>, <keras.callbacks.ModelCheckpoint object at 0x7f73b2a7c6d0>, <keras.callbacks.EarlyStopping object at 0x7f73b2a7c940>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73b2a7c970>, <keras.callbacks.TerminateOnNaN object at 0x7f73b2a7c5b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_438/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 438/720 with hyperparameters:
timestamp = 2023-10-28 12:03:34.399871
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:05:24.547 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11904.4072, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 110s - loss: nan - MinusLogProbMetric: 11904.4072 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 110s/epoch - 562ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 438.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_438
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_277"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_278 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_32 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_32/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_32'")
self.model: <keras.engine.functional.Functional object at 0x7f7146929b70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7ad18f1630>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7ad18f1630>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f740075be50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f71445a9cc0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f71445aa230>, <keras.callbacks.ModelCheckpoint object at 0x7f71445aa2f0>, <keras.callbacks.EarlyStopping object at 0x7f71445aa560>, <keras.callbacks.ReduceLROnPlateau object at 0x7f71445aa590>, <keras.callbacks.TerminateOnNaN object at 0x7f71445aa1d0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_438/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 438/720 with hyperparameters:
timestamp = 2023-10-28 12:05:32.204039
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:07:23.640 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11904.4072, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 111s - loss: nan - MinusLogProbMetric: 11904.4072 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 111s/epoch - 569ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 438.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_438
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_288"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_289 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_33 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_33/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_33'")
self.model: <keras.engine.functional.Functional object at 0x7f7403053fa0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f73b3f2c0d0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f73b3f2c0d0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73705eca30>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7220574310>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7220574880>, <keras.callbacks.ModelCheckpoint object at 0x7f7220574940>, <keras.callbacks.EarlyStopping object at 0x7f7220574bb0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7220574be0>, <keras.callbacks.TerminateOnNaN object at 0x7f7220574820>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_438/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 438/720 with hyperparameters:
timestamp = 2023-10-28 12:07:30.652408
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:09:21.087 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11904.4072, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 110s - loss: nan - MinusLogProbMetric: 11904.4072 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 110s/epoch - 564ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 438.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_438
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_299"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_300 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_34 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_34/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_34'")
self.model: <keras.engine.functional.Functional object at 0x7f73a848d450>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7401d18a60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7401d18a60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f761820e5f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f75205055d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7520505b40>, <keras.callbacks.ModelCheckpoint object at 0x7f7520505c00>, <keras.callbacks.EarlyStopping object at 0x7f7520505e70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7520505ea0>, <keras.callbacks.TerminateOnNaN object at 0x7f7520505ae0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_438/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 438/720 with hyperparameters:
timestamp = 2023-10-28 12:09:28.930244
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:11:11.223 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11904.4072, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 102s - loss: nan - MinusLogProbMetric: 11904.4072 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 102s/epoch - 521ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 438.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_438
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_310"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_311 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_35 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_35/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_35'")
self.model: <keras.engine.functional.Functional object at 0x7f73b2218580>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f75205062c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f75205062c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f70359e32b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73a8e95510>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73a8e95a80>, <keras.callbacks.ModelCheckpoint object at 0x7f73a8e95b40>, <keras.callbacks.EarlyStopping object at 0x7f73a8e95db0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73a8e95de0>, <keras.callbacks.TerminateOnNaN object at 0x7f73a8e95a20>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_438/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 438/720 with hyperparameters:
timestamp = 2023-10-28 12:11:17.562993
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:13:07.537 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11904.4072, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 110s - loss: nan - MinusLogProbMetric: 11904.4072 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 110s/epoch - 561ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 438.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_438
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_321"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_322 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_36 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_36/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_36'")
self.model: <keras.engine.functional.Functional object at 0x7f73b1d9cca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f71b97d2f50>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f71b97d2f50>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f70855427d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7036d36230>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7036d367a0>, <keras.callbacks.ModelCheckpoint object at 0x7f7036d36860>, <keras.callbacks.EarlyStopping object at 0x7f7036d36ad0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7036d36b00>, <keras.callbacks.TerminateOnNaN object at 0x7f7036d36740>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_438/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 438/720 with hyperparameters:
timestamp = 2023-10-28 12:13:14.392560
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:15:04.857 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11904.4072, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 110s - loss: nan - MinusLogProbMetric: 11904.4072 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 110s/epoch - 564ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 438.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_438
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_332"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_333 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_37 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_37/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_37'")
self.model: <keras.engine.functional.Functional object at 0x7f740226df90>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f71b8aa9810>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f71b8aa9810>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f72f0585270>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f703570db40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f703570e0b0>, <keras.callbacks.ModelCheckpoint object at 0x7f703570e170>, <keras.callbacks.EarlyStopping object at 0x7f703570e3e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f703570e410>, <keras.callbacks.TerminateOnNaN object at 0x7f703570e050>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_438/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 438/720 with hyperparameters:
timestamp = 2023-10-28 12:15:11.602436
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:17:05.927 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11904.4072, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 114s - loss: nan - MinusLogProbMetric: 11904.4072 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 114s/epoch - 584ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 438.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_438/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_438
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_343"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_344 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_38 (LogProbL  (None,)                  4401900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 4,401,900
Trainable params: 4,401,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_38/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_38'")
self.model: <keras.engine.functional.Functional object at 0x7f73f31d0430>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f72104bf7f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f72104bf7f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f750c2bca60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f702c34ea40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_438/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f702c34ddb0>, <keras.callbacks.ModelCheckpoint object at 0x7f702c34dc90>, <keras.callbacks.EarlyStopping object at 0x7f702c34d210>, <keras.callbacks.ReduceLROnPlateau object at 0x7f702c34d960>, <keras.callbacks.TerminateOnNaN object at 0x7f702c34dff0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_438/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 438/720 with hyperparameters:
timestamp = 2023-10-28 12:17:13.528412
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 8
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 4401900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is below threshold. Removing them from average.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 1: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:19:00.140 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11904.4072, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 107s - loss: nan - MinusLogProbMetric: 11904.4072 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 107s/epoch - 544ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 438/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 439.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_439
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_354"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_355 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_39 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_39/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_39'")
self.model: <keras.engine.functional.Functional object at 0x7f73a19552a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f73a3af6a70>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f73a3af6a70>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f76d0315bd0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7373e720e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7373e72650>, <keras.callbacks.ModelCheckpoint object at 0x7f7373e72710>, <keras.callbacks.EarlyStopping object at 0x7f7373e72980>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7373e729b0>, <keras.callbacks.TerminateOnNaN object at 0x7f7373e725f0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_439/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 439/720 with hyperparameters:
timestamp = 2023-10-28 12:19:08.474465
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:21:35.668 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12551.7969, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 147s - loss: nan - MinusLogProbMetric: 12551.7969 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 147s/epoch - 751ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 439.
===========
Train data generated in 0.29 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_439
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_365"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_366 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_40 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_40/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_40'")
self.model: <keras.engine.functional.Functional object at 0x7f7147ab81c0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f702d3819c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f702d3819c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7ad120dd20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73a0aa4670>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73a0aa4be0>, <keras.callbacks.ModelCheckpoint object at 0x7f73a0aa4ca0>, <keras.callbacks.EarlyStopping object at 0x7f73a0aa4f10>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73a0aa4f40>, <keras.callbacks.TerminateOnNaN object at 0x7f73a0aa4b80>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_439/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 439/720 with hyperparameters:
timestamp = 2023-10-28 12:21:44.393610
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:24:08.876 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12551.7969, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 144s - loss: nan - MinusLogProbMetric: 12551.7969 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 144s/epoch - 737ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 439.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_439
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_376"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_377 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_41 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_41/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_41'")
self.model: <keras.engine.functional.Functional object at 0x7f71d03bf910>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f73342208e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f73342208e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f75a47257b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73b1805540>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73b1805ab0>, <keras.callbacks.ModelCheckpoint object at 0x7f73b1805b70>, <keras.callbacks.EarlyStopping object at 0x7f73b1805de0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73b1805e10>, <keras.callbacks.TerminateOnNaN object at 0x7f73b1805a50>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_439/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 439/720 with hyperparameters:
timestamp = 2023-10-28 12:24:17.370095
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:26:42.447 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12551.7969, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 145s - loss: nan - MinusLogProbMetric: 12551.7969 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 145s/epoch - 740ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 439.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_439
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_387"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_388 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_42 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_42/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_42'")
self.model: <keras.engine.functional.Functional object at 0x7f73b1a798d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f73a36a8670>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f73a36a8670>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f708516d090>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73a8ba0af0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73a8ba1060>, <keras.callbacks.ModelCheckpoint object at 0x7f73a8ba1120>, <keras.callbacks.EarlyStopping object at 0x7f73a8ba1390>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73a8ba13c0>, <keras.callbacks.TerminateOnNaN object at 0x7f73a8ba1000>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_439/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 439/720 with hyperparameters:
timestamp = 2023-10-28 12:26:51.716210
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:29:24.814 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12551.7969, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 153s - loss: nan - MinusLogProbMetric: 12551.7969 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 153s/epoch - 781ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 439.
===========
Train data generated in 0.32 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_439
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_398"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_399 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_43 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_43/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_43'")
self.model: <keras.engine.functional.Functional object at 0x7f7084a78400>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7373eb3a90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7373eb3a90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f70353ce560>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f75f84ed1e0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f75f84ede40>, <keras.callbacks.ModelCheckpoint object at 0x7f75f84edc00>, <keras.callbacks.EarlyStopping object at 0x7f75f84ee1a0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f75f84eebf0>, <keras.callbacks.TerminateOnNaN object at 0x7f75f84ef610>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_439/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 439/720 with hyperparameters:
timestamp = 2023-10-28 12:29:34.360356
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:31:50.269 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12551.7969, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 136s - loss: nan - MinusLogProbMetric: 12551.7969 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 136s/epoch - 693ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 439.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_439
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_409"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_410 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_44 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_44/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_44'")
self.model: <keras.engine.functional.Functional object at 0x7f737352b850>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f72385a3100>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f72385a3100>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7146779450>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f714677be80>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f714677a0e0>, <keras.callbacks.ModelCheckpoint object at 0x7f714677a470>, <keras.callbacks.EarlyStopping object at 0x7f71467794e0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7146778be0>, <keras.callbacks.TerminateOnNaN object at 0x7f714677ab60>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_439/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 439/720 with hyperparameters:
timestamp = 2023-10-28 12:32:04.513660
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:34:20.507 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12551.7969, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 136s - loss: nan - MinusLogProbMetric: 12551.7969 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 136s/epoch - 693ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 439.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_439
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_420"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_421 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_45 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_45/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_45'")
self.model: <keras.engine.functional.Functional object at 0x7f73721fbb20>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7084d041c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7084d041c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7017cfbc70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7337f36b30>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7337f370a0>, <keras.callbacks.ModelCheckpoint object at 0x7f7337f37160>, <keras.callbacks.EarlyStopping object at 0x7f7337f373d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7337f37400>, <keras.callbacks.TerminateOnNaN object at 0x7f7337f37040>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_439/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 439/720 with hyperparameters:
timestamp = 2023-10-28 12:34:28.712510
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:36:57.917 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12551.7969, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 149s - loss: nan - MinusLogProbMetric: 12551.7969 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 149s/epoch - 761ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 439.
===========
Train data generated in 0.30 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_439
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_431"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_432 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_46 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_46/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_46'")
self.model: <keras.engine.functional.Functional object at 0x7f7084c9a9b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7084c99c30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7084c99c30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73b36ba6b0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7084ce10f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7084ce1660>, <keras.callbacks.ModelCheckpoint object at 0x7f7084ce1720>, <keras.callbacks.EarlyStopping object at 0x7f7084ce1990>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7084ce19c0>, <keras.callbacks.TerminateOnNaN object at 0x7f7084ce1600>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_439/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 439/720 with hyperparameters:
timestamp = 2023-10-28 12:37:06.637583
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:39:42.205 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12551.7969, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 155s - loss: nan - MinusLogProbMetric: 12551.7969 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 155s/epoch - 793ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 439.
===========
Train data generated in 0.35 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_439
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_442"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_443 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_47 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_47/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_47'")
self.model: <keras.engine.functional.Functional object at 0x7f73f0b024a0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7084ce1900>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7084ce1900>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7ac9087a00>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73107b9b40>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73107b93c0>, <keras.callbacks.ModelCheckpoint object at 0x7f73107baa40>, <keras.callbacks.EarlyStopping object at 0x7f73107b98d0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73107b9f60>, <keras.callbacks.TerminateOnNaN object at 0x7f73107b95a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_439/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 439/720 with hyperparameters:
timestamp = 2023-10-28 12:39:52.505052
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:42:05.712 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12551.7969, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 133s - loss: nan - MinusLogProbMetric: 12551.7969 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 133s/epoch - 679ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 439.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_439
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_453"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_454 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_48 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_48/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_48'")
self.model: <keras.engine.functional.Functional object at 0x7f6fd52765f0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7337e695a0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7337e695a0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f6fd4fa3bb0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7337c84df0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7337c85360>, <keras.callbacks.ModelCheckpoint object at 0x7f7337c85420>, <keras.callbacks.EarlyStopping object at 0x7f7337c85690>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7337c856c0>, <keras.callbacks.TerminateOnNaN object at 0x7f7337c85300>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_439/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 439/720 with hyperparameters:
timestamp = 2023-10-28 12:42:15.398995
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:44:44.718 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12551.7969, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 149s - loss: nan - MinusLogProbMetric: 12551.7969 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 149s/epoch - 762ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 439.
===========
Train data generated in 0.26 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_439/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_439
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_464"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_465 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_49 (LogProbL  (None,)                  2653020   
 ayer)                                                           
                                                                 
=================================================================
Total params: 2,653,020
Trainable params: 2,653,020
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_49/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_49'")
self.model: <keras.engine.functional.Functional object at 0x7f707db22950>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f707fec6d40>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f707fec6d40>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f73b03b2890>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f73b03ac460>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_439/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73b03ac9d0>, <keras.callbacks.ModelCheckpoint object at 0x7f73b03aca90>, <keras.callbacks.EarlyStopping object at 0x7f73b03acd00>, <keras.callbacks.ReduceLROnPlateau object at 0x7f73b03acd30>, <keras.callbacks.TerminateOnNaN object at 0x7f73b03ac970>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_439/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 439/720 with hyperparameters:
timestamp = 2023-10-28 12:44:52.984854
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 2653020
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:47:25.432 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 12551.7969, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 152s - loss: nan - MinusLogProbMetric: 12551.7969 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 152s/epoch - 778ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 439/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 440.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_440
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_475"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_476 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_50 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_50/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_50'")
self.model: <keras.engine.functional.Functional object at 0x7f73aa29fca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7145af0c10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7145af0c10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7ac84928f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7ad183a3b0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7ad183b490>, <keras.callbacks.ModelCheckpoint object at 0x7f7ad1839720>, <keras.callbacks.EarlyStopping object at 0x7f7ad1839ea0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7ad183ae30>, <keras.callbacks.TerminateOnNaN object at 0x7f7ad183b940>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_440/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 440/720 with hyperparameters:
timestamp = 2023-10-28 12:47:34.116941
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:49:48.888 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11911.8662, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 135s - loss: nan - MinusLogProbMetric: 11911.8662 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 135s/epoch - 688ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0003333333333333333.
===========
Generating train data for run 440.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_440
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_486"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_487 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_51 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_51/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_51'")
self.model: <keras.engine.functional.Functional object at 0x7f733790f6d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7ad183bd90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7ad183bd90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f6fdf83bd60>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7336e80490>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7336e80a00>, <keras.callbacks.ModelCheckpoint object at 0x7f7336e80ac0>, <keras.callbacks.EarlyStopping object at 0x7f7336e80d30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7336e80d60>, <keras.callbacks.TerminateOnNaN object at 0x7f7336e809a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_440/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 440/720 with hyperparameters:
timestamp = 2023-10-28 12:49:57.029314
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:52:20.392 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11911.8662, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 143s - loss: nan - MinusLogProbMetric: 11911.8662 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 143s/epoch - 731ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 0.0001111111111111111.
===========
Generating train data for run 440.
===========
Train data generated in 0.23 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_440
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_497"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_498 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_52 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_52/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_52'")
self.model: <keras.engine.functional.Functional object at 0x7f707ca2bfd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f70378e69e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f70378e69e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7085753e80>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f6fd58aa680>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f6fd58aabf0>, <keras.callbacks.ModelCheckpoint object at 0x7f6fd58aacb0>, <keras.callbacks.EarlyStopping object at 0x7f6fd58aaf20>, <keras.callbacks.ReduceLROnPlateau object at 0x7f6fd58aaf50>, <keras.callbacks.TerminateOnNaN object at 0x7f6fd58aab90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_440/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 440/720 with hyperparameters:
timestamp = 2023-10-28 12:52:30.042762
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:54:51.862 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11911.8662, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 142s - loss: nan - MinusLogProbMetric: 11911.8662 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 142s/epoch - 724ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 3.703703703703703e-05.
===========
Generating train data for run 440.
===========
Train data generated in 0.25 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_440
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_508"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_509 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_53 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_53/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_53'")
self.model: <keras.engine.functional.Functional object at 0x7f6fd46b3ca0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f706febe620>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f706febe620>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f706e33f190>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7014acb520>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7014acba90>, <keras.callbacks.ModelCheckpoint object at 0x7f7014acbb50>, <keras.callbacks.EarlyStopping object at 0x7f7014acbdc0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7014acbdf0>, <keras.callbacks.TerminateOnNaN object at 0x7f7014acba30>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_440/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 440/720 with hyperparameters:
timestamp = 2023-10-28 12:55:01.100793
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:57:15.356 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11911.8662, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 134s - loss: nan - MinusLogProbMetric: 11911.8662 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 134s/epoch - 685ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.2345679012345677e-05.
===========
Generating train data for run 440.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_440
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_519"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_520 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_54 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_54/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_54'")
self.model: <keras.engine.functional.Functional object at 0x7f7335e6bfd0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7335f77730>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7335f77730>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7014a06c20>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7335ea3850>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7335ea3dc0>, <keras.callbacks.ModelCheckpoint object at 0x7f7335ea3e80>, <keras.callbacks.EarlyStopping object at 0x7f7335ea3f70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7335ea3f10>, <keras.callbacks.TerminateOnNaN object at 0x7f7335ea3d90>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_440/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 440/720 with hyperparameters:
timestamp = 2023-10-28 12:57:23.187117
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 12:59:47.873 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11911.8662, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 145s - loss: nan - MinusLogProbMetric: 11911.8662 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 145s/epoch - 738ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.115226337448558e-06.
===========
Generating train data for run 440.
===========
Train data generated in 0.24 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_440
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_530"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_531 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_55 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_55/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_55'")
self.model: <keras.engine.functional.Functional object at 0x7f707c0ccc70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f6fe4028700>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f6fe4028700>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f6fcc3cd960>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f703dc089a0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f703dc08f10>, <keras.callbacks.ModelCheckpoint object at 0x7f703dc08fd0>, <keras.callbacks.EarlyStopping object at 0x7f703dc09240>, <keras.callbacks.ReduceLROnPlateau object at 0x7f703dc09270>, <keras.callbacks.TerminateOnNaN object at 0x7f703dc08eb0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_440/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 440/720 with hyperparameters:
timestamp = 2023-10-28 12:59:56.063569
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 13:02:22.953 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11911.8662, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 147s - loss: nan - MinusLogProbMetric: 11911.8662 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.1152e-06 - 147s/epoch - 749ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.3717421124828526e-06.
===========
Generating train data for run 440.
===========
Train data generated in 0.17 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_440
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_541"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_542 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_56 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_56/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_56'")
self.model: <keras.engine.functional.Functional object at 0x7f73f14b9240>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f76185202e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.3717421124828526e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f76185202e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7373eacbe0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7005bb71f0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7005bb5450>, <keras.callbacks.ModelCheckpoint object at 0x7f7005bb68f0>, <keras.callbacks.EarlyStopping object at 0x7f7005bb5210>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7005bb5600>, <keras.callbacks.TerminateOnNaN object at 0x7f7005bb5060>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_440/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 440/720 with hyperparameters:
timestamp = 2023-10-28 13:02:29.541309
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.3717421124828526e-06...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 13:04:42.921 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11911.8662, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 133s - loss: nan - MinusLogProbMetric: 11911.8662 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.3717e-06 - 133s/epoch - 680ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 4.572473708276175e-07.
===========
Generating train data for run 440.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_440
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_552"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_553 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_57 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_57/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_57'")
self.model: <keras.engine.functional.Functional object at 0x7f73139f4d60>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7313881ae0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.572473708276175e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7313881ae0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f6fd53f6bf0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7312fe03d0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7312fe0940>, <keras.callbacks.ModelCheckpoint object at 0x7f7312fe0a00>, <keras.callbacks.EarlyStopping object at 0x7f7312fe0c70>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7312fe0ca0>, <keras.callbacks.TerminateOnNaN object at 0x7f7312fe08e0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_440/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 440/720 with hyperparameters:
timestamp = 2023-10-28 13:04:51.093379
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.572473708276175e-07...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 13:07:14.412 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11911.8662, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 143s - loss: nan - MinusLogProbMetric: 11911.8662 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 4.5725e-07 - 143s/epoch - 731ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.524157902758725e-07.
===========
Generating train data for run 440.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_440
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_563"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_564 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_58 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_58/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_58'")
self.model: <keras.engine.functional.Functional object at 0x7f6fd5f6e4d0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f76d04105e0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.524157902758725e-07, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f76d04105e0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7086b6a440>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7086b73bb0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7086b50610>, <keras.callbacks.ModelCheckpoint object at 0x7f7086b506d0>, <keras.callbacks.EarlyStopping object at 0x7f7086b50940>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7086b50970>, <keras.callbacks.TerminateOnNaN object at 0x7f7086b505b0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_440/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 440/720 with hyperparameters:
timestamp = 2023-10-28 13:07:23.216280
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.524157902758725e-07...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 13:09:39.284 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11911.8662, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 136s - loss: nan - MinusLogProbMetric: 11911.8662 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.5242e-07 - 136s/epoch - 695ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.0805263425290834e-08.
===========
Generating train data for run 440.
===========
Train data generated in 0.21 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_440
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_574"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_575 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_59 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_59/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_59'")
self.model: <keras.engine.functional.Functional object at 0x7f70263cbe50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f6fb74bebc0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 5.0805263425290834e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f6fb74bebc0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f6fe424aa10>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f6fcc186ef0>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f6fcc187460>, <keras.callbacks.ModelCheckpoint object at 0x7f6fcc187520>, <keras.callbacks.EarlyStopping object at 0x7f6fcc187790>, <keras.callbacks.ReduceLROnPlateau object at 0x7f6fcc1877c0>, <keras.callbacks.TerminateOnNaN object at 0x7f6fcc187400>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_440/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 440/720 with hyperparameters:
timestamp = 2023-10-28 13:09:48.594069
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 5.0805263425290834e-08...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 13:12:15.235 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11911.8662, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 147s - loss: nan - MinusLogProbMetric: 11911.8662 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 5.0805e-08 - 147s/epoch - 748ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 1.6935087808430278e-08.
===========
Generating train data for run 440.
===========
Train data generated in 0.20 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 520}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_440/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_440
self.data_kwargs: {'seed': 520}
self.x_data: [[ 2.964998    6.790415    4.176491   ...  8.638963    9.874324
   9.877977  ]
 [ 2.9298463   7.1481295   3.3337944  ...  8.9422455   9.642555
   9.967966  ]
 [ 3.201615    6.7843637   4.300337   ...  8.685237   10.108791
  10.444715  ]
 ...
 [ 2.772429    7.236581    4.1707683  ...  8.839664    9.754404
  10.437025  ]
 [ 4.880482    7.8517284   6.343484   ...  1.026491    8.286235
   0.36894342]
 [ 3.1949604   6.513362    3.561295   ...  8.139516    9.981941
   8.512237  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_585"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_586 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_60 (LogProbL  (None,)                  5943900   
 ayer)                                                           
                                                                 
=================================================================
Total params: 5,943,900
Trainable params: 5,943,900
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_60/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_60'")
self.model: <keras.engine.functional.Functional object at 0x7f7016154160>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f728c412da0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.6935087808430278e-08, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f728c412da0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f702cd189d0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f71456b1960>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_440/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f71456b1ed0>, <keras.callbacks.ModelCheckpoint object at 0x7f71456b1f90>, <keras.callbacks.EarlyStopping object at 0x7f71456b2200>, <keras.callbacks.ReduceLROnPlateau object at 0x7f71456b2230>, <keras.callbacks.TerminateOnNaN object at 0x7f71456b1e70>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 7.1415815 ,  3.409545  ,  7.4013114 , ...,  2.7286146 ,
         1.0321306 ,  4.0325246 ],
       [ 3.049732  ,  7.3001237 ,  3.6959662 , ...,  8.488205  ,
         9.770978  , 10.456195  ],
       [ 7.3487005 ,  2.0359287 ,  7.4519386 , ...,  1.9214683 ,
         1.2261972 ,  3.5696049 ],
       ...,
       [ 3.5542827 ,  7.0820537 ,  3.7522032 , ...,  8.560173  ,
        10.155418  ,  9.167977  ],
       [ 6.757861  ,  3.124924  ,  7.544984  , ...,  2.7349882 ,
         0.81453794,  4.609927  ],
       [ 3.4225125 ,  6.358974  ,  4.1732883 , ...,  8.465443  ,
         9.600047  ,  9.602268  ]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_440/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 440/720 with hyperparameters:
timestamp = 2023-10-28 13:12:24.578079
ndims = 100
seed_train = 520
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 10
spline_knots = 12
range_min = -16
hidden_layers = 256-256-256
trainable_parameters = 5943900
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.6935087808430278e-08...
Train first sample: [ 2.964998    6.790415    4.176491    1.5818496   0.43715188 -0.7955233
  6.924872    4.525374    5.453677    9.321713    9.967441    2.2645552
  6.645713    3.448174    0.6748355   7.2054677   3.206739    3.9393213
  6.129664    7.3915524   6.721222    9.264137    2.7355173   8.935845
  1.8561007   9.770632    6.4505334   1.7204767   9.464344    7.728185
  2.5236232   2.1994019   5.1121893  -0.24492815  2.2547657   3.9803915
  3.7226124   4.451376    1.6016978   4.9925528   8.638066    0.2631895
  6.1193147   1.7877061   6.4556723   3.9666224   6.416565    0.51306784
  1.248136    4.9291797   2.457083    9.537456    7.6413703   7.424919
  9.664918    0.7982496   5.640124    5.9732943   9.44346     1.8386309
  3.412871    1.0435264   0.411201    8.653742    6.981474    6.2671413
  2.8744175   6.6132393  -0.927398    5.1027546   8.473179    8.803543
  3.2730348   9.122765    4.0005293   8.618069    9.245787    7.8784695
  6.546346    9.735427    3.05189     8.468583    6.657468    0.14129382
  4.2014775   1.532992    9.867379    4.7963347   4.995381    6.2222304
  4.4571123   1.364804    7.913327    1.0370703   4.699076    1.8984432
  1.2985027   8.638963    9.874324    9.877977  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 13:14:39.094 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 11911.8662, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 134s - loss: nan - MinusLogProbMetric: 11911.8662 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.6935e-08 - 134s/epoch - 686ms/step
The loss history contains NaN values.
Training failed: trying again with seed 105132 and lr 5.645029269476759e-09.
===========
Run 440/720 failed.
Exception type: Exception
Exception message: Training failed for the maximum number of retry.
Stack trace: ['File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 649, Func.Name : <module>, Message : hyperparams_dict, NFObject, seed_train, training_time = train_function(seeds = [seed_train, seed_test, seed_dist, seed_metrics],', 'File : /mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/Mains/CsplineN/c_Main_CsplineN_teogpu02.py , Line : 322, Func.Name : train_function, Message : raise Exception("Training failed for the maximum number of retry.")']
===========

===========
Generating train data for run 441.
===========
Train data generated in 0.22 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_441/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_441/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_441/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_441
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.8196216   7.352534    4.3137803  ...  8.349218    9.6399975
  10.511833  ]
 [ 2.7526174   6.8874693   2.2910128  ...  8.183364    9.663754
   9.738063  ]
 [ 3.2483912   7.135125    3.4702203  ...  9.088437    9.530681
  10.125393  ]
 ...
 [ 6.490836    3.8605149   7.442003   ...  3.0635746   0.83504534
   3.6644914 ]
 [ 3.1387556   7.0266705   3.615142   ...  8.433815    9.469447
   9.153159  ]
 [ 3.3042033   6.1923413   4.2528     ...  8.321929    9.30474
   9.914487  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_591"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_592 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_61 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_61/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_61'")
self.model: <keras.engine.functional.Functional object at 0x7f6f9f2f17b0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f6fb751bf10>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.001, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f6fb751bf10>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f6fb764fb50>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f6fb71cba90>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_441/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f6fb71cbfa0>, <keras.callbacks.ModelCheckpoint object at 0x7f6fb71cbfd0>, <keras.callbacks.EarlyStopping object at 0x7f7312d2c340>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7312d2c3a0>, <keras.callbacks.TerminateOnNaN object at 0x7f7312d2c370>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_441/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 441/720 with hyperparameters:
timestamp = 2023-10-28 13:14:44.256086
ndims = 100
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.001...
Train first sample: [ 3.8196216   7.352534    4.3137803   1.9025187  -0.01362735 -0.7737045
  6.107416    4.5390534   5.4286737   9.059374   10.008538    1.6677362
  6.0350723   1.7213604  -0.16004305  7.7809725   3.1822338   4.7347813
  6.5276837   8.523988    5.459942    8.776709    2.6793067   8.455664
  1.842243    9.510314    7.4895406   0.6126207   9.604811    7.5533233
  2.5600433   2.44656     4.5196915   0.24073066  1.8520832   4.2740364
  4.2971277   3.6692986   3.0170891   5.291486    8.636747    1.9775515
  5.7492237   1.9318466   8.113486    3.9665818   5.9971104   1.6739144
  1.409387    4.9013453   3.8748677   9.304877    6.942658    8.544345
  9.0841875   0.9904293   5.277098    5.845995   10.226088    3.9238465
  2.4825075   0.98767704  0.15303165 10.518439    7.3086348   6.9791408
  2.6091816   8.299133    0.01815706  4.3511486  11.220155    8.7436
  3.0565658   9.669933    1.9547316   9.169304    9.500812    7.900913
  6.929075    8.661879    3.0731711   8.227158    6.2899137   0.21501312
  4.4748106   1.6470722   9.777415    4.8366313   4.9664555   6.1797996
  5.3417387   2.1408544   8.709873    1.811256    4.8512735   1.5080717
  0.2304368   8.349218    9.6399975  10.511833  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 13:15:51.387 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10143.7090, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 67s - loss: nan - MinusLogProbMetric: 10143.7090 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 0.0010 - 67s/epoch - 342ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0003333333333333333.
===========
Generating train data for run 441.
===========
Train data generated in 0.31 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_441/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_441/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_441/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_441
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.8196216   7.352534    4.3137803  ...  8.349218    9.6399975
  10.511833  ]
 [ 2.7526174   6.8874693   2.2910128  ...  8.183364    9.663754
   9.738063  ]
 [ 3.2483912   7.135125    3.4702203  ...  9.088437    9.530681
  10.125393  ]
 ...
 [ 6.490836    3.8605149   7.442003   ...  3.0635746   0.83504534
   3.6644914 ]
 [ 3.1387556   7.0266705   3.615142   ...  8.433815    9.469447
   9.153159  ]
 [ 3.3042033   6.1923413   4.2528     ...  8.321929    9.30474
   9.914487  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_597"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_598 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_62 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_62/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_62'")
self.model: <keras.engine.functional.Functional object at 0x7f7312e1d630>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7313c67a60>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0003333333333333333, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7313c67a60>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7024a88af0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7312fe2380>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_441/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7312fe1fc0>, <keras.callbacks.ModelCheckpoint object at 0x7f7312fe0b50>, <keras.callbacks.EarlyStopping object at 0x7f7312fe3220>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7312fe1ba0>, <keras.callbacks.TerminateOnNaN object at 0x7f7312fe3340>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_441/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 441/720 with hyperparameters:
timestamp = 2023-10-28 13:15:56.208717
ndims = 100
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0003333333333333333...
Train first sample: [ 3.8196216   7.352534    4.3137803   1.9025187  -0.01362735 -0.7737045
  6.107416    4.5390534   5.4286737   9.059374   10.008538    1.6677362
  6.0350723   1.7213604  -0.16004305  7.7809725   3.1822338   4.7347813
  6.5276837   8.523988    5.459942    8.776709    2.6793067   8.455664
  1.842243    9.510314    7.4895406   0.6126207   9.604811    7.5533233
  2.5600433   2.44656     4.5196915   0.24073066  1.8520832   4.2740364
  4.2971277   3.6692986   3.0170891   5.291486    8.636747    1.9775515
  5.7492237   1.9318466   8.113486    3.9665818   5.9971104   1.6739144
  1.409387    4.9013453   3.8748677   9.304877    6.942658    8.544345
  9.0841875   0.9904293   5.277098    5.845995   10.226088    3.9238465
  2.4825075   0.98767704  0.15303165 10.518439    7.3086348   6.9791408
  2.6091816   8.299133    0.01815706  4.3511486  11.220155    8.7436
  3.0565658   9.669933    1.9547316   9.169304    9.500812    7.900913
  6.929075    8.661879    3.0731711   8.227158    6.2899137   0.21501312
  4.4748106   1.6470722   9.777415    4.8366313   4.9664555   6.1797996
  5.3417387   2.1408544   8.709873    1.811256    4.8512735   1.5080717
  0.2304368   8.349218    9.6399975  10.511833  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 13:16:49.046 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10143.7090, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 53s - loss: nan - MinusLogProbMetric: 10143.7090 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.3333e-04 - 53s/epoch - 269ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 0.0001111111111111111.
===========
Generating train data for run 441.
===========
Train data generated in 0.16 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_441/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_441/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_441/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_441
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.8196216   7.352534    4.3137803  ...  8.349218    9.6399975
  10.511833  ]
 [ 2.7526174   6.8874693   2.2910128  ...  8.183364    9.663754
   9.738063  ]
 [ 3.2483912   7.135125    3.4702203  ...  9.088437    9.530681
  10.125393  ]
 ...
 [ 6.490836    3.8605149   7.442003   ...  3.0635746   0.83504534
   3.6644914 ]
 [ 3.1387556   7.0266705   3.615142   ...  8.433815    9.469447
   9.153159  ]
 [ 3.3042033   6.1923413   4.2528     ...  8.321929    9.30474
   9.914487  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_603"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_604 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_63 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_63/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_63'")
self.model: <keras.engine.functional.Functional object at 0x7f7015b30ee0>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f70163e87f0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 0.0001111111111111111, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f70163e87f0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f6fd4956a70>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7146b3e500>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_441/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f7146b3ea70>, <keras.callbacks.ModelCheckpoint object at 0x7f7146b3eb30>, <keras.callbacks.EarlyStopping object at 0x7f7146b3eda0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7146b3edd0>, <keras.callbacks.TerminateOnNaN object at 0x7f7146b3ea10>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_441/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 441/720 with hyperparameters:
timestamp = 2023-10-28 13:16:51.951958
ndims = 100
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 0.0001111111111111111...
Train first sample: [ 3.8196216   7.352534    4.3137803   1.9025187  -0.01362735 -0.7737045
  6.107416    4.5390534   5.4286737   9.059374   10.008538    1.6677362
  6.0350723   1.7213604  -0.16004305  7.7809725   3.1822338   4.7347813
  6.5276837   8.523988    5.459942    8.776709    2.6793067   8.455664
  1.842243    9.510314    7.4895406   0.6126207   9.604811    7.5533233
  2.5600433   2.44656     4.5196915   0.24073066  1.8520832   4.2740364
  4.2971277   3.6692986   3.0170891   5.291486    8.636747    1.9775515
  5.7492237   1.9318466   8.113486    3.9665818   5.9971104   1.6739144
  1.409387    4.9013453   3.8748677   9.304877    6.942658    8.544345
  9.0841875   0.9904293   5.277098    5.845995   10.226088    3.9238465
  2.4825075   0.98767704  0.15303165 10.518439    7.3086348   6.9791408
  2.6091816   8.299133    0.01815706  4.3511486  11.220155    8.7436
  3.0565658   9.669933    1.9547316   9.169304    9.500812    7.900913
  6.929075    8.661879    3.0731711   8.227158    6.2899137   0.21501312
  4.4748106   1.6470722   9.777415    4.8366313   4.9664555   6.1797996
  5.3417387   2.1408544   8.709873    1.811256    4.8512735   1.5080717
  0.2304368   8.349218    9.6399975  10.511833  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 13:17:41.926 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10143.7090, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 50s - loss: nan - MinusLogProbMetric: 10143.7090 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.1111e-04 - 50s/epoch - 255ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 3.703703703703703e-05.
===========
Generating train data for run 441.
===========
Train data generated in 0.13 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_441/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_441/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_441/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_441
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.8196216   7.352534    4.3137803  ...  8.349218    9.6399975
  10.511833  ]
 [ 2.7526174   6.8874693   2.2910128  ...  8.183364    9.663754
   9.738063  ]
 [ 3.2483912   7.135125    3.4702203  ...  9.088437    9.530681
  10.125393  ]
 ...
 [ 6.490836    3.8605149   7.442003   ...  3.0635746   0.83504534
   3.6644914 ]
 [ 3.1387556   7.0266705   3.615142   ...  8.433815    9.469447
   9.153159  ]
 [ 3.3042033   6.1923413   4.2528     ...  8.321929    9.30474
   9.914487  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_609"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_610 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_64 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_64/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_64'")
self.model: <keras.engine.functional.Functional object at 0x7f706c5a4e50>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f707d870d90>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 3.703703703703703e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f707d870d90>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f6fdfc162f0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f6fdfc56290>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_441/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f6fdfc56800>, <keras.callbacks.ModelCheckpoint object at 0x7f6fdfc568c0>, <keras.callbacks.EarlyStopping object at 0x7f6fdfc56b30>, <keras.callbacks.ReduceLROnPlateau object at 0x7f6fdfc56b60>, <keras.callbacks.TerminateOnNaN object at 0x7f6fdfc567a0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_441/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 441/720 with hyperparameters:
timestamp = 2023-10-28 13:17:44.590584
ndims = 100
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 3.703703703703703e-05...
Train first sample: [ 3.8196216   7.352534    4.3137803   1.9025187  -0.01362735 -0.7737045
  6.107416    4.5390534   5.4286737   9.059374   10.008538    1.6677362
  6.0350723   1.7213604  -0.16004305  7.7809725   3.1822338   4.7347813
  6.5276837   8.523988    5.459942    8.776709    2.6793067   8.455664
  1.842243    9.510314    7.4895406   0.6126207   9.604811    7.5533233
  2.5600433   2.44656     4.5196915   0.24073066  1.8520832   4.2740364
  4.2971277   3.6692986   3.0170891   5.291486    8.636747    1.9775515
  5.7492237   1.9318466   8.113486    3.9665818   5.9971104   1.6739144
  1.409387    4.9013453   3.8748677   9.304877    6.942658    8.544345
  9.0841875   0.9904293   5.277098    5.845995   10.226088    3.9238465
  2.4825075   0.98767704  0.15303165 10.518439    7.3086348   6.9791408
  2.6091816   8.299133    0.01815706  4.3511486  11.220155    8.7436
  3.0565658   9.669933    1.9547316   9.169304    9.500812    7.900913
  6.929075    8.661879    3.0731711   8.227158    6.2899137   0.21501312
  4.4748106   1.6470722   9.777415    4.8366313   4.9664555   6.1797996
  5.3417387   2.1408544   8.709873    1.811256    4.8512735   1.5080717
  0.2304368   8.349218    9.6399975  10.511833  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 13:18:34.046 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10143.7090, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 49s - loss: nan - MinusLogProbMetric: 10143.7090 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 3.7037e-05 - 49s/epoch - 252ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 1.2345679012345677e-05.
===========
Generating train data for run 441.
===========
Train data generated in 0.14 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_441/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_441/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_441/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_441
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.8196216   7.352534    4.3137803  ...  8.349218    9.6399975
  10.511833  ]
 [ 2.7526174   6.8874693   2.2910128  ...  8.183364    9.663754
   9.738063  ]
 [ 3.2483912   7.135125    3.4702203  ...  9.088437    9.530681
  10.125393  ]
 ...
 [ 6.490836    3.8605149   7.442003   ...  3.0635746   0.83504534
   3.6644914 ]
 [ 3.1387556   7.0266705   3.615142   ...  8.433815    9.469447
   9.153159  ]
 [ 3.3042033   6.1923413   4.2528     ...  8.321929    9.30474
   9.914487  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_615"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_616 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_65 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_65/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_65'")
self.model: <keras.engine.functional.Functional object at 0x7f73128ce110>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7312727d30>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 1.2345679012345677e-05, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7312727d30>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f6f96a8fca0>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f731249e020>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_441/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f731249e590>, <keras.callbacks.ModelCheckpoint object at 0x7f731249e650>, <keras.callbacks.EarlyStopping object at 0x7f731249e8c0>, <keras.callbacks.ReduceLROnPlateau object at 0x7f731249e8f0>, <keras.callbacks.TerminateOnNaN object at 0x7f731249e530>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_441/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 441/720 with hyperparameters:
timestamp = 2023-10-28 13:18:37.740271
ndims = 100
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 1.2345679012345677e-05...
Train first sample: [ 3.8196216   7.352534    4.3137803   1.9025187  -0.01362735 -0.7737045
  6.107416    4.5390534   5.4286737   9.059374   10.008538    1.6677362
  6.0350723   1.7213604  -0.16004305  7.7809725   3.1822338   4.7347813
  6.5276837   8.523988    5.459942    8.776709    2.6793067   8.455664
  1.842243    9.510314    7.4895406   0.6126207   9.604811    7.5533233
  2.5600433   2.44656     4.5196915   0.24073066  1.8520832   4.2740364
  4.2971277   3.6692986   3.0170891   5.291486    8.636747    1.9775515
  5.7492237   1.9318466   8.113486    3.9665818   5.9971104   1.6739144
  1.409387    4.9013453   3.8748677   9.304877    6.942658    8.544345
  9.0841875   0.9904293   5.277098    5.845995   10.226088    3.9238465
  2.4825075   0.98767704  0.15303165 10.518439    7.3086348   6.9791408
  2.6091816   8.299133    0.01815706  4.3511486  11.220155    8.7436
  3.0565658   9.669933    1.9547316   9.169304    9.500812    7.900913
  6.929075    8.661879    3.0731711   8.227158    6.2899137   0.21501312
  4.4748106   1.6470722   9.777415    4.8366313   4.9664555   6.1797996
  5.3417387   2.1408544   8.709873    1.811256    4.8512735   1.5080717
  0.2304368   8.349218    9.6399975  10.511833  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Batch 0: Invalid loss, terminating training
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
2023-10-28 13:19:45.537 
Epoch 1/1000 
	 loss: nan, MinusLogProbMetric: 10143.7090, val_loss: nan, val_MinusLogProbMetric: nan

Epoch 1: val_loss did not improve from inf
196/196 - 68s - loss: nan - MinusLogProbMetric: 10143.7090 - val_loss: nan - val_MinusLogProbMetric: nan - lr: 1.2346e-05 - 68s/epoch - 345ms/step
The loss history contains NaN values.
Training failed: trying again with seed 810886 and lr 4.115226337448558e-06.
===========
Generating train data for run 441.
===========
Train data generated in 0.18 s.

Building Trainer NFObject.


--------------- Debub info ---------------
Initializing Trainer with following parameters:
base_distribution: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
x_data_train shape: (100000, 100)
y_data_train shape: (100000, 0)
io_kwargs: {'results_path': '../../results/CsplineN_new/run_441/', 'load_weights': True, 'load_results': True}
data_kwargs: {'seed': 541}
compiler_kwargs: {'optimizer': {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}, 'metrics': [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}], 'loss': {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}}
callbacks_kwargs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '../../results/CsplineN_new/run_441/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}

--------------- Debub info ---------------
Defined attributes:
self.base_dist: tfp.distributions.Sample("SampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.flow: tfp.bijectors._Chain("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline", batch_shape=[], min_event_ndims=1, bijectors=[Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline, Permute, Cspline])
self.nf_dist: tfp.distributions._TransformedDistribution("chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal", batch_shape=[], event_shape=[100], dtype=float32)
self.io_kwargs: {'results_path': '../../results/CsplineN_new/run_441/', 'load_weights': True, 'load_results': True}
self.results_path: ../../results/CsplineN_new/run_441
self.data_kwargs: {'seed': 541}
self.x_data: [[ 3.8196216   7.352534    4.3137803  ...  8.349218    9.6399975
  10.511833  ]
 [ 2.7526174   6.8874693   2.2910128  ...  8.183364    9.663754
   9.738063  ]
 [ 3.2483912   7.135125    3.4702203  ...  9.088437    9.530681
  10.125393  ]
 ...
 [ 6.490836    3.8605149   7.442003   ...  3.0635746   0.83504534
   3.6644914 ]
 [ 3.1387556   7.0266705   3.615142   ...  8.433815    9.469447
   9.153159  ]
 [ 3.3042033   6.1923413   4.2528     ...  8.321929    9.30474
   9.914487  ]]
self.y_data: []
self.ndims: 100
Model defined.
Model: "model_621"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_622 (InputLayer)      [(None, 100)]             0         
                                                                 
 log_prob_layer_66 (LogProbL  (None,)                  939510    
 ayer)                                                           
                                                                 
=================================================================
Total params: 939,510
Trainable params: 939,510
Non-trainable params: 0
_________________________________________________________________
Model summary:  None
self.log_prob: KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.float32, name=None), name='log_prob_layer_66/chain_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_cspline_of_permute_of_csplineSampleNormal_CONSTRUCTED_AT_top_level/log_prob/sub:0', description="created by layer 'log_prob_layer_66'")
self.model: <keras.engine.functional.Functional object at 0x7f74027cde70>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
optimizer: <keras.optimizers.adam.Adam object at 0x7f7aaf8081c0>
type(optimizer): <class 'keras.optimizers.adam.Adam'>
self.optimizer_config: {'class_name': 'Custom>Adam', 'config': {'learning_rate': 4.115226337448558e-06, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': True}}
self.optimizer: <keras.optimizers.adam.Adam object at 0x7f7aaf8081c0>
self.loss_config: {'class_name': 'MinusLogProbLoss', 'config': {'name': 'MLP', 'ignore_nans': True, 'nan_threshold': 0.01, 'debug_print_mode': False}}
self.loss: <Trainer.MinusLogProbLoss object at 0x7f7404e2e230>
self.metrics_configs: [{'class_name': 'MinusLogProbMetric', 'config': {'ignore_nans': True, 'debug_print_mode': False}}]
self.metrics: [<Trainer.MinusLogProbMetric object at 0x7f7370903e20>]
self.compile_kwargs: {}
self.callbacks_configs: [{'class_name': 'PrintEpochInfo', 'config': {}}, {'class_name': 'ModelCheckpoint', 'config': {'filepath': '/mnt/project_mnt/teo_fs/rtorre/cernbox/git/GitHub/NormalizingFlows/NF4HEP/NormalizingFlowsHD/CMoG/results/CsplineN_new/run_441/weights/best_weights.h5', 'monitor': 'val_loss', 'save_best_only': True, 'save_weights_only': True, 'verbose': 1, 'mode': 'auto', 'save_freq': 'epoch'}}, {'class_name': 'EarlyStopping', 'config': {'monitor': 'val_loss', 'min_delta': 0.0001, 'patience': 100, 'verbose': 1, 'mode': 'auto', 'baseline': None, 'restore_best_weights': True}}, {'class_name': 'ReduceLROnPlateau', 'config': {'monitor': 'val_loss', 'factor': 0.5, 'min_delta': 0.0001, 'patience': 50, 'min_lr': 1e-06}}, {'class_name': 'TerminateOnNaN', 'config': {}}]
self.callbacks: [<keras.callbacks.LambdaCallback object at 0x7f73709002b0>, <keras.callbacks.ModelCheckpoint object at 0x7f7370903dc0>, <keras.callbacks.EarlyStopping object at 0x7f7370901180>, <keras.callbacks.ReduceLROnPlateau object at 0x7f7370901b10>, <keras.callbacks.TerminateOnNaN object at 0x7f73709010c0>]
self.fit_kwargs: {'batch_size': 512, 'epochs': 1000, 'validation_data': (array([[ 2.8903666 ,  7.0204897 ,  4.052104  , ...,  8.401623  ,
         9.556685  ,  9.2562895 ],
       [ 3.3991427 ,  7.264298  ,  4.1017666 , ...,  8.748237  ,
         9.218689  , 10.18592   ],
       [ 2.7886415 ,  6.9446044 ,  3.7425206 , ...,  8.939978  ,
        10.302486  , 10.092964  ],
       ...,
       [ 4.941906  ,  6.8643713 ,  5.9536133 , ..., -0.23324785,
         8.273231  ,  0.20600395],
       [ 6.7767553 ,  2.5892656 ,  7.3095675 , ...,  2.7915924 ,
         0.33373857,  4.671312  ],
       [ 5.0069637 ,  6.390732  ,  6.305041  , ...,  0.19884644,
         8.278515  , -0.14602856]], dtype=float32), <tf.Tensor: shape=(30000, 0), dtype=float32, numpy=array([], shape=(30000, 0), dtype=float32)>), 'shuffle': True, 'verbose': 2}
self.is_compiled: False
self.training_time: 0.0
self.history: {}
Model successfully compiled.
No weights found in ../../results/CsplineN_new/run_441/weights/best_weights.h5. Training from scratch.
No history found. Generating new history.
===============
Running 441/720 with hyperparameters:
timestamp = 2023-10-28 13:19:48.404293
ndims = 100
seed_train = 541
nsamples_train = 100000
nsamples_val = 30000
nsamples_test = 100000
bijector = CsplineN
nbijectors = 5
spline_knots = 8
range_min = -16
hidden_layers = 128-128-128
trainable_parameters = 939510
epochs_input = 1000
batch_size = 512
activation = relu
training_device = NVIDIA A40
===============

Training model with initial learning rate 4.115226337448558e-06...
Train first sample: [ 3.8196216   7.352534    4.3137803   1.9025187  -0.01362735 -0.7737045
  6.107416    4.5390534   5.4286737   9.059374   10.008538    1.6677362
  6.0350723   1.7213604  -0.16004305  7.7809725   3.1822338   4.7347813
  6.5276837   8.523988    5.459942    8.776709    2.6793067   8.455664
  1.842243    9.510314    7.4895406   0.6126207   9.604811    7.5533233
  2.5600433   2.44656     4.5196915   0.24073066  1.8520832   4.2740364
  4.2971277   3.6692986   3.0170891   5.291486    8.636747    1.9775515
  5.7492237   1.9318466   8.113486    3.9665818   5.9971104   1.6739144
  1.409387    4.9013453   3.8748677   9.304877    6.942658    8.544345
  9.0841875   0.9904293   5.277098    5.845995   10.226088    3.9238465
  2.4825075   0.98767704  0.15303165 10.518439    7.3086348   6.9791408
  2.6091816   8.299133    0.01815706  4.3511486  11.220155    8.7436
  3.0565658   9.669933    1.9547316   9.169304    9.500812    7.900913
  6.929075    8.661879    3.0731711   8.227158    6.2899137   0.21501312
  4.4748106   1.6470722   9.777415    4.8366313   4.9664555   6.1797996
  5.3417387   2.1408544   8.709873    1.811256    4.8512735   1.5080717
  0.2304368   8.349218    9.6399975  10.511833  ]
Epoch 1/1000
Warning: The fraction of NaNs in loss is above threshold. Loss will be NaN.
JIT session error: Cannot allocate memory
JIT session error: Cannot allocate memory
JIT session error: Cannot allocate memory
LLVM ERROR: pthread_create failed: Resource temporarily unavailable
